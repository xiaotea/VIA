import os
from collections import deque
from datasketch import MinHashLSH, MinHash
import json


def minhash_file(file_path):
    """计算文件内容的 MinHash 值"""
    minhash = MinHash()

    try:

        with open(file_path, 'r', encoding=detect_encoding(file_path), errors='ignore') as f:
            for line in f:
                # 将每一行拆分为单词，编码为 UTF-8 并更新 MinHash
                words = [word.encode('utf-8') for word in line.strip().split()]
                for word in words:
                    minhash.update(word)

    except Exception as e:
        print(f"Error processing file {file_path}: {e}")
        return None

    return minhash



def detect_encoding(file_path):
    """检测文本文件的编码"""
    try:
        import chardet
        with open(file_path, 'rb') as f:
            raw_data = f.read(4096)  # 读取前 4KB 数据用于检测编码
        result = chardet.detect(raw_data)
        return result['encoding'] or 'utf-8'
    except Exception as e:
        print(f"Error detecting encoding for {file_path}: {e}")
        return 'utf-8'


from concurrent.futures import ThreadPoolExecutor, as_completed

black_list = (r'.py',)


def get_all_file_hashing(file_list):
    file_hash_dict = {}


    with ThreadPoolExecutor(max_workers=80) as executor:
        future_to_file = {executor.submit(minhash_file, file_path): file_path for file_path in file_list}

        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                minhash = future.result()
                if minhash:
                    file_hash_dict[file_path] = minhash
            except Exception as e:
                print(f"Error processing file {file_path}: {e}")

    return file_hash_dict


def find_closest_pairs_lsh(dict1, dict2, num_perm=128):
    """使用 LSH 优化最近对匹配"""

    pair_list = []
    # 遍历字典1和字典2的所有元素
    for key1, value1 in dict1.items():
        max_similarity = 0
        pair = []
        for key2, value2 in dict2.items():
            # 计算绝对差值
            similarity = value1.jaccard(value2)
            if similarity > max_similarity:
                max_similarity = similarity
                pair = [key1, key2]
            # differences.append([similarity, [key1, key2]])
        if not pair:
            continue
        pair_list.append([max_similarity, pair])
    return pair_list

def from_two_path_get_closest_pairs(vv1_file_list,vv2_file_list):
    reslist = []
    file_hashes_1 = get_all_file_hashing(vv1_file_list)
    file_hashes_2 = get_all_file_hashing(vv2_file_list)
    result = find_closest_pairs_lsh(file_hashes_1, file_hashes_2)
    for hash in result:
        if hash[0] < 0.05:
            continue
        reslist.append(hash)
    return reslist

if __name__ == "__main__":
    resdict = []
    # if flaw != 'flaw12':
    #     continue
    vv1_path = r"D:\D\个人文件\开源软件供应链安全\python漏洞影响性分析实验\知识图谱创建\PatchAnalyze\vul_file\0c0313f375bed7b035c8c0482bbb09599e16bfcf"
    vv2_path = r"F:\python_project\data\calibreweb@0.6.12"
    file_hashes_1 = get_all_file_hashing(vv1_path)
    # # 打印每个文件的 MinHash 值
    # for file_path, minhash in file_hashes_1.items():
    #     print(f"{file_path}: {minhash}")
    file_hashes_2 = get_all_file_hashing(vv2_path)

    # 匹配
    result = find_closest_pairs_lsh(file_hashes_1, file_hashes_2)

    for hash in result:
        if hash[0] < 0.2:
            continue
        resdict.append(hash)
    print(resdict)
