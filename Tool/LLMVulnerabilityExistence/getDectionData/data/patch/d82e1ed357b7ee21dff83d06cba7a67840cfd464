{
    "synapse/federation/transport/server/federation.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 509,
                "afterPatchRowNumber": 509,
                "PatchRowcode": "         event = content[\"event\"]"
            },
            "1": {
                "beforePatchRowNumber": 510,
                "afterPatchRowNumber": 510,
                "PatchRowcode": "         invite_room_state = content.get(\"invite_room_state\", [])"
            },
            "2": {
                "beforePatchRowNumber": 511,
                "afterPatchRowNumber": 511,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 512,
                "PatchRowcode": "+        if not isinstance(invite_room_state, list):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 513,
                "PatchRowcode": "+            invite_room_state = []"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 514,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 512,
                "afterPatchRowNumber": 515,
                "PatchRowcode": "         # Synapse expects invite_room_state to be in unsigned, as it is in v1"
            },
            "7": {
                "beforePatchRowNumber": 513,
                "afterPatchRowNumber": 516,
                "PatchRowcode": "         # API"
            },
            "8": {
                "beforePatchRowNumber": 514,
                "afterPatchRowNumber": 517,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "#  Copyright 2021 The Matrix.org Foundation C.I.C.",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "import logging",
            "from collections import Counter",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Dict,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Sequence,",
            "    Tuple,",
            "    Type,",
            "    Union,",
            ")",
            "",
            "from typing_extensions import Literal",
            "",
            "from synapse.api.constants import Direction, EduTypes",
            "from synapse.api.errors import Codes, SynapseError",
            "from synapse.api.room_versions import RoomVersions",
            "from synapse.api.urls import FEDERATION_UNSTABLE_PREFIX, FEDERATION_V2_PREFIX",
            "from synapse.federation.transport.server._base import (",
            "    Authenticator,",
            "    BaseFederationServlet,",
            ")",
            "from synapse.http.servlet import (",
            "    parse_boolean_from_args,",
            "    parse_integer,",
            "    parse_integer_from_args,",
            "    parse_string,",
            "    parse_string_from_args,",
            "    parse_strings_from_args,",
            ")",
            "from synapse.http.site import SynapseRequest",
            "from synapse.media._base import DEFAULT_MAX_TIMEOUT_MS, MAXIMUM_ALLOWED_MAX_TIMEOUT_MS",
            "from synapse.media.thumbnailer import ThumbnailProvider",
            "from synapse.types import JsonDict",
            "from synapse.util import SYNAPSE_VERSION",
            "from synapse.util.ratelimitutils import FederationRateLimiter",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "issue_8631_logger = logging.getLogger(\"synapse.8631_debug\")",
            "",
            "",
            "class BaseFederationServerServlet(BaseFederationServlet):",
            "    \"\"\"Abstract base class for federation servlet classes which provides a federation server handler.",
            "",
            "    See BaseFederationServlet for more information.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.handler = hs.get_federation_server()",
            "",
            "",
            "class FederationSendServlet(BaseFederationServerServlet):",
            "    PATH = \"/send/(?P<transaction_id>[^/]*)/?\"",
            "    CATEGORY = \"Inbound federation transaction request\"",
            "",
            "    # We ratelimit manually in the handler as we queue up the requests and we",
            "    # don't want to fill up the ratelimiter with blocked requests.",
            "    RATELIMIT = False",
            "",
            "    # This is when someone is trying to send us a bunch of data.",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        transaction_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        \"\"\"Called on PUT /send/<transaction_id>/",
            "",
            "        Args:",
            "            transaction_id: The transaction_id associated with this request. This",
            "                is *not* None.",
            "",
            "        Returns:",
            "            Tuple of `(code, response)`, where",
            "            `response` is a python dict to be converted into JSON that is",
            "            used as the response body.",
            "        \"\"\"",
            "        # Parse the request",
            "        try:",
            "            transaction_data = content",
            "",
            "            logger.debug(\"Decoded %s: %s\", transaction_id, str(transaction_data))",
            "",
            "            logger.info(",
            "                \"Received txn %s from %s. (PDUs: %d, EDUs: %d)\",",
            "                transaction_id,",
            "                origin,",
            "                len(transaction_data.get(\"pdus\", [])),",
            "                len(transaction_data.get(\"edus\", [])),",
            "            )",
            "",
            "            if issue_8631_logger.isEnabledFor(logging.DEBUG):",
            "                DEVICE_UPDATE_EDUS = [",
            "                    EduTypes.DEVICE_LIST_UPDATE,",
            "                    EduTypes.SIGNING_KEY_UPDATE,",
            "                ]",
            "                device_list_updates = [",
            "                    edu.get(\"content\", {})",
            "                    for edu in transaction_data.get(\"edus\", [])",
            "                    if edu.get(\"edu_type\") in DEVICE_UPDATE_EDUS",
            "                ]",
            "                if device_list_updates:",
            "                    issue_8631_logger.debug(",
            "                        \"received transaction [%s] including device list updates: %s\",",
            "                        transaction_id,",
            "                        device_list_updates,",
            "                    )",
            "",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            return 400, {\"error\": \"Invalid transaction\"}",
            "",
            "        code, response = await self.handler.on_incoming_transaction(",
            "            origin, transaction_id, self.server_name, transaction_data",
            "        )",
            "",
            "        return code, response",
            "",
            "",
            "class FederationEventServlet(BaseFederationServerServlet):",
            "    PATH = \"/event/(?P<event_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    # This is when someone asks for a data item for a given server data_id pair.",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        event_id: str,",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        return await self.handler.on_pdu_request(origin, event_id)",
            "",
            "",
            "class FederationStateV1Servlet(BaseFederationServerServlet):",
            "    PATH = \"/state/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    # This is when someone asks for all data for a given room.",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_room_state_request(",
            "            origin,",
            "            room_id,",
            "            parse_string_from_args(query, \"event_id\", None, required=True),",
            "        )",
            "",
            "",
            "class FederationStateIdsServlet(BaseFederationServerServlet):",
            "    PATH = \"/state_ids/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_state_ids_request(",
            "            origin,",
            "            room_id,",
            "            parse_string_from_args(query, \"event_id\", None, required=True),",
            "        )",
            "",
            "",
            "class FederationBackfillServlet(BaseFederationServerServlet):",
            "    PATH = \"/backfill/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        versions = [x.decode(\"ascii\") for x in query[b\"v\"]]",
            "        limit = parse_integer_from_args(query, \"limit\", None)",
            "",
            "        if not limit:",
            "            return 400, {\"error\": \"Did not include limit param\"}",
            "",
            "        return await self.handler.on_backfill_request(origin, room_id, versions, limit)",
            "",
            "",
            "class FederationTimestampLookupServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    API endpoint to fetch the `event_id` of the closest event to the given",
            "    timestamp (`ts` query parameter) in the given direction (`dir` query",
            "    parameter).",
            "",
            "    Useful for other homeservers when they're unable to find an event locally.",
            "",
            "    `ts` is a timestamp in milliseconds where we will find the closest event in",
            "    the given direction.",
            "",
            "    `dir` can be `f` or `b` to indicate forwards and backwards in time from the",
            "    given timestamp.",
            "",
            "    GET /_matrix/federation/v1/timestamp_to_event/<roomID>?ts=<timestamp>&dir=<direction>",
            "    {",
            "        \"event_id\": ...",
            "    }",
            "    \"\"\"",
            "",
            "    PATH = \"/timestamp_to_event/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        timestamp = parse_integer_from_args(query, \"ts\", required=True)",
            "        direction_str = parse_string_from_args(",
            "            query, \"dir\", allowed_values=[\"f\", \"b\"], required=True",
            "        )",
            "        direction = Direction(direction_str)",
            "",
            "        return await self.handler.on_timestamp_to_event_request(",
            "            origin, room_id, timestamp, direction",
            "        )",
            "",
            "",
            "class FederationQueryServlet(BaseFederationServerServlet):",
            "    PATH = \"/query/(?P<query_type>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    # This is when we receive a server-server Query",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        query_type: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        args = {k.decode(\"utf8\"): v[0].decode(\"utf-8\") for k, v in query.items()}",
            "        args[\"origin\"] = origin",
            "        return await self.handler.on_query_request(query_type, args)",
            "",
            "",
            "class FederationMakeJoinServlet(BaseFederationServerServlet):",
            "    PATH = \"/make_join/(?P<room_id>[^/]*)/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        \"\"\"",
            "        Args:",
            "            origin: The authenticated server_name of the calling server",
            "",
            "            content: (GETs don't have bodies)",
            "",
            "            query: Query params from the request.",
            "",
            "            **kwargs: the dict mapping keys to path components as specified in",
            "                the path match regexp.",
            "",
            "        Returns:",
            "            Tuple of (response code, response object)",
            "        \"\"\"",
            "        supported_versions = parse_strings_from_args(query, \"ver\", encoding=\"utf-8\")",
            "        if supported_versions is None:",
            "            supported_versions = [\"1\"]",
            "",
            "        result = await self.handler.on_make_join_request(",
            "            origin, room_id, user_id, supported_versions=supported_versions",
            "        )",
            "        return 200, result",
            "",
            "",
            "class FederationMakeLeaveServlet(BaseFederationServerServlet):",
            "    PATH = \"/make_leave/(?P<room_id>[^/]*)/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        result = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "        return 200, result",
            "",
            "",
            "class FederationV1SendLeaveServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_leave/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, Tuple[int, JsonDict]]:",
            "        result = await self.handler.on_send_leave_request(origin, content, room_id)",
            "        return 200, (200, result)",
            "",
            "",
            "class FederationV2SendLeaveServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_leave/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    PREFIX = FEDERATION_V2_PREFIX",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        result = await self.handler.on_send_leave_request(origin, content, room_id)",
            "        return 200, result",
            "",
            "",
            "class FederationMakeKnockServlet(BaseFederationServerServlet):",
            "    PATH = \"/make_knock/(?P<room_id>[^/]*)/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # Retrieve the room versions the remote homeserver claims to support",
            "        supported_versions = parse_strings_from_args(",
            "            query, \"ver\", required=True, encoding=\"utf-8\"",
            "        )",
            "",
            "        result = await self.handler.on_make_knock_request(",
            "            origin, room_id, user_id, supported_versions=supported_versions",
            "        )",
            "        return 200, result",
            "",
            "",
            "class FederationV1SendKnockServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_knock/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        result = await self.handler.on_send_knock_request(origin, content, room_id)",
            "        return 200, result",
            "",
            "",
            "class FederationEventAuthServlet(BaseFederationServerServlet):",
            "    PATH = \"/event_auth/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_event_auth(origin, room_id, event_id)",
            "",
            "",
            "class FederationV1SendJoinServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_join/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, Tuple[int, JsonDict]]:",
            "        # TODO(paul): assert that event_id parsed from path actually",
            "        #   match those given in content",
            "        result = await self.handler.on_send_join_request(origin, content, room_id)",
            "        return 200, (200, result)",
            "",
            "",
            "class FederationV2SendJoinServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_join/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    PREFIX = FEDERATION_V2_PREFIX",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # TODO(paul): assert that event_id parsed from path actually",
            "        #   match those given in content",
            "",
            "        partial_state = parse_boolean_from_args(query, \"omit_members\", default=False)",
            "",
            "        result = await self.handler.on_send_join_request(",
            "            origin, content, room_id, caller_supports_partial_state=partial_state",
            "        )",
            "        return 200, result",
            "",
            "",
            "class FederationV1InviteServlet(BaseFederationServerServlet):",
            "    PATH = \"/invite/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, Tuple[int, JsonDict]]:",
            "        # We don't get a room version, so we have to assume its EITHER v1 or",
            "        # v2. This is \"fine\" as the only difference between V1 and V2 is the",
            "        # state resolution algorithm, and we don't use that for processing",
            "        # invites",
            "        result = await self.handler.on_invite_request(",
            "            origin, content, room_version_id=RoomVersions.V1.identifier",
            "        )",
            "",
            "        # V1 federation API is defined to return a content of `[200, {...}]`",
            "        # due to a historical bug.",
            "        return 200, (200, result)",
            "",
            "",
            "class FederationV2InviteServlet(BaseFederationServerServlet):",
            "    PATH = \"/invite/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    PREFIX = FEDERATION_V2_PREFIX",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # TODO(paul): assert that room_id/event_id parsed from path actually",
            "        #   match those given in content",
            "",
            "        room_version = content[\"room_version\"]",
            "        event = content[\"event\"]",
            "        invite_room_state = content.get(\"invite_room_state\", [])",
            "",
            "        # Synapse expects invite_room_state to be in unsigned, as it is in v1",
            "        # API",
            "",
            "        event.setdefault(\"unsigned\", {})[\"invite_room_state\"] = invite_room_state",
            "",
            "        result = await self.handler.on_invite_request(",
            "            origin, event, room_version_id=room_version",
            "        )",
            "",
            "        # We only store invite_room_state for internal use, so remove it before",
            "        # returning the event to the remote homeserver.",
            "        result[\"event\"].get(\"unsigned\", {}).pop(\"invite_room_state\", None)",
            "",
            "        return 200, result",
            "",
            "",
            "class FederationThirdPartyInviteExchangeServlet(BaseFederationServerServlet):",
            "    PATH = \"/exchange_third_party_invite/(?P<room_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        await self.handler.on_exchange_third_party_invite_request(content)",
            "        return 200, {}",
            "",
            "",
            "class FederationClientKeysQueryServlet(BaseFederationServerServlet):",
            "    PATH = \"/user/keys/query\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self, origin: str, content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_query_client_keys(origin, content)",
            "",
            "",
            "class FederationUserDevicesQueryServlet(BaseFederationServerServlet):",
            "    PATH = \"/user/devices/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_query_user_devices(origin, user_id)",
            "",
            "",
            "class FederationClientKeysClaimServlet(BaseFederationServerServlet):",
            "    PATH = \"/user/keys/claim\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self, origin: str, content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        # Generate a count for each algorithm, which is hard-coded to 1.",
            "        key_query: List[Tuple[str, str, str, int]] = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithm in device_keys.items():",
            "                key_query.append((user_id, device_id, algorithm, 1))",
            "",
            "        response = await self.handler.on_claim_client_keys(",
            "            key_query, always_include_fallback_keys=False",
            "        )",
            "        return 200, response",
            "",
            "",
            "class FederationUnstableClientKeysClaimServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    Identical to the stable endpoint (FederationClientKeysClaimServlet) except",
            "    it allows for querying for multiple OTKs at once and always includes fallback",
            "    keys in the response.",
            "    \"\"\"",
            "",
            "    PREFIX = FEDERATION_UNSTABLE_PREFIX",
            "    PATH = \"/user/keys/claim\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self, origin: str, content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        # Generate a count for each algorithm.",
            "        key_query: List[Tuple[str, str, str, int]] = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithms in device_keys.items():",
            "                counts = Counter(algorithms)",
            "                for algorithm, count in counts.items():",
            "                    key_query.append((user_id, device_id, algorithm, count))",
            "",
            "        response = await self.handler.on_claim_client_keys(",
            "            key_query, always_include_fallback_keys=True",
            "        )",
            "        return 200, response",
            "",
            "",
            "class FederationGetMissingEventsServlet(BaseFederationServerServlet):",
            "    PATH = \"/get_missing_events/(?P<room_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        limit = int(content.get(\"limit\", 10))",
            "        earliest_events = content.get(\"earliest_events\", [])",
            "        latest_events = content.get(\"latest_events\", [])",
            "",
            "        result = await self.handler.on_get_missing_events(",
            "            origin,",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        return 200, result",
            "",
            "",
            "class On3pidBindServlet(BaseFederationServerServlet):",
            "    PATH = \"/3pid/onbind\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    REQUIRE_AUTH = False",
            "",
            "    async def on_POST(",
            "        self, origin: Optional[str], content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        if \"invites\" in content:",
            "            last_exception = None",
            "            for invite in content[\"invites\"]:",
            "                try:",
            "                    if \"signed\" not in invite or \"token\" not in invite[\"signed\"]:",
            "                        message = (",
            "                            \"Rejecting received notification of third-\"",
            "                            \"party invite without signed: %s\" % (invite,)",
            "                        )",
            "                        logger.info(message)",
            "                        raise SynapseError(400, message)",
            "                    await self.handler.exchange_third_party_invite(",
            "                        invite[\"sender\"],",
            "                        invite[\"mxid\"],",
            "                        invite[\"room_id\"],",
            "                        invite[\"signed\"],",
            "                    )",
            "                except Exception as e:",
            "                    last_exception = e",
            "            if last_exception:",
            "                raise last_exception",
            "        return 200, {}",
            "",
            "",
            "class FederationVersionServlet(BaseFederationServlet):",
            "    PATH = \"/version\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    REQUIRE_AUTH = False",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: Optional[str],",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "    ) -> Tuple[int, JsonDict]:",
            "        return (",
            "            200,",
            "            {",
            "                \"server\": {",
            "                    \"name\": \"Synapse\",",
            "                    \"version\": SYNAPSE_VERSION,",
            "                }",
            "            },",
            "        )",
            "",
            "",
            "class FederationRoomHierarchyServlet(BaseFederationServlet):",
            "    PATH = \"/hierarchy/(?P<room_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.handler = hs.get_room_summary_handler()",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Mapping[bytes, Sequence[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        suggested_only = parse_boolean_from_args(query, \"suggested_only\", default=False)",
            "        return 200, await self.handler.get_federation_hierarchy(",
            "            origin, room_id, suggested_only",
            "        )",
            "",
            "",
            "class RoomComplexityServlet(BaseFederationServlet):",
            "    \"\"\"",
            "    Indicates to other servers how complex (and therefore likely",
            "    resource-intensive) a public room this server knows about is.",
            "    \"\"\"",
            "",
            "    PATH = \"/rooms/(?P<room_id>[^/]*)/complexity\"",
            "    PREFIX = FEDERATION_UNSTABLE_PREFIX",
            "    CATEGORY = \"Federation requests (unstable)\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self._store = self.hs.get_datastores().main",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        is_public = await self._store.is_room_world_readable_or_publicly_joinable(",
            "            room_id",
            "        )",
            "",
            "        if not is_public:",
            "            raise SynapseError(404, \"Room not found\", errcode=Codes.INVALID_PARAM)",
            "",
            "        complexity = await self._store.get_room_complexity(room_id)",
            "        return 200, complexity",
            "",
            "",
            "class FederationAccountStatusServlet(BaseFederationServerServlet):",
            "    PATH = \"/query/account_status\"",
            "    PREFIX = FEDERATION_UNSTABLE_PREFIX + \"/org.matrix.msc3720\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self._account_handler = hs.get_account_handler()",
            "",
            "    async def on_POST(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Mapping[bytes, Sequence[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        if \"user_ids\" not in content:",
            "            raise SynapseError(",
            "                400, \"Required parameter 'user_ids' is missing\", Codes.MISSING_PARAM",
            "            )",
            "",
            "        statuses, failures = await self._account_handler.get_account_statuses(",
            "            content[\"user_ids\"],",
            "            allow_remote=False,",
            "        )",
            "",
            "        return 200, {\"account_statuses\": statuses, \"failures\": failures}",
            "",
            "",
            "class FederationMediaDownloadServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    Implementation of new federation media `/download` endpoint outlined in MSC3916. Returns",
            "    a multipart/mixed response consisting of a JSON object and the requested media",
            "    item. This endpoint only returns local media.",
            "    \"\"\"",
            "",
            "    PATH = \"/media/download/(?P<media_id>[^/]*)\"",
            "    RATELIMIT = True",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        ratelimiter: FederationRateLimiter,",
            "        authenticator: Authenticator,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.media_repo = self.hs.get_media_repository()",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: Optional[str],",
            "        content: Literal[None],",
            "        request: SynapseRequest,",
            "        media_id: str,",
            "    ) -> None:",
            "        max_timeout_ms = parse_integer(",
            "            request, \"timeout_ms\", default=DEFAULT_MAX_TIMEOUT_MS",
            "        )",
            "        max_timeout_ms = min(max_timeout_ms, MAXIMUM_ALLOWED_MAX_TIMEOUT_MS)",
            "        await self.media_repo.get_local_media(",
            "            request, media_id, None, max_timeout_ms, federation=True",
            "        )",
            "",
            "",
            "class FederationMediaThumbnailServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    Implementation of new federation media `/thumbnail` endpoint outlined in MSC3916. Returns",
            "    a multipart/mixed response consisting of a JSON object and the requested media",
            "    item. This endpoint only returns local media.",
            "    \"\"\"",
            "",
            "    PATH = \"/media/thumbnail/(?P<media_id>[^/]*)\"",
            "    RATELIMIT = True",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        ratelimiter: FederationRateLimiter,",
            "        authenticator: Authenticator,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.media_repo = self.hs.get_media_repository()",
            "        self.dynamic_thumbnails = hs.config.media.dynamic_thumbnails",
            "        self.thumbnail_provider = ThumbnailProvider(",
            "            hs, self.media_repo, self.media_repo.media_storage",
            "        )",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: Optional[str],",
            "        content: Literal[None],",
            "        request: SynapseRequest,",
            "        media_id: str,",
            "    ) -> None:",
            "        width = parse_integer(request, \"width\", required=True)",
            "        height = parse_integer(request, \"height\", required=True)",
            "        method = parse_string(request, \"method\", \"scale\")",
            "        # TODO Parse the Accept header to get an prioritised list of thumbnail types.",
            "        m_type = \"image/png\"",
            "        max_timeout_ms = parse_integer(",
            "            request, \"timeout_ms\", default=DEFAULT_MAX_TIMEOUT_MS",
            "        )",
            "        max_timeout_ms = min(max_timeout_ms, MAXIMUM_ALLOWED_MAX_TIMEOUT_MS)",
            "",
            "        if self.dynamic_thumbnails:",
            "            await self.thumbnail_provider.select_or_generate_local_thumbnail(",
            "                request, media_id, width, height, method, m_type, max_timeout_ms, True",
            "            )",
            "        else:",
            "            await self.thumbnail_provider.respond_local_thumbnail(",
            "                request, media_id, width, height, method, m_type, max_timeout_ms, True",
            "            )",
            "        self.media_repo.mark_recently_accessed(None, media_id)",
            "",
            "",
            "FEDERATION_SERVLET_CLASSES: Tuple[Type[BaseFederationServlet], ...] = (",
            "    FederationSendServlet,",
            "    FederationEventServlet,",
            "    FederationStateV1Servlet,",
            "    FederationStateIdsServlet,",
            "    FederationBackfillServlet,",
            "    FederationTimestampLookupServlet,",
            "    FederationQueryServlet,",
            "    FederationMakeJoinServlet,",
            "    FederationMakeLeaveServlet,",
            "    FederationEventServlet,",
            "    FederationV1SendJoinServlet,",
            "    FederationV2SendJoinServlet,",
            "    FederationV1SendLeaveServlet,",
            "    FederationV2SendLeaveServlet,",
            "    FederationV1InviteServlet,",
            "    FederationV2InviteServlet,",
            "    FederationGetMissingEventsServlet,",
            "    FederationEventAuthServlet,",
            "    FederationClientKeysQueryServlet,",
            "    FederationUserDevicesQueryServlet,",
            "    FederationClientKeysClaimServlet,",
            "    FederationUnstableClientKeysClaimServlet,",
            "    FederationThirdPartyInviteExchangeServlet,",
            "    On3pidBindServlet,",
            "    FederationVersionServlet,",
            "    RoomComplexityServlet,",
            "    FederationRoomHierarchyServlet,",
            "    FederationV1SendKnockServlet,",
            "    FederationMakeKnockServlet,",
            "    FederationAccountStatusServlet,",
            ")"
        ],
        "afterPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "#  Copyright 2021 The Matrix.org Foundation C.I.C.",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "import logging",
            "from collections import Counter",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Dict,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Sequence,",
            "    Tuple,",
            "    Type,",
            "    Union,",
            ")",
            "",
            "from typing_extensions import Literal",
            "",
            "from synapse.api.constants import Direction, EduTypes",
            "from synapse.api.errors import Codes, SynapseError",
            "from synapse.api.room_versions import RoomVersions",
            "from synapse.api.urls import FEDERATION_UNSTABLE_PREFIX, FEDERATION_V2_PREFIX",
            "from synapse.federation.transport.server._base import (",
            "    Authenticator,",
            "    BaseFederationServlet,",
            ")",
            "from synapse.http.servlet import (",
            "    parse_boolean_from_args,",
            "    parse_integer,",
            "    parse_integer_from_args,",
            "    parse_string,",
            "    parse_string_from_args,",
            "    parse_strings_from_args,",
            ")",
            "from synapse.http.site import SynapseRequest",
            "from synapse.media._base import DEFAULT_MAX_TIMEOUT_MS, MAXIMUM_ALLOWED_MAX_TIMEOUT_MS",
            "from synapse.media.thumbnailer import ThumbnailProvider",
            "from synapse.types import JsonDict",
            "from synapse.util import SYNAPSE_VERSION",
            "from synapse.util.ratelimitutils import FederationRateLimiter",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "issue_8631_logger = logging.getLogger(\"synapse.8631_debug\")",
            "",
            "",
            "class BaseFederationServerServlet(BaseFederationServlet):",
            "    \"\"\"Abstract base class for federation servlet classes which provides a federation server handler.",
            "",
            "    See BaseFederationServlet for more information.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.handler = hs.get_federation_server()",
            "",
            "",
            "class FederationSendServlet(BaseFederationServerServlet):",
            "    PATH = \"/send/(?P<transaction_id>[^/]*)/?\"",
            "    CATEGORY = \"Inbound federation transaction request\"",
            "",
            "    # We ratelimit manually in the handler as we queue up the requests and we",
            "    # don't want to fill up the ratelimiter with blocked requests.",
            "    RATELIMIT = False",
            "",
            "    # This is when someone is trying to send us a bunch of data.",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        transaction_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        \"\"\"Called on PUT /send/<transaction_id>/",
            "",
            "        Args:",
            "            transaction_id: The transaction_id associated with this request. This",
            "                is *not* None.",
            "",
            "        Returns:",
            "            Tuple of `(code, response)`, where",
            "            `response` is a python dict to be converted into JSON that is",
            "            used as the response body.",
            "        \"\"\"",
            "        # Parse the request",
            "        try:",
            "            transaction_data = content",
            "",
            "            logger.debug(\"Decoded %s: %s\", transaction_id, str(transaction_data))",
            "",
            "            logger.info(",
            "                \"Received txn %s from %s. (PDUs: %d, EDUs: %d)\",",
            "                transaction_id,",
            "                origin,",
            "                len(transaction_data.get(\"pdus\", [])),",
            "                len(transaction_data.get(\"edus\", [])),",
            "            )",
            "",
            "            if issue_8631_logger.isEnabledFor(logging.DEBUG):",
            "                DEVICE_UPDATE_EDUS = [",
            "                    EduTypes.DEVICE_LIST_UPDATE,",
            "                    EduTypes.SIGNING_KEY_UPDATE,",
            "                ]",
            "                device_list_updates = [",
            "                    edu.get(\"content\", {})",
            "                    for edu in transaction_data.get(\"edus\", [])",
            "                    if edu.get(\"edu_type\") in DEVICE_UPDATE_EDUS",
            "                ]",
            "                if device_list_updates:",
            "                    issue_8631_logger.debug(",
            "                        \"received transaction [%s] including device list updates: %s\",",
            "                        transaction_id,",
            "                        device_list_updates,",
            "                    )",
            "",
            "        except Exception as e:",
            "            logger.exception(e)",
            "            return 400, {\"error\": \"Invalid transaction\"}",
            "",
            "        code, response = await self.handler.on_incoming_transaction(",
            "            origin, transaction_id, self.server_name, transaction_data",
            "        )",
            "",
            "        return code, response",
            "",
            "",
            "class FederationEventServlet(BaseFederationServerServlet):",
            "    PATH = \"/event/(?P<event_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    # This is when someone asks for a data item for a given server data_id pair.",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        event_id: str,",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        return await self.handler.on_pdu_request(origin, event_id)",
            "",
            "",
            "class FederationStateV1Servlet(BaseFederationServerServlet):",
            "    PATH = \"/state/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    # This is when someone asks for all data for a given room.",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_room_state_request(",
            "            origin,",
            "            room_id,",
            "            parse_string_from_args(query, \"event_id\", None, required=True),",
            "        )",
            "",
            "",
            "class FederationStateIdsServlet(BaseFederationServerServlet):",
            "    PATH = \"/state_ids/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_state_ids_request(",
            "            origin,",
            "            room_id,",
            "            parse_string_from_args(query, \"event_id\", None, required=True),",
            "        )",
            "",
            "",
            "class FederationBackfillServlet(BaseFederationServerServlet):",
            "    PATH = \"/backfill/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        versions = [x.decode(\"ascii\") for x in query[b\"v\"]]",
            "        limit = parse_integer_from_args(query, \"limit\", None)",
            "",
            "        if not limit:",
            "            return 400, {\"error\": \"Did not include limit param\"}",
            "",
            "        return await self.handler.on_backfill_request(origin, room_id, versions, limit)",
            "",
            "",
            "class FederationTimestampLookupServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    API endpoint to fetch the `event_id` of the closest event to the given",
            "    timestamp (`ts` query parameter) in the given direction (`dir` query",
            "    parameter).",
            "",
            "    Useful for other homeservers when they're unable to find an event locally.",
            "",
            "    `ts` is a timestamp in milliseconds where we will find the closest event in",
            "    the given direction.",
            "",
            "    `dir` can be `f` or `b` to indicate forwards and backwards in time from the",
            "    given timestamp.",
            "",
            "    GET /_matrix/federation/v1/timestamp_to_event/<roomID>?ts=<timestamp>&dir=<direction>",
            "    {",
            "        \"event_id\": ...",
            "    }",
            "    \"\"\"",
            "",
            "    PATH = \"/timestamp_to_event/(?P<room_id>[^/]*)/?\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        timestamp = parse_integer_from_args(query, \"ts\", required=True)",
            "        direction_str = parse_string_from_args(",
            "            query, \"dir\", allowed_values=[\"f\", \"b\"], required=True",
            "        )",
            "        direction = Direction(direction_str)",
            "",
            "        return await self.handler.on_timestamp_to_event_request(",
            "            origin, room_id, timestamp, direction",
            "        )",
            "",
            "",
            "class FederationQueryServlet(BaseFederationServerServlet):",
            "    PATH = \"/query/(?P<query_type>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    # This is when we receive a server-server Query",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        query_type: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        args = {k.decode(\"utf8\"): v[0].decode(\"utf-8\") for k, v in query.items()}",
            "        args[\"origin\"] = origin",
            "        return await self.handler.on_query_request(query_type, args)",
            "",
            "",
            "class FederationMakeJoinServlet(BaseFederationServerServlet):",
            "    PATH = \"/make_join/(?P<room_id>[^/]*)/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        \"\"\"",
            "        Args:",
            "            origin: The authenticated server_name of the calling server",
            "",
            "            content: (GETs don't have bodies)",
            "",
            "            query: Query params from the request.",
            "",
            "            **kwargs: the dict mapping keys to path components as specified in",
            "                the path match regexp.",
            "",
            "        Returns:",
            "            Tuple of (response code, response object)",
            "        \"\"\"",
            "        supported_versions = parse_strings_from_args(query, \"ver\", encoding=\"utf-8\")",
            "        if supported_versions is None:",
            "            supported_versions = [\"1\"]",
            "",
            "        result = await self.handler.on_make_join_request(",
            "            origin, room_id, user_id, supported_versions=supported_versions",
            "        )",
            "        return 200, result",
            "",
            "",
            "class FederationMakeLeaveServlet(BaseFederationServerServlet):",
            "    PATH = \"/make_leave/(?P<room_id>[^/]*)/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        result = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "        return 200, result",
            "",
            "",
            "class FederationV1SendLeaveServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_leave/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, Tuple[int, JsonDict]]:",
            "        result = await self.handler.on_send_leave_request(origin, content, room_id)",
            "        return 200, (200, result)",
            "",
            "",
            "class FederationV2SendLeaveServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_leave/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    PREFIX = FEDERATION_V2_PREFIX",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        result = await self.handler.on_send_leave_request(origin, content, room_id)",
            "        return 200, result",
            "",
            "",
            "class FederationMakeKnockServlet(BaseFederationServerServlet):",
            "    PATH = \"/make_knock/(?P<room_id>[^/]*)/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # Retrieve the room versions the remote homeserver claims to support",
            "        supported_versions = parse_strings_from_args(",
            "            query, \"ver\", required=True, encoding=\"utf-8\"",
            "        )",
            "",
            "        result = await self.handler.on_make_knock_request(",
            "            origin, room_id, user_id, supported_versions=supported_versions",
            "        )",
            "        return 200, result",
            "",
            "",
            "class FederationV1SendKnockServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_knock/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        result = await self.handler.on_send_knock_request(origin, content, room_id)",
            "        return 200, result",
            "",
            "",
            "class FederationEventAuthServlet(BaseFederationServerServlet):",
            "    PATH = \"/event_auth/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_event_auth(origin, room_id, event_id)",
            "",
            "",
            "class FederationV1SendJoinServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_join/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, Tuple[int, JsonDict]]:",
            "        # TODO(paul): assert that event_id parsed from path actually",
            "        #   match those given in content",
            "        result = await self.handler.on_send_join_request(origin, content, room_id)",
            "        return 200, (200, result)",
            "",
            "",
            "class FederationV2SendJoinServlet(BaseFederationServerServlet):",
            "    PATH = \"/send_join/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    PREFIX = FEDERATION_V2_PREFIX",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # TODO(paul): assert that event_id parsed from path actually",
            "        #   match those given in content",
            "",
            "        partial_state = parse_boolean_from_args(query, \"omit_members\", default=False)",
            "",
            "        result = await self.handler.on_send_join_request(",
            "            origin, content, room_id, caller_supports_partial_state=partial_state",
            "        )",
            "        return 200, result",
            "",
            "",
            "class FederationV1InviteServlet(BaseFederationServerServlet):",
            "    PATH = \"/invite/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, Tuple[int, JsonDict]]:",
            "        # We don't get a room version, so we have to assume its EITHER v1 or",
            "        # v2. This is \"fine\" as the only difference between V1 and V2 is the",
            "        # state resolution algorithm, and we don't use that for processing",
            "        # invites",
            "        result = await self.handler.on_invite_request(",
            "            origin, content, room_version_id=RoomVersions.V1.identifier",
            "        )",
            "",
            "        # V1 federation API is defined to return a content of `[200, {...}]`",
            "        # due to a historical bug.",
            "        return 200, (200, result)",
            "",
            "",
            "class FederationV2InviteServlet(BaseFederationServerServlet):",
            "    PATH = \"/invite/(?P<room_id>[^/]*)/(?P<event_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    PREFIX = FEDERATION_V2_PREFIX",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # TODO(paul): assert that room_id/event_id parsed from path actually",
            "        #   match those given in content",
            "",
            "        room_version = content[\"room_version\"]",
            "        event = content[\"event\"]",
            "        invite_room_state = content.get(\"invite_room_state\", [])",
            "",
            "        if not isinstance(invite_room_state, list):",
            "            invite_room_state = []",
            "",
            "        # Synapse expects invite_room_state to be in unsigned, as it is in v1",
            "        # API",
            "",
            "        event.setdefault(\"unsigned\", {})[\"invite_room_state\"] = invite_room_state",
            "",
            "        result = await self.handler.on_invite_request(",
            "            origin, event, room_version_id=room_version",
            "        )",
            "",
            "        # We only store invite_room_state for internal use, so remove it before",
            "        # returning the event to the remote homeserver.",
            "        result[\"event\"].get(\"unsigned\", {}).pop(\"invite_room_state\", None)",
            "",
            "        return 200, result",
            "",
            "",
            "class FederationThirdPartyInviteExchangeServlet(BaseFederationServerServlet):",
            "    PATH = \"/exchange_third_party_invite/(?P<room_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_PUT(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        await self.handler.on_exchange_third_party_invite_request(content)",
            "        return 200, {}",
            "",
            "",
            "class FederationClientKeysQueryServlet(BaseFederationServerServlet):",
            "    PATH = \"/user/keys/query\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self, origin: str, content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_query_client_keys(origin, content)",
            "",
            "",
            "class FederationUserDevicesQueryServlet(BaseFederationServerServlet):",
            "    PATH = \"/user/devices/(?P<user_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        user_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        return await self.handler.on_query_user_devices(origin, user_id)",
            "",
            "",
            "class FederationClientKeysClaimServlet(BaseFederationServerServlet):",
            "    PATH = \"/user/keys/claim\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self, origin: str, content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        # Generate a count for each algorithm, which is hard-coded to 1.",
            "        key_query: List[Tuple[str, str, str, int]] = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithm in device_keys.items():",
            "                key_query.append((user_id, device_id, algorithm, 1))",
            "",
            "        response = await self.handler.on_claim_client_keys(",
            "            key_query, always_include_fallback_keys=False",
            "        )",
            "        return 200, response",
            "",
            "",
            "class FederationUnstableClientKeysClaimServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    Identical to the stable endpoint (FederationClientKeysClaimServlet) except",
            "    it allows for querying for multiple OTKs at once and always includes fallback",
            "    keys in the response.",
            "    \"\"\"",
            "",
            "    PREFIX = FEDERATION_UNSTABLE_PREFIX",
            "    PATH = \"/user/keys/claim\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self, origin: str, content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        # Generate a count for each algorithm.",
            "        key_query: List[Tuple[str, str, str, int]] = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithms in device_keys.items():",
            "                counts = Counter(algorithms)",
            "                for algorithm, count in counts.items():",
            "                    key_query.append((user_id, device_id, algorithm, count))",
            "",
            "        response = await self.handler.on_claim_client_keys(",
            "            key_query, always_include_fallback_keys=True",
            "        )",
            "        return 200, response",
            "",
            "",
            "class FederationGetMissingEventsServlet(BaseFederationServerServlet):",
            "    PATH = \"/get_missing_events/(?P<room_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    async def on_POST(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        limit = int(content.get(\"limit\", 10))",
            "        earliest_events = content.get(\"earliest_events\", [])",
            "        latest_events = content.get(\"latest_events\", [])",
            "",
            "        result = await self.handler.on_get_missing_events(",
            "            origin,",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        return 200, result",
            "",
            "",
            "class On3pidBindServlet(BaseFederationServerServlet):",
            "    PATH = \"/3pid/onbind\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    REQUIRE_AUTH = False",
            "",
            "    async def on_POST(",
            "        self, origin: Optional[str], content: JsonDict, query: Dict[bytes, List[bytes]]",
            "    ) -> Tuple[int, JsonDict]:",
            "        if \"invites\" in content:",
            "            last_exception = None",
            "            for invite in content[\"invites\"]:",
            "                try:",
            "                    if \"signed\" not in invite or \"token\" not in invite[\"signed\"]:",
            "                        message = (",
            "                            \"Rejecting received notification of third-\"",
            "                            \"party invite without signed: %s\" % (invite,)",
            "                        )",
            "                        logger.info(message)",
            "                        raise SynapseError(400, message)",
            "                    await self.handler.exchange_third_party_invite(",
            "                        invite[\"sender\"],",
            "                        invite[\"mxid\"],",
            "                        invite[\"room_id\"],",
            "                        invite[\"signed\"],",
            "                    )",
            "                except Exception as e:",
            "                    last_exception = e",
            "            if last_exception:",
            "                raise last_exception",
            "        return 200, {}",
            "",
            "",
            "class FederationVersionServlet(BaseFederationServlet):",
            "    PATH = \"/version\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    REQUIRE_AUTH = False",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: Optional[str],",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "    ) -> Tuple[int, JsonDict]:",
            "        return (",
            "            200,",
            "            {",
            "                \"server\": {",
            "                    \"name\": \"Synapse\",",
            "                    \"version\": SYNAPSE_VERSION,",
            "                }",
            "            },",
            "        )",
            "",
            "",
            "class FederationRoomHierarchyServlet(BaseFederationServlet):",
            "    PATH = \"/hierarchy/(?P<room_id>[^/]*)\"",
            "    CATEGORY = \"Federation requests\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.handler = hs.get_room_summary_handler()",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Mapping[bytes, Sequence[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        suggested_only = parse_boolean_from_args(query, \"suggested_only\", default=False)",
            "        return 200, await self.handler.get_federation_hierarchy(",
            "            origin, room_id, suggested_only",
            "        )",
            "",
            "",
            "class RoomComplexityServlet(BaseFederationServlet):",
            "    \"\"\"",
            "    Indicates to other servers how complex (and therefore likely",
            "    resource-intensive) a public room this server knows about is.",
            "    \"\"\"",
            "",
            "    PATH = \"/rooms/(?P<room_id>[^/]*)/complexity\"",
            "    PREFIX = FEDERATION_UNSTABLE_PREFIX",
            "    CATEGORY = \"Federation requests (unstable)\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self._store = self.hs.get_datastores().main",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: str,",
            "        content: Literal[None],",
            "        query: Dict[bytes, List[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        is_public = await self._store.is_room_world_readable_or_publicly_joinable(",
            "            room_id",
            "        )",
            "",
            "        if not is_public:",
            "            raise SynapseError(404, \"Room not found\", errcode=Codes.INVALID_PARAM)",
            "",
            "        complexity = await self._store.get_room_complexity(room_id)",
            "        return 200, complexity",
            "",
            "",
            "class FederationAccountStatusServlet(BaseFederationServerServlet):",
            "    PATH = \"/query/account_status\"",
            "    PREFIX = FEDERATION_UNSTABLE_PREFIX + \"/org.matrix.msc3720\"",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        authenticator: Authenticator,",
            "        ratelimiter: FederationRateLimiter,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self._account_handler = hs.get_account_handler()",
            "",
            "    async def on_POST(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        query: Mapping[bytes, Sequence[bytes]],",
            "        room_id: str,",
            "    ) -> Tuple[int, JsonDict]:",
            "        if \"user_ids\" not in content:",
            "            raise SynapseError(",
            "                400, \"Required parameter 'user_ids' is missing\", Codes.MISSING_PARAM",
            "            )",
            "",
            "        statuses, failures = await self._account_handler.get_account_statuses(",
            "            content[\"user_ids\"],",
            "            allow_remote=False,",
            "        )",
            "",
            "        return 200, {\"account_statuses\": statuses, \"failures\": failures}",
            "",
            "",
            "class FederationMediaDownloadServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    Implementation of new federation media `/download` endpoint outlined in MSC3916. Returns",
            "    a multipart/mixed response consisting of a JSON object and the requested media",
            "    item. This endpoint only returns local media.",
            "    \"\"\"",
            "",
            "    PATH = \"/media/download/(?P<media_id>[^/]*)\"",
            "    RATELIMIT = True",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        ratelimiter: FederationRateLimiter,",
            "        authenticator: Authenticator,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.media_repo = self.hs.get_media_repository()",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: Optional[str],",
            "        content: Literal[None],",
            "        request: SynapseRequest,",
            "        media_id: str,",
            "    ) -> None:",
            "        max_timeout_ms = parse_integer(",
            "            request, \"timeout_ms\", default=DEFAULT_MAX_TIMEOUT_MS",
            "        )",
            "        max_timeout_ms = min(max_timeout_ms, MAXIMUM_ALLOWED_MAX_TIMEOUT_MS)",
            "        await self.media_repo.get_local_media(",
            "            request, media_id, None, max_timeout_ms, federation=True",
            "        )",
            "",
            "",
            "class FederationMediaThumbnailServlet(BaseFederationServerServlet):",
            "    \"\"\"",
            "    Implementation of new federation media `/thumbnail` endpoint outlined in MSC3916. Returns",
            "    a multipart/mixed response consisting of a JSON object and the requested media",
            "    item. This endpoint only returns local media.",
            "    \"\"\"",
            "",
            "    PATH = \"/media/thumbnail/(?P<media_id>[^/]*)\"",
            "    RATELIMIT = True",
            "",
            "    def __init__(",
            "        self,",
            "        hs: \"HomeServer\",",
            "        ratelimiter: FederationRateLimiter,",
            "        authenticator: Authenticator,",
            "        server_name: str,",
            "    ):",
            "        super().__init__(hs, authenticator, ratelimiter, server_name)",
            "        self.media_repo = self.hs.get_media_repository()",
            "        self.dynamic_thumbnails = hs.config.media.dynamic_thumbnails",
            "        self.thumbnail_provider = ThumbnailProvider(",
            "            hs, self.media_repo, self.media_repo.media_storage",
            "        )",
            "",
            "    async def on_GET(",
            "        self,",
            "        origin: Optional[str],",
            "        content: Literal[None],",
            "        request: SynapseRequest,",
            "        media_id: str,",
            "    ) -> None:",
            "        width = parse_integer(request, \"width\", required=True)",
            "        height = parse_integer(request, \"height\", required=True)",
            "        method = parse_string(request, \"method\", \"scale\")",
            "        # TODO Parse the Accept header to get an prioritised list of thumbnail types.",
            "        m_type = \"image/png\"",
            "        max_timeout_ms = parse_integer(",
            "            request, \"timeout_ms\", default=DEFAULT_MAX_TIMEOUT_MS",
            "        )",
            "        max_timeout_ms = min(max_timeout_ms, MAXIMUM_ALLOWED_MAX_TIMEOUT_MS)",
            "",
            "        if self.dynamic_thumbnails:",
            "            await self.thumbnail_provider.select_or_generate_local_thumbnail(",
            "                request, media_id, width, height, method, m_type, max_timeout_ms, True",
            "            )",
            "        else:",
            "            await self.thumbnail_provider.respond_local_thumbnail(",
            "                request, media_id, width, height, method, m_type, max_timeout_ms, True",
            "            )",
            "        self.media_repo.mark_recently_accessed(None, media_id)",
            "",
            "",
            "FEDERATION_SERVLET_CLASSES: Tuple[Type[BaseFederationServlet], ...] = (",
            "    FederationSendServlet,",
            "    FederationEventServlet,",
            "    FederationStateV1Servlet,",
            "    FederationStateIdsServlet,",
            "    FederationBackfillServlet,",
            "    FederationTimestampLookupServlet,",
            "    FederationQueryServlet,",
            "    FederationMakeJoinServlet,",
            "    FederationMakeLeaveServlet,",
            "    FederationEventServlet,",
            "    FederationV1SendJoinServlet,",
            "    FederationV2SendJoinServlet,",
            "    FederationV1SendLeaveServlet,",
            "    FederationV2SendLeaveServlet,",
            "    FederationV1InviteServlet,",
            "    FederationV2InviteServlet,",
            "    FederationGetMissingEventsServlet,",
            "    FederationEventAuthServlet,",
            "    FederationClientKeysQueryServlet,",
            "    FederationUserDevicesQueryServlet,",
            "    FederationClientKeysClaimServlet,",
            "    FederationUnstableClientKeysClaimServlet,",
            "    FederationThirdPartyInviteExchangeServlet,",
            "    On3pidBindServlet,",
            "    FederationVersionServlet,",
            "    RoomComplexityServlet,",
            "    FederationRoomHierarchyServlet,",
            "    FederationV1SendKnockServlet,",
            "    FederationMakeKnockServlet,",
            "    FederationAccountStatusServlet,",
            ")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.federation.transport.server.federation.FederationV2InviteServlet.self"
        ]
    },
    "synapse/handlers/federation.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 880,
                "afterPatchRowNumber": 880,
                "PatchRowcode": "         if stripped_room_state is None:"
            },
            "1": {
                "beforePatchRowNumber": 881,
                "afterPatchRowNumber": 881,
                "PatchRowcode": "             raise KeyError(\"Missing 'knock_room_state' field in send_knock response\")"
            },
            "2": {
                "beforePatchRowNumber": 882,
                "afterPatchRowNumber": 882,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 883,
                "PatchRowcode": "+        if not isinstance(stripped_room_state, list):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 884,
                "PatchRowcode": "+            raise TypeError(\"'knock_room_state' has wrong type\")"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 885,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 883,
                "afterPatchRowNumber": 886,
                "PatchRowcode": "         event.unsigned[\"knock_room_state\"] = stripped_room_state"
            },
            "7": {
                "beforePatchRowNumber": 884,
                "afterPatchRowNumber": 887,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 885,
                "afterPatchRowNumber": 888,
                "PatchRowcode": "         context = EventContext.for_outlier(self._storage_controllers)"
            }
        },
        "frontPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright 2020 Sorunome",
            "# Copyright 2014-2022 The Matrix.org Foundation C.I.C.",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "",
            "\"\"\"Contains handlers for federation events.\"\"\"",
            "",
            "import enum",
            "import itertools",
            "import logging",
            "from enum import Enum",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    AbstractSet,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "import attr",
            "from prometheus_client import Histogram",
            "from signedjson.key import decode_verify_key_bytes",
            "from signedjson.sign import verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import MAX_DEPTH, EventContentFields, EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    CodeMessageException,",
            "    Codes,",
            "    FederationDeniedError,",
            "    FederationError,",
            "    FederationPullAttemptBackoffError,",
            "    HttpResponseException,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext, UnpersistedEventContextBase",
            "from synapse.events.validator import EventValidator",
            "from synapse.federation.federation_client import InvalidResponseError",
            "from synapse.handlers.pagination import PURGE_PAGINATION_LOCK_NAME",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import SynapseTags, set_tag, tag_args, trace",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.module_api import NOT_SPAM",
            "from synapse.replication.http.federation import (",
            "    ReplicationCleanRoomRestServlet,",
            "    ReplicationStoreRoomOnOutlierMembershipRestServlet,",
            ")",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import JsonDict, StrCollection, get_domain_from_id",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.visibility import filter_events_for_server",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_before_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_before_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        30.0,",
            "        40.0,",
            "        60.0,",
            "        80.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "# TODO: We can refactor this away now that there is only one backfill point again",
            "class _BackfillPointType(Enum):",
            "    # a regular backwards extremity (ie, an event which we don't yet have, but which",
            "    # is referred to by other events in the DAG)",
            "    BACKWARDS_EXTREMITY = enum.auto()",
            "",
            "",
            "@attr.s(slots=True, auto_attribs=True, frozen=True)",
            "class _BackfillPoint:",
            "    \"\"\"A potential point we might backfill from\"\"\"",
            "",
            "    event_id: str",
            "    depth: int",
            "    type: _BackfillPointType",
            "",
            "",
            "class FederationHandler:",
            "    \"\"\"Handles general incoming federation requests",
            "",
            "    Incoming events are *not* handled here, for which see FederationEventHandler.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "",
            "        self.clock = hs.get_clock()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self.federation_client = hs.get_federation_client()",
            "        self.state_handler = hs.get_state_handler()",
            "        self.server_name = hs.hostname",
            "        self.keyring = hs.get_keyring()",
            "        self.is_mine_id = hs.is_mine_id",
            "        self.is_mine_server_name = hs.is_mine_server_name",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self.event_creation_handler = hs.get_event_creation_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._server_notices_mxid = hs.config.servernotices.server_notices_mxid",
            "        self.config = hs.config",
            "        self.http_client = hs.get_proxied_blocklisted_http_client()",
            "        self._replication = hs.get_replication_data_handler()",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self._device_handler = hs.get_device_handler()",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "        self._notifier = hs.get_notifier()",
            "        self._worker_locks = hs.get_worker_locks_handler()",
            "",
            "        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(",
            "            hs",
            "        )",
            "",
            "        if hs.config.worker.worker_app:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                self.store.maybe_store_room_on_outlier_membership",
            "            )",
            "",
            "        self._room_backfill = Linearizer(\"room_backfill\")",
            "",
            "        self._third_party_event_rules = (",
            "            hs.get_module_api_callbacks().third_party_event_rules",
            "        )",
            "",
            "        # Tracks running partial state syncs by room ID.",
            "        # Partial state syncs currently only run on the main process, so it's okay to",
            "        # track them in-memory for now.",
            "        self._active_partial_state_syncs: Set[str] = set()",
            "        # Tracks partial state syncs we may want to restart.",
            "        # A dictionary mapping room IDs to (initial destination, other destinations)",
            "        # tuples.",
            "        self._partial_state_syncs_maybe_needing_restart: Dict[",
            "            str, Tuple[Optional[str], AbstractSet[str]]",
            "        ] = {}",
            "        # A lock guarding the partial state flag for rooms.",
            "        # When the lock is held for a given room, no other concurrent code may",
            "        # partial state or un-partial state the room.",
            "        self._is_partial_state_room_linearizer = Linearizer(",
            "            name=\"_is_partial_state_room_linearizer\"",
            "        )",
            "",
            "        # if this is the main process, fire off a background process to resume",
            "        # any partial-state-resync operations which were in flight when we",
            "        # were shut down.",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"resume_sync_partial_state_room\", self._resume_partial_state_room_sync",
            "            )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def maybe_backfill(",
            "        self, room_id: str, current_depth: int, limit: int, record_time: bool = True",
            "    ) -> bool:",
            "        \"\"\"Checks the database to see if we should backfill before paginating,",
            "        and if so do.",
            "",
            "        Args:",
            "            room_id",
            "            current_depth: The depth from which we're paginating from. This is",
            "                used to decide if we should backfill and what extremities to",
            "                use.",
            "            limit: The number of events that the pagination request will",
            "                return. This is used as part of the heuristic to decide if we",
            "                should back paginate.",
            "            record_time: Whether to record the time it takes to backfill.",
            "",
            "        Returns:",
            "            True if we actually tried to backfill something, otherwise False.",
            "        \"\"\"",
            "        # Starting the processing time here so we can include the room backfill",
            "        # linearizer lock queue in the timing",
            "        processing_start_time = self.clock.time_msec() if record_time else 0",
            "",
            "        async with self._room_backfill.queue(room_id):",
            "            async with self._worker_locks.acquire_read_write_lock(",
            "                PURGE_PAGINATION_LOCK_NAME, room_id, write=False",
            "            ):",
            "                return await self._maybe_backfill_inner(",
            "                    room_id,",
            "                    current_depth,",
            "                    limit,",
            "                    processing_start_time=processing_start_time,",
            "                )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _maybe_backfill_inner(",
            "        self,",
            "        room_id: str,",
            "        current_depth: int,",
            "        limit: int,",
            "        *,",
            "        processing_start_time: Optional[int],",
            "    ) -> bool:",
            "        \"\"\"",
            "        Checks whether the `current_depth` is at or approaching any backfill",
            "        points in the room and if so, will backfill. We only care about",
            "        checking backfill points that happened before the `current_depth`",
            "        (meaning less than or equal to the `current_depth`).",
            "",
            "        Args:",
            "            room_id: The room to backfill in.",
            "            current_depth: The depth to check at for any upcoming backfill points.",
            "            limit: The max number of events to request from the remote federated server.",
            "            processing_start_time: The time when `maybe_backfill` started processing.",
            "                Only used for timing. If `None`, no timing observation will be made.",
            "",
            "        Returns:",
            "            True if we actually tried to backfill something, otherwise False.",
            "        \"\"\"",
            "        backwards_extremities = [",
            "            _BackfillPoint(event_id, depth, _BackfillPointType.BACKWARDS_EXTREMITY)",
            "            for event_id, depth in await self.store.get_backfill_points_in_room(",
            "                room_id=room_id,",
            "                current_depth=current_depth,",
            "                # We only need to end up with 5 extremities combined with the",
            "                # insertion event extremities to make the `/backfill` request",
            "                # but fetch an order of magnitude more to make sure there is",
            "                # enough even after we filter them by whether visible in the",
            "                # history. This isn't fool-proof as all backfill points within",
            "                # our limit could be filtered out but seems like a good amount",
            "                # to try with at least.",
            "                limit=50,",
            "            )",
            "        ]",
            "",
            "        # we now have a list of potential places to backpaginate from. We prefer to",
            "        # start with the most recent (ie, max depth), so let's sort the list.",
            "        sorted_backfill_points: List[_BackfillPoint] = sorted(",
            "            backwards_extremities,",
            "            key=lambda e: -int(e.depth),",
            "        )",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: room_id: %s: current_depth: %s, limit: %s, \"",
            "            \"backfill points (%d): %s\",",
            "            room_id,",
            "            current_depth,",
            "            limit,",
            "            len(sorted_backfill_points),",
            "            sorted_backfill_points,",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"sorted_backfill_points\",",
            "            str(sorted_backfill_points),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"sorted_backfill_points.length\",",
            "            str(len(sorted_backfill_points)),",
            "        )",
            "",
            "        # If we have no backfill points lower than the `current_depth` then either we",
            "        # can a) bail or b) still attempt to backfill. We opt to try backfilling anyway",
            "        # just in case we do get relevant events. This is good for eventual consistency",
            "        # sake but we don't need to block the client for something that is just as",
            "        # likely not to return anything relevant so we backfill in the background. The",
            "        # only way, this could return something relevant is if we discover a new branch",
            "        # of history that extends all the way back to where we are currently paginating",
            "        # and it's within the 100 events that are returned from `/backfill`.",
            "        if not sorted_backfill_points and current_depth != MAX_DEPTH:",
            "            # Check that we actually have later backfill points, if not just return.",
            "            have_later_backfill_points = await self.store.get_backfill_points_in_room(",
            "                room_id=room_id,",
            "                current_depth=MAX_DEPTH,",
            "                limit=1,",
            "            )",
            "            if not have_later_backfill_points:",
            "                return False",
            "",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: all backfill points are *after* current depth. Trying again with later backfill points.\"",
            "            )",
            "            run_as_background_process(",
            "                \"_maybe_backfill_inner_anyway_with_max_depth\",",
            "                self.maybe_backfill,",
            "                room_id=room_id,",
            "                # We use `MAX_DEPTH` so that we find all backfill points next",
            "                # time (all events are below the `MAX_DEPTH`)",
            "                current_depth=MAX_DEPTH,",
            "                limit=limit,",
            "                # We don't want to start another timing observation from this",
            "                # nested recursive call. The top-most call can record the time",
            "                # overall otherwise the smaller one will throw off the results.",
            "                record_time=False,",
            "            )",
            "            # We return `False` because we're backfilling in the background and there is",
            "            # no new events immediately for the caller to know about yet.",
            "            return False",
            "",
            "        # Even after recursing with `MAX_DEPTH`, we didn't find any",
            "        # backward extremities to backfill from.",
            "        if not sorted_backfill_points:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: Not backfilling as no backward extremeties found.\"",
            "            )",
            "            return False",
            "",
            "        # If we're approaching an extremity we trigger a backfill, otherwise we",
            "        # no-op.",
            "        #",
            "        # We chose twice the limit here as then clients paginating backwards",
            "        # will send pagination requests that trigger backfill at least twice",
            "        # using the most recent extremity before it gets removed (see below). We",
            "        # chose more than one times the limit in case of failure, but choosing a",
            "        # much larger factor will result in triggering a backfill request much",
            "        # earlier than necessary.",
            "        max_depth_of_backfill_points = sorted_backfill_points[0].depth",
            "        if current_depth - 2 * limit > max_depth_of_backfill_points:",
            "            logger.debug(",
            "                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",",
            "                max_depth_of_backfill_points,",
            "                current_depth,",
            "                limit,",
            "            )",
            "            return False",
            "",
            "        # For performance's sake, we only want to paginate from a particular extremity",
            "        # if we can actually see the events we'll get. Otherwise, we'd just spend a lot",
            "        # of resources to get redacted events. We check each extremity in turn and",
            "        # ignore those which users on our server wouldn't be able to see.",
            "        #",
            "        # Additionally, we limit ourselves to backfilling from at most 5 extremities,",
            "        # for two reasons:",
            "        #",
            "        # - The check which determines if we can see an extremity's events can be",
            "        #   expensive (we load the full state for the room at each of the backfill",
            "        #   points, or (worse) their successors)",
            "        # - We want to avoid the server-server API request URI becoming too long.",
            "        #",
            "        # *Note*: the spec wants us to keep backfilling until we reach the start",
            "        # of the room in case we are allowed to see some of the history. However,",
            "        # in practice that causes more issues than its worth, as (a) it's",
            "        # relatively rare for there to be any visible history and (b) even when",
            "        # there is it's often sufficiently long ago that clients would stop",
            "        # attempting to paginate before backfill reached the visible history.",
            "",
            "        extremities_to_request: List[str] = []",
            "        for bp in sorted_backfill_points:",
            "            if len(extremities_to_request) >= 5:",
            "                break",
            "",
            "            # For regular backwards extremities, we don't have the extremity events",
            "            # themselves, so we need to actually check the events that reference them -",
            "            # their \"successor\" events.",
            "            #",
            "            # TODO: Correctly handle the case where we are allowed to see the",
            "            #   successor event but not the backward extremity, e.g. in the case of",
            "            #   initial join of the server where we are allowed to see the join",
            "            #   event but not anything before it. This would require looking at the",
            "            #   state *before* the event, ignoring the special casing certain event",
            "            #   types have.",
            "            event_ids_to_check = await self.store.get_successor_events(bp.event_id)",
            "",
            "            events_to_check = await self.store.get_events_as_list(",
            "                event_ids_to_check,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                get_prev_content=False,",
            "            )",
            "",
            "            # We unset `filter_out_erased_senders` as we might otherwise get false",
            "            # positives from users having been erased.",
            "            filtered_extremities = await filter_events_for_server(",
            "                self._storage_controllers,",
            "                self.server_name,",
            "                self.server_name,",
            "                events_to_check,",
            "                redact=False,",
            "                filter_out_erased_senders=False,",
            "                filter_out_remote_partial_state_events=False,",
            "            )",
            "            if filtered_extremities:",
            "                extremities_to_request.append(bp.event_id)",
            "            else:",
            "                logger.debug(",
            "                    \"_maybe_backfill_inner: skipping extremity %s as it would not be visible\",",
            "                    bp,",
            "                )",
            "",
            "        if not extremities_to_request:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: found no extremities which would be visible\"",
            "            )",
            "            return False",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: extremities_to_request %s\", extremities_to_request",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request\",",
            "            str(extremities_to_request),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request.length\",",
            "            str(len(extremities_to_request)),",
            "        )",
            "",
            "        # Now we need to decide which hosts to hit first.",
            "        # First we try hosts that are already in the room.",
            "        # TODO: HEURISTIC ALERT.",
            "        likely_domains = (",
            "            await self._storage_controllers.state.get_current_hosts_in_room_ordered(",
            "                room_id",
            "            )",
            "        )",
            "",
            "        async def try_backfill(domains: StrCollection) -> bool:",
            "            # TODO: Should we try multiple of these at a time?",
            "",
            "            # Number of contacted remote homeservers that have denied our backfill",
            "            # request with a 4xx code.",
            "            denied_count = 0",
            "",
            "            # Maximum number of contacted remote homeservers that can deny our",
            "            # backfill request with 4xx codes before we give up.",
            "            max_denied_count = 5",
            "",
            "            for dom in domains:",
            "                # We don't want to ask our own server for information we don't have",
            "                if self.is_mine_server_name(dom):",
            "                    continue",
            "",
            "                try:",
            "                    await self._federation_event_handler.backfill(",
            "                        dom, room_id, limit=100, extremities=extremities_to_request",
            "                    )",
            "                    # If this succeeded then we probably already have the",
            "                    # appropriate stuff.",
            "                    # TODO: We can probably do something more intelligent here.",
            "                    return True",
            "                except NotRetryingDestination as e:",
            "                    logger.info(\"_maybe_backfill_inner: %s\", e)",
            "                    continue",
            "                except FederationDeniedError:",
            "                    logger.info(",
            "                        \"_maybe_backfill_inner: Not attempting to backfill from %s because the homeserver is not on our federation whitelist\",",
            "                        dom,",
            "                    )",
            "                    continue",
            "                except (SynapseError, InvalidResponseError) as e:",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except HttpResponseException as e:",
            "                    if 400 <= e.code < 500:",
            "                        logger.warning(",
            "                            \"Backfill denied from %s because %s [%d/%d]\",",
            "                            dom,",
            "                            e,",
            "                            denied_count,",
            "                            max_denied_count,",
            "                        )",
            "                        denied_count += 1",
            "                        if denied_count >= max_denied_count:",
            "                            return False",
            "                        continue",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except CodeMessageException as e:",
            "                    if 400 <= e.code < 500:",
            "                        logger.warning(",
            "                            \"Backfill denied from %s because %s [%d/%d]\",",
            "                            dom,",
            "                            e,",
            "                            denied_count,",
            "                            max_denied_count,",
            "                        )",
            "                        denied_count += 1",
            "                        if denied_count >= max_denied_count:",
            "                            return False",
            "                        continue",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except RequestSendFailed as e:",
            "                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except Exception as e:",
            "                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "",
            "            return False",
            "",
            "        # If we have the `processing_start_time`, then we can make an",
            "        # observation. We wouldn't have the `processing_start_time` in the case",
            "        # where `_maybe_backfill_inner` is recursively called to find any",
            "        # backfill points regardless of `current_depth`.",
            "        if processing_start_time is not None:",
            "            processing_end_time = self.clock.time_msec()",
            "            backfill_processing_before_timer.observe(",
            "                (processing_end_time - processing_start_time) / 1000",
            "            )",
            "",
            "        success = await try_backfill(likely_domains)",
            "        if success:",
            "            return True",
            "",
            "        # TODO: we could also try servers which were previously in the room, but",
            "        #   are no longer.",
            "",
            "        return False",
            "",
            "    async def send_invite(self, target_host: str, event: EventBase) -> EventBase:",
            "        \"\"\"Sends the invite to the remote server for signing.",
            "",
            "        Invites must be signed by the invitee's server before distribution.",
            "        \"\"\"",
            "        try:",
            "            pdu = await self.federation_client.send_invite(",
            "                destination=target_host,",
            "                room_id=event.room_id,",
            "                event_id=event.event_id,",
            "                pdu=event,",
            "            )",
            "        except RequestSendFailed:",
            "            raise SynapseError(502, f\"Can't connect to server {target_host}\")",
            "",
            "        return pdu",
            "",
            "    async def on_event_auth(self, event_id: str) -> List[EventBase]:",
            "        event = await self.store.get_event(event_id)",
            "        auth = await self.store.get_auth_chain(",
            "            event.room_id, list(event.auth_event_ids()), include_given=True",
            "        )",
            "        return list(auth)",
            "",
            "    async def do_invite_join(",
            "        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Attempts to join the `joinee` to the room `room_id` via the",
            "        servers contained in `target_hosts`.",
            "",
            "        This first triggers a /make_join/ request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via /send_join/ which responds with the state at that",
            "        event and the auth_chains.",
            "",
            "        We suspend processing of any received events from this room until we",
            "        have finished processing the join.",
            "",
            "        Args:",
            "            target_hosts: List of servers to attempt to join the room with.",
            "",
            "            room_id: The ID of the room to join.",
            "",
            "            joinee: The User ID of the joining user.",
            "",
            "            content: The event content to use for the join event.",
            "        \"\"\"",
            "        # TODO: We should be able to call this on workers, but the upgrading of",
            "        # room stuff after join currently doesn't work on workers.",
            "        # TODO: Before we relax this condition, we need to allow re-syncing of",
            "        # partial room state to happen on workers.",
            "        assert self.config.worker.worker_app is None",
            "",
            "        logger.debug(\"Joining %s to %s\", joinee, room_id)",
            "",
            "        origin, event, room_version_obj = await self._make_and_verify_event(",
            "            target_hosts,",
            "            room_id,",
            "            joinee,",
            "            \"join\",",
            "            content,",
            "            params={\"ver\": KNOWN_ROOM_VERSIONS},",
            "        )",
            "",
            "        # This shouldn't happen, because the RoomMemberHandler has a",
            "        # linearizer lock which only allows one operation per user per room",
            "        # at a time - so this is just paranoia.",
            "        assert room_id not in self._federation_event_handler.room_queues",
            "",
            "        self._federation_event_handler.room_queues[room_id] = []",
            "",
            "        is_host_joined = await self.store.is_host_joined(room_id, self.server_name)",
            "",
            "        if not is_host_joined:",
            "            # We may have old forward extremities lying around if the homeserver left",
            "            # the room completely in the past. Clear them out.",
            "            #",
            "            # Note that this check-then-clear is subject to races where",
            "            #  * the homeserver is in the room and stops being in the room just after",
            "            #    the check. We won't reset the forward extremities, but that's okay,",
            "            #    since they will be almost up to date.",
            "            #  * the homeserver is not in the room and starts being in the room just",
            "            #    after the check. This can't happen, since `RoomMemberHandler` has a",
            "            #    linearizer lock which prevents concurrent remote joins into the same",
            "            #    room.",
            "            # In short, the races either have an acceptable outcome or should be",
            "            # impossible.",
            "            await self._clean_room_for_join(room_id)",
            "",
            "        try:",
            "            # Try the host we successfully got a response to /make_join/",
            "            # request first.",
            "            host_list = list(target_hosts)",
            "            try:",
            "                host_list.remove(origin)",
            "                host_list.insert(0, origin)",
            "            except ValueError:",
            "                pass",
            "",
            "            async with self._is_partial_state_room_linearizer.queue(room_id):",
            "                already_partial_state_room = await self.store.is_partial_state_room(",
            "                    room_id",
            "                )",
            "",
            "                ret = await self.federation_client.send_join(",
            "                    host_list,",
            "                    event,",
            "                    room_version_obj,",
            "                    # Perform a full join when we are already in the room and it is a",
            "                    # full state room, since we are not allowed to persist a partial",
            "                    # state join event in a full state room. In the future, we could",
            "                    # optimize this by always performing a partial state join and",
            "                    # computing the state ourselves or retrieving it from the remote",
            "                    # homeserver if necessary.",
            "                    #",
            "                    # There's a race where we leave the room, then perform a full join",
            "                    # anyway. This should end up being fast anyway, since we would",
            "                    # already have the full room state and auth chain persisted.",
            "                    partial_state=not is_host_joined or already_partial_state_room,",
            "                )",
            "",
            "                event = ret.event",
            "                origin = ret.origin",
            "                state = ret.state",
            "                auth_chain = ret.auth_chain",
            "                auth_chain.sort(key=lambda e: e.depth)",
            "",
            "                logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)",
            "                logger.debug(\"do_invite_join state: %s\", state)",
            "",
            "                logger.debug(\"do_invite_join event: %s\", event)",
            "",
            "                # if this is the first time we've joined this room, it's time to add",
            "                # a row to `rooms` with the correct room version. If there's already a",
            "                # row there, we should override it, since it may have been populated",
            "                # based on an invite request which lied about the room version.",
            "                #",
            "                # federation_client.send_join has already checked that the room",
            "                # version in the received create event is the same as room_version_obj,",
            "                # so we can rely on it now.",
            "                #",
            "                await self.store.upsert_room_on_join(",
            "                    room_id=room_id,",
            "                    room_version=room_version_obj,",
            "                    state_events=state,",
            "                )",
            "",
            "                if ret.partial_state and not already_partial_state_room:",
            "                    # Mark the room as having partial state.",
            "                    # The background process is responsible for unmarking this flag,",
            "                    # even if the join fails.",
            "                    # TODO(faster_joins):",
            "                    #     We may want to reset the partial state info if it's from an",
            "                    #     old, failed partial state join.",
            "                    #     https://github.com/matrix-org/synapse/issues/13000",
            "                    await self.store.store_partial_state_room(",
            "                        room_id=room_id,",
            "                        servers=ret.servers_in_room,",
            "                        device_lists_stream_id=self.store.get_device_stream_token(),",
            "                        joined_via=origin,",
            "                    )",
            "",
            "                try:",
            "                    max_stream_id = (",
            "                        await self._federation_event_handler.process_remote_join(",
            "                            origin,",
            "                            room_id,",
            "                            auth_chain,",
            "                            state,",
            "                            event,",
            "                            room_version_obj,",
            "                            partial_state=ret.partial_state,",
            "                        )",
            "                    )",
            "                except PartialStateConflictError:",
            "                    # This should be impossible, since we hold the lock on the room's",
            "                    # partial statedness.",
            "                    logger.error(",
            "                        \"Room %s was un-partial stated while processing remote join.\",",
            "                        room_id,",
            "                    )",
            "                    raise",
            "                else:",
            "                    # Record the join event id for future use (when we finish the full",
            "                    # join). We have to do this after persisting the event to keep",
            "                    # foreign key constraints intact.",
            "                    if ret.partial_state and not already_partial_state_room:",
            "                        # TODO(faster_joins):",
            "                        #     We may want to reset the partial state info if it's from",
            "                        #     an old, failed partial state join.",
            "                        #     https://github.com/matrix-org/synapse/issues/13000",
            "                        await self.store.write_partial_state_rooms_join_event_id(",
            "                            room_id, event.event_id",
            "                        )",
            "                finally:",
            "                    # Always kick off the background process that asynchronously fetches",
            "                    # state for the room.",
            "                    # If the join failed, the background process is responsible for",
            "                    # cleaning up \u2014 including unmarking the room as a partial state",
            "                    # room.",
            "                    if ret.partial_state:",
            "                        # Kick off the process of asynchronously fetching the state for",
            "                        # this room.",
            "                        self._start_partial_state_room_sync(",
            "                            initial_destination=origin,",
            "                            other_destinations=ret.servers_in_room,",
            "                            room_id=room_id,",
            "                        )",
            "",
            "            # We wait here until this instance has seen the events come down",
            "            # replication (if we're using replication) as the below uses caches.",
            "            await self._replication.wait_for_stream_position(",
            "                self.config.worker.events_shard_config.get_instance(room_id),",
            "                \"events\",",
            "                max_stream_id,",
            "            )",
            "",
            "            # Check whether this room is the result of an upgrade of a room we already know",
            "            # about. If so, migrate over user information",
            "            predecessor = await self.store.get_room_predecessor(room_id)",
            "            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):",
            "                return event.event_id, max_stream_id",
            "            old_room_id = predecessor[\"room_id\"]",
            "            logger.debug(",
            "                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id",
            "            )",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.transfer_room_state_on_room_upgrade(",
            "                old_room_id, room_id",
            "            )",
            "",
            "            logger.debug(\"Finished joining %s to %s\", joinee, room_id)",
            "            return event.event_id, max_stream_id",
            "        finally:",
            "            room_queue = self._federation_event_handler.room_queues[room_id]",
            "            del self._federation_event_handler.room_queues[room_id]",
            "",
            "            # we don't need to wait for the queued events to be processed -",
            "            # it's just a best-effort thing at this point. We do want to do",
            "            # them roughly in order, though, otherwise we'll end up making",
            "            # lots of requests for missing prev_events which we do actually",
            "            # have. Hence we fire off the background task, but don't wait for it.",
            "",
            "            run_as_background_process(",
            "                \"handle_queued_pdus\", self._handle_queued_pdus, room_queue",
            "            )",
            "",
            "    async def do_knock(",
            "        self,",
            "        target_hosts: List[str],",
            "        room_id: str,",
            "        knockee: str,",
            "        content: JsonDict,",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Sends the knock to the remote server.",
            "",
            "        This first triggers a make_knock request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via send_knock.",
            "",
            "        Knock events must be signed by the knockee's server before distributing.",
            "",
            "        Args:",
            "            target_hosts: A list of hosts that we want to try knocking through.",
            "            room_id: The ID of the room to knock on.",
            "            knockee: The ID of the user who is knocking.",
            "            content: The content of the knock event.",
            "",
            "        Returns:",
            "            A tuple of (event ID, stream ID).",
            "",
            "        Raises:",
            "            SynapseError: If the chosen remote server returns a 3xx/4xx code.",
            "            RuntimeError: If no servers were reachable.",
            "        \"\"\"",
            "        logger.debug(\"Knocking on room %s on behalf of user %s\", room_id, knockee)",
            "",
            "        # Inform the remote server of the room versions we support",
            "        supported_room_versions = list(KNOWN_ROOM_VERSIONS.keys())",
            "",
            "        # Ask the remote server to create a valid knock event for us. Once received,",
            "        # we sign the event",
            "        params: Dict[str, Iterable[str]] = {\"ver\": supported_room_versions}",
            "        origin, event, event_format_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, knockee, Membership.KNOCK, content, params=params",
            "        )",
            "",
            "        # Mark the knock as an outlier as we don't yet have the state at this point in",
            "        # the DAG.",
            "        event.internal_metadata.outlier = True",
            "",
            "        # ... but tell /sync to send it to clients anyway.",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Record the room ID and its version so that we have a record of the room",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=event_format_version",
            "        )",
            "",
            "        # Initially try the host that we successfully called /make_knock on",
            "        try:",
            "            target_hosts.remove(origin)",
            "            target_hosts.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        # Send the signed event back to the room, and potentially receive some",
            "        # further information about the room in the form of partial state events",
            "        knock_response = await self.federation_client.send_knock(target_hosts, event)",
            "",
            "        # Store any stripped room state events in the \"unsigned\" key of the event.",
            "        # This is a bit of a hack and is cribbing off of invites. Basically we",
            "        # store the room state here and retrieve it again when this event appears",
            "        # in the invitee's sync stream. It is stripped out for all other local users.",
            "        stripped_room_state = knock_response.get(\"knock_room_state\")",
            "",
            "        if stripped_room_state is None:",
            "            raise KeyError(\"Missing 'knock_room_state' field in send_knock response\")",
            "",
            "        event.unsigned[\"knock_room_state\"] = stripped_room_state",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "        return event.event_id, stream_id",
            "",
            "    async def _handle_queued_pdus(",
            "        self, room_queue: List[Tuple[EventBase, str]]",
            "    ) -> None:",
            "        \"\"\"Process PDUs which got queued up while we were busy send_joining.",
            "",
            "        Args:",
            "            room_queue: list of PDUs to be processed and the servers that sent them",
            "        \"\"\"",
            "        for p, origin in room_queue:",
            "            try:",
            "                logger.info(",
            "                    \"Processing queued PDU %s which was received while we were joining\",",
            "                    p,",
            "                )",
            "                with nested_logging_context(p.event_id):",
            "                    await self._federation_event_handler.on_receive_pdu(origin, p)",
            "            except Exception as e:",
            "                logger.warning(",
            "                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e",
            "                )",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_join/ request, so we create a partial",
            "        join event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create join event in",
            "            user_id: The user to create the join for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_join request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        # checking the room version will check that we've actually heard of the room",
            "        # (and return a 404 otherwise)",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /make_join, so return a 404 as we would if we weren't in the",
            "            # room at all.",
            "            # The main reason we can't respond properly is that we need to know about",
            "            # the auth events for the join event that we would return.",
            "            # We also should not bother entertaining the /make_join since we cannot",
            "            # handle the /send_join.",
            "            logger.info(",
            "                \"Rejecting /make_join to %s because it's a partial state room\", room_id",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_join right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        # now check that we are *still* in the room",
            "        is_in_room = await self._event_auth_handler.is_host_in_room(",
            "            room_id, self.server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Got /make_join request for room %s we are no longer in\",",
            "                room_id,",
            "            )",
            "            raise NotFoundError(\"Not an active room on this server\")",
            "",
            "        event_content = {\"membership\": Membership.JOIN}",
            "",
            "        # If the current room is using restricted join rules, additional information",
            "        # may need to be included in the event content in order to efficiently",
            "        # validate the event.",
            "        #",
            "        # Note that this requires the /send_join request to come back to the",
            "        # same server.",
            "        prev_event_ids = None",
            "        if room_version.restricted_join_rule:",
            "            # Note that the room's state can change out from under us and render our",
            "            # nice join rules-conformant event non-conformant by the time we build the",
            "            # event. When this happens, our validation at the end fails and we respond",
            "            # to the requesting server with a 403, which is misleading \u2014 it indicates",
            "            # that the user is not allowed to join the room and the joining server",
            "            # should not bother retrying via this homeserver or any others, when",
            "            # in fact we've just messed up with building the event.",
            "            #",
            "            # To reduce the likelihood of this race, we capture the forward extremities",
            "            # of the room (prev_event_ids) just before fetching the current state, and",
            "            # hope that the state we fetch corresponds to the prev events we chose.",
            "            prev_event_ids = await self.store.get_prev_events_for_room(room_id)",
            "            state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                room_id",
            "            )",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                prev_member_event_id = state_ids.get((EventTypes.Member, user_id), None)",
            "                # If the user is invited or joined to the room already, then",
            "                # no additional info is needed.",
            "                include_auth_user_id = True",
            "                if prev_member_event_id:",
            "                    prev_member_event = await self.store.get_event(prev_member_event_id)",
            "                    include_auth_user_id = prev_member_event.membership not in (",
            "                        Membership.JOIN,",
            "                        Membership.INVITE,",
            "                    )",
            "",
            "                if include_auth_user_id:",
            "                    event_content[",
            "                        EventContentFields.AUTHORISING_USER",
            "                    ] = await self._event_auth_handler.get_user_which_could_invite(",
            "                        room_id,",
            "                        state_ids,",
            "                    )",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": event_content,",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        try:",
            "            (",
            "                event,",
            "                unpersisted_context,",
            "            ) = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder,",
            "                prev_event_ids=prev_event_ids,",
            "            )",
            "        except SynapseError as e:",
            "            logger.warning(\"Failed to create join to %s because %s\", room_id, e)",
            "            raise",
            "",
            "        # Ensure the user can even join the room.",
            "        await self._federation_event_handler.check_join_restrictions(",
            "            unpersisted_context, event",
            "        )",
            "",
            "        # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "        # when we get the event back in `on_send_join_request`",
            "        await self._event_auth_handler.check_auth_rules_from_context(event)",
            "        return event",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, event: EventBase, room_version: RoomVersion",
            "    ) -> EventBase:",
            "        \"\"\"We've got an invite event. Process and persist it. Sign it.",
            "",
            "        Respond with the now signed event.",
            "        \"\"\"",
            "        if event.state_key is None:",
            "            raise SynapseError(400, \"The invite event did not have a state key\")",
            "",
            "        is_blocked = await self.store.is_room_blocked(event.room_id)",
            "        if is_blocked:",
            "            raise SynapseError(403, \"This room has been blocked on this server\")",
            "",
            "        if self.hs.config.server.block_non_admin_invites:",
            "            raise SynapseError(403, \"This server does not accept room invites\")",
            "",
            "        spam_check = await self._spam_checker_module_callbacks.user_may_invite(",
            "            event.sender, event.state_key, event.room_id",
            "        )",
            "        if spam_check != NOT_SPAM:",
            "            raise SynapseError(",
            "                403,",
            "                \"This user is not permitted to send invites to this server/user\",",
            "                errcode=spam_check[0],",
            "                additional_fields=spam_check[1],",
            "            )",
            "",
            "        membership = event.content.get(\"membership\")",
            "        if event.type != EventTypes.Member or membership != Membership.INVITE:",
            "            raise SynapseError(400, \"The event was not an m.room.member invite event\")",
            "",
            "        sender_domain = get_domain_from_id(event.sender)",
            "        if sender_domain != origin:",
            "            raise SynapseError(",
            "                400, \"The invite event was not from the server sending it\"",
            "            )",
            "",
            "        if not self.is_mine_id(event.state_key):",
            "            raise SynapseError(400, \"The invite event must be for this server\")",
            "",
            "        # block any attempts to invite the server notices mxid",
            "        if event.state_key == self._server_notices_mxid:",
            "            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        # We don't rate limit based on room ID, as that should be done by",
            "        # sending server.",
            "        await member_handler.ratelimit_invite(None, None, event.state_key)",
            "",
            "        # keep a record of the room version, if we don't yet know it.",
            "        # (this may get overwritten if we later get a different room version in a",
            "        # join dance).",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=room_version",
            "        )",
            "",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        event.signatures.update(",
            "            compute_event_signature(",
            "                room_version,",
            "                event.get_pdu_json(),",
            "                self.hs.hostname,",
            "                self.hs.signing_key,",
            "            )",
            "        )",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "",
            "        await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "            [(event, context)]",
            "        )",
            "        try:",
            "            await self._federation_event_handler.persist_events_and_notify(",
            "                event.room_id, [(event, context)]",
            "            )",
            "        except Exception:",
            "            await self.store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "        return event",
            "",
            "    async def do_remotely_reject_invite(",
            "        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict",
            "    ) -> Tuple[EventBase, int]:",
            "        origin, event, room_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, user_id, \"leave\", content=content",
            "        )",
            "        # Mark as outlier as we don't have any state for this event; we're not",
            "        # even in the room.",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Try the host that we successfully called /make_leave/ on first for",
            "        # the /send_leave/ request.",
            "        host_list = list(target_hosts)",
            "        try:",
            "            host_list.remove(origin)",
            "            host_list.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        await self.federation_client.send_leave(host_list, event)",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event, stream_id",
            "",
            "    async def _make_and_verify_event(",
            "        self,",
            "        target_hosts: Iterable[str],",
            "        room_id: str,",
            "        user_id: str,",
            "        membership: str,",
            "        content: JsonDict,",
            "        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,",
            "    ) -> Tuple[str, EventBase, RoomVersion]:",
            "        (",
            "            origin,",
            "            event,",
            "            room_version,",
            "        ) = await self.federation_client.make_membership_event(",
            "            target_hosts, room_id, user_id, membership, content, params=params",
            "        )",
            "",
            "        logger.debug(\"Got response to make_%s: %s\", membership, event)",
            "",
            "        # We should assert some things.",
            "        # FIXME: Do this in a nicer way",
            "        assert event.type == EventTypes.Member",
            "        assert event.user_id == user_id",
            "        assert event.state_key == user_id",
            "        assert event.room_id == room_id",
            "        return origin, event, room_version",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_leave/ request, so we create a partial",
            "        leave event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create leave event in",
            "            user_id: The user to create the leave for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_leave request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.LEAVE},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, _ = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_leave_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new leave %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a make_knock request, so we create a partial",
            "        knock event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Get /make_knock request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.KNOCK},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        (",
            "            event,",
            "            unpersisted_context,",
            "        ) = await self.event_creation_handler.create_new_client_event(builder=builder)",
            "",
            "        event_allowed, _ = await self._third_party_event_rules.check_event_allowed(",
            "            event, unpersisted_context",
            "        )",
            "        if not event_allowed:",
            "            logger.warning(\"Creation of knock %s forbidden by third-party rules\", event)",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_knock_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new knock %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.\"\"\"",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "        if event.internal_metadata.outlier:",
            "            raise NotFoundError(\"State not known at event %s\" % (event_id,))",
            "",
            "        state_groups = await self._state_storage_controller.get_state_groups_ids(",
            "            room_id, [event_id]",
            "        )",
            "",
            "        # get_state_groups_ids should return exactly one result",
            "        assert len(state_groups) == 1",
            "",
            "        state_map = next(iter(state_groups.values()))",
            "",
            "        state_key = event.get_state_key()",
            "        if state_key is not None:",
            "            # the event was not rejected (get_event raises a NotFoundError for rejected",
            "            # events) so the state at the event should include the event itself.",
            "            assert (",
            "                state_map.get((event.type, state_key)) == event.event_id",
            "            ), \"State at event did not include event itself\"",
            "",
            "            # ... but we need the state *before* that event",
            "            if \"replaces_state\" in event.unsigned:",
            "                prev_id = event.unsigned[\"replaces_state\"]",
            "                state_map[(event.type, state_key)] = prev_id",
            "            else:",
            "                del state_map[(event.type, state_key)]",
            "",
            "        return list(state_map.values())",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, pdu_list: List[str], limit: int",
            "    ) -> List[EventBase]:",
            "        # We allow partially joined rooms since in this case we are filtering out",
            "        # non-local events in `filter_events_for_server`.",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin, True)",
            "",
            "        # Synapse asks for 100 events per backfill request. Do not allow more.",
            "        limit = min(limit, 100)",
            "",
            "        events = await self.store.get_backfill_events(room_id, pdu_list, limit)",
            "        logger.debug(",
            "            \"on_backfill_request: backfill events=%s\",",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        events = await filter_events_for_server(",
            "            self._storage_controllers,",
            "            origin,",
            "            self.server_name,",
            "            events,",
            "            redact=True,",
            "            filter_out_erased_senders=True,",
            "            filter_out_remote_partial_state_events=True,",
            "        )",
            "",
            "        return events",
            "",
            "    async def get_persisted_pdu(",
            "        self, origin: str, event_id: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get an event from the database for the given server.",
            "",
            "        Args:",
            "            origin: hostname of server which is requesting the event; we",
            "               will check that the server is allowed to see it.",
            "            event_id: id of the event being requested",
            "",
            "        Returns:",
            "            None if we know nothing about the event; otherwise the (possibly-redacted) event.",
            "",
            "        Raises:",
            "            AuthError if the server is not currently in the room",
            "        \"\"\"",
            "        event = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        if not event:",
            "            return None",
            "",
            "        await self._event_auth_handler.assert_host_in_room(event.room_id, origin)",
            "",
            "        events = await filter_events_for_server(",
            "            self._storage_controllers,",
            "            origin,",
            "            self.server_name,",
            "            [event],",
            "            redact=True,",
            "            filter_out_erased_senders=True,",
            "            filter_out_remote_partial_state_events=True,",
            "        )",
            "        event = events[0]",
            "        return event",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> List[EventBase]:",
            "        # We allow partially joined rooms since in this case we are filtering out",
            "        # non-local events in `filter_events_for_server`.",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin, True)",
            "",
            "        # Only allow up to 20 events to be retrieved per request.",
            "        limit = min(limit, 20)",
            "",
            "        missing_events = await self.store.get_missing_events(",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        missing_events = await filter_events_for_server(",
            "            self._storage_controllers,",
            "            origin,",
            "            self.server_name,",
            "            missing_events,",
            "            redact=True,",
            "            filter_out_erased_senders=True,",
            "            filter_out_remote_partial_state_events=True,",
            "        )",
            "",
            "        return missing_events",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: JsonDict",
            "    ) -> None:",
            "        third_party_invite = {\"signed\": signed}",
            "",
            "        event_dict = {",
            "            \"type\": EventTypes.Member,",
            "            \"content\": {",
            "                \"membership\": Membership.INVITE,",
            "                \"third_party_invite\": third_party_invite,",
            "            },",
            "            \"room_id\": room_id,",
            "            \"sender\": sender_user_id,",
            "            \"state_key\": target_user_id,",
            "        }",
            "",
            "        if await self._event_auth_handler.is_host_in_room(room_id, self.hs.hostname):",
            "            room_version_obj = await self.store.get_room_version(room_id)",
            "            builder = self.event_builder_factory.for_room_version(",
            "                room_version_obj, event_dict",
            "            )",
            "",
            "            EventValidator().validate_builder(builder)",
            "",
            "            # Try several times, it could fail with PartialStateConflictError",
            "            # in send_membership_event, cf comment in except block.",
            "            max_retries = 5",
            "            for i in range(max_retries):",
            "                try:",
            "                    (",
            "                        event,",
            "                        unpersisted_context,",
            "                    ) = await self.event_creation_handler.create_new_client_event(",
            "                        builder=builder",
            "                    )",
            "",
            "                    (",
            "                        event,",
            "                        unpersisted_context,",
            "                    ) = await self.add_display_name_to_third_party_invite(",
            "                        room_version_obj, event_dict, event, unpersisted_context",
            "                    )",
            "",
            "                    context = await unpersisted_context.persist(event)",
            "",
            "                    EventValidator().validate_new(event, self.config)",
            "",
            "                    # We need to tell the transaction queue to send this out, even",
            "                    # though the sender isn't a local user.",
            "                    event.internal_metadata.send_on_behalf_of = self.hs.hostname",
            "",
            "                    try:",
            "                        validate_event_for_room_version(event)",
            "                        await self._event_auth_handler.check_auth_rules_from_context(",
            "                            event",
            "                        )",
            "                    except AuthError as e:",
            "                        logger.warning(",
            "                            \"Denying new third party invite %r because %s\", event, e",
            "                        )",
            "                        raise e",
            "",
            "                    await self._check_signature(event, context)",
            "",
            "                    # We retrieve the room member handler here as to not cause a cyclic dependency",
            "                    member_handler = self.hs.get_room_member_handler()",
            "                    await member_handler.send_membership_event(None, event, context)",
            "",
            "                    break",
            "                except PartialStateConflictError as e:",
            "                    # Persisting couldn't happen because the room got un-partial stated",
            "                    # in the meantime and context needs to be recomputed, so let's do so.",
            "                    if i == max_retries - 1:",
            "                        raise e",
            "        else:",
            "            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}",
            "",
            "            try:",
            "                await self.federation_client.forward_third_party_invite(",
            "                    destinations, room_id, event_dict",
            "                )",
            "            except (RequestSendFailed, HttpResponseException):",
            "                raise SynapseError(502, \"Failed to forward third party invite\")",
            "",
            "    async def on_exchange_third_party_invite_request(",
            "        self, event_dict: JsonDict",
            "    ) -> None:",
            "        \"\"\"Handle an exchange_third_party_invite request from a remote server",
            "",
            "        The remote server will call this when it wants to turn a 3pid invite",
            "        into a normal m.room.member invite.",
            "",
            "        Args:",
            "            event_dict: Dictionary containing the event body.",
            "",
            "        \"\"\"",
            "        assert_params_in_dict(event_dict, [\"room_id\"])",
            "        room_version_obj = await self.store.get_room_version(event_dict[\"room_id\"])",
            "",
            "        # NB: event_dict has a particular specced format we might need to fudge",
            "        # if we change event formats too much.",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        # Try several times, it could fail with PartialStateConflictError",
            "        # in send_membership_event, cf comment in except block.",
            "        max_retries = 5",
            "        for i in range(max_retries):",
            "            try:",
            "                (",
            "                    event,",
            "                    unpersisted_context,",
            "                ) = await self.event_creation_handler.create_new_client_event(",
            "                    builder=builder",
            "                )",
            "                (",
            "                    event,",
            "                    unpersisted_context,",
            "                ) = await self.add_display_name_to_third_party_invite(",
            "                    room_version_obj, event_dict, event, unpersisted_context",
            "                )",
            "",
            "                context = await unpersisted_context.persist(event)",
            "",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    await self._event_auth_handler.check_auth_rules_from_context(event)",
            "                except AuthError as e:",
            "                    logger.warning(\"Denying third party invite %r because %s\", event, e)",
            "                    raise e",
            "                await self._check_signature(event, context)",
            "",
            "                # We need to tell the transaction queue to send this out, even",
            "                # though the sender isn't a local user.",
            "                event.internal_metadata.send_on_behalf_of = get_domain_from_id(",
            "                    event.sender",
            "                )",
            "",
            "                # We retrieve the room member handler here as to not cause a cyclic dependency",
            "                member_handler = self.hs.get_room_member_handler()",
            "                await member_handler.send_membership_event(None, event, context)",
            "",
            "                break",
            "            except PartialStateConflictError as e:",
            "                # Persisting couldn't happen because the room got un-partial stated",
            "                # in the meantime and context needs to be recomputed, so let's do so.",
            "                if i == max_retries - 1:",
            "                    raise e",
            "",
            "    async def add_display_name_to_third_party_invite(",
            "        self,",
            "        room_version_obj: RoomVersion,",
            "        event_dict: JsonDict,",
            "        event: EventBase,",
            "        context: UnpersistedEventContextBase,",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        key = (",
            "            EventTypes.ThirdPartyInvite,",
            "            event.content[\"third_party_invite\"][\"signed\"][\"token\"],",
            "        )",
            "        original_invite = None",
            "        prev_state_ids = await context.get_prev_state_ids(StateFilter.from_types([key]))",
            "        original_invite_id = prev_state_ids.get(key)",
            "        if original_invite_id:",
            "            original_invite = await self.store.get_event(",
            "                original_invite_id, allow_none=True",
            "            )",
            "        if original_invite:",
            "            # If the m.room.third_party_invite event's content is empty, it means the",
            "            # invite has been revoked. In this case, we don't have to raise an error here",
            "            # because the auth check will fail on the invite (because it's not able to",
            "            # fetch public keys from the m.room.third_party_invite event's content, which",
            "            # is empty).",
            "            display_name = original_invite.content.get(\"display_name\")",
            "            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name",
            "        else:",
            "            logger.info(",
            "                \"Could not find invite event for third_party_invite: %r\", event_dict",
            "            )",
            "            # We don't discard here as this is not the appropriate place to do",
            "            # auth checks. If we need the invite and don't have it then the",
            "            # auth check code will explode appropriately.",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "        EventValidator().validate_builder(builder)",
            "",
            "        (",
            "            event,",
            "            unpersisted_context,",
            "        ) = await self.event_creation_handler.create_new_client_event(builder=builder)",
            "",
            "        EventValidator().validate_new(event, self.config)",
            "        return event, unpersisted_context",
            "",
            "    async def _check_signature(self, event: EventBase, context: EventContext) -> None:",
            "        \"\"\"",
            "        Checks that the signature in the event is consistent with its invite.",
            "",
            "        Args:",
            "            event: The m.room.member event to check",
            "            context:",
            "",
            "        Raises:",
            "            AuthError: if signature didn't match any keys, or key has been",
            "                revoked,",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        signed = event.content[\"third_party_invite\"][\"signed\"]",
            "        token = signed[\"token\"]",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(EventTypes.ThirdPartyInvite, token)])",
            "        )",
            "        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))",
            "",
            "        invite_event = None",
            "        if invite_event_id:",
            "            invite_event = await self.store.get_event(invite_event_id, allow_none=True)",
            "",
            "        if not invite_event:",
            "            raise AuthError(403, \"Could not find invite\")",
            "",
            "        logger.debug(\"Checking auth on event %r\", event.content)",
            "",
            "        last_exception: Optional[Exception] = None",
            "",
            "        # for each public key in the 3pid invite event",
            "        for public_key_object in event_auth.get_public_keys(invite_event):",
            "            try:",
            "                # for each sig on the third_party_invite block of the actual invite",
            "                for server, signature_block in signed[\"signatures\"].items():",
            "                    for key_name in signature_block.keys():",
            "                        if not key_name.startswith(\"ed25519:\"):",
            "                            continue",
            "",
            "                        logger.debug(",
            "                            \"Attempting to verify sig with key %s from %r \"",
            "                            \"against pubkey %r\",",
            "                            key_name,",
            "                            server,",
            "                            public_key_object,",
            "                        )",
            "",
            "                        try:",
            "                            public_key = public_key_object[\"public_key\"]",
            "                            verify_key = decode_verify_key_bytes(",
            "                                key_name, decode_base64(public_key)",
            "                            )",
            "                            verify_signed_json(signed, server, verify_key)",
            "                            logger.debug(",
            "                                \"Successfully verified sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to verify sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                            raise",
            "                        try:",
            "                            if \"key_validity_url\" in public_key_object:",
            "                                await self._check_key_revocation(",
            "                                    public_key, public_key_object[\"key_validity_url\"]",
            "                                )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to query key_validity_url %s\",",
            "                                public_key_object[\"key_validity_url\"],",
            "                            )",
            "                            raise",
            "                        return",
            "            except Exception as e:",
            "                last_exception = e",
            "",
            "        if last_exception is None:",
            "            # we can only get here if get_public_keys() returned an empty list",
            "            # TODO: make this better",
            "            raise RuntimeError(\"no public key in invite event\")",
            "",
            "        raise last_exception",
            "",
            "    async def _check_key_revocation(self, public_key: str, url: str) -> None:",
            "        \"\"\"",
            "        Checks whether public_key has been revoked.",
            "",
            "        Args:",
            "            public_key: base-64 encoded public key.",
            "            url: Key revocation URL.",
            "",
            "        Raises:",
            "            AuthError: if they key has been revoked.",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        try:",
            "            response = await self.http_client.get_json(url, {\"public_key\": public_key})",
            "        except Exception:",
            "            raise SynapseError(502, \"Third party certificate could not be checked\")",
            "        if \"valid\" not in response or not response[\"valid\"]:",
            "            raise AuthError(403, \"Third party certificate was invalid\")",
            "",
            "    async def _clean_room_for_join(self, room_id: str) -> None:",
            "        \"\"\"Called to clean up any data in DB for a given room, ready for the",
            "        server to join the room.",
            "",
            "        Args:",
            "            room_id",
            "        \"\"\"",
            "        if self.config.worker.worker_app:",
            "            await self._clean_room_for_join_client(room_id)",
            "        else:",
            "            await self.store.clean_room_for_join(room_id)",
            "",
            "    async def get_room_complexity(",
            "        self, remote_room_hosts: List[str], room_id: str",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Fetch the complexity of a remote room over federation.",
            "",
            "        Args:",
            "            remote_room_hosts: The remote servers to ask.",
            "            room_id: The room ID to ask about.",
            "",
            "        Returns:",
            "            Dict contains the complexity",
            "            metric versions, while None means we could not fetch the complexity.",
            "        \"\"\"",
            "",
            "        for host in remote_room_hosts:",
            "            res = await self.federation_client.get_room_complexity(host, room_id)",
            "",
            "            # We got a result, return it.",
            "            if res:",
            "                return res",
            "",
            "        # We fell off the bottom, couldn't get the complexity from anyone. Oh",
            "        # well.",
            "        return None",
            "",
            "    async def _resume_partial_state_room_sync(self) -> None:",
            "        \"\"\"Resumes resyncing of all partial-state rooms after a restart.\"\"\"",
            "        assert not self.config.worker.worker_app",
            "",
            "        partial_state_rooms = await self.store.get_partial_state_room_resync_info()",
            "        for room_id, resync_info in partial_state_rooms.items():",
            "            self._start_partial_state_room_sync(",
            "                initial_destination=resync_info.joined_via,",
            "                other_destinations=resync_info.servers_in_room,",
            "                room_id=room_id,",
            "            )",
            "",
            "    def _start_partial_state_room_sync(",
            "        self,",
            "        initial_destination: Optional[str],",
            "        other_destinations: AbstractSet[str],",
            "        room_id: str,",
            "    ) -> None:",
            "        \"\"\"Starts the background process to resync the state of a partial state room,",
            "        if it is not already running.",
            "",
            "        Args:",
            "            initial_destination: the initial homeserver to pull the state from",
            "            other_destinations: other homeservers to try to pull the state from, if",
            "                `initial_destination` is unavailable",
            "            room_id: room to be resynced",
            "        \"\"\"",
            "",
            "        async def _sync_partial_state_room_wrapper() -> None:",
            "            if room_id in self._active_partial_state_syncs:",
            "                # Another local user has joined the room while there is already a",
            "                # partial state sync running. This implies that there is a new join",
            "                # event to un-partial state. We might find ourselves in one of a few",
            "                # scenarios:",
            "                #  1. There is an existing partial state sync. The partial state sync",
            "                #     un-partial states the new join event before completing and all is",
            "                #     well.",
            "                #  2. Before the latest join, the homeserver was no longer in the room",
            "                #     and there is an existing partial state sync from our previous",
            "                #     membership of the room. The partial state sync may have:",
            "                #      a) succeeded, but not yet terminated. The room will not be",
            "                #         un-partial stated again unless we restart the partial state",
            "                #         sync.",
            "                #      b) failed, because we were no longer in the room and remote",
            "                #         homeservers were refusing our requests, but not yet",
            "                #         terminated. After the latest join, remote homeservers may",
            "                #         start answering our requests again, so we should restart the",
            "                #         partial state sync.",
            "                # In the cases where we would want to restart the partial state sync,",
            "                # the room would have the partial state flag when the partial state sync",
            "                # terminates.",
            "                self._partial_state_syncs_maybe_needing_restart[room_id] = (",
            "                    initial_destination,",
            "                    other_destinations,",
            "                )",
            "                return",
            "",
            "            self._active_partial_state_syncs.add(room_id)",
            "",
            "            try:",
            "                await self._sync_partial_state_room(",
            "                    initial_destination=initial_destination,",
            "                    other_destinations=other_destinations,",
            "                    room_id=room_id,",
            "                )",
            "            finally:",
            "                # Read the room's partial state flag while we still hold the claim to",
            "                # being the active partial state sync (so that another partial state",
            "                # sync can't come along and mess with it under us).",
            "                # Normally, the partial state flag will be gone. If it isn't, then we",
            "                # may find ourselves in scenario 2a or 2b as described in the comment",
            "                # above, where we want to restart the partial state sync.",
            "                is_still_partial_state_room = await self.store.is_partial_state_room(",
            "                    room_id",
            "                )",
            "                self._active_partial_state_syncs.remove(room_id)",
            "",
            "                if room_id in self._partial_state_syncs_maybe_needing_restart:",
            "                    (",
            "                        restart_initial_destination,",
            "                        restart_other_destinations,",
            "                    ) = self._partial_state_syncs_maybe_needing_restart.pop(room_id)",
            "",
            "                    if is_still_partial_state_room:",
            "                        self._start_partial_state_room_sync(",
            "                            initial_destination=restart_initial_destination,",
            "                            other_destinations=restart_other_destinations,",
            "                            room_id=room_id,",
            "                        )",
            "",
            "        run_as_background_process(",
            "            desc=\"sync_partial_state_room\", func=_sync_partial_state_room_wrapper",
            "        )",
            "",
            "    async def _sync_partial_state_room(",
            "        self,",
            "        initial_destination: Optional[str],",
            "        other_destinations: AbstractSet[str],",
            "        room_id: str,",
            "    ) -> None:",
            "        \"\"\"Background process to resync the state of a partial-state room",
            "",
            "        Args:",
            "            initial_destination: the initial homeserver to pull the state from",
            "            other_destinations: other homeservers to try to pull the state from, if",
            "                `initial_destination` is unavailable",
            "            room_id: room to be resynced",
            "        \"\"\"",
            "        # Assume that we run on the main process for now.",
            "        # TODO(faster_joins,multiple workers)",
            "        # When moving the sync to workers, we need to ensure that",
            "        #  * `_start_partial_state_room_sync` still prevents duplicate resyncs",
            "        #  * `_is_partial_state_room_linearizer` correctly guards partial state flags",
            "        #    for rooms between the workers doing remote joins and resync.",
            "        assert not self.config.worker.worker_app",
            "",
            "        # TODO(faster_joins): do we need to lock to avoid races? What happens if other",
            "        #   worker processes kick off a resync in parallel? Perhaps we should just elect",
            "        #   a single worker to do the resync.",
            "        #   https://github.com/matrix-org/synapse/issues/12994",
            "        #",
            "        # TODO(faster_joins): what happens if we leave the room during a resync? if we",
            "        #   really leave, that might mean we have difficulty getting the room state over",
            "        #   federation.",
            "        #   https://github.com/matrix-org/synapse/issues/12802",
            "",
            "        # Make an infinite iterator of destinations to try. Once we find a working",
            "        # destination, we'll stick with it until it flakes.",
            "        destinations = _prioritise_destinations_for_partial_state_resync(",
            "            initial_destination, other_destinations, room_id",
            "        )",
            "        destination_iter = itertools.cycle(destinations)",
            "",
            "        # `destination` is the current remote homeserver we're pulling from.",
            "        destination = next(destination_iter)",
            "        logger.info(\"Syncing state for room %s via %s\", room_id, destination)",
            "",
            "        # we work through the queue in order of increasing stream ordering.",
            "        while True:",
            "            batch = await self.store.get_partial_state_events_batch(room_id)",
            "            if not batch:",
            "                # all the events are updated, so we can update current state and",
            "                # clear the lazy-loading flag.",
            "                logger.info(\"Updating current state for %s\", room_id)",
            "                # TODO(faster_joins): notify workers in notify_room_un_partial_stated",
            "                #   https://github.com/matrix-org/synapse/issues/12994",
            "                #",
            "                # NB: there's a potential race here. If room is purged just before we",
            "                # call this, we _might_ end up inserting rows into current_state_events.",
            "                # (The logic is hard to chase through.) We think this is fine, but if",
            "                # not the HS admin should purge the room again.",
            "                await self.state_handler.update_current_state(room_id)",
            "",
            "                logger.info(\"Handling any pending device list updates\")",
            "                await self._device_handler.handle_room_un_partial_stated(room_id)",
            "",
            "                async with self._is_partial_state_room_linearizer.queue(room_id):",
            "                    logger.info(\"Clearing partial-state flag for %s\", room_id)",
            "                    new_stream_id = await self.store.clear_partial_state_room(room_id)",
            "",
            "                if new_stream_id is not None:",
            "                    logger.info(\"State resync complete for %s\", room_id)",
            "                    self._storage_controllers.state.notify_room_un_partial_stated(",
            "                        room_id",
            "                    )",
            "",
            "                    await self._notifier.on_un_partial_stated_room(",
            "                        room_id, new_stream_id",
            "                    )",
            "                    return",
            "",
            "                # we raced against more events arriving with partial state. Go round",
            "                # the loop again. We've already logged a warning, so no need for more.",
            "                continue",
            "",
            "            events = await self.store.get_events_as_list(",
            "                batch,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                allow_rejected=True,",
            "            )",
            "            for event in events:",
            "                for attempt in itertools.count():",
            "                    # We try a new destination on every iteration.",
            "                    try:",
            "                        while True:",
            "                            try:",
            "                                await self._federation_event_handler.update_state_for_partial_state_event(",
            "                                    destination, event",
            "                                )",
            "                                break",
            "                            except FederationPullAttemptBackoffError as e:",
            "                                # We are in the backoff period for one of the event's",
            "                                # prev_events. Wait it out and try again after.",
            "                                logger.warning(",
            "                                    \"%s; waiting for %d ms...\", e, e.retry_after_ms",
            "                                )",
            "                                await self.clock.sleep(e.retry_after_ms / 1000)",
            "",
            "                        # Success, no need to try the rest of the destinations.",
            "                        break",
            "                    except FederationError as e:",
            "                        if attempt == len(destinations) - 1:",
            "                            # We have tried every remote server for this event. Give up.",
            "                            # TODO(faster_joins) giving up isn't the right thing to do",
            "                            #   if there's a temporary network outage. retrying",
            "                            #   indefinitely is also not the right thing to do if we can",
            "                            #   reach all homeservers and they all claim they don't have",
            "                            #   the state we want.",
            "                            #   https://github.com/matrix-org/synapse/issues/13000",
            "                            logger.error(",
            "                                \"Failed to get state for %s at %s from %s because %s, \"",
            "                                \"giving up!\",",
            "                                room_id,",
            "                                event,",
            "                                destination,",
            "                                e,",
            "                            )",
            "                            # TODO: We should `record_event_failed_pull_attempt` here,",
            "                            #   see https://github.com/matrix-org/synapse/issues/13700",
            "                            raise",
            "",
            "                        # Try the next remote server.",
            "                        logger.info(",
            "                            \"Failed to get state for %s at %s from %s because %s\",",
            "                            room_id,",
            "                            event,",
            "                            destination,",
            "                            e,",
            "                        )",
            "                        destination = next(destination_iter)",
            "                        logger.info(",
            "                            \"Syncing state for room %s via %s instead\",",
            "                            room_id,",
            "                            destination,",
            "                        )",
            "",
            "",
            "def _prioritise_destinations_for_partial_state_resync(",
            "    initial_destination: Optional[str],",
            "    other_destinations: AbstractSet[str],",
            "    room_id: str,",
            ") -> StrCollection:",
            "    \"\"\"Work out the order in which we should ask servers to resync events.",
            "",
            "    If an `initial_destination` is given, it takes top priority. Otherwise",
            "    all servers are treated equally.",
            "",
            "    :raises ValueError: if no destination is provided at all.",
            "    \"\"\"",
            "    if initial_destination is None and len(other_destinations) == 0:",
            "        raise ValueError(f\"Cannot resync state of {room_id}: no destinations provided\")",
            "",
            "    if initial_destination is None:",
            "        return other_destinations",
            "",
            "    # Move `initial_destination` to the front of the list.",
            "    destinations = list(other_destinations)",
            "    if initial_destination in destinations:",
            "        destinations.remove(initial_destination)",
            "    destinations = [initial_destination] + destinations",
            "    return destinations"
        ],
        "afterPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright 2020 Sorunome",
            "# Copyright 2014-2022 The Matrix.org Foundation C.I.C.",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "",
            "\"\"\"Contains handlers for federation events.\"\"\"",
            "",
            "import enum",
            "import itertools",
            "import logging",
            "from enum import Enum",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    AbstractSet,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "import attr",
            "from prometheus_client import Histogram",
            "from signedjson.key import decode_verify_key_bytes",
            "from signedjson.sign import verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import MAX_DEPTH, EventContentFields, EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    CodeMessageException,",
            "    Codes,",
            "    FederationDeniedError,",
            "    FederationError,",
            "    FederationPullAttemptBackoffError,",
            "    HttpResponseException,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext, UnpersistedEventContextBase",
            "from synapse.events.validator import EventValidator",
            "from synapse.federation.federation_client import InvalidResponseError",
            "from synapse.handlers.pagination import PURGE_PAGINATION_LOCK_NAME",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import SynapseTags, set_tag, tag_args, trace",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.module_api import NOT_SPAM",
            "from synapse.replication.http.federation import (",
            "    ReplicationCleanRoomRestServlet,",
            "    ReplicationStoreRoomOnOutlierMembershipRestServlet,",
            ")",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import JsonDict, StrCollection, get_domain_from_id",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.visibility import filter_events_for_server",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_before_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_before_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        30.0,",
            "        40.0,",
            "        60.0,",
            "        80.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "# TODO: We can refactor this away now that there is only one backfill point again",
            "class _BackfillPointType(Enum):",
            "    # a regular backwards extremity (ie, an event which we don't yet have, but which",
            "    # is referred to by other events in the DAG)",
            "    BACKWARDS_EXTREMITY = enum.auto()",
            "",
            "",
            "@attr.s(slots=True, auto_attribs=True, frozen=True)",
            "class _BackfillPoint:",
            "    \"\"\"A potential point we might backfill from\"\"\"",
            "",
            "    event_id: str",
            "    depth: int",
            "    type: _BackfillPointType",
            "",
            "",
            "class FederationHandler:",
            "    \"\"\"Handles general incoming federation requests",
            "",
            "    Incoming events are *not* handled here, for which see FederationEventHandler.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "",
            "        self.clock = hs.get_clock()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self.federation_client = hs.get_federation_client()",
            "        self.state_handler = hs.get_state_handler()",
            "        self.server_name = hs.hostname",
            "        self.keyring = hs.get_keyring()",
            "        self.is_mine_id = hs.is_mine_id",
            "        self.is_mine_server_name = hs.is_mine_server_name",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self.event_creation_handler = hs.get_event_creation_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._server_notices_mxid = hs.config.servernotices.server_notices_mxid",
            "        self.config = hs.config",
            "        self.http_client = hs.get_proxied_blocklisted_http_client()",
            "        self._replication = hs.get_replication_data_handler()",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self._device_handler = hs.get_device_handler()",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "        self._notifier = hs.get_notifier()",
            "        self._worker_locks = hs.get_worker_locks_handler()",
            "",
            "        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(",
            "            hs",
            "        )",
            "",
            "        if hs.config.worker.worker_app:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                self.store.maybe_store_room_on_outlier_membership",
            "            )",
            "",
            "        self._room_backfill = Linearizer(\"room_backfill\")",
            "",
            "        self._third_party_event_rules = (",
            "            hs.get_module_api_callbacks().third_party_event_rules",
            "        )",
            "",
            "        # Tracks running partial state syncs by room ID.",
            "        # Partial state syncs currently only run on the main process, so it's okay to",
            "        # track them in-memory for now.",
            "        self._active_partial_state_syncs: Set[str] = set()",
            "        # Tracks partial state syncs we may want to restart.",
            "        # A dictionary mapping room IDs to (initial destination, other destinations)",
            "        # tuples.",
            "        self._partial_state_syncs_maybe_needing_restart: Dict[",
            "            str, Tuple[Optional[str], AbstractSet[str]]",
            "        ] = {}",
            "        # A lock guarding the partial state flag for rooms.",
            "        # When the lock is held for a given room, no other concurrent code may",
            "        # partial state or un-partial state the room.",
            "        self._is_partial_state_room_linearizer = Linearizer(",
            "            name=\"_is_partial_state_room_linearizer\"",
            "        )",
            "",
            "        # if this is the main process, fire off a background process to resume",
            "        # any partial-state-resync operations which were in flight when we",
            "        # were shut down.",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"resume_sync_partial_state_room\", self._resume_partial_state_room_sync",
            "            )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def maybe_backfill(",
            "        self, room_id: str, current_depth: int, limit: int, record_time: bool = True",
            "    ) -> bool:",
            "        \"\"\"Checks the database to see if we should backfill before paginating,",
            "        and if so do.",
            "",
            "        Args:",
            "            room_id",
            "            current_depth: The depth from which we're paginating from. This is",
            "                used to decide if we should backfill and what extremities to",
            "                use.",
            "            limit: The number of events that the pagination request will",
            "                return. This is used as part of the heuristic to decide if we",
            "                should back paginate.",
            "            record_time: Whether to record the time it takes to backfill.",
            "",
            "        Returns:",
            "            True if we actually tried to backfill something, otherwise False.",
            "        \"\"\"",
            "        # Starting the processing time here so we can include the room backfill",
            "        # linearizer lock queue in the timing",
            "        processing_start_time = self.clock.time_msec() if record_time else 0",
            "",
            "        async with self._room_backfill.queue(room_id):",
            "            async with self._worker_locks.acquire_read_write_lock(",
            "                PURGE_PAGINATION_LOCK_NAME, room_id, write=False",
            "            ):",
            "                return await self._maybe_backfill_inner(",
            "                    room_id,",
            "                    current_depth,",
            "                    limit,",
            "                    processing_start_time=processing_start_time,",
            "                )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _maybe_backfill_inner(",
            "        self,",
            "        room_id: str,",
            "        current_depth: int,",
            "        limit: int,",
            "        *,",
            "        processing_start_time: Optional[int],",
            "    ) -> bool:",
            "        \"\"\"",
            "        Checks whether the `current_depth` is at or approaching any backfill",
            "        points in the room and if so, will backfill. We only care about",
            "        checking backfill points that happened before the `current_depth`",
            "        (meaning less than or equal to the `current_depth`).",
            "",
            "        Args:",
            "            room_id: The room to backfill in.",
            "            current_depth: The depth to check at for any upcoming backfill points.",
            "            limit: The max number of events to request from the remote federated server.",
            "            processing_start_time: The time when `maybe_backfill` started processing.",
            "                Only used for timing. If `None`, no timing observation will be made.",
            "",
            "        Returns:",
            "            True if we actually tried to backfill something, otherwise False.",
            "        \"\"\"",
            "        backwards_extremities = [",
            "            _BackfillPoint(event_id, depth, _BackfillPointType.BACKWARDS_EXTREMITY)",
            "            for event_id, depth in await self.store.get_backfill_points_in_room(",
            "                room_id=room_id,",
            "                current_depth=current_depth,",
            "                # We only need to end up with 5 extremities combined with the",
            "                # insertion event extremities to make the `/backfill` request",
            "                # but fetch an order of magnitude more to make sure there is",
            "                # enough even after we filter them by whether visible in the",
            "                # history. This isn't fool-proof as all backfill points within",
            "                # our limit could be filtered out but seems like a good amount",
            "                # to try with at least.",
            "                limit=50,",
            "            )",
            "        ]",
            "",
            "        # we now have a list of potential places to backpaginate from. We prefer to",
            "        # start with the most recent (ie, max depth), so let's sort the list.",
            "        sorted_backfill_points: List[_BackfillPoint] = sorted(",
            "            backwards_extremities,",
            "            key=lambda e: -int(e.depth),",
            "        )",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: room_id: %s: current_depth: %s, limit: %s, \"",
            "            \"backfill points (%d): %s\",",
            "            room_id,",
            "            current_depth,",
            "            limit,",
            "            len(sorted_backfill_points),",
            "            sorted_backfill_points,",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"sorted_backfill_points\",",
            "            str(sorted_backfill_points),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"sorted_backfill_points.length\",",
            "            str(len(sorted_backfill_points)),",
            "        )",
            "",
            "        # If we have no backfill points lower than the `current_depth` then either we",
            "        # can a) bail or b) still attempt to backfill. We opt to try backfilling anyway",
            "        # just in case we do get relevant events. This is good for eventual consistency",
            "        # sake but we don't need to block the client for something that is just as",
            "        # likely not to return anything relevant so we backfill in the background. The",
            "        # only way, this could return something relevant is if we discover a new branch",
            "        # of history that extends all the way back to where we are currently paginating",
            "        # and it's within the 100 events that are returned from `/backfill`.",
            "        if not sorted_backfill_points and current_depth != MAX_DEPTH:",
            "            # Check that we actually have later backfill points, if not just return.",
            "            have_later_backfill_points = await self.store.get_backfill_points_in_room(",
            "                room_id=room_id,",
            "                current_depth=MAX_DEPTH,",
            "                limit=1,",
            "            )",
            "            if not have_later_backfill_points:",
            "                return False",
            "",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: all backfill points are *after* current depth. Trying again with later backfill points.\"",
            "            )",
            "            run_as_background_process(",
            "                \"_maybe_backfill_inner_anyway_with_max_depth\",",
            "                self.maybe_backfill,",
            "                room_id=room_id,",
            "                # We use `MAX_DEPTH` so that we find all backfill points next",
            "                # time (all events are below the `MAX_DEPTH`)",
            "                current_depth=MAX_DEPTH,",
            "                limit=limit,",
            "                # We don't want to start another timing observation from this",
            "                # nested recursive call. The top-most call can record the time",
            "                # overall otherwise the smaller one will throw off the results.",
            "                record_time=False,",
            "            )",
            "            # We return `False` because we're backfilling in the background and there is",
            "            # no new events immediately for the caller to know about yet.",
            "            return False",
            "",
            "        # Even after recursing with `MAX_DEPTH`, we didn't find any",
            "        # backward extremities to backfill from.",
            "        if not sorted_backfill_points:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: Not backfilling as no backward extremeties found.\"",
            "            )",
            "            return False",
            "",
            "        # If we're approaching an extremity we trigger a backfill, otherwise we",
            "        # no-op.",
            "        #",
            "        # We chose twice the limit here as then clients paginating backwards",
            "        # will send pagination requests that trigger backfill at least twice",
            "        # using the most recent extremity before it gets removed (see below). We",
            "        # chose more than one times the limit in case of failure, but choosing a",
            "        # much larger factor will result in triggering a backfill request much",
            "        # earlier than necessary.",
            "        max_depth_of_backfill_points = sorted_backfill_points[0].depth",
            "        if current_depth - 2 * limit > max_depth_of_backfill_points:",
            "            logger.debug(",
            "                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",",
            "                max_depth_of_backfill_points,",
            "                current_depth,",
            "                limit,",
            "            )",
            "            return False",
            "",
            "        # For performance's sake, we only want to paginate from a particular extremity",
            "        # if we can actually see the events we'll get. Otherwise, we'd just spend a lot",
            "        # of resources to get redacted events. We check each extremity in turn and",
            "        # ignore those which users on our server wouldn't be able to see.",
            "        #",
            "        # Additionally, we limit ourselves to backfilling from at most 5 extremities,",
            "        # for two reasons:",
            "        #",
            "        # - The check which determines if we can see an extremity's events can be",
            "        #   expensive (we load the full state for the room at each of the backfill",
            "        #   points, or (worse) their successors)",
            "        # - We want to avoid the server-server API request URI becoming too long.",
            "        #",
            "        # *Note*: the spec wants us to keep backfilling until we reach the start",
            "        # of the room in case we are allowed to see some of the history. However,",
            "        # in practice that causes more issues than its worth, as (a) it's",
            "        # relatively rare for there to be any visible history and (b) even when",
            "        # there is it's often sufficiently long ago that clients would stop",
            "        # attempting to paginate before backfill reached the visible history.",
            "",
            "        extremities_to_request: List[str] = []",
            "        for bp in sorted_backfill_points:",
            "            if len(extremities_to_request) >= 5:",
            "                break",
            "",
            "            # For regular backwards extremities, we don't have the extremity events",
            "            # themselves, so we need to actually check the events that reference them -",
            "            # their \"successor\" events.",
            "            #",
            "            # TODO: Correctly handle the case where we are allowed to see the",
            "            #   successor event but not the backward extremity, e.g. in the case of",
            "            #   initial join of the server where we are allowed to see the join",
            "            #   event but not anything before it. This would require looking at the",
            "            #   state *before* the event, ignoring the special casing certain event",
            "            #   types have.",
            "            event_ids_to_check = await self.store.get_successor_events(bp.event_id)",
            "",
            "            events_to_check = await self.store.get_events_as_list(",
            "                event_ids_to_check,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                get_prev_content=False,",
            "            )",
            "",
            "            # We unset `filter_out_erased_senders` as we might otherwise get false",
            "            # positives from users having been erased.",
            "            filtered_extremities = await filter_events_for_server(",
            "                self._storage_controllers,",
            "                self.server_name,",
            "                self.server_name,",
            "                events_to_check,",
            "                redact=False,",
            "                filter_out_erased_senders=False,",
            "                filter_out_remote_partial_state_events=False,",
            "            )",
            "            if filtered_extremities:",
            "                extremities_to_request.append(bp.event_id)",
            "            else:",
            "                logger.debug(",
            "                    \"_maybe_backfill_inner: skipping extremity %s as it would not be visible\",",
            "                    bp,",
            "                )",
            "",
            "        if not extremities_to_request:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: found no extremities which would be visible\"",
            "            )",
            "            return False",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: extremities_to_request %s\", extremities_to_request",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request\",",
            "            str(extremities_to_request),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request.length\",",
            "            str(len(extremities_to_request)),",
            "        )",
            "",
            "        # Now we need to decide which hosts to hit first.",
            "        # First we try hosts that are already in the room.",
            "        # TODO: HEURISTIC ALERT.",
            "        likely_domains = (",
            "            await self._storage_controllers.state.get_current_hosts_in_room_ordered(",
            "                room_id",
            "            )",
            "        )",
            "",
            "        async def try_backfill(domains: StrCollection) -> bool:",
            "            # TODO: Should we try multiple of these at a time?",
            "",
            "            # Number of contacted remote homeservers that have denied our backfill",
            "            # request with a 4xx code.",
            "            denied_count = 0",
            "",
            "            # Maximum number of contacted remote homeservers that can deny our",
            "            # backfill request with 4xx codes before we give up.",
            "            max_denied_count = 5",
            "",
            "            for dom in domains:",
            "                # We don't want to ask our own server for information we don't have",
            "                if self.is_mine_server_name(dom):",
            "                    continue",
            "",
            "                try:",
            "                    await self._federation_event_handler.backfill(",
            "                        dom, room_id, limit=100, extremities=extremities_to_request",
            "                    )",
            "                    # If this succeeded then we probably already have the",
            "                    # appropriate stuff.",
            "                    # TODO: We can probably do something more intelligent here.",
            "                    return True",
            "                except NotRetryingDestination as e:",
            "                    logger.info(\"_maybe_backfill_inner: %s\", e)",
            "                    continue",
            "                except FederationDeniedError:",
            "                    logger.info(",
            "                        \"_maybe_backfill_inner: Not attempting to backfill from %s because the homeserver is not on our federation whitelist\",",
            "                        dom,",
            "                    )",
            "                    continue",
            "                except (SynapseError, InvalidResponseError) as e:",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except HttpResponseException as e:",
            "                    if 400 <= e.code < 500:",
            "                        logger.warning(",
            "                            \"Backfill denied from %s because %s [%d/%d]\",",
            "                            dom,",
            "                            e,",
            "                            denied_count,",
            "                            max_denied_count,",
            "                        )",
            "                        denied_count += 1",
            "                        if denied_count >= max_denied_count:",
            "                            return False",
            "                        continue",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except CodeMessageException as e:",
            "                    if 400 <= e.code < 500:",
            "                        logger.warning(",
            "                            \"Backfill denied from %s because %s [%d/%d]\",",
            "                            dom,",
            "                            e,",
            "                            denied_count,",
            "                            max_denied_count,",
            "                        )",
            "                        denied_count += 1",
            "                        if denied_count >= max_denied_count:",
            "                            return False",
            "                        continue",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except RequestSendFailed as e:",
            "                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except Exception as e:",
            "                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "",
            "            return False",
            "",
            "        # If we have the `processing_start_time`, then we can make an",
            "        # observation. We wouldn't have the `processing_start_time` in the case",
            "        # where `_maybe_backfill_inner` is recursively called to find any",
            "        # backfill points regardless of `current_depth`.",
            "        if processing_start_time is not None:",
            "            processing_end_time = self.clock.time_msec()",
            "            backfill_processing_before_timer.observe(",
            "                (processing_end_time - processing_start_time) / 1000",
            "            )",
            "",
            "        success = await try_backfill(likely_domains)",
            "        if success:",
            "            return True",
            "",
            "        # TODO: we could also try servers which were previously in the room, but",
            "        #   are no longer.",
            "",
            "        return False",
            "",
            "    async def send_invite(self, target_host: str, event: EventBase) -> EventBase:",
            "        \"\"\"Sends the invite to the remote server for signing.",
            "",
            "        Invites must be signed by the invitee's server before distribution.",
            "        \"\"\"",
            "        try:",
            "            pdu = await self.federation_client.send_invite(",
            "                destination=target_host,",
            "                room_id=event.room_id,",
            "                event_id=event.event_id,",
            "                pdu=event,",
            "            )",
            "        except RequestSendFailed:",
            "            raise SynapseError(502, f\"Can't connect to server {target_host}\")",
            "",
            "        return pdu",
            "",
            "    async def on_event_auth(self, event_id: str) -> List[EventBase]:",
            "        event = await self.store.get_event(event_id)",
            "        auth = await self.store.get_auth_chain(",
            "            event.room_id, list(event.auth_event_ids()), include_given=True",
            "        )",
            "        return list(auth)",
            "",
            "    async def do_invite_join(",
            "        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Attempts to join the `joinee` to the room `room_id` via the",
            "        servers contained in `target_hosts`.",
            "",
            "        This first triggers a /make_join/ request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via /send_join/ which responds with the state at that",
            "        event and the auth_chains.",
            "",
            "        We suspend processing of any received events from this room until we",
            "        have finished processing the join.",
            "",
            "        Args:",
            "            target_hosts: List of servers to attempt to join the room with.",
            "",
            "            room_id: The ID of the room to join.",
            "",
            "            joinee: The User ID of the joining user.",
            "",
            "            content: The event content to use for the join event.",
            "        \"\"\"",
            "        # TODO: We should be able to call this on workers, but the upgrading of",
            "        # room stuff after join currently doesn't work on workers.",
            "        # TODO: Before we relax this condition, we need to allow re-syncing of",
            "        # partial room state to happen on workers.",
            "        assert self.config.worker.worker_app is None",
            "",
            "        logger.debug(\"Joining %s to %s\", joinee, room_id)",
            "",
            "        origin, event, room_version_obj = await self._make_and_verify_event(",
            "            target_hosts,",
            "            room_id,",
            "            joinee,",
            "            \"join\",",
            "            content,",
            "            params={\"ver\": KNOWN_ROOM_VERSIONS},",
            "        )",
            "",
            "        # This shouldn't happen, because the RoomMemberHandler has a",
            "        # linearizer lock which only allows one operation per user per room",
            "        # at a time - so this is just paranoia.",
            "        assert room_id not in self._federation_event_handler.room_queues",
            "",
            "        self._federation_event_handler.room_queues[room_id] = []",
            "",
            "        is_host_joined = await self.store.is_host_joined(room_id, self.server_name)",
            "",
            "        if not is_host_joined:",
            "            # We may have old forward extremities lying around if the homeserver left",
            "            # the room completely in the past. Clear them out.",
            "            #",
            "            # Note that this check-then-clear is subject to races where",
            "            #  * the homeserver is in the room and stops being in the room just after",
            "            #    the check. We won't reset the forward extremities, but that's okay,",
            "            #    since they will be almost up to date.",
            "            #  * the homeserver is not in the room and starts being in the room just",
            "            #    after the check. This can't happen, since `RoomMemberHandler` has a",
            "            #    linearizer lock which prevents concurrent remote joins into the same",
            "            #    room.",
            "            # In short, the races either have an acceptable outcome or should be",
            "            # impossible.",
            "            await self._clean_room_for_join(room_id)",
            "",
            "        try:",
            "            # Try the host we successfully got a response to /make_join/",
            "            # request first.",
            "            host_list = list(target_hosts)",
            "            try:",
            "                host_list.remove(origin)",
            "                host_list.insert(0, origin)",
            "            except ValueError:",
            "                pass",
            "",
            "            async with self._is_partial_state_room_linearizer.queue(room_id):",
            "                already_partial_state_room = await self.store.is_partial_state_room(",
            "                    room_id",
            "                )",
            "",
            "                ret = await self.federation_client.send_join(",
            "                    host_list,",
            "                    event,",
            "                    room_version_obj,",
            "                    # Perform a full join when we are already in the room and it is a",
            "                    # full state room, since we are not allowed to persist a partial",
            "                    # state join event in a full state room. In the future, we could",
            "                    # optimize this by always performing a partial state join and",
            "                    # computing the state ourselves or retrieving it from the remote",
            "                    # homeserver if necessary.",
            "                    #",
            "                    # There's a race where we leave the room, then perform a full join",
            "                    # anyway. This should end up being fast anyway, since we would",
            "                    # already have the full room state and auth chain persisted.",
            "                    partial_state=not is_host_joined or already_partial_state_room,",
            "                )",
            "",
            "                event = ret.event",
            "                origin = ret.origin",
            "                state = ret.state",
            "                auth_chain = ret.auth_chain",
            "                auth_chain.sort(key=lambda e: e.depth)",
            "",
            "                logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)",
            "                logger.debug(\"do_invite_join state: %s\", state)",
            "",
            "                logger.debug(\"do_invite_join event: %s\", event)",
            "",
            "                # if this is the first time we've joined this room, it's time to add",
            "                # a row to `rooms` with the correct room version. If there's already a",
            "                # row there, we should override it, since it may have been populated",
            "                # based on an invite request which lied about the room version.",
            "                #",
            "                # federation_client.send_join has already checked that the room",
            "                # version in the received create event is the same as room_version_obj,",
            "                # so we can rely on it now.",
            "                #",
            "                await self.store.upsert_room_on_join(",
            "                    room_id=room_id,",
            "                    room_version=room_version_obj,",
            "                    state_events=state,",
            "                )",
            "",
            "                if ret.partial_state and not already_partial_state_room:",
            "                    # Mark the room as having partial state.",
            "                    # The background process is responsible for unmarking this flag,",
            "                    # even if the join fails.",
            "                    # TODO(faster_joins):",
            "                    #     We may want to reset the partial state info if it's from an",
            "                    #     old, failed partial state join.",
            "                    #     https://github.com/matrix-org/synapse/issues/13000",
            "                    await self.store.store_partial_state_room(",
            "                        room_id=room_id,",
            "                        servers=ret.servers_in_room,",
            "                        device_lists_stream_id=self.store.get_device_stream_token(),",
            "                        joined_via=origin,",
            "                    )",
            "",
            "                try:",
            "                    max_stream_id = (",
            "                        await self._federation_event_handler.process_remote_join(",
            "                            origin,",
            "                            room_id,",
            "                            auth_chain,",
            "                            state,",
            "                            event,",
            "                            room_version_obj,",
            "                            partial_state=ret.partial_state,",
            "                        )",
            "                    )",
            "                except PartialStateConflictError:",
            "                    # This should be impossible, since we hold the lock on the room's",
            "                    # partial statedness.",
            "                    logger.error(",
            "                        \"Room %s was un-partial stated while processing remote join.\",",
            "                        room_id,",
            "                    )",
            "                    raise",
            "                else:",
            "                    # Record the join event id for future use (when we finish the full",
            "                    # join). We have to do this after persisting the event to keep",
            "                    # foreign key constraints intact.",
            "                    if ret.partial_state and not already_partial_state_room:",
            "                        # TODO(faster_joins):",
            "                        #     We may want to reset the partial state info if it's from",
            "                        #     an old, failed partial state join.",
            "                        #     https://github.com/matrix-org/synapse/issues/13000",
            "                        await self.store.write_partial_state_rooms_join_event_id(",
            "                            room_id, event.event_id",
            "                        )",
            "                finally:",
            "                    # Always kick off the background process that asynchronously fetches",
            "                    # state for the room.",
            "                    # If the join failed, the background process is responsible for",
            "                    # cleaning up \u2014 including unmarking the room as a partial state",
            "                    # room.",
            "                    if ret.partial_state:",
            "                        # Kick off the process of asynchronously fetching the state for",
            "                        # this room.",
            "                        self._start_partial_state_room_sync(",
            "                            initial_destination=origin,",
            "                            other_destinations=ret.servers_in_room,",
            "                            room_id=room_id,",
            "                        )",
            "",
            "            # We wait here until this instance has seen the events come down",
            "            # replication (if we're using replication) as the below uses caches.",
            "            await self._replication.wait_for_stream_position(",
            "                self.config.worker.events_shard_config.get_instance(room_id),",
            "                \"events\",",
            "                max_stream_id,",
            "            )",
            "",
            "            # Check whether this room is the result of an upgrade of a room we already know",
            "            # about. If so, migrate over user information",
            "            predecessor = await self.store.get_room_predecessor(room_id)",
            "            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):",
            "                return event.event_id, max_stream_id",
            "            old_room_id = predecessor[\"room_id\"]",
            "            logger.debug(",
            "                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id",
            "            )",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.transfer_room_state_on_room_upgrade(",
            "                old_room_id, room_id",
            "            )",
            "",
            "            logger.debug(\"Finished joining %s to %s\", joinee, room_id)",
            "            return event.event_id, max_stream_id",
            "        finally:",
            "            room_queue = self._federation_event_handler.room_queues[room_id]",
            "            del self._federation_event_handler.room_queues[room_id]",
            "",
            "            # we don't need to wait for the queued events to be processed -",
            "            # it's just a best-effort thing at this point. We do want to do",
            "            # them roughly in order, though, otherwise we'll end up making",
            "            # lots of requests for missing prev_events which we do actually",
            "            # have. Hence we fire off the background task, but don't wait for it.",
            "",
            "            run_as_background_process(",
            "                \"handle_queued_pdus\", self._handle_queued_pdus, room_queue",
            "            )",
            "",
            "    async def do_knock(",
            "        self,",
            "        target_hosts: List[str],",
            "        room_id: str,",
            "        knockee: str,",
            "        content: JsonDict,",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Sends the knock to the remote server.",
            "",
            "        This first triggers a make_knock request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via send_knock.",
            "",
            "        Knock events must be signed by the knockee's server before distributing.",
            "",
            "        Args:",
            "            target_hosts: A list of hosts that we want to try knocking through.",
            "            room_id: The ID of the room to knock on.",
            "            knockee: The ID of the user who is knocking.",
            "            content: The content of the knock event.",
            "",
            "        Returns:",
            "            A tuple of (event ID, stream ID).",
            "",
            "        Raises:",
            "            SynapseError: If the chosen remote server returns a 3xx/4xx code.",
            "            RuntimeError: If no servers were reachable.",
            "        \"\"\"",
            "        logger.debug(\"Knocking on room %s on behalf of user %s\", room_id, knockee)",
            "",
            "        # Inform the remote server of the room versions we support",
            "        supported_room_versions = list(KNOWN_ROOM_VERSIONS.keys())",
            "",
            "        # Ask the remote server to create a valid knock event for us. Once received,",
            "        # we sign the event",
            "        params: Dict[str, Iterable[str]] = {\"ver\": supported_room_versions}",
            "        origin, event, event_format_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, knockee, Membership.KNOCK, content, params=params",
            "        )",
            "",
            "        # Mark the knock as an outlier as we don't yet have the state at this point in",
            "        # the DAG.",
            "        event.internal_metadata.outlier = True",
            "",
            "        # ... but tell /sync to send it to clients anyway.",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Record the room ID and its version so that we have a record of the room",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=event_format_version",
            "        )",
            "",
            "        # Initially try the host that we successfully called /make_knock on",
            "        try:",
            "            target_hosts.remove(origin)",
            "            target_hosts.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        # Send the signed event back to the room, and potentially receive some",
            "        # further information about the room in the form of partial state events",
            "        knock_response = await self.federation_client.send_knock(target_hosts, event)",
            "",
            "        # Store any stripped room state events in the \"unsigned\" key of the event.",
            "        # This is a bit of a hack and is cribbing off of invites. Basically we",
            "        # store the room state here and retrieve it again when this event appears",
            "        # in the invitee's sync stream. It is stripped out for all other local users.",
            "        stripped_room_state = knock_response.get(\"knock_room_state\")",
            "",
            "        if stripped_room_state is None:",
            "            raise KeyError(\"Missing 'knock_room_state' field in send_knock response\")",
            "",
            "        if not isinstance(stripped_room_state, list):",
            "            raise TypeError(\"'knock_room_state' has wrong type\")",
            "",
            "        event.unsigned[\"knock_room_state\"] = stripped_room_state",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "        return event.event_id, stream_id",
            "",
            "    async def _handle_queued_pdus(",
            "        self, room_queue: List[Tuple[EventBase, str]]",
            "    ) -> None:",
            "        \"\"\"Process PDUs which got queued up while we were busy send_joining.",
            "",
            "        Args:",
            "            room_queue: list of PDUs to be processed and the servers that sent them",
            "        \"\"\"",
            "        for p, origin in room_queue:",
            "            try:",
            "                logger.info(",
            "                    \"Processing queued PDU %s which was received while we were joining\",",
            "                    p,",
            "                )",
            "                with nested_logging_context(p.event_id):",
            "                    await self._federation_event_handler.on_receive_pdu(origin, p)",
            "            except Exception as e:",
            "                logger.warning(",
            "                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e",
            "                )",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_join/ request, so we create a partial",
            "        join event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create join event in",
            "            user_id: The user to create the join for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_join request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        # checking the room version will check that we've actually heard of the room",
            "        # (and return a 404 otherwise)",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /make_join, so return a 404 as we would if we weren't in the",
            "            # room at all.",
            "            # The main reason we can't respond properly is that we need to know about",
            "            # the auth events for the join event that we would return.",
            "            # We also should not bother entertaining the /make_join since we cannot",
            "            # handle the /send_join.",
            "            logger.info(",
            "                \"Rejecting /make_join to %s because it's a partial state room\", room_id",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_join right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        # now check that we are *still* in the room",
            "        is_in_room = await self._event_auth_handler.is_host_in_room(",
            "            room_id, self.server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Got /make_join request for room %s we are no longer in\",",
            "                room_id,",
            "            )",
            "            raise NotFoundError(\"Not an active room on this server\")",
            "",
            "        event_content = {\"membership\": Membership.JOIN}",
            "",
            "        # If the current room is using restricted join rules, additional information",
            "        # may need to be included in the event content in order to efficiently",
            "        # validate the event.",
            "        #",
            "        # Note that this requires the /send_join request to come back to the",
            "        # same server.",
            "        prev_event_ids = None",
            "        if room_version.restricted_join_rule:",
            "            # Note that the room's state can change out from under us and render our",
            "            # nice join rules-conformant event non-conformant by the time we build the",
            "            # event. When this happens, our validation at the end fails and we respond",
            "            # to the requesting server with a 403, which is misleading \u2014 it indicates",
            "            # that the user is not allowed to join the room and the joining server",
            "            # should not bother retrying via this homeserver or any others, when",
            "            # in fact we've just messed up with building the event.",
            "            #",
            "            # To reduce the likelihood of this race, we capture the forward extremities",
            "            # of the room (prev_event_ids) just before fetching the current state, and",
            "            # hope that the state we fetch corresponds to the prev events we chose.",
            "            prev_event_ids = await self.store.get_prev_events_for_room(room_id)",
            "            state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                room_id",
            "            )",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                prev_member_event_id = state_ids.get((EventTypes.Member, user_id), None)",
            "                # If the user is invited or joined to the room already, then",
            "                # no additional info is needed.",
            "                include_auth_user_id = True",
            "                if prev_member_event_id:",
            "                    prev_member_event = await self.store.get_event(prev_member_event_id)",
            "                    include_auth_user_id = prev_member_event.membership not in (",
            "                        Membership.JOIN,",
            "                        Membership.INVITE,",
            "                    )",
            "",
            "                if include_auth_user_id:",
            "                    event_content[",
            "                        EventContentFields.AUTHORISING_USER",
            "                    ] = await self._event_auth_handler.get_user_which_could_invite(",
            "                        room_id,",
            "                        state_ids,",
            "                    )",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": event_content,",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        try:",
            "            (",
            "                event,",
            "                unpersisted_context,",
            "            ) = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder,",
            "                prev_event_ids=prev_event_ids,",
            "            )",
            "        except SynapseError as e:",
            "            logger.warning(\"Failed to create join to %s because %s\", room_id, e)",
            "            raise",
            "",
            "        # Ensure the user can even join the room.",
            "        await self._federation_event_handler.check_join_restrictions(",
            "            unpersisted_context, event",
            "        )",
            "",
            "        # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "        # when we get the event back in `on_send_join_request`",
            "        await self._event_auth_handler.check_auth_rules_from_context(event)",
            "        return event",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, event: EventBase, room_version: RoomVersion",
            "    ) -> EventBase:",
            "        \"\"\"We've got an invite event. Process and persist it. Sign it.",
            "",
            "        Respond with the now signed event.",
            "        \"\"\"",
            "        if event.state_key is None:",
            "            raise SynapseError(400, \"The invite event did not have a state key\")",
            "",
            "        is_blocked = await self.store.is_room_blocked(event.room_id)",
            "        if is_blocked:",
            "            raise SynapseError(403, \"This room has been blocked on this server\")",
            "",
            "        if self.hs.config.server.block_non_admin_invites:",
            "            raise SynapseError(403, \"This server does not accept room invites\")",
            "",
            "        spam_check = await self._spam_checker_module_callbacks.user_may_invite(",
            "            event.sender, event.state_key, event.room_id",
            "        )",
            "        if spam_check != NOT_SPAM:",
            "            raise SynapseError(",
            "                403,",
            "                \"This user is not permitted to send invites to this server/user\",",
            "                errcode=spam_check[0],",
            "                additional_fields=spam_check[1],",
            "            )",
            "",
            "        membership = event.content.get(\"membership\")",
            "        if event.type != EventTypes.Member or membership != Membership.INVITE:",
            "            raise SynapseError(400, \"The event was not an m.room.member invite event\")",
            "",
            "        sender_domain = get_domain_from_id(event.sender)",
            "        if sender_domain != origin:",
            "            raise SynapseError(",
            "                400, \"The invite event was not from the server sending it\"",
            "            )",
            "",
            "        if not self.is_mine_id(event.state_key):",
            "            raise SynapseError(400, \"The invite event must be for this server\")",
            "",
            "        # block any attempts to invite the server notices mxid",
            "        if event.state_key == self._server_notices_mxid:",
            "            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        # We don't rate limit based on room ID, as that should be done by",
            "        # sending server.",
            "        await member_handler.ratelimit_invite(None, None, event.state_key)",
            "",
            "        # keep a record of the room version, if we don't yet know it.",
            "        # (this may get overwritten if we later get a different room version in a",
            "        # join dance).",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=room_version",
            "        )",
            "",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        event.signatures.update(",
            "            compute_event_signature(",
            "                room_version,",
            "                event.get_pdu_json(),",
            "                self.hs.hostname,",
            "                self.hs.signing_key,",
            "            )",
            "        )",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "",
            "        await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "            [(event, context)]",
            "        )",
            "        try:",
            "            await self._federation_event_handler.persist_events_and_notify(",
            "                event.room_id, [(event, context)]",
            "            )",
            "        except Exception:",
            "            await self.store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "        return event",
            "",
            "    async def do_remotely_reject_invite(",
            "        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict",
            "    ) -> Tuple[EventBase, int]:",
            "        origin, event, room_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, user_id, \"leave\", content=content",
            "        )",
            "        # Mark as outlier as we don't have any state for this event; we're not",
            "        # even in the room.",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Try the host that we successfully called /make_leave/ on first for",
            "        # the /send_leave/ request.",
            "        host_list = list(target_hosts)",
            "        try:",
            "            host_list.remove(origin)",
            "            host_list.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        await self.federation_client.send_leave(host_list, event)",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event, stream_id",
            "",
            "    async def _make_and_verify_event(",
            "        self,",
            "        target_hosts: Iterable[str],",
            "        room_id: str,",
            "        user_id: str,",
            "        membership: str,",
            "        content: JsonDict,",
            "        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,",
            "    ) -> Tuple[str, EventBase, RoomVersion]:",
            "        (",
            "            origin,",
            "            event,",
            "            room_version,",
            "        ) = await self.federation_client.make_membership_event(",
            "            target_hosts, room_id, user_id, membership, content, params=params",
            "        )",
            "",
            "        logger.debug(\"Got response to make_%s: %s\", membership, event)",
            "",
            "        # We should assert some things.",
            "        # FIXME: Do this in a nicer way",
            "        assert event.type == EventTypes.Member",
            "        assert event.user_id == user_id",
            "        assert event.state_key == user_id",
            "        assert event.room_id == room_id",
            "        return origin, event, room_version",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_leave/ request, so we create a partial",
            "        leave event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create leave event in",
            "            user_id: The user to create the leave for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_leave request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.LEAVE},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, _ = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_leave_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new leave %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a make_knock request, so we create a partial",
            "        knock event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Get /make_knock request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.KNOCK},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        (",
            "            event,",
            "            unpersisted_context,",
            "        ) = await self.event_creation_handler.create_new_client_event(builder=builder)",
            "",
            "        event_allowed, _ = await self._third_party_event_rules.check_event_allowed(",
            "            event, unpersisted_context",
            "        )",
            "        if not event_allowed:",
            "            logger.warning(\"Creation of knock %s forbidden by third-party rules\", event)",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_knock_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new knock %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.\"\"\"",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "        if event.internal_metadata.outlier:",
            "            raise NotFoundError(\"State not known at event %s\" % (event_id,))",
            "",
            "        state_groups = await self._state_storage_controller.get_state_groups_ids(",
            "            room_id, [event_id]",
            "        )",
            "",
            "        # get_state_groups_ids should return exactly one result",
            "        assert len(state_groups) == 1",
            "",
            "        state_map = next(iter(state_groups.values()))",
            "",
            "        state_key = event.get_state_key()",
            "        if state_key is not None:",
            "            # the event was not rejected (get_event raises a NotFoundError for rejected",
            "            # events) so the state at the event should include the event itself.",
            "            assert (",
            "                state_map.get((event.type, state_key)) == event.event_id",
            "            ), \"State at event did not include event itself\"",
            "",
            "            # ... but we need the state *before* that event",
            "            if \"replaces_state\" in event.unsigned:",
            "                prev_id = event.unsigned[\"replaces_state\"]",
            "                state_map[(event.type, state_key)] = prev_id",
            "            else:",
            "                del state_map[(event.type, state_key)]",
            "",
            "        return list(state_map.values())",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, pdu_list: List[str], limit: int",
            "    ) -> List[EventBase]:",
            "        # We allow partially joined rooms since in this case we are filtering out",
            "        # non-local events in `filter_events_for_server`.",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin, True)",
            "",
            "        # Synapse asks for 100 events per backfill request. Do not allow more.",
            "        limit = min(limit, 100)",
            "",
            "        events = await self.store.get_backfill_events(room_id, pdu_list, limit)",
            "        logger.debug(",
            "            \"on_backfill_request: backfill events=%s\",",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        events = await filter_events_for_server(",
            "            self._storage_controllers,",
            "            origin,",
            "            self.server_name,",
            "            events,",
            "            redact=True,",
            "            filter_out_erased_senders=True,",
            "            filter_out_remote_partial_state_events=True,",
            "        )",
            "",
            "        return events",
            "",
            "    async def get_persisted_pdu(",
            "        self, origin: str, event_id: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get an event from the database for the given server.",
            "",
            "        Args:",
            "            origin: hostname of server which is requesting the event; we",
            "               will check that the server is allowed to see it.",
            "            event_id: id of the event being requested",
            "",
            "        Returns:",
            "            None if we know nothing about the event; otherwise the (possibly-redacted) event.",
            "",
            "        Raises:",
            "            AuthError if the server is not currently in the room",
            "        \"\"\"",
            "        event = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        if not event:",
            "            return None",
            "",
            "        await self._event_auth_handler.assert_host_in_room(event.room_id, origin)",
            "",
            "        events = await filter_events_for_server(",
            "            self._storage_controllers,",
            "            origin,",
            "            self.server_name,",
            "            [event],",
            "            redact=True,",
            "            filter_out_erased_senders=True,",
            "            filter_out_remote_partial_state_events=True,",
            "        )",
            "        event = events[0]",
            "        return event",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> List[EventBase]:",
            "        # We allow partially joined rooms since in this case we are filtering out",
            "        # non-local events in `filter_events_for_server`.",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin, True)",
            "",
            "        # Only allow up to 20 events to be retrieved per request.",
            "        limit = min(limit, 20)",
            "",
            "        missing_events = await self.store.get_missing_events(",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        missing_events = await filter_events_for_server(",
            "            self._storage_controllers,",
            "            origin,",
            "            self.server_name,",
            "            missing_events,",
            "            redact=True,",
            "            filter_out_erased_senders=True,",
            "            filter_out_remote_partial_state_events=True,",
            "        )",
            "",
            "        return missing_events",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: JsonDict",
            "    ) -> None:",
            "        third_party_invite = {\"signed\": signed}",
            "",
            "        event_dict = {",
            "            \"type\": EventTypes.Member,",
            "            \"content\": {",
            "                \"membership\": Membership.INVITE,",
            "                \"third_party_invite\": third_party_invite,",
            "            },",
            "            \"room_id\": room_id,",
            "            \"sender\": sender_user_id,",
            "            \"state_key\": target_user_id,",
            "        }",
            "",
            "        if await self._event_auth_handler.is_host_in_room(room_id, self.hs.hostname):",
            "            room_version_obj = await self.store.get_room_version(room_id)",
            "            builder = self.event_builder_factory.for_room_version(",
            "                room_version_obj, event_dict",
            "            )",
            "",
            "            EventValidator().validate_builder(builder)",
            "",
            "            # Try several times, it could fail with PartialStateConflictError",
            "            # in send_membership_event, cf comment in except block.",
            "            max_retries = 5",
            "            for i in range(max_retries):",
            "                try:",
            "                    (",
            "                        event,",
            "                        unpersisted_context,",
            "                    ) = await self.event_creation_handler.create_new_client_event(",
            "                        builder=builder",
            "                    )",
            "",
            "                    (",
            "                        event,",
            "                        unpersisted_context,",
            "                    ) = await self.add_display_name_to_third_party_invite(",
            "                        room_version_obj, event_dict, event, unpersisted_context",
            "                    )",
            "",
            "                    context = await unpersisted_context.persist(event)",
            "",
            "                    EventValidator().validate_new(event, self.config)",
            "",
            "                    # We need to tell the transaction queue to send this out, even",
            "                    # though the sender isn't a local user.",
            "                    event.internal_metadata.send_on_behalf_of = self.hs.hostname",
            "",
            "                    try:",
            "                        validate_event_for_room_version(event)",
            "                        await self._event_auth_handler.check_auth_rules_from_context(",
            "                            event",
            "                        )",
            "                    except AuthError as e:",
            "                        logger.warning(",
            "                            \"Denying new third party invite %r because %s\", event, e",
            "                        )",
            "                        raise e",
            "",
            "                    await self._check_signature(event, context)",
            "",
            "                    # We retrieve the room member handler here as to not cause a cyclic dependency",
            "                    member_handler = self.hs.get_room_member_handler()",
            "                    await member_handler.send_membership_event(None, event, context)",
            "",
            "                    break",
            "                except PartialStateConflictError as e:",
            "                    # Persisting couldn't happen because the room got un-partial stated",
            "                    # in the meantime and context needs to be recomputed, so let's do so.",
            "                    if i == max_retries - 1:",
            "                        raise e",
            "        else:",
            "            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}",
            "",
            "            try:",
            "                await self.federation_client.forward_third_party_invite(",
            "                    destinations, room_id, event_dict",
            "                )",
            "            except (RequestSendFailed, HttpResponseException):",
            "                raise SynapseError(502, \"Failed to forward third party invite\")",
            "",
            "    async def on_exchange_third_party_invite_request(",
            "        self, event_dict: JsonDict",
            "    ) -> None:",
            "        \"\"\"Handle an exchange_third_party_invite request from a remote server",
            "",
            "        The remote server will call this when it wants to turn a 3pid invite",
            "        into a normal m.room.member invite.",
            "",
            "        Args:",
            "            event_dict: Dictionary containing the event body.",
            "",
            "        \"\"\"",
            "        assert_params_in_dict(event_dict, [\"room_id\"])",
            "        room_version_obj = await self.store.get_room_version(event_dict[\"room_id\"])",
            "",
            "        # NB: event_dict has a particular specced format we might need to fudge",
            "        # if we change event formats too much.",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        # Try several times, it could fail with PartialStateConflictError",
            "        # in send_membership_event, cf comment in except block.",
            "        max_retries = 5",
            "        for i in range(max_retries):",
            "            try:",
            "                (",
            "                    event,",
            "                    unpersisted_context,",
            "                ) = await self.event_creation_handler.create_new_client_event(",
            "                    builder=builder",
            "                )",
            "                (",
            "                    event,",
            "                    unpersisted_context,",
            "                ) = await self.add_display_name_to_third_party_invite(",
            "                    room_version_obj, event_dict, event, unpersisted_context",
            "                )",
            "",
            "                context = await unpersisted_context.persist(event)",
            "",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    await self._event_auth_handler.check_auth_rules_from_context(event)",
            "                except AuthError as e:",
            "                    logger.warning(\"Denying third party invite %r because %s\", event, e)",
            "                    raise e",
            "                await self._check_signature(event, context)",
            "",
            "                # We need to tell the transaction queue to send this out, even",
            "                # though the sender isn't a local user.",
            "                event.internal_metadata.send_on_behalf_of = get_domain_from_id(",
            "                    event.sender",
            "                )",
            "",
            "                # We retrieve the room member handler here as to not cause a cyclic dependency",
            "                member_handler = self.hs.get_room_member_handler()",
            "                await member_handler.send_membership_event(None, event, context)",
            "",
            "                break",
            "            except PartialStateConflictError as e:",
            "                # Persisting couldn't happen because the room got un-partial stated",
            "                # in the meantime and context needs to be recomputed, so let's do so.",
            "                if i == max_retries - 1:",
            "                    raise e",
            "",
            "    async def add_display_name_to_third_party_invite(",
            "        self,",
            "        room_version_obj: RoomVersion,",
            "        event_dict: JsonDict,",
            "        event: EventBase,",
            "        context: UnpersistedEventContextBase,",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        key = (",
            "            EventTypes.ThirdPartyInvite,",
            "            event.content[\"third_party_invite\"][\"signed\"][\"token\"],",
            "        )",
            "        original_invite = None",
            "        prev_state_ids = await context.get_prev_state_ids(StateFilter.from_types([key]))",
            "        original_invite_id = prev_state_ids.get(key)",
            "        if original_invite_id:",
            "            original_invite = await self.store.get_event(",
            "                original_invite_id, allow_none=True",
            "            )",
            "        if original_invite:",
            "            # If the m.room.third_party_invite event's content is empty, it means the",
            "            # invite has been revoked. In this case, we don't have to raise an error here",
            "            # because the auth check will fail on the invite (because it's not able to",
            "            # fetch public keys from the m.room.third_party_invite event's content, which",
            "            # is empty).",
            "            display_name = original_invite.content.get(\"display_name\")",
            "            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name",
            "        else:",
            "            logger.info(",
            "                \"Could not find invite event for third_party_invite: %r\", event_dict",
            "            )",
            "            # We don't discard here as this is not the appropriate place to do",
            "            # auth checks. If we need the invite and don't have it then the",
            "            # auth check code will explode appropriately.",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "        EventValidator().validate_builder(builder)",
            "",
            "        (",
            "            event,",
            "            unpersisted_context,",
            "        ) = await self.event_creation_handler.create_new_client_event(builder=builder)",
            "",
            "        EventValidator().validate_new(event, self.config)",
            "        return event, unpersisted_context",
            "",
            "    async def _check_signature(self, event: EventBase, context: EventContext) -> None:",
            "        \"\"\"",
            "        Checks that the signature in the event is consistent with its invite.",
            "",
            "        Args:",
            "            event: The m.room.member event to check",
            "            context:",
            "",
            "        Raises:",
            "            AuthError: if signature didn't match any keys, or key has been",
            "                revoked,",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        signed = event.content[\"third_party_invite\"][\"signed\"]",
            "        token = signed[\"token\"]",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(EventTypes.ThirdPartyInvite, token)])",
            "        )",
            "        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))",
            "",
            "        invite_event = None",
            "        if invite_event_id:",
            "            invite_event = await self.store.get_event(invite_event_id, allow_none=True)",
            "",
            "        if not invite_event:",
            "            raise AuthError(403, \"Could not find invite\")",
            "",
            "        logger.debug(\"Checking auth on event %r\", event.content)",
            "",
            "        last_exception: Optional[Exception] = None",
            "",
            "        # for each public key in the 3pid invite event",
            "        for public_key_object in event_auth.get_public_keys(invite_event):",
            "            try:",
            "                # for each sig on the third_party_invite block of the actual invite",
            "                for server, signature_block in signed[\"signatures\"].items():",
            "                    for key_name in signature_block.keys():",
            "                        if not key_name.startswith(\"ed25519:\"):",
            "                            continue",
            "",
            "                        logger.debug(",
            "                            \"Attempting to verify sig with key %s from %r \"",
            "                            \"against pubkey %r\",",
            "                            key_name,",
            "                            server,",
            "                            public_key_object,",
            "                        )",
            "",
            "                        try:",
            "                            public_key = public_key_object[\"public_key\"]",
            "                            verify_key = decode_verify_key_bytes(",
            "                                key_name, decode_base64(public_key)",
            "                            )",
            "                            verify_signed_json(signed, server, verify_key)",
            "                            logger.debug(",
            "                                \"Successfully verified sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to verify sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                            raise",
            "                        try:",
            "                            if \"key_validity_url\" in public_key_object:",
            "                                await self._check_key_revocation(",
            "                                    public_key, public_key_object[\"key_validity_url\"]",
            "                                )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to query key_validity_url %s\",",
            "                                public_key_object[\"key_validity_url\"],",
            "                            )",
            "                            raise",
            "                        return",
            "            except Exception as e:",
            "                last_exception = e",
            "",
            "        if last_exception is None:",
            "            # we can only get here if get_public_keys() returned an empty list",
            "            # TODO: make this better",
            "            raise RuntimeError(\"no public key in invite event\")",
            "",
            "        raise last_exception",
            "",
            "    async def _check_key_revocation(self, public_key: str, url: str) -> None:",
            "        \"\"\"",
            "        Checks whether public_key has been revoked.",
            "",
            "        Args:",
            "            public_key: base-64 encoded public key.",
            "            url: Key revocation URL.",
            "",
            "        Raises:",
            "            AuthError: if they key has been revoked.",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        try:",
            "            response = await self.http_client.get_json(url, {\"public_key\": public_key})",
            "        except Exception:",
            "            raise SynapseError(502, \"Third party certificate could not be checked\")",
            "        if \"valid\" not in response or not response[\"valid\"]:",
            "            raise AuthError(403, \"Third party certificate was invalid\")",
            "",
            "    async def _clean_room_for_join(self, room_id: str) -> None:",
            "        \"\"\"Called to clean up any data in DB for a given room, ready for the",
            "        server to join the room.",
            "",
            "        Args:",
            "            room_id",
            "        \"\"\"",
            "        if self.config.worker.worker_app:",
            "            await self._clean_room_for_join_client(room_id)",
            "        else:",
            "            await self.store.clean_room_for_join(room_id)",
            "",
            "    async def get_room_complexity(",
            "        self, remote_room_hosts: List[str], room_id: str",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Fetch the complexity of a remote room over federation.",
            "",
            "        Args:",
            "            remote_room_hosts: The remote servers to ask.",
            "            room_id: The room ID to ask about.",
            "",
            "        Returns:",
            "            Dict contains the complexity",
            "            metric versions, while None means we could not fetch the complexity.",
            "        \"\"\"",
            "",
            "        for host in remote_room_hosts:",
            "            res = await self.federation_client.get_room_complexity(host, room_id)",
            "",
            "            # We got a result, return it.",
            "            if res:",
            "                return res",
            "",
            "        # We fell off the bottom, couldn't get the complexity from anyone. Oh",
            "        # well.",
            "        return None",
            "",
            "    async def _resume_partial_state_room_sync(self) -> None:",
            "        \"\"\"Resumes resyncing of all partial-state rooms after a restart.\"\"\"",
            "        assert not self.config.worker.worker_app",
            "",
            "        partial_state_rooms = await self.store.get_partial_state_room_resync_info()",
            "        for room_id, resync_info in partial_state_rooms.items():",
            "            self._start_partial_state_room_sync(",
            "                initial_destination=resync_info.joined_via,",
            "                other_destinations=resync_info.servers_in_room,",
            "                room_id=room_id,",
            "            )",
            "",
            "    def _start_partial_state_room_sync(",
            "        self,",
            "        initial_destination: Optional[str],",
            "        other_destinations: AbstractSet[str],",
            "        room_id: str,",
            "    ) -> None:",
            "        \"\"\"Starts the background process to resync the state of a partial state room,",
            "        if it is not already running.",
            "",
            "        Args:",
            "            initial_destination: the initial homeserver to pull the state from",
            "            other_destinations: other homeservers to try to pull the state from, if",
            "                `initial_destination` is unavailable",
            "            room_id: room to be resynced",
            "        \"\"\"",
            "",
            "        async def _sync_partial_state_room_wrapper() -> None:",
            "            if room_id in self._active_partial_state_syncs:",
            "                # Another local user has joined the room while there is already a",
            "                # partial state sync running. This implies that there is a new join",
            "                # event to un-partial state. We might find ourselves in one of a few",
            "                # scenarios:",
            "                #  1. There is an existing partial state sync. The partial state sync",
            "                #     un-partial states the new join event before completing and all is",
            "                #     well.",
            "                #  2. Before the latest join, the homeserver was no longer in the room",
            "                #     and there is an existing partial state sync from our previous",
            "                #     membership of the room. The partial state sync may have:",
            "                #      a) succeeded, but not yet terminated. The room will not be",
            "                #         un-partial stated again unless we restart the partial state",
            "                #         sync.",
            "                #      b) failed, because we were no longer in the room and remote",
            "                #         homeservers were refusing our requests, but not yet",
            "                #         terminated. After the latest join, remote homeservers may",
            "                #         start answering our requests again, so we should restart the",
            "                #         partial state sync.",
            "                # In the cases where we would want to restart the partial state sync,",
            "                # the room would have the partial state flag when the partial state sync",
            "                # terminates.",
            "                self._partial_state_syncs_maybe_needing_restart[room_id] = (",
            "                    initial_destination,",
            "                    other_destinations,",
            "                )",
            "                return",
            "",
            "            self._active_partial_state_syncs.add(room_id)",
            "",
            "            try:",
            "                await self._sync_partial_state_room(",
            "                    initial_destination=initial_destination,",
            "                    other_destinations=other_destinations,",
            "                    room_id=room_id,",
            "                )",
            "            finally:",
            "                # Read the room's partial state flag while we still hold the claim to",
            "                # being the active partial state sync (so that another partial state",
            "                # sync can't come along and mess with it under us).",
            "                # Normally, the partial state flag will be gone. If it isn't, then we",
            "                # may find ourselves in scenario 2a or 2b as described in the comment",
            "                # above, where we want to restart the partial state sync.",
            "                is_still_partial_state_room = await self.store.is_partial_state_room(",
            "                    room_id",
            "                )",
            "                self._active_partial_state_syncs.remove(room_id)",
            "",
            "                if room_id in self._partial_state_syncs_maybe_needing_restart:",
            "                    (",
            "                        restart_initial_destination,",
            "                        restart_other_destinations,",
            "                    ) = self._partial_state_syncs_maybe_needing_restart.pop(room_id)",
            "",
            "                    if is_still_partial_state_room:",
            "                        self._start_partial_state_room_sync(",
            "                            initial_destination=restart_initial_destination,",
            "                            other_destinations=restart_other_destinations,",
            "                            room_id=room_id,",
            "                        )",
            "",
            "        run_as_background_process(",
            "            desc=\"sync_partial_state_room\", func=_sync_partial_state_room_wrapper",
            "        )",
            "",
            "    async def _sync_partial_state_room(",
            "        self,",
            "        initial_destination: Optional[str],",
            "        other_destinations: AbstractSet[str],",
            "        room_id: str,",
            "    ) -> None:",
            "        \"\"\"Background process to resync the state of a partial-state room",
            "",
            "        Args:",
            "            initial_destination: the initial homeserver to pull the state from",
            "            other_destinations: other homeservers to try to pull the state from, if",
            "                `initial_destination` is unavailable",
            "            room_id: room to be resynced",
            "        \"\"\"",
            "        # Assume that we run on the main process for now.",
            "        # TODO(faster_joins,multiple workers)",
            "        # When moving the sync to workers, we need to ensure that",
            "        #  * `_start_partial_state_room_sync` still prevents duplicate resyncs",
            "        #  * `_is_partial_state_room_linearizer` correctly guards partial state flags",
            "        #    for rooms between the workers doing remote joins and resync.",
            "        assert not self.config.worker.worker_app",
            "",
            "        # TODO(faster_joins): do we need to lock to avoid races? What happens if other",
            "        #   worker processes kick off a resync in parallel? Perhaps we should just elect",
            "        #   a single worker to do the resync.",
            "        #   https://github.com/matrix-org/synapse/issues/12994",
            "        #",
            "        # TODO(faster_joins): what happens if we leave the room during a resync? if we",
            "        #   really leave, that might mean we have difficulty getting the room state over",
            "        #   federation.",
            "        #   https://github.com/matrix-org/synapse/issues/12802",
            "",
            "        # Make an infinite iterator of destinations to try. Once we find a working",
            "        # destination, we'll stick with it until it flakes.",
            "        destinations = _prioritise_destinations_for_partial_state_resync(",
            "            initial_destination, other_destinations, room_id",
            "        )",
            "        destination_iter = itertools.cycle(destinations)",
            "",
            "        # `destination` is the current remote homeserver we're pulling from.",
            "        destination = next(destination_iter)",
            "        logger.info(\"Syncing state for room %s via %s\", room_id, destination)",
            "",
            "        # we work through the queue in order of increasing stream ordering.",
            "        while True:",
            "            batch = await self.store.get_partial_state_events_batch(room_id)",
            "            if not batch:",
            "                # all the events are updated, so we can update current state and",
            "                # clear the lazy-loading flag.",
            "                logger.info(\"Updating current state for %s\", room_id)",
            "                # TODO(faster_joins): notify workers in notify_room_un_partial_stated",
            "                #   https://github.com/matrix-org/synapse/issues/12994",
            "                #",
            "                # NB: there's a potential race here. If room is purged just before we",
            "                # call this, we _might_ end up inserting rows into current_state_events.",
            "                # (The logic is hard to chase through.) We think this is fine, but if",
            "                # not the HS admin should purge the room again.",
            "                await self.state_handler.update_current_state(room_id)",
            "",
            "                logger.info(\"Handling any pending device list updates\")",
            "                await self._device_handler.handle_room_un_partial_stated(room_id)",
            "",
            "                async with self._is_partial_state_room_linearizer.queue(room_id):",
            "                    logger.info(\"Clearing partial-state flag for %s\", room_id)",
            "                    new_stream_id = await self.store.clear_partial_state_room(room_id)",
            "",
            "                if new_stream_id is not None:",
            "                    logger.info(\"State resync complete for %s\", room_id)",
            "                    self._storage_controllers.state.notify_room_un_partial_stated(",
            "                        room_id",
            "                    )",
            "",
            "                    await self._notifier.on_un_partial_stated_room(",
            "                        room_id, new_stream_id",
            "                    )",
            "                    return",
            "",
            "                # we raced against more events arriving with partial state. Go round",
            "                # the loop again. We've already logged a warning, so no need for more.",
            "                continue",
            "",
            "            events = await self.store.get_events_as_list(",
            "                batch,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                allow_rejected=True,",
            "            )",
            "            for event in events:",
            "                for attempt in itertools.count():",
            "                    # We try a new destination on every iteration.",
            "                    try:",
            "                        while True:",
            "                            try:",
            "                                await self._federation_event_handler.update_state_for_partial_state_event(",
            "                                    destination, event",
            "                                )",
            "                                break",
            "                            except FederationPullAttemptBackoffError as e:",
            "                                # We are in the backoff period for one of the event's",
            "                                # prev_events. Wait it out and try again after.",
            "                                logger.warning(",
            "                                    \"%s; waiting for %d ms...\", e, e.retry_after_ms",
            "                                )",
            "                                await self.clock.sleep(e.retry_after_ms / 1000)",
            "",
            "                        # Success, no need to try the rest of the destinations.",
            "                        break",
            "                    except FederationError as e:",
            "                        if attempt == len(destinations) - 1:",
            "                            # We have tried every remote server for this event. Give up.",
            "                            # TODO(faster_joins) giving up isn't the right thing to do",
            "                            #   if there's a temporary network outage. retrying",
            "                            #   indefinitely is also not the right thing to do if we can",
            "                            #   reach all homeservers and they all claim they don't have",
            "                            #   the state we want.",
            "                            #   https://github.com/matrix-org/synapse/issues/13000",
            "                            logger.error(",
            "                                \"Failed to get state for %s at %s from %s because %s, \"",
            "                                \"giving up!\",",
            "                                room_id,",
            "                                event,",
            "                                destination,",
            "                                e,",
            "                            )",
            "                            # TODO: We should `record_event_failed_pull_attempt` here,",
            "                            #   see https://github.com/matrix-org/synapse/issues/13700",
            "                            raise",
            "",
            "                        # Try the next remote server.",
            "                        logger.info(",
            "                            \"Failed to get state for %s at %s from %s because %s\",",
            "                            room_id,",
            "                            event,",
            "                            destination,",
            "                            e,",
            "                        )",
            "                        destination = next(destination_iter)",
            "                        logger.info(",
            "                            \"Syncing state for room %s via %s instead\",",
            "                            room_id,",
            "                            destination,",
            "                        )",
            "",
            "",
            "def _prioritise_destinations_for_partial_state_resync(",
            "    initial_destination: Optional[str],",
            "    other_destinations: AbstractSet[str],",
            "    room_id: str,",
            ") -> StrCollection:",
            "    \"\"\"Work out the order in which we should ask servers to resync events.",
            "",
            "    If an `initial_destination` is given, it takes top priority. Otherwise",
            "    all servers are treated equally.",
            "",
            "    :raises ValueError: if no destination is provided at all.",
            "    \"\"\"",
            "    if initial_destination is None and len(other_destinations) == 0:",
            "        raise ValueError(f\"Cannot resync state of {room_id}: no destinations provided\")",
            "",
            "    if initial_destination is None:",
            "        return other_destinations",
            "",
            "    # Move `initial_destination` to the front of the list.",
            "    destinations = list(other_destinations)",
            "    if initial_destination in destinations:",
            "        destinations.remove(initial_destination)",
            "    destinations = [initial_destination] + destinations",
            "    return destinations"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.handlers.federation.FederationHandler.self"
        ]
    },
    "synapse/handlers/sliding_sync/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 815,
                "afterPatchRowNumber": 815,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 816,
                "afterPatchRowNumber": 816,
                "PatchRowcode": "             stripped_state = []"
            },
            "2": {
                "beforePatchRowNumber": 817,
                "afterPatchRowNumber": 817,
                "PatchRowcode": "             if invite_or_knock_event.membership == Membership.INVITE:"
            },
            "3": {
                "beforePatchRowNumber": 818,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                stripped_state.extend("
            },
            "4": {
                "beforePatchRowNumber": 819,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    invite_or_knock_event.unsigned.get(\"invite_room_state\", [])"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 818,
                "PatchRowcode": "+                invite_state = invite_or_knock_event.unsigned.get("
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 819,
                "PatchRowcode": "+                    \"invite_room_state\", []"
            },
            "7": {
                "beforePatchRowNumber": 820,
                "afterPatchRowNumber": 820,
                "PatchRowcode": "                 )"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 821,
                "PatchRowcode": "+                if not isinstance(invite_state, list):"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 822,
                "PatchRowcode": "+                    invite_state = []"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 823,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 824,
                "PatchRowcode": "+                stripped_state.extend(invite_state)"
            },
            "12": {
                "beforePatchRowNumber": 821,
                "afterPatchRowNumber": 825,
                "PatchRowcode": "             elif invite_or_knock_event.membership == Membership.KNOCK:"
            },
            "13": {
                "beforePatchRowNumber": 822,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                stripped_state.extend("
            },
            "14": {
                "beforePatchRowNumber": 823,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    invite_or_knock_event.unsigned.get(\"knock_room_state\", [])"
            },
            "15": {
                "beforePatchRowNumber": 824,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                )"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 826,
                "PatchRowcode": "+                knock_state = invite_or_knock_event.unsigned.get(\"knock_room_state\", [])"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 827,
                "PatchRowcode": "+                if not isinstance(knock_state, list):"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 828,
                "PatchRowcode": "+                    knock_state = []"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 829,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 830,
                "PatchRowcode": "+                stripped_state.extend(knock_state)"
            },
            "21": {
                "beforePatchRowNumber": 825,
                "afterPatchRowNumber": 831,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 826,
                "afterPatchRowNumber": 832,
                "PatchRowcode": "             stripped_state.append(strip_event(invite_or_knock_event))"
            },
            "23": {
                "beforePatchRowNumber": 827,
                "afterPatchRowNumber": 833,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "",
            "import itertools",
            "import logging",
            "from itertools import chain",
            "from typing import TYPE_CHECKING, AbstractSet, Dict, List, Mapping, Optional, Set, Tuple",
            "",
            "from prometheus_client import Histogram",
            "from typing_extensions import assert_never",
            "",
            "from synapse.api.constants import Direction, EventTypes, Membership",
            "from synapse.events import EventBase",
            "from synapse.events.utils import strip_event",
            "from synapse.handlers.relations import BundledAggregations",
            "from synapse.handlers.sliding_sync.extensions import SlidingSyncExtensionHandler",
            "from synapse.handlers.sliding_sync.room_lists import (",
            "    RoomsForUserType,",
            "    SlidingSyncRoomLists,",
            ")",
            "from synapse.handlers.sliding_sync.store import SlidingSyncConnectionStore",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    log_kv,",
            "    set_tag,",
            "    start_active_span,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.storage.databases.main.roommember import extract_heroes_from_room_summary",
            "from synapse.storage.databases.main.state_deltas import StateDelta",
            "from synapse.storage.databases.main.stream import PaginateFunction",
            "from synapse.storage.roommember import (",
            "    MemberSummary,",
            ")",
            "from synapse.types import (",
            "    JsonDict,",
            "    MutableStateMap,",
            "    PersistedEventPosition,",
            "    Requester,",
            "    RoomStreamToken,",
            "    SlidingSyncStreamToken,",
            "    StateMap,",
            "    StrCollection,",
            "    StreamKeyType,",
            "    StreamToken,",
            ")",
            "from synapse.types.handlers import SLIDING_SYNC_DEFAULT_BUMP_EVENT_TYPES",
            "from synapse.types.handlers.sliding_sync import (",
            "    HaveSentRoomFlag,",
            "    MutablePerConnectionState,",
            "    PerConnectionState,",
            "    RoomSyncConfig,",
            "    SlidingSyncConfig,",
            "    SlidingSyncResult,",
            "    StateValues,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import concurrently_execute",
            "from synapse.visibility import filter_events_for_client",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "sync_processing_time = Histogram(",
            "    \"synapse_sliding_sync_processing_time\",",
            "    \"Time taken to generate a sliding sync response, ignoring wait times.\",",
            "    [\"initial\"],",
            ")",
            "",
            "# Limit the number of state_keys we should remember sending down the connection for each",
            "# (room_id, user_id). We don't want to store and pull out too much data in the database.",
            "#",
            "# 100 is an arbitrary but small-ish number. The idea is that we probably won't send down",
            "# too many redundant member state events (that the client already knows about) for a",
            "# given ongoing conversation if we keep 100 around. Most rooms don't have 100 members",
            "# anyway and it takes a while to cycle through 100 members.",
            "MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER = 100",
            "",
            "",
            "class SlidingSyncHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.clock = hs.get_clock()",
            "        self.store = hs.get_datastores().main",
            "        self.storage_controllers = hs.get_storage_controllers()",
            "        self.auth_blocking = hs.get_auth_blocking()",
            "        self.notifier = hs.get_notifier()",
            "        self.event_sources = hs.get_event_sources()",
            "        self.relations_handler = hs.get_relations_handler()",
            "        self.rooms_to_exclude_globally = hs.config.server.rooms_to_exclude_from_sync",
            "        self.is_mine_id = hs.is_mine_id",
            "",
            "        self.connection_store = SlidingSyncConnectionStore(self.store)",
            "        self.extensions = SlidingSyncExtensionHandler(hs)",
            "        self.room_lists = SlidingSyncRoomLists(hs)",
            "",
            "    async def wait_for_sync_for_user(",
            "        self,",
            "        requester: Requester,",
            "        sync_config: SlidingSyncConfig,",
            "        from_token: Optional[SlidingSyncStreamToken] = None,",
            "        timeout_ms: int = 0,",
            "    ) -> SlidingSyncResult:",
            "        \"\"\"",
            "        Get the sync for a client if we have new data for it now. Otherwise",
            "        wait for new data to arrive on the server. If the timeout expires, then",
            "        return an empty sync result.",
            "",
            "        Args:",
            "            requester: The user making the request",
            "            sync_config: Sync configuration",
            "            from_token: The point in the stream to sync from. Token of the end of the",
            "                previous batch. May be `None` if this is the initial sync request.",
            "            timeout_ms: The time in milliseconds to wait for new data to arrive. If 0,",
            "                we will immediately but there might not be any new data so we just return an",
            "                empty response.",
            "        \"\"\"",
            "        # If the user is not part of the mau group, then check that limits have",
            "        # not been exceeded (if not part of the group by this point, almost certain",
            "        # auth_blocking will occur)",
            "        await self.auth_blocking.check_auth_blocking(requester=requester)",
            "",
            "        # If we're working with a user-provided token, we need to make sure to wait for",
            "        # this worker to catch up with the token so we don't skip past any incoming",
            "        # events or future events if the user is nefariously, manually modifying the",
            "        # token.",
            "        if from_token is not None:",
            "            # We need to make sure this worker has caught up with the token. If",
            "            # this returns false, it means we timed out waiting, and we should",
            "            # just return an empty response.",
            "            before_wait_ts = self.clock.time_msec()",
            "            if not await self.notifier.wait_for_stream_token(from_token.stream_token):",
            "                logger.warning(",
            "                    \"Timed out waiting for worker to catch up. Returning empty response\"",
            "                )",
            "                return SlidingSyncResult.empty(from_token)",
            "",
            "            # If we've spent significant time waiting to catch up, take it off",
            "            # the timeout.",
            "            after_wait_ts = self.clock.time_msec()",
            "            if after_wait_ts - before_wait_ts > 1_000:",
            "                timeout_ms -= after_wait_ts - before_wait_ts",
            "                timeout_ms = max(timeout_ms, 0)",
            "",
            "        # We're going to respond immediately if the timeout is 0 or if this is an",
            "        # initial sync (without a `from_token`) so we can avoid calling",
            "        # `notifier.wait_for_events()`.",
            "        if timeout_ms == 0 or from_token is None:",
            "            now_token = self.event_sources.get_current_token()",
            "            result = await self.current_sync_for_user(",
            "                sync_config,",
            "                from_token=from_token,",
            "                to_token=now_token,",
            "            )",
            "        else:",
            "            # Otherwise, we wait for something to happen and report it to the user.",
            "            async def current_sync_callback(",
            "                before_token: StreamToken, after_token: StreamToken",
            "            ) -> SlidingSyncResult:",
            "                return await self.current_sync_for_user(",
            "                    sync_config,",
            "                    from_token=from_token,",
            "                    to_token=after_token,",
            "                )",
            "",
            "            result = await self.notifier.wait_for_events(",
            "                sync_config.user.to_string(),",
            "                timeout_ms,",
            "                current_sync_callback,",
            "                from_token=from_token.stream_token,",
            "            )",
            "",
            "        return result",
            "",
            "    @trace",
            "    async def current_sync_for_user(",
            "        self,",
            "        sync_config: SlidingSyncConfig,",
            "        to_token: StreamToken,",
            "        from_token: Optional[SlidingSyncStreamToken] = None,",
            "    ) -> SlidingSyncResult:",
            "        \"\"\"",
            "        Generates the response body of a Sliding Sync result, represented as a",
            "        `SlidingSyncResult`.",
            "",
            "        We fetch data according to the token range (> `from_token` and <= `to_token`).",
            "",
            "        Args:",
            "            sync_config: Sync configuration",
            "            to_token: The point in the stream to sync up to.",
            "            from_token: The point in the stream to sync from. Token of the end of the",
            "                previous batch. May be `None` if this is the initial sync request.",
            "        \"\"\"",
            "        start_time_s = self.clock.time()",
            "",
            "        user_id = sync_config.user.to_string()",
            "        app_service = self.store.get_app_service_by_user_id(user_id)",
            "        if app_service:",
            "            # We no longer support AS users using /sync directly.",
            "            # See https://github.com/matrix-org/matrix-doc/issues/1144",
            "            raise NotImplementedError()",
            "",
            "        # Get the per-connection state (if any).",
            "        #",
            "        # Raises an exception if there is a `connection_position` that we don't",
            "        # recognize. If we don't do this and the client asks for the full range",
            "        # of rooms, we end up sending down all rooms and their state from",
            "        # scratch (which can be very slow). By expiring the connection we allow",
            "        # the client a chance to do an initial request with a smaller range of",
            "        # rooms to get them some results sooner but will end up taking the same",
            "        # amount of time (more with round-trips and re-processing) in the end to",
            "        # get everything again.",
            "        previous_connection_state = (",
            "            await self.connection_store.get_and_clear_connection_positions(",
            "                sync_config, from_token",
            "            )",
            "        )",
            "",
            "        # Get all of the room IDs that the user should be able to see in the sync",
            "        # response",
            "        has_lists = sync_config.lists is not None and len(sync_config.lists) > 0",
            "        has_room_subscriptions = (",
            "            sync_config.room_subscriptions is not None",
            "            and len(sync_config.room_subscriptions) > 0",
            "        )",
            "",
            "        interested_rooms = await self.room_lists.compute_interested_rooms(",
            "            sync_config=sync_config,",
            "            previous_connection_state=previous_connection_state,",
            "            from_token=from_token.stream_token if from_token else None,",
            "            to_token=to_token,",
            "        )",
            "",
            "        lists = interested_rooms.lists",
            "        relevant_room_map = interested_rooms.relevant_room_map",
            "        all_rooms = interested_rooms.all_rooms",
            "        room_membership_for_user_map = interested_rooms.room_membership_for_user_map",
            "        relevant_rooms_to_send_map = interested_rooms.relevant_rooms_to_send_map",
            "",
            "        # Fetch room data",
            "        rooms: Dict[str, SlidingSyncResult.RoomResult] = {}",
            "",
            "        new_connection_state = previous_connection_state.get_mutable()",
            "",
            "        @trace",
            "        @tag_args",
            "        async def handle_room(room_id: str) -> None:",
            "            room_sync_result = await self.get_room_sync_data(",
            "                sync_config=sync_config,",
            "                previous_connection_state=previous_connection_state,",
            "                new_connection_state=new_connection_state,",
            "                room_id=room_id,",
            "                room_sync_config=relevant_rooms_to_send_map[room_id],",
            "                room_membership_for_user_at_to_token=room_membership_for_user_map[",
            "                    room_id",
            "                ],",
            "                from_token=from_token,",
            "                to_token=to_token,",
            "                newly_joined=room_id in interested_rooms.newly_joined_rooms,",
            "                is_dm=room_id in interested_rooms.dm_room_ids,",
            "            )",
            "",
            "            # Filter out empty room results during incremental sync",
            "            if room_sync_result or not from_token:",
            "                rooms[room_id] = room_sync_result",
            "",
            "        if relevant_rooms_to_send_map:",
            "            with start_active_span(\"sliding_sync.generate_room_entries\"):",
            "                await concurrently_execute(handle_room, relevant_rooms_to_send_map, 20)",
            "",
            "        extensions = await self.extensions.get_extensions_response(",
            "            sync_config=sync_config,",
            "            actual_lists=lists,",
            "            previous_connection_state=previous_connection_state,",
            "            new_connection_state=new_connection_state,",
            "            # We're purposely using `relevant_room_map` instead of",
            "            # `relevant_rooms_to_send_map` here. This needs to be all room_ids we could",
            "            # send regardless of whether they have an event update or not. The",
            "            # extensions care about more than just normal events in the rooms (like",
            "            # account data, read receipts, typing indicators, to-device messages, etc).",
            "            actual_room_ids=set(relevant_room_map.keys()),",
            "            actual_room_response_map=rooms,",
            "            from_token=from_token,",
            "            to_token=to_token,",
            "        )",
            "",
            "        if has_lists or has_room_subscriptions:",
            "            # We now calculate if any rooms outside the range have had updates,",
            "            # which we are not sending down.",
            "            #",
            "            # We *must* record rooms that have had updates, but it is also fine",
            "            # to record rooms as having updates even if there might not actually",
            "            # be anything new for the user (e.g. due to event filters, events",
            "            # having happened after the user left, etc).",
            "            if from_token:",
            "                # The set of rooms that the client (may) care about, but aren't",
            "                # in any list range (or subscribed to).",
            "                missing_rooms = all_rooms - relevant_room_map.keys()",
            "",
            "                # We now just go and try fetching any events in the above rooms",
            "                # to see if anything has happened since the `from_token`.",
            "                #",
            "                # TODO: Replace this with something faster. When we land the",
            "                # sliding sync tables that record the most recent event",
            "                # positions we can use that.",
            "                unsent_room_ids: StrCollection",
            "                if await self.store.have_finished_sliding_sync_background_jobs():",
            "                    unsent_room_ids = await (",
            "                        self.store.get_rooms_that_have_updates_since_sliding_sync_table(",
            "                            room_ids=missing_rooms,",
            "                            from_key=from_token.stream_token.room_key,",
            "                        )",
            "                    )",
            "                else:",
            "                    missing_event_map_by_room = (",
            "                        await self.store.get_room_events_stream_for_rooms(",
            "                            room_ids=missing_rooms,",
            "                            from_key=to_token.room_key,",
            "                            to_key=from_token.stream_token.room_key,",
            "                            limit=1,",
            "                        )",
            "                    )",
            "                    unsent_room_ids = list(missing_event_map_by_room)",
            "",
            "                new_connection_state.rooms.record_unsent_rooms(",
            "                    unsent_room_ids, from_token.stream_token.room_key",
            "                )",
            "",
            "            new_connection_state.rooms.record_sent_rooms(",
            "                relevant_rooms_to_send_map.keys()",
            "            )",
            "",
            "            connection_position = await self.connection_store.record_new_state(",
            "                sync_config=sync_config,",
            "                from_token=from_token,",
            "                new_connection_state=new_connection_state,",
            "            )",
            "        elif from_token:",
            "            connection_position = from_token.connection_position",
            "        else:",
            "            # Initial sync without a `from_token` starts at `0`",
            "            connection_position = 0",
            "",
            "        sliding_sync_result = SlidingSyncResult(",
            "            next_pos=SlidingSyncStreamToken(to_token, connection_position),",
            "            lists=lists,",
            "            rooms=rooms,",
            "            extensions=extensions,",
            "        )",
            "",
            "        # Make it easy to find traces for syncs that aren't empty",
            "        set_tag(SynapseTags.RESULT_PREFIX + \"result\", bool(sliding_sync_result))",
            "        set_tag(SynapseTags.FUNC_ARG_PREFIX + \"sync_config.user\", user_id)",
            "",
            "        end_time_s = self.clock.time()",
            "        sync_processing_time.labels(from_token is not None).observe(",
            "            end_time_s - start_time_s",
            "        )",
            "",
            "        return sliding_sync_result",
            "",
            "    @trace",
            "    async def get_current_state_ids_at(",
            "        self,",
            "        room_id: str,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        state_filter: StateFilter,",
            "        to_token: StreamToken,",
            "    ) -> StateMap[str]:",
            "        \"\"\"",
            "        Get current state IDs for the user in the room according to their membership. This",
            "        will be the current state at the time of their LEAVE/BAN, otherwise will be the",
            "        current state <= to_token.",
            "",
            "        Args:",
            "            room_id: The room ID to fetch data for",
            "            room_membership_for_user_at_token: Membership information for the user",
            "                in the room at the time of `to_token`.",
            "            to_token: The point in the stream to sync up to.",
            "        \"\"\"",
            "        state_ids: StateMap[str]",
            "        # People shouldn't see past their leave/ban event",
            "        if room_membership_for_user_at_to_token.membership in (",
            "            Membership.LEAVE,",
            "            Membership.BAN,",
            "        ):",
            "            # TODO: `get_state_ids_at(...)` doesn't take into account the \"current",
            "            # state\". Maybe we need to use",
            "            # `get_forward_extremities_for_room_at_stream_ordering(...)` to \"Fetch the",
            "            # current state at the time.\"",
            "            state_ids = await self.storage_controllers.state.get_state_ids_at(",
            "                room_id,",
            "                stream_position=to_token.copy_and_replace(",
            "                    StreamKeyType.ROOM,",
            "                    room_membership_for_user_at_to_token.event_pos.to_room_stream_token(),",
            "                ),",
            "                state_filter=state_filter,",
            "                # Partially-stated rooms should have all state events except for",
            "                # remote membership events. Since we've already excluded",
            "                # partially-stated rooms unless `required_state` only has",
            "                # `[\"m.room.member\", \"$LAZY\"]` for membership, we should be able to",
            "                # retrieve everything requested. When we're lazy-loading, if there",
            "                # are some remote senders in the timeline, we should also have their",
            "                # membership event because we had to auth that timeline event. Plus",
            "                # we don't want to block the whole sync waiting for this one room.",
            "                await_full_state=False,",
            "            )",
            "        # Otherwise, we can get the latest current state in the room",
            "        else:",
            "            state_ids = await self.storage_controllers.state.get_current_state_ids(",
            "                room_id,",
            "                state_filter,",
            "                # Partially-stated rooms should have all state events except for",
            "                # remote membership events. Since we've already excluded",
            "                # partially-stated rooms unless `required_state` only has",
            "                # `[\"m.room.member\", \"$LAZY\"]` for membership, we should be able to",
            "                # retrieve everything requested. When we're lazy-loading, if there",
            "                # are some remote senders in the timeline, we should also have their",
            "                # membership event because we had to auth that timeline event. Plus",
            "                # we don't want to block the whole sync waiting for this one room.",
            "                await_full_state=False,",
            "            )",
            "            # TODO: Query `current_state_delta_stream` and reverse/rewind back to the `to_token`",
            "",
            "        return state_ids",
            "",
            "    @trace",
            "    async def get_current_state_at(",
            "        self,",
            "        room_id: str,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        state_filter: StateFilter,",
            "        to_token: StreamToken,",
            "    ) -> StateMap[EventBase]:",
            "        \"\"\"",
            "        Get current state for the user in the room according to their membership. This",
            "        will be the current state at the time of their LEAVE/BAN, otherwise will be the",
            "        current state <= to_token.",
            "",
            "        Args:",
            "            room_id: The room ID to fetch data for",
            "            room_membership_for_user_at_token: Membership information for the user",
            "                in the room at the time of `to_token`.",
            "            to_token: The point in the stream to sync up to.",
            "        \"\"\"",
            "        state_ids = await self.get_current_state_ids_at(",
            "            room_id=room_id,",
            "            room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "            state_filter=state_filter,",
            "            to_token=to_token,",
            "        )",
            "",
            "        events = await self.store.get_events_as_list(list(state_ids.values()))",
            "",
            "        state_map = {}",
            "        for event in events:",
            "            state_map[(event.type, event.state_key)] = event",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    async def get_current_state_deltas_for_room(",
            "        self,",
            "        room_id: str,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        from_token: RoomStreamToken,",
            "        to_token: RoomStreamToken,",
            "    ) -> List[StateDelta]:",
            "        \"\"\"",
            "        Get the state deltas between two tokens taking into account the user's",
            "        membership. If the user is LEAVE/BAN, we will only get the state deltas up to",
            "        their LEAVE/BAN event (inclusive).",
            "",
            "        (> `from_token` and <= `to_token`)",
            "        \"\"\"",
            "        membership = room_membership_for_user_at_to_token.membership",
            "        # We don't know how to handle `membership` values other than these. The",
            "        # code below would need to be updated.",
            "        assert membership in (",
            "            Membership.JOIN,",
            "            Membership.INVITE,",
            "            Membership.KNOCK,",
            "            Membership.LEAVE,",
            "            Membership.BAN,",
            "        )",
            "",
            "        # People shouldn't see past their leave/ban event",
            "        if membership in (",
            "            Membership.LEAVE,",
            "            Membership.BAN,",
            "        ):",
            "            to_bound = (",
            "                room_membership_for_user_at_to_token.event_pos.to_room_stream_token()",
            "            )",
            "        # If we are participating in the room, we can get the latest current state in",
            "        # the room",
            "        elif membership == Membership.JOIN:",
            "            to_bound = to_token",
            "        # We can only rely on the stripped state included in the invite/knock event",
            "        # itself so there will never be any state deltas to send down.",
            "        elif membership in (Membership.INVITE, Membership.KNOCK):",
            "            return []",
            "        else:",
            "            # We don't know how to handle this type of membership yet",
            "            #",
            "            # FIXME: We should use `assert_never` here but for some reason",
            "            # the exhaustive matching doesn't recognize the `Never` here.",
            "            # assert_never(membership)",
            "            raise AssertionError(",
            "                f\"Unexpected membership {membership} that we don't know how to handle yet\"",
            "            )",
            "",
            "        return await self.store.get_current_state_deltas_for_room(",
            "            room_id=room_id,",
            "            from_token=from_token,",
            "            to_token=to_bound,",
            "        )",
            "",
            "    @trace",
            "    async def get_room_sync_data(",
            "        self,",
            "        sync_config: SlidingSyncConfig,",
            "        previous_connection_state: \"PerConnectionState\",",
            "        new_connection_state: \"MutablePerConnectionState\",",
            "        room_id: str,",
            "        room_sync_config: RoomSyncConfig,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        from_token: Optional[SlidingSyncStreamToken],",
            "        to_token: StreamToken,",
            "        newly_joined: bool,",
            "        is_dm: bool,",
            "    ) -> SlidingSyncResult.RoomResult:",
            "        \"\"\"",
            "        Fetch room data for the sync response.",
            "",
            "        We fetch data according to the token range (> `from_token` and <= `to_token`).",
            "",
            "        Args:",
            "            user: User to fetch data for",
            "            room_id: The room ID to fetch data for",
            "            room_sync_config: Config for what data we should fetch for a room in the",
            "                sync response.",
            "            room_membership_for_user_at_to_token: Membership information for the user",
            "                in the room at the time of `to_token`.",
            "            from_token: The point in the stream to sync from.",
            "            to_token: The point in the stream to sync up to.",
            "            newly_joined: If the user has newly joined the room",
            "            is_dm: Whether the room is a DM room",
            "        \"\"\"",
            "        user = sync_config.user",
            "",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"membership\",",
            "            room_membership_for_user_at_to_token.membership,",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"timeline_limit\",",
            "            room_sync_config.timeline_limit,",
            "        )",
            "",
            "        # Handle state resets. For example, if we see",
            "        # `room_membership_for_user_at_to_token.event_id=None and",
            "        # room_membership_for_user_at_to_token.membership is not None`, we should",
            "        # indicate to the client that a state reset happened. Perhaps we should indicate",
            "        # this by setting `initial: True` and empty `required_state: []`.",
            "        state_reset_out_of_room = False",
            "        if (",
            "            room_membership_for_user_at_to_token.event_id is None",
            "            and room_membership_for_user_at_to_token.membership is not None",
            "        ):",
            "            # We only expect the `event_id` to be `None` if you've been state reset out",
            "            # of the room (meaning you're no longer in the room). We could put this as",
            "            # part of the if-statement above but we want to handle every case where",
            "            # `event_id` is `None`.",
            "            assert room_membership_for_user_at_to_token.membership is Membership.LEAVE",
            "",
            "            state_reset_out_of_room = True",
            "",
            "        prev_room_sync_config = previous_connection_state.room_configs.get(room_id)",
            "",
            "        # Determine whether we should limit the timeline to the token range.",
            "        #",
            "        # We should return historical messages (before token range) in the",
            "        # following cases because we want clients to be able to show a basic",
            "        # screen of information:",
            "        #",
            "        #  - Initial sync (because no `from_token` to limit us anyway)",
            "        #  - When users `newly_joined`",
            "        #  - For an incremental sync where we haven't sent it down this",
            "        #    connection before",
            "        #",
            "        # Relevant spec issue:",
            "        # https://github.com/matrix-org/matrix-spec/issues/1917",
            "        #",
            "        # XXX: Odd behavior - We also check if the `timeline_limit` has increased, if so",
            "        # we ignore the from bound for the timeline to send down a larger chunk of",
            "        # history and set `unstable_expanded_timeline` to true. This is only being added",
            "        # to match the behavior of the Sliding Sync proxy as we expect the ElementX",
            "        # client to feel a certain way and be able to trickle in a full page of timeline",
            "        # messages to fill up the screen. This is a bit different to the behavior of the",
            "        # Sliding Sync proxy (which sets initial=true, but then doesn't send down the",
            "        # full state again), but existing apps, e.g. ElementX, just need `limited` set.",
            "        # We don't explicitly set `limited` but this will be the case for any room that",
            "        # has more history than we're trying to pull out. Using",
            "        # `unstable_expanded_timeline` allows us to avoid contaminating what `initial`",
            "        # or `limited` mean for clients that interpret them correctly. In future this",
            "        # behavior is almost certainly going to change.",
            "        #",
            "        from_bound = None",
            "        initial = True",
            "        ignore_timeline_bound = False",
            "        if from_token and not newly_joined and not state_reset_out_of_room:",
            "            room_status = previous_connection_state.rooms.have_sent_room(room_id)",
            "            if room_status.status == HaveSentRoomFlag.LIVE:",
            "                from_bound = from_token.stream_token.room_key",
            "                initial = False",
            "            elif room_status.status == HaveSentRoomFlag.PREVIOUSLY:",
            "                assert room_status.last_token is not None",
            "                from_bound = room_status.last_token",
            "                initial = False",
            "            elif room_status.status == HaveSentRoomFlag.NEVER:",
            "                from_bound = None",
            "                initial = True",
            "            else:",
            "                assert_never(room_status.status)",
            "",
            "            log_kv({\"sliding_sync.room_status\": room_status})",
            "",
            "            if prev_room_sync_config is not None:",
            "                # Check if the timeline limit has increased, if so ignore the",
            "                # timeline bound and record the change (see \"XXX: Odd behavior\"",
            "                # above).",
            "                if (",
            "                    prev_room_sync_config.timeline_limit",
            "                    < room_sync_config.timeline_limit",
            "                ):",
            "                    ignore_timeline_bound = True",
            "",
            "        log_kv(",
            "            {",
            "                \"sliding_sync.from_bound\": from_bound,",
            "                \"sliding_sync.initial\": initial,",
            "                \"sliding_sync.ignore_timeline_bound\": ignore_timeline_bound,",
            "            }",
            "        )",
            "",
            "        # Assemble the list of timeline events",
            "        #",
            "        # FIXME: It would be nice to make the `rooms` response more uniform regardless of",
            "        # membership. Currently, we have to make all of these optional because",
            "        # `invite`/`knock` rooms only have `stripped_state`. See",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1653045932",
            "        timeline_events: List[EventBase] = []",
            "        bundled_aggregations: Optional[Dict[str, BundledAggregations]] = None",
            "        limited: Optional[bool] = None",
            "        prev_batch_token: Optional[StreamToken] = None",
            "        num_live: Optional[int] = None",
            "        if (",
            "            room_sync_config.timeline_limit > 0",
            "            # No timeline for invite/knock rooms (just `stripped_state`)",
            "            and room_membership_for_user_at_to_token.membership",
            "            not in (Membership.INVITE, Membership.KNOCK)",
            "        ):",
            "            limited = False",
            "            # We want to start off using the `to_token` (vs `from_token`) because we look",
            "            # backwards from the `to_token` up to the `timeline_limit` and we might not",
            "            # reach the `from_token` before we hit the limit. We will update the room stream",
            "            # position once we've fetched the events to point to the earliest event fetched.",
            "            prev_batch_token = to_token",
            "",
            "            # We're going to paginate backwards from the `to_token`",
            "            to_bound = to_token.room_key",
            "            # People shouldn't see past their leave/ban event",
            "            if room_membership_for_user_at_to_token.membership in (",
            "                Membership.LEAVE,",
            "                Membership.BAN,",
            "            ):",
            "                to_bound = room_membership_for_user_at_to_token.event_pos.to_room_stream_token()",
            "",
            "            timeline_from_bound = from_bound",
            "            if ignore_timeline_bound:",
            "                timeline_from_bound = None",
            "",
            "            # For initial `/sync` (and other historical scenarios mentioned above), we",
            "            # want to view a historical section of the timeline; to fetch events by",
            "            # `topological_ordering` (best representation of the room DAG as others were",
            "            # seeing it at the time). This also aligns with the order that `/messages`",
            "            # returns events in.",
            "            #",
            "            # For incremental `/sync`, we want to get all updates for rooms since",
            "            # the last `/sync` (regardless if those updates arrived late or happened",
            "            # a while ago in the past); to fetch events by `stream_ordering` (in the",
            "            # order they were received by the server).",
            "            #",
            "            # Relevant spec issue: https://github.com/matrix-org/matrix-spec/issues/1917",
            "            #",
            "            # FIXME: Using workaround for mypy,",
            "            # https://github.com/python/mypy/issues/10740#issuecomment-1997047277 and",
            "            # https://github.com/python/mypy/issues/17479",
            "            paginate_room_events_by_topological_ordering: PaginateFunction = (",
            "                self.store.paginate_room_events_by_topological_ordering",
            "            )",
            "            paginate_room_events_by_stream_ordering: PaginateFunction = (",
            "                self.store.paginate_room_events_by_stream_ordering",
            "            )",
            "            pagination_method: PaginateFunction = (",
            "                # Use `topographical_ordering` for historical events",
            "                paginate_room_events_by_topological_ordering",
            "                if timeline_from_bound is None",
            "                # Use `stream_ordering` for updates",
            "                else paginate_room_events_by_stream_ordering",
            "            )",
            "            timeline_events, new_room_key, limited = await pagination_method(",
            "                room_id=room_id,",
            "                # The bounds are reversed so we can paginate backwards",
            "                # (from newer to older events) starting at to_bound.",
            "                # This ensures we fill the `limit` with the newest events first,",
            "                from_key=to_bound,",
            "                to_key=timeline_from_bound,",
            "                direction=Direction.BACKWARDS,",
            "                limit=room_sync_config.timeline_limit,",
            "            )",
            "",
            "            # We want to return the events in ascending order (the last event is the",
            "            # most recent).",
            "            timeline_events.reverse()",
            "",
            "            # Make sure we don't expose any events that the client shouldn't see",
            "            timeline_events = await filter_events_for_client(",
            "                self.storage_controllers,",
            "                user.to_string(),",
            "                timeline_events,",
            "                is_peeking=room_membership_for_user_at_to_token.membership",
            "                != Membership.JOIN,",
            "                filter_send_to_client=True,",
            "            )",
            "            # TODO: Filter out `EventTypes.CallInvite` in public rooms,",
            "            # see https://github.com/element-hq/synapse/issues/17359",
            "",
            "            # TODO: Handle timeline gaps (`get_timeline_gaps()`)",
            "",
            "            # Determine how many \"live\" events we have (events within the given token range).",
            "            #",
            "            # This is mostly useful to determine whether a given @mention event should",
            "            # make a noise or not. Clients cannot rely solely on the absence of",
            "            # `initial: true` to determine live events because if a room not in the",
            "            # sliding window bumps into the window because of an @mention it will have",
            "            # `initial: true` yet contain a single live event (with potentially other",
            "            # old events in the timeline)",
            "            num_live = 0",
            "            if from_token is not None:",
            "                for timeline_event in reversed(timeline_events):",
            "                    # This fields should be present for all persisted events",
            "                    assert timeline_event.internal_metadata.stream_ordering is not None",
            "                    assert timeline_event.internal_metadata.instance_name is not None",
            "",
            "                    persisted_position = PersistedEventPosition(",
            "                        instance_name=timeline_event.internal_metadata.instance_name,",
            "                        stream=timeline_event.internal_metadata.stream_ordering,",
            "                    )",
            "                    if persisted_position.persisted_after(",
            "                        from_token.stream_token.room_key",
            "                    ):",
            "                        num_live += 1",
            "                    else:",
            "                        # Since we're iterating over the timeline events in",
            "                        # reverse-chronological order, we can break once we hit an event",
            "                        # that's not live. In the future, we could potentially optimize",
            "                        # this more with a binary search (bisect).",
            "                        break",
            "",
            "            # If the timeline is `limited=True`, the client does not have all events",
            "            # necessary to calculate aggregations themselves.",
            "            if limited:",
            "                bundled_aggregations = (",
            "                    await self.relations_handler.get_bundled_aggregations(",
            "                        timeline_events, user.to_string()",
            "                    )",
            "                )",
            "",
            "            # Update the `prev_batch_token` to point to the position that allows us to",
            "            # keep paginating backwards from the oldest event we return in the timeline.",
            "            prev_batch_token = prev_batch_token.copy_and_replace(",
            "                StreamKeyType.ROOM, new_room_key",
            "            )",
            "",
            "        # Figure out any stripped state events for invite/knocks. This allows the",
            "        # potential joiner to identify the room.",
            "        stripped_state: List[JsonDict] = []",
            "        if room_membership_for_user_at_to_token.membership in (",
            "            Membership.INVITE,",
            "            Membership.KNOCK,",
            "        ):",
            "            # This should never happen. If someone is invited/knocked on room, then",
            "            # there should be an event for it.",
            "            assert room_membership_for_user_at_to_token.event_id is not None",
            "",
            "            invite_or_knock_event = await self.store.get_event(",
            "                room_membership_for_user_at_to_token.event_id",
            "            )",
            "",
            "            stripped_state = []",
            "            if invite_or_knock_event.membership == Membership.INVITE:",
            "                stripped_state.extend(",
            "                    invite_or_knock_event.unsigned.get(\"invite_room_state\", [])",
            "                )",
            "            elif invite_or_knock_event.membership == Membership.KNOCK:",
            "                stripped_state.extend(",
            "                    invite_or_knock_event.unsigned.get(\"knock_room_state\", [])",
            "                )",
            "",
            "            stripped_state.append(strip_event(invite_or_knock_event))",
            "",
            "        # Get the changes to current state in the token range from the",
            "        # `current_state_delta_stream` table.",
            "        #",
            "        # For incremental syncs, we can do this first to determine if something relevant",
            "        # has changed and strategically avoid fetching other costly things.",
            "        room_state_delta_id_map: MutableStateMap[str] = {}",
            "        name_event_id: Optional[str] = None",
            "        membership_changed = False",
            "        name_changed = False",
            "        avatar_changed = False",
            "        if initial:",
            "            # Check whether the room has a name set",
            "            name_state_ids = await self.get_current_state_ids_at(",
            "                room_id=room_id,",
            "                room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                state_filter=StateFilter.from_types([(EventTypes.Name, \"\")]),",
            "                to_token=to_token,",
            "            )",
            "            name_event_id = name_state_ids.get((EventTypes.Name, \"\"))",
            "        else:",
            "            assert from_bound is not None",
            "",
            "            # TODO: Limit the number of state events we're about to send down",
            "            # the room, if its too many we should change this to an",
            "            # `initial=True`?",
            "            deltas = await self.get_current_state_deltas_for_room(",
            "                room_id=room_id,",
            "                room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                from_token=from_bound,",
            "                to_token=to_token.room_key,",
            "            )",
            "            for delta in deltas:",
            "                # TODO: Handle state resets where event_id is None",
            "                if delta.event_id is not None:",
            "                    room_state_delta_id_map[(delta.event_type, delta.state_key)] = (",
            "                        delta.event_id",
            "                    )",
            "",
            "                if delta.event_type == EventTypes.Member:",
            "                    membership_changed = True",
            "                elif delta.event_type == EventTypes.Name and delta.state_key == \"\":",
            "                    name_changed = True",
            "                elif (",
            "                    delta.event_type == EventTypes.RoomAvatar and delta.state_key == \"\"",
            "                ):",
            "                    avatar_changed = True",
            "",
            "        # We only need the room summary for calculating heroes, however if we do",
            "        # fetch it then we can use it to calculate `joined_count` and",
            "        # `invited_count`.",
            "        room_membership_summary: Optional[Mapping[str, MemberSummary]] = None",
            "",
            "        # `heroes` are required if the room name is not set.",
            "        #",
            "        # Note: When you're the first one on your server to be invited to a new room",
            "        # over federation, we only have access to some stripped state in",
            "        # `event.unsigned.invite_room_state` which currently doesn't include `heroes`,",
            "        # see https://github.com/matrix-org/matrix-spec/issues/380. This means that",
            "        # clients won't be able to calculate the room name when necessary and just a",
            "        # pitfall we have to deal with until that spec issue is resolved.",
            "        hero_user_ids: List[str] = []",
            "        # TODO: Should we also check for `EventTypes.CanonicalAlias`",
            "        # (`m.room.canonical_alias`) as a fallback for the room name? see",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1671260153",
            "        #",
            "        # We need to fetch the `heroes` if the room name is not set. But we only need to",
            "        # get them on initial syncs (or the first time we send down the room) or if the",
            "        # membership has changed which may change the heroes.",
            "        if name_event_id is None and (initial or (not initial and membership_changed)):",
            "            # We need the room summary to extract the heroes from",
            "            if room_membership_for_user_at_to_token.membership != Membership.JOIN:",
            "                # TODO: Figure out how to get the membership summary for left/banned rooms",
            "                # For invite/knock rooms we don't include the information.",
            "                room_membership_summary = {}",
            "            else:",
            "                room_membership_summary = await self.store.get_room_summary(room_id)",
            "                # TODO: Reverse/rewind back to the `to_token`",
            "",
            "            hero_user_ids = extract_heroes_from_room_summary(",
            "                room_membership_summary, me=user.to_string()",
            "            )",
            "",
            "        # Fetch the membership counts for rooms we're joined to.",
            "        #",
            "        # Similarly to other metadata, we only need to calculate the member",
            "        # counts if this is an initial sync or the memberships have changed.",
            "        joined_count: Optional[int] = None",
            "        invited_count: Optional[int] = None",
            "        if (",
            "            initial or membership_changed",
            "        ) and room_membership_for_user_at_to_token.membership == Membership.JOIN:",
            "            # If we have the room summary (because we calculated heroes above)",
            "            # then we can simply pull the counts from there.",
            "            if room_membership_summary is not None:",
            "                empty_membership_summary = MemberSummary([], 0)",
            "",
            "                joined_count = room_membership_summary.get(",
            "                    Membership.JOIN, empty_membership_summary",
            "                ).count",
            "",
            "                invited_count = room_membership_summary.get(",
            "                    Membership.INVITE, empty_membership_summary",
            "                ).count",
            "            else:",
            "                member_counts = await self.store.get_member_counts(room_id)",
            "                joined_count = member_counts.get(Membership.JOIN, 0)",
            "                invited_count = member_counts.get(Membership.INVITE, 0)",
            "",
            "        # Fetch the `required_state` for the room",
            "        #",
            "        # No `required_state` for invite/knock rooms (just `stripped_state`)",
            "        #",
            "        # FIXME: It would be nice to make the `rooms` response more uniform regardless",
            "        # of membership. Currently, we have to make this optional because",
            "        # `invite`/`knock` rooms only have `stripped_state`. See",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1653045932",
            "        #",
            "        # Calculate the `StateFilter` based on the `required_state` for the room",
            "        required_state_filter = StateFilter.none()",
            "        # The requested `required_state_map` with the lazy membership expanded and",
            "        # `$ME` replaced with the user's ID. This allows us to see what membership we've",
            "        # sent down to the client in the next request.",
            "        #",
            "        # Make a copy so we can modify it. Still need to be careful to make a copy of",
            "        # the state key sets if we want to add/remove from them. We could make a deep",
            "        # copy but this saves us some work.",
            "        expanded_required_state_map = dict(room_sync_config.required_state_map)",
            "        if room_membership_for_user_at_to_token.membership not in (",
            "            Membership.INVITE,",
            "            Membership.KNOCK,",
            "        ):",
            "            # If we have a double wildcard (\"*\", \"*\") in the `required_state`, we need",
            "            # to fetch all state for the room",
            "            #",
            "            # Note: MSC3575 describes different behavior to how we're handling things",
            "            # here but since it's not wrong to return more state than requested",
            "            # (`required_state` is just the minimum requested), it doesn't matter if we",
            "            # include more than client wanted. This complexity is also under scrutiny,",
            "            # see",
            "            # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1185109050",
            "            #",
            "            # > One unique exception is when you request all state events via [\"*\", \"*\"]. When used,",
            "            # > all state events are returned by default, and additional entries FILTER OUT the returned set",
            "            # > of state events. These additional entries cannot use '*' themselves.",
            "            # > For example, [\"*\", \"*\"], [\"m.room.member\", \"@alice:example.com\"] will _exclude_ every m.room.member",
            "            # > event _except_ for @alice:example.com, and include every other state event.",
            "            # > In addition, [\"*\", \"*\"], [\"m.space.child\", \"*\"] is an error, the m.space.child filter is not",
            "            # > required as it would have been returned anyway.",
            "            # >",
            "            # > -- MSC3575 (https://github.com/matrix-org/matrix-spec-proposals/pull/3575)",
            "            if StateValues.WILDCARD in room_sync_config.required_state_map.get(",
            "                StateValues.WILDCARD, set()",
            "            ):",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_wildcard\",",
            "                    True,",
            "                )",
            "                required_state_filter = StateFilter.all()",
            "            # TODO: `StateFilter` currently doesn't support wildcard event types. We're",
            "            # currently working around this by returning all state to the client but it",
            "            # would be nice to fetch less from the database and return just what the",
            "            # client wanted.",
            "            elif (",
            "                room_sync_config.required_state_map.get(StateValues.WILDCARD)",
            "                is not None",
            "            ):",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_wildcard_event_type\",",
            "                    True,",
            "                )",
            "                required_state_filter = StateFilter.all()",
            "            else:",
            "                required_state_types: List[Tuple[str, Optional[str]]] = []",
            "                num_wild_state_keys = 0",
            "                lazy_load_room_members = False",
            "                num_others = 0",
            "                for (",
            "                    state_type,",
            "                    state_key_set,",
            "                ) in room_sync_config.required_state_map.items():",
            "                    for state_key in state_key_set:",
            "                        if state_key == StateValues.WILDCARD:",
            "                            num_wild_state_keys += 1",
            "                            # `None` is a wildcard in the `StateFilter`",
            "                            required_state_types.append((state_type, None))",
            "                        # We need to fetch all relevant people when we're lazy-loading membership",
            "                        elif (",
            "                            state_type == EventTypes.Member",
            "                            and state_key == StateValues.LAZY",
            "                        ):",
            "                            lazy_load_room_members = True",
            "                            # Everyone in the timeline is relevant",
            "                            #",
            "                            # FIXME: We probably also care about invite, ban, kick, targets, etc",
            "                            # but the spec only mentions \"senders\".",
            "                            timeline_membership: Set[str] = set()",
            "                            if timeline_events is not None:",
            "                                for timeline_event in timeline_events:",
            "                                    timeline_membership.add(timeline_event.sender)",
            "",
            "                            # Update the required state filter so we pick up the new",
            "                            # membership",
            "                            for user_id in timeline_membership:",
            "                                required_state_types.append(",
            "                                    (EventTypes.Member, user_id)",
            "                                )",
            "",
            "                            # Add an explicit entry for each user in the timeline",
            "                            #",
            "                            # Make a new set or copy of the state key set so we can",
            "                            # modify it without affecting the original",
            "                            # `required_state_map`",
            "                            expanded_required_state_map[EventTypes.Member] = (",
            "                                expanded_required_state_map.get(",
            "                                    EventTypes.Member, set()",
            "                                )",
            "                                | timeline_membership",
            "                            )",
            "                        elif state_key == StateValues.ME:",
            "                            num_others += 1",
            "                            required_state_types.append((state_type, user.to_string()))",
            "                            # Replace `$ME` with the user's ID so we can deduplicate",
            "                            # when someone requests the same state with `$ME` or with",
            "                            # their user ID.",
            "                            #",
            "                            # Make a new set or copy of the state key set so we can",
            "                            # modify it without affecting the original",
            "                            # `required_state_map`",
            "                            expanded_required_state_map[EventTypes.Member] = (",
            "                                expanded_required_state_map.get(",
            "                                    EventTypes.Member, set()",
            "                                )",
            "                                | {user.to_string()}",
            "                            )",
            "                        else:",
            "                            num_others += 1",
            "                            required_state_types.append((state_type, state_key))",
            "",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX",
            "                    + \"required_state_wildcard_state_key_count\",",
            "                    num_wild_state_keys,",
            "                )",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_lazy\",",
            "                    lazy_load_room_members,",
            "                )",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_other_count\",",
            "                    num_others,",
            "                )",
            "",
            "                required_state_filter = StateFilter.from_types(required_state_types)",
            "",
            "        # We need this base set of info for the response so let's just fetch it along",
            "        # with the `required_state` for the room",
            "        hero_room_state = [",
            "            (EventTypes.Member, hero_user_id) for hero_user_id in hero_user_ids",
            "        ]",
            "        meta_room_state = list(hero_room_state)",
            "        if initial or name_changed:",
            "            meta_room_state.append((EventTypes.Name, \"\"))",
            "        if initial or avatar_changed:",
            "            meta_room_state.append((EventTypes.RoomAvatar, \"\"))",
            "",
            "        state_filter = StateFilter.all()",
            "        if required_state_filter != StateFilter.all():",
            "            state_filter = StateFilter(",
            "                types=StateFilter.from_types(",
            "                    chain(meta_room_state, required_state_filter.to_types())",
            "                ).types,",
            "                include_others=required_state_filter.include_others,",
            "            )",
            "",
            "        # The required state map to store in the room sync config, if it has",
            "        # changed.",
            "        changed_required_state_map: Optional[Mapping[str, AbstractSet[str]]] = None",
            "",
            "        # We can return all of the state that was requested if this was the first",
            "        # time we've sent the room down this connection.",
            "        room_state: StateMap[EventBase] = {}",
            "        if initial:",
            "            room_state = await self.get_current_state_at(",
            "                room_id=room_id,",
            "                room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                state_filter=state_filter,",
            "                to_token=to_token,",
            "            )",
            "        else:",
            "            assert from_bound is not None",
            "",
            "            if prev_room_sync_config is not None:",
            "                # Check if there are any changes to the required state config",
            "                # that we need to handle.",
            "                changed_required_state_map, added_state_filter = (",
            "                    _required_state_changes(",
            "                        user.to_string(),",
            "                        prev_required_state_map=prev_room_sync_config.required_state_map,",
            "                        request_required_state_map=expanded_required_state_map,",
            "                        state_deltas=room_state_delta_id_map,",
            "                    )",
            "                )",
            "",
            "                if added_state_filter:",
            "                    # Some state entries got added, so we pull out the current",
            "                    # state for them. If we don't do this we'd only send down new deltas.",
            "                    state_ids = await self.get_current_state_ids_at(",
            "                        room_id=room_id,",
            "                        room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                        state_filter=added_state_filter,",
            "                        to_token=to_token,",
            "                    )",
            "                    room_state_delta_id_map.update(state_ids)",
            "",
            "            events = await self.store.get_events(",
            "                state_filter.filter_state(room_state_delta_id_map).values()",
            "            )",
            "            room_state = {(s.type, s.state_key): s for s in events.values()}",
            "",
            "            # If the membership changed and we have to get heroes, get the remaining",
            "            # heroes from the state",
            "            if hero_user_ids:",
            "                hero_membership_state = await self.get_current_state_at(",
            "                    room_id=room_id,",
            "                    room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                    state_filter=StateFilter.from_types(hero_room_state),",
            "                    to_token=to_token,",
            "                )",
            "                room_state.update(hero_membership_state)",
            "",
            "        required_room_state: StateMap[EventBase] = {}",
            "        if required_state_filter != StateFilter.none():",
            "            required_room_state = required_state_filter.filter_state(room_state)",
            "",
            "        # Find the room name and avatar from the state",
            "        room_name: Optional[str] = None",
            "        # TODO: Should we also check for `EventTypes.CanonicalAlias`",
            "        # (`m.room.canonical_alias`) as a fallback for the room name? see",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1671260153",
            "        name_event = room_state.get((EventTypes.Name, \"\"))",
            "        if name_event is not None:",
            "            room_name = name_event.content.get(\"name\")",
            "",
            "        room_avatar: Optional[str] = None",
            "        avatar_event = room_state.get((EventTypes.RoomAvatar, \"\"))",
            "        if avatar_event is not None:",
            "            room_avatar = avatar_event.content.get(\"url\")",
            "",
            "        # Assemble heroes: extract the info from the state we just fetched",
            "        heroes: List[SlidingSyncResult.RoomResult.StrippedHero] = []",
            "        for hero_user_id in hero_user_ids:",
            "            member_event = room_state.get((EventTypes.Member, hero_user_id))",
            "            if member_event is not None:",
            "                heroes.append(",
            "                    SlidingSyncResult.RoomResult.StrippedHero(",
            "                        user_id=hero_user_id,",
            "                        display_name=member_event.content.get(\"displayname\"),",
            "                        avatar_url=member_event.content.get(\"avatar_url\"),",
            "                    )",
            "                )",
            "",
            "        # Figure out the last bump event in the room. If the bump stamp hasn't",
            "        # changed we omit it from the response.",
            "        bump_stamp = None",
            "",
            "        always_return_bump_stamp = (",
            "            # We use the membership event position for any non-join",
            "            room_membership_for_user_at_to_token.membership != Membership.JOIN",
            "            # We didn't fetch any timeline events but we should still check for",
            "            # a bump_stamp that might be somewhere",
            "            or limited is None",
            "            # There might be a bump event somewhere before the timeline events",
            "            # that we fetched, that we didn't previously send down",
            "            or limited is True",
            "            # Always give the client some frame of reference if this is the",
            "            # first time they are seeing the room down the connection",
            "            or initial",
            "        )",
            "",
            "        # If we're joined to the room, we need to find the last bump event before the",
            "        # `to_token`",
            "        if room_membership_for_user_at_to_token.membership == Membership.JOIN:",
            "            # Try and get a bump stamp",
            "            new_bump_stamp = await self._get_bump_stamp(",
            "                room_id,",
            "                to_token,",
            "                timeline_events,",
            "                check_outside_timeline=always_return_bump_stamp,",
            "            )",
            "            if new_bump_stamp is not None:",
            "                bump_stamp = new_bump_stamp",
            "",
            "        if bump_stamp is None and always_return_bump_stamp:",
            "            # By default, just choose the membership event position for any non-join membership",
            "            bump_stamp = room_membership_for_user_at_to_token.event_pos.stream",
            "",
            "        if bump_stamp is not None and bump_stamp < 0:",
            "            # We never want to send down negative stream orderings, as you can't",
            "            # sensibly compare positive and negative stream orderings (they have",
            "            # different meanings).",
            "            #",
            "            # A negative bump stamp here can only happen if the stream ordering",
            "            # of the membership event is negative (and there are no further bump",
            "            # stamps), which can happen if the server leaves and deletes a room,",
            "            # and then rejoins it.",
            "            #",
            "            # To deal with this, we just set the bump stamp to zero, which will",
            "            # shove this room to the bottom of the list. This is OK as the",
            "            # moment a new message happens in the room it will get put into a",
            "            # sensible order again.",
            "            bump_stamp = 0",
            "",
            "        room_sync_required_state_map_to_persist: Mapping[str, AbstractSet[str]] = (",
            "            expanded_required_state_map",
            "        )",
            "        if changed_required_state_map:",
            "            room_sync_required_state_map_to_persist = changed_required_state_map",
            "",
            "        # Record the `room_sync_config` if we're `ignore_timeline_bound` (which means",
            "        # that the `timeline_limit` has increased)",
            "        unstable_expanded_timeline = False",
            "        if ignore_timeline_bound:",
            "            # FIXME: We signal the fact that we're sending down more events to",
            "            # the client by setting `unstable_expanded_timeline` to true (see",
            "            # \"XXX: Odd behavior\" above).",
            "            unstable_expanded_timeline = True",
            "",
            "            new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                timeline_limit=room_sync_config.timeline_limit,",
            "                required_state_map=room_sync_required_state_map_to_persist,",
            "            )",
            "        elif prev_room_sync_config is not None:",
            "            # If the result is `limited` then we need to record that the",
            "            # `timeline_limit` has been reduced, as when/if the client later requests",
            "            # more timeline then we have more data to send.",
            "            #",
            "            # Otherwise (when not `limited`) we don't need to record that the",
            "            # `timeline_limit` has been reduced, as the *effective* `timeline_limit`",
            "            # (i.e. the amount of timeline we have previously sent to the client) is at",
            "            # least the previous `timeline_limit`.",
            "            #",
            "            # This is to handle the case where the `timeline_limit` e.g. goes from 10 to",
            "            # 5 to 10 again (without any timeline gaps), where there's no point sending",
            "            # down the initial historical chunk events when the `timeline_limit` is",
            "            # increased as the client already has the 10 previous events. However, if",
            "            # client has a gap in the timeline (i.e. `limited` is True), then we *do*",
            "            # need to record the reduced timeline.",
            "            #",
            "            # TODO: Handle timeline gaps (`get_timeline_gaps()`) - This is separate from",
            "            # the gaps we might see on the client because a response was `limited` we're",
            "            # talking about above.",
            "            if (",
            "                limited",
            "                and prev_room_sync_config.timeline_limit",
            "                > room_sync_config.timeline_limit",
            "            ):",
            "                new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                    timeline_limit=room_sync_config.timeline_limit,",
            "                    required_state_map=room_sync_required_state_map_to_persist,",
            "                )",
            "",
            "            elif changed_required_state_map is not None:",
            "                new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                    timeline_limit=room_sync_config.timeline_limit,",
            "                    required_state_map=room_sync_required_state_map_to_persist,",
            "                )",
            "",
            "        else:",
            "            new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                timeline_limit=room_sync_config.timeline_limit,",
            "                required_state_map=room_sync_required_state_map_to_persist,",
            "            )",
            "",
            "        set_tag(SynapseTags.RESULT_PREFIX + \"initial\", initial)",
            "",
            "        return SlidingSyncResult.RoomResult(",
            "            name=room_name,",
            "            avatar=room_avatar,",
            "            heroes=heroes,",
            "            is_dm=is_dm,",
            "            initial=initial,",
            "            required_state=list(required_room_state.values()),",
            "            timeline_events=timeline_events,",
            "            bundled_aggregations=bundled_aggregations,",
            "            stripped_state=stripped_state,",
            "            prev_batch=prev_batch_token,",
            "            limited=limited,",
            "            unstable_expanded_timeline=unstable_expanded_timeline,",
            "            num_live=num_live,",
            "            bump_stamp=bump_stamp,",
            "            joined_count=joined_count,",
            "            invited_count=invited_count,",
            "            # TODO: These are just dummy values. We could potentially just remove these",
            "            # since notifications can only really be done correctly on the client anyway",
            "            # (encrypted rooms).",
            "            notification_count=0,",
            "            highlight_count=0,",
            "        )",
            "",
            "    @trace",
            "    async def _get_bump_stamp(",
            "        self,",
            "        room_id: str,",
            "        to_token: StreamToken,",
            "        timeline: List[EventBase],",
            "        check_outside_timeline: bool,",
            "    ) -> Optional[int]:",
            "        \"\"\"Get a bump stamp for the room, if we have a bump event and it has",
            "        changed.",
            "",
            "        Args:",
            "            room_id",
            "            to_token: The upper bound of token to return",
            "            timeline: The list of events we have fetched.",
            "            limited: If the timeline was limited.",
            "            check_outside_timeline: Whether we need to check for bump stamp for",
            "                events before the timeline if we didn't find a bump stamp in",
            "                the timeline events.",
            "        \"\"\"",
            "",
            "        # First check the timeline events we're returning to see if one of",
            "        # those matches. We iterate backwards and take the stream ordering",
            "        # of the first event that matches the bump event types.",
            "        for timeline_event in reversed(timeline):",
            "            if timeline_event.type in SLIDING_SYNC_DEFAULT_BUMP_EVENT_TYPES:",
            "                new_bump_stamp = timeline_event.internal_metadata.stream_ordering",
            "",
            "                # All persisted events have a stream ordering",
            "                assert new_bump_stamp is not None",
            "",
            "                # If we've just joined a remote room, then the last bump event may",
            "                # have been backfilled (and so have a negative stream ordering).",
            "                # These negative stream orderings can't sensibly be compared, so",
            "                # instead we use the membership event position.",
            "                if new_bump_stamp > 0:",
            "                    return new_bump_stamp",
            "",
            "        if not check_outside_timeline:",
            "            # If we are not a limited sync, then we know the bump stamp can't",
            "            # have changed.",
            "            return None",
            "",
            "        # We can quickly query for the latest bump event in the room using the",
            "        # sliding sync tables.",
            "        latest_room_bump_stamp = await self.store.get_latest_bump_stamp_for_room(",
            "            room_id",
            "        )",
            "",
            "        min_to_token_position = to_token.room_key.stream",
            "",
            "        # If we can rely on the new sliding sync tables and the `bump_stamp` is",
            "        # `None`, just fallback to the membership event position. This can happen",
            "        # when we've just joined a remote room and all the events are backfilled.",
            "        if (",
            "            # FIXME: The background job check can be removed once we bump",
            "            # `SCHEMA_COMPAT_VERSION` and run the foreground update for",
            "            # `sliding_sync_joined_rooms`/`sliding_sync_membership_snapshots`",
            "            # (tracked by https://github.com/element-hq/synapse/issues/17623)",
            "            latest_room_bump_stamp is None",
            "            and await self.store.have_finished_sliding_sync_background_jobs()",
            "        ):",
            "            return None",
            "",
            "        # The `bump_stamp` stored in the database might be ahead of our token. Since",
            "        # `bump_stamp` is only a `stream_ordering` position, we can't be 100% sure",
            "        # that's before the `to_token` in all scenarios. The only scenario we can be",
            "        # sure of is if the `bump_stamp` is totally before the minimum position from",
            "        # the token.",
            "        #",
            "        # We don't need to check if the background update has finished, as if the",
            "        # returned bump stamp is not None then it must be up to date.",
            "        elif (",
            "            latest_room_bump_stamp is not None",
            "            and latest_room_bump_stamp < min_to_token_position",
            "        ):",
            "            if latest_room_bump_stamp > 0:",
            "                return latest_room_bump_stamp",
            "            else:",
            "                return None",
            "",
            "        # Otherwise, if it's within or after the `to_token`, we need to find the",
            "        # last bump event before the `to_token`.",
            "        else:",
            "            last_bump_event_result = (",
            "                await self.store.get_last_event_pos_in_room_before_stream_ordering(",
            "                    room_id,",
            "                    to_token.room_key,",
            "                    event_types=SLIDING_SYNC_DEFAULT_BUMP_EVENT_TYPES,",
            "                )",
            "            )",
            "            if last_bump_event_result is not None:",
            "                _, new_bump_event_pos = last_bump_event_result",
            "",
            "                # If we've just joined a remote room, then the last bump event may",
            "                # have been backfilled (and so have a negative stream ordering).",
            "                # These negative stream orderings can't sensibly be compared, so",
            "                # instead we use the membership event position.",
            "                if new_bump_event_pos.stream > 0:",
            "                    return new_bump_event_pos.stream",
            "",
            "            return None",
            "",
            "",
            "def _required_state_changes(",
            "    user_id: str,",
            "    *,",
            "    prev_required_state_map: Mapping[str, AbstractSet[str]],",
            "    request_required_state_map: Mapping[str, AbstractSet[str]],",
            "    state_deltas: StateMap[str],",
            ") -> Tuple[Optional[Mapping[str, AbstractSet[str]]], StateFilter]:",
            "    \"\"\"Calculates the changes between the required state room config from the",
            "    previous requests compared with the current request.",
            "",
            "    This does two things. First, it calculates if we need to update the room",
            "    config due to changes to required state. Secondly, it works out which state",
            "    entries we need to pull from current state and return due to the state entry",
            "    now appearing in the required state when it previously wasn't (on top of the",
            "    state deltas).",
            "",
            "    This function tries to ensure to handle the case where a state entry is",
            "    added, removed and then added again to the required state. In that case we",
            "    only want to re-send that entry down sync if it has changed.",
            "",
            "    Returns:",
            "        A 2-tuple of updated required state config (or None if there is no update)",
            "        and the state filter to use to fetch extra current state that we need to",
            "        return.",
            "    \"\"\"",
            "    if prev_required_state_map == request_required_state_map:",
            "        # There has been no change. Return immediately.",
            "        return None, StateFilter.none()",
            "",
            "    prev_wildcard = prev_required_state_map.get(StateValues.WILDCARD, set())",
            "    request_wildcard = request_required_state_map.get(StateValues.WILDCARD, set())",
            "",
            "    # If we were previously fetching everything (\"*\", \"*\"), always update the effective",
            "    # room required state config to match the request. And since we we're previously",
            "    # already fetching everything, we don't have to fetch anything now that they've",
            "    # narrowed.",
            "    if StateValues.WILDCARD in prev_wildcard:",
            "        return request_required_state_map, StateFilter.none()",
            "",
            "    # If a event type wildcard has been added or removed we don't try and do",
            "    # anything fancy, and instead always update the effective room required",
            "    # state config to match the request.",
            "    if request_wildcard - prev_wildcard:",
            "        # Some keys were added, so we need to fetch everything",
            "        return request_required_state_map, StateFilter.all()",
            "    if prev_wildcard - request_wildcard:",
            "        # Keys were only removed, so we don't have to fetch everything.",
            "        return request_required_state_map, StateFilter.none()",
            "",
            "    # Contains updates to the required state map compared with the previous room",
            "    # config. This has the same format as `RoomSyncConfig.required_state`",
            "    changes: Dict[str, AbstractSet[str]] = {}",
            "",
            "    # The set of types/state keys that we need to fetch and return to the",
            "    # client. Passed to `StateFilter.from_types(...)`",
            "    added: List[Tuple[str, Optional[str]]] = []",
            "",
            "    # Convert the list of state deltas to map from type to state_keys that have",
            "    # changed.",
            "    changed_types_to_state_keys: Dict[str, Set[str]] = {}",
            "    for event_type, state_key in state_deltas:",
            "        changed_types_to_state_keys.setdefault(event_type, set()).add(state_key)",
            "",
            "    # First we calculate what, if anything, has been *added*.",
            "    for event_type in (",
            "        prev_required_state_map.keys() | request_required_state_map.keys()",
            "    ):",
            "        old_state_keys = prev_required_state_map.get(event_type, set())",
            "        request_state_keys = request_required_state_map.get(event_type, set())",
            "        changed_state_keys = changed_types_to_state_keys.get(event_type, set())",
            "",
            "        if old_state_keys == request_state_keys:",
            "            # No change to this type",
            "            continue",
            "",
            "        if not request_state_keys - old_state_keys:",
            "            # Nothing *added*, so we skip. Removals happen below.",
            "            continue",
            "",
            "        # We only remove state keys from the effective state if they've been",
            "        # removed from the request *and* the state has changed. This ensures",
            "        # that if a client removes and then re-adds a state key, we only send",
            "        # down the associated current state event if its changed (rather than",
            "        # sending down the same event twice).",
            "        invalidated_state_keys = (",
            "            old_state_keys - request_state_keys",
            "        ) & changed_state_keys",
            "",
            "        # Figure out which state keys we should remember sending down the connection",
            "        inheritable_previous_state_keys = (",
            "            # Retain the previous state_keys that we've sent down before.",
            "            # Wildcard and lazy state keys are not sticky from previous requests.",
            "            (old_state_keys - {StateValues.WILDCARD, StateValues.LAZY})",
            "            - invalidated_state_keys",
            "        )",
            "",
            "        # Always update changes to include the newly added keys (we've expanded the set",
            "        # of state keys), use the new requested set with whatever hasn't been",
            "        # invalidated from the previous set.",
            "        changes[event_type] = request_state_keys | inheritable_previous_state_keys",
            "        # Limit the number of state_keys we should remember sending down the connection",
            "        # for each (room_id, user_id). We don't want to store and pull out too much data",
            "        # in the database. This is a happy-medium between remembering nothing and",
            "        # everything. We can avoid sending redundant state down the connection most of",
            "        # the time given that most rooms don't have 100 members anyway and it takes a",
            "        # while to cycle through 100 members.",
            "        #",
            "        # Only remember up to (MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER)",
            "        if len(changes[event_type]) > MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER:",
            "            # Reset back to only the requested state keys",
            "            changes[event_type] = request_state_keys",
            "",
            "            # Skip if there isn't any room to fill in the rest with previous state keys",
            "            if len(request_state_keys) < MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER:",
            "                # Fill the rest with previous state_keys. Ideally, we could sort",
            "                # these by recency but it's just a set so just pick an arbitrary",
            "                # subset (good enough).",
            "                changes[event_type] = changes[event_type] | set(",
            "                    itertools.islice(",
            "                        inheritable_previous_state_keys,",
            "                        # Just taking the difference isn't perfect as there could be",
            "                        # overlap in the keys between the requested and previous but we",
            "                        # will decide to just take the easy route for now and avoid",
            "                        # additional set operations to figure it out.",
            "                        MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER",
            "                        - len(request_state_keys),",
            "                    )",
            "                )",
            "",
            "        if StateValues.WILDCARD in old_state_keys:",
            "            # We were previously fetching everything for this type, so we don't need to",
            "            # fetch anything new.",
            "            continue",
            "",
            "        # Record the new state keys to fetch for this type.",
            "        if StateValues.WILDCARD in request_state_keys:",
            "            # If we have added a wildcard then we always just fetch everything.",
            "            added.append((event_type, None))",
            "        else:",
            "            for state_key in request_state_keys - old_state_keys:",
            "                if state_key == StateValues.ME:",
            "                    added.append((event_type, user_id))",
            "                elif state_key == StateValues.LAZY:",
            "                    # We handle lazy loading separately (outside this function),",
            "                    # so don't need to explicitly add anything here.",
            "                    #",
            "                    # LAZY values should also be ignore for event types that are",
            "                    # not membership.",
            "                    pass",
            "                else:",
            "                    added.append((event_type, state_key))",
            "",
            "    added_state_filter = StateFilter.from_types(added)",
            "",
            "    # Figure out what changes we need to apply to the effective required state",
            "    # config.",
            "    for event_type, changed_state_keys in changed_types_to_state_keys.items():",
            "        old_state_keys = prev_required_state_map.get(event_type, set())",
            "        request_state_keys = request_required_state_map.get(event_type, set())",
            "",
            "        if old_state_keys == request_state_keys:",
            "            # No change.",
            "            continue",
            "",
            "        # If we see the `user_id` as a state_key, also add \"$ME\" to the list of state",
            "        # that has changed to account for people requesting `required_state` with `$ME`",
            "        # or their user ID.",
            "        if user_id in changed_state_keys:",
            "            changed_state_keys.add(StateValues.ME)",
            "",
            "        # We only remove state keys from the effective state if they've been",
            "        # removed from the request *and* the state has changed. This ensures",
            "        # that if a client removes and then re-adds a state key, we only send",
            "        # down the associated current state event if its changed (rather than",
            "        # sending down the same event twice).",
            "        invalidated_state_keys = (",
            "            old_state_keys - request_state_keys",
            "        ) & changed_state_keys",
            "",
            "        # We've expanded the set of state keys, ... (already handled above)",
            "        if request_state_keys - old_state_keys:",
            "            continue",
            "",
            "        old_state_key_wildcard = StateValues.WILDCARD in old_state_keys",
            "        request_state_key_wildcard = StateValues.WILDCARD in request_state_keys",
            "",
            "        if old_state_key_wildcard != request_state_key_wildcard:",
            "            # If a state_key wildcard has been added or removed, we always update the",
            "            # effective room required state config to match the request.",
            "            changes[event_type] = request_state_keys",
            "            continue",
            "",
            "        if event_type == EventTypes.Member:",
            "            old_state_key_lazy = StateValues.LAZY in old_state_keys",
            "            request_state_key_lazy = StateValues.LAZY in request_state_keys",
            "",
            "            if old_state_key_lazy != request_state_key_lazy:",
            "                # If a \"$LAZY\" has been added or removed we always update the effective room",
            "                # required state config to match the request.",
            "                changes[event_type] = request_state_keys",
            "                continue",
            "",
            "        # At this point there are no wildcards and no additions to the set of",
            "        # state keys requested, only deletions.",
            "        #",
            "        # We only remove state keys from the effective state if they've been",
            "        # removed from the request *and* the state has changed. This ensures",
            "        # that if a client removes and then re-adds a state key, we only send",
            "        # down the associated current state event if its changed (rather than",
            "        # sending down the same event twice).",
            "        if invalidated_state_keys:",
            "            changes[event_type] = old_state_keys - invalidated_state_keys",
            "",
            "    if changes:",
            "        # Update the required state config based on the changes.",
            "        new_required_state_map = dict(prev_required_state_map)",
            "        for event_type, state_keys in changes.items():",
            "            if state_keys:",
            "                new_required_state_map[event_type] = state_keys",
            "            else:",
            "                # Remove entries with empty state keys.",
            "                new_required_state_map.pop(event_type, None)",
            "",
            "        return new_required_state_map, added_state_filter",
            "    else:",
            "        return None, added_state_filter"
        ],
        "afterPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "",
            "import itertools",
            "import logging",
            "from itertools import chain",
            "from typing import TYPE_CHECKING, AbstractSet, Dict, List, Mapping, Optional, Set, Tuple",
            "",
            "from prometheus_client import Histogram",
            "from typing_extensions import assert_never",
            "",
            "from synapse.api.constants import Direction, EventTypes, Membership",
            "from synapse.events import EventBase",
            "from synapse.events.utils import strip_event",
            "from synapse.handlers.relations import BundledAggregations",
            "from synapse.handlers.sliding_sync.extensions import SlidingSyncExtensionHandler",
            "from synapse.handlers.sliding_sync.room_lists import (",
            "    RoomsForUserType,",
            "    SlidingSyncRoomLists,",
            ")",
            "from synapse.handlers.sliding_sync.store import SlidingSyncConnectionStore",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    log_kv,",
            "    set_tag,",
            "    start_active_span,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.storage.databases.main.roommember import extract_heroes_from_room_summary",
            "from synapse.storage.databases.main.state_deltas import StateDelta",
            "from synapse.storage.databases.main.stream import PaginateFunction",
            "from synapse.storage.roommember import (",
            "    MemberSummary,",
            ")",
            "from synapse.types import (",
            "    JsonDict,",
            "    MutableStateMap,",
            "    PersistedEventPosition,",
            "    Requester,",
            "    RoomStreamToken,",
            "    SlidingSyncStreamToken,",
            "    StateMap,",
            "    StrCollection,",
            "    StreamKeyType,",
            "    StreamToken,",
            ")",
            "from synapse.types.handlers import SLIDING_SYNC_DEFAULT_BUMP_EVENT_TYPES",
            "from synapse.types.handlers.sliding_sync import (",
            "    HaveSentRoomFlag,",
            "    MutablePerConnectionState,",
            "    PerConnectionState,",
            "    RoomSyncConfig,",
            "    SlidingSyncConfig,",
            "    SlidingSyncResult,",
            "    StateValues,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import concurrently_execute",
            "from synapse.visibility import filter_events_for_client",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "sync_processing_time = Histogram(",
            "    \"synapse_sliding_sync_processing_time\",",
            "    \"Time taken to generate a sliding sync response, ignoring wait times.\",",
            "    [\"initial\"],",
            ")",
            "",
            "# Limit the number of state_keys we should remember sending down the connection for each",
            "# (room_id, user_id). We don't want to store and pull out too much data in the database.",
            "#",
            "# 100 is an arbitrary but small-ish number. The idea is that we probably won't send down",
            "# too many redundant member state events (that the client already knows about) for a",
            "# given ongoing conversation if we keep 100 around. Most rooms don't have 100 members",
            "# anyway and it takes a while to cycle through 100 members.",
            "MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER = 100",
            "",
            "",
            "class SlidingSyncHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.clock = hs.get_clock()",
            "        self.store = hs.get_datastores().main",
            "        self.storage_controllers = hs.get_storage_controllers()",
            "        self.auth_blocking = hs.get_auth_blocking()",
            "        self.notifier = hs.get_notifier()",
            "        self.event_sources = hs.get_event_sources()",
            "        self.relations_handler = hs.get_relations_handler()",
            "        self.rooms_to_exclude_globally = hs.config.server.rooms_to_exclude_from_sync",
            "        self.is_mine_id = hs.is_mine_id",
            "",
            "        self.connection_store = SlidingSyncConnectionStore(self.store)",
            "        self.extensions = SlidingSyncExtensionHandler(hs)",
            "        self.room_lists = SlidingSyncRoomLists(hs)",
            "",
            "    async def wait_for_sync_for_user(",
            "        self,",
            "        requester: Requester,",
            "        sync_config: SlidingSyncConfig,",
            "        from_token: Optional[SlidingSyncStreamToken] = None,",
            "        timeout_ms: int = 0,",
            "    ) -> SlidingSyncResult:",
            "        \"\"\"",
            "        Get the sync for a client if we have new data for it now. Otherwise",
            "        wait for new data to arrive on the server. If the timeout expires, then",
            "        return an empty sync result.",
            "",
            "        Args:",
            "            requester: The user making the request",
            "            sync_config: Sync configuration",
            "            from_token: The point in the stream to sync from. Token of the end of the",
            "                previous batch. May be `None` if this is the initial sync request.",
            "            timeout_ms: The time in milliseconds to wait for new data to arrive. If 0,",
            "                we will immediately but there might not be any new data so we just return an",
            "                empty response.",
            "        \"\"\"",
            "        # If the user is not part of the mau group, then check that limits have",
            "        # not been exceeded (if not part of the group by this point, almost certain",
            "        # auth_blocking will occur)",
            "        await self.auth_blocking.check_auth_blocking(requester=requester)",
            "",
            "        # If we're working with a user-provided token, we need to make sure to wait for",
            "        # this worker to catch up with the token so we don't skip past any incoming",
            "        # events or future events if the user is nefariously, manually modifying the",
            "        # token.",
            "        if from_token is not None:",
            "            # We need to make sure this worker has caught up with the token. If",
            "            # this returns false, it means we timed out waiting, and we should",
            "            # just return an empty response.",
            "            before_wait_ts = self.clock.time_msec()",
            "            if not await self.notifier.wait_for_stream_token(from_token.stream_token):",
            "                logger.warning(",
            "                    \"Timed out waiting for worker to catch up. Returning empty response\"",
            "                )",
            "                return SlidingSyncResult.empty(from_token)",
            "",
            "            # If we've spent significant time waiting to catch up, take it off",
            "            # the timeout.",
            "            after_wait_ts = self.clock.time_msec()",
            "            if after_wait_ts - before_wait_ts > 1_000:",
            "                timeout_ms -= after_wait_ts - before_wait_ts",
            "                timeout_ms = max(timeout_ms, 0)",
            "",
            "        # We're going to respond immediately if the timeout is 0 or if this is an",
            "        # initial sync (without a `from_token`) so we can avoid calling",
            "        # `notifier.wait_for_events()`.",
            "        if timeout_ms == 0 or from_token is None:",
            "            now_token = self.event_sources.get_current_token()",
            "            result = await self.current_sync_for_user(",
            "                sync_config,",
            "                from_token=from_token,",
            "                to_token=now_token,",
            "            )",
            "        else:",
            "            # Otherwise, we wait for something to happen and report it to the user.",
            "            async def current_sync_callback(",
            "                before_token: StreamToken, after_token: StreamToken",
            "            ) -> SlidingSyncResult:",
            "                return await self.current_sync_for_user(",
            "                    sync_config,",
            "                    from_token=from_token,",
            "                    to_token=after_token,",
            "                )",
            "",
            "            result = await self.notifier.wait_for_events(",
            "                sync_config.user.to_string(),",
            "                timeout_ms,",
            "                current_sync_callback,",
            "                from_token=from_token.stream_token,",
            "            )",
            "",
            "        return result",
            "",
            "    @trace",
            "    async def current_sync_for_user(",
            "        self,",
            "        sync_config: SlidingSyncConfig,",
            "        to_token: StreamToken,",
            "        from_token: Optional[SlidingSyncStreamToken] = None,",
            "    ) -> SlidingSyncResult:",
            "        \"\"\"",
            "        Generates the response body of a Sliding Sync result, represented as a",
            "        `SlidingSyncResult`.",
            "",
            "        We fetch data according to the token range (> `from_token` and <= `to_token`).",
            "",
            "        Args:",
            "            sync_config: Sync configuration",
            "            to_token: The point in the stream to sync up to.",
            "            from_token: The point in the stream to sync from. Token of the end of the",
            "                previous batch. May be `None` if this is the initial sync request.",
            "        \"\"\"",
            "        start_time_s = self.clock.time()",
            "",
            "        user_id = sync_config.user.to_string()",
            "        app_service = self.store.get_app_service_by_user_id(user_id)",
            "        if app_service:",
            "            # We no longer support AS users using /sync directly.",
            "            # See https://github.com/matrix-org/matrix-doc/issues/1144",
            "            raise NotImplementedError()",
            "",
            "        # Get the per-connection state (if any).",
            "        #",
            "        # Raises an exception if there is a `connection_position` that we don't",
            "        # recognize. If we don't do this and the client asks for the full range",
            "        # of rooms, we end up sending down all rooms and their state from",
            "        # scratch (which can be very slow). By expiring the connection we allow",
            "        # the client a chance to do an initial request with a smaller range of",
            "        # rooms to get them some results sooner but will end up taking the same",
            "        # amount of time (more with round-trips and re-processing) in the end to",
            "        # get everything again.",
            "        previous_connection_state = (",
            "            await self.connection_store.get_and_clear_connection_positions(",
            "                sync_config, from_token",
            "            )",
            "        )",
            "",
            "        # Get all of the room IDs that the user should be able to see in the sync",
            "        # response",
            "        has_lists = sync_config.lists is not None and len(sync_config.lists) > 0",
            "        has_room_subscriptions = (",
            "            sync_config.room_subscriptions is not None",
            "            and len(sync_config.room_subscriptions) > 0",
            "        )",
            "",
            "        interested_rooms = await self.room_lists.compute_interested_rooms(",
            "            sync_config=sync_config,",
            "            previous_connection_state=previous_connection_state,",
            "            from_token=from_token.stream_token if from_token else None,",
            "            to_token=to_token,",
            "        )",
            "",
            "        lists = interested_rooms.lists",
            "        relevant_room_map = interested_rooms.relevant_room_map",
            "        all_rooms = interested_rooms.all_rooms",
            "        room_membership_for_user_map = interested_rooms.room_membership_for_user_map",
            "        relevant_rooms_to_send_map = interested_rooms.relevant_rooms_to_send_map",
            "",
            "        # Fetch room data",
            "        rooms: Dict[str, SlidingSyncResult.RoomResult] = {}",
            "",
            "        new_connection_state = previous_connection_state.get_mutable()",
            "",
            "        @trace",
            "        @tag_args",
            "        async def handle_room(room_id: str) -> None:",
            "            room_sync_result = await self.get_room_sync_data(",
            "                sync_config=sync_config,",
            "                previous_connection_state=previous_connection_state,",
            "                new_connection_state=new_connection_state,",
            "                room_id=room_id,",
            "                room_sync_config=relevant_rooms_to_send_map[room_id],",
            "                room_membership_for_user_at_to_token=room_membership_for_user_map[",
            "                    room_id",
            "                ],",
            "                from_token=from_token,",
            "                to_token=to_token,",
            "                newly_joined=room_id in interested_rooms.newly_joined_rooms,",
            "                is_dm=room_id in interested_rooms.dm_room_ids,",
            "            )",
            "",
            "            # Filter out empty room results during incremental sync",
            "            if room_sync_result or not from_token:",
            "                rooms[room_id] = room_sync_result",
            "",
            "        if relevant_rooms_to_send_map:",
            "            with start_active_span(\"sliding_sync.generate_room_entries\"):",
            "                await concurrently_execute(handle_room, relevant_rooms_to_send_map, 20)",
            "",
            "        extensions = await self.extensions.get_extensions_response(",
            "            sync_config=sync_config,",
            "            actual_lists=lists,",
            "            previous_connection_state=previous_connection_state,",
            "            new_connection_state=new_connection_state,",
            "            # We're purposely using `relevant_room_map` instead of",
            "            # `relevant_rooms_to_send_map` here. This needs to be all room_ids we could",
            "            # send regardless of whether they have an event update or not. The",
            "            # extensions care about more than just normal events in the rooms (like",
            "            # account data, read receipts, typing indicators, to-device messages, etc).",
            "            actual_room_ids=set(relevant_room_map.keys()),",
            "            actual_room_response_map=rooms,",
            "            from_token=from_token,",
            "            to_token=to_token,",
            "        )",
            "",
            "        if has_lists or has_room_subscriptions:",
            "            # We now calculate if any rooms outside the range have had updates,",
            "            # which we are not sending down.",
            "            #",
            "            # We *must* record rooms that have had updates, but it is also fine",
            "            # to record rooms as having updates even if there might not actually",
            "            # be anything new for the user (e.g. due to event filters, events",
            "            # having happened after the user left, etc).",
            "            if from_token:",
            "                # The set of rooms that the client (may) care about, but aren't",
            "                # in any list range (or subscribed to).",
            "                missing_rooms = all_rooms - relevant_room_map.keys()",
            "",
            "                # We now just go and try fetching any events in the above rooms",
            "                # to see if anything has happened since the `from_token`.",
            "                #",
            "                # TODO: Replace this with something faster. When we land the",
            "                # sliding sync tables that record the most recent event",
            "                # positions we can use that.",
            "                unsent_room_ids: StrCollection",
            "                if await self.store.have_finished_sliding_sync_background_jobs():",
            "                    unsent_room_ids = await (",
            "                        self.store.get_rooms_that_have_updates_since_sliding_sync_table(",
            "                            room_ids=missing_rooms,",
            "                            from_key=from_token.stream_token.room_key,",
            "                        )",
            "                    )",
            "                else:",
            "                    missing_event_map_by_room = (",
            "                        await self.store.get_room_events_stream_for_rooms(",
            "                            room_ids=missing_rooms,",
            "                            from_key=to_token.room_key,",
            "                            to_key=from_token.stream_token.room_key,",
            "                            limit=1,",
            "                        )",
            "                    )",
            "                    unsent_room_ids = list(missing_event_map_by_room)",
            "",
            "                new_connection_state.rooms.record_unsent_rooms(",
            "                    unsent_room_ids, from_token.stream_token.room_key",
            "                )",
            "",
            "            new_connection_state.rooms.record_sent_rooms(",
            "                relevant_rooms_to_send_map.keys()",
            "            )",
            "",
            "            connection_position = await self.connection_store.record_new_state(",
            "                sync_config=sync_config,",
            "                from_token=from_token,",
            "                new_connection_state=new_connection_state,",
            "            )",
            "        elif from_token:",
            "            connection_position = from_token.connection_position",
            "        else:",
            "            # Initial sync without a `from_token` starts at `0`",
            "            connection_position = 0",
            "",
            "        sliding_sync_result = SlidingSyncResult(",
            "            next_pos=SlidingSyncStreamToken(to_token, connection_position),",
            "            lists=lists,",
            "            rooms=rooms,",
            "            extensions=extensions,",
            "        )",
            "",
            "        # Make it easy to find traces for syncs that aren't empty",
            "        set_tag(SynapseTags.RESULT_PREFIX + \"result\", bool(sliding_sync_result))",
            "        set_tag(SynapseTags.FUNC_ARG_PREFIX + \"sync_config.user\", user_id)",
            "",
            "        end_time_s = self.clock.time()",
            "        sync_processing_time.labels(from_token is not None).observe(",
            "            end_time_s - start_time_s",
            "        )",
            "",
            "        return sliding_sync_result",
            "",
            "    @trace",
            "    async def get_current_state_ids_at(",
            "        self,",
            "        room_id: str,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        state_filter: StateFilter,",
            "        to_token: StreamToken,",
            "    ) -> StateMap[str]:",
            "        \"\"\"",
            "        Get current state IDs for the user in the room according to their membership. This",
            "        will be the current state at the time of their LEAVE/BAN, otherwise will be the",
            "        current state <= to_token.",
            "",
            "        Args:",
            "            room_id: The room ID to fetch data for",
            "            room_membership_for_user_at_token: Membership information for the user",
            "                in the room at the time of `to_token`.",
            "            to_token: The point in the stream to sync up to.",
            "        \"\"\"",
            "        state_ids: StateMap[str]",
            "        # People shouldn't see past their leave/ban event",
            "        if room_membership_for_user_at_to_token.membership in (",
            "            Membership.LEAVE,",
            "            Membership.BAN,",
            "        ):",
            "            # TODO: `get_state_ids_at(...)` doesn't take into account the \"current",
            "            # state\". Maybe we need to use",
            "            # `get_forward_extremities_for_room_at_stream_ordering(...)` to \"Fetch the",
            "            # current state at the time.\"",
            "            state_ids = await self.storage_controllers.state.get_state_ids_at(",
            "                room_id,",
            "                stream_position=to_token.copy_and_replace(",
            "                    StreamKeyType.ROOM,",
            "                    room_membership_for_user_at_to_token.event_pos.to_room_stream_token(),",
            "                ),",
            "                state_filter=state_filter,",
            "                # Partially-stated rooms should have all state events except for",
            "                # remote membership events. Since we've already excluded",
            "                # partially-stated rooms unless `required_state` only has",
            "                # `[\"m.room.member\", \"$LAZY\"]` for membership, we should be able to",
            "                # retrieve everything requested. When we're lazy-loading, if there",
            "                # are some remote senders in the timeline, we should also have their",
            "                # membership event because we had to auth that timeline event. Plus",
            "                # we don't want to block the whole sync waiting for this one room.",
            "                await_full_state=False,",
            "            )",
            "        # Otherwise, we can get the latest current state in the room",
            "        else:",
            "            state_ids = await self.storage_controllers.state.get_current_state_ids(",
            "                room_id,",
            "                state_filter,",
            "                # Partially-stated rooms should have all state events except for",
            "                # remote membership events. Since we've already excluded",
            "                # partially-stated rooms unless `required_state` only has",
            "                # `[\"m.room.member\", \"$LAZY\"]` for membership, we should be able to",
            "                # retrieve everything requested. When we're lazy-loading, if there",
            "                # are some remote senders in the timeline, we should also have their",
            "                # membership event because we had to auth that timeline event. Plus",
            "                # we don't want to block the whole sync waiting for this one room.",
            "                await_full_state=False,",
            "            )",
            "            # TODO: Query `current_state_delta_stream` and reverse/rewind back to the `to_token`",
            "",
            "        return state_ids",
            "",
            "    @trace",
            "    async def get_current_state_at(",
            "        self,",
            "        room_id: str,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        state_filter: StateFilter,",
            "        to_token: StreamToken,",
            "    ) -> StateMap[EventBase]:",
            "        \"\"\"",
            "        Get current state for the user in the room according to their membership. This",
            "        will be the current state at the time of their LEAVE/BAN, otherwise will be the",
            "        current state <= to_token.",
            "",
            "        Args:",
            "            room_id: The room ID to fetch data for",
            "            room_membership_for_user_at_token: Membership information for the user",
            "                in the room at the time of `to_token`.",
            "            to_token: The point in the stream to sync up to.",
            "        \"\"\"",
            "        state_ids = await self.get_current_state_ids_at(",
            "            room_id=room_id,",
            "            room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "            state_filter=state_filter,",
            "            to_token=to_token,",
            "        )",
            "",
            "        events = await self.store.get_events_as_list(list(state_ids.values()))",
            "",
            "        state_map = {}",
            "        for event in events:",
            "            state_map[(event.type, event.state_key)] = event",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    async def get_current_state_deltas_for_room(",
            "        self,",
            "        room_id: str,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        from_token: RoomStreamToken,",
            "        to_token: RoomStreamToken,",
            "    ) -> List[StateDelta]:",
            "        \"\"\"",
            "        Get the state deltas between two tokens taking into account the user's",
            "        membership. If the user is LEAVE/BAN, we will only get the state deltas up to",
            "        their LEAVE/BAN event (inclusive).",
            "",
            "        (> `from_token` and <= `to_token`)",
            "        \"\"\"",
            "        membership = room_membership_for_user_at_to_token.membership",
            "        # We don't know how to handle `membership` values other than these. The",
            "        # code below would need to be updated.",
            "        assert membership in (",
            "            Membership.JOIN,",
            "            Membership.INVITE,",
            "            Membership.KNOCK,",
            "            Membership.LEAVE,",
            "            Membership.BAN,",
            "        )",
            "",
            "        # People shouldn't see past their leave/ban event",
            "        if membership in (",
            "            Membership.LEAVE,",
            "            Membership.BAN,",
            "        ):",
            "            to_bound = (",
            "                room_membership_for_user_at_to_token.event_pos.to_room_stream_token()",
            "            )",
            "        # If we are participating in the room, we can get the latest current state in",
            "        # the room",
            "        elif membership == Membership.JOIN:",
            "            to_bound = to_token",
            "        # We can only rely on the stripped state included in the invite/knock event",
            "        # itself so there will never be any state deltas to send down.",
            "        elif membership in (Membership.INVITE, Membership.KNOCK):",
            "            return []",
            "        else:",
            "            # We don't know how to handle this type of membership yet",
            "            #",
            "            # FIXME: We should use `assert_never` here but for some reason",
            "            # the exhaustive matching doesn't recognize the `Never` here.",
            "            # assert_never(membership)",
            "            raise AssertionError(",
            "                f\"Unexpected membership {membership} that we don't know how to handle yet\"",
            "            )",
            "",
            "        return await self.store.get_current_state_deltas_for_room(",
            "            room_id=room_id,",
            "            from_token=from_token,",
            "            to_token=to_bound,",
            "        )",
            "",
            "    @trace",
            "    async def get_room_sync_data(",
            "        self,",
            "        sync_config: SlidingSyncConfig,",
            "        previous_connection_state: \"PerConnectionState\",",
            "        new_connection_state: \"MutablePerConnectionState\",",
            "        room_id: str,",
            "        room_sync_config: RoomSyncConfig,",
            "        room_membership_for_user_at_to_token: RoomsForUserType,",
            "        from_token: Optional[SlidingSyncStreamToken],",
            "        to_token: StreamToken,",
            "        newly_joined: bool,",
            "        is_dm: bool,",
            "    ) -> SlidingSyncResult.RoomResult:",
            "        \"\"\"",
            "        Fetch room data for the sync response.",
            "",
            "        We fetch data according to the token range (> `from_token` and <= `to_token`).",
            "",
            "        Args:",
            "            user: User to fetch data for",
            "            room_id: The room ID to fetch data for",
            "            room_sync_config: Config for what data we should fetch for a room in the",
            "                sync response.",
            "            room_membership_for_user_at_to_token: Membership information for the user",
            "                in the room at the time of `to_token`.",
            "            from_token: The point in the stream to sync from.",
            "            to_token: The point in the stream to sync up to.",
            "            newly_joined: If the user has newly joined the room",
            "            is_dm: Whether the room is a DM room",
            "        \"\"\"",
            "        user = sync_config.user",
            "",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"membership\",",
            "            room_membership_for_user_at_to_token.membership,",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"timeline_limit\",",
            "            room_sync_config.timeline_limit,",
            "        )",
            "",
            "        # Handle state resets. For example, if we see",
            "        # `room_membership_for_user_at_to_token.event_id=None and",
            "        # room_membership_for_user_at_to_token.membership is not None`, we should",
            "        # indicate to the client that a state reset happened. Perhaps we should indicate",
            "        # this by setting `initial: True` and empty `required_state: []`.",
            "        state_reset_out_of_room = False",
            "        if (",
            "            room_membership_for_user_at_to_token.event_id is None",
            "            and room_membership_for_user_at_to_token.membership is not None",
            "        ):",
            "            # We only expect the `event_id` to be `None` if you've been state reset out",
            "            # of the room (meaning you're no longer in the room). We could put this as",
            "            # part of the if-statement above but we want to handle every case where",
            "            # `event_id` is `None`.",
            "            assert room_membership_for_user_at_to_token.membership is Membership.LEAVE",
            "",
            "            state_reset_out_of_room = True",
            "",
            "        prev_room_sync_config = previous_connection_state.room_configs.get(room_id)",
            "",
            "        # Determine whether we should limit the timeline to the token range.",
            "        #",
            "        # We should return historical messages (before token range) in the",
            "        # following cases because we want clients to be able to show a basic",
            "        # screen of information:",
            "        #",
            "        #  - Initial sync (because no `from_token` to limit us anyway)",
            "        #  - When users `newly_joined`",
            "        #  - For an incremental sync where we haven't sent it down this",
            "        #    connection before",
            "        #",
            "        # Relevant spec issue:",
            "        # https://github.com/matrix-org/matrix-spec/issues/1917",
            "        #",
            "        # XXX: Odd behavior - We also check if the `timeline_limit` has increased, if so",
            "        # we ignore the from bound for the timeline to send down a larger chunk of",
            "        # history and set `unstable_expanded_timeline` to true. This is only being added",
            "        # to match the behavior of the Sliding Sync proxy as we expect the ElementX",
            "        # client to feel a certain way and be able to trickle in a full page of timeline",
            "        # messages to fill up the screen. This is a bit different to the behavior of the",
            "        # Sliding Sync proxy (which sets initial=true, but then doesn't send down the",
            "        # full state again), but existing apps, e.g. ElementX, just need `limited` set.",
            "        # We don't explicitly set `limited` but this will be the case for any room that",
            "        # has more history than we're trying to pull out. Using",
            "        # `unstable_expanded_timeline` allows us to avoid contaminating what `initial`",
            "        # or `limited` mean for clients that interpret them correctly. In future this",
            "        # behavior is almost certainly going to change.",
            "        #",
            "        from_bound = None",
            "        initial = True",
            "        ignore_timeline_bound = False",
            "        if from_token and not newly_joined and not state_reset_out_of_room:",
            "            room_status = previous_connection_state.rooms.have_sent_room(room_id)",
            "            if room_status.status == HaveSentRoomFlag.LIVE:",
            "                from_bound = from_token.stream_token.room_key",
            "                initial = False",
            "            elif room_status.status == HaveSentRoomFlag.PREVIOUSLY:",
            "                assert room_status.last_token is not None",
            "                from_bound = room_status.last_token",
            "                initial = False",
            "            elif room_status.status == HaveSentRoomFlag.NEVER:",
            "                from_bound = None",
            "                initial = True",
            "            else:",
            "                assert_never(room_status.status)",
            "",
            "            log_kv({\"sliding_sync.room_status\": room_status})",
            "",
            "            if prev_room_sync_config is not None:",
            "                # Check if the timeline limit has increased, if so ignore the",
            "                # timeline bound and record the change (see \"XXX: Odd behavior\"",
            "                # above).",
            "                if (",
            "                    prev_room_sync_config.timeline_limit",
            "                    < room_sync_config.timeline_limit",
            "                ):",
            "                    ignore_timeline_bound = True",
            "",
            "        log_kv(",
            "            {",
            "                \"sliding_sync.from_bound\": from_bound,",
            "                \"sliding_sync.initial\": initial,",
            "                \"sliding_sync.ignore_timeline_bound\": ignore_timeline_bound,",
            "            }",
            "        )",
            "",
            "        # Assemble the list of timeline events",
            "        #",
            "        # FIXME: It would be nice to make the `rooms` response more uniform regardless of",
            "        # membership. Currently, we have to make all of these optional because",
            "        # `invite`/`knock` rooms only have `stripped_state`. See",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1653045932",
            "        timeline_events: List[EventBase] = []",
            "        bundled_aggregations: Optional[Dict[str, BundledAggregations]] = None",
            "        limited: Optional[bool] = None",
            "        prev_batch_token: Optional[StreamToken] = None",
            "        num_live: Optional[int] = None",
            "        if (",
            "            room_sync_config.timeline_limit > 0",
            "            # No timeline for invite/knock rooms (just `stripped_state`)",
            "            and room_membership_for_user_at_to_token.membership",
            "            not in (Membership.INVITE, Membership.KNOCK)",
            "        ):",
            "            limited = False",
            "            # We want to start off using the `to_token` (vs `from_token`) because we look",
            "            # backwards from the `to_token` up to the `timeline_limit` and we might not",
            "            # reach the `from_token` before we hit the limit. We will update the room stream",
            "            # position once we've fetched the events to point to the earliest event fetched.",
            "            prev_batch_token = to_token",
            "",
            "            # We're going to paginate backwards from the `to_token`",
            "            to_bound = to_token.room_key",
            "            # People shouldn't see past their leave/ban event",
            "            if room_membership_for_user_at_to_token.membership in (",
            "                Membership.LEAVE,",
            "                Membership.BAN,",
            "            ):",
            "                to_bound = room_membership_for_user_at_to_token.event_pos.to_room_stream_token()",
            "",
            "            timeline_from_bound = from_bound",
            "            if ignore_timeline_bound:",
            "                timeline_from_bound = None",
            "",
            "            # For initial `/sync` (and other historical scenarios mentioned above), we",
            "            # want to view a historical section of the timeline; to fetch events by",
            "            # `topological_ordering` (best representation of the room DAG as others were",
            "            # seeing it at the time). This also aligns with the order that `/messages`",
            "            # returns events in.",
            "            #",
            "            # For incremental `/sync`, we want to get all updates for rooms since",
            "            # the last `/sync` (regardless if those updates arrived late or happened",
            "            # a while ago in the past); to fetch events by `stream_ordering` (in the",
            "            # order they were received by the server).",
            "            #",
            "            # Relevant spec issue: https://github.com/matrix-org/matrix-spec/issues/1917",
            "            #",
            "            # FIXME: Using workaround for mypy,",
            "            # https://github.com/python/mypy/issues/10740#issuecomment-1997047277 and",
            "            # https://github.com/python/mypy/issues/17479",
            "            paginate_room_events_by_topological_ordering: PaginateFunction = (",
            "                self.store.paginate_room_events_by_topological_ordering",
            "            )",
            "            paginate_room_events_by_stream_ordering: PaginateFunction = (",
            "                self.store.paginate_room_events_by_stream_ordering",
            "            )",
            "            pagination_method: PaginateFunction = (",
            "                # Use `topographical_ordering` for historical events",
            "                paginate_room_events_by_topological_ordering",
            "                if timeline_from_bound is None",
            "                # Use `stream_ordering` for updates",
            "                else paginate_room_events_by_stream_ordering",
            "            )",
            "            timeline_events, new_room_key, limited = await pagination_method(",
            "                room_id=room_id,",
            "                # The bounds are reversed so we can paginate backwards",
            "                # (from newer to older events) starting at to_bound.",
            "                # This ensures we fill the `limit` with the newest events first,",
            "                from_key=to_bound,",
            "                to_key=timeline_from_bound,",
            "                direction=Direction.BACKWARDS,",
            "                limit=room_sync_config.timeline_limit,",
            "            )",
            "",
            "            # We want to return the events in ascending order (the last event is the",
            "            # most recent).",
            "            timeline_events.reverse()",
            "",
            "            # Make sure we don't expose any events that the client shouldn't see",
            "            timeline_events = await filter_events_for_client(",
            "                self.storage_controllers,",
            "                user.to_string(),",
            "                timeline_events,",
            "                is_peeking=room_membership_for_user_at_to_token.membership",
            "                != Membership.JOIN,",
            "                filter_send_to_client=True,",
            "            )",
            "            # TODO: Filter out `EventTypes.CallInvite` in public rooms,",
            "            # see https://github.com/element-hq/synapse/issues/17359",
            "",
            "            # TODO: Handle timeline gaps (`get_timeline_gaps()`)",
            "",
            "            # Determine how many \"live\" events we have (events within the given token range).",
            "            #",
            "            # This is mostly useful to determine whether a given @mention event should",
            "            # make a noise or not. Clients cannot rely solely on the absence of",
            "            # `initial: true` to determine live events because if a room not in the",
            "            # sliding window bumps into the window because of an @mention it will have",
            "            # `initial: true` yet contain a single live event (with potentially other",
            "            # old events in the timeline)",
            "            num_live = 0",
            "            if from_token is not None:",
            "                for timeline_event in reversed(timeline_events):",
            "                    # This fields should be present for all persisted events",
            "                    assert timeline_event.internal_metadata.stream_ordering is not None",
            "                    assert timeline_event.internal_metadata.instance_name is not None",
            "",
            "                    persisted_position = PersistedEventPosition(",
            "                        instance_name=timeline_event.internal_metadata.instance_name,",
            "                        stream=timeline_event.internal_metadata.stream_ordering,",
            "                    )",
            "                    if persisted_position.persisted_after(",
            "                        from_token.stream_token.room_key",
            "                    ):",
            "                        num_live += 1",
            "                    else:",
            "                        # Since we're iterating over the timeline events in",
            "                        # reverse-chronological order, we can break once we hit an event",
            "                        # that's not live. In the future, we could potentially optimize",
            "                        # this more with a binary search (bisect).",
            "                        break",
            "",
            "            # If the timeline is `limited=True`, the client does not have all events",
            "            # necessary to calculate aggregations themselves.",
            "            if limited:",
            "                bundled_aggregations = (",
            "                    await self.relations_handler.get_bundled_aggregations(",
            "                        timeline_events, user.to_string()",
            "                    )",
            "                )",
            "",
            "            # Update the `prev_batch_token` to point to the position that allows us to",
            "            # keep paginating backwards from the oldest event we return in the timeline.",
            "            prev_batch_token = prev_batch_token.copy_and_replace(",
            "                StreamKeyType.ROOM, new_room_key",
            "            )",
            "",
            "        # Figure out any stripped state events for invite/knocks. This allows the",
            "        # potential joiner to identify the room.",
            "        stripped_state: List[JsonDict] = []",
            "        if room_membership_for_user_at_to_token.membership in (",
            "            Membership.INVITE,",
            "            Membership.KNOCK,",
            "        ):",
            "            # This should never happen. If someone is invited/knocked on room, then",
            "            # there should be an event for it.",
            "            assert room_membership_for_user_at_to_token.event_id is not None",
            "",
            "            invite_or_knock_event = await self.store.get_event(",
            "                room_membership_for_user_at_to_token.event_id",
            "            )",
            "",
            "            stripped_state = []",
            "            if invite_or_knock_event.membership == Membership.INVITE:",
            "                invite_state = invite_or_knock_event.unsigned.get(",
            "                    \"invite_room_state\", []",
            "                )",
            "                if not isinstance(invite_state, list):",
            "                    invite_state = []",
            "",
            "                stripped_state.extend(invite_state)",
            "            elif invite_or_knock_event.membership == Membership.KNOCK:",
            "                knock_state = invite_or_knock_event.unsigned.get(\"knock_room_state\", [])",
            "                if not isinstance(knock_state, list):",
            "                    knock_state = []",
            "",
            "                stripped_state.extend(knock_state)",
            "",
            "            stripped_state.append(strip_event(invite_or_knock_event))",
            "",
            "        # Get the changes to current state in the token range from the",
            "        # `current_state_delta_stream` table.",
            "        #",
            "        # For incremental syncs, we can do this first to determine if something relevant",
            "        # has changed and strategically avoid fetching other costly things.",
            "        room_state_delta_id_map: MutableStateMap[str] = {}",
            "        name_event_id: Optional[str] = None",
            "        membership_changed = False",
            "        name_changed = False",
            "        avatar_changed = False",
            "        if initial:",
            "            # Check whether the room has a name set",
            "            name_state_ids = await self.get_current_state_ids_at(",
            "                room_id=room_id,",
            "                room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                state_filter=StateFilter.from_types([(EventTypes.Name, \"\")]),",
            "                to_token=to_token,",
            "            )",
            "            name_event_id = name_state_ids.get((EventTypes.Name, \"\"))",
            "        else:",
            "            assert from_bound is not None",
            "",
            "            # TODO: Limit the number of state events we're about to send down",
            "            # the room, if its too many we should change this to an",
            "            # `initial=True`?",
            "            deltas = await self.get_current_state_deltas_for_room(",
            "                room_id=room_id,",
            "                room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                from_token=from_bound,",
            "                to_token=to_token.room_key,",
            "            )",
            "            for delta in deltas:",
            "                # TODO: Handle state resets where event_id is None",
            "                if delta.event_id is not None:",
            "                    room_state_delta_id_map[(delta.event_type, delta.state_key)] = (",
            "                        delta.event_id",
            "                    )",
            "",
            "                if delta.event_type == EventTypes.Member:",
            "                    membership_changed = True",
            "                elif delta.event_type == EventTypes.Name and delta.state_key == \"\":",
            "                    name_changed = True",
            "                elif (",
            "                    delta.event_type == EventTypes.RoomAvatar and delta.state_key == \"\"",
            "                ):",
            "                    avatar_changed = True",
            "",
            "        # We only need the room summary for calculating heroes, however if we do",
            "        # fetch it then we can use it to calculate `joined_count` and",
            "        # `invited_count`.",
            "        room_membership_summary: Optional[Mapping[str, MemberSummary]] = None",
            "",
            "        # `heroes` are required if the room name is not set.",
            "        #",
            "        # Note: When you're the first one on your server to be invited to a new room",
            "        # over federation, we only have access to some stripped state in",
            "        # `event.unsigned.invite_room_state` which currently doesn't include `heroes`,",
            "        # see https://github.com/matrix-org/matrix-spec/issues/380. This means that",
            "        # clients won't be able to calculate the room name when necessary and just a",
            "        # pitfall we have to deal with until that spec issue is resolved.",
            "        hero_user_ids: List[str] = []",
            "        # TODO: Should we also check for `EventTypes.CanonicalAlias`",
            "        # (`m.room.canonical_alias`) as a fallback for the room name? see",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1671260153",
            "        #",
            "        # We need to fetch the `heroes` if the room name is not set. But we only need to",
            "        # get them on initial syncs (or the first time we send down the room) or if the",
            "        # membership has changed which may change the heroes.",
            "        if name_event_id is None and (initial or (not initial and membership_changed)):",
            "            # We need the room summary to extract the heroes from",
            "            if room_membership_for_user_at_to_token.membership != Membership.JOIN:",
            "                # TODO: Figure out how to get the membership summary for left/banned rooms",
            "                # For invite/knock rooms we don't include the information.",
            "                room_membership_summary = {}",
            "            else:",
            "                room_membership_summary = await self.store.get_room_summary(room_id)",
            "                # TODO: Reverse/rewind back to the `to_token`",
            "",
            "            hero_user_ids = extract_heroes_from_room_summary(",
            "                room_membership_summary, me=user.to_string()",
            "            )",
            "",
            "        # Fetch the membership counts for rooms we're joined to.",
            "        #",
            "        # Similarly to other metadata, we only need to calculate the member",
            "        # counts if this is an initial sync or the memberships have changed.",
            "        joined_count: Optional[int] = None",
            "        invited_count: Optional[int] = None",
            "        if (",
            "            initial or membership_changed",
            "        ) and room_membership_for_user_at_to_token.membership == Membership.JOIN:",
            "            # If we have the room summary (because we calculated heroes above)",
            "            # then we can simply pull the counts from there.",
            "            if room_membership_summary is not None:",
            "                empty_membership_summary = MemberSummary([], 0)",
            "",
            "                joined_count = room_membership_summary.get(",
            "                    Membership.JOIN, empty_membership_summary",
            "                ).count",
            "",
            "                invited_count = room_membership_summary.get(",
            "                    Membership.INVITE, empty_membership_summary",
            "                ).count",
            "            else:",
            "                member_counts = await self.store.get_member_counts(room_id)",
            "                joined_count = member_counts.get(Membership.JOIN, 0)",
            "                invited_count = member_counts.get(Membership.INVITE, 0)",
            "",
            "        # Fetch the `required_state` for the room",
            "        #",
            "        # No `required_state` for invite/knock rooms (just `stripped_state`)",
            "        #",
            "        # FIXME: It would be nice to make the `rooms` response more uniform regardless",
            "        # of membership. Currently, we have to make this optional because",
            "        # `invite`/`knock` rooms only have `stripped_state`. See",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1653045932",
            "        #",
            "        # Calculate the `StateFilter` based on the `required_state` for the room",
            "        required_state_filter = StateFilter.none()",
            "        # The requested `required_state_map` with the lazy membership expanded and",
            "        # `$ME` replaced with the user's ID. This allows us to see what membership we've",
            "        # sent down to the client in the next request.",
            "        #",
            "        # Make a copy so we can modify it. Still need to be careful to make a copy of",
            "        # the state key sets if we want to add/remove from them. We could make a deep",
            "        # copy but this saves us some work.",
            "        expanded_required_state_map = dict(room_sync_config.required_state_map)",
            "        if room_membership_for_user_at_to_token.membership not in (",
            "            Membership.INVITE,",
            "            Membership.KNOCK,",
            "        ):",
            "            # If we have a double wildcard (\"*\", \"*\") in the `required_state`, we need",
            "            # to fetch all state for the room",
            "            #",
            "            # Note: MSC3575 describes different behavior to how we're handling things",
            "            # here but since it's not wrong to return more state than requested",
            "            # (`required_state` is just the minimum requested), it doesn't matter if we",
            "            # include more than client wanted. This complexity is also under scrutiny,",
            "            # see",
            "            # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1185109050",
            "            #",
            "            # > One unique exception is when you request all state events via [\"*\", \"*\"]. When used,",
            "            # > all state events are returned by default, and additional entries FILTER OUT the returned set",
            "            # > of state events. These additional entries cannot use '*' themselves.",
            "            # > For example, [\"*\", \"*\"], [\"m.room.member\", \"@alice:example.com\"] will _exclude_ every m.room.member",
            "            # > event _except_ for @alice:example.com, and include every other state event.",
            "            # > In addition, [\"*\", \"*\"], [\"m.space.child\", \"*\"] is an error, the m.space.child filter is not",
            "            # > required as it would have been returned anyway.",
            "            # >",
            "            # > -- MSC3575 (https://github.com/matrix-org/matrix-spec-proposals/pull/3575)",
            "            if StateValues.WILDCARD in room_sync_config.required_state_map.get(",
            "                StateValues.WILDCARD, set()",
            "            ):",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_wildcard\",",
            "                    True,",
            "                )",
            "                required_state_filter = StateFilter.all()",
            "            # TODO: `StateFilter` currently doesn't support wildcard event types. We're",
            "            # currently working around this by returning all state to the client but it",
            "            # would be nice to fetch less from the database and return just what the",
            "            # client wanted.",
            "            elif (",
            "                room_sync_config.required_state_map.get(StateValues.WILDCARD)",
            "                is not None",
            "            ):",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_wildcard_event_type\",",
            "                    True,",
            "                )",
            "                required_state_filter = StateFilter.all()",
            "            else:",
            "                required_state_types: List[Tuple[str, Optional[str]]] = []",
            "                num_wild_state_keys = 0",
            "                lazy_load_room_members = False",
            "                num_others = 0",
            "                for (",
            "                    state_type,",
            "                    state_key_set,",
            "                ) in room_sync_config.required_state_map.items():",
            "                    for state_key in state_key_set:",
            "                        if state_key == StateValues.WILDCARD:",
            "                            num_wild_state_keys += 1",
            "                            # `None` is a wildcard in the `StateFilter`",
            "                            required_state_types.append((state_type, None))",
            "                        # We need to fetch all relevant people when we're lazy-loading membership",
            "                        elif (",
            "                            state_type == EventTypes.Member",
            "                            and state_key == StateValues.LAZY",
            "                        ):",
            "                            lazy_load_room_members = True",
            "                            # Everyone in the timeline is relevant",
            "                            #",
            "                            # FIXME: We probably also care about invite, ban, kick, targets, etc",
            "                            # but the spec only mentions \"senders\".",
            "                            timeline_membership: Set[str] = set()",
            "                            if timeline_events is not None:",
            "                                for timeline_event in timeline_events:",
            "                                    timeline_membership.add(timeline_event.sender)",
            "",
            "                            # Update the required state filter so we pick up the new",
            "                            # membership",
            "                            for user_id in timeline_membership:",
            "                                required_state_types.append(",
            "                                    (EventTypes.Member, user_id)",
            "                                )",
            "",
            "                            # Add an explicit entry for each user in the timeline",
            "                            #",
            "                            # Make a new set or copy of the state key set so we can",
            "                            # modify it without affecting the original",
            "                            # `required_state_map`",
            "                            expanded_required_state_map[EventTypes.Member] = (",
            "                                expanded_required_state_map.get(",
            "                                    EventTypes.Member, set()",
            "                                )",
            "                                | timeline_membership",
            "                            )",
            "                        elif state_key == StateValues.ME:",
            "                            num_others += 1",
            "                            required_state_types.append((state_type, user.to_string()))",
            "                            # Replace `$ME` with the user's ID so we can deduplicate",
            "                            # when someone requests the same state with `$ME` or with",
            "                            # their user ID.",
            "                            #",
            "                            # Make a new set or copy of the state key set so we can",
            "                            # modify it without affecting the original",
            "                            # `required_state_map`",
            "                            expanded_required_state_map[EventTypes.Member] = (",
            "                                expanded_required_state_map.get(",
            "                                    EventTypes.Member, set()",
            "                                )",
            "                                | {user.to_string()}",
            "                            )",
            "                        else:",
            "                            num_others += 1",
            "                            required_state_types.append((state_type, state_key))",
            "",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX",
            "                    + \"required_state_wildcard_state_key_count\",",
            "                    num_wild_state_keys,",
            "                )",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_lazy\",",
            "                    lazy_load_room_members,",
            "                )",
            "                set_tag(",
            "                    SynapseTags.FUNC_ARG_PREFIX + \"required_state_other_count\",",
            "                    num_others,",
            "                )",
            "",
            "                required_state_filter = StateFilter.from_types(required_state_types)",
            "",
            "        # We need this base set of info for the response so let's just fetch it along",
            "        # with the `required_state` for the room",
            "        hero_room_state = [",
            "            (EventTypes.Member, hero_user_id) for hero_user_id in hero_user_ids",
            "        ]",
            "        meta_room_state = list(hero_room_state)",
            "        if initial or name_changed:",
            "            meta_room_state.append((EventTypes.Name, \"\"))",
            "        if initial or avatar_changed:",
            "            meta_room_state.append((EventTypes.RoomAvatar, \"\"))",
            "",
            "        state_filter = StateFilter.all()",
            "        if required_state_filter != StateFilter.all():",
            "            state_filter = StateFilter(",
            "                types=StateFilter.from_types(",
            "                    chain(meta_room_state, required_state_filter.to_types())",
            "                ).types,",
            "                include_others=required_state_filter.include_others,",
            "            )",
            "",
            "        # The required state map to store in the room sync config, if it has",
            "        # changed.",
            "        changed_required_state_map: Optional[Mapping[str, AbstractSet[str]]] = None",
            "",
            "        # We can return all of the state that was requested if this was the first",
            "        # time we've sent the room down this connection.",
            "        room_state: StateMap[EventBase] = {}",
            "        if initial:",
            "            room_state = await self.get_current_state_at(",
            "                room_id=room_id,",
            "                room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                state_filter=state_filter,",
            "                to_token=to_token,",
            "            )",
            "        else:",
            "            assert from_bound is not None",
            "",
            "            if prev_room_sync_config is not None:",
            "                # Check if there are any changes to the required state config",
            "                # that we need to handle.",
            "                changed_required_state_map, added_state_filter = (",
            "                    _required_state_changes(",
            "                        user.to_string(),",
            "                        prev_required_state_map=prev_room_sync_config.required_state_map,",
            "                        request_required_state_map=expanded_required_state_map,",
            "                        state_deltas=room_state_delta_id_map,",
            "                    )",
            "                )",
            "",
            "                if added_state_filter:",
            "                    # Some state entries got added, so we pull out the current",
            "                    # state for them. If we don't do this we'd only send down new deltas.",
            "                    state_ids = await self.get_current_state_ids_at(",
            "                        room_id=room_id,",
            "                        room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                        state_filter=added_state_filter,",
            "                        to_token=to_token,",
            "                    )",
            "                    room_state_delta_id_map.update(state_ids)",
            "",
            "            events = await self.store.get_events(",
            "                state_filter.filter_state(room_state_delta_id_map).values()",
            "            )",
            "            room_state = {(s.type, s.state_key): s for s in events.values()}",
            "",
            "            # If the membership changed and we have to get heroes, get the remaining",
            "            # heroes from the state",
            "            if hero_user_ids:",
            "                hero_membership_state = await self.get_current_state_at(",
            "                    room_id=room_id,",
            "                    room_membership_for_user_at_to_token=room_membership_for_user_at_to_token,",
            "                    state_filter=StateFilter.from_types(hero_room_state),",
            "                    to_token=to_token,",
            "                )",
            "                room_state.update(hero_membership_state)",
            "",
            "        required_room_state: StateMap[EventBase] = {}",
            "        if required_state_filter != StateFilter.none():",
            "            required_room_state = required_state_filter.filter_state(room_state)",
            "",
            "        # Find the room name and avatar from the state",
            "        room_name: Optional[str] = None",
            "        # TODO: Should we also check for `EventTypes.CanonicalAlias`",
            "        # (`m.room.canonical_alias`) as a fallback for the room name? see",
            "        # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1671260153",
            "        name_event = room_state.get((EventTypes.Name, \"\"))",
            "        if name_event is not None:",
            "            room_name = name_event.content.get(\"name\")",
            "",
            "        room_avatar: Optional[str] = None",
            "        avatar_event = room_state.get((EventTypes.RoomAvatar, \"\"))",
            "        if avatar_event is not None:",
            "            room_avatar = avatar_event.content.get(\"url\")",
            "",
            "        # Assemble heroes: extract the info from the state we just fetched",
            "        heroes: List[SlidingSyncResult.RoomResult.StrippedHero] = []",
            "        for hero_user_id in hero_user_ids:",
            "            member_event = room_state.get((EventTypes.Member, hero_user_id))",
            "            if member_event is not None:",
            "                heroes.append(",
            "                    SlidingSyncResult.RoomResult.StrippedHero(",
            "                        user_id=hero_user_id,",
            "                        display_name=member_event.content.get(\"displayname\"),",
            "                        avatar_url=member_event.content.get(\"avatar_url\"),",
            "                    )",
            "                )",
            "",
            "        # Figure out the last bump event in the room. If the bump stamp hasn't",
            "        # changed we omit it from the response.",
            "        bump_stamp = None",
            "",
            "        always_return_bump_stamp = (",
            "            # We use the membership event position for any non-join",
            "            room_membership_for_user_at_to_token.membership != Membership.JOIN",
            "            # We didn't fetch any timeline events but we should still check for",
            "            # a bump_stamp that might be somewhere",
            "            or limited is None",
            "            # There might be a bump event somewhere before the timeline events",
            "            # that we fetched, that we didn't previously send down",
            "            or limited is True",
            "            # Always give the client some frame of reference if this is the",
            "            # first time they are seeing the room down the connection",
            "            or initial",
            "        )",
            "",
            "        # If we're joined to the room, we need to find the last bump event before the",
            "        # `to_token`",
            "        if room_membership_for_user_at_to_token.membership == Membership.JOIN:",
            "            # Try and get a bump stamp",
            "            new_bump_stamp = await self._get_bump_stamp(",
            "                room_id,",
            "                to_token,",
            "                timeline_events,",
            "                check_outside_timeline=always_return_bump_stamp,",
            "            )",
            "            if new_bump_stamp is not None:",
            "                bump_stamp = new_bump_stamp",
            "",
            "        if bump_stamp is None and always_return_bump_stamp:",
            "            # By default, just choose the membership event position for any non-join membership",
            "            bump_stamp = room_membership_for_user_at_to_token.event_pos.stream",
            "",
            "        if bump_stamp is not None and bump_stamp < 0:",
            "            # We never want to send down negative stream orderings, as you can't",
            "            # sensibly compare positive and negative stream orderings (they have",
            "            # different meanings).",
            "            #",
            "            # A negative bump stamp here can only happen if the stream ordering",
            "            # of the membership event is negative (and there are no further bump",
            "            # stamps), which can happen if the server leaves and deletes a room,",
            "            # and then rejoins it.",
            "            #",
            "            # To deal with this, we just set the bump stamp to zero, which will",
            "            # shove this room to the bottom of the list. This is OK as the",
            "            # moment a new message happens in the room it will get put into a",
            "            # sensible order again.",
            "            bump_stamp = 0",
            "",
            "        room_sync_required_state_map_to_persist: Mapping[str, AbstractSet[str]] = (",
            "            expanded_required_state_map",
            "        )",
            "        if changed_required_state_map:",
            "            room_sync_required_state_map_to_persist = changed_required_state_map",
            "",
            "        # Record the `room_sync_config` if we're `ignore_timeline_bound` (which means",
            "        # that the `timeline_limit` has increased)",
            "        unstable_expanded_timeline = False",
            "        if ignore_timeline_bound:",
            "            # FIXME: We signal the fact that we're sending down more events to",
            "            # the client by setting `unstable_expanded_timeline` to true (see",
            "            # \"XXX: Odd behavior\" above).",
            "            unstable_expanded_timeline = True",
            "",
            "            new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                timeline_limit=room_sync_config.timeline_limit,",
            "                required_state_map=room_sync_required_state_map_to_persist,",
            "            )",
            "        elif prev_room_sync_config is not None:",
            "            # If the result is `limited` then we need to record that the",
            "            # `timeline_limit` has been reduced, as when/if the client later requests",
            "            # more timeline then we have more data to send.",
            "            #",
            "            # Otherwise (when not `limited`) we don't need to record that the",
            "            # `timeline_limit` has been reduced, as the *effective* `timeline_limit`",
            "            # (i.e. the amount of timeline we have previously sent to the client) is at",
            "            # least the previous `timeline_limit`.",
            "            #",
            "            # This is to handle the case where the `timeline_limit` e.g. goes from 10 to",
            "            # 5 to 10 again (without any timeline gaps), where there's no point sending",
            "            # down the initial historical chunk events when the `timeline_limit` is",
            "            # increased as the client already has the 10 previous events. However, if",
            "            # client has a gap in the timeline (i.e. `limited` is True), then we *do*",
            "            # need to record the reduced timeline.",
            "            #",
            "            # TODO: Handle timeline gaps (`get_timeline_gaps()`) - This is separate from",
            "            # the gaps we might see on the client because a response was `limited` we're",
            "            # talking about above.",
            "            if (",
            "                limited",
            "                and prev_room_sync_config.timeline_limit",
            "                > room_sync_config.timeline_limit",
            "            ):",
            "                new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                    timeline_limit=room_sync_config.timeline_limit,",
            "                    required_state_map=room_sync_required_state_map_to_persist,",
            "                )",
            "",
            "            elif changed_required_state_map is not None:",
            "                new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                    timeline_limit=room_sync_config.timeline_limit,",
            "                    required_state_map=room_sync_required_state_map_to_persist,",
            "                )",
            "",
            "        else:",
            "            new_connection_state.room_configs[room_id] = RoomSyncConfig(",
            "                timeline_limit=room_sync_config.timeline_limit,",
            "                required_state_map=room_sync_required_state_map_to_persist,",
            "            )",
            "",
            "        set_tag(SynapseTags.RESULT_PREFIX + \"initial\", initial)",
            "",
            "        return SlidingSyncResult.RoomResult(",
            "            name=room_name,",
            "            avatar=room_avatar,",
            "            heroes=heroes,",
            "            is_dm=is_dm,",
            "            initial=initial,",
            "            required_state=list(required_room_state.values()),",
            "            timeline_events=timeline_events,",
            "            bundled_aggregations=bundled_aggregations,",
            "            stripped_state=stripped_state,",
            "            prev_batch=prev_batch_token,",
            "            limited=limited,",
            "            unstable_expanded_timeline=unstable_expanded_timeline,",
            "            num_live=num_live,",
            "            bump_stamp=bump_stamp,",
            "            joined_count=joined_count,",
            "            invited_count=invited_count,",
            "            # TODO: These are just dummy values. We could potentially just remove these",
            "            # since notifications can only really be done correctly on the client anyway",
            "            # (encrypted rooms).",
            "            notification_count=0,",
            "            highlight_count=0,",
            "        )",
            "",
            "    @trace",
            "    async def _get_bump_stamp(",
            "        self,",
            "        room_id: str,",
            "        to_token: StreamToken,",
            "        timeline: List[EventBase],",
            "        check_outside_timeline: bool,",
            "    ) -> Optional[int]:",
            "        \"\"\"Get a bump stamp for the room, if we have a bump event and it has",
            "        changed.",
            "",
            "        Args:",
            "            room_id",
            "            to_token: The upper bound of token to return",
            "            timeline: The list of events we have fetched.",
            "            limited: If the timeline was limited.",
            "            check_outside_timeline: Whether we need to check for bump stamp for",
            "                events before the timeline if we didn't find a bump stamp in",
            "                the timeline events.",
            "        \"\"\"",
            "",
            "        # First check the timeline events we're returning to see if one of",
            "        # those matches. We iterate backwards and take the stream ordering",
            "        # of the first event that matches the bump event types.",
            "        for timeline_event in reversed(timeline):",
            "            if timeline_event.type in SLIDING_SYNC_DEFAULT_BUMP_EVENT_TYPES:",
            "                new_bump_stamp = timeline_event.internal_metadata.stream_ordering",
            "",
            "                # All persisted events have a stream ordering",
            "                assert new_bump_stamp is not None",
            "",
            "                # If we've just joined a remote room, then the last bump event may",
            "                # have been backfilled (and so have a negative stream ordering).",
            "                # These negative stream orderings can't sensibly be compared, so",
            "                # instead we use the membership event position.",
            "                if new_bump_stamp > 0:",
            "                    return new_bump_stamp",
            "",
            "        if not check_outside_timeline:",
            "            # If we are not a limited sync, then we know the bump stamp can't",
            "            # have changed.",
            "            return None",
            "",
            "        # We can quickly query for the latest bump event in the room using the",
            "        # sliding sync tables.",
            "        latest_room_bump_stamp = await self.store.get_latest_bump_stamp_for_room(",
            "            room_id",
            "        )",
            "",
            "        min_to_token_position = to_token.room_key.stream",
            "",
            "        # If we can rely on the new sliding sync tables and the `bump_stamp` is",
            "        # `None`, just fallback to the membership event position. This can happen",
            "        # when we've just joined a remote room and all the events are backfilled.",
            "        if (",
            "            # FIXME: The background job check can be removed once we bump",
            "            # `SCHEMA_COMPAT_VERSION` and run the foreground update for",
            "            # `sliding_sync_joined_rooms`/`sliding_sync_membership_snapshots`",
            "            # (tracked by https://github.com/element-hq/synapse/issues/17623)",
            "            latest_room_bump_stamp is None",
            "            and await self.store.have_finished_sliding_sync_background_jobs()",
            "        ):",
            "            return None",
            "",
            "        # The `bump_stamp` stored in the database might be ahead of our token. Since",
            "        # `bump_stamp` is only a `stream_ordering` position, we can't be 100% sure",
            "        # that's before the `to_token` in all scenarios. The only scenario we can be",
            "        # sure of is if the `bump_stamp` is totally before the minimum position from",
            "        # the token.",
            "        #",
            "        # We don't need to check if the background update has finished, as if the",
            "        # returned bump stamp is not None then it must be up to date.",
            "        elif (",
            "            latest_room_bump_stamp is not None",
            "            and latest_room_bump_stamp < min_to_token_position",
            "        ):",
            "            if latest_room_bump_stamp > 0:",
            "                return latest_room_bump_stamp",
            "            else:",
            "                return None",
            "",
            "        # Otherwise, if it's within or after the `to_token`, we need to find the",
            "        # last bump event before the `to_token`.",
            "        else:",
            "            last_bump_event_result = (",
            "                await self.store.get_last_event_pos_in_room_before_stream_ordering(",
            "                    room_id,",
            "                    to_token.room_key,",
            "                    event_types=SLIDING_SYNC_DEFAULT_BUMP_EVENT_TYPES,",
            "                )",
            "            )",
            "            if last_bump_event_result is not None:",
            "                _, new_bump_event_pos = last_bump_event_result",
            "",
            "                # If we've just joined a remote room, then the last bump event may",
            "                # have been backfilled (and so have a negative stream ordering).",
            "                # These negative stream orderings can't sensibly be compared, so",
            "                # instead we use the membership event position.",
            "                if new_bump_event_pos.stream > 0:",
            "                    return new_bump_event_pos.stream",
            "",
            "            return None",
            "",
            "",
            "def _required_state_changes(",
            "    user_id: str,",
            "    *,",
            "    prev_required_state_map: Mapping[str, AbstractSet[str]],",
            "    request_required_state_map: Mapping[str, AbstractSet[str]],",
            "    state_deltas: StateMap[str],",
            ") -> Tuple[Optional[Mapping[str, AbstractSet[str]]], StateFilter]:",
            "    \"\"\"Calculates the changes between the required state room config from the",
            "    previous requests compared with the current request.",
            "",
            "    This does two things. First, it calculates if we need to update the room",
            "    config due to changes to required state. Secondly, it works out which state",
            "    entries we need to pull from current state and return due to the state entry",
            "    now appearing in the required state when it previously wasn't (on top of the",
            "    state deltas).",
            "",
            "    This function tries to ensure to handle the case where a state entry is",
            "    added, removed and then added again to the required state. In that case we",
            "    only want to re-send that entry down sync if it has changed.",
            "",
            "    Returns:",
            "        A 2-tuple of updated required state config (or None if there is no update)",
            "        and the state filter to use to fetch extra current state that we need to",
            "        return.",
            "    \"\"\"",
            "    if prev_required_state_map == request_required_state_map:",
            "        # There has been no change. Return immediately.",
            "        return None, StateFilter.none()",
            "",
            "    prev_wildcard = prev_required_state_map.get(StateValues.WILDCARD, set())",
            "    request_wildcard = request_required_state_map.get(StateValues.WILDCARD, set())",
            "",
            "    # If we were previously fetching everything (\"*\", \"*\"), always update the effective",
            "    # room required state config to match the request. And since we we're previously",
            "    # already fetching everything, we don't have to fetch anything now that they've",
            "    # narrowed.",
            "    if StateValues.WILDCARD in prev_wildcard:",
            "        return request_required_state_map, StateFilter.none()",
            "",
            "    # If a event type wildcard has been added or removed we don't try and do",
            "    # anything fancy, and instead always update the effective room required",
            "    # state config to match the request.",
            "    if request_wildcard - prev_wildcard:",
            "        # Some keys were added, so we need to fetch everything",
            "        return request_required_state_map, StateFilter.all()",
            "    if prev_wildcard - request_wildcard:",
            "        # Keys were only removed, so we don't have to fetch everything.",
            "        return request_required_state_map, StateFilter.none()",
            "",
            "    # Contains updates to the required state map compared with the previous room",
            "    # config. This has the same format as `RoomSyncConfig.required_state`",
            "    changes: Dict[str, AbstractSet[str]] = {}",
            "",
            "    # The set of types/state keys that we need to fetch and return to the",
            "    # client. Passed to `StateFilter.from_types(...)`",
            "    added: List[Tuple[str, Optional[str]]] = []",
            "",
            "    # Convert the list of state deltas to map from type to state_keys that have",
            "    # changed.",
            "    changed_types_to_state_keys: Dict[str, Set[str]] = {}",
            "    for event_type, state_key in state_deltas:",
            "        changed_types_to_state_keys.setdefault(event_type, set()).add(state_key)",
            "",
            "    # First we calculate what, if anything, has been *added*.",
            "    for event_type in (",
            "        prev_required_state_map.keys() | request_required_state_map.keys()",
            "    ):",
            "        old_state_keys = prev_required_state_map.get(event_type, set())",
            "        request_state_keys = request_required_state_map.get(event_type, set())",
            "        changed_state_keys = changed_types_to_state_keys.get(event_type, set())",
            "",
            "        if old_state_keys == request_state_keys:",
            "            # No change to this type",
            "            continue",
            "",
            "        if not request_state_keys - old_state_keys:",
            "            # Nothing *added*, so we skip. Removals happen below.",
            "            continue",
            "",
            "        # We only remove state keys from the effective state if they've been",
            "        # removed from the request *and* the state has changed. This ensures",
            "        # that if a client removes and then re-adds a state key, we only send",
            "        # down the associated current state event if its changed (rather than",
            "        # sending down the same event twice).",
            "        invalidated_state_keys = (",
            "            old_state_keys - request_state_keys",
            "        ) & changed_state_keys",
            "",
            "        # Figure out which state keys we should remember sending down the connection",
            "        inheritable_previous_state_keys = (",
            "            # Retain the previous state_keys that we've sent down before.",
            "            # Wildcard and lazy state keys are not sticky from previous requests.",
            "            (old_state_keys - {StateValues.WILDCARD, StateValues.LAZY})",
            "            - invalidated_state_keys",
            "        )",
            "",
            "        # Always update changes to include the newly added keys (we've expanded the set",
            "        # of state keys), use the new requested set with whatever hasn't been",
            "        # invalidated from the previous set.",
            "        changes[event_type] = request_state_keys | inheritable_previous_state_keys",
            "        # Limit the number of state_keys we should remember sending down the connection",
            "        # for each (room_id, user_id). We don't want to store and pull out too much data",
            "        # in the database. This is a happy-medium between remembering nothing and",
            "        # everything. We can avoid sending redundant state down the connection most of",
            "        # the time given that most rooms don't have 100 members anyway and it takes a",
            "        # while to cycle through 100 members.",
            "        #",
            "        # Only remember up to (MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER)",
            "        if len(changes[event_type]) > MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER:",
            "            # Reset back to only the requested state keys",
            "            changes[event_type] = request_state_keys",
            "",
            "            # Skip if there isn't any room to fill in the rest with previous state keys",
            "            if len(request_state_keys) < MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER:",
            "                # Fill the rest with previous state_keys. Ideally, we could sort",
            "                # these by recency but it's just a set so just pick an arbitrary",
            "                # subset (good enough).",
            "                changes[event_type] = changes[event_type] | set(",
            "                    itertools.islice(",
            "                        inheritable_previous_state_keys,",
            "                        # Just taking the difference isn't perfect as there could be",
            "                        # overlap in the keys between the requested and previous but we",
            "                        # will decide to just take the easy route for now and avoid",
            "                        # additional set operations to figure it out.",
            "                        MAX_NUMBER_PREVIOUS_STATE_KEYS_TO_REMEMBER",
            "                        - len(request_state_keys),",
            "                    )",
            "                )",
            "",
            "        if StateValues.WILDCARD in old_state_keys:",
            "            # We were previously fetching everything for this type, so we don't need to",
            "            # fetch anything new.",
            "            continue",
            "",
            "        # Record the new state keys to fetch for this type.",
            "        if StateValues.WILDCARD in request_state_keys:",
            "            # If we have added a wildcard then we always just fetch everything.",
            "            added.append((event_type, None))",
            "        else:",
            "            for state_key in request_state_keys - old_state_keys:",
            "                if state_key == StateValues.ME:",
            "                    added.append((event_type, user_id))",
            "                elif state_key == StateValues.LAZY:",
            "                    # We handle lazy loading separately (outside this function),",
            "                    # so don't need to explicitly add anything here.",
            "                    #",
            "                    # LAZY values should also be ignore for event types that are",
            "                    # not membership.",
            "                    pass",
            "                else:",
            "                    added.append((event_type, state_key))",
            "",
            "    added_state_filter = StateFilter.from_types(added)",
            "",
            "    # Figure out what changes we need to apply to the effective required state",
            "    # config.",
            "    for event_type, changed_state_keys in changed_types_to_state_keys.items():",
            "        old_state_keys = prev_required_state_map.get(event_type, set())",
            "        request_state_keys = request_required_state_map.get(event_type, set())",
            "",
            "        if old_state_keys == request_state_keys:",
            "            # No change.",
            "            continue",
            "",
            "        # If we see the `user_id` as a state_key, also add \"$ME\" to the list of state",
            "        # that has changed to account for people requesting `required_state` with `$ME`",
            "        # or their user ID.",
            "        if user_id in changed_state_keys:",
            "            changed_state_keys.add(StateValues.ME)",
            "",
            "        # We only remove state keys from the effective state if they've been",
            "        # removed from the request *and* the state has changed. This ensures",
            "        # that if a client removes and then re-adds a state key, we only send",
            "        # down the associated current state event if its changed (rather than",
            "        # sending down the same event twice).",
            "        invalidated_state_keys = (",
            "            old_state_keys - request_state_keys",
            "        ) & changed_state_keys",
            "",
            "        # We've expanded the set of state keys, ... (already handled above)",
            "        if request_state_keys - old_state_keys:",
            "            continue",
            "",
            "        old_state_key_wildcard = StateValues.WILDCARD in old_state_keys",
            "        request_state_key_wildcard = StateValues.WILDCARD in request_state_keys",
            "",
            "        if old_state_key_wildcard != request_state_key_wildcard:",
            "            # If a state_key wildcard has been added or removed, we always update the",
            "            # effective room required state config to match the request.",
            "            changes[event_type] = request_state_keys",
            "            continue",
            "",
            "        if event_type == EventTypes.Member:",
            "            old_state_key_lazy = StateValues.LAZY in old_state_keys",
            "            request_state_key_lazy = StateValues.LAZY in request_state_keys",
            "",
            "            if old_state_key_lazy != request_state_key_lazy:",
            "                # If a \"$LAZY\" has been added or removed we always update the effective room",
            "                # required state config to match the request.",
            "                changes[event_type] = request_state_keys",
            "                continue",
            "",
            "        # At this point there are no wildcards and no additions to the set of",
            "        # state keys requested, only deletions.",
            "        #",
            "        # We only remove state keys from the effective state if they've been",
            "        # removed from the request *and* the state has changed. This ensures",
            "        # that if a client removes and then re-adds a state key, we only send",
            "        # down the associated current state event if its changed (rather than",
            "        # sending down the same event twice).",
            "        if invalidated_state_keys:",
            "            changes[event_type] = old_state_keys - invalidated_state_keys",
            "",
            "    if changes:",
            "        # Update the required state config based on the changes.",
            "        new_required_state_map = dict(prev_required_state_map)",
            "        for event_type, state_keys in changes.items():",
            "            if state_keys:",
            "                new_required_state_map[event_type] = state_keys",
            "            else:",
            "                # Remove entries with empty state keys.",
            "                new_required_state_map.pop(event_type, None)",
            "",
            "        return new_required_state_map, added_state_filter",
            "    else:",
            "        return None, added_state_filter"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "818": [
                "SlidingSyncHandler"
            ],
            "819": [
                "SlidingSyncHandler"
            ],
            "822": [
                "SlidingSyncHandler"
            ],
            "823": [
                "SlidingSyncHandler"
            ],
            "824": [
                "SlidingSyncHandler"
            ]
        },
        "addLocation": []
    },
    "synapse/push/push_tools.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         room_state = []"
            },
            "2": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "         if ev.content.get(\"membership\") == Membership.INVITE:"
            },
            "3": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            room_state = ev.unsigned.get(\"invite_room_state\", [])"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+            invite_room_state = ev.unsigned.get(\"invite_room_state\", [])"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+            if isinstance(invite_room_state, list):"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+                room_state = invite_room_state"
            },
            "7": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "         elif ev.content.get(\"membership\") == Membership.KNOCK:"
            },
            "8": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            room_state = ev.unsigned.get(\"knock_room_state\", [])"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+            knock_room_state = ev.unsigned.get(\"knock_room_state\", [])"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+            if isinstance(knock_room_state, list):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+                room_state = knock_room_state"
            },
            "12": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "         # Ideally we'd reuse the logic in `calculate_room_name`, but that gets"
            },
            "14": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "         # complicated to handle partial events vs pulling events from the DB."
            }
        },
        "frontPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "from typing import Dict",
            "",
            "from synapse.api.constants import EventTypes, Membership",
            "from synapse.events import EventBase",
            "from synapse.push.presentable_names import calculate_room_name, name_from_member_event",
            "from synapse.storage.controllers import StorageControllers",
            "from synapse.storage.databases.main import DataStore",
            "",
            "",
            "async def get_badge_count(store: DataStore, user_id: str, group_by_room: bool) -> int:",
            "    invites = await store.get_invited_rooms_for_local_user(user_id)",
            "    joins = await store.get_rooms_for_user(user_id)",
            "",
            "    badge = len(invites)",
            "",
            "    room_to_count = await store.get_unread_counts_by_room_for_user(user_id)",
            "    for room_id, notify_count in room_to_count.items():",
            "        # room_to_count may include rooms which the user has left,",
            "        # ignore those.",
            "        if room_id not in joins:",
            "            continue",
            "",
            "        if notify_count == 0:",
            "            continue",
            "",
            "        if group_by_room:",
            "            # return one badge count per conversation",
            "            badge += 1",
            "        else:",
            "            # Increase badge by number of notifications in room",
            "            # NOTE: this includes threaded and unthreaded notifications.",
            "            badge += notify_count",
            "",
            "    return badge",
            "",
            "",
            "async def get_context_for_event(",
            "    storage: StorageControllers, ev: EventBase, user_id: str",
            ") -> Dict[str, str]:",
            "    ctx: Dict[str, str] = {}",
            "",
            "    if ev.internal_metadata.outlier:",
            "        # We don't have state for outliers, so we can't compute the context",
            "        # except for invites and knocks. (Such events are known as 'out-of-band",
            "        # memberships' for the user).",
            "        if ev.type != EventTypes.Member:",
            "            return ctx",
            "",
            "        # We might be able to pull out the display name for the sender straight",
            "        # from the membership event",
            "        event_display_name = ev.content.get(\"displayname\")",
            "        if event_display_name and ev.state_key == ev.sender:",
            "            ctx[\"sender_display_name\"] = event_display_name",
            "",
            "        room_state = []",
            "        if ev.content.get(\"membership\") == Membership.INVITE:",
            "            room_state = ev.unsigned.get(\"invite_room_state\", [])",
            "        elif ev.content.get(\"membership\") == Membership.KNOCK:",
            "            room_state = ev.unsigned.get(\"knock_room_state\", [])",
            "",
            "        # Ideally we'd reuse the logic in `calculate_room_name`, but that gets",
            "        # complicated to handle partial events vs pulling events from the DB.",
            "        for state_dict in room_state:",
            "            type_tuple = (state_dict[\"type\"], state_dict.get(\"state_key\"))",
            "            if type_tuple == (EventTypes.Member, ev.sender):",
            "                display_name = state_dict[\"content\"].get(\"displayname\")",
            "                if display_name:",
            "                    ctx[\"sender_display_name\"] = display_name",
            "            elif type_tuple == (EventTypes.Name, \"\"):",
            "                room_name = state_dict[\"content\"].get(\"name\")",
            "                if room_name:",
            "                    ctx[\"name\"] = room_name",
            "",
            "        return ctx",
            "",
            "    room_state_ids = await storage.state.get_state_ids_for_event(ev.event_id)",
            "",
            "    # we no longer bother setting room_alias, and make room_name the",
            "    # human-readable name instead, be that m.room.name, an alias or",
            "    # a list of people in the room",
            "    name = await calculate_room_name(",
            "        storage.main, room_state_ids, user_id, fallback_to_single_member=False",
            "    )",
            "    if name:",
            "        ctx[\"name\"] = name",
            "",
            "    sender_state_event_id = room_state_ids[(\"m.room.member\", ev.sender)]",
            "    sender_state_event = await storage.main.get_event(sender_state_event_id)",
            "    ctx[\"sender_display_name\"] = name_from_member_event(sender_state_event)",
            "",
            "    return ctx"
        ],
        "afterPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "from typing import Dict",
            "",
            "from synapse.api.constants import EventTypes, Membership",
            "from synapse.events import EventBase",
            "from synapse.push.presentable_names import calculate_room_name, name_from_member_event",
            "from synapse.storage.controllers import StorageControllers",
            "from synapse.storage.databases.main import DataStore",
            "",
            "",
            "async def get_badge_count(store: DataStore, user_id: str, group_by_room: bool) -> int:",
            "    invites = await store.get_invited_rooms_for_local_user(user_id)",
            "    joins = await store.get_rooms_for_user(user_id)",
            "",
            "    badge = len(invites)",
            "",
            "    room_to_count = await store.get_unread_counts_by_room_for_user(user_id)",
            "    for room_id, notify_count in room_to_count.items():",
            "        # room_to_count may include rooms which the user has left,",
            "        # ignore those.",
            "        if room_id not in joins:",
            "            continue",
            "",
            "        if notify_count == 0:",
            "            continue",
            "",
            "        if group_by_room:",
            "            # return one badge count per conversation",
            "            badge += 1",
            "        else:",
            "            # Increase badge by number of notifications in room",
            "            # NOTE: this includes threaded and unthreaded notifications.",
            "            badge += notify_count",
            "",
            "    return badge",
            "",
            "",
            "async def get_context_for_event(",
            "    storage: StorageControllers, ev: EventBase, user_id: str",
            ") -> Dict[str, str]:",
            "    ctx: Dict[str, str] = {}",
            "",
            "    if ev.internal_metadata.outlier:",
            "        # We don't have state for outliers, so we can't compute the context",
            "        # except for invites and knocks. (Such events are known as 'out-of-band",
            "        # memberships' for the user).",
            "        if ev.type != EventTypes.Member:",
            "            return ctx",
            "",
            "        # We might be able to pull out the display name for the sender straight",
            "        # from the membership event",
            "        event_display_name = ev.content.get(\"displayname\")",
            "        if event_display_name and ev.state_key == ev.sender:",
            "            ctx[\"sender_display_name\"] = event_display_name",
            "",
            "        room_state = []",
            "        if ev.content.get(\"membership\") == Membership.INVITE:",
            "            invite_room_state = ev.unsigned.get(\"invite_room_state\", [])",
            "            if isinstance(invite_room_state, list):",
            "                room_state = invite_room_state",
            "        elif ev.content.get(\"membership\") == Membership.KNOCK:",
            "            knock_room_state = ev.unsigned.get(\"knock_room_state\", [])",
            "            if isinstance(knock_room_state, list):",
            "                room_state = knock_room_state",
            "",
            "        # Ideally we'd reuse the logic in `calculate_room_name`, but that gets",
            "        # complicated to handle partial events vs pulling events from the DB.",
            "        for state_dict in room_state:",
            "            type_tuple = (state_dict[\"type\"], state_dict.get(\"state_key\"))",
            "            if type_tuple == (EventTypes.Member, ev.sender):",
            "                display_name = state_dict[\"content\"].get(\"displayname\")",
            "                if display_name:",
            "                    ctx[\"sender_display_name\"] = display_name",
            "            elif type_tuple == (EventTypes.Name, \"\"):",
            "                room_name = state_dict[\"content\"].get(\"name\")",
            "                if room_name:",
            "                    ctx[\"name\"] = room_name",
            "",
            "        return ctx",
            "",
            "    room_state_ids = await storage.state.get_state_ids_for_event(ev.event_id)",
            "",
            "    # we no longer bother setting room_alias, and make room_name the",
            "    # human-readable name instead, be that m.room.name, an alias or",
            "    # a list of people in the room",
            "    name = await calculate_room_name(",
            "        storage.main, room_state_ids, user_id, fallback_to_single_member=False",
            "    )",
            "    if name:",
            "        ctx[\"name\"] = name",
            "",
            "    sender_state_event_id = room_state_ids[(\"m.room.member\", ev.sender)]",
            "    sender_state_event = await storage.main.get_event(sender_state_event_id)",
            "    ctx[\"sender_display_name\"] = name_from_member_event(sender_state_event)",
            "",
            "    return ctx"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "77": [
                "room_state"
            ],
            "79": [
                "room_state"
            ]
        },
        "addLocation": []
    },
    "synapse/rest/client/sync.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 436,
                "afterPatchRowNumber": 436,
                "PatchRowcode": "             )"
            },
            "1": {
                "beforePatchRowNumber": 437,
                "afterPatchRowNumber": 437,
                "PatchRowcode": "             unsigned = dict(invite.get(\"unsigned\", {}))"
            },
            "2": {
                "beforePatchRowNumber": 438,
                "afterPatchRowNumber": 438,
                "PatchRowcode": "             invite[\"unsigned\"] = unsigned"
            },
            "3": {
                "beforePatchRowNumber": 439,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            invited_state = list(unsigned.pop(\"invite_room_state\", []))"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 439,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 440,
                "PatchRowcode": "+            invited_state = unsigned.pop(\"invite_room_state\", [])"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 441,
                "PatchRowcode": "+            if not isinstance(invited_state, list):"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 442,
                "PatchRowcode": "+                invited_state = []"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 443,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 444,
                "PatchRowcode": "+            invited_state = list(invited_state)"
            },
            "10": {
                "beforePatchRowNumber": 440,
                "afterPatchRowNumber": 445,
                "PatchRowcode": "             invited_state.append(invite)"
            },
            "11": {
                "beforePatchRowNumber": 441,
                "afterPatchRowNumber": 446,
                "PatchRowcode": "             invited[room.room_id] = {\"invite_state\": {\"events\": invited_state}}"
            },
            "12": {
                "beforePatchRowNumber": 442,
                "afterPatchRowNumber": 447,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 481,
                "PatchRowcode": "             # Extract the stripped room state from the unsigned dict"
            },
            "14": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": 482,
                "PatchRowcode": "             # This is for clients to get a little bit of information about"
            },
            "15": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": 483,
                "PatchRowcode": "             # the room they've knocked on, without revealing any sensitive information"
            },
            "16": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            knocked_state = list(unsigned.pop(\"knock_room_state\", []))"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 484,
                "PatchRowcode": "+            knocked_state = unsigned.pop(\"knock_room_state\", [])"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 485,
                "PatchRowcode": "+            if not isinstance(knocked_state, list):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 486,
                "PatchRowcode": "+                knocked_state = []"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 487,
                "PatchRowcode": "+            knocked_state = list(knocked_state)"
            },
            "21": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": 488,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 481,
                "afterPatchRowNumber": 489,
                "PatchRowcode": "             # Append the actual knock membership event itself as well. This provides"
            },
            "23": {
                "beforePatchRowNumber": 482,
                "afterPatchRowNumber": 490,
                "PatchRowcode": "             # the client with:"
            }
        },
        "frontPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "import itertools",
            "import logging",
            "from collections import defaultdict",
            "from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "",
            "from synapse.api.constants import AccountDataTypes, EduTypes, Membership, PresenceState",
            "from synapse.api.errors import Codes, StoreError, SynapseError",
            "from synapse.api.filtering import FilterCollection",
            "from synapse.api.presence import UserPresenceState",
            "from synapse.events.utils import (",
            "    SerializeEventConfig,",
            "    format_event_for_client_v2_without_room_id,",
            "    format_event_raw,",
            ")",
            "from synapse.handlers.presence import format_user_presence_state",
            "from synapse.handlers.sliding_sync import SlidingSyncConfig, SlidingSyncResult",
            "from synapse.handlers.sync import (",
            "    ArchivedSyncResult,",
            "    InvitedSyncResult,",
            "    JoinedSyncResult,",
            "    KnockedSyncResult,",
            "    SyncConfig,",
            "    SyncResult,",
            "    SyncVersion,",
            ")",
            "from synapse.http.server import HttpServer",
            "from synapse.http.servlet import (",
            "    RestServlet,",
            "    parse_and_validate_json_object_from_request,",
            "    parse_boolean,",
            "    parse_integer,",
            "    parse_string,",
            ")",
            "from synapse.http.site import SynapseRequest",
            "from synapse.logging.opentracing import log_kv, set_tag, trace_with_opname",
            "from synapse.rest.admin.experimental_features import ExperimentalFeature",
            "from synapse.types import JsonDict, Requester, SlidingSyncStreamToken, StreamToken",
            "from synapse.types.rest.client import SlidingSyncBody",
            "from synapse.util import json_decoder",
            "from synapse.util.caches.lrucache import LruCache",
            "",
            "from ._base import client_patterns, set_timeline_upper_limit",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SyncRestServlet(RestServlet):",
            "    \"\"\"",
            "",
            "    GET parameters::",
            "        timeout(int): How long to wait for new events in milliseconds.",
            "        since(batch_token): Batch token when asking for incremental deltas.",
            "        set_presence(str): What state the device presence should be set to.",
            "            default is \"online\".",
            "        filter(filter_id): A filter to apply to the events returned.",
            "",
            "    Response JSON::",
            "        {",
            "          \"next_batch\": // batch token for the next /sync",
            "          \"presence\": // presence data for the user.",
            "          \"rooms\": {",
            "            \"join\": { // Joined rooms being updated.",
            "              \"${room_id}\": { // Id of the room being updated",
            "                \"event_map\": // Map of EventID -> event JSON.",
            "                \"timeline\": { // The recent events in the room if gap is \"true\"",
            "                  \"limited\": // Was the per-room event limit exceeded?",
            "                             // otherwise the next events in the room.",
            "                  \"events\": [] // list of EventIDs in the \"event_map\".",
            "                  \"prev_batch\": // back token for getting previous events.",
            "                }",
            "                \"state\": {\"events\": []} // list of EventIDs updating the",
            "                                        // current state to be what it should",
            "                                        // be at the end of the batch.",
            "                \"ephemeral\": {\"events\": []} // list of event objects",
            "              }",
            "            },",
            "            \"invite\": {}, // Invited rooms being updated.",
            "            \"leave\": {} // Archived rooms being updated.",
            "          }",
            "        }",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(\"/sync$\")",
            "    ALLOWED_PRESENCE = {\"online\", \"offline\", \"unavailable\"}",
            "    CATEGORY = \"Sync requests\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__()",
            "        self.hs = hs",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastores().main",
            "        self.sync_handler = hs.get_sync_handler()",
            "        self.clock = hs.get_clock()",
            "        self.filtering = hs.get_filtering()",
            "        self.presence_handler = hs.get_presence_handler()",
            "        self._server_notices_sender = hs.get_server_notices_sender()",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._msc2654_enabled = hs.config.experimental.msc2654_enabled",
            "        self._msc3773_enabled = hs.config.experimental.msc3773_enabled",
            "",
            "        self._json_filter_cache: LruCache[str, bool] = LruCache(",
            "            max_size=1000,",
            "            cache_name=\"sync_valid_filter\",",
            "        )",
            "",
            "    async def on_GET(self, request: SynapseRequest) -> Tuple[int, JsonDict]:",
            "        # This will always be set by the time Twisted calls us.",
            "        assert request.args is not None",
            "",
            "        if b\"from\" in request.args:",
            "            # /events used to use 'from', but /sync uses 'since'.",
            "            # Lets be helpful and whine if we see a 'from'.",
            "            raise SynapseError(",
            "                400, \"'from' is not a valid query parameter. Did you mean 'since'?\"",
            "            )",
            "",
            "        requester = await self.auth.get_user_by_req(request, allow_guest=True)",
            "        user = requester.user",
            "        device_id = requester.device_id",
            "",
            "        timeout = parse_integer(request, \"timeout\", default=0)",
            "        since = parse_string(request, \"since\")",
            "        set_presence = parse_string(",
            "            request,",
            "            \"set_presence\",",
            "            default=\"online\",",
            "            allowed_values=self.ALLOWED_PRESENCE,",
            "        )",
            "        filter_id = parse_string(request, \"filter\")",
            "        full_state = parse_boolean(request, \"full_state\", default=False)",
            "",
            "        use_state_after = False",
            "        if await self.store.is_feature_enabled(",
            "            user.to_string(), ExperimentalFeature.MSC4222",
            "        ):",
            "            use_state_after = parse_boolean(",
            "                request, \"org.matrix.msc4222.use_state_after\", default=False",
            "            )",
            "",
            "        logger.debug(",
            "            \"/sync: user=%r, timeout=%r, since=%r, \"",
            "            \"set_presence=%r, filter_id=%r, device_id=%r\",",
            "            user,",
            "            timeout,",
            "            since,",
            "            set_presence,",
            "            filter_id,",
            "            device_id,",
            "        )",
            "",
            "        # Stream position of the last ignored users account data event for this user,",
            "        # if we're initial syncing.",
            "        # We include this in the request key to invalidate an initial sync",
            "        # in the response cache once the set of ignored users has changed.",
            "        # (We filter out ignored users from timeline events, so our sync response",
            "        # is invalid once the set of ignored users changes.)",
            "        last_ignore_accdata_streampos: Optional[int] = None",
            "        if not since:",
            "            # No `since`, so this is an initial sync.",
            "            last_ignore_accdata_streampos = await self.store.get_latest_stream_id_for_global_account_data_by_type_for_user(",
            "                user.to_string(), AccountDataTypes.IGNORED_USER_LIST",
            "            )",
            "",
            "        request_key = (",
            "            user,",
            "            timeout,",
            "            since,",
            "            filter_id,",
            "            full_state,",
            "            device_id,",
            "            last_ignore_accdata_streampos,",
            "            use_state_after,",
            "        )",
            "",
            "        if filter_id is None:",
            "            filter_collection = self.filtering.DEFAULT_FILTER_COLLECTION",
            "        elif filter_id.startswith(\"{\"):",
            "            try:",
            "                filter_object = json_decoder.decode(filter_id)",
            "            except Exception:",
            "                raise SynapseError(400, \"Invalid filter JSON\", errcode=Codes.NOT_JSON)",
            "",
            "            # We cache the validation, as this can get quite expensive if people use",
            "            # a literal json blob as a query param.",
            "            if not self._json_filter_cache.get(filter_id):",
            "                self.filtering.check_valid_filter(filter_object)",
            "                self._json_filter_cache[filter_id] = True",
            "",
            "            set_timeline_upper_limit(",
            "                filter_object, self.hs.config.server.filter_timeline_limit",
            "            )",
            "            filter_collection = FilterCollection(self.hs, filter_object)",
            "        else:",
            "            try:",
            "                filter_collection = await self.filtering.get_user_filter(",
            "                    user, filter_id",
            "                )",
            "            except StoreError as err:",
            "                if err.code != 404:",
            "                    raise",
            "                # fix up the description and errcode to be more useful",
            "                raise SynapseError(400, \"No such filter\", errcode=Codes.INVALID_PARAM)",
            "",
            "        sync_config = SyncConfig(",
            "            user=user,",
            "            filter_collection=filter_collection,",
            "            is_guest=requester.is_guest,",
            "            device_id=device_id,",
            "            use_state_after=use_state_after,",
            "        )",
            "",
            "        since_token = None",
            "        if since is not None:",
            "            since_token = await StreamToken.from_string(self.store, since)",
            "",
            "        # send any outstanding server notices to the user.",
            "        await self._server_notices_sender.on_user_syncing(user.to_string())",
            "",
            "        affect_presence = set_presence != PresenceState.OFFLINE",
            "",
            "        context = await self.presence_handler.user_syncing(",
            "            user.to_string(),",
            "            requester.device_id,",
            "            affect_presence=affect_presence,",
            "            presence_state=set_presence,",
            "        )",
            "        with context:",
            "            sync_result = await self.sync_handler.wait_for_sync_for_user(",
            "                requester,",
            "                sync_config,",
            "                SyncVersion.SYNC_V2,",
            "                request_key,",
            "                since_token=since_token,",
            "                timeout=timeout,",
            "                full_state=full_state,",
            "            )",
            "",
            "        # the client may have disconnected by now; don't bother to serialize the",
            "        # response if so.",
            "        if request._disconnected:",
            "            logger.info(\"Client has disconnected; not serializing response.\")",
            "            return 200, {}",
            "",
            "        time_now = self.clock.time_msec()",
            "        # We know that the the requester has an access token since appservices",
            "        # cannot use sync.",
            "        response_content = await self.encode_response(",
            "            time_now, sync_config, sync_result, requester, filter_collection",
            "        )",
            "",
            "        logger.debug(\"Event formatting complete\")",
            "        return 200, response_content",
            "",
            "    @trace_with_opname(\"sync.encode_response\")",
            "    async def encode_response(",
            "        self,",
            "        time_now: int,",
            "        sync_config: SyncConfig,",
            "        sync_result: SyncResult,",
            "        requester: Requester,",
            "        filter: FilterCollection,",
            "    ) -> JsonDict:",
            "        logger.debug(\"Formatting events in sync response\")",
            "        if filter.event_format == \"client\":",
            "            event_formatter = format_event_for_client_v2_without_room_id",
            "        elif filter.event_format == \"federation\":",
            "            event_formatter = format_event_raw",
            "        else:",
            "            raise Exception(\"Unknown event format %s\" % (filter.event_format,))",
            "",
            "        serialize_options = SerializeEventConfig(",
            "            event_format=event_formatter,",
            "            requester=requester,",
            "            only_event_fields=filter.event_fields,",
            "        )",
            "        stripped_serialize_options = SerializeEventConfig(",
            "            event_format=event_formatter,",
            "            requester=requester,",
            "            include_stripped_room_state=True,",
            "        )",
            "",
            "        joined = await self.encode_joined(",
            "            sync_config, sync_result.joined, time_now, serialize_options",
            "        )",
            "",
            "        invited = await self.encode_invited(",
            "            sync_result.invited, time_now, stripped_serialize_options",
            "        )",
            "",
            "        knocked = await self.encode_knocked(",
            "            sync_result.knocked, time_now, stripped_serialize_options",
            "        )",
            "",
            "        archived = await self.encode_archived(",
            "            sync_config, sync_result.archived, time_now, serialize_options",
            "        )",
            "",
            "        logger.debug(\"building sync response dict\")",
            "",
            "        response: JsonDict = defaultdict(dict)",
            "        response[\"next_batch\"] = await sync_result.next_batch.to_string(self.store)",
            "",
            "        if sync_result.account_data:",
            "            response[\"account_data\"] = {\"events\": sync_result.account_data}",
            "        if sync_result.presence:",
            "            response[\"presence\"] = SyncRestServlet.encode_presence(",
            "                sync_result.presence, time_now",
            "            )",
            "",
            "        if sync_result.to_device:",
            "            response[\"to_device\"] = {\"events\": sync_result.to_device}",
            "",
            "        if sync_result.device_lists.changed:",
            "            response[\"device_lists\"][\"changed\"] = list(sync_result.device_lists.changed)",
            "        if sync_result.device_lists.left:",
            "            response[\"device_lists\"][\"left\"] = list(sync_result.device_lists.left)",
            "",
            "        # We always include this because https://github.com/vector-im/element-android/issues/3725",
            "        # The spec isn't terribly clear on when this can be omitted and how a client would tell",
            "        # the difference between \"no keys present\" and \"nothing changed\" in terms of whole field",
            "        # absent / individual key type entry absent",
            "        # Corresponding synapse issue: https://github.com/matrix-org/synapse/issues/10456",
            "        response[\"device_one_time_keys_count\"] = sync_result.device_one_time_keys_count",
            "",
            "        # https://github.com/matrix-org/matrix-doc/blob/54255851f642f84a4f1aaf7bc063eebe3d76752b/proposals/2732-olm-fallback-keys.md",
            "        # states that this field should always be included, as long as the server supports the feature.",
            "        response[\"org.matrix.msc2732.device_unused_fallback_key_types\"] = (",
            "            sync_result.device_unused_fallback_key_types",
            "        )",
            "        response[\"device_unused_fallback_key_types\"] = (",
            "            sync_result.device_unused_fallback_key_types",
            "        )",
            "",
            "        if joined:",
            "            response[\"rooms\"][Membership.JOIN] = joined",
            "        if invited:",
            "            response[\"rooms\"][Membership.INVITE] = invited",
            "        if knocked:",
            "            response[\"rooms\"][Membership.KNOCK] = knocked",
            "        if archived:",
            "            response[\"rooms\"][Membership.LEAVE] = archived",
            "",
            "        return response",
            "",
            "    @staticmethod",
            "    def encode_presence(events: List[UserPresenceState], time_now: int) -> JsonDict:",
            "        return {",
            "            \"events\": [",
            "                {",
            "                    \"type\": EduTypes.PRESENCE,",
            "                    \"sender\": event.user_id,",
            "                    \"content\": format_user_presence_state(",
            "                        event, time_now, include_user_id=False",
            "                    ),",
            "                }",
            "                for event in events",
            "            ]",
            "        }",
            "",
            "    @trace_with_opname(\"sync.encode_joined\")",
            "    async def encode_joined(",
            "        self,",
            "        sync_config: SyncConfig,",
            "        rooms: List[JoinedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Encode the joined rooms in a sync result",
            "",
            "        Args:",
            "            sync_config",
            "            rooms: list of sync results for rooms this user is joined to",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "        Returns:",
            "            The joined rooms list, in our response format",
            "        \"\"\"",
            "        joined = {}",
            "        for room in rooms:",
            "            joined[room.room_id] = await self.encode_room(",
            "                sync_config,",
            "                room,",
            "                time_now,",
            "                joined=True,",
            "                serialize_options=serialize_options,",
            "            )",
            "",
            "        return joined",
            "",
            "    @trace_with_opname(\"sync.encode_invited\")",
            "    async def encode_invited(",
            "        self,",
            "        rooms: List[InvitedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Encode the invited rooms in a sync result",
            "",
            "        Args:",
            "            rooms: list of sync results for rooms this user is invited to",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "",
            "        Returns:",
            "            The invited rooms list, in our response format",
            "        \"\"\"",
            "        invited = {}",
            "        for room in rooms:",
            "            invite = await self._event_serializer.serialize_event(",
            "                room.invite, time_now, config=serialize_options",
            "            )",
            "            unsigned = dict(invite.get(\"unsigned\", {}))",
            "            invite[\"unsigned\"] = unsigned",
            "            invited_state = list(unsigned.pop(\"invite_room_state\", []))",
            "            invited_state.append(invite)",
            "            invited[room.room_id] = {\"invite_state\": {\"events\": invited_state}}",
            "",
            "        return invited",
            "",
            "    @trace_with_opname(\"sync.encode_knocked\")",
            "    async def encode_knocked(",
            "        self,",
            "        rooms: List[KnockedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> Dict[str, Dict[str, Any]]:",
            "        \"\"\"",
            "        Encode the rooms we've knocked on in a sync result.",
            "",
            "        Args:",
            "            rooms: list of sync results for rooms this user is knocking on",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "",
            "        Returns:",
            "            The list of rooms the user has knocked on, in our response format.",
            "        \"\"\"",
            "        knocked = {}",
            "        for room in rooms:",
            "            knock = await self._event_serializer.serialize_event(",
            "                room.knock, time_now, config=serialize_options",
            "            )",
            "",
            "            # Extract the `unsigned` key from the knock event.",
            "            # This is where we (cheekily) store the knock state events",
            "            unsigned = knock.setdefault(\"unsigned\", {})",
            "",
            "            # Duplicate the dictionary in order to avoid modifying the original",
            "            unsigned = dict(unsigned)",
            "",
            "            # Extract the stripped room state from the unsigned dict",
            "            # This is for clients to get a little bit of information about",
            "            # the room they've knocked on, without revealing any sensitive information",
            "            knocked_state = list(unsigned.pop(\"knock_room_state\", []))",
            "",
            "            # Append the actual knock membership event itself as well. This provides",
            "            # the client with:",
            "            #",
            "            # * A knock state event that they can use for easier internal tracking",
            "            # * The rough timestamp of when the knock occurred contained within the event",
            "            knocked_state.append(knock)",
            "",
            "            # Build the `knock_state` dictionary, which will contain the state of the",
            "            # room that the client has knocked on",
            "            knocked[room.room_id] = {\"knock_state\": {\"events\": knocked_state}}",
            "",
            "        return knocked",
            "",
            "    @trace_with_opname(\"sync.encode_archived\")",
            "    async def encode_archived(",
            "        self,",
            "        sync_config: SyncConfig,",
            "        rooms: List[ArchivedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Encode the archived rooms in a sync result",
            "",
            "        Args:",
            "            sync_config",
            "            rooms: list of sync results for rooms this user is joined to",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "        Returns:",
            "            The archived rooms list, in our response format",
            "        \"\"\"",
            "        joined = {}",
            "        for room in rooms:",
            "            joined[room.room_id] = await self.encode_room(",
            "                sync_config,",
            "                room,",
            "                time_now,",
            "                joined=False,",
            "                serialize_options=serialize_options,",
            "            )",
            "",
            "        return joined",
            "",
            "    async def encode_room(",
            "        self,",
            "        sync_config: SyncConfig,",
            "        room: Union[JoinedSyncResult, ArchivedSyncResult],",
            "        time_now: int,",
            "        joined: bool,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Args:",
            "            sync_config",
            "            room: sync result for a single room",
            "            time_now: current time - used as a baseline for age calculations",
            "            token_id: ID of the user's auth token - used for namespacing",
            "                of transaction IDs",
            "            joined: True if the user is joined to this room - will mean",
            "                we handle ephemeral events",
            "            only_fields: Optional. The list of event fields to include.",
            "            event_formatter: function to convert from federation format",
            "                to client format",
            "        Returns:",
            "            The room, encoded in our response format",
            "        \"\"\"",
            "        state_dict = room.state",
            "        timeline_events = room.timeline.events",
            "",
            "        state_events = state_dict.values()",
            "",
            "        for event in itertools.chain(state_events, timeline_events):",
            "            # We've had bug reports that events were coming down under the",
            "            # wrong room.",
            "            if event.room_id != room.room_id:",
            "                logger.warning(",
            "                    \"Event %r is under room %r instead of %r\",",
            "                    event.event_id,",
            "                    room.room_id,",
            "                    event.room_id,",
            "                )",
            "",
            "        serialized_state = await self._event_serializer.serialize_events(",
            "            state_events, time_now, config=serialize_options",
            "        )",
            "        serialized_timeline = await self._event_serializer.serialize_events(",
            "            timeline_events,",
            "            time_now,",
            "            config=serialize_options,",
            "            bundle_aggregations=room.timeline.bundled_aggregations,",
            "        )",
            "",
            "        account_data = room.account_data",
            "",
            "        # We either include a `state` or `state_after` field depending on",
            "        # whether the client has opted in to the newer `state_after` behavior.",
            "        if sync_config.use_state_after:",
            "            state_key_name = \"org.matrix.msc4222.state_after\"",
            "        else:",
            "            state_key_name = \"state\"",
            "",
            "        result: JsonDict = {",
            "            \"timeline\": {",
            "                \"events\": serialized_timeline,",
            "                \"prev_batch\": await room.timeline.prev_batch.to_string(self.store),",
            "                \"limited\": room.timeline.limited,",
            "            },",
            "            state_key_name: {\"events\": serialized_state},",
            "            \"account_data\": {\"events\": account_data},",
            "        }",
            "",
            "        if joined:",
            "            assert isinstance(room, JoinedSyncResult)",
            "            ephemeral_events = room.ephemeral",
            "            result[\"ephemeral\"] = {\"events\": ephemeral_events}",
            "            result[\"unread_notifications\"] = room.unread_notifications",
            "            if room.unread_thread_notifications:",
            "                result[\"unread_thread_notifications\"] = room.unread_thread_notifications",
            "                if self._msc3773_enabled:",
            "                    result[\"org.matrix.msc3773.unread_thread_notifications\"] = (",
            "                        room.unread_thread_notifications",
            "                    )",
            "            result[\"summary\"] = room.summary",
            "            if self._msc2654_enabled:",
            "                result[\"org.matrix.msc2654.unread_count\"] = room.unread_count",
            "",
            "        return result",
            "",
            "",
            "class SlidingSyncE2eeRestServlet(RestServlet):",
            "    \"\"\"",
            "    API endpoint for MSC3575 Sliding Sync `/sync/e2ee`. This is being introduced as part",
            "    of Sliding Sync but doesn't have any sliding window component. It's just a way to",
            "    get E2EE events without having to sit through a big initial sync (`/sync` v2). And",
            "    we can avoid encryption events being backed up by the main sync response.",
            "",
            "    Having To-Device messages split out to this sync endpoint also helps when clients",
            "    need to have 2 or more sync streams open at a time, e.g a push notification process",
            "    and a main process. This can cause the two processes to race to fetch the To-Device",
            "    events, resulting in the need for complex synchronisation rules to ensure the token",
            "    is correctly and atomically exchanged between processes.",
            "",
            "    GET parameters::",
            "        timeout(int): How long to wait for new events in milliseconds.",
            "        since(batch_token): Batch token when asking for incremental deltas.",
            "",
            "    Response JSON::",
            "        {",
            "            \"next_batch\": // batch token for the next /sync",
            "            \"to_device\": {",
            "                // list of to-device events",
            "                \"events\": [",
            "                    {",
            "                        \"content: { \"algorithm\": \"m.olm.v1.curve25519-aes-sha2\", \"ciphertext\": { ... }, \"org.matrix.msgid\": \"abcd\", \"session_id\": \"abcd\" },",
            "                        \"type\": \"m.room.encrypted\",",
            "                        \"sender\": \"@alice:example.com\",",
            "                    }",
            "                    // ...",
            "                ]",
            "            },",
            "            \"device_lists\": {",
            "                \"changed\": [\"@alice:example.com\"],",
            "                \"left\": [\"@bob:example.com\"]",
            "            },",
            "            \"device_one_time_keys_count\": {",
            "                \"signed_curve25519\": 50",
            "            },",
            "            \"device_unused_fallback_key_types\": [",
            "                \"signed_curve25519\"",
            "            ]",
            "        }",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(",
            "        \"/org.matrix.msc3575/sync/e2ee$\", releases=[], v1=False, unstable=True",
            "    )",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__()",
            "        self.hs = hs",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastores().main",
            "        self.sync_handler = hs.get_sync_handler()",
            "",
            "        # Filtering only matters for the `device_lists` because it requires a bunch of",
            "        # derived information from rooms (see how `_generate_sync_entry_for_rooms()`",
            "        # prepares a bunch of data for `_generate_sync_entry_for_device_list()`).",
            "        self.only_member_events_filter_collection = FilterCollection(",
            "            self.hs,",
            "            {",
            "                \"room\": {",
            "                    # We only care about membership events for the `device_lists`.",
            "                    # Membership will tell us whether a user has joined/left a room and",
            "                    # if there are new devices to encrypt for.",
            "                    \"timeline\": {",
            "                        \"types\": [\"m.room.member\"],",
            "                    },",
            "                    \"state\": {",
            "                        \"types\": [\"m.room.member\"],",
            "                    },",
            "                    # We don't want any extra account_data generated because it's not",
            "                    # returned by this endpoint. This helps us avoid work in",
            "                    # `_generate_sync_entry_for_rooms()`",
            "                    \"account_data\": {",
            "                        \"not_types\": [\"*\"],",
            "                    },",
            "                    # We don't want any extra ephemeral data generated because it's not",
            "                    # returned by this endpoint. This helps us avoid work in",
            "                    # `_generate_sync_entry_for_rooms()`",
            "                    \"ephemeral\": {",
            "                        \"not_types\": [\"*\"],",
            "                    },",
            "                },",
            "                # We don't want any extra account_data generated because it's not",
            "                # returned by this endpoint. (This is just here for good measure)",
            "                \"account_data\": {",
            "                    \"not_types\": [\"*\"],",
            "                },",
            "                # We don't want any extra presence data generated because it's not",
            "                # returned by this endpoint. (This is just here for good measure)",
            "                \"presence\": {",
            "                    \"not_types\": [\"*\"],",
            "                },",
            "            },",
            "        )",
            "",
            "    async def on_GET(self, request: SynapseRequest) -> Tuple[int, JsonDict]:",
            "        requester = await self.auth.get_user_by_req_experimental_feature(",
            "            request, allow_guest=True, feature=ExperimentalFeature.MSC3575",
            "        )",
            "        user = requester.user",
            "        device_id = requester.device_id",
            "",
            "        timeout = parse_integer(request, \"timeout\", default=0)",
            "        since = parse_string(request, \"since\")",
            "",
            "        sync_config = SyncConfig(",
            "            user=user,",
            "            filter_collection=self.only_member_events_filter_collection,",
            "            is_guest=requester.is_guest,",
            "            device_id=device_id,",
            "            use_state_after=False,  # We don't return any rooms so this flag is a no-op",
            "        )",
            "",
            "        since_token = None",
            "        if since is not None:",
            "            since_token = await StreamToken.from_string(self.store, since)",
            "",
            "        # Request cache key",
            "        request_key = (",
            "            SyncVersion.E2EE_SYNC,",
            "            user,",
            "            timeout,",
            "            since,",
            "        )",
            "",
            "        # Gather data for the response",
            "        sync_result = await self.sync_handler.wait_for_sync_for_user(",
            "            requester,",
            "            sync_config,",
            "            SyncVersion.E2EE_SYNC,",
            "            request_key,",
            "            since_token=since_token,",
            "            timeout=timeout,",
            "            full_state=False,",
            "        )",
            "",
            "        # The client may have disconnected by now; don't bother to serialize the",
            "        # response if so.",
            "        if request._disconnected:",
            "            logger.info(\"Client has disconnected; not serializing response.\")",
            "            return 200, {}",
            "",
            "        response: JsonDict = defaultdict(dict)",
            "        response[\"next_batch\"] = await sync_result.next_batch.to_string(self.store)",
            "",
            "        if sync_result.to_device:",
            "            response[\"to_device\"] = {\"events\": sync_result.to_device}",
            "",
            "        if sync_result.device_lists.changed:",
            "            response[\"device_lists\"][\"changed\"] = list(sync_result.device_lists.changed)",
            "        if sync_result.device_lists.left:",
            "            response[\"device_lists\"][\"left\"] = list(sync_result.device_lists.left)",
            "",
            "        # We always include this because https://github.com/vector-im/element-android/issues/3725",
            "        # The spec isn't terribly clear on when this can be omitted and how a client would tell",
            "        # the difference between \"no keys present\" and \"nothing changed\" in terms of whole field",
            "        # absent / individual key type entry absent",
            "        # Corresponding synapse issue: https://github.com/matrix-org/synapse/issues/10456",
            "        response[\"device_one_time_keys_count\"] = sync_result.device_one_time_keys_count",
            "",
            "        # https://github.com/matrix-org/matrix-doc/blob/54255851f642f84a4f1aaf7bc063eebe3d76752b/proposals/2732-olm-fallback-keys.md",
            "        # states that this field should always be included, as long as the server supports the feature.",
            "        response[\"device_unused_fallback_key_types\"] = (",
            "            sync_result.device_unused_fallback_key_types",
            "        )",
            "",
            "        return 200, response",
            "",
            "",
            "class SlidingSyncRestServlet(RestServlet):",
            "    \"\"\"",
            "    API endpoint for MSC3575 Sliding Sync `/sync`. Allows for clients to request a",
            "    subset (sliding window) of rooms, state, and timeline events (just what they need)",
            "    in order to bootstrap quickly and subscribe to only what the client cares about.",
            "    Because the client can specify what it cares about, we can respond quickly and skip",
            "    all of the work we would normally have to do with a sync v2 response.",
            "",
            "    Request query parameters:",
            "        timeout: How long to wait for new events in milliseconds.",
            "        pos: Stream position token when asking for incremental deltas.",
            "",
            "    Request body::",
            "        {",
            "            // Sliding Window API",
            "            \"lists\": {",
            "                \"foo-list\": {",
            "                    \"ranges\": [ [0, 99] ],",
            "                    \"required_state\": [",
            "                        [\"m.room.join_rules\", \"\"],",
            "                        [\"m.room.history_visibility\", \"\"],",
            "                        [\"m.space.child\", \"*\"]",
            "                    ],",
            "                    \"timeline_limit\": 10,",
            "                    \"filters\": {",
            "                        \"is_dm\": true",
            "                    },",
            "                }",
            "            },",
            "            // Room Subscriptions API",
            "            \"room_subscriptions\": {",
            "                \"!sub1:bar\": {",
            "                    \"required_state\": [ [\"*\",\"*\"] ],",
            "                    \"timeline_limit\": 10,",
            "                }",
            "            },",
            "            // Extensions API",
            "            \"extensions\": {}",
            "        }",
            "",
            "    Response JSON::",
            "        {",
            "            \"pos\": \"s58_224_0_13_10_1_1_16_0_1\",",
            "            \"lists\": {",
            "                \"foo-list\": {",
            "                    \"count\": 1337,",
            "                    \"ops\": [{",
            "                        \"op\": \"SYNC\",",
            "                        \"range\": [0, 99],",
            "                        \"room_ids\": [",
            "                            \"!foo:bar\",",
            "                            // ... 99 more room IDs",
            "                        ]",
            "                    }]",
            "                }",
            "            },",
            "            // Aggregated rooms from lists and room subscriptions",
            "            \"rooms\": {",
            "                // Room from room subscription",
            "                \"!sub1:bar\": {",
            "                    \"name\": \"Alice and Bob\",",
            "                    \"avatar\": \"mxc://...\",",
            "                    \"initial\": true,",
            "                    \"required_state\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.create\", \"state_key\":\"\", \"content\":{\"creator\":\"@alice:example.com\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.history_visibility\", \"state_key\":\"\", \"content\":{\"history_visibility\":\"joined\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.member\", \"state_key\":\"@alice:example.com\", \"content\":{\"membership\":\"join\"}}",
            "                    ],",
            "                    \"timeline\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.create\", \"state_key\":\"\", \"content\":{\"creator\":\"@alice:example.com\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.history_visibility\", \"state_key\":\"\", \"content\":{\"history_visibility\":\"joined\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.member\", \"state_key\":\"@alice:example.com\", \"content\":{\"membership\":\"join\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"A\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"B\"}},",
            "                    ],",
            "                    \"prev_batch\": \"t111_222_333\",",
            "                    \"joined_count\": 41,",
            "                    \"invited_count\": 1,",
            "                    \"notification_count\": 1,",
            "                    \"highlight_count\": 0,",
            "                    \"num_live\": 2\"",
            "                },",
            "                // rooms from list",
            "                \"!foo:bar\": {",
            "                    \"name\": \"The calculated room name\",",
            "                    \"avatar\": \"mxc://...\",",
            "                    \"initial\": true,",
            "                    \"required_state\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.history_visibility\", \"state_key\":\"\", \"content\":{\"history_visibility\":\"joined\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.space.child\", \"state_key\":\"!foo:example.com\", \"content\":{\"via\":[\"example.com\"]}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.space.child\", \"state_key\":\"!bar:example.com\", \"content\":{\"via\":[\"example.com\"]}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.space.child\", \"state_key\":\"!baz:example.com\", \"content\":{\"via\":[\"example.com\"]}}",
            "                    ],",
            "                    \"timeline\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"A\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"B\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"C\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"D\"}},",
            "                    ],",
            "                    \"prev_batch\": \"t111_222_333\",",
            "                    \"joined_count\": 4,",
            "                    \"invited_count\": 0,",
            "                    \"notification_count\": 54,",
            "                    \"highlight_count\": 3,",
            "                    \"num_live\": 1,",
            "                },",
            "                 // ... 99 more items",
            "            },",
            "            \"extensions\": {}",
            "        }",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(",
            "        \"/org.matrix.simplified_msc3575/sync$\", releases=[], v1=False, unstable=True",
            "    )",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__()",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastores().main",
            "        self.clock = hs.get_clock()",
            "        self.filtering = hs.get_filtering()",
            "        self.sliding_sync_handler = hs.get_sliding_sync_handler()",
            "        self.event_serializer = hs.get_event_client_serializer()",
            "",
            "    async def on_POST(self, request: SynapseRequest) -> Tuple[int, JsonDict]:",
            "        requester = await self.auth.get_user_by_req_experimental_feature(",
            "            request, allow_guest=True, feature=ExperimentalFeature.MSC3575",
            "        )",
            "",
            "        user = requester.user",
            "",
            "        timeout = parse_integer(request, \"timeout\", default=0)",
            "        # Position in the stream",
            "        from_token_string = parse_string(request, \"pos\")",
            "",
            "        from_token = None",
            "        if from_token_string is not None:",
            "            from_token = await SlidingSyncStreamToken.from_string(",
            "                self.store, from_token_string",
            "            )",
            "",
            "        # TODO: We currently don't know whether we're going to use sticky params or",
            "        # maybe some filters like sync v2  where they are built up once and referenced",
            "        # by filter ID. For now, we will just prototype with always passing everything",
            "        # in.",
            "        body = parse_and_validate_json_object_from_request(request, SlidingSyncBody)",
            "",
            "        # Tag and log useful data to differentiate requests.",
            "        set_tag(",
            "            \"sliding_sync.sync_type\", \"initial\" if from_token is None else \"incremental\"",
            "        )",
            "        set_tag(\"sliding_sync.conn_id\", body.conn_id or \"\")",
            "        log_kv(",
            "            {",
            "                \"sliding_sync.lists\": {",
            "                    list_name: {",
            "                        \"ranges\": list_config.ranges,",
            "                        \"timeline_limit\": list_config.timeline_limit,",
            "                    }",
            "                    for list_name, list_config in (body.lists or {}).items()",
            "                },",
            "                \"sliding_sync.room_subscriptions\": list(",
            "                    (body.room_subscriptions or {}).keys()",
            "                ),",
            "                # We also include the number of room subscriptions because logs are",
            "                # limited to 1024 characters and the large room ID list above can be cut",
            "                # off.",
            "                \"sliding_sync.num_room_subscriptions\": len(",
            "                    (body.room_subscriptions or {}).keys()",
            "                ),",
            "            }",
            "        )",
            "",
            "        sync_config = SlidingSyncConfig(",
            "            user=user,",
            "            requester=requester,",
            "            # FIXME: Currently, we're just manually copying the fields from the",
            "            # `SlidingSyncBody` into the config. How can we guarantee into the future",
            "            # that we don't forget any? I would like something more structured like",
            "            # `copy_attributes(from=body, to=config)`",
            "            conn_id=body.conn_id,",
            "            lists=body.lists,",
            "            room_subscriptions=body.room_subscriptions,",
            "            extensions=body.extensions,",
            "        )",
            "",
            "        sliding_sync_results = await self.sliding_sync_handler.wait_for_sync_for_user(",
            "            requester,",
            "            sync_config,",
            "            from_token,",
            "            timeout,",
            "        )",
            "",
            "        # The client may have disconnected by now; don't bother to serialize the",
            "        # response if so.",
            "        if request._disconnected:",
            "            logger.info(\"Client has disconnected; not serializing response.\")",
            "            return 200, {}",
            "",
            "        response_content = await self.encode_response(requester, sliding_sync_results)",
            "",
            "        return 200, response_content",
            "",
            "    async def encode_response(",
            "        self,",
            "        requester: Requester,",
            "        sliding_sync_result: SlidingSyncResult,",
            "    ) -> JsonDict:",
            "        response: JsonDict = defaultdict(dict)",
            "",
            "        response[\"pos\"] = await sliding_sync_result.next_pos.to_string(self.store)",
            "        serialized_lists = self.encode_lists(sliding_sync_result.lists)",
            "        if serialized_lists:",
            "            response[\"lists\"] = serialized_lists",
            "        response[\"rooms\"] = await self.encode_rooms(",
            "            requester, sliding_sync_result.rooms",
            "        )",
            "        response[\"extensions\"] = await self.encode_extensions(",
            "            requester, sliding_sync_result.extensions",
            "        )",
            "",
            "        return response",
            "",
            "    def encode_lists(",
            "        self, lists: Mapping[str, SlidingSyncResult.SlidingWindowList]",
            "    ) -> JsonDict:",
            "        def encode_operation(",
            "            operation: SlidingSyncResult.SlidingWindowList.Operation,",
            "        ) -> JsonDict:",
            "            return {",
            "                \"op\": operation.op.value,",
            "                \"range\": operation.range,",
            "                \"room_ids\": operation.room_ids,",
            "            }",
            "",
            "        serialized_lists = {}",
            "        for list_key, list_result in lists.items():",
            "            serialized_lists[list_key] = {",
            "                \"count\": list_result.count,",
            "                \"ops\": [encode_operation(op) for op in list_result.ops],",
            "            }",
            "",
            "        return serialized_lists",
            "",
            "    async def encode_rooms(",
            "        self,",
            "        requester: Requester,",
            "        rooms: Dict[str, SlidingSyncResult.RoomResult],",
            "    ) -> JsonDict:",
            "        time_now = self.clock.time_msec()",
            "",
            "        serialize_options = SerializeEventConfig(",
            "            event_format=format_event_for_client_v2_without_room_id,",
            "            requester=requester,",
            "        )",
            "",
            "        serialized_rooms: Dict[str, JsonDict] = {}",
            "        for room_id, room_result in rooms.items():",
            "            serialized_rooms[room_id] = {",
            "                \"notification_count\": room_result.notification_count,",
            "                \"highlight_count\": room_result.highlight_count,",
            "            }",
            "",
            "            if room_result.bump_stamp is not None:",
            "                serialized_rooms[room_id][\"bump_stamp\"] = room_result.bump_stamp",
            "",
            "            if room_result.joined_count is not None:",
            "                serialized_rooms[room_id][\"joined_count\"] = room_result.joined_count",
            "",
            "            if room_result.invited_count is not None:",
            "                serialized_rooms[room_id][\"invited_count\"] = room_result.invited_count",
            "",
            "            if room_result.name:",
            "                serialized_rooms[room_id][\"name\"] = room_result.name",
            "",
            "            if room_result.avatar:",
            "                serialized_rooms[room_id][\"avatar\"] = room_result.avatar",
            "",
            "            if room_result.heroes is not None and len(room_result.heroes) > 0:",
            "                serialized_heroes = []",
            "                for hero in room_result.heroes:",
            "                    serialized_hero = {",
            "                        \"user_id\": hero.user_id,",
            "                    }",
            "                    if hero.display_name is not None:",
            "                        # Not a typo, just how \"displayname\" is spelled in the spec",
            "                        serialized_hero[\"displayname\"] = hero.display_name",
            "",
            "                    if hero.avatar_url is not None:",
            "                        serialized_hero[\"avatar_url\"] = hero.avatar_url",
            "",
            "                    serialized_heroes.append(serialized_hero)",
            "                serialized_rooms[room_id][\"heroes\"] = serialized_heroes",
            "",
            "            # We should only include the `initial` key if it's `True` to save bandwidth.",
            "            # The absence of this flag means `False`.",
            "            if room_result.initial:",
            "                serialized_rooms[room_id][\"initial\"] = room_result.initial",
            "",
            "            if room_result.unstable_expanded_timeline:",
            "                serialized_rooms[room_id][\"unstable_expanded_timeline\"] = (",
            "                    room_result.unstable_expanded_timeline",
            "                )",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if (",
            "                room_result.required_state is not None",
            "                and len(room_result.required_state) > 0",
            "            ):",
            "                serialized_required_state = (",
            "                    await self.event_serializer.serialize_events(",
            "                        room_result.required_state,",
            "                        time_now,",
            "                        config=serialize_options,",
            "                    )",
            "                )",
            "                serialized_rooms[room_id][\"required_state\"] = serialized_required_state",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if (",
            "                room_result.timeline_events is not None",
            "                and len(room_result.timeline_events) > 0",
            "            ):",
            "                serialized_timeline = await self.event_serializer.serialize_events(",
            "                    room_result.timeline_events,",
            "                    time_now,",
            "                    config=serialize_options,",
            "                    bundle_aggregations=room_result.bundled_aggregations,",
            "                )",
            "                serialized_rooms[room_id][\"timeline\"] = serialized_timeline",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if room_result.limited is not None:",
            "                serialized_rooms[room_id][\"limited\"] = room_result.limited",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if room_result.prev_batch is not None:",
            "                serialized_rooms[room_id][",
            "                    \"prev_batch\"",
            "                ] = await room_result.prev_batch.to_string(self.store)",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if room_result.num_live is not None:",
            "                serialized_rooms[room_id][\"num_live\"] = room_result.num_live",
            "",
            "            # Field should be absent on non-DM rooms",
            "            if room_result.is_dm:",
            "                serialized_rooms[room_id][\"is_dm\"] = room_result.is_dm",
            "",
            "            # Stripped state only applies to invite/knock rooms",
            "            if (",
            "                room_result.stripped_state is not None",
            "                and len(room_result.stripped_state) > 0",
            "            ):",
            "                # TODO: `knocked_state` but that isn't specced yet.",
            "                #",
            "                # TODO: Instead of adding `knocked_state`, it would be good to rename",
            "                # this to `stripped_state` so it can be shared between invite and knock",
            "                # rooms, see",
            "                # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1117629919",
            "                serialized_rooms[room_id][\"invite_state\"] = room_result.stripped_state",
            "",
            "        return serialized_rooms",
            "",
            "    async def encode_extensions(",
            "        self, requester: Requester, extensions: SlidingSyncResult.Extensions",
            "    ) -> JsonDict:",
            "        serialized_extensions: JsonDict = {}",
            "",
            "        if extensions.to_device is not None:",
            "            serialized_extensions[\"to_device\"] = {",
            "                \"next_batch\": extensions.to_device.next_batch,",
            "                \"events\": extensions.to_device.events,",
            "            }",
            "",
            "        if extensions.e2ee is not None:",
            "            serialized_extensions[\"e2ee\"] = {",
            "                # We always include this because",
            "                # https://github.com/vector-im/element-android/issues/3725. The spec",
            "                # isn't terribly clear on when this can be omitted and how a client",
            "                # would tell the difference between \"no keys present\" and \"nothing",
            "                # changed\" in terms of whole field absent / individual key type entry",
            "                # absent Corresponding synapse issue:",
            "                # https://github.com/matrix-org/synapse/issues/10456",
            "                \"device_one_time_keys_count\": extensions.e2ee.device_one_time_keys_count,",
            "                # https://github.com/matrix-org/matrix-doc/blob/54255851f642f84a4f1aaf7bc063eebe3d76752b/proposals/2732-olm-fallback-keys.md",
            "                # states that this field should always be included, as long as the",
            "                # server supports the feature.",
            "                \"device_unused_fallback_key_types\": extensions.e2ee.device_unused_fallback_key_types,",
            "            }",
            "",
            "            if extensions.e2ee.device_list_updates is not None:",
            "                serialized_extensions[\"e2ee\"][\"device_lists\"] = {}",
            "",
            "                serialized_extensions[\"e2ee\"][\"device_lists\"][\"changed\"] = list(",
            "                    extensions.e2ee.device_list_updates.changed",
            "                )",
            "                serialized_extensions[\"e2ee\"][\"device_lists\"][\"left\"] = list(",
            "                    extensions.e2ee.device_list_updates.left",
            "                )",
            "",
            "        if extensions.account_data is not None:",
            "            serialized_extensions[\"account_data\"] = {",
            "                # Same as the the top-level `account_data.events` field in Sync v2.",
            "                \"global\": [",
            "                    {\"type\": account_data_type, \"content\": content}",
            "                    for account_data_type, content in extensions.account_data.global_account_data_map.items()",
            "                ],",
            "                # Same as the joined room's account_data field in Sync v2, e.g the path",
            "                # `rooms.join[\"!foo:bar\"].account_data.events`.",
            "                \"rooms\": {",
            "                    room_id: [",
            "                        {\"type\": account_data_type, \"content\": content}",
            "                        for account_data_type, content in event_map.items()",
            "                    ]",
            "                    for room_id, event_map in extensions.account_data.account_data_by_room_map.items()",
            "                },",
            "            }",
            "",
            "        if extensions.receipts is not None:",
            "            serialized_extensions[\"receipts\"] = {",
            "                \"rooms\": extensions.receipts.room_id_to_receipt_map,",
            "            }",
            "",
            "        if extensions.typing is not None:",
            "            serialized_extensions[\"typing\"] = {",
            "                \"rooms\": extensions.typing.room_id_to_typing_map,",
            "            }",
            "",
            "        return serialized_extensions",
            "",
            "",
            "def register_servlets(hs: \"HomeServer\", http_server: HttpServer) -> None:",
            "    SyncRestServlet(hs).register(http_server)",
            "",
            "    SlidingSyncRestServlet(hs).register(http_server)",
            "    SlidingSyncE2eeRestServlet(hs).register(http_server)"
        ],
        "afterPatchFile": [
            "#",
            "# This file is licensed under the Affero General Public License (AGPL) version 3.",
            "#",
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright (C) 2023 New Vector, Ltd",
            "#",
            "# This program is free software: you can redistribute it and/or modify",
            "# it under the terms of the GNU Affero General Public License as",
            "# published by the Free Software Foundation, either version 3 of the",
            "# License, or (at your option) any later version.",
            "#",
            "# See the GNU Affero General Public License for more details:",
            "# <https://www.gnu.org/licenses/agpl-3.0.html>.",
            "#",
            "# Originally licensed under the Apache License, Version 2.0:",
            "# <http://www.apache.org/licenses/LICENSE-2.0>.",
            "#",
            "# [This file includes modifications made by New Vector Limited]",
            "#",
            "#",
            "import itertools",
            "import logging",
            "from collections import defaultdict",
            "from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "",
            "from synapse.api.constants import AccountDataTypes, EduTypes, Membership, PresenceState",
            "from synapse.api.errors import Codes, StoreError, SynapseError",
            "from synapse.api.filtering import FilterCollection",
            "from synapse.api.presence import UserPresenceState",
            "from synapse.events.utils import (",
            "    SerializeEventConfig,",
            "    format_event_for_client_v2_without_room_id,",
            "    format_event_raw,",
            ")",
            "from synapse.handlers.presence import format_user_presence_state",
            "from synapse.handlers.sliding_sync import SlidingSyncConfig, SlidingSyncResult",
            "from synapse.handlers.sync import (",
            "    ArchivedSyncResult,",
            "    InvitedSyncResult,",
            "    JoinedSyncResult,",
            "    KnockedSyncResult,",
            "    SyncConfig,",
            "    SyncResult,",
            "    SyncVersion,",
            ")",
            "from synapse.http.server import HttpServer",
            "from synapse.http.servlet import (",
            "    RestServlet,",
            "    parse_and_validate_json_object_from_request,",
            "    parse_boolean,",
            "    parse_integer,",
            "    parse_string,",
            ")",
            "from synapse.http.site import SynapseRequest",
            "from synapse.logging.opentracing import log_kv, set_tag, trace_with_opname",
            "from synapse.rest.admin.experimental_features import ExperimentalFeature",
            "from synapse.types import JsonDict, Requester, SlidingSyncStreamToken, StreamToken",
            "from synapse.types.rest.client import SlidingSyncBody",
            "from synapse.util import json_decoder",
            "from synapse.util.caches.lrucache import LruCache",
            "",
            "from ._base import client_patterns, set_timeline_upper_limit",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SyncRestServlet(RestServlet):",
            "    \"\"\"",
            "",
            "    GET parameters::",
            "        timeout(int): How long to wait for new events in milliseconds.",
            "        since(batch_token): Batch token when asking for incremental deltas.",
            "        set_presence(str): What state the device presence should be set to.",
            "            default is \"online\".",
            "        filter(filter_id): A filter to apply to the events returned.",
            "",
            "    Response JSON::",
            "        {",
            "          \"next_batch\": // batch token for the next /sync",
            "          \"presence\": // presence data for the user.",
            "          \"rooms\": {",
            "            \"join\": { // Joined rooms being updated.",
            "              \"${room_id}\": { // Id of the room being updated",
            "                \"event_map\": // Map of EventID -> event JSON.",
            "                \"timeline\": { // The recent events in the room if gap is \"true\"",
            "                  \"limited\": // Was the per-room event limit exceeded?",
            "                             // otherwise the next events in the room.",
            "                  \"events\": [] // list of EventIDs in the \"event_map\".",
            "                  \"prev_batch\": // back token for getting previous events.",
            "                }",
            "                \"state\": {\"events\": []} // list of EventIDs updating the",
            "                                        // current state to be what it should",
            "                                        // be at the end of the batch.",
            "                \"ephemeral\": {\"events\": []} // list of event objects",
            "              }",
            "            },",
            "            \"invite\": {}, // Invited rooms being updated.",
            "            \"leave\": {} // Archived rooms being updated.",
            "          }",
            "        }",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(\"/sync$\")",
            "    ALLOWED_PRESENCE = {\"online\", \"offline\", \"unavailable\"}",
            "    CATEGORY = \"Sync requests\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__()",
            "        self.hs = hs",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastores().main",
            "        self.sync_handler = hs.get_sync_handler()",
            "        self.clock = hs.get_clock()",
            "        self.filtering = hs.get_filtering()",
            "        self.presence_handler = hs.get_presence_handler()",
            "        self._server_notices_sender = hs.get_server_notices_sender()",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._msc2654_enabled = hs.config.experimental.msc2654_enabled",
            "        self._msc3773_enabled = hs.config.experimental.msc3773_enabled",
            "",
            "        self._json_filter_cache: LruCache[str, bool] = LruCache(",
            "            max_size=1000,",
            "            cache_name=\"sync_valid_filter\",",
            "        )",
            "",
            "    async def on_GET(self, request: SynapseRequest) -> Tuple[int, JsonDict]:",
            "        # This will always be set by the time Twisted calls us.",
            "        assert request.args is not None",
            "",
            "        if b\"from\" in request.args:",
            "            # /events used to use 'from', but /sync uses 'since'.",
            "            # Lets be helpful and whine if we see a 'from'.",
            "            raise SynapseError(",
            "                400, \"'from' is not a valid query parameter. Did you mean 'since'?\"",
            "            )",
            "",
            "        requester = await self.auth.get_user_by_req(request, allow_guest=True)",
            "        user = requester.user",
            "        device_id = requester.device_id",
            "",
            "        timeout = parse_integer(request, \"timeout\", default=0)",
            "        since = parse_string(request, \"since\")",
            "        set_presence = parse_string(",
            "            request,",
            "            \"set_presence\",",
            "            default=\"online\",",
            "            allowed_values=self.ALLOWED_PRESENCE,",
            "        )",
            "        filter_id = parse_string(request, \"filter\")",
            "        full_state = parse_boolean(request, \"full_state\", default=False)",
            "",
            "        use_state_after = False",
            "        if await self.store.is_feature_enabled(",
            "            user.to_string(), ExperimentalFeature.MSC4222",
            "        ):",
            "            use_state_after = parse_boolean(",
            "                request, \"org.matrix.msc4222.use_state_after\", default=False",
            "            )",
            "",
            "        logger.debug(",
            "            \"/sync: user=%r, timeout=%r, since=%r, \"",
            "            \"set_presence=%r, filter_id=%r, device_id=%r\",",
            "            user,",
            "            timeout,",
            "            since,",
            "            set_presence,",
            "            filter_id,",
            "            device_id,",
            "        )",
            "",
            "        # Stream position of the last ignored users account data event for this user,",
            "        # if we're initial syncing.",
            "        # We include this in the request key to invalidate an initial sync",
            "        # in the response cache once the set of ignored users has changed.",
            "        # (We filter out ignored users from timeline events, so our sync response",
            "        # is invalid once the set of ignored users changes.)",
            "        last_ignore_accdata_streampos: Optional[int] = None",
            "        if not since:",
            "            # No `since`, so this is an initial sync.",
            "            last_ignore_accdata_streampos = await self.store.get_latest_stream_id_for_global_account_data_by_type_for_user(",
            "                user.to_string(), AccountDataTypes.IGNORED_USER_LIST",
            "            )",
            "",
            "        request_key = (",
            "            user,",
            "            timeout,",
            "            since,",
            "            filter_id,",
            "            full_state,",
            "            device_id,",
            "            last_ignore_accdata_streampos,",
            "            use_state_after,",
            "        )",
            "",
            "        if filter_id is None:",
            "            filter_collection = self.filtering.DEFAULT_FILTER_COLLECTION",
            "        elif filter_id.startswith(\"{\"):",
            "            try:",
            "                filter_object = json_decoder.decode(filter_id)",
            "            except Exception:",
            "                raise SynapseError(400, \"Invalid filter JSON\", errcode=Codes.NOT_JSON)",
            "",
            "            # We cache the validation, as this can get quite expensive if people use",
            "            # a literal json blob as a query param.",
            "            if not self._json_filter_cache.get(filter_id):",
            "                self.filtering.check_valid_filter(filter_object)",
            "                self._json_filter_cache[filter_id] = True",
            "",
            "            set_timeline_upper_limit(",
            "                filter_object, self.hs.config.server.filter_timeline_limit",
            "            )",
            "            filter_collection = FilterCollection(self.hs, filter_object)",
            "        else:",
            "            try:",
            "                filter_collection = await self.filtering.get_user_filter(",
            "                    user, filter_id",
            "                )",
            "            except StoreError as err:",
            "                if err.code != 404:",
            "                    raise",
            "                # fix up the description and errcode to be more useful",
            "                raise SynapseError(400, \"No such filter\", errcode=Codes.INVALID_PARAM)",
            "",
            "        sync_config = SyncConfig(",
            "            user=user,",
            "            filter_collection=filter_collection,",
            "            is_guest=requester.is_guest,",
            "            device_id=device_id,",
            "            use_state_after=use_state_after,",
            "        )",
            "",
            "        since_token = None",
            "        if since is not None:",
            "            since_token = await StreamToken.from_string(self.store, since)",
            "",
            "        # send any outstanding server notices to the user.",
            "        await self._server_notices_sender.on_user_syncing(user.to_string())",
            "",
            "        affect_presence = set_presence != PresenceState.OFFLINE",
            "",
            "        context = await self.presence_handler.user_syncing(",
            "            user.to_string(),",
            "            requester.device_id,",
            "            affect_presence=affect_presence,",
            "            presence_state=set_presence,",
            "        )",
            "        with context:",
            "            sync_result = await self.sync_handler.wait_for_sync_for_user(",
            "                requester,",
            "                sync_config,",
            "                SyncVersion.SYNC_V2,",
            "                request_key,",
            "                since_token=since_token,",
            "                timeout=timeout,",
            "                full_state=full_state,",
            "            )",
            "",
            "        # the client may have disconnected by now; don't bother to serialize the",
            "        # response if so.",
            "        if request._disconnected:",
            "            logger.info(\"Client has disconnected; not serializing response.\")",
            "            return 200, {}",
            "",
            "        time_now = self.clock.time_msec()",
            "        # We know that the the requester has an access token since appservices",
            "        # cannot use sync.",
            "        response_content = await self.encode_response(",
            "            time_now, sync_config, sync_result, requester, filter_collection",
            "        )",
            "",
            "        logger.debug(\"Event formatting complete\")",
            "        return 200, response_content",
            "",
            "    @trace_with_opname(\"sync.encode_response\")",
            "    async def encode_response(",
            "        self,",
            "        time_now: int,",
            "        sync_config: SyncConfig,",
            "        sync_result: SyncResult,",
            "        requester: Requester,",
            "        filter: FilterCollection,",
            "    ) -> JsonDict:",
            "        logger.debug(\"Formatting events in sync response\")",
            "        if filter.event_format == \"client\":",
            "            event_formatter = format_event_for_client_v2_without_room_id",
            "        elif filter.event_format == \"federation\":",
            "            event_formatter = format_event_raw",
            "        else:",
            "            raise Exception(\"Unknown event format %s\" % (filter.event_format,))",
            "",
            "        serialize_options = SerializeEventConfig(",
            "            event_format=event_formatter,",
            "            requester=requester,",
            "            only_event_fields=filter.event_fields,",
            "        )",
            "        stripped_serialize_options = SerializeEventConfig(",
            "            event_format=event_formatter,",
            "            requester=requester,",
            "            include_stripped_room_state=True,",
            "        )",
            "",
            "        joined = await self.encode_joined(",
            "            sync_config, sync_result.joined, time_now, serialize_options",
            "        )",
            "",
            "        invited = await self.encode_invited(",
            "            sync_result.invited, time_now, stripped_serialize_options",
            "        )",
            "",
            "        knocked = await self.encode_knocked(",
            "            sync_result.knocked, time_now, stripped_serialize_options",
            "        )",
            "",
            "        archived = await self.encode_archived(",
            "            sync_config, sync_result.archived, time_now, serialize_options",
            "        )",
            "",
            "        logger.debug(\"building sync response dict\")",
            "",
            "        response: JsonDict = defaultdict(dict)",
            "        response[\"next_batch\"] = await sync_result.next_batch.to_string(self.store)",
            "",
            "        if sync_result.account_data:",
            "            response[\"account_data\"] = {\"events\": sync_result.account_data}",
            "        if sync_result.presence:",
            "            response[\"presence\"] = SyncRestServlet.encode_presence(",
            "                sync_result.presence, time_now",
            "            )",
            "",
            "        if sync_result.to_device:",
            "            response[\"to_device\"] = {\"events\": sync_result.to_device}",
            "",
            "        if sync_result.device_lists.changed:",
            "            response[\"device_lists\"][\"changed\"] = list(sync_result.device_lists.changed)",
            "        if sync_result.device_lists.left:",
            "            response[\"device_lists\"][\"left\"] = list(sync_result.device_lists.left)",
            "",
            "        # We always include this because https://github.com/vector-im/element-android/issues/3725",
            "        # The spec isn't terribly clear on when this can be omitted and how a client would tell",
            "        # the difference between \"no keys present\" and \"nothing changed\" in terms of whole field",
            "        # absent / individual key type entry absent",
            "        # Corresponding synapse issue: https://github.com/matrix-org/synapse/issues/10456",
            "        response[\"device_one_time_keys_count\"] = sync_result.device_one_time_keys_count",
            "",
            "        # https://github.com/matrix-org/matrix-doc/blob/54255851f642f84a4f1aaf7bc063eebe3d76752b/proposals/2732-olm-fallback-keys.md",
            "        # states that this field should always be included, as long as the server supports the feature.",
            "        response[\"org.matrix.msc2732.device_unused_fallback_key_types\"] = (",
            "            sync_result.device_unused_fallback_key_types",
            "        )",
            "        response[\"device_unused_fallback_key_types\"] = (",
            "            sync_result.device_unused_fallback_key_types",
            "        )",
            "",
            "        if joined:",
            "            response[\"rooms\"][Membership.JOIN] = joined",
            "        if invited:",
            "            response[\"rooms\"][Membership.INVITE] = invited",
            "        if knocked:",
            "            response[\"rooms\"][Membership.KNOCK] = knocked",
            "        if archived:",
            "            response[\"rooms\"][Membership.LEAVE] = archived",
            "",
            "        return response",
            "",
            "    @staticmethod",
            "    def encode_presence(events: List[UserPresenceState], time_now: int) -> JsonDict:",
            "        return {",
            "            \"events\": [",
            "                {",
            "                    \"type\": EduTypes.PRESENCE,",
            "                    \"sender\": event.user_id,",
            "                    \"content\": format_user_presence_state(",
            "                        event, time_now, include_user_id=False",
            "                    ),",
            "                }",
            "                for event in events",
            "            ]",
            "        }",
            "",
            "    @trace_with_opname(\"sync.encode_joined\")",
            "    async def encode_joined(",
            "        self,",
            "        sync_config: SyncConfig,",
            "        rooms: List[JoinedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Encode the joined rooms in a sync result",
            "",
            "        Args:",
            "            sync_config",
            "            rooms: list of sync results for rooms this user is joined to",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "        Returns:",
            "            The joined rooms list, in our response format",
            "        \"\"\"",
            "        joined = {}",
            "        for room in rooms:",
            "            joined[room.room_id] = await self.encode_room(",
            "                sync_config,",
            "                room,",
            "                time_now,",
            "                joined=True,",
            "                serialize_options=serialize_options,",
            "            )",
            "",
            "        return joined",
            "",
            "    @trace_with_opname(\"sync.encode_invited\")",
            "    async def encode_invited(",
            "        self,",
            "        rooms: List[InvitedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Encode the invited rooms in a sync result",
            "",
            "        Args:",
            "            rooms: list of sync results for rooms this user is invited to",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "",
            "        Returns:",
            "            The invited rooms list, in our response format",
            "        \"\"\"",
            "        invited = {}",
            "        for room in rooms:",
            "            invite = await self._event_serializer.serialize_event(",
            "                room.invite, time_now, config=serialize_options",
            "            )",
            "            unsigned = dict(invite.get(\"unsigned\", {}))",
            "            invite[\"unsigned\"] = unsigned",
            "",
            "            invited_state = unsigned.pop(\"invite_room_state\", [])",
            "            if not isinstance(invited_state, list):",
            "                invited_state = []",
            "",
            "            invited_state = list(invited_state)",
            "            invited_state.append(invite)",
            "            invited[room.room_id] = {\"invite_state\": {\"events\": invited_state}}",
            "",
            "        return invited",
            "",
            "    @trace_with_opname(\"sync.encode_knocked\")",
            "    async def encode_knocked(",
            "        self,",
            "        rooms: List[KnockedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> Dict[str, Dict[str, Any]]:",
            "        \"\"\"",
            "        Encode the rooms we've knocked on in a sync result.",
            "",
            "        Args:",
            "            rooms: list of sync results for rooms this user is knocking on",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "",
            "        Returns:",
            "            The list of rooms the user has knocked on, in our response format.",
            "        \"\"\"",
            "        knocked = {}",
            "        for room in rooms:",
            "            knock = await self._event_serializer.serialize_event(",
            "                room.knock, time_now, config=serialize_options",
            "            )",
            "",
            "            # Extract the `unsigned` key from the knock event.",
            "            # This is where we (cheekily) store the knock state events",
            "            unsigned = knock.setdefault(\"unsigned\", {})",
            "",
            "            # Duplicate the dictionary in order to avoid modifying the original",
            "            unsigned = dict(unsigned)",
            "",
            "            # Extract the stripped room state from the unsigned dict",
            "            # This is for clients to get a little bit of information about",
            "            # the room they've knocked on, without revealing any sensitive information",
            "            knocked_state = unsigned.pop(\"knock_room_state\", [])",
            "            if not isinstance(knocked_state, list):",
            "                knocked_state = []",
            "            knocked_state = list(knocked_state)",
            "",
            "            # Append the actual knock membership event itself as well. This provides",
            "            # the client with:",
            "            #",
            "            # * A knock state event that they can use for easier internal tracking",
            "            # * The rough timestamp of when the knock occurred contained within the event",
            "            knocked_state.append(knock)",
            "",
            "            # Build the `knock_state` dictionary, which will contain the state of the",
            "            # room that the client has knocked on",
            "            knocked[room.room_id] = {\"knock_state\": {\"events\": knocked_state}}",
            "",
            "        return knocked",
            "",
            "    @trace_with_opname(\"sync.encode_archived\")",
            "    async def encode_archived(",
            "        self,",
            "        sync_config: SyncConfig,",
            "        rooms: List[ArchivedSyncResult],",
            "        time_now: int,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Encode the archived rooms in a sync result",
            "",
            "        Args:",
            "            sync_config",
            "            rooms: list of sync results for rooms this user is joined to",
            "            time_now: current time - used as a baseline for age calculations",
            "            serialize_options: Event serializer options",
            "        Returns:",
            "            The archived rooms list, in our response format",
            "        \"\"\"",
            "        joined = {}",
            "        for room in rooms:",
            "            joined[room.room_id] = await self.encode_room(",
            "                sync_config,",
            "                room,",
            "                time_now,",
            "                joined=False,",
            "                serialize_options=serialize_options,",
            "            )",
            "",
            "        return joined",
            "",
            "    async def encode_room(",
            "        self,",
            "        sync_config: SyncConfig,",
            "        room: Union[JoinedSyncResult, ArchivedSyncResult],",
            "        time_now: int,",
            "        joined: bool,",
            "        serialize_options: SerializeEventConfig,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Args:",
            "            sync_config",
            "            room: sync result for a single room",
            "            time_now: current time - used as a baseline for age calculations",
            "            token_id: ID of the user's auth token - used for namespacing",
            "                of transaction IDs",
            "            joined: True if the user is joined to this room - will mean",
            "                we handle ephemeral events",
            "            only_fields: Optional. The list of event fields to include.",
            "            event_formatter: function to convert from federation format",
            "                to client format",
            "        Returns:",
            "            The room, encoded in our response format",
            "        \"\"\"",
            "        state_dict = room.state",
            "        timeline_events = room.timeline.events",
            "",
            "        state_events = state_dict.values()",
            "",
            "        for event in itertools.chain(state_events, timeline_events):",
            "            # We've had bug reports that events were coming down under the",
            "            # wrong room.",
            "            if event.room_id != room.room_id:",
            "                logger.warning(",
            "                    \"Event %r is under room %r instead of %r\",",
            "                    event.event_id,",
            "                    room.room_id,",
            "                    event.room_id,",
            "                )",
            "",
            "        serialized_state = await self._event_serializer.serialize_events(",
            "            state_events, time_now, config=serialize_options",
            "        )",
            "        serialized_timeline = await self._event_serializer.serialize_events(",
            "            timeline_events,",
            "            time_now,",
            "            config=serialize_options,",
            "            bundle_aggregations=room.timeline.bundled_aggregations,",
            "        )",
            "",
            "        account_data = room.account_data",
            "",
            "        # We either include a `state` or `state_after` field depending on",
            "        # whether the client has opted in to the newer `state_after` behavior.",
            "        if sync_config.use_state_after:",
            "            state_key_name = \"org.matrix.msc4222.state_after\"",
            "        else:",
            "            state_key_name = \"state\"",
            "",
            "        result: JsonDict = {",
            "            \"timeline\": {",
            "                \"events\": serialized_timeline,",
            "                \"prev_batch\": await room.timeline.prev_batch.to_string(self.store),",
            "                \"limited\": room.timeline.limited,",
            "            },",
            "            state_key_name: {\"events\": serialized_state},",
            "            \"account_data\": {\"events\": account_data},",
            "        }",
            "",
            "        if joined:",
            "            assert isinstance(room, JoinedSyncResult)",
            "            ephemeral_events = room.ephemeral",
            "            result[\"ephemeral\"] = {\"events\": ephemeral_events}",
            "            result[\"unread_notifications\"] = room.unread_notifications",
            "            if room.unread_thread_notifications:",
            "                result[\"unread_thread_notifications\"] = room.unread_thread_notifications",
            "                if self._msc3773_enabled:",
            "                    result[\"org.matrix.msc3773.unread_thread_notifications\"] = (",
            "                        room.unread_thread_notifications",
            "                    )",
            "            result[\"summary\"] = room.summary",
            "            if self._msc2654_enabled:",
            "                result[\"org.matrix.msc2654.unread_count\"] = room.unread_count",
            "",
            "        return result",
            "",
            "",
            "class SlidingSyncE2eeRestServlet(RestServlet):",
            "    \"\"\"",
            "    API endpoint for MSC3575 Sliding Sync `/sync/e2ee`. This is being introduced as part",
            "    of Sliding Sync but doesn't have any sliding window component. It's just a way to",
            "    get E2EE events without having to sit through a big initial sync (`/sync` v2). And",
            "    we can avoid encryption events being backed up by the main sync response.",
            "",
            "    Having To-Device messages split out to this sync endpoint also helps when clients",
            "    need to have 2 or more sync streams open at a time, e.g a push notification process",
            "    and a main process. This can cause the two processes to race to fetch the To-Device",
            "    events, resulting in the need for complex synchronisation rules to ensure the token",
            "    is correctly and atomically exchanged between processes.",
            "",
            "    GET parameters::",
            "        timeout(int): How long to wait for new events in milliseconds.",
            "        since(batch_token): Batch token when asking for incremental deltas.",
            "",
            "    Response JSON::",
            "        {",
            "            \"next_batch\": // batch token for the next /sync",
            "            \"to_device\": {",
            "                // list of to-device events",
            "                \"events\": [",
            "                    {",
            "                        \"content: { \"algorithm\": \"m.olm.v1.curve25519-aes-sha2\", \"ciphertext\": { ... }, \"org.matrix.msgid\": \"abcd\", \"session_id\": \"abcd\" },",
            "                        \"type\": \"m.room.encrypted\",",
            "                        \"sender\": \"@alice:example.com\",",
            "                    }",
            "                    // ...",
            "                ]",
            "            },",
            "            \"device_lists\": {",
            "                \"changed\": [\"@alice:example.com\"],",
            "                \"left\": [\"@bob:example.com\"]",
            "            },",
            "            \"device_one_time_keys_count\": {",
            "                \"signed_curve25519\": 50",
            "            },",
            "            \"device_unused_fallback_key_types\": [",
            "                \"signed_curve25519\"",
            "            ]",
            "        }",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(",
            "        \"/org.matrix.msc3575/sync/e2ee$\", releases=[], v1=False, unstable=True",
            "    )",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__()",
            "        self.hs = hs",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastores().main",
            "        self.sync_handler = hs.get_sync_handler()",
            "",
            "        # Filtering only matters for the `device_lists` because it requires a bunch of",
            "        # derived information from rooms (see how `_generate_sync_entry_for_rooms()`",
            "        # prepares a bunch of data for `_generate_sync_entry_for_device_list()`).",
            "        self.only_member_events_filter_collection = FilterCollection(",
            "            self.hs,",
            "            {",
            "                \"room\": {",
            "                    # We only care about membership events for the `device_lists`.",
            "                    # Membership will tell us whether a user has joined/left a room and",
            "                    # if there are new devices to encrypt for.",
            "                    \"timeline\": {",
            "                        \"types\": [\"m.room.member\"],",
            "                    },",
            "                    \"state\": {",
            "                        \"types\": [\"m.room.member\"],",
            "                    },",
            "                    # We don't want any extra account_data generated because it's not",
            "                    # returned by this endpoint. This helps us avoid work in",
            "                    # `_generate_sync_entry_for_rooms()`",
            "                    \"account_data\": {",
            "                        \"not_types\": [\"*\"],",
            "                    },",
            "                    # We don't want any extra ephemeral data generated because it's not",
            "                    # returned by this endpoint. This helps us avoid work in",
            "                    # `_generate_sync_entry_for_rooms()`",
            "                    \"ephemeral\": {",
            "                        \"not_types\": [\"*\"],",
            "                    },",
            "                },",
            "                # We don't want any extra account_data generated because it's not",
            "                # returned by this endpoint. (This is just here for good measure)",
            "                \"account_data\": {",
            "                    \"not_types\": [\"*\"],",
            "                },",
            "                # We don't want any extra presence data generated because it's not",
            "                # returned by this endpoint. (This is just here for good measure)",
            "                \"presence\": {",
            "                    \"not_types\": [\"*\"],",
            "                },",
            "            },",
            "        )",
            "",
            "    async def on_GET(self, request: SynapseRequest) -> Tuple[int, JsonDict]:",
            "        requester = await self.auth.get_user_by_req_experimental_feature(",
            "            request, allow_guest=True, feature=ExperimentalFeature.MSC3575",
            "        )",
            "        user = requester.user",
            "        device_id = requester.device_id",
            "",
            "        timeout = parse_integer(request, \"timeout\", default=0)",
            "        since = parse_string(request, \"since\")",
            "",
            "        sync_config = SyncConfig(",
            "            user=user,",
            "            filter_collection=self.only_member_events_filter_collection,",
            "            is_guest=requester.is_guest,",
            "            device_id=device_id,",
            "            use_state_after=False,  # We don't return any rooms so this flag is a no-op",
            "        )",
            "",
            "        since_token = None",
            "        if since is not None:",
            "            since_token = await StreamToken.from_string(self.store, since)",
            "",
            "        # Request cache key",
            "        request_key = (",
            "            SyncVersion.E2EE_SYNC,",
            "            user,",
            "            timeout,",
            "            since,",
            "        )",
            "",
            "        # Gather data for the response",
            "        sync_result = await self.sync_handler.wait_for_sync_for_user(",
            "            requester,",
            "            sync_config,",
            "            SyncVersion.E2EE_SYNC,",
            "            request_key,",
            "            since_token=since_token,",
            "            timeout=timeout,",
            "            full_state=False,",
            "        )",
            "",
            "        # The client may have disconnected by now; don't bother to serialize the",
            "        # response if so.",
            "        if request._disconnected:",
            "            logger.info(\"Client has disconnected; not serializing response.\")",
            "            return 200, {}",
            "",
            "        response: JsonDict = defaultdict(dict)",
            "        response[\"next_batch\"] = await sync_result.next_batch.to_string(self.store)",
            "",
            "        if sync_result.to_device:",
            "            response[\"to_device\"] = {\"events\": sync_result.to_device}",
            "",
            "        if sync_result.device_lists.changed:",
            "            response[\"device_lists\"][\"changed\"] = list(sync_result.device_lists.changed)",
            "        if sync_result.device_lists.left:",
            "            response[\"device_lists\"][\"left\"] = list(sync_result.device_lists.left)",
            "",
            "        # We always include this because https://github.com/vector-im/element-android/issues/3725",
            "        # The spec isn't terribly clear on when this can be omitted and how a client would tell",
            "        # the difference between \"no keys present\" and \"nothing changed\" in terms of whole field",
            "        # absent / individual key type entry absent",
            "        # Corresponding synapse issue: https://github.com/matrix-org/synapse/issues/10456",
            "        response[\"device_one_time_keys_count\"] = sync_result.device_one_time_keys_count",
            "",
            "        # https://github.com/matrix-org/matrix-doc/blob/54255851f642f84a4f1aaf7bc063eebe3d76752b/proposals/2732-olm-fallback-keys.md",
            "        # states that this field should always be included, as long as the server supports the feature.",
            "        response[\"device_unused_fallback_key_types\"] = (",
            "            sync_result.device_unused_fallback_key_types",
            "        )",
            "",
            "        return 200, response",
            "",
            "",
            "class SlidingSyncRestServlet(RestServlet):",
            "    \"\"\"",
            "    API endpoint for MSC3575 Sliding Sync `/sync`. Allows for clients to request a",
            "    subset (sliding window) of rooms, state, and timeline events (just what they need)",
            "    in order to bootstrap quickly and subscribe to only what the client cares about.",
            "    Because the client can specify what it cares about, we can respond quickly and skip",
            "    all of the work we would normally have to do with a sync v2 response.",
            "",
            "    Request query parameters:",
            "        timeout: How long to wait for new events in milliseconds.",
            "        pos: Stream position token when asking for incremental deltas.",
            "",
            "    Request body::",
            "        {",
            "            // Sliding Window API",
            "            \"lists\": {",
            "                \"foo-list\": {",
            "                    \"ranges\": [ [0, 99] ],",
            "                    \"required_state\": [",
            "                        [\"m.room.join_rules\", \"\"],",
            "                        [\"m.room.history_visibility\", \"\"],",
            "                        [\"m.space.child\", \"*\"]",
            "                    ],",
            "                    \"timeline_limit\": 10,",
            "                    \"filters\": {",
            "                        \"is_dm\": true",
            "                    },",
            "                }",
            "            },",
            "            // Room Subscriptions API",
            "            \"room_subscriptions\": {",
            "                \"!sub1:bar\": {",
            "                    \"required_state\": [ [\"*\",\"*\"] ],",
            "                    \"timeline_limit\": 10,",
            "                }",
            "            },",
            "            // Extensions API",
            "            \"extensions\": {}",
            "        }",
            "",
            "    Response JSON::",
            "        {",
            "            \"pos\": \"s58_224_0_13_10_1_1_16_0_1\",",
            "            \"lists\": {",
            "                \"foo-list\": {",
            "                    \"count\": 1337,",
            "                    \"ops\": [{",
            "                        \"op\": \"SYNC\",",
            "                        \"range\": [0, 99],",
            "                        \"room_ids\": [",
            "                            \"!foo:bar\",",
            "                            // ... 99 more room IDs",
            "                        ]",
            "                    }]",
            "                }",
            "            },",
            "            // Aggregated rooms from lists and room subscriptions",
            "            \"rooms\": {",
            "                // Room from room subscription",
            "                \"!sub1:bar\": {",
            "                    \"name\": \"Alice and Bob\",",
            "                    \"avatar\": \"mxc://...\",",
            "                    \"initial\": true,",
            "                    \"required_state\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.create\", \"state_key\":\"\", \"content\":{\"creator\":\"@alice:example.com\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.history_visibility\", \"state_key\":\"\", \"content\":{\"history_visibility\":\"joined\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.member\", \"state_key\":\"@alice:example.com\", \"content\":{\"membership\":\"join\"}}",
            "                    ],",
            "                    \"timeline\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.create\", \"state_key\":\"\", \"content\":{\"creator\":\"@alice:example.com\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.history_visibility\", \"state_key\":\"\", \"content\":{\"history_visibility\":\"joined\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.member\", \"state_key\":\"@alice:example.com\", \"content\":{\"membership\":\"join\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"A\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"B\"}},",
            "                    ],",
            "                    \"prev_batch\": \"t111_222_333\",",
            "                    \"joined_count\": 41,",
            "                    \"invited_count\": 1,",
            "                    \"notification_count\": 1,",
            "                    \"highlight_count\": 0,",
            "                    \"num_live\": 2\"",
            "                },",
            "                // rooms from list",
            "                \"!foo:bar\": {",
            "                    \"name\": \"The calculated room name\",",
            "                    \"avatar\": \"mxc://...\",",
            "                    \"initial\": true,",
            "                    \"required_state\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.history_visibility\", \"state_key\":\"\", \"content\":{\"history_visibility\":\"joined\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.space.child\", \"state_key\":\"!foo:example.com\", \"content\":{\"via\":[\"example.com\"]}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.space.child\", \"state_key\":\"!bar:example.com\", \"content\":{\"via\":[\"example.com\"]}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.space.child\", \"state_key\":\"!baz:example.com\", \"content\":{\"via\":[\"example.com\"]}}",
            "                    ],",
            "                    \"timeline\": [",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.join_rules\", \"state_key\":\"\", \"content\":{\"join_rule\":\"invite\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"A\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"B\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"C\"}},",
            "                        {\"sender\":\"@alice:example.com\",\"type\":\"m.room.message\", \"content\":{\"body\":\"D\"}},",
            "                    ],",
            "                    \"prev_batch\": \"t111_222_333\",",
            "                    \"joined_count\": 4,",
            "                    \"invited_count\": 0,",
            "                    \"notification_count\": 54,",
            "                    \"highlight_count\": 3,",
            "                    \"num_live\": 1,",
            "                },",
            "                 // ... 99 more items",
            "            },",
            "            \"extensions\": {}",
            "        }",
            "    \"\"\"",
            "",
            "    PATTERNS = client_patterns(",
            "        \"/org.matrix.simplified_msc3575/sync$\", releases=[], v1=False, unstable=True",
            "    )",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__()",
            "        self.auth = hs.get_auth()",
            "        self.store = hs.get_datastores().main",
            "        self.clock = hs.get_clock()",
            "        self.filtering = hs.get_filtering()",
            "        self.sliding_sync_handler = hs.get_sliding_sync_handler()",
            "        self.event_serializer = hs.get_event_client_serializer()",
            "",
            "    async def on_POST(self, request: SynapseRequest) -> Tuple[int, JsonDict]:",
            "        requester = await self.auth.get_user_by_req_experimental_feature(",
            "            request, allow_guest=True, feature=ExperimentalFeature.MSC3575",
            "        )",
            "",
            "        user = requester.user",
            "",
            "        timeout = parse_integer(request, \"timeout\", default=0)",
            "        # Position in the stream",
            "        from_token_string = parse_string(request, \"pos\")",
            "",
            "        from_token = None",
            "        if from_token_string is not None:",
            "            from_token = await SlidingSyncStreamToken.from_string(",
            "                self.store, from_token_string",
            "            )",
            "",
            "        # TODO: We currently don't know whether we're going to use sticky params or",
            "        # maybe some filters like sync v2  where they are built up once and referenced",
            "        # by filter ID. For now, we will just prototype with always passing everything",
            "        # in.",
            "        body = parse_and_validate_json_object_from_request(request, SlidingSyncBody)",
            "",
            "        # Tag and log useful data to differentiate requests.",
            "        set_tag(",
            "            \"sliding_sync.sync_type\", \"initial\" if from_token is None else \"incremental\"",
            "        )",
            "        set_tag(\"sliding_sync.conn_id\", body.conn_id or \"\")",
            "        log_kv(",
            "            {",
            "                \"sliding_sync.lists\": {",
            "                    list_name: {",
            "                        \"ranges\": list_config.ranges,",
            "                        \"timeline_limit\": list_config.timeline_limit,",
            "                    }",
            "                    for list_name, list_config in (body.lists or {}).items()",
            "                },",
            "                \"sliding_sync.room_subscriptions\": list(",
            "                    (body.room_subscriptions or {}).keys()",
            "                ),",
            "                # We also include the number of room subscriptions because logs are",
            "                # limited to 1024 characters and the large room ID list above can be cut",
            "                # off.",
            "                \"sliding_sync.num_room_subscriptions\": len(",
            "                    (body.room_subscriptions or {}).keys()",
            "                ),",
            "            }",
            "        )",
            "",
            "        sync_config = SlidingSyncConfig(",
            "            user=user,",
            "            requester=requester,",
            "            # FIXME: Currently, we're just manually copying the fields from the",
            "            # `SlidingSyncBody` into the config. How can we guarantee into the future",
            "            # that we don't forget any? I would like something more structured like",
            "            # `copy_attributes(from=body, to=config)`",
            "            conn_id=body.conn_id,",
            "            lists=body.lists,",
            "            room_subscriptions=body.room_subscriptions,",
            "            extensions=body.extensions,",
            "        )",
            "",
            "        sliding_sync_results = await self.sliding_sync_handler.wait_for_sync_for_user(",
            "            requester,",
            "            sync_config,",
            "            from_token,",
            "            timeout,",
            "        )",
            "",
            "        # The client may have disconnected by now; don't bother to serialize the",
            "        # response if so.",
            "        if request._disconnected:",
            "            logger.info(\"Client has disconnected; not serializing response.\")",
            "            return 200, {}",
            "",
            "        response_content = await self.encode_response(requester, sliding_sync_results)",
            "",
            "        return 200, response_content",
            "",
            "    async def encode_response(",
            "        self,",
            "        requester: Requester,",
            "        sliding_sync_result: SlidingSyncResult,",
            "    ) -> JsonDict:",
            "        response: JsonDict = defaultdict(dict)",
            "",
            "        response[\"pos\"] = await sliding_sync_result.next_pos.to_string(self.store)",
            "        serialized_lists = self.encode_lists(sliding_sync_result.lists)",
            "        if serialized_lists:",
            "            response[\"lists\"] = serialized_lists",
            "        response[\"rooms\"] = await self.encode_rooms(",
            "            requester, sliding_sync_result.rooms",
            "        )",
            "        response[\"extensions\"] = await self.encode_extensions(",
            "            requester, sliding_sync_result.extensions",
            "        )",
            "",
            "        return response",
            "",
            "    def encode_lists(",
            "        self, lists: Mapping[str, SlidingSyncResult.SlidingWindowList]",
            "    ) -> JsonDict:",
            "        def encode_operation(",
            "            operation: SlidingSyncResult.SlidingWindowList.Operation,",
            "        ) -> JsonDict:",
            "            return {",
            "                \"op\": operation.op.value,",
            "                \"range\": operation.range,",
            "                \"room_ids\": operation.room_ids,",
            "            }",
            "",
            "        serialized_lists = {}",
            "        for list_key, list_result in lists.items():",
            "            serialized_lists[list_key] = {",
            "                \"count\": list_result.count,",
            "                \"ops\": [encode_operation(op) for op in list_result.ops],",
            "            }",
            "",
            "        return serialized_lists",
            "",
            "    async def encode_rooms(",
            "        self,",
            "        requester: Requester,",
            "        rooms: Dict[str, SlidingSyncResult.RoomResult],",
            "    ) -> JsonDict:",
            "        time_now = self.clock.time_msec()",
            "",
            "        serialize_options = SerializeEventConfig(",
            "            event_format=format_event_for_client_v2_without_room_id,",
            "            requester=requester,",
            "        )",
            "",
            "        serialized_rooms: Dict[str, JsonDict] = {}",
            "        for room_id, room_result in rooms.items():",
            "            serialized_rooms[room_id] = {",
            "                \"notification_count\": room_result.notification_count,",
            "                \"highlight_count\": room_result.highlight_count,",
            "            }",
            "",
            "            if room_result.bump_stamp is not None:",
            "                serialized_rooms[room_id][\"bump_stamp\"] = room_result.bump_stamp",
            "",
            "            if room_result.joined_count is not None:",
            "                serialized_rooms[room_id][\"joined_count\"] = room_result.joined_count",
            "",
            "            if room_result.invited_count is not None:",
            "                serialized_rooms[room_id][\"invited_count\"] = room_result.invited_count",
            "",
            "            if room_result.name:",
            "                serialized_rooms[room_id][\"name\"] = room_result.name",
            "",
            "            if room_result.avatar:",
            "                serialized_rooms[room_id][\"avatar\"] = room_result.avatar",
            "",
            "            if room_result.heroes is not None and len(room_result.heroes) > 0:",
            "                serialized_heroes = []",
            "                for hero in room_result.heroes:",
            "                    serialized_hero = {",
            "                        \"user_id\": hero.user_id,",
            "                    }",
            "                    if hero.display_name is not None:",
            "                        # Not a typo, just how \"displayname\" is spelled in the spec",
            "                        serialized_hero[\"displayname\"] = hero.display_name",
            "",
            "                    if hero.avatar_url is not None:",
            "                        serialized_hero[\"avatar_url\"] = hero.avatar_url",
            "",
            "                    serialized_heroes.append(serialized_hero)",
            "                serialized_rooms[room_id][\"heroes\"] = serialized_heroes",
            "",
            "            # We should only include the `initial` key if it's `True` to save bandwidth.",
            "            # The absence of this flag means `False`.",
            "            if room_result.initial:",
            "                serialized_rooms[room_id][\"initial\"] = room_result.initial",
            "",
            "            if room_result.unstable_expanded_timeline:",
            "                serialized_rooms[room_id][\"unstable_expanded_timeline\"] = (",
            "                    room_result.unstable_expanded_timeline",
            "                )",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if (",
            "                room_result.required_state is not None",
            "                and len(room_result.required_state) > 0",
            "            ):",
            "                serialized_required_state = (",
            "                    await self.event_serializer.serialize_events(",
            "                        room_result.required_state,",
            "                        time_now,",
            "                        config=serialize_options,",
            "                    )",
            "                )",
            "                serialized_rooms[room_id][\"required_state\"] = serialized_required_state",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if (",
            "                room_result.timeline_events is not None",
            "                and len(room_result.timeline_events) > 0",
            "            ):",
            "                serialized_timeline = await self.event_serializer.serialize_events(",
            "                    room_result.timeline_events,",
            "                    time_now,",
            "                    config=serialize_options,",
            "                    bundle_aggregations=room_result.bundled_aggregations,",
            "                )",
            "                serialized_rooms[room_id][\"timeline\"] = serialized_timeline",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if room_result.limited is not None:",
            "                serialized_rooms[room_id][\"limited\"] = room_result.limited",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if room_result.prev_batch is not None:",
            "                serialized_rooms[room_id][",
            "                    \"prev_batch\"",
            "                ] = await room_result.prev_batch.to_string(self.store)",
            "",
            "            # This will be omitted for invite/knock rooms with `stripped_state`",
            "            if room_result.num_live is not None:",
            "                serialized_rooms[room_id][\"num_live\"] = room_result.num_live",
            "",
            "            # Field should be absent on non-DM rooms",
            "            if room_result.is_dm:",
            "                serialized_rooms[room_id][\"is_dm\"] = room_result.is_dm",
            "",
            "            # Stripped state only applies to invite/knock rooms",
            "            if (",
            "                room_result.stripped_state is not None",
            "                and len(room_result.stripped_state) > 0",
            "            ):",
            "                # TODO: `knocked_state` but that isn't specced yet.",
            "                #",
            "                # TODO: Instead of adding `knocked_state`, it would be good to rename",
            "                # this to `stripped_state` so it can be shared between invite and knock",
            "                # rooms, see",
            "                # https://github.com/matrix-org/matrix-spec-proposals/pull/3575#discussion_r1117629919",
            "                serialized_rooms[room_id][\"invite_state\"] = room_result.stripped_state",
            "",
            "        return serialized_rooms",
            "",
            "    async def encode_extensions(",
            "        self, requester: Requester, extensions: SlidingSyncResult.Extensions",
            "    ) -> JsonDict:",
            "        serialized_extensions: JsonDict = {}",
            "",
            "        if extensions.to_device is not None:",
            "            serialized_extensions[\"to_device\"] = {",
            "                \"next_batch\": extensions.to_device.next_batch,",
            "                \"events\": extensions.to_device.events,",
            "            }",
            "",
            "        if extensions.e2ee is not None:",
            "            serialized_extensions[\"e2ee\"] = {",
            "                # We always include this because",
            "                # https://github.com/vector-im/element-android/issues/3725. The spec",
            "                # isn't terribly clear on when this can be omitted and how a client",
            "                # would tell the difference between \"no keys present\" and \"nothing",
            "                # changed\" in terms of whole field absent / individual key type entry",
            "                # absent Corresponding synapse issue:",
            "                # https://github.com/matrix-org/synapse/issues/10456",
            "                \"device_one_time_keys_count\": extensions.e2ee.device_one_time_keys_count,",
            "                # https://github.com/matrix-org/matrix-doc/blob/54255851f642f84a4f1aaf7bc063eebe3d76752b/proposals/2732-olm-fallback-keys.md",
            "                # states that this field should always be included, as long as the",
            "                # server supports the feature.",
            "                \"device_unused_fallback_key_types\": extensions.e2ee.device_unused_fallback_key_types,",
            "            }",
            "",
            "            if extensions.e2ee.device_list_updates is not None:",
            "                serialized_extensions[\"e2ee\"][\"device_lists\"] = {}",
            "",
            "                serialized_extensions[\"e2ee\"][\"device_lists\"][\"changed\"] = list(",
            "                    extensions.e2ee.device_list_updates.changed",
            "                )",
            "                serialized_extensions[\"e2ee\"][\"device_lists\"][\"left\"] = list(",
            "                    extensions.e2ee.device_list_updates.left",
            "                )",
            "",
            "        if extensions.account_data is not None:",
            "            serialized_extensions[\"account_data\"] = {",
            "                # Same as the the top-level `account_data.events` field in Sync v2.",
            "                \"global\": [",
            "                    {\"type\": account_data_type, \"content\": content}",
            "                    for account_data_type, content in extensions.account_data.global_account_data_map.items()",
            "                ],",
            "                # Same as the joined room's account_data field in Sync v2, e.g the path",
            "                # `rooms.join[\"!foo:bar\"].account_data.events`.",
            "                \"rooms\": {",
            "                    room_id: [",
            "                        {\"type\": account_data_type, \"content\": content}",
            "                        for account_data_type, content in event_map.items()",
            "                    ]",
            "                    for room_id, event_map in extensions.account_data.account_data_by_room_map.items()",
            "                },",
            "            }",
            "",
            "        if extensions.receipts is not None:",
            "            serialized_extensions[\"receipts\"] = {",
            "                \"rooms\": extensions.receipts.room_id_to_receipt_map,",
            "            }",
            "",
            "        if extensions.typing is not None:",
            "            serialized_extensions[\"typing\"] = {",
            "                \"rooms\": extensions.typing.room_id_to_typing_map,",
            "            }",
            "",
            "        return serialized_extensions",
            "",
            "",
            "def register_servlets(hs: \"HomeServer\", http_server: HttpServer) -> None:",
            "    SyncRestServlet(hs).register(http_server)",
            "",
            "    SlidingSyncRestServlet(hs).register(http_server)",
            "    SlidingSyncE2eeRestServlet(hs).register(http_server)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "439": [
                "SyncRestServlet"
            ],
            "479": [
                "SyncRestServlet"
            ]
        },
        "addLocation": []
    }
}