{
    "airflow/www/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " import inspect"
            },
            "1": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " import json"
            },
            "2": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " import time"
            },
            "3": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import wtforms"
            },
            "4": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " import markdown"
            },
            "5": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " import re"
            },
            "6": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " import zipfile"
            },
            "7": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " from flask_appbuilder.models.sqla.interface import SQLAInterface"
            },
            "8": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " import flask_appbuilder.models.sqla.filters as fab_sqlafilters"
            },
            "9": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " import sqlalchemy as sqla"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+from six.moves.urllib.parse import urlencode"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " from airflow import configuration"
            },
            "13": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " from airflow.models import BaseOperator"
            },
            "14": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " from airflow.operators.subdag_operator import SubDagOperator"
            },
            "15": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " def get_params(**kwargs):"
            },
            "18": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    params = []"
            },
            "19": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for k, v in kwargs.items():"
            },
            "20": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if k == 'showPaused':"
            },
            "21": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # True is default or None"
            },
            "22": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if v or v is None:"
            },
            "23": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                continue"
            },
            "24": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            params.append('{}={}'.format(k, v))"
            },
            "25": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        elif v:"
            },
            "26": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            params.append('{}={}'.format(k, v))"
            },
            "27": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    params = sorted(params, key=lambda x: x.split('=')[0])"
            },
            "28": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return '&'.join(params)"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+    if 'showPaused' in kwargs:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+        v = kwargs['showPaused']"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+        if v or v is None:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+            kwargs.pop('showPaused')"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    return urlencode({d: v if v is not None else '' for d, v in kwargs.items()})"
            },
            "34": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 77,
                "PatchRowcode": " "
            },
            "35": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 78,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 79,
                "PatchRowcode": " def generate_pages(current_page, num_of_pages,"
            },
            "37": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "     \"\"\""
            },
            "38": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 105,
                "PatchRowcode": " "
            },
            "39": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "     void_link = 'javascript:void(0)'"
            },
            "40": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    first_node = \"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_first\">"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+    first_node = Markup(\"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_first\">"
            },
            "42": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "     <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&laquo;</a>"
            },
            "43": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-</li>\"\"\""
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+</li>\"\"\")"
            },
            "45": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 110,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    previous_node = \"\"\"<li class=\"paginate_button previous {disabled}\" id=\"dags_previous\">"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+    previous_node = Markup(\"\"\"<li class=\"paginate_button previous {disabled}\" id=\"dags_previous\">"
            },
            "48": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "     <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&lt;</a>"
            },
            "49": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-</li>\"\"\""
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+</li>\"\"\")"
            },
            "51": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 114,
                "PatchRowcode": " "
            },
            "52": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    next_node = \"\"\"<li class=\"paginate_button next {disabled}\" id=\"dags_next\">"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+    next_node = Markup(\"\"\"<li class=\"paginate_button next {disabled}\" id=\"dags_next\">"
            },
            "54": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "     <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&gt;</a>"
            },
            "55": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-</li>\"\"\""
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+</li>\"\"\")"
            },
            "57": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 118,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    last_node = \"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_last\">"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+    last_node = Markup(\"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_last\">"
            },
            "60": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "     <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&raquo;</a>"
            },
            "61": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-</li>\"\"\""
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+</li>\"\"\")"
            },
            "63": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 122,
                "PatchRowcode": " "
            },
            "64": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    page_node = \"\"\"<li class=\"paginate_button {is_active}\">"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+    page_node = Markup(\"\"\"<li class=\"paginate_button {is_active}\">"
            },
            "66": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 124,
                "PatchRowcode": "     <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"2\" tabindex=\"0\">{page_num}</a>"
            },
            "67": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-</li>\"\"\""
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+</li>\"\"\")"
            },
            "69": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 126,
                "PatchRowcode": " "
            },
            "70": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    output = ['<ul class=\"pagination\" style=\"margin-top:0px;\">']"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+    output = [Markup('<ul class=\"pagination\" style=\"margin-top:0px;\">')]"
            },
            "72": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": 128,
                "PatchRowcode": " "
            },
            "73": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 129,
                "PatchRowcode": "     is_disabled = 'disabled' if current_page <= 0 else ''"
            },
            "74": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "     output.append(first_node.format(href_link=\"?{}\""
            },
            "75": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "                                                       showPaused=showPaused)),"
            },
            "76": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "                                    disabled=is_disabled))"
            },
            "77": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 182,
                "PatchRowcode": " "
            },
            "78": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    output.append('</ul>')"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+    output.append(Markup('</ul>'))"
            },
            "80": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 184,
                "PatchRowcode": " "
            },
            "81": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return wtforms.widgets.core.HTMLString('\\n'.join(output))"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+    return Markup('\\n'.join(output))"
            },
            "83": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 186,
                "PatchRowcode": " "
            },
            "84": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 187,
                "PatchRowcode": " "
            },
            "85": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": 188,
                "PatchRowcode": " def epoch(dttm):"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "from future import standard_library  # noqa",
            "standard_library.install_aliases()  # noqa",
            "",
            "import inspect",
            "import json",
            "import time",
            "import wtforms",
            "import markdown",
            "import re",
            "import zipfile",
            "import os",
            "import io",
            "",
            "from builtins import str",
            "from past.builtins import basestring",
            "",
            "from pygments import highlight, lexers",
            "from pygments.formatters import HtmlFormatter",
            "from flask import request, Response, Markup, url_for",
            "from flask_appbuilder.models.sqla.interface import SQLAInterface",
            "import flask_appbuilder.models.sqla.filters as fab_sqlafilters",
            "import sqlalchemy as sqla",
            "from airflow import configuration",
            "from airflow.models import BaseOperator",
            "from airflow.operators.subdag_operator import SubDagOperator",
            "from airflow.utils import timezone",
            "from airflow.utils.json import AirflowJsonEncoder",
            "from airflow.utils.state import State",
            "",
            "DEFAULT_SENSITIVE_VARIABLE_FIELDS = (",
            "    'password',",
            "    'secret',",
            "    'passwd',",
            "    'authorization',",
            "    'api_key',",
            "    'apikey',",
            "    'access_token',",
            ")",
            "",
            "",
            "def should_hide_value_for_key(key_name):",
            "    # It is possible via importing variables from file that a key is empty.",
            "    if key_name:",
            "        config_set = configuration.conf.getboolean('admin',",
            "                                                   'hide_sensitive_variable_fields')",
            "        field_comp = any(s in key_name.lower() for s in DEFAULT_SENSITIVE_VARIABLE_FIELDS)",
            "        return config_set and field_comp",
            "    return False",
            "",
            "",
            "def get_params(**kwargs):",
            "    params = []",
            "    for k, v in kwargs.items():",
            "        if k == 'showPaused':",
            "            # True is default or None",
            "            if v or v is None:",
            "                continue",
            "            params.append('{}={}'.format(k, v))",
            "        elif v:",
            "            params.append('{}={}'.format(k, v))",
            "    params = sorted(params, key=lambda x: x.split('=')[0])",
            "    return '&'.join(params)",
            "",
            "",
            "def generate_pages(current_page, num_of_pages,",
            "                   search=None, showPaused=None, window=7):",
            "    \"\"\"",
            "    Generates the HTML for a paging component using a similar logic to the paging",
            "    auto-generated by Flask managed views. The paging component defines a number of",
            "    pages visible in the pager (window) and once the user goes to a page beyond the",
            "    largest visible, it would scroll to the right the page numbers and keeps the",
            "    current one in the middle of the pager component. When in the last pages,",
            "    the pages won't scroll and just keep moving until the last page. Pager also contains",
            "    <first, previous, ..., next, last> pages.",
            "    This component takes into account custom parameters such as search and showPaused,",
            "    which could be added to the pages link in order to maintain the state between",
            "    client and server. It also allows to make a bookmark on a specific paging state.",
            "    :param current_page:",
            "        the current page number, 0-indexed",
            "    :param num_of_pages:",
            "        the total number of pages",
            "    :param search:",
            "        the search query string, if any",
            "    :param showPaused:",
            "        false if paused dags will be hidden, otherwise true to show them",
            "    :param window:",
            "        the number of pages to be shown in the paging component (7 default)",
            "    :return:",
            "        the HTML string of the paging component",
            "    \"\"\"",
            "",
            "    void_link = 'javascript:void(0)'",
            "    first_node = \"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_first\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&laquo;</a>",
            "</li>\"\"\"",
            "",
            "    previous_node = \"\"\"<li class=\"paginate_button previous {disabled}\" id=\"dags_previous\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&lt;</a>",
            "</li>\"\"\"",
            "",
            "    next_node = \"\"\"<li class=\"paginate_button next {disabled}\" id=\"dags_next\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&gt;</a>",
            "</li>\"\"\"",
            "",
            "    last_node = \"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_last\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&raquo;</a>",
            "</li>\"\"\"",
            "",
            "    page_node = \"\"\"<li class=\"paginate_button {is_active}\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"2\" tabindex=\"0\">{page_num}</a>",
            "</li>\"\"\"",
            "",
            "    output = ['<ul class=\"pagination\" style=\"margin-top:0px;\">']",
            "",
            "    is_disabled = 'disabled' if current_page <= 0 else ''",
            "    output.append(first_node.format(href_link=\"?{}\"",
            "                                    .format(get_params(page=0,",
            "                                                       search=search,",
            "                                                       showPaused=showPaused)),",
            "                                    disabled=is_disabled))",
            "",
            "    page_link = void_link",
            "    if current_page > 0:",
            "        page_link = '?{}'.format(get_params(page=(current_page - 1),",
            "                                            search=search,",
            "                                            showPaused=showPaused))",
            "",
            "    output.append(previous_node.format(href_link=page_link,",
            "                                       disabled=is_disabled))",
            "",
            "    mid = int(window / 2)",
            "    last_page = num_of_pages - 1",
            "",
            "    if current_page <= mid or num_of_pages < window:",
            "        pages = [i for i in range(0, min(num_of_pages, window))]",
            "    elif mid < current_page < last_page - mid:",
            "        pages = [i for i in range(current_page - mid, current_page + mid + 1)]",
            "    else:",
            "        pages = [i for i in range(num_of_pages - window, last_page + 1)]",
            "",
            "    def is_current(current, page):",
            "        return page == current",
            "",
            "    for page in pages:",
            "        vals = {",
            "            'is_active': 'active' if is_current(current_page, page) else '',",
            "            'href_link': void_link if is_current(current_page, page)",
            "                         else '?{}'.format(get_params(page=page,",
            "                                                      search=search,",
            "                                                      showPaused=showPaused)),",
            "            'page_num': page + 1",
            "        }",
            "        output.append(page_node.format(**vals))",
            "",
            "    is_disabled = 'disabled' if current_page >= num_of_pages - 1 else ''",
            "",
            "    page_link = (void_link if current_page >= num_of_pages - 1",
            "                 else '?{}'.format(get_params(page=current_page + 1,",
            "                                              search=search,",
            "                                              showPaused=showPaused)))",
            "",
            "    output.append(next_node.format(href_link=page_link, disabled=is_disabled))",
            "    output.append(last_node.format(href_link=\"?{}\"",
            "                                   .format(get_params(page=last_page,",
            "                                                      search=search,",
            "                                                      showPaused=showPaused)),",
            "                                   disabled=is_disabled))",
            "",
            "    output.append('</ul>')",
            "",
            "    return wtforms.widgets.core.HTMLString('\\n'.join(output))",
            "",
            "",
            "def epoch(dttm):",
            "    \"\"\"Returns an epoch-type date\"\"\"",
            "    return int(time.mktime(dttm.timetuple())) * 1000,",
            "",
            "",
            "def json_response(obj):",
            "    \"\"\"",
            "    returns a json response from a json serializable python object",
            "    \"\"\"",
            "    return Response(",
            "        response=json.dumps(",
            "            obj, indent=4, cls=AirflowJsonEncoder),",
            "        status=200,",
            "        mimetype=\"application/json\")",
            "",
            "",
            "ZIP_REGEX = re.compile(r'((.*\\.zip){})?(.*)'.format(re.escape(os.sep)))",
            "",
            "",
            "def open_maybe_zipped(f, mode='r'):",
            "    \"\"\"",
            "    Opens the given file. If the path contains a folder with a .zip suffix, then",
            "    the folder is treated as a zip archive, opening the file inside the archive.",
            "",
            "    :return: a file object, as in `open`, or as in `ZipFile.open`.",
            "    \"\"\"",
            "",
            "    _, archive, filename = ZIP_REGEX.search(f).groups()",
            "    if archive and zipfile.is_zipfile(archive):",
            "        return zipfile.ZipFile(archive, mode=mode).open(filename)",
            "    else:",
            "        return io.open(f, mode=mode)",
            "",
            "",
            "def make_cache_key(*args, **kwargs):",
            "    \"\"\"",
            "    Used by cache to get a unique key per URL",
            "    \"\"\"",
            "    path = request.path",
            "    args = str(hash(frozenset(request.args.items())))",
            "    return (path + args).encode('ascii', 'ignore')",
            "",
            "",
            "def task_instance_link(attr):",
            "    dag_id = attr.get('dag_id')",
            "    task_id = attr.get('task_id')",
            "    execution_date = attr.get('execution_date')",
            "    url = url_for(",
            "        'Airflow.task',",
            "        dag_id=dag_id,",
            "        task_id=task_id,",
            "        execution_date=execution_date.isoformat())",
            "    url_root = url_for(",
            "        'Airflow.graph',",
            "        dag_id=dag_id,",
            "        root=task_id,",
            "        execution_date=execution_date.isoformat())",
            "    return Markup(",
            "        \"\"\"",
            "        <span style=\"white-space: nowrap;\">",
            "        <a href=\"{url}\">{task_id}</a>",
            "        <a href=\"{url_root}\" title=\"Filter on this task and upstream\">",
            "        <span class=\"glyphicon glyphicon-filter\" style=\"margin-left: 0px;\"",
            "            aria-hidden=\"true\"></span>",
            "        </a>",
            "        </span>",
            "        \"\"\").format(**locals())",
            "",
            "",
            "def state_token(state):",
            "    color = State.color(state)",
            "    return Markup(",
            "        '<span class=\"label\" style=\"background-color:{color};\">'",
            "        '{state}</span>').format(**locals())",
            "",
            "",
            "def state_f(attr):",
            "    state = attr.get('state')",
            "    return state_token(state)",
            "",
            "",
            "def nobr_f(attr_name):",
            "    def nobr(attr):",
            "        f = attr.get(attr_name)",
            "        return Markup(\"<nobr>{}</nobr>\").format(f)",
            "    return nobr",
            "",
            "",
            "def datetime_f(attr_name):",
            "    def dt(attr):",
            "        f = attr.get(attr_name)",
            "        f = f.isoformat() if f else ''",
            "        if timezone.utcnow().isoformat()[:4] == f[:4]:",
            "            f = f[5:]",
            "        return Markup(\"<nobr>{}</nobr>\").format(f)",
            "    return dt",
            "",
            "",
            "def dag_link(attr):",
            "    dag_id = attr.get('dag_id')",
            "    execution_date = attr.get('execution_date')",
            "    url = url_for(",
            "        'Airflow.graph',",
            "        dag_id=dag_id,",
            "        execution_date=execution_date)",
            "    return Markup(",
            "        '<a href=\"{}\">{}</a>').format(url, dag_id)",
            "",
            "",
            "def dag_run_link(attr):",
            "    dag_id = attr.get('dag_id')",
            "    run_id = attr.get('run_id')",
            "    execution_date = attr.get('execution_date')",
            "    url = url_for(",
            "        'Airflow.graph',",
            "        dag_id=dag_id,",
            "        run_id=run_id,",
            "        execution_date=execution_date)",
            "    return Markup(",
            "        '<a href=\"{url}\">{run_id}</a>').format(**locals())",
            "",
            "",
            "def pygment_html_render(s, lexer=lexers.TextLexer):",
            "    return highlight(",
            "        s,",
            "        lexer(),",
            "        HtmlFormatter(linenos=True),",
            "    )",
            "",
            "",
            "def render(obj, lexer):",
            "    out = \"\"",
            "    if isinstance(obj, basestring):",
            "        out += pygment_html_render(obj, lexer)",
            "    elif isinstance(obj, (tuple, list)):",
            "        for i, s in enumerate(obj):",
            "            out += \"<div>List item #{}</div>\".format(i)",
            "            out += \"<div>\" + pygment_html_render(s, lexer) + \"</div>\"",
            "    elif isinstance(obj, dict):",
            "        for k, v in obj.items():",
            "            out += '<div>Dict item \"{}\"</div>'.format(k)",
            "            out += \"<div>\" + pygment_html_render(v, lexer) + \"</div>\"",
            "    return out",
            "",
            "",
            "def wrapped_markdown(s):",
            "    return (",
            "        '<div class=\"rich_doc\">' + markdown.markdown(s) + \"</div>\"",
            "        if s is not None",
            "        else None",
            "    )",
            "",
            "",
            "def get_attr_renderer():",
            "    return {",
            "        'bash_command': lambda x: render(x, lexers.BashLexer),",
            "        'hql': lambda x: render(x, lexers.SqlLexer),",
            "        'sql': lambda x: render(x, lexers.SqlLexer),",
            "        'doc': lambda x: render(x, lexers.TextLexer),",
            "        'doc_json': lambda x: render(x, lexers.JsonLexer),",
            "        'doc_rst': lambda x: render(x, lexers.RstLexer),",
            "        'doc_yaml': lambda x: render(x, lexers.YamlLexer),",
            "        'doc_md': wrapped_markdown,",
            "        'python_callable': lambda x: render(",
            "            inspect.getsource(x) if x is not None else None, lexers.PythonLexer),",
            "    }",
            "",
            "",
            "def recurse_tasks(tasks, task_ids, dag_ids, task_id_to_dag):",
            "    if isinstance(tasks, list):",
            "        for task in tasks:",
            "            recurse_tasks(task, task_ids, dag_ids, task_id_to_dag)",
            "        return",
            "    if isinstance(tasks, SubDagOperator):",
            "        subtasks = tasks.subdag.tasks",
            "        dag_ids.append(tasks.subdag.dag_id)",
            "        for subtask in subtasks:",
            "            if subtask.task_id not in task_ids:",
            "                task_ids.append(subtask.task_id)",
            "                task_id_to_dag[subtask.task_id] = tasks.subdag",
            "        recurse_tasks(subtasks, task_ids, dag_ids, task_id_to_dag)",
            "    if isinstance(tasks, BaseOperator):",
            "        task_id_to_dag[tasks.task_id] = tasks.dag",
            "",
            "",
            "def get_chart_height(dag):",
            "    \"\"\"",
            "    TODO(aoen): See [AIRFLOW-1263] We use the number of tasks in the DAG as a heuristic to",
            "    approximate the size of generated chart (otherwise the charts are tiny and unreadable",
            "    when DAGs have a large number of tasks). Ideally nvd3 should allow for dynamic-height",
            "    charts, that is charts that take up space based on the size of the components within.",
            "    \"\"\"",
            "    return 600 + len(dag.tasks) * 10",
            "",
            "",
            "class UtcAwareFilterMixin(object):",
            "    def apply(self, query, value):",
            "        value = timezone.parse(value, timezone=timezone.utc)",
            "",
            "        return super(UtcAwareFilterMixin, self).apply(query, value)",
            "",
            "",
            "class UtcAwareFilterEqual(UtcAwareFilterMixin, fab_sqlafilters.FilterEqual):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterGreater(UtcAwareFilterMixin, fab_sqlafilters.FilterGreater):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterSmaller(UtcAwareFilterMixin, fab_sqlafilters.FilterSmaller):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterNotEqual(UtcAwareFilterMixin, fab_sqlafilters.FilterNotEqual):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterConverter(fab_sqlafilters.SQLAFilterConverter):",
            "",
            "    conversion_table = (",
            "        (('is_utcdatetime', [UtcAwareFilterEqual,",
            "                             UtcAwareFilterGreater,",
            "                             UtcAwareFilterSmaller,",
            "                             UtcAwareFilterNotEqual]),) +",
            "        fab_sqlafilters.SQLAFilterConverter.conversion_table",
            "    )",
            "",
            "",
            "class CustomSQLAInterface(SQLAInterface):",
            "    \"\"\"",
            "    FAB does not know how to handle columns with leading underscores because",
            "    they are not supported by WTForm. This hack will remove the leading",
            "    '_' from the key to lookup the column names.",
            "",
            "    \"\"\"",
            "    def __init__(self, obj):",
            "        super(CustomSQLAInterface, self).__init__(obj)",
            "",
            "        def clean_column_names():",
            "            if self.list_properties:",
            "                self.list_properties = dict(",
            "                    (k.lstrip('_'), v) for k, v in self.list_properties.items())",
            "            if self.list_columns:",
            "                self.list_columns = dict(",
            "                    (k.lstrip('_'), v) for k, v in self.list_columns.items())",
            "",
            "        clean_column_names()",
            "",
            "    def is_utcdatetime(self, col_name):",
            "        from airflow.utils.sqlalchemy import UtcDateTime",
            "        obj = self.list_columns[col_name].type",
            "        return isinstance(obj, UtcDateTime) or \\",
            "            isinstance(obj, sqla.types.TypeDecorator) and \\",
            "            isinstance(obj.impl, UtcDateTime)",
            "",
            "    filter_converter_class = UtcAwareFilterConverter"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "from future import standard_library  # noqa",
            "standard_library.install_aliases()  # noqa",
            "",
            "import inspect",
            "import json",
            "import time",
            "import markdown",
            "import re",
            "import zipfile",
            "import os",
            "import io",
            "",
            "from builtins import str",
            "from past.builtins import basestring",
            "",
            "from pygments import highlight, lexers",
            "from pygments.formatters import HtmlFormatter",
            "from flask import request, Response, Markup, url_for",
            "from flask_appbuilder.models.sqla.interface import SQLAInterface",
            "import flask_appbuilder.models.sqla.filters as fab_sqlafilters",
            "import sqlalchemy as sqla",
            "from six.moves.urllib.parse import urlencode",
            "",
            "from airflow import configuration",
            "from airflow.models import BaseOperator",
            "from airflow.operators.subdag_operator import SubDagOperator",
            "from airflow.utils import timezone",
            "from airflow.utils.json import AirflowJsonEncoder",
            "from airflow.utils.state import State",
            "",
            "DEFAULT_SENSITIVE_VARIABLE_FIELDS = (",
            "    'password',",
            "    'secret',",
            "    'passwd',",
            "    'authorization',",
            "    'api_key',",
            "    'apikey',",
            "    'access_token',",
            ")",
            "",
            "",
            "def should_hide_value_for_key(key_name):",
            "    # It is possible via importing variables from file that a key is empty.",
            "    if key_name:",
            "        config_set = configuration.conf.getboolean('admin',",
            "                                                   'hide_sensitive_variable_fields')",
            "        field_comp = any(s in key_name.lower() for s in DEFAULT_SENSITIVE_VARIABLE_FIELDS)",
            "        return config_set and field_comp",
            "    return False",
            "",
            "",
            "def get_params(**kwargs):",
            "    if 'showPaused' in kwargs:",
            "        v = kwargs['showPaused']",
            "        if v or v is None:",
            "            kwargs.pop('showPaused')",
            "    return urlencode({d: v if v is not None else '' for d, v in kwargs.items()})",
            "",
            "",
            "def generate_pages(current_page, num_of_pages,",
            "                   search=None, showPaused=None, window=7):",
            "    \"\"\"",
            "    Generates the HTML for a paging component using a similar logic to the paging",
            "    auto-generated by Flask managed views. The paging component defines a number of",
            "    pages visible in the pager (window) and once the user goes to a page beyond the",
            "    largest visible, it would scroll to the right the page numbers and keeps the",
            "    current one in the middle of the pager component. When in the last pages,",
            "    the pages won't scroll and just keep moving until the last page. Pager also contains",
            "    <first, previous, ..., next, last> pages.",
            "    This component takes into account custom parameters such as search and showPaused,",
            "    which could be added to the pages link in order to maintain the state between",
            "    client and server. It also allows to make a bookmark on a specific paging state.",
            "    :param current_page:",
            "        the current page number, 0-indexed",
            "    :param num_of_pages:",
            "        the total number of pages",
            "    :param search:",
            "        the search query string, if any",
            "    :param showPaused:",
            "        false if paused dags will be hidden, otherwise true to show them",
            "    :param window:",
            "        the number of pages to be shown in the paging component (7 default)",
            "    :return:",
            "        the HTML string of the paging component",
            "    \"\"\"",
            "",
            "    void_link = 'javascript:void(0)'",
            "    first_node = Markup(\"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_first\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&laquo;</a>",
            "</li>\"\"\")",
            "",
            "    previous_node = Markup(\"\"\"<li class=\"paginate_button previous {disabled}\" id=\"dags_previous\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"0\" tabindex=\"0\">&lt;</a>",
            "</li>\"\"\")",
            "",
            "    next_node = Markup(\"\"\"<li class=\"paginate_button next {disabled}\" id=\"dags_next\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&gt;</a>",
            "</li>\"\"\")",
            "",
            "    last_node = Markup(\"\"\"<li class=\"paginate_button {disabled}\" id=\"dags_last\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"3\" tabindex=\"0\">&raquo;</a>",
            "</li>\"\"\")",
            "",
            "    page_node = Markup(\"\"\"<li class=\"paginate_button {is_active}\">",
            "    <a href=\"{href_link}\" aria-controls=\"dags\" data-dt-idx=\"2\" tabindex=\"0\">{page_num}</a>",
            "</li>\"\"\")",
            "",
            "    output = [Markup('<ul class=\"pagination\" style=\"margin-top:0px;\">')]",
            "",
            "    is_disabled = 'disabled' if current_page <= 0 else ''",
            "    output.append(first_node.format(href_link=\"?{}\"",
            "                                    .format(get_params(page=0,",
            "                                                       search=search,",
            "                                                       showPaused=showPaused)),",
            "                                    disabled=is_disabled))",
            "",
            "    page_link = void_link",
            "    if current_page > 0:",
            "        page_link = '?{}'.format(get_params(page=(current_page - 1),",
            "                                            search=search,",
            "                                            showPaused=showPaused))",
            "",
            "    output.append(previous_node.format(href_link=page_link,",
            "                                       disabled=is_disabled))",
            "",
            "    mid = int(window / 2)",
            "    last_page = num_of_pages - 1",
            "",
            "    if current_page <= mid or num_of_pages < window:",
            "        pages = [i for i in range(0, min(num_of_pages, window))]",
            "    elif mid < current_page < last_page - mid:",
            "        pages = [i for i in range(current_page - mid, current_page + mid + 1)]",
            "    else:",
            "        pages = [i for i in range(num_of_pages - window, last_page + 1)]",
            "",
            "    def is_current(current, page):",
            "        return page == current",
            "",
            "    for page in pages:",
            "        vals = {",
            "            'is_active': 'active' if is_current(current_page, page) else '',",
            "            'href_link': void_link if is_current(current_page, page)",
            "                         else '?{}'.format(get_params(page=page,",
            "                                                      search=search,",
            "                                                      showPaused=showPaused)),",
            "            'page_num': page + 1",
            "        }",
            "        output.append(page_node.format(**vals))",
            "",
            "    is_disabled = 'disabled' if current_page >= num_of_pages - 1 else ''",
            "",
            "    page_link = (void_link if current_page >= num_of_pages - 1",
            "                 else '?{}'.format(get_params(page=current_page + 1,",
            "                                              search=search,",
            "                                              showPaused=showPaused)))",
            "",
            "    output.append(next_node.format(href_link=page_link, disabled=is_disabled))",
            "    output.append(last_node.format(href_link=\"?{}\"",
            "                                   .format(get_params(page=last_page,",
            "                                                      search=search,",
            "                                                      showPaused=showPaused)),",
            "                                   disabled=is_disabled))",
            "",
            "    output.append(Markup('</ul>'))",
            "",
            "    return Markup('\\n'.join(output))",
            "",
            "",
            "def epoch(dttm):",
            "    \"\"\"Returns an epoch-type date\"\"\"",
            "    return int(time.mktime(dttm.timetuple())) * 1000,",
            "",
            "",
            "def json_response(obj):",
            "    \"\"\"",
            "    returns a json response from a json serializable python object",
            "    \"\"\"",
            "    return Response(",
            "        response=json.dumps(",
            "            obj, indent=4, cls=AirflowJsonEncoder),",
            "        status=200,",
            "        mimetype=\"application/json\")",
            "",
            "",
            "ZIP_REGEX = re.compile(r'((.*\\.zip){})?(.*)'.format(re.escape(os.sep)))",
            "",
            "",
            "def open_maybe_zipped(f, mode='r'):",
            "    \"\"\"",
            "    Opens the given file. If the path contains a folder with a .zip suffix, then",
            "    the folder is treated as a zip archive, opening the file inside the archive.",
            "",
            "    :return: a file object, as in `open`, or as in `ZipFile.open`.",
            "    \"\"\"",
            "",
            "    _, archive, filename = ZIP_REGEX.search(f).groups()",
            "    if archive and zipfile.is_zipfile(archive):",
            "        return zipfile.ZipFile(archive, mode=mode).open(filename)",
            "    else:",
            "        return io.open(f, mode=mode)",
            "",
            "",
            "def make_cache_key(*args, **kwargs):",
            "    \"\"\"",
            "    Used by cache to get a unique key per URL",
            "    \"\"\"",
            "    path = request.path",
            "    args = str(hash(frozenset(request.args.items())))",
            "    return (path + args).encode('ascii', 'ignore')",
            "",
            "",
            "def task_instance_link(attr):",
            "    dag_id = attr.get('dag_id')",
            "    task_id = attr.get('task_id')",
            "    execution_date = attr.get('execution_date')",
            "    url = url_for(",
            "        'Airflow.task',",
            "        dag_id=dag_id,",
            "        task_id=task_id,",
            "        execution_date=execution_date.isoformat())",
            "    url_root = url_for(",
            "        'Airflow.graph',",
            "        dag_id=dag_id,",
            "        root=task_id,",
            "        execution_date=execution_date.isoformat())",
            "    return Markup(",
            "        \"\"\"",
            "        <span style=\"white-space: nowrap;\">",
            "        <a href=\"{url}\">{task_id}</a>",
            "        <a href=\"{url_root}\" title=\"Filter on this task and upstream\">",
            "        <span class=\"glyphicon glyphicon-filter\" style=\"margin-left: 0px;\"",
            "            aria-hidden=\"true\"></span>",
            "        </a>",
            "        </span>",
            "        \"\"\").format(**locals())",
            "",
            "",
            "def state_token(state):",
            "    color = State.color(state)",
            "    return Markup(",
            "        '<span class=\"label\" style=\"background-color:{color};\">'",
            "        '{state}</span>').format(**locals())",
            "",
            "",
            "def state_f(attr):",
            "    state = attr.get('state')",
            "    return state_token(state)",
            "",
            "",
            "def nobr_f(attr_name):",
            "    def nobr(attr):",
            "        f = attr.get(attr_name)",
            "        return Markup(\"<nobr>{}</nobr>\").format(f)",
            "    return nobr",
            "",
            "",
            "def datetime_f(attr_name):",
            "    def dt(attr):",
            "        f = attr.get(attr_name)",
            "        f = f.isoformat() if f else ''",
            "        if timezone.utcnow().isoformat()[:4] == f[:4]:",
            "            f = f[5:]",
            "        return Markup(\"<nobr>{}</nobr>\").format(f)",
            "    return dt",
            "",
            "",
            "def dag_link(attr):",
            "    dag_id = attr.get('dag_id')",
            "    execution_date = attr.get('execution_date')",
            "    url = url_for(",
            "        'Airflow.graph',",
            "        dag_id=dag_id,",
            "        execution_date=execution_date)",
            "    return Markup(",
            "        '<a href=\"{}\">{}</a>').format(url, dag_id)",
            "",
            "",
            "def dag_run_link(attr):",
            "    dag_id = attr.get('dag_id')",
            "    run_id = attr.get('run_id')",
            "    execution_date = attr.get('execution_date')",
            "    url = url_for(",
            "        'Airflow.graph',",
            "        dag_id=dag_id,",
            "        run_id=run_id,",
            "        execution_date=execution_date)",
            "    return Markup(",
            "        '<a href=\"{url}\">{run_id}</a>').format(**locals())",
            "",
            "",
            "def pygment_html_render(s, lexer=lexers.TextLexer):",
            "    return highlight(",
            "        s,",
            "        lexer(),",
            "        HtmlFormatter(linenos=True),",
            "    )",
            "",
            "",
            "def render(obj, lexer):",
            "    out = \"\"",
            "    if isinstance(obj, basestring):",
            "        out += pygment_html_render(obj, lexer)",
            "    elif isinstance(obj, (tuple, list)):",
            "        for i, s in enumerate(obj):",
            "            out += \"<div>List item #{}</div>\".format(i)",
            "            out += \"<div>\" + pygment_html_render(s, lexer) + \"</div>\"",
            "    elif isinstance(obj, dict):",
            "        for k, v in obj.items():",
            "            out += '<div>Dict item \"{}\"</div>'.format(k)",
            "            out += \"<div>\" + pygment_html_render(v, lexer) + \"</div>\"",
            "    return out",
            "",
            "",
            "def wrapped_markdown(s):",
            "    return (",
            "        '<div class=\"rich_doc\">' + markdown.markdown(s) + \"</div>\"",
            "        if s is not None",
            "        else None",
            "    )",
            "",
            "",
            "def get_attr_renderer():",
            "    return {",
            "        'bash_command': lambda x: render(x, lexers.BashLexer),",
            "        'hql': lambda x: render(x, lexers.SqlLexer),",
            "        'sql': lambda x: render(x, lexers.SqlLexer),",
            "        'doc': lambda x: render(x, lexers.TextLexer),",
            "        'doc_json': lambda x: render(x, lexers.JsonLexer),",
            "        'doc_rst': lambda x: render(x, lexers.RstLexer),",
            "        'doc_yaml': lambda x: render(x, lexers.YamlLexer),",
            "        'doc_md': wrapped_markdown,",
            "        'python_callable': lambda x: render(",
            "            inspect.getsource(x) if x is not None else None, lexers.PythonLexer),",
            "    }",
            "",
            "",
            "def recurse_tasks(tasks, task_ids, dag_ids, task_id_to_dag):",
            "    if isinstance(tasks, list):",
            "        for task in tasks:",
            "            recurse_tasks(task, task_ids, dag_ids, task_id_to_dag)",
            "        return",
            "    if isinstance(tasks, SubDagOperator):",
            "        subtasks = tasks.subdag.tasks",
            "        dag_ids.append(tasks.subdag.dag_id)",
            "        for subtask in subtasks:",
            "            if subtask.task_id not in task_ids:",
            "                task_ids.append(subtask.task_id)",
            "                task_id_to_dag[subtask.task_id] = tasks.subdag",
            "        recurse_tasks(subtasks, task_ids, dag_ids, task_id_to_dag)",
            "    if isinstance(tasks, BaseOperator):",
            "        task_id_to_dag[tasks.task_id] = tasks.dag",
            "",
            "",
            "def get_chart_height(dag):",
            "    \"\"\"",
            "    TODO(aoen): See [AIRFLOW-1263] We use the number of tasks in the DAG as a heuristic to",
            "    approximate the size of generated chart (otherwise the charts are tiny and unreadable",
            "    when DAGs have a large number of tasks). Ideally nvd3 should allow for dynamic-height",
            "    charts, that is charts that take up space based on the size of the components within.",
            "    \"\"\"",
            "    return 600 + len(dag.tasks) * 10",
            "",
            "",
            "class UtcAwareFilterMixin(object):",
            "    def apply(self, query, value):",
            "        value = timezone.parse(value, timezone=timezone.utc)",
            "",
            "        return super(UtcAwareFilterMixin, self).apply(query, value)",
            "",
            "",
            "class UtcAwareFilterEqual(UtcAwareFilterMixin, fab_sqlafilters.FilterEqual):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterGreater(UtcAwareFilterMixin, fab_sqlafilters.FilterGreater):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterSmaller(UtcAwareFilterMixin, fab_sqlafilters.FilterSmaller):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterNotEqual(UtcAwareFilterMixin, fab_sqlafilters.FilterNotEqual):",
            "    pass",
            "",
            "",
            "class UtcAwareFilterConverter(fab_sqlafilters.SQLAFilterConverter):",
            "",
            "    conversion_table = (",
            "        (('is_utcdatetime', [UtcAwareFilterEqual,",
            "                             UtcAwareFilterGreater,",
            "                             UtcAwareFilterSmaller,",
            "                             UtcAwareFilterNotEqual]),) +",
            "        fab_sqlafilters.SQLAFilterConverter.conversion_table",
            "    )",
            "",
            "",
            "class CustomSQLAInterface(SQLAInterface):",
            "    \"\"\"",
            "    FAB does not know how to handle columns with leading underscores because",
            "    they are not supported by WTForm. This hack will remove the leading",
            "    '_' from the key to lookup the column names.",
            "",
            "    \"\"\"",
            "    def __init__(self, obj):",
            "        super(CustomSQLAInterface, self).__init__(obj)",
            "",
            "        def clean_column_names():",
            "            if self.list_properties:",
            "                self.list_properties = dict(",
            "                    (k.lstrip('_'), v) for k, v in self.list_properties.items())",
            "            if self.list_columns:",
            "                self.list_columns = dict(",
            "                    (k.lstrip('_'), v) for k, v in self.list_columns.items())",
            "",
            "        clean_column_names()",
            "",
            "    def is_utcdatetime(self, col_name):",
            "        from airflow.utils.sqlalchemy import UtcDateTime",
            "        obj = self.list_columns[col_name].type",
            "        return isinstance(obj, UtcDateTime) or \\",
            "            isinstance(obj, sqla.types.TypeDecorator) and \\",
            "            isinstance(obj.impl, UtcDateTime)",
            "",
            "    filter_converter_class = UtcAwareFilterConverter"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "26": [],
            "71": [
                "get_params"
            ],
            "72": [
                "get_params"
            ],
            "73": [
                "get_params"
            ],
            "74": [
                "get_params"
            ],
            "75": [
                "get_params"
            ],
            "76": [
                "get_params"
            ],
            "77": [
                "get_params"
            ],
            "78": [
                "get_params"
            ],
            "79": [
                "get_params"
            ],
            "80": [
                "get_params"
            ],
            "81": [
                "get_params"
            ],
            "112": [
                "generate_pages"
            ],
            "114": [
                "generate_pages"
            ],
            "116": [
                "generate_pages"
            ],
            "118": [
                "generate_pages"
            ],
            "120": [
                "generate_pages"
            ],
            "122": [
                "generate_pages"
            ],
            "124": [
                "generate_pages"
            ],
            "126": [
                "generate_pages"
            ],
            "128": [
                "generate_pages"
            ],
            "130": [
                "generate_pages"
            ],
            "132": [
                "generate_pages"
            ],
            "188": [
                "generate_pages"
            ],
            "190": [
                "generate_pages"
            ]
        },
        "addLocation": []
    },
    "setup.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "     + cassandra + mongo"
            },
            "1": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": 236,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 237,
                "PatchRowcode": " devel = ["
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 238,
                "PatchRowcode": "+    'beautifulsoup4~=4.7.1',"
            },
            "4": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "     'click==6.7',"
            },
            "5": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 240,
                "PatchRowcode": "     'freezegun',"
            },
            "6": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "     'jira',"
            },
            "7": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    'lxml>=4.0.0',"
            },
            "8": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "     'mock',"
            },
            "9": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "     'mongomock',"
            },
            "10": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "     'moto==1.3.5',"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "from setuptools import setup, find_packages, Command",
            "from setuptools.command.test import test as TestCommand",
            "",
            "import imp",
            "import io",
            "import logging",
            "import os",
            "import sys",
            "import subprocess",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# Kept manually in sync with airflow.__version__",
            "version = imp.load_source(",
            "    'airflow.version', os.path.join('airflow', 'version.py')).version",
            "",
            "PY3 = sys.version_info[0] == 3",
            "",
            "with io.open('README.md', encoding='utf-8') as f:",
            "    long_description = f.read()",
            "",
            "",
            "class Tox(TestCommand):",
            "    user_options = [('tox-args=', None, \"Arguments to pass to tox\")]",
            "",
            "    def initialize_options(self):",
            "        TestCommand.initialize_options(self)",
            "        self.tox_args = ''",
            "",
            "    def finalize_options(self):",
            "        TestCommand.finalize_options(self)",
            "        self.test_args = []",
            "        self.test_suite = True",
            "",
            "    def run_tests(self):",
            "        # import here, cause outside the eggs aren't loaded",
            "        import tox",
            "        errno = tox.cmdline(args=self.tox_args.split())",
            "        sys.exit(errno)",
            "",
            "",
            "class CleanCommand(Command):",
            "    \"\"\"Custom clean command to tidy up the project root.\"\"\"",
            "    user_options = []",
            "",
            "    def initialize_options(self):",
            "        pass",
            "",
            "    def finalize_options(self):",
            "        pass",
            "",
            "    def run(self):",
            "        os.system('rm -vrf ./build ./dist ./*.pyc ./*.tgz ./*.egg-info')",
            "",
            "",
            "class CompileAssets(Command):",
            "    \"\"\"",
            "    Custom compile assets command to compile and build the frontend",
            "    assets using npm and webpack.",
            "    \"\"\"",
            "    user_options = []",
            "",
            "    def initialize_options(self):",
            "        pass",
            "",
            "    def finalize_options(self):",
            "        pass",
            "",
            "    def run(self):",
            "        subprocess.call('./airflow/www/compile_assets.sh')",
            "",
            "",
            "def git_version(version):",
            "    \"\"\"",
            "    Return a version to identify the state of the underlying git repo. The version will",
            "    indicate whether the head of the current git-backed working directory is tied to a",
            "    release tag or not : it will indicate the former with a 'release:{version}' prefix",
            "    and the latter with a 'dev0' prefix. Following the prefix will be a sha of the current",
            "    branch head. Finally, a \"dirty\" suffix is appended to indicate that uncommitted",
            "    changes are present.",
            "    \"\"\"",
            "    repo = None",
            "    try:",
            "        import git",
            "        repo = git.Repo('.git')",
            "    except ImportError:",
            "        logger.warning('gitpython not found: Cannot compute the git version.')",
            "        return ''",
            "    except Exception as e:",
            "        logger.warning('Cannot compute the git version. {}'.format(e))",
            "        return ''",
            "    if repo:",
            "        sha = repo.head.commit.hexsha",
            "        if repo.is_dirty():",
            "            return '.dev0+{sha}.dirty'.format(sha=sha)",
            "        # commit is clean",
            "        return '.release:{version}+{sha}'.format(version=version, sha=sha)",
            "    else:",
            "        return 'no_git_version'",
            "",
            "",
            "def write_version(filename=os.path.join(*['airflow',",
            "                                          'git_version'])):",
            "    text = \"{}\".format(git_version(version))",
            "    with open(filename, 'w') as a:",
            "        a.write(text)",
            "",
            "",
            "async_packages = [",
            "    'greenlet>=0.4.9',",
            "    'eventlet>= 0.9.7',",
            "    'gevent>=0.13'",
            "]",
            "atlas = ['atlasclient>=0.1.2']",
            "aws = [",
            "    'boto3>=1.7.0, <1.8.0',",
            "]",
            "azure = [",
            "    'azure-storage>=0.34.0',",
            "    'azure-mgmt-resource==1.2.2',",
            "    'azure-mgmt-datalake-store==0.4.0',",
            "    'azure-datalake-store==0.0.19',",
            "    'azure-cosmos>=3.0.1',",
            "    'azure-mgmt-containerinstance',",
            "]",
            "cassandra = ['cassandra-driver>=3.13.0']",
            "celery = [",
            "    'celery>=4.1.1, <4.2.0',",
            "    'flower>=0.7.3, <1.0',",
            "    'tornado>=4.2.0, <6.0',  # Dep of flower. Pin to a version that works on Py3.5.2",
            "]",
            "cgroups = [",
            "    'cgroupspy>=0.1.4',",
            "]",
            "# major update coming soon, clamp to 0.x",
            "cloudant = ['cloudant>=0.5.9,<2.0']",
            "crypto = ['cryptography>=0.9.3']",
            "dask = [",
            "    'distributed>=1.17.1, <2'",
            "]",
            "databricks = ['requests>=2.20.0, <3']",
            "datadog = ['datadog>=0.14.0']",
            "doc = [",
            "    'sphinx>=1.2.3',",
            "    'sphinx-argparse>=0.1.13',",
            "    'sphinx-rtd-theme>=0.1.6',",
            "    'sphinxcontrib-httpdomain>=1.7.0',",
            "    'Sphinx-PyPI-upload>=0.2.1'",
            "]",
            "docker = ['docker~=3.0']",
            "druid = ['pydruid>=0.4.1']",
            "elasticsearch = [",
            "    'elasticsearch>=5.0.0,<6.0.0',",
            "    'elasticsearch-dsl>=5.0.0,<6.0.0'",
            "]",
            "gcp = [",
            "    'httplib2>=0.9.2',",
            "    'google-api-python-client>=1.6.0, <2.0.0dev',",
            "    'google-auth>=1.0.0, <2.0.0dev',",
            "    'google-auth-httplib2>=0.0.1',",
            "    'google-cloud-container>=0.1.1',",
            "    'google-cloud-bigtable==0.31.0',",
            "    'google-cloud-spanner>=1.7.1',",
            "    'google-cloud-translate>=1.3.3',",
            "    'google-cloud-vision>=0.35.2',",
            "    'grpcio-gcp>=0.2.2',",
            "    'PyOpenSSL',",
            "    'pandas-gbq'",
            "]",
            "github_enterprise = ['Flask-OAuthlib>=0.9.1']",
            "grpc = ['grpcio>=1.15.0']",
            "google_auth = ['Flask-OAuthlib>=0.9.1']",
            "hdfs = ['snakebite>=2.7.8']",
            "hive = [",
            "    'hmsclient>=0.1.0',",
            "    'pyhive>=0.6.0',",
            "]",
            "jdbc = ['jaydebeapi>=1.1.1']",
            "jenkins = ['python-jenkins>=1.0.0']",
            "jira = ['JIRA>1.0.7']",
            "kerberos = ['pykerberos>=1.1.13',",
            "            'requests_kerberos>=0.10.0',",
            "            'thrift_sasl>=0.2.0',",
            "            'snakebite[kerberos]>=2.7.8']",
            "kubernetes = ['kubernetes>=3.0.0',",
            "              'cryptography>=2.0.0']",
            "ldap = ['ldap3>=2.5.1']",
            "mssql = ['pymssql>=2.1.1']",
            "mysql = ['mysqlclient>=1.3.6,<1.4']",
            "oracle = ['cx_Oracle>=5.1.2']",
            "password = [",
            "    'bcrypt>=2.0.0',",
            "    'flask-bcrypt>=0.7.1',",
            "]",
            "pinot = ['pinotdb==0.1.1']",
            "postgres = ['psycopg2>=2.7.4']",
            "qds = ['qds-sdk>=1.10.4']",
            "rabbitmq = ['librabbitmq>=1.6.1']",
            "redis = ['redis~=3.2']",
            "salesforce = ['simple-salesforce>=0.72']",
            "samba = ['pysmbclient>=0.1.3']",
            "segment = ['analytics-python>=1.2.9']",
            "sendgrid = ['sendgrid>=5.2.0']",
            "slack = ['slackclient>=1.0.0']",
            "mongo = ['pymongo>=3.6.0']",
            "snowflake = ['snowflake-connector-python>=1.5.2',",
            "             'snowflake-sqlalchemy>=1.1.0']",
            "ssh = ['paramiko>=2.1.1', 'pysftp>=0.2.9', 'sshtunnel>=0.1.4,<0.2']",
            "statsd = ['statsd>=3.0.1, <4.0']",
            "vertica = ['vertica-python>=0.5.1']",
            "webhdfs = ['hdfs[dataframe,avro,kerberos]>=2.0.4']",
            "winrm = ['pywinrm==0.2.2']",
            "zendesk = ['zdesk']",
            "",
            "all_dbs = postgres + mysql + hive + mssql + hdfs + vertica + cloudant + druid + pinot \\",
            "    + cassandra + mongo",
            "",
            "devel = [",
            "    'click==6.7',",
            "    'freezegun',",
            "    'jira',",
            "    'lxml>=4.0.0',",
            "    'mock',",
            "    'mongomock',",
            "    'moto==1.3.5',",
            "    'nose',",
            "    'nose-ignore-docstring==0.2',",
            "    'nose-timer',",
            "    'parameterized',",
            "    'paramiko',",
            "    'pysftp',",
            "    'pywinrm',",
            "    'qds-sdk>=1.9.6',",
            "    'rednose',",
            "    'requests_mock',",
            "    'flake8>=3.6.0',",
            "]",
            "",
            "if not PY3:",
            "    devel += ['unittest2']",
            "",
            "devel_minreq = devel + kubernetes + mysql + doc + password + cgroups",
            "devel_hadoop = devel_minreq + hive + hdfs + webhdfs + kerberos",
            "devel_all = (sendgrid + devel + all_dbs + doc + samba + slack + crypto + oracle +",
            "             docker + ssh + kubernetes + celery + redis + gcp + grpc +",
            "             datadog + zendesk + jdbc + ldap + kerberos + password + webhdfs + jenkins +",
            "             druid + pinot + segment + snowflake + elasticsearch +",
            "             atlas + azure + aws)",
            "",
            "# Snakebite & Google Cloud Dataflow are not Python 3 compatible :'(",
            "if PY3:",
            "    devel_ci = [package for package in devel_all if package not in",
            "                ['snakebite>=2.7.8', 'snakebite[kerberos]>=2.7.8']]",
            "else:",
            "    devel_ci = devel_all",
            "",
            "",
            "def do_setup():",
            "    write_version()",
            "    setup(",
            "        name='apache-airflow',",
            "        description='Programmatically author, schedule and monitor data pipelines',",
            "        long_description=long_description,",
            "        long_description_content_type='text/markdown',",
            "        license='Apache License 2.0',",
            "        version=version,",
            "        packages=find_packages(exclude=['tests*']),",
            "        package_data={'': ['airflow/alembic.ini', \"airflow/git_version\"]},",
            "        include_package_data=True,",
            "        zip_safe=False,",
            "        scripts=['airflow/bin/airflow'],",
            "        install_requires=[",
            "            'alembic>=0.9, <1.0',",
            "            'configparser>=3.5.0, <3.6.0',",
            "            'croniter>=0.3.17, <0.4',",
            "            'dill>=0.2.2, <0.3',",
            "            'enum34~=1.1.6;python_version<\"3.4\"',",
            "            'flask>=1.0, <2.0',",
            "            'flask-appbuilder==1.12.3',",
            "            'flask-caching>=1.3.3, <1.4.0',",
            "            'flask-login>=0.3, <0.5',",
            "            'flask-swagger==0.2.13',",
            "            'flask-wtf>=0.14.2, <0.15',",
            "            'funcsigs==1.0.0',",
            "            'future>=0.16.0, <0.17',",
            "            'gitpython>=2.0.2',",
            "            'gunicorn>=19.5.0, <20.0',",
            "            'iso8601>=0.1.12',",
            "            'json-merge-patch==0.2',",
            "            'jinja2>=2.7.3, <=2.10.0',",
            "            'markdown>=2.5.2, <3.0',",
            "            'pandas>=0.17.1, <1.0.0',",
            "            'pendulum==1.4.4',",
            "            'psutil>=4.2.0, <6.0.0',",
            "            'pygments>=2.0.1, <3.0',",
            "            'python-daemon>=2.1.1, <2.2',",
            "            'python-dateutil>=2.3, <3',",
            "            'requests>=2.20.0, <3',",
            "            'setproctitle>=1.1.8, <2',",
            "            'sqlalchemy>=1.1.15, <1.3.0',",
            "            'tabulate>=0.7.5, <0.9',",
            "            'tenacity==4.12.0',",
            "            'text-unidecode==1.2',",
            "            'typing;python_version<\"3.5\"',",
            "            'thrift>=0.9.2',",
            "            'tzlocal>=1.4',",
            "            'unicodecsv>=0.14.1',",
            "            'werkzeug>=0.14.1, <0.15.0',",
            "            'zope.deprecation>=4.0, <5.0',",
            "        ],",
            "        setup_requires=[",
            "            'docutils>=0.14, <1.0',",
            "        ],",
            "        extras_require={",
            "            'all': devel_all,",
            "            'devel_ci': devel_ci,",
            "            'all_dbs': all_dbs,",
            "            'atlas': atlas,",
            "            'async': async_packages,",
            "            'aws': aws,",
            "            'azure': azure,",
            "            'cassandra': cassandra,",
            "            'celery': celery,",
            "            'cgroups': cgroups,",
            "            'cloudant': cloudant,",
            "            'crypto': crypto,",
            "            'dask': dask,",
            "            'databricks': databricks,",
            "            'datadog': datadog,",
            "            'devel': devel_minreq,",
            "            'devel_hadoop': devel_hadoop,",
            "            'doc': doc,",
            "            'docker': docker,",
            "            'druid': druid,",
            "            'elasticsearch': elasticsearch,",
            "            'gcp': gcp,",
            "            'gcp_api': gcp,  # TODO: remove this in Airflow 2.1",
            "            'github_enterprise': github_enterprise,",
            "            'google_auth': google_auth,",
            "            'grpc': grpc,",
            "            'hdfs': hdfs,",
            "            'hive': hive,",
            "            'jdbc': jdbc,",
            "            'jira': jira,",
            "            'kerberos': kerberos,",
            "            'kubernetes': kubernetes,",
            "            'ldap': ldap,",
            "            'mongo': mongo,",
            "            'mssql': mssql,",
            "            'mysql': mysql,",
            "            'oracle': oracle,",
            "            'password': password,",
            "            'pinot': pinot,",
            "            'postgres': postgres,",
            "            'qds': qds,",
            "            'rabbitmq': rabbitmq,",
            "            'redis': redis,",
            "            'salesforce': salesforce,",
            "            'samba': samba,",
            "            'sendgrid': sendgrid,",
            "            'segment': segment,",
            "            'slack': slack,",
            "            'snowflake': snowflake,",
            "            'ssh': ssh,",
            "            'statsd': statsd,",
            "            'vertica': vertica,",
            "            'webhdfs': webhdfs,",
            "            'winrm': winrm",
            "        },",
            "        classifiers=[",
            "            'Development Status :: 5 - Production/Stable',",
            "            'Environment :: Console',",
            "            'Environment :: Web Environment',",
            "            'Intended Audience :: Developers',",
            "            'Intended Audience :: System Administrators',",
            "            'License :: OSI Approved :: Apache Software License',",
            "            'Programming Language :: Python :: 2.7',",
            "            'Programming Language :: Python :: 3.5',",
            "            'Topic :: System :: Monitoring',",
            "        ],",
            "        author='Apache Software Foundation',",
            "        author_email='dev@airflow.apache.org',",
            "        url='http://airflow.apache.org/',",
            "        download_url=(",
            "            'https://dist.apache.org/repos/dist/release/airflow/' + version),",
            "        cmdclass={",
            "            'test': Tox,",
            "            'extra_clean': CleanCommand,",
            "            'compile_assets': CompileAssets",
            "        },",
            "        python_requires='>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*',",
            "    )",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    do_setup()"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "from setuptools import setup, find_packages, Command",
            "from setuptools.command.test import test as TestCommand",
            "",
            "import imp",
            "import io",
            "import logging",
            "import os",
            "import sys",
            "import subprocess",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# Kept manually in sync with airflow.__version__",
            "version = imp.load_source(",
            "    'airflow.version', os.path.join('airflow', 'version.py')).version",
            "",
            "PY3 = sys.version_info[0] == 3",
            "",
            "with io.open('README.md', encoding='utf-8') as f:",
            "    long_description = f.read()",
            "",
            "",
            "class Tox(TestCommand):",
            "    user_options = [('tox-args=', None, \"Arguments to pass to tox\")]",
            "",
            "    def initialize_options(self):",
            "        TestCommand.initialize_options(self)",
            "        self.tox_args = ''",
            "",
            "    def finalize_options(self):",
            "        TestCommand.finalize_options(self)",
            "        self.test_args = []",
            "        self.test_suite = True",
            "",
            "    def run_tests(self):",
            "        # import here, cause outside the eggs aren't loaded",
            "        import tox",
            "        errno = tox.cmdline(args=self.tox_args.split())",
            "        sys.exit(errno)",
            "",
            "",
            "class CleanCommand(Command):",
            "    \"\"\"Custom clean command to tidy up the project root.\"\"\"",
            "    user_options = []",
            "",
            "    def initialize_options(self):",
            "        pass",
            "",
            "    def finalize_options(self):",
            "        pass",
            "",
            "    def run(self):",
            "        os.system('rm -vrf ./build ./dist ./*.pyc ./*.tgz ./*.egg-info')",
            "",
            "",
            "class CompileAssets(Command):",
            "    \"\"\"",
            "    Custom compile assets command to compile and build the frontend",
            "    assets using npm and webpack.",
            "    \"\"\"",
            "    user_options = []",
            "",
            "    def initialize_options(self):",
            "        pass",
            "",
            "    def finalize_options(self):",
            "        pass",
            "",
            "    def run(self):",
            "        subprocess.call('./airflow/www/compile_assets.sh')",
            "",
            "",
            "def git_version(version):",
            "    \"\"\"",
            "    Return a version to identify the state of the underlying git repo. The version will",
            "    indicate whether the head of the current git-backed working directory is tied to a",
            "    release tag or not : it will indicate the former with a 'release:{version}' prefix",
            "    and the latter with a 'dev0' prefix. Following the prefix will be a sha of the current",
            "    branch head. Finally, a \"dirty\" suffix is appended to indicate that uncommitted",
            "    changes are present.",
            "    \"\"\"",
            "    repo = None",
            "    try:",
            "        import git",
            "        repo = git.Repo('.git')",
            "    except ImportError:",
            "        logger.warning('gitpython not found: Cannot compute the git version.')",
            "        return ''",
            "    except Exception as e:",
            "        logger.warning('Cannot compute the git version. {}'.format(e))",
            "        return ''",
            "    if repo:",
            "        sha = repo.head.commit.hexsha",
            "        if repo.is_dirty():",
            "            return '.dev0+{sha}.dirty'.format(sha=sha)",
            "        # commit is clean",
            "        return '.release:{version}+{sha}'.format(version=version, sha=sha)",
            "    else:",
            "        return 'no_git_version'",
            "",
            "",
            "def write_version(filename=os.path.join(*['airflow',",
            "                                          'git_version'])):",
            "    text = \"{}\".format(git_version(version))",
            "    with open(filename, 'w') as a:",
            "        a.write(text)",
            "",
            "",
            "async_packages = [",
            "    'greenlet>=0.4.9',",
            "    'eventlet>= 0.9.7',",
            "    'gevent>=0.13'",
            "]",
            "atlas = ['atlasclient>=0.1.2']",
            "aws = [",
            "    'boto3>=1.7.0, <1.8.0',",
            "]",
            "azure = [",
            "    'azure-storage>=0.34.0',",
            "    'azure-mgmt-resource==1.2.2',",
            "    'azure-mgmt-datalake-store==0.4.0',",
            "    'azure-datalake-store==0.0.19',",
            "    'azure-cosmos>=3.0.1',",
            "    'azure-mgmt-containerinstance',",
            "]",
            "cassandra = ['cassandra-driver>=3.13.0']",
            "celery = [",
            "    'celery>=4.1.1, <4.2.0',",
            "    'flower>=0.7.3, <1.0',",
            "    'tornado>=4.2.0, <6.0',  # Dep of flower. Pin to a version that works on Py3.5.2",
            "]",
            "cgroups = [",
            "    'cgroupspy>=0.1.4',",
            "]",
            "# major update coming soon, clamp to 0.x",
            "cloudant = ['cloudant>=0.5.9,<2.0']",
            "crypto = ['cryptography>=0.9.3']",
            "dask = [",
            "    'distributed>=1.17.1, <2'",
            "]",
            "databricks = ['requests>=2.20.0, <3']",
            "datadog = ['datadog>=0.14.0']",
            "doc = [",
            "    'sphinx>=1.2.3',",
            "    'sphinx-argparse>=0.1.13',",
            "    'sphinx-rtd-theme>=0.1.6',",
            "    'sphinxcontrib-httpdomain>=1.7.0',",
            "    'Sphinx-PyPI-upload>=0.2.1'",
            "]",
            "docker = ['docker~=3.0']",
            "druid = ['pydruid>=0.4.1']",
            "elasticsearch = [",
            "    'elasticsearch>=5.0.0,<6.0.0',",
            "    'elasticsearch-dsl>=5.0.0,<6.0.0'",
            "]",
            "gcp = [",
            "    'httplib2>=0.9.2',",
            "    'google-api-python-client>=1.6.0, <2.0.0dev',",
            "    'google-auth>=1.0.0, <2.0.0dev',",
            "    'google-auth-httplib2>=0.0.1',",
            "    'google-cloud-container>=0.1.1',",
            "    'google-cloud-bigtable==0.31.0',",
            "    'google-cloud-spanner>=1.7.1',",
            "    'google-cloud-translate>=1.3.3',",
            "    'google-cloud-vision>=0.35.2',",
            "    'grpcio-gcp>=0.2.2',",
            "    'PyOpenSSL',",
            "    'pandas-gbq'",
            "]",
            "github_enterprise = ['Flask-OAuthlib>=0.9.1']",
            "grpc = ['grpcio>=1.15.0']",
            "google_auth = ['Flask-OAuthlib>=0.9.1']",
            "hdfs = ['snakebite>=2.7.8']",
            "hive = [",
            "    'hmsclient>=0.1.0',",
            "    'pyhive>=0.6.0',",
            "]",
            "jdbc = ['jaydebeapi>=1.1.1']",
            "jenkins = ['python-jenkins>=1.0.0']",
            "jira = ['JIRA>1.0.7']",
            "kerberos = ['pykerberos>=1.1.13',",
            "            'requests_kerberos>=0.10.0',",
            "            'thrift_sasl>=0.2.0',",
            "            'snakebite[kerberos]>=2.7.8']",
            "kubernetes = ['kubernetes>=3.0.0',",
            "              'cryptography>=2.0.0']",
            "ldap = ['ldap3>=2.5.1']",
            "mssql = ['pymssql>=2.1.1']",
            "mysql = ['mysqlclient>=1.3.6,<1.4']",
            "oracle = ['cx_Oracle>=5.1.2']",
            "password = [",
            "    'bcrypt>=2.0.0',",
            "    'flask-bcrypt>=0.7.1',",
            "]",
            "pinot = ['pinotdb==0.1.1']",
            "postgres = ['psycopg2>=2.7.4']",
            "qds = ['qds-sdk>=1.10.4']",
            "rabbitmq = ['librabbitmq>=1.6.1']",
            "redis = ['redis~=3.2']",
            "salesforce = ['simple-salesforce>=0.72']",
            "samba = ['pysmbclient>=0.1.3']",
            "segment = ['analytics-python>=1.2.9']",
            "sendgrid = ['sendgrid>=5.2.0']",
            "slack = ['slackclient>=1.0.0']",
            "mongo = ['pymongo>=3.6.0']",
            "snowflake = ['snowflake-connector-python>=1.5.2',",
            "             'snowflake-sqlalchemy>=1.1.0']",
            "ssh = ['paramiko>=2.1.1', 'pysftp>=0.2.9', 'sshtunnel>=0.1.4,<0.2']",
            "statsd = ['statsd>=3.0.1, <4.0']",
            "vertica = ['vertica-python>=0.5.1']",
            "webhdfs = ['hdfs[dataframe,avro,kerberos]>=2.0.4']",
            "winrm = ['pywinrm==0.2.2']",
            "zendesk = ['zdesk']",
            "",
            "all_dbs = postgres + mysql + hive + mssql + hdfs + vertica + cloudant + druid + pinot \\",
            "    + cassandra + mongo",
            "",
            "devel = [",
            "    'beautifulsoup4~=4.7.1',",
            "    'click==6.7',",
            "    'freezegun',",
            "    'jira',",
            "    'mock',",
            "    'mongomock',",
            "    'moto==1.3.5',",
            "    'nose',",
            "    'nose-ignore-docstring==0.2',",
            "    'nose-timer',",
            "    'parameterized',",
            "    'paramiko',",
            "    'pysftp',",
            "    'pywinrm',",
            "    'qds-sdk>=1.9.6',",
            "    'rednose',",
            "    'requests_mock',",
            "    'flake8>=3.6.0',",
            "]",
            "",
            "if not PY3:",
            "    devel += ['unittest2']",
            "",
            "devel_minreq = devel + kubernetes + mysql + doc + password + cgroups",
            "devel_hadoop = devel_minreq + hive + hdfs + webhdfs + kerberos",
            "devel_all = (sendgrid + devel + all_dbs + doc + samba + slack + crypto + oracle +",
            "             docker + ssh + kubernetes + celery + redis + gcp + grpc +",
            "             datadog + zendesk + jdbc + ldap + kerberos + password + webhdfs + jenkins +",
            "             druid + pinot + segment + snowflake + elasticsearch +",
            "             atlas + azure + aws)",
            "",
            "# Snakebite & Google Cloud Dataflow are not Python 3 compatible :'(",
            "if PY3:",
            "    devel_ci = [package for package in devel_all if package not in",
            "                ['snakebite>=2.7.8', 'snakebite[kerberos]>=2.7.8']]",
            "else:",
            "    devel_ci = devel_all",
            "",
            "",
            "def do_setup():",
            "    write_version()",
            "    setup(",
            "        name='apache-airflow',",
            "        description='Programmatically author, schedule and monitor data pipelines',",
            "        long_description=long_description,",
            "        long_description_content_type='text/markdown',",
            "        license='Apache License 2.0',",
            "        version=version,",
            "        packages=find_packages(exclude=['tests*']),",
            "        package_data={'': ['airflow/alembic.ini', \"airflow/git_version\"]},",
            "        include_package_data=True,",
            "        zip_safe=False,",
            "        scripts=['airflow/bin/airflow'],",
            "        install_requires=[",
            "            'alembic>=0.9, <1.0',",
            "            'configparser>=3.5.0, <3.6.0',",
            "            'croniter>=0.3.17, <0.4',",
            "            'dill>=0.2.2, <0.3',",
            "            'enum34~=1.1.6;python_version<\"3.4\"',",
            "            'flask>=1.0, <2.0',",
            "            'flask-appbuilder==1.12.3',",
            "            'flask-caching>=1.3.3, <1.4.0',",
            "            'flask-login>=0.3, <0.5',",
            "            'flask-swagger==0.2.13',",
            "            'flask-wtf>=0.14.2, <0.15',",
            "            'funcsigs==1.0.0',",
            "            'future>=0.16.0, <0.17',",
            "            'gitpython>=2.0.2',",
            "            'gunicorn>=19.5.0, <20.0',",
            "            'iso8601>=0.1.12',",
            "            'json-merge-patch==0.2',",
            "            'jinja2>=2.7.3, <=2.10.0',",
            "            'markdown>=2.5.2, <3.0',",
            "            'pandas>=0.17.1, <1.0.0',",
            "            'pendulum==1.4.4',",
            "            'psutil>=4.2.0, <6.0.0',",
            "            'pygments>=2.0.1, <3.0',",
            "            'python-daemon>=2.1.1, <2.2',",
            "            'python-dateutil>=2.3, <3',",
            "            'requests>=2.20.0, <3',",
            "            'setproctitle>=1.1.8, <2',",
            "            'sqlalchemy>=1.1.15, <1.3.0',",
            "            'tabulate>=0.7.5, <0.9',",
            "            'tenacity==4.12.0',",
            "            'text-unidecode==1.2',",
            "            'typing;python_version<\"3.5\"',",
            "            'thrift>=0.9.2',",
            "            'tzlocal>=1.4',",
            "            'unicodecsv>=0.14.1',",
            "            'werkzeug>=0.14.1, <0.15.0',",
            "            'zope.deprecation>=4.0, <5.0',",
            "        ],",
            "        setup_requires=[",
            "            'docutils>=0.14, <1.0',",
            "        ],",
            "        extras_require={",
            "            'all': devel_all,",
            "            'devel_ci': devel_ci,",
            "            'all_dbs': all_dbs,",
            "            'atlas': atlas,",
            "            'async': async_packages,",
            "            'aws': aws,",
            "            'azure': azure,",
            "            'cassandra': cassandra,",
            "            'celery': celery,",
            "            'cgroups': cgroups,",
            "            'cloudant': cloudant,",
            "            'crypto': crypto,",
            "            'dask': dask,",
            "            'databricks': databricks,",
            "            'datadog': datadog,",
            "            'devel': devel_minreq,",
            "            'devel_hadoop': devel_hadoop,",
            "            'doc': doc,",
            "            'docker': docker,",
            "            'druid': druid,",
            "            'elasticsearch': elasticsearch,",
            "            'gcp': gcp,",
            "            'gcp_api': gcp,  # TODO: remove this in Airflow 2.1",
            "            'github_enterprise': github_enterprise,",
            "            'google_auth': google_auth,",
            "            'grpc': grpc,",
            "            'hdfs': hdfs,",
            "            'hive': hive,",
            "            'jdbc': jdbc,",
            "            'jira': jira,",
            "            'kerberos': kerberos,",
            "            'kubernetes': kubernetes,",
            "            'ldap': ldap,",
            "            'mongo': mongo,",
            "            'mssql': mssql,",
            "            'mysql': mysql,",
            "            'oracle': oracle,",
            "            'password': password,",
            "            'pinot': pinot,",
            "            'postgres': postgres,",
            "            'qds': qds,",
            "            'rabbitmq': rabbitmq,",
            "            'redis': redis,",
            "            'salesforce': salesforce,",
            "            'samba': samba,",
            "            'sendgrid': sendgrid,",
            "            'segment': segment,",
            "            'slack': slack,",
            "            'snowflake': snowflake,",
            "            'ssh': ssh,",
            "            'statsd': statsd,",
            "            'vertica': vertica,",
            "            'webhdfs': webhdfs,",
            "            'winrm': winrm",
            "        },",
            "        classifiers=[",
            "            'Development Status :: 5 - Production/Stable',",
            "            'Environment :: Console',",
            "            'Environment :: Web Environment',",
            "            'Intended Audience :: Developers',",
            "            'Intended Audience :: System Administrators',",
            "            'License :: OSI Approved :: Apache Software License',",
            "            'Programming Language :: Python :: 2.7',",
            "            'Programming Language :: Python :: 3.5',",
            "            'Topic :: System :: Monitoring',",
            "        ],",
            "        author='Apache Software Foundation',",
            "        author_email='dev@airflow.apache.org',",
            "        url='http://airflow.apache.org/',",
            "        download_url=(",
            "            'https://dist.apache.org/repos/dist/release/airflow/' + version),",
            "        cmdclass={",
            "            'test': Tox,",
            "            'extra_clean': CleanCommand,",
            "            'compile_assets': CompileAssets",
            "        },",
            "        python_requires='>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*',",
            "    )",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    do_setup()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "241": []
        },
        "addLocation": []
    }
}