{
    "synapse/api/errors.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 100,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "     UNREDACTED_CONTENT_DELETED = \"FI.MAU.MSC2815_UNREDACTED_CONTENT_DELETED\""
            },
            "2": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 102,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+    # Returned for federation requests where we can't process a request as we"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+    # can't ensure the sending server is in a room which is partial-stated on"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+    # our side."
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+    # Part of MSC3895."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+    UNABLE_DUE_TO_PARTIAL_STATE = \"ORG.MATRIX.MSC3895_UNABLE_DUE_TO_PARTIAL_STATE\""
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 109,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 110,
                "PatchRowcode": " class CodeMessageException(RuntimeError):"
            },
            "11": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "     \"\"\"An exception with integer code and message string attributes."
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Contains exceptions and error codes.\"\"\"",
            "",
            "import logging",
            "import typing",
            "from enum import Enum",
            "from http import HTTPStatus",
            "from typing import Any, Dict, List, Optional, Union",
            "",
            "from twisted.web import http",
            "",
            "from synapse.util import json_decoder",
            "",
            "if typing.TYPE_CHECKING:",
            "    from synapse.config.homeserver import HomeServerConfig",
            "    from synapse.types import JsonDict",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class Codes(str, Enum):",
            "    \"\"\"",
            "    All known error codes, as an enum of strings.",
            "    \"\"\"",
            "",
            "    UNRECOGNIZED = \"M_UNRECOGNIZED\"",
            "    UNAUTHORIZED = \"M_UNAUTHORIZED\"",
            "    FORBIDDEN = \"M_FORBIDDEN\"",
            "    BAD_JSON = \"M_BAD_JSON\"",
            "    NOT_JSON = \"M_NOT_JSON\"",
            "    USER_IN_USE = \"M_USER_IN_USE\"",
            "    ROOM_IN_USE = \"M_ROOM_IN_USE\"",
            "    BAD_PAGINATION = \"M_BAD_PAGINATION\"",
            "    BAD_STATE = \"M_BAD_STATE\"",
            "    UNKNOWN = \"M_UNKNOWN\"",
            "    NOT_FOUND = \"M_NOT_FOUND\"",
            "    MISSING_TOKEN = \"M_MISSING_TOKEN\"",
            "    UNKNOWN_TOKEN = \"M_UNKNOWN_TOKEN\"",
            "    GUEST_ACCESS_FORBIDDEN = \"M_GUEST_ACCESS_FORBIDDEN\"",
            "    LIMIT_EXCEEDED = \"M_LIMIT_EXCEEDED\"",
            "    CAPTCHA_NEEDED = \"M_CAPTCHA_NEEDED\"",
            "    CAPTCHA_INVALID = \"M_CAPTCHA_INVALID\"",
            "    MISSING_PARAM = \"M_MISSING_PARAM\"",
            "    INVALID_PARAM = \"M_INVALID_PARAM\"",
            "    TOO_LARGE = \"M_TOO_LARGE\"",
            "    EXCLUSIVE = \"M_EXCLUSIVE\"",
            "    THREEPID_AUTH_FAILED = \"M_THREEPID_AUTH_FAILED\"",
            "    THREEPID_IN_USE = \"M_THREEPID_IN_USE\"",
            "    THREEPID_NOT_FOUND = \"M_THREEPID_NOT_FOUND\"",
            "    THREEPID_DENIED = \"M_THREEPID_DENIED\"",
            "    INVALID_USERNAME = \"M_INVALID_USERNAME\"",
            "    SERVER_NOT_TRUSTED = \"M_SERVER_NOT_TRUSTED\"",
            "    CONSENT_NOT_GIVEN = \"M_CONSENT_NOT_GIVEN\"",
            "    CANNOT_LEAVE_SERVER_NOTICE_ROOM = \"M_CANNOT_LEAVE_SERVER_NOTICE_ROOM\"",
            "    RESOURCE_LIMIT_EXCEEDED = \"M_RESOURCE_LIMIT_EXCEEDED\"",
            "    UNSUPPORTED_ROOM_VERSION = \"M_UNSUPPORTED_ROOM_VERSION\"",
            "    INCOMPATIBLE_ROOM_VERSION = \"M_INCOMPATIBLE_ROOM_VERSION\"",
            "    WRONG_ROOM_KEYS_VERSION = \"M_WRONG_ROOM_KEYS_VERSION\"",
            "    EXPIRED_ACCOUNT = \"ORG_MATRIX_EXPIRED_ACCOUNT\"",
            "    PASSWORD_TOO_SHORT = \"M_PASSWORD_TOO_SHORT\"",
            "    PASSWORD_NO_DIGIT = \"M_PASSWORD_NO_DIGIT\"",
            "    PASSWORD_NO_UPPERCASE = \"M_PASSWORD_NO_UPPERCASE\"",
            "    PASSWORD_NO_LOWERCASE = \"M_PASSWORD_NO_LOWERCASE\"",
            "    PASSWORD_NO_SYMBOL = \"M_PASSWORD_NO_SYMBOL\"",
            "    PASSWORD_IN_DICTIONARY = \"M_PASSWORD_IN_DICTIONARY\"",
            "    WEAK_PASSWORD = \"M_WEAK_PASSWORD\"",
            "    INVALID_SIGNATURE = \"M_INVALID_SIGNATURE\"",
            "    USER_DEACTIVATED = \"M_USER_DEACTIVATED\"",
            "",
            "    # Part of MSC3848",
            "    # https://github.com/matrix-org/matrix-spec-proposals/pull/3848",
            "    ALREADY_JOINED = \"ORG.MATRIX.MSC3848.ALREADY_JOINED\"",
            "    NOT_JOINED = \"ORG.MATRIX.MSC3848.NOT_JOINED\"",
            "    INSUFFICIENT_POWER = \"ORG.MATRIX.MSC3848.INSUFFICIENT_POWER\"",
            "",
            "    # The account has been suspended on the server.",
            "    # By opposition to `USER_DEACTIVATED`, this is a reversible measure",
            "    # that can possibly be appealed and reverted.",
            "    # Part of MSC3823.",
            "    USER_ACCOUNT_SUSPENDED = \"ORG.MATRIX.MSC3823.USER_ACCOUNT_SUSPENDED\"",
            "",
            "    BAD_ALIAS = \"M_BAD_ALIAS\"",
            "    # For restricted join rules.",
            "    UNABLE_AUTHORISE_JOIN = \"M_UNABLE_TO_AUTHORISE_JOIN\"",
            "    UNABLE_TO_GRANT_JOIN = \"M_UNABLE_TO_GRANT_JOIN\"",
            "",
            "    UNREDACTED_CONTENT_DELETED = \"FI.MAU.MSC2815_UNREDACTED_CONTENT_DELETED\"",
            "",
            "",
            "class CodeMessageException(RuntimeError):",
            "    \"\"\"An exception with integer code and message string attributes.",
            "",
            "    Attributes:",
            "        code: HTTP error code",
            "        msg: string describing the error",
            "    \"\"\"",
            "",
            "    def __init__(self, code: Union[int, HTTPStatus], msg: str):",
            "        super().__init__(\"%d: %s\" % (code, msg))",
            "",
            "        # Some calls to this method pass instances of http.HTTPStatus for `code`.",
            "        # While HTTPStatus is a subclass of int, it has magic __str__ methods",
            "        # which emit `HTTPStatus.FORBIDDEN` when converted to a str, instead of `403`.",
            "        # This causes inconsistency in our log lines.",
            "        #",
            "        # To eliminate this behaviour, we convert them to their integer equivalents here.",
            "        self.code = int(code)",
            "        self.msg = msg",
            "",
            "",
            "class RedirectException(CodeMessageException):",
            "    \"\"\"A pseudo-error indicating that we want to redirect the client to a different",
            "    location",
            "",
            "    Attributes:",
            "        cookies: a list of set-cookies values to add to the response. For example:",
            "           b\"sessionId=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT\"",
            "    \"\"\"",
            "",
            "    def __init__(self, location: bytes, http_code: int = http.FOUND):",
            "        \"\"\"",
            "",
            "        Args:",
            "            location: the URI to redirect to",
            "            http_code: the HTTP response code",
            "        \"\"\"",
            "        msg = \"Redirect to %s\" % (location.decode(\"utf-8\"),)",
            "        super().__init__(code=http_code, msg=msg)",
            "        self.location = location",
            "",
            "        self.cookies: List[bytes] = []",
            "",
            "",
            "class SynapseError(CodeMessageException):",
            "    \"\"\"A base exception type for matrix errors which have an errcode and error",
            "    message (as well as an HTTP status code).",
            "",
            "    Attributes:",
            "        errcode: Matrix error code e.g 'M_FORBIDDEN'",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.UNKNOWN,",
            "        additional_fields: Optional[Dict] = None,",
            "    ):",
            "        \"\"\"Constructs a synapse error.",
            "",
            "        Args:",
            "            code: The integer error code (an HTTP response code)",
            "            msg: The human-readable error message.",
            "            errcode: The matrix error code e.g 'M_FORBIDDEN'",
            "        \"\"\"",
            "        super().__init__(code, msg)",
            "        self.errcode = errcode",
            "        if additional_fields is None:",
            "            self._additional_fields: Dict = {}",
            "        else:",
            "            self._additional_fields = dict(additional_fields)",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, **self._additional_fields)",
            "",
            "",
            "class InvalidAPICallError(SynapseError):",
            "    \"\"\"You called an existing API endpoint, but fed that endpoint",
            "    invalid or incomplete data.\"\"\"",
            "",
            "    def __init__(self, msg: str):",
            "        super().__init__(HTTPStatus.BAD_REQUEST, msg, Codes.BAD_JSON)",
            "",
            "",
            "class ProxiedRequestError(SynapseError):",
            "    \"\"\"An error from a general matrix endpoint, eg. from a proxied Matrix API call.",
            "",
            "    Attributes:",
            "        errcode: Matrix error code e.g 'M_FORBIDDEN'",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.UNKNOWN,",
            "        additional_fields: Optional[Dict] = None,",
            "    ):",
            "        super().__init__(code, msg, errcode, additional_fields)",
            "",
            "",
            "class ConsentNotGivenError(SynapseError):",
            "    \"\"\"The error returned to the client when the user has not consented to the",
            "    privacy policy.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg: str, consent_uri: str):",
            "        \"\"\"Constructs a ConsentNotGivenError",
            "",
            "        Args:",
            "            msg: The human-readable error message",
            "            consent_url: The URL where the user can give their consent",
            "        \"\"\"",
            "        super().__init__(",
            "            code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.CONSENT_NOT_GIVEN",
            "        )",
            "        self._consent_uri = consent_uri",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, consent_uri=self._consent_uri)",
            "",
            "",
            "class UserDeactivatedError(SynapseError):",
            "    \"\"\"The error returned to the client when the user attempted to access an",
            "    authenticated endpoint, but the account has been deactivated.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg: str):",
            "        \"\"\"Constructs a UserDeactivatedError",
            "",
            "        Args:",
            "            msg: The human-readable error message",
            "        \"\"\"",
            "        super().__init__(",
            "            code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.USER_DEACTIVATED",
            "        )",
            "",
            "",
            "class FederationDeniedError(SynapseError):",
            "    \"\"\"An error raised when the server tries to federate with a server which",
            "    is not on its federation whitelist.",
            "",
            "    Attributes:",
            "        destination: The destination which has been denied",
            "    \"\"\"",
            "",
            "    def __init__(self, destination: Optional[str]):",
            "        \"\"\"Raised by federation client or server to indicate that we are",
            "        are deliberately not attempting to contact a given server because it is",
            "        not on our federation whitelist.",
            "",
            "        Args:",
            "            destination: the domain in question",
            "        \"\"\"",
            "",
            "        self.destination = destination",
            "",
            "        super().__init__(",
            "            code=403,",
            "            msg=\"Federation denied with %s.\" % (self.destination,),",
            "            errcode=Codes.FORBIDDEN,",
            "        )",
            "",
            "",
            "class InteractiveAuthIncompleteError(Exception):",
            "    \"\"\"An error raised when UI auth is not yet complete",
            "",
            "    (This indicates we should return a 401 with 'result' as the body)",
            "",
            "    Attributes:",
            "        session_id: The ID of the ongoing interactive auth session.",
            "        result: the server response to the request, which should be",
            "            passed back to the client",
            "    \"\"\"",
            "",
            "    def __init__(self, session_id: str, result: \"JsonDict\"):",
            "        super().__init__(\"Interactive auth not yet complete\")",
            "        self.session_id = session_id",
            "        self.result = result",
            "",
            "",
            "class UnrecognizedRequestError(SynapseError):",
            "    \"\"\"An error indicating we don't understand the request you're trying to make\"\"\"",
            "",
            "    def __init__(",
            "        self, msg: str = \"Unrecognized request\", errcode: str = Codes.UNRECOGNIZED",
            "    ):",
            "        super().__init__(400, msg, errcode)",
            "",
            "",
            "class NotFoundError(SynapseError):",
            "    \"\"\"An error indicating we can't find the thing you asked for\"\"\"",
            "",
            "    def __init__(self, msg: str = \"Not found\", errcode: str = Codes.NOT_FOUND):",
            "        super().__init__(404, msg, errcode=errcode)",
            "",
            "",
            "class AuthError(SynapseError):",
            "    \"\"\"An error raised when there was a problem authorising an event, and at various",
            "    other poorly-defined times.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.FORBIDDEN,",
            "        additional_fields: Optional[dict] = None,",
            "    ):",
            "        super().__init__(code, msg, errcode, additional_fields)",
            "",
            "",
            "class UnstableSpecAuthError(AuthError):",
            "    \"\"\"An error raised when a new error code is being proposed to replace a previous one.",
            "    This error will return a \"org.matrix.unstable.errcode\" property with the new error code,",
            "    with the previous error code still being defined in the \"errcode\" property.",
            "",
            "    This error will include `org.matrix.msc3848.unstable.errcode` in the C-S error body.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str,",
            "        previous_errcode: str = Codes.FORBIDDEN,",
            "        additional_fields: Optional[dict] = None,",
            "    ):",
            "        self.previous_errcode = previous_errcode",
            "        super().__init__(code, msg, errcode, additional_fields)",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        fields = {}",
            "        if config is not None and config.experimental.msc3848_enabled:",
            "            fields[\"org.matrix.msc3848.unstable.errcode\"] = self.errcode",
            "        return cs_error(",
            "            self.msg,",
            "            self.previous_errcode,",
            "            **fields,",
            "            **self._additional_fields,",
            "        )",
            "",
            "",
            "class InvalidClientCredentialsError(SynapseError):",
            "    \"\"\"An error raised when there was a problem with the authorisation credentials",
            "    in a client request.",
            "",
            "    https://matrix.org/docs/spec/client_server/r0.5.0#using-access-tokens:",
            "",
            "    When credentials are required but missing or invalid, the HTTP call will",
            "    return with a status of 401 and the error code, M_MISSING_TOKEN or",
            "    M_UNKNOWN_TOKEN respectively.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg: str, errcode: str):",
            "        super().__init__(code=401, msg=msg, errcode=errcode)",
            "",
            "",
            "class MissingClientTokenError(InvalidClientCredentialsError):",
            "    \"\"\"Raised when we couldn't find the access token in a request\"\"\"",
            "",
            "    def __init__(self, msg: str = \"Missing access token\"):",
            "        super().__init__(msg=msg, errcode=\"M_MISSING_TOKEN\")",
            "",
            "",
            "class InvalidClientTokenError(InvalidClientCredentialsError):",
            "    \"\"\"Raised when we didn't understand the access token in a request\"\"\"",
            "",
            "    def __init__(",
            "        self, msg: str = \"Unrecognised access token\", soft_logout: bool = False",
            "    ):",
            "        super().__init__(msg=msg, errcode=\"M_UNKNOWN_TOKEN\")",
            "        self._soft_logout = soft_logout",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        d = super().error_dict(config)",
            "        d[\"soft_logout\"] = self._soft_logout",
            "        return d",
            "",
            "",
            "class ResourceLimitError(SynapseError):",
            "    \"\"\"",
            "    Any error raised when there is a problem with resource usage.",
            "    For instance, the monthly active user limit for the server has been exceeded",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.RESOURCE_LIMIT_EXCEEDED,",
            "        admin_contact: Optional[str] = None,",
            "        limit_type: Optional[str] = None,",
            "    ):",
            "        self.admin_contact = admin_contact",
            "        self.limit_type = limit_type",
            "        super().__init__(code, msg, errcode=errcode)",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(",
            "            self.msg,",
            "            self.errcode,",
            "            admin_contact=self.admin_contact,",
            "            limit_type=self.limit_type,",
            "        )",
            "",
            "",
            "class EventSizeError(SynapseError):",
            "    \"\"\"An error raised when an event is too big.\"\"\"",
            "",
            "    def __init__(self, msg: str):",
            "        super().__init__(413, msg, Codes.TOO_LARGE)",
            "",
            "",
            "class LoginError(SynapseError):",
            "    \"\"\"An error raised when there was a problem logging in.\"\"\"",
            "",
            "",
            "class StoreError(SynapseError):",
            "    \"\"\"An error raised when there was a problem storing some data.\"\"\"",
            "",
            "",
            "class InvalidCaptchaError(SynapseError):",
            "    def __init__(",
            "        self,",
            "        code: int = 400,",
            "        msg: str = \"Invalid captcha.\",",
            "        error_url: Optional[str] = None,",
            "        errcode: str = Codes.CAPTCHA_INVALID,",
            "    ):",
            "        super().__init__(code, msg, errcode)",
            "        self.error_url = error_url",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, error_url=self.error_url)",
            "",
            "",
            "class LimitExceededError(SynapseError):",
            "    \"\"\"A client has sent too many requests and is being throttled.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int = 429,",
            "        msg: str = \"Too Many Requests\",",
            "        retry_after_ms: Optional[int] = None,",
            "        errcode: str = Codes.LIMIT_EXCEEDED,",
            "    ):",
            "        super().__init__(code, msg, errcode)",
            "        self.retry_after_ms = retry_after_ms",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, retry_after_ms=self.retry_after_ms)",
            "",
            "",
            "class RoomKeysVersionError(SynapseError):",
            "    \"\"\"A client has tried to upload to a non-current version of the room_keys store\"\"\"",
            "",
            "    def __init__(self, current_version: str):",
            "        \"\"\"",
            "        Args:",
            "            current_version: the current version of the store they should have used",
            "        \"\"\"",
            "        super().__init__(403, \"Wrong room_keys version\", Codes.WRONG_ROOM_KEYS_VERSION)",
            "        self.current_version = current_version",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, current_version=self.current_version)",
            "",
            "",
            "class UnsupportedRoomVersionError(SynapseError):",
            "    \"\"\"The client's request to create a room used a room version that the server does",
            "    not support.\"\"\"",
            "",
            "    def __init__(self, msg: str = \"Homeserver does not support this room version\"):",
            "        super().__init__(",
            "            code=400,",
            "            msg=msg,",
            "            errcode=Codes.UNSUPPORTED_ROOM_VERSION,",
            "        )",
            "",
            "",
            "class ThreepidValidationError(SynapseError):",
            "    \"\"\"An error raised when there was a problem authorising an event.\"\"\"",
            "",
            "    def __init__(self, msg: str, errcode: str = Codes.FORBIDDEN):",
            "        super().__init__(400, msg, errcode)",
            "",
            "",
            "class IncompatibleRoomVersionError(SynapseError):",
            "    \"\"\"A server is trying to join a room whose version it does not support.",
            "",
            "    Unlike UnsupportedRoomVersionError, it is specific to the case of the make_join",
            "    failing.",
            "    \"\"\"",
            "",
            "    def __init__(self, room_version: str):",
            "        super().__init__(",
            "            code=400,",
            "            msg=\"Your homeserver does not support the features required to \"",
            "            \"interact with this room\",",
            "            errcode=Codes.INCOMPATIBLE_ROOM_VERSION,",
            "        )",
            "",
            "        self._room_version = room_version",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, room_version=self._room_version)",
            "",
            "",
            "class PasswordRefusedError(SynapseError):",
            "    \"\"\"A password has been refused, either during password reset/change or registration.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        msg: str = \"This password doesn't comply with the server's policy\",",
            "        errcode: str = Codes.WEAK_PASSWORD,",
            "    ):",
            "        super().__init__(",
            "            code=400,",
            "            msg=msg,",
            "            errcode=errcode,",
            "        )",
            "",
            "",
            "class RequestSendFailed(RuntimeError):",
            "    \"\"\"Sending a HTTP request over federation failed due to not being able to",
            "    talk to the remote server for some reason.",
            "",
            "    This exception is used to differentiate \"expected\" errors that arise due to",
            "    networking (e.g. DNS failures, connection timeouts etc), versus unexpected",
            "    errors (like programming errors).",
            "    \"\"\"",
            "",
            "    def __init__(self, inner_exception: BaseException, can_retry: bool):",
            "        super().__init__(",
            "            \"Failed to send request: %s: %s\"",
            "            % (type(inner_exception).__name__, inner_exception)",
            "        )",
            "        self.inner_exception = inner_exception",
            "        self.can_retry = can_retry",
            "",
            "",
            "class UnredactedContentDeletedError(SynapseError):",
            "    def __init__(self, content_keep_ms: Optional[int] = None):",
            "        super().__init__(",
            "            404,",
            "            \"The content for that event has already been erased from the database\",",
            "            errcode=Codes.UNREDACTED_CONTENT_DELETED,",
            "        )",
            "        self.content_keep_ms = content_keep_ms",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        extra = {}",
            "        if self.content_keep_ms is not None:",
            "            extra = {\"fi.mau.msc2815.content_keep_ms\": self.content_keep_ms}",
            "        return cs_error(self.msg, self.errcode, **extra)",
            "",
            "",
            "def cs_error(msg: str, code: str = Codes.UNKNOWN, **kwargs: Any) -> \"JsonDict\":",
            "    \"\"\"Utility method for constructing an error response for client-server",
            "    interactions.",
            "",
            "    Args:",
            "        msg: The error message.",
            "        code: The error code.",
            "        kwargs: Additional keys to add to the response.",
            "    Returns:",
            "        A dict representing the error response JSON.",
            "    \"\"\"",
            "    err = {\"error\": msg, \"errcode\": code}",
            "    for key, value in kwargs.items():",
            "        err[key] = value",
            "    return err",
            "",
            "",
            "class FederationError(RuntimeError):",
            "    \"\"\"This class is used to inform remote homeservers about erroneous",
            "    PDUs they sent us.",
            "",
            "    FATAL: The remote server could not interpret the source event.",
            "        (e.g., it was missing a required field)",
            "    ERROR: The remote server interpreted the event, but it failed some other",
            "        check (e.g. auth)",
            "    WARN: The remote server accepted the event, but believes some part of it",
            "        is wrong (e.g., it referred to an invalid event)",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        level: str,",
            "        code: int,",
            "        reason: str,",
            "        affected: str,",
            "        source: Optional[str] = None,",
            "    ):",
            "        if level not in [\"FATAL\", \"ERROR\", \"WARN\"]:",
            "            raise ValueError(\"Level is not valid: %s\" % (level,))",
            "        self.level = level",
            "        self.code = code",
            "        self.reason = reason",
            "        self.affected = affected",
            "        self.source = source",
            "",
            "        msg = \"%s %s: %s\" % (level, code, reason)",
            "        super().__init__(msg)",
            "",
            "    def get_dict(self) -> \"JsonDict\":",
            "        return {",
            "            \"level\": self.level,",
            "            \"code\": self.code,",
            "            \"reason\": self.reason,",
            "            \"affected\": self.affected,",
            "            \"source\": self.source if self.source else self.affected,",
            "        }",
            "",
            "",
            "class HttpResponseException(CodeMessageException):",
            "    \"\"\"",
            "    Represents an HTTP-level failure of an outbound request",
            "",
            "    Attributes:",
            "        response: body of response",
            "    \"\"\"",
            "",
            "    def __init__(self, code: int, msg: str, response: bytes):",
            "        \"\"\"",
            "",
            "        Args:",
            "            code: HTTP status code",
            "            msg: reason phrase from HTTP response status line",
            "            response: body of response",
            "        \"\"\"",
            "        super().__init__(code, msg)",
            "        self.response = response",
            "",
            "    def to_synapse_error(self) -> SynapseError:",
            "        \"\"\"Make a SynapseError based on an HTTPResponseException",
            "",
            "        This is useful when a proxied request has failed, and we need to",
            "        decide how to map the failure onto a matrix error to send back to the",
            "        client.",
            "",
            "        An attempt is made to parse the body of the http response as a matrix",
            "        error. If that succeeds, the errcode and error message from the body",
            "        are used as the errcode and error message in the new synapse error.",
            "",
            "        Otherwise, the errcode is set to M_UNKNOWN, and the error message is",
            "        set to the reason code from the HTTP response.",
            "",
            "        Returns:",
            "            SynapseError:",
            "        \"\"\"",
            "        # try to parse the body as json, to get better errcode/msg, but",
            "        # default to M_UNKNOWN with the HTTP status as the error text",
            "        try:",
            "            j = json_decoder.decode(self.response.decode(\"utf-8\"))",
            "        except ValueError:",
            "            j = {}",
            "",
            "        if not isinstance(j, dict):",
            "            j = {}",
            "",
            "        errcode = j.pop(\"errcode\", Codes.UNKNOWN)",
            "        errmsg = j.pop(\"error\", self.msg)",
            "",
            "        return ProxiedRequestError(self.code, errmsg, errcode, j)",
            "",
            "",
            "class ShadowBanError(Exception):",
            "    \"\"\"",
            "    Raised when a shadow-banned user attempts to perform an action.",
            "",
            "    This should be caught and a proper \"fake\" success response sent to the user.",
            "    \"\"\"",
            "",
            "",
            "class ModuleFailedException(Exception):",
            "    \"\"\"",
            "    Raised when a module API callback fails, for example because it raised an",
            "    exception.",
            "    \"\"\""
        ],
        "afterPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Contains exceptions and error codes.\"\"\"",
            "",
            "import logging",
            "import typing",
            "from enum import Enum",
            "from http import HTTPStatus",
            "from typing import Any, Dict, List, Optional, Union",
            "",
            "from twisted.web import http",
            "",
            "from synapse.util import json_decoder",
            "",
            "if typing.TYPE_CHECKING:",
            "    from synapse.config.homeserver import HomeServerConfig",
            "    from synapse.types import JsonDict",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class Codes(str, Enum):",
            "    \"\"\"",
            "    All known error codes, as an enum of strings.",
            "    \"\"\"",
            "",
            "    UNRECOGNIZED = \"M_UNRECOGNIZED\"",
            "    UNAUTHORIZED = \"M_UNAUTHORIZED\"",
            "    FORBIDDEN = \"M_FORBIDDEN\"",
            "    BAD_JSON = \"M_BAD_JSON\"",
            "    NOT_JSON = \"M_NOT_JSON\"",
            "    USER_IN_USE = \"M_USER_IN_USE\"",
            "    ROOM_IN_USE = \"M_ROOM_IN_USE\"",
            "    BAD_PAGINATION = \"M_BAD_PAGINATION\"",
            "    BAD_STATE = \"M_BAD_STATE\"",
            "    UNKNOWN = \"M_UNKNOWN\"",
            "    NOT_FOUND = \"M_NOT_FOUND\"",
            "    MISSING_TOKEN = \"M_MISSING_TOKEN\"",
            "    UNKNOWN_TOKEN = \"M_UNKNOWN_TOKEN\"",
            "    GUEST_ACCESS_FORBIDDEN = \"M_GUEST_ACCESS_FORBIDDEN\"",
            "    LIMIT_EXCEEDED = \"M_LIMIT_EXCEEDED\"",
            "    CAPTCHA_NEEDED = \"M_CAPTCHA_NEEDED\"",
            "    CAPTCHA_INVALID = \"M_CAPTCHA_INVALID\"",
            "    MISSING_PARAM = \"M_MISSING_PARAM\"",
            "    INVALID_PARAM = \"M_INVALID_PARAM\"",
            "    TOO_LARGE = \"M_TOO_LARGE\"",
            "    EXCLUSIVE = \"M_EXCLUSIVE\"",
            "    THREEPID_AUTH_FAILED = \"M_THREEPID_AUTH_FAILED\"",
            "    THREEPID_IN_USE = \"M_THREEPID_IN_USE\"",
            "    THREEPID_NOT_FOUND = \"M_THREEPID_NOT_FOUND\"",
            "    THREEPID_DENIED = \"M_THREEPID_DENIED\"",
            "    INVALID_USERNAME = \"M_INVALID_USERNAME\"",
            "    SERVER_NOT_TRUSTED = \"M_SERVER_NOT_TRUSTED\"",
            "    CONSENT_NOT_GIVEN = \"M_CONSENT_NOT_GIVEN\"",
            "    CANNOT_LEAVE_SERVER_NOTICE_ROOM = \"M_CANNOT_LEAVE_SERVER_NOTICE_ROOM\"",
            "    RESOURCE_LIMIT_EXCEEDED = \"M_RESOURCE_LIMIT_EXCEEDED\"",
            "    UNSUPPORTED_ROOM_VERSION = \"M_UNSUPPORTED_ROOM_VERSION\"",
            "    INCOMPATIBLE_ROOM_VERSION = \"M_INCOMPATIBLE_ROOM_VERSION\"",
            "    WRONG_ROOM_KEYS_VERSION = \"M_WRONG_ROOM_KEYS_VERSION\"",
            "    EXPIRED_ACCOUNT = \"ORG_MATRIX_EXPIRED_ACCOUNT\"",
            "    PASSWORD_TOO_SHORT = \"M_PASSWORD_TOO_SHORT\"",
            "    PASSWORD_NO_DIGIT = \"M_PASSWORD_NO_DIGIT\"",
            "    PASSWORD_NO_UPPERCASE = \"M_PASSWORD_NO_UPPERCASE\"",
            "    PASSWORD_NO_LOWERCASE = \"M_PASSWORD_NO_LOWERCASE\"",
            "    PASSWORD_NO_SYMBOL = \"M_PASSWORD_NO_SYMBOL\"",
            "    PASSWORD_IN_DICTIONARY = \"M_PASSWORD_IN_DICTIONARY\"",
            "    WEAK_PASSWORD = \"M_WEAK_PASSWORD\"",
            "    INVALID_SIGNATURE = \"M_INVALID_SIGNATURE\"",
            "    USER_DEACTIVATED = \"M_USER_DEACTIVATED\"",
            "",
            "    # Part of MSC3848",
            "    # https://github.com/matrix-org/matrix-spec-proposals/pull/3848",
            "    ALREADY_JOINED = \"ORG.MATRIX.MSC3848.ALREADY_JOINED\"",
            "    NOT_JOINED = \"ORG.MATRIX.MSC3848.NOT_JOINED\"",
            "    INSUFFICIENT_POWER = \"ORG.MATRIX.MSC3848.INSUFFICIENT_POWER\"",
            "",
            "    # The account has been suspended on the server.",
            "    # By opposition to `USER_DEACTIVATED`, this is a reversible measure",
            "    # that can possibly be appealed and reverted.",
            "    # Part of MSC3823.",
            "    USER_ACCOUNT_SUSPENDED = \"ORG.MATRIX.MSC3823.USER_ACCOUNT_SUSPENDED\"",
            "",
            "    BAD_ALIAS = \"M_BAD_ALIAS\"",
            "    # For restricted join rules.",
            "    UNABLE_AUTHORISE_JOIN = \"M_UNABLE_TO_AUTHORISE_JOIN\"",
            "    UNABLE_TO_GRANT_JOIN = \"M_UNABLE_TO_GRANT_JOIN\"",
            "",
            "    UNREDACTED_CONTENT_DELETED = \"FI.MAU.MSC2815_UNREDACTED_CONTENT_DELETED\"",
            "",
            "    # Returned for federation requests where we can't process a request as we",
            "    # can't ensure the sending server is in a room which is partial-stated on",
            "    # our side.",
            "    # Part of MSC3895.",
            "    UNABLE_DUE_TO_PARTIAL_STATE = \"ORG.MATRIX.MSC3895_UNABLE_DUE_TO_PARTIAL_STATE\"",
            "",
            "",
            "class CodeMessageException(RuntimeError):",
            "    \"\"\"An exception with integer code and message string attributes.",
            "",
            "    Attributes:",
            "        code: HTTP error code",
            "        msg: string describing the error",
            "    \"\"\"",
            "",
            "    def __init__(self, code: Union[int, HTTPStatus], msg: str):",
            "        super().__init__(\"%d: %s\" % (code, msg))",
            "",
            "        # Some calls to this method pass instances of http.HTTPStatus for `code`.",
            "        # While HTTPStatus is a subclass of int, it has magic __str__ methods",
            "        # which emit `HTTPStatus.FORBIDDEN` when converted to a str, instead of `403`.",
            "        # This causes inconsistency in our log lines.",
            "        #",
            "        # To eliminate this behaviour, we convert them to their integer equivalents here.",
            "        self.code = int(code)",
            "        self.msg = msg",
            "",
            "",
            "class RedirectException(CodeMessageException):",
            "    \"\"\"A pseudo-error indicating that we want to redirect the client to a different",
            "    location",
            "",
            "    Attributes:",
            "        cookies: a list of set-cookies values to add to the response. For example:",
            "           b\"sessionId=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT\"",
            "    \"\"\"",
            "",
            "    def __init__(self, location: bytes, http_code: int = http.FOUND):",
            "        \"\"\"",
            "",
            "        Args:",
            "            location: the URI to redirect to",
            "            http_code: the HTTP response code",
            "        \"\"\"",
            "        msg = \"Redirect to %s\" % (location.decode(\"utf-8\"),)",
            "        super().__init__(code=http_code, msg=msg)",
            "        self.location = location",
            "",
            "        self.cookies: List[bytes] = []",
            "",
            "",
            "class SynapseError(CodeMessageException):",
            "    \"\"\"A base exception type for matrix errors which have an errcode and error",
            "    message (as well as an HTTP status code).",
            "",
            "    Attributes:",
            "        errcode: Matrix error code e.g 'M_FORBIDDEN'",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.UNKNOWN,",
            "        additional_fields: Optional[Dict] = None,",
            "    ):",
            "        \"\"\"Constructs a synapse error.",
            "",
            "        Args:",
            "            code: The integer error code (an HTTP response code)",
            "            msg: The human-readable error message.",
            "            errcode: The matrix error code e.g 'M_FORBIDDEN'",
            "        \"\"\"",
            "        super().__init__(code, msg)",
            "        self.errcode = errcode",
            "        if additional_fields is None:",
            "            self._additional_fields: Dict = {}",
            "        else:",
            "            self._additional_fields = dict(additional_fields)",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, **self._additional_fields)",
            "",
            "",
            "class InvalidAPICallError(SynapseError):",
            "    \"\"\"You called an existing API endpoint, but fed that endpoint",
            "    invalid or incomplete data.\"\"\"",
            "",
            "    def __init__(self, msg: str):",
            "        super().__init__(HTTPStatus.BAD_REQUEST, msg, Codes.BAD_JSON)",
            "",
            "",
            "class ProxiedRequestError(SynapseError):",
            "    \"\"\"An error from a general matrix endpoint, eg. from a proxied Matrix API call.",
            "",
            "    Attributes:",
            "        errcode: Matrix error code e.g 'M_FORBIDDEN'",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.UNKNOWN,",
            "        additional_fields: Optional[Dict] = None,",
            "    ):",
            "        super().__init__(code, msg, errcode, additional_fields)",
            "",
            "",
            "class ConsentNotGivenError(SynapseError):",
            "    \"\"\"The error returned to the client when the user has not consented to the",
            "    privacy policy.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg: str, consent_uri: str):",
            "        \"\"\"Constructs a ConsentNotGivenError",
            "",
            "        Args:",
            "            msg: The human-readable error message",
            "            consent_url: The URL where the user can give their consent",
            "        \"\"\"",
            "        super().__init__(",
            "            code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.CONSENT_NOT_GIVEN",
            "        )",
            "        self._consent_uri = consent_uri",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, consent_uri=self._consent_uri)",
            "",
            "",
            "class UserDeactivatedError(SynapseError):",
            "    \"\"\"The error returned to the client when the user attempted to access an",
            "    authenticated endpoint, but the account has been deactivated.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg: str):",
            "        \"\"\"Constructs a UserDeactivatedError",
            "",
            "        Args:",
            "            msg: The human-readable error message",
            "        \"\"\"",
            "        super().__init__(",
            "            code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.USER_DEACTIVATED",
            "        )",
            "",
            "",
            "class FederationDeniedError(SynapseError):",
            "    \"\"\"An error raised when the server tries to federate with a server which",
            "    is not on its federation whitelist.",
            "",
            "    Attributes:",
            "        destination: The destination which has been denied",
            "    \"\"\"",
            "",
            "    def __init__(self, destination: Optional[str]):",
            "        \"\"\"Raised by federation client or server to indicate that we are",
            "        are deliberately not attempting to contact a given server because it is",
            "        not on our federation whitelist.",
            "",
            "        Args:",
            "            destination: the domain in question",
            "        \"\"\"",
            "",
            "        self.destination = destination",
            "",
            "        super().__init__(",
            "            code=403,",
            "            msg=\"Federation denied with %s.\" % (self.destination,),",
            "            errcode=Codes.FORBIDDEN,",
            "        )",
            "",
            "",
            "class InteractiveAuthIncompleteError(Exception):",
            "    \"\"\"An error raised when UI auth is not yet complete",
            "",
            "    (This indicates we should return a 401 with 'result' as the body)",
            "",
            "    Attributes:",
            "        session_id: The ID of the ongoing interactive auth session.",
            "        result: the server response to the request, which should be",
            "            passed back to the client",
            "    \"\"\"",
            "",
            "    def __init__(self, session_id: str, result: \"JsonDict\"):",
            "        super().__init__(\"Interactive auth not yet complete\")",
            "        self.session_id = session_id",
            "        self.result = result",
            "",
            "",
            "class UnrecognizedRequestError(SynapseError):",
            "    \"\"\"An error indicating we don't understand the request you're trying to make\"\"\"",
            "",
            "    def __init__(",
            "        self, msg: str = \"Unrecognized request\", errcode: str = Codes.UNRECOGNIZED",
            "    ):",
            "        super().__init__(400, msg, errcode)",
            "",
            "",
            "class NotFoundError(SynapseError):",
            "    \"\"\"An error indicating we can't find the thing you asked for\"\"\"",
            "",
            "    def __init__(self, msg: str = \"Not found\", errcode: str = Codes.NOT_FOUND):",
            "        super().__init__(404, msg, errcode=errcode)",
            "",
            "",
            "class AuthError(SynapseError):",
            "    \"\"\"An error raised when there was a problem authorising an event, and at various",
            "    other poorly-defined times.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.FORBIDDEN,",
            "        additional_fields: Optional[dict] = None,",
            "    ):",
            "        super().__init__(code, msg, errcode, additional_fields)",
            "",
            "",
            "class UnstableSpecAuthError(AuthError):",
            "    \"\"\"An error raised when a new error code is being proposed to replace a previous one.",
            "    This error will return a \"org.matrix.unstable.errcode\" property with the new error code,",
            "    with the previous error code still being defined in the \"errcode\" property.",
            "",
            "    This error will include `org.matrix.msc3848.unstable.errcode` in the C-S error body.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str,",
            "        previous_errcode: str = Codes.FORBIDDEN,",
            "        additional_fields: Optional[dict] = None,",
            "    ):",
            "        self.previous_errcode = previous_errcode",
            "        super().__init__(code, msg, errcode, additional_fields)",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        fields = {}",
            "        if config is not None and config.experimental.msc3848_enabled:",
            "            fields[\"org.matrix.msc3848.unstable.errcode\"] = self.errcode",
            "        return cs_error(",
            "            self.msg,",
            "            self.previous_errcode,",
            "            **fields,",
            "            **self._additional_fields,",
            "        )",
            "",
            "",
            "class InvalidClientCredentialsError(SynapseError):",
            "    \"\"\"An error raised when there was a problem with the authorisation credentials",
            "    in a client request.",
            "",
            "    https://matrix.org/docs/spec/client_server/r0.5.0#using-access-tokens:",
            "",
            "    When credentials are required but missing or invalid, the HTTP call will",
            "    return with a status of 401 and the error code, M_MISSING_TOKEN or",
            "    M_UNKNOWN_TOKEN respectively.",
            "    \"\"\"",
            "",
            "    def __init__(self, msg: str, errcode: str):",
            "        super().__init__(code=401, msg=msg, errcode=errcode)",
            "",
            "",
            "class MissingClientTokenError(InvalidClientCredentialsError):",
            "    \"\"\"Raised when we couldn't find the access token in a request\"\"\"",
            "",
            "    def __init__(self, msg: str = \"Missing access token\"):",
            "        super().__init__(msg=msg, errcode=\"M_MISSING_TOKEN\")",
            "",
            "",
            "class InvalidClientTokenError(InvalidClientCredentialsError):",
            "    \"\"\"Raised when we didn't understand the access token in a request\"\"\"",
            "",
            "    def __init__(",
            "        self, msg: str = \"Unrecognised access token\", soft_logout: bool = False",
            "    ):",
            "        super().__init__(msg=msg, errcode=\"M_UNKNOWN_TOKEN\")",
            "        self._soft_logout = soft_logout",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        d = super().error_dict(config)",
            "        d[\"soft_logout\"] = self._soft_logout",
            "        return d",
            "",
            "",
            "class ResourceLimitError(SynapseError):",
            "    \"\"\"",
            "    Any error raised when there is a problem with resource usage.",
            "    For instance, the monthly active user limit for the server has been exceeded",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int,",
            "        msg: str,",
            "        errcode: str = Codes.RESOURCE_LIMIT_EXCEEDED,",
            "        admin_contact: Optional[str] = None,",
            "        limit_type: Optional[str] = None,",
            "    ):",
            "        self.admin_contact = admin_contact",
            "        self.limit_type = limit_type",
            "        super().__init__(code, msg, errcode=errcode)",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(",
            "            self.msg,",
            "            self.errcode,",
            "            admin_contact=self.admin_contact,",
            "            limit_type=self.limit_type,",
            "        )",
            "",
            "",
            "class EventSizeError(SynapseError):",
            "    \"\"\"An error raised when an event is too big.\"\"\"",
            "",
            "    def __init__(self, msg: str):",
            "        super().__init__(413, msg, Codes.TOO_LARGE)",
            "",
            "",
            "class LoginError(SynapseError):",
            "    \"\"\"An error raised when there was a problem logging in.\"\"\"",
            "",
            "",
            "class StoreError(SynapseError):",
            "    \"\"\"An error raised when there was a problem storing some data.\"\"\"",
            "",
            "",
            "class InvalidCaptchaError(SynapseError):",
            "    def __init__(",
            "        self,",
            "        code: int = 400,",
            "        msg: str = \"Invalid captcha.\",",
            "        error_url: Optional[str] = None,",
            "        errcode: str = Codes.CAPTCHA_INVALID,",
            "    ):",
            "        super().__init__(code, msg, errcode)",
            "        self.error_url = error_url",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, error_url=self.error_url)",
            "",
            "",
            "class LimitExceededError(SynapseError):",
            "    \"\"\"A client has sent too many requests and is being throttled.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        code: int = 429,",
            "        msg: str = \"Too Many Requests\",",
            "        retry_after_ms: Optional[int] = None,",
            "        errcode: str = Codes.LIMIT_EXCEEDED,",
            "    ):",
            "        super().__init__(code, msg, errcode)",
            "        self.retry_after_ms = retry_after_ms",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, retry_after_ms=self.retry_after_ms)",
            "",
            "",
            "class RoomKeysVersionError(SynapseError):",
            "    \"\"\"A client has tried to upload to a non-current version of the room_keys store\"\"\"",
            "",
            "    def __init__(self, current_version: str):",
            "        \"\"\"",
            "        Args:",
            "            current_version: the current version of the store they should have used",
            "        \"\"\"",
            "        super().__init__(403, \"Wrong room_keys version\", Codes.WRONG_ROOM_KEYS_VERSION)",
            "        self.current_version = current_version",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, current_version=self.current_version)",
            "",
            "",
            "class UnsupportedRoomVersionError(SynapseError):",
            "    \"\"\"The client's request to create a room used a room version that the server does",
            "    not support.\"\"\"",
            "",
            "    def __init__(self, msg: str = \"Homeserver does not support this room version\"):",
            "        super().__init__(",
            "            code=400,",
            "            msg=msg,",
            "            errcode=Codes.UNSUPPORTED_ROOM_VERSION,",
            "        )",
            "",
            "",
            "class ThreepidValidationError(SynapseError):",
            "    \"\"\"An error raised when there was a problem authorising an event.\"\"\"",
            "",
            "    def __init__(self, msg: str, errcode: str = Codes.FORBIDDEN):",
            "        super().__init__(400, msg, errcode)",
            "",
            "",
            "class IncompatibleRoomVersionError(SynapseError):",
            "    \"\"\"A server is trying to join a room whose version it does not support.",
            "",
            "    Unlike UnsupportedRoomVersionError, it is specific to the case of the make_join",
            "    failing.",
            "    \"\"\"",
            "",
            "    def __init__(self, room_version: str):",
            "        super().__init__(",
            "            code=400,",
            "            msg=\"Your homeserver does not support the features required to \"",
            "            \"interact with this room\",",
            "            errcode=Codes.INCOMPATIBLE_ROOM_VERSION,",
            "        )",
            "",
            "        self._room_version = room_version",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        return cs_error(self.msg, self.errcode, room_version=self._room_version)",
            "",
            "",
            "class PasswordRefusedError(SynapseError):",
            "    \"\"\"A password has been refused, either during password reset/change or registration.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        msg: str = \"This password doesn't comply with the server's policy\",",
            "        errcode: str = Codes.WEAK_PASSWORD,",
            "    ):",
            "        super().__init__(",
            "            code=400,",
            "            msg=msg,",
            "            errcode=errcode,",
            "        )",
            "",
            "",
            "class RequestSendFailed(RuntimeError):",
            "    \"\"\"Sending a HTTP request over federation failed due to not being able to",
            "    talk to the remote server for some reason.",
            "",
            "    This exception is used to differentiate \"expected\" errors that arise due to",
            "    networking (e.g. DNS failures, connection timeouts etc), versus unexpected",
            "    errors (like programming errors).",
            "    \"\"\"",
            "",
            "    def __init__(self, inner_exception: BaseException, can_retry: bool):",
            "        super().__init__(",
            "            \"Failed to send request: %s: %s\"",
            "            % (type(inner_exception).__name__, inner_exception)",
            "        )",
            "        self.inner_exception = inner_exception",
            "        self.can_retry = can_retry",
            "",
            "",
            "class UnredactedContentDeletedError(SynapseError):",
            "    def __init__(self, content_keep_ms: Optional[int] = None):",
            "        super().__init__(",
            "            404,",
            "            \"The content for that event has already been erased from the database\",",
            "            errcode=Codes.UNREDACTED_CONTENT_DELETED,",
            "        )",
            "        self.content_keep_ms = content_keep_ms",
            "",
            "    def error_dict(self, config: Optional[\"HomeServerConfig\"]) -> \"JsonDict\":",
            "        extra = {}",
            "        if self.content_keep_ms is not None:",
            "            extra = {\"fi.mau.msc2815.content_keep_ms\": self.content_keep_ms}",
            "        return cs_error(self.msg, self.errcode, **extra)",
            "",
            "",
            "def cs_error(msg: str, code: str = Codes.UNKNOWN, **kwargs: Any) -> \"JsonDict\":",
            "    \"\"\"Utility method for constructing an error response for client-server",
            "    interactions.",
            "",
            "    Args:",
            "        msg: The error message.",
            "        code: The error code.",
            "        kwargs: Additional keys to add to the response.",
            "    Returns:",
            "        A dict representing the error response JSON.",
            "    \"\"\"",
            "    err = {\"error\": msg, \"errcode\": code}",
            "    for key, value in kwargs.items():",
            "        err[key] = value",
            "    return err",
            "",
            "",
            "class FederationError(RuntimeError):",
            "    \"\"\"This class is used to inform remote homeservers about erroneous",
            "    PDUs they sent us.",
            "",
            "    FATAL: The remote server could not interpret the source event.",
            "        (e.g., it was missing a required field)",
            "    ERROR: The remote server interpreted the event, but it failed some other",
            "        check (e.g. auth)",
            "    WARN: The remote server accepted the event, but believes some part of it",
            "        is wrong (e.g., it referred to an invalid event)",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        level: str,",
            "        code: int,",
            "        reason: str,",
            "        affected: str,",
            "        source: Optional[str] = None,",
            "    ):",
            "        if level not in [\"FATAL\", \"ERROR\", \"WARN\"]:",
            "            raise ValueError(\"Level is not valid: %s\" % (level,))",
            "        self.level = level",
            "        self.code = code",
            "        self.reason = reason",
            "        self.affected = affected",
            "        self.source = source",
            "",
            "        msg = \"%s %s: %s\" % (level, code, reason)",
            "        super().__init__(msg)",
            "",
            "    def get_dict(self) -> \"JsonDict\":",
            "        return {",
            "            \"level\": self.level,",
            "            \"code\": self.code,",
            "            \"reason\": self.reason,",
            "            \"affected\": self.affected,",
            "            \"source\": self.source if self.source else self.affected,",
            "        }",
            "",
            "",
            "class HttpResponseException(CodeMessageException):",
            "    \"\"\"",
            "    Represents an HTTP-level failure of an outbound request",
            "",
            "    Attributes:",
            "        response: body of response",
            "    \"\"\"",
            "",
            "    def __init__(self, code: int, msg: str, response: bytes):",
            "        \"\"\"",
            "",
            "        Args:",
            "            code: HTTP status code",
            "            msg: reason phrase from HTTP response status line",
            "            response: body of response",
            "        \"\"\"",
            "        super().__init__(code, msg)",
            "        self.response = response",
            "",
            "    def to_synapse_error(self) -> SynapseError:",
            "        \"\"\"Make a SynapseError based on an HTTPResponseException",
            "",
            "        This is useful when a proxied request has failed, and we need to",
            "        decide how to map the failure onto a matrix error to send back to the",
            "        client.",
            "",
            "        An attempt is made to parse the body of the http response as a matrix",
            "        error. If that succeeds, the errcode and error message from the body",
            "        are used as the errcode and error message in the new synapse error.",
            "",
            "        Otherwise, the errcode is set to M_UNKNOWN, and the error message is",
            "        set to the reason code from the HTTP response.",
            "",
            "        Returns:",
            "            SynapseError:",
            "        \"\"\"",
            "        # try to parse the body as json, to get better errcode/msg, but",
            "        # default to M_UNKNOWN with the HTTP status as the error text",
            "        try:",
            "            j = json_decoder.decode(self.response.decode(\"utf-8\"))",
            "        except ValueError:",
            "            j = {}",
            "",
            "        if not isinstance(j, dict):",
            "            j = {}",
            "",
            "        errcode = j.pop(\"errcode\", Codes.UNKNOWN)",
            "        errmsg = j.pop(\"error\", self.msg)",
            "",
            "        return ProxiedRequestError(self.code, errmsg, errcode, j)",
            "",
            "",
            "class ShadowBanError(Exception):",
            "    \"\"\"",
            "    Raised when a shadow-banned user attempts to perform an action.",
            "",
            "    This should be caught and a proper \"fake\" success response sent to the user.",
            "    \"\"\"",
            "",
            "",
            "class ModuleFailedException(Exception):",
            "    \"\"\"",
            "    Raised when a module API callback fails, for example because it raised an",
            "    exception.",
            "    \"\"\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.api.errors.Codes.self"
        ]
    },
    "synapse/config/experimental.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "         # MSC3706 (server-side support for partial state in /send_join responses)"
            },
            "1": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "         self.msc3706_enabled: bool = experimental.get(\"msc3706_enabled\", False)"
            },
            "2": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # experimental support for faster joins over federation (msc2775, msc3706)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+        # experimental support for faster joins over federation"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+        # (MSC2775, MSC3706, MSC3895)"
            },
            "6": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "         # requires a target server with msc3706_enabled enabled."
            },
            "7": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "         self.faster_joins_enabled: bool = experimental.get(\"faster_joins\", False)"
            },
            "8": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "from typing import Any",
            "",
            "from synapse.config._base import Config",
            "from synapse.types import JsonDict",
            "",
            "",
            "class ExperimentalConfig(Config):",
            "    \"\"\"Config section for enabling experimental features\"\"\"",
            "",
            "    section = \"experimental\"",
            "",
            "    def read_config(self, config: JsonDict, **kwargs: Any) -> None:",
            "        experimental = config.get(\"experimental_features\") or {}",
            "",
            "        # MSC3026 (busy presence state)",
            "        self.msc3026_enabled: bool = experimental.get(\"msc3026_enabled\", False)",
            "",
            "        # MSC2716 (importing historical messages)",
            "        self.msc2716_enabled: bool = experimental.get(\"msc2716_enabled\", False)",
            "",
            "        # MSC3244 (room version capabilities)",
            "        self.msc3244_enabled: bool = experimental.get(\"msc3244_enabled\", True)",
            "",
            "        # MSC3266 (room summary api)",
            "        self.msc3266_enabled: bool = experimental.get(\"msc3266_enabled\", False)",
            "",
            "        # MSC3030 (Jump to date API endpoint)",
            "        self.msc3030_enabled: bool = experimental.get(\"msc3030_enabled\", False)",
            "",
            "        # MSC2409 (this setting only relates to optionally sending to-device messages).",
            "        # Presence, typing and read receipt EDUs are already sent to application services that",
            "        # have opted in to receive them. If enabled, this adds to-device messages to that list.",
            "        self.msc2409_to_device_messages_enabled: bool = experimental.get(",
            "            \"msc2409_to_device_messages_enabled\", False",
            "        )",
            "",
            "        # The portion of MSC3202 which is related to device masquerading.",
            "        self.msc3202_device_masquerading_enabled: bool = experimental.get(",
            "            \"msc3202_device_masquerading\", False",
            "        )",
            "",
            "        # The portion of MSC3202 related to transaction extensions:",
            "        # sending device list changes, one-time key counts and fallback key",
            "        # usage to application services.",
            "        self.msc3202_transaction_extensions: bool = experimental.get(",
            "            \"msc3202_transaction_extensions\", False",
            "        )",
            "",
            "        # MSC3706 (server-side support for partial state in /send_join responses)",
            "        self.msc3706_enabled: bool = experimental.get(\"msc3706_enabled\", False)",
            "",
            "        # experimental support for faster joins over federation (msc2775, msc3706)",
            "        # requires a target server with msc3706_enabled enabled.",
            "        self.faster_joins_enabled: bool = experimental.get(\"faster_joins\", False)",
            "",
            "        # MSC3720 (Account status endpoint)",
            "        self.msc3720_enabled: bool = experimental.get(\"msc3720_enabled\", False)",
            "",
            "        # MSC2654: Unread counts",
            "        #",
            "        # Note that enabling this will result in an incorrect unread count for",
            "        # previously calculated push actions.",
            "        self.msc2654_enabled: bool = experimental.get(\"msc2654_enabled\", False)",
            "",
            "        # MSC2815 (allow room moderators to view redacted event content)",
            "        self.msc2815_enabled: bool = experimental.get(\"msc2815_enabled\", False)",
            "",
            "        # MSC3786 (Add a default push rule to ignore m.room.server_acl events)",
            "        self.msc3786_enabled: bool = experimental.get(\"msc3786_enabled\", False)",
            "",
            "        # MSC3772: A push rule for mutual relations.",
            "        self.msc3772_enabled: bool = experimental.get(\"msc3772_enabled\", False)",
            "",
            "        # MSC3715: dir param on /relations.",
            "        self.msc3715_enabled: bool = experimental.get(\"msc3715_enabled\", False)",
            "",
            "        # MSC3848: Introduce errcodes for specific event sending failures",
            "        self.msc3848_enabled: bool = experimental.get(\"msc3848_enabled\", False)",
            "",
            "        # MSC3852: Expose last seen user agent field on /_matrix/client/v3/devices.",
            "        self.msc3852_enabled: bool = experimental.get(\"msc3852_enabled\", False)",
            "",
            "        # MSC3881: Remotely toggle push notifications for another client",
            "        self.msc3881_enabled: bool = experimental.get(\"msc3881_enabled\", False)",
            "",
            "        # MSC3882: Allow an existing session to sign in a new session",
            "        self.msc3882_enabled: bool = experimental.get(\"msc3882_enabled\", False)",
            "        self.msc3882_ui_auth: bool = experimental.get(\"msc3882_ui_auth\", True)",
            "        self.msc3882_token_timeout = self.parse_duration(",
            "            experimental.get(\"msc3882_token_timeout\", \"5m\")",
            "        )"
        ],
        "afterPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "from typing import Any",
            "",
            "from synapse.config._base import Config",
            "from synapse.types import JsonDict",
            "",
            "",
            "class ExperimentalConfig(Config):",
            "    \"\"\"Config section for enabling experimental features\"\"\"",
            "",
            "    section = \"experimental\"",
            "",
            "    def read_config(self, config: JsonDict, **kwargs: Any) -> None:",
            "        experimental = config.get(\"experimental_features\") or {}",
            "",
            "        # MSC3026 (busy presence state)",
            "        self.msc3026_enabled: bool = experimental.get(\"msc3026_enabled\", False)",
            "",
            "        # MSC2716 (importing historical messages)",
            "        self.msc2716_enabled: bool = experimental.get(\"msc2716_enabled\", False)",
            "",
            "        # MSC3244 (room version capabilities)",
            "        self.msc3244_enabled: bool = experimental.get(\"msc3244_enabled\", True)",
            "",
            "        # MSC3266 (room summary api)",
            "        self.msc3266_enabled: bool = experimental.get(\"msc3266_enabled\", False)",
            "",
            "        # MSC3030 (Jump to date API endpoint)",
            "        self.msc3030_enabled: bool = experimental.get(\"msc3030_enabled\", False)",
            "",
            "        # MSC2409 (this setting only relates to optionally sending to-device messages).",
            "        # Presence, typing and read receipt EDUs are already sent to application services that",
            "        # have opted in to receive them. If enabled, this adds to-device messages to that list.",
            "        self.msc2409_to_device_messages_enabled: bool = experimental.get(",
            "            \"msc2409_to_device_messages_enabled\", False",
            "        )",
            "",
            "        # The portion of MSC3202 which is related to device masquerading.",
            "        self.msc3202_device_masquerading_enabled: bool = experimental.get(",
            "            \"msc3202_device_masquerading\", False",
            "        )",
            "",
            "        # The portion of MSC3202 related to transaction extensions:",
            "        # sending device list changes, one-time key counts and fallback key",
            "        # usage to application services.",
            "        self.msc3202_transaction_extensions: bool = experimental.get(",
            "            \"msc3202_transaction_extensions\", False",
            "        )",
            "",
            "        # MSC3706 (server-side support for partial state in /send_join responses)",
            "        self.msc3706_enabled: bool = experimental.get(\"msc3706_enabled\", False)",
            "",
            "        # experimental support for faster joins over federation",
            "        # (MSC2775, MSC3706, MSC3895)",
            "        # requires a target server with msc3706_enabled enabled.",
            "        self.faster_joins_enabled: bool = experimental.get(\"faster_joins\", False)",
            "",
            "        # MSC3720 (Account status endpoint)",
            "        self.msc3720_enabled: bool = experimental.get(\"msc3720_enabled\", False)",
            "",
            "        # MSC2654: Unread counts",
            "        #",
            "        # Note that enabling this will result in an incorrect unread count for",
            "        # previously calculated push actions.",
            "        self.msc2654_enabled: bool = experimental.get(\"msc2654_enabled\", False)",
            "",
            "        # MSC2815 (allow room moderators to view redacted event content)",
            "        self.msc2815_enabled: bool = experimental.get(\"msc2815_enabled\", False)",
            "",
            "        # MSC3786 (Add a default push rule to ignore m.room.server_acl events)",
            "        self.msc3786_enabled: bool = experimental.get(\"msc3786_enabled\", False)",
            "",
            "        # MSC3772: A push rule for mutual relations.",
            "        self.msc3772_enabled: bool = experimental.get(\"msc3772_enabled\", False)",
            "",
            "        # MSC3715: dir param on /relations.",
            "        self.msc3715_enabled: bool = experimental.get(\"msc3715_enabled\", False)",
            "",
            "        # MSC3848: Introduce errcodes for specific event sending failures",
            "        self.msc3848_enabled: bool = experimental.get(\"msc3848_enabled\", False)",
            "",
            "        # MSC3852: Expose last seen user agent field on /_matrix/client/v3/devices.",
            "        self.msc3852_enabled: bool = experimental.get(\"msc3852_enabled\", False)",
            "",
            "        # MSC3881: Remotely toggle push notifications for another client",
            "        self.msc3881_enabled: bool = experimental.get(\"msc3881_enabled\", False)",
            "",
            "        # MSC3882: Allow an existing session to sign in a new session",
            "        self.msc3882_enabled: bool = experimental.get(\"msc3882_enabled\", False)",
            "        self.msc3882_ui_auth: bool = experimental.get(\"msc3882_ui_auth\", True)",
            "        self.msc3882_token_timeout = self.parse_duration(",
            "            experimental.get(\"msc3882_token_timeout\", \"5m\")",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "66": [
                "ExperimentalConfig",
                "read_config"
            ]
        },
        "addLocation": []
    },
    "synapse/federation/federation_server.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 530,
                "afterPatchRowNumber": 530,
                "PatchRowcode": "     async def on_room_state_request("
            },
            "1": {
                "beforePatchRowNumber": 531,
                "afterPatchRowNumber": 531,
                "PatchRowcode": "         self, origin: str, room_id: str, event_id: str"
            },
            "2": {
                "beforePatchRowNumber": 532,
                "afterPatchRowNumber": 532,
                "PatchRowcode": "     ) -> Tuple[int, JsonDict]:"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 533,
                "PatchRowcode": "+        await self._event_auth_handler.assert_host_in_room(room_id, origin)"
            },
            "4": {
                "beforePatchRowNumber": 533,
                "afterPatchRowNumber": 534,
                "PatchRowcode": "         origin_host, _ = parse_server_name(origin)"
            },
            "5": {
                "beforePatchRowNumber": 534,
                "afterPatchRowNumber": 535,
                "PatchRowcode": "         await self.check_server_matches_acl(origin_host, room_id)"
            },
            "6": {
                "beforePatchRowNumber": 535,
                "afterPatchRowNumber": 536,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 536,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)"
            },
            "8": {
                "beforePatchRowNumber": 537,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not in_room:"
            },
            "9": {
                "beforePatchRowNumber": 538,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise AuthError(403, \"Host not in room.\")"
            },
            "10": {
                "beforePatchRowNumber": 539,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "11": {
                "beforePatchRowNumber": 540,
                "afterPatchRowNumber": 537,
                "PatchRowcode": "         # we grab the linearizer to protect ourselves from servers which hammer"
            },
            "12": {
                "beforePatchRowNumber": 541,
                "afterPatchRowNumber": 538,
                "PatchRowcode": "         # us. In theory we might already have the response to this query"
            },
            "13": {
                "beforePatchRowNumber": 542,
                "afterPatchRowNumber": 539,
                "PatchRowcode": "         # in the cache so we could return it without waiting for the linearizer"
            },
            "14": {
                "beforePatchRowNumber": 560,
                "afterPatchRowNumber": 557,
                "PatchRowcode": "         if not event_id:"
            },
            "15": {
                "beforePatchRowNumber": 561,
                "afterPatchRowNumber": 558,
                "PatchRowcode": "             raise NotImplementedError(\"Specify an event\")"
            },
            "16": {
                "beforePatchRowNumber": 562,
                "afterPatchRowNumber": 559,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 560,
                "PatchRowcode": "+        await self._event_auth_handler.assert_host_in_room(room_id, origin)"
            },
            "18": {
                "beforePatchRowNumber": 563,
                "afterPatchRowNumber": 561,
                "PatchRowcode": "         origin_host, _ = parse_server_name(origin)"
            },
            "19": {
                "beforePatchRowNumber": 564,
                "afterPatchRowNumber": 562,
                "PatchRowcode": "         await self.check_server_matches_acl(origin_host, room_id)"
            },
            "20": {
                "beforePatchRowNumber": 565,
                "afterPatchRowNumber": 563,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 566,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)"
            },
            "22": {
                "beforePatchRowNumber": 567,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not in_room:"
            },
            "23": {
                "beforePatchRowNumber": 568,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise AuthError(403, \"Host not in room.\")"
            },
            "24": {
                "beforePatchRowNumber": 569,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "25": {
                "beforePatchRowNumber": 570,
                "afterPatchRowNumber": 564,
                "PatchRowcode": "         resp = await self._state_ids_resp_cache.wrap("
            },
            "26": {
                "beforePatchRowNumber": 571,
                "afterPatchRowNumber": 565,
                "PatchRowcode": "             (room_id, event_id),"
            },
            "27": {
                "beforePatchRowNumber": 572,
                "afterPatchRowNumber": 566,
                "PatchRowcode": "             self._on_state_ids_request_compute,"
            },
            "28": {
                "beforePatchRowNumber": 955,
                "afterPatchRowNumber": 949,
                "PatchRowcode": "         self, origin: str, room_id: str, event_id: str"
            },
            "29": {
                "beforePatchRowNumber": 956,
                "afterPatchRowNumber": 950,
                "PatchRowcode": "     ) -> Tuple[int, Dict[str, Any]]:"
            },
            "30": {
                "beforePatchRowNumber": 957,
                "afterPatchRowNumber": 951,
                "PatchRowcode": "         async with self._server_linearizer.queue((origin, room_id)):"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 952,
                "PatchRowcode": "+            await self._event_auth_handler.assert_host_in_room(room_id, origin)"
            },
            "32": {
                "beforePatchRowNumber": 958,
                "afterPatchRowNumber": 953,
                "PatchRowcode": "             origin_host, _ = parse_server_name(origin)"
            },
            "33": {
                "beforePatchRowNumber": 959,
                "afterPatchRowNumber": 954,
                "PatchRowcode": "             await self.check_server_matches_acl(origin_host, room_id)"
            },
            "34": {
                "beforePatchRowNumber": 960,
                "afterPatchRowNumber": 955,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019-2021 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from matrix_common.regex import glob_to_regex",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.internet.abstract import isIPAddress",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import EduTypes, EventContentFields, EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_base import (",
            "    FederationBase,",
            "    InvalidEventSignatureError,",
            "    event_from_pdu_json,",
            ")",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import (",
            "    log_kv,",
            "    start_active_span_from_edu,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import wrap_as_background_process",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.lock import Lock",
            "from synapse.types import JsonDict, StateMap, get_domain_from_id",
            "from synapse.util import json_decoder, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute, gather_results",
            "from synapse.util.caches.response_cache import ResponseCache",
            "from synapse.util.stringutils import parse_server_name",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\",",
            "    \"Time taken to process an event\",",
            ")",
            "",
            "last_pdu_ts_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_time\",",
            "    \"The timestamp of the last PDU which was successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "# The name of the lock to use when process events in a room received over",
            "# federation.",
            "_INBOUND_EVENT_HANDLING_LOCK_NAME = \"federation_inbound_pdu\"",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.handler = hs.get_federation_handler()",
            "        self._spam_checker = hs.get_spam_checker()",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self.state = hs.get_state_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._room_member_handler = hs.get_room_member_handler()",
            "",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "",
            "        # origins that we are currently processing a transaction from.",
            "        # a dict from origin to txn id.",
            "        self._active_transactions: Dict[str, str] = {}",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"fed_txn_handler\", timeout_ms=30000",
            "        )",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache: ResponseCache[",
            "            Tuple[str, Optional[str]]",
            "        ] = ResponseCache(hs.get_clock(), \"state_resp\", timeout_ms=30000)",
            "        self._state_ids_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"state_ids_resp\", timeout_ms=30000",
            "        )",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.config.federation.federation_metrics_domains",
            "        )",
            "",
            "        self._room_prejoin_state_types = hs.config.api.room_prejoin_state",
            "",
            "        # Whether we have started handling old events in the staging area.",
            "        self._started_handling_of_staged_events = False",
            "",
            "    @wrap_as_background_process(\"_handle_old_staged_events\")",
            "    async def _handle_old_staged_events(self) -> None:",
            "        \"\"\"Handle old staged events by fetching all rooms that have staged",
            "        events and start the processing of each of those rooms.",
            "        \"\"\"",
            "",
            "        # Get all the rooms IDs with staged events.",
            "        room_ids = await self.store.get_all_rooms_with_staged_incoming_events()",
            "",
            "        # We then shuffle them so that if there are multiple instances doing",
            "        # this work they're less likely to collide.",
            "        random.shuffle(room_ids)",
            "",
            "        for room_id in room_ids:",
            "            room_version = await self.store.get_room_version(room_id)",
            "",
            "            # Try and acquire the processing lock for the room, if we get it start a",
            "            # background process for handling the events in the room.",
            "            lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if lock:",
            "                logger.info(\"Handling old staged inbound events in %s\", room_id)",
            "                self._process_incoming_pdus_in_room_inner(",
            "                    room_id,",
            "                    room_version,",
            "                    lock,",
            "                )",
            "",
            "            # We pause a bit so that we don't start handling all rooms at once.",
            "            await self._clock.sleep(random.uniform(0, 0.1))",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_dict_from_pdus(pdus)",
            "",
            "        return 200, res",
            "",
            "    async def on_timestamp_to_event_request(",
            "        self, origin: str, room_id: str, timestamp: int, direction: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"When we receive a federated `/timestamp_to_event` request,",
            "        handle all of the logic for validating and fetching the event.",
            "",
            "        Args:",
            "            origin: The server we received the event from",
            "            room_id: Room to fetch the event from",
            "            timestamp: The point in time (inclusive) we should navigate from in",
            "                the given direction to find the closest event.",
            "            direction: [\"f\"|\"b\"] to indicate whether we should navigate forward",
            "                or backward from the given timestamp to find the closest event.",
            "",
            "        Returns:",
            "            Tuple indicating the response status code and dictionary response",
            "            body including `event_id`.",
            "        \"\"\"",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            # We only try to fetch data from the local database",
            "            event_id = await self.store.get_event_id_for_timestamp(",
            "                room_id, timestamp, direction",
            "            )",
            "            if event_id:",
            "                event = await self.store.get_event(",
            "                    event_id, allow_none=False, allow_rejected=False",
            "                )",
            "",
            "                return 200, {",
            "                    \"event_id\": event_id,",
            "                    \"origin_server_ts\": event.origin_server_ts,",
            "                }",
            "",
            "        raise SynapseError(",
            "            404,",
            "            \"Unable to find event from %s in direction %s\" % (timestamp, direction),",
            "            errcode=Codes.NOT_FOUND,",
            "        )",
            "",
            "    async def on_incoming_transaction(",
            "        self,",
            "        origin: str,",
            "        transaction_id: str,",
            "        destination: str,",
            "        transaction_data: JsonDict,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # If we receive a transaction we should make sure that kick off handling",
            "        # any old events in the staging area.",
            "        if not self._started_handling_of_staged_events:",
            "            self._started_handling_of_staged_events = True",
            "            self._handle_old_staged_events()",
            "",
            "            # Start a periodic check for old staged events. This is to handle",
            "            # the case where locks time out, e.g. if another process gets killed",
            "            # without dropping its locks.",
            "            self._clock.looping_call(self._handle_old_staged_events, 60 * 1000)",
            "",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(",
            "            transaction_id=transaction_id,",
            "            destination=destination,",
            "            origin=origin,",
            "            origin_server_ts=transaction_data.get(\"origin_server_ts\"),  # type: ignore[arg-type]",
            "            pdus=transaction_data.get(\"pdus\"),",
            "            edus=transaction_data.get(\"edus\"),",
            "        )",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # Reject malformed transactions early: reject if too many PDUs/EDUs",
            "        if len(transaction.pdus) > 50 or len(transaction.edus) > 100:",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "            return 400, {}",
            "",
            "        # we only process one transaction from each origin at a time. We need to do",
            "        # this check here, rather than in _on_incoming_transaction_inner so that we",
            "        # don't cache the rejection in _transaction_resp_cache (so that if the txn",
            "        # arrives again later, we can process it).",
            "        current_transaction = self._active_transactions.get(origin)",
            "        if current_transaction and current_transaction != transaction_id:",
            "            logger.warning(",
            "                \"Received another txn %s from %s while still processing %s\",",
            "                transaction_id,",
            "                origin,",
            "                current_transaction,",
            "            )",
            "            return 429, {",
            "                \"errcode\": Codes.UNKNOWN,",
            "                \"error\": \"Too many concurrent transactions\",",
            "            }",
            "",
            "        # CRITICAL SECTION: we must now not await until we populate _active_transactions",
            "        # in _on_incoming_transaction_inner.",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # CRITICAL SECTION: the first thing we must do (before awaiting) is",
            "        # add an entry to _active_transactions.",
            "        assert origin not in self._active_transactions",
            "        self._active_transactions[origin] = transaction.transaction_id",
            "",
            "        try:",
            "            result = await self._handle_incoming_transaction(",
            "                origin, transaction, request_time",
            "            )",
            "            return result",
            "        finally:",
            "            del self._active_transactions[origin]",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        existing_response = await self.transaction_actions.have_responded(",
            "            origin, transaction",
            "        )",
            "",
            "        if existing_response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,",
            "            )",
            "            return existing_response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            gather_results(",
            "                (",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ),",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room: Dict[str, List[EventBase]] = {}",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str) -> None:",
            "            with nested_logging_context(room_id):",
            "                logger.debug(\"Processing PDUs for %s\", room_id)",
            "",
            "                try:",
            "                    await self.check_server_matches_acl(origin_host, room_id)",
            "                except AuthError as e:",
            "                    logger.warning(",
            "                        \"Ignoring PDUs for room %s from banned server\", room_id",
            "                    )",
            "                    for pdu in pdus_by_room[room_id]:",
            "                        event_id = pdu.event_id",
            "                        pdu_results[event_id] = e.error_dict(self.hs.config)",
            "                    return",
            "",
            "                for pdu in pdus_by_room[room_id]:",
            "                    pdu_results[pdu.event_id] = await process_pdu(pdu)",
            "",
            "        async def process_pdu(pdu: EventBase) -> JsonDict:",
            "            event_id = pdu.event_id",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    await self._handle_received_pdu(origin, pdu)",
            "                    return {}",
            "                except FederationError as e:",
            "                    logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                    return {\"error\": str(e)}",
            "                except Exception as e:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),  # type: ignore",
            "                    )",
            "                    return {\"error\": str(e)}",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            last_pdu_ts_metric.labels(server_name=origin).set(newest_pdu_ts / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction) -> None:",
            "        \"\"\"Process the EDUs in a received transaction.\"\"\"",
            "",
            "        async def _process_edu(edu_dict: JsonDict) -> None:",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            transaction.edus,",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            resp = await self._state_resp_cache.wrap(",
            "                (room_id, event_id),",
            "                self._on_context_state_request_compute,",
            "                room_id,",
            "                event_id,",
            "            )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id),",
            "            self._on_state_ids_request_compute,",
            "            room_id,",
            "            event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _on_state_ids_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> JsonDict:",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(room_id, state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": list(auth_chain_ids)}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        pdus: Collection[EventBase]",
            "        event_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        pdus = await self.store.get_events_as_list(event_ids)",
            "",
            "        auth_chain = await self.store.get_auth_chain(",
            "            room_id, [pdu.event_id for pdu in pdus]",
            "        )",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_dict_from_pdus([pdu])",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        # Refuse the request if that room has seen too many joins recently.",
            "        # This is in addition to the HS-level rate limiting applied by",
            "        # BaseFederationServlet.",
            "        # type-ignore: mypy doesn't seem able to deduce the type of the limiter(!?)",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {pdu.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "        caller_supports_partial_state: bool = False,",
            "    ) -> Dict[str, Any]:",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "",
            "        event, context = await self._on_send_membership_event(",
            "            origin, content, Membership.JOIN, room_id",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_event_ids: Collection[str]",
            "        servers_in_room: Optional[Collection[str]]",
            "        if caller_supports_partial_state:",
            "            state_event_ids = _get_event_ids_for_partial_state_join(",
            "                event, prev_state_ids",
            "            )",
            "            servers_in_room = await self.state.get_hosts_in_room_at_events(",
            "                room_id, event_ids=event.prev_event_ids()",
            "            )",
            "        else:",
            "            state_event_ids = prev_state_ids.values()",
            "            servers_in_room = None",
            "",
            "        auth_chain_event_ids = await self.store.get_auth_chain_ids(",
            "            room_id, state_event_ids",
            "        )",
            "",
            "        # if the caller has opted in, we can omit any auth_chain events which are",
            "        # already in state_event_ids",
            "        if caller_supports_partial_state:",
            "            auth_chain_event_ids.difference_update(state_event_ids)",
            "",
            "        auth_chain_events = await self.store.get_events_as_list(auth_chain_event_ids)",
            "        state_events = await self.store.get_events_as_list(state_event_ids)",
            "",
            "        # we try to do all the async stuff before this point, so that time_now is as",
            "        # accurate as possible.",
            "        time_now = self._clock.time_msec()",
            "        event_json = event.get_pdu_json(time_now)",
            "        resp = {",
            "            \"event\": event_json,",
            "            \"state\": [p.get_pdu_json(time_now) for p in state_events],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in auth_chain_events],",
            "            \"org.matrix.msc3706.partial_state\": caller_supports_partial_state,",
            "        }",
            "",
            "        if servers_in_room is not None:",
            "            resp[\"org.matrix.msc3706.servers_in_room\"] = list(servers_in_room)",
            "",
            "        return resp",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(",
            "        self, origin: str, content: JsonDict, room_id: str",
            "    ) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "        await self._on_send_membership_event(origin, content, Membership.LEAVE, room_id)",
            "        return {}",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> JsonDict:",
            "        \"\"\"We've received a /make_knock/ request, so we create a partial knock",
            "        event for the room and hand that back, along with the room version, to the knocking",
            "        homeserver. We do *not* persist or process this event until the other server has",
            "        signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "            supported_versions: The room versions supported by the requesting server.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # Before we do anything: check if the room is partial-stated.",
            "            # Note that at the time this check was added, `on_make_knock_request` would",
            "            # block due to https://github.com/matrix-org/synapse/issues/12997.",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_knock right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        # Check that this room version is supported by the remote homeserver",
            "        if room_version.identifier not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version.identifier, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version.identifier)",
            "",
            "        # Check that this room supports knocking as defined by its room version",
            "        if not room_version.msc2403_knocking:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        pdu = await self.handler.on_make_knock_request(origin, room_id, user_id)",
            "        return {",
            "            \"event\": pdu.get_templated_pdu_json(),",
            "            \"room_version\": room_version.identifier,",
            "        }",
            "",
            "    async def on_send_knock_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "    ) -> Dict[str, List[JsonDict]]:",
            "        \"\"\"",
            "        We have received a knock event for a room. Verify and send the event into the room",
            "        on the knocking homeserver's behalf. Then reply with some stripped state from the",
            "        room for the knockee.",
            "",
            "        Args:",
            "            origin: The remote homeserver of the knocking user.",
            "            content: The content of the request.",
            "            room_id: The ID of the room to knock on.",
            "",
            "        Returns:",
            "            The stripped room state.",
            "        \"\"\"",
            "        _, context = await self._on_send_membership_event(",
            "            origin, content, Membership.KNOCK, room_id",
            "        )",
            "",
            "        # Retrieve stripped state events from the room and send them back to the remote",
            "        # server. This will allow the remote server's clients to display information",
            "        # related to the room while the knock request is pending.",
            "        stripped_room_state = (",
            "            await self.store.get_stripped_room_state_from_event_context(",
            "                context, self._room_prejoin_state_types",
            "            )",
            "        )",
            "        return {\"knock_state_events\": stripped_room_state}",
            "",
            "    async def _on_send_membership_event(",
            "        self, origin: str, content: JsonDict, membership_type: str, room_id: str",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Handle an on_send_{join,leave,knock} request",
            "",
            "        Does some preliminary validation before passing the request on to the",
            "        federation handler.",
            "",
            "        Args:",
            "            origin: The (authenticated) requesting server",
            "            content: The body of the send_* request - a complete membership event",
            "            membership_type: The expected membership type (join or leave, depending",
            "                on the endpoint)",
            "            room_id: The room_id from the request, to be validated against the room_id",
            "                in the event",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            SynapseError if there is a problem with the request, including things like",
            "               the room_id not matching or the event not being authorized.",
            "        \"\"\"",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        if content[\"room_id\"] != room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room ID in body does not match that in request path\",",
            "                Codes.BAD_JSON,",
            "            )",
            "",
            "        # Note that get_room_version throws if the room does not exist here.",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /send_join, /send_knock or /send_leave.",
            "            # This is because we will not be able to provide the server list (for partial",
            "            # joins) or the full state (for full joins).",
            "            # Return a 404 as we would if we weren't in the room at all.",
            "            logger.info(",
            "                f\"Rejecting /send_{membership_type} to %s because it's a partial state room\",",
            "                room_id,",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                f\"Unable to handle /send_{membership_type} right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        if membership_type == Membership.KNOCK and not room_version.msc2403_knocking:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        event = event_from_pdu_json(content, room_version)",
            "",
            "        if event.type != EventTypes.Member or not event.is_state():",
            "            raise SynapseError(400, \"Not an m.room.member event\", Codes.BAD_JSON)",
            "",
            "        if event.content.get(\"membership\") != membership_type:",
            "            raise SynapseError(400, \"Not a %s event\" % membership_type, Codes.BAD_JSON)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, event.room_id)",
            "",
            "        logger.debug(\"_on_send_membership_event: pdu sigs: %s\", event.signatures)",
            "",
            "        # Sign the event since we're vouching on behalf of the remote server that",
            "        # the event is valid to be sent into the room. Currently this is only done",
            "        # if the user is being joined via restricted join rules.",
            "        if (",
            "            room_version.msc3083_join_rules",
            "            and event.membership == Membership.JOIN",
            "            and EventContentFields.AUTHORISING_USER in event.content",
            "        ):",
            "            # We can only authorise our own users.",
            "            authorising_server = get_domain_from_id(",
            "                event.content[EventContentFields.AUTHORISING_USER]",
            "            )",
            "            if authorising_server != self.server_name:",
            "                raise SynapseError(",
            "                    400,",
            "                    f\"Cannot authorise request from resident server: {authorising_server}\",",
            "                )",
            "",
            "            event.signatures.update(",
            "                compute_event_signature(",
            "                    room_version,",
            "                    event.get_pdu_json(),",
            "                    self.hs.hostname,",
            "                    self.hs.signing_key,",
            "                )",
            "            )",
            "",
            "        try:",
            "            event = await self._check_sigs_and_hash(room_version, event)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {event.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "",
            "        try:",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were persisting the event.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated during `on_send_membership_event`, trying again.\",",
            "                room_id,",
            "            )",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, origin: str, content: JsonDict",
            "    ) -> Dict[str, Any]:",
            "        query = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithm in device_keys.items():",
            "                query.append((user_id, device_id, algorithm))",
            "",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self.store.claim_e2e_one_time_keys(query)",
            "",
            "        json_result: Dict[str, Dict[str, dict]] = {}",
            "        for user_id, device_keys in results.items():",
            "            for device_id, keys in device_keys.items():",
            "                for key_id, json_str in keys.items():",
            "                    json_result.setdefault(user_id, {})[device_id] = {",
            "                        key_id: json_decoder.decode(json_str)",
            "                    }",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_dict_from_pdus(self, pdu_list: List[EventBase]) -> JsonDict:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            # Just need a dummy transaction ID and destination since it won't be used.",
            "            transaction_id=\"\",",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=\"\",",
            "        ).get_dict()",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            logger.warning(\"event id %s: %s\", pdu.event_id, e)",
            "            raise FederationError(\"ERROR\", 403, str(e), affected=pdu.event_id)",
            "",
            "        if await self._spam_checker.should_drop_federated_event(pdu):",
            "            logger.warning(",
            "                \"Unstaged federated event contains spam, dropping %s\", pdu.event_id",
            "            )",
            "            return",
            "",
            "        # Add the event to our staging area",
            "        await self.store.insert_received_event_to_staging(origin, pdu)",
            "",
            "        # Try and acquire the processing lock for the room, if we get it start a",
            "        # background process for handling the events in the room.",
            "        lock = await self.store.try_acquire_lock(",
            "            _INBOUND_EVENT_HANDLING_LOCK_NAME, pdu.room_id",
            "        )",
            "        if lock:",
            "            self._process_incoming_pdus_in_room_inner(",
            "                pdu.room_id, room_version, lock, origin, pdu",
            "            )",
            "",
            "    async def _get_next_nonspam_staged_event_for_room(",
            "        self, room_id: str, room_version: RoomVersion",
            "    ) -> Optional[Tuple[str, EventBase]]:",
            "        \"\"\"Fetch the first non-spam event from staging queue.",
            "",
            "        Args:",
            "            room_id: the room to fetch the first non-spam event in.",
            "            room_version: the version of the room.",
            "",
            "        Returns:",
            "            The first non-spam event in that room.",
            "        \"\"\"",
            "",
            "        while True:",
            "            # We need to do this check outside the lock to avoid a race between",
            "            # a new event being inserted by another instance and it attempting",
            "            # to acquire the lock.",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if next is None:",
            "                return None",
            "",
            "            origin, event = next",
            "",
            "            if await self._spam_checker.should_drop_federated_event(event):",
            "                logger.warning(",
            "                    \"Staged federated event contains spam, dropping %s\",",
            "                    event.event_id,",
            "                )",
            "                continue",
            "",
            "            return next",
            "",
            "    @wrap_as_background_process(\"_process_incoming_pdus_in_room_inner\")",
            "    async def _process_incoming_pdus_in_room_inner(",
            "        self,",
            "        room_id: str,",
            "        room_version: RoomVersion,",
            "        lock: Lock,",
            "        latest_origin: Optional[str] = None,",
            "        latest_event: Optional[EventBase] = None,",
            "    ) -> None:",
            "        \"\"\"Process events in the staging area for the given room.",
            "",
            "        The latest_origin and latest_event args are the latest origin and event",
            "        received (or None to simply pull the next event from the database).",
            "        \"\"\"",
            "",
            "        # The common path is for the event we just received be the only event in",
            "        # the room, so instead of pulling the event out of the DB and parsing",
            "        # the event we just pull out the next event ID and check if that matches.",
            "        if latest_event is not None and latest_origin is not None:",
            "            result = await self.store.get_next_staged_event_id_for_room(room_id)",
            "            if result is None:",
            "                latest_origin = None",
            "                latest_event = None",
            "            else:",
            "                next_origin, next_event_id = result",
            "                if (",
            "                    next_origin != latest_origin",
            "                    or next_event_id != latest_event.event_id",
            "                ):",
            "                    latest_origin = None",
            "                    latest_event = None",
            "",
            "        if latest_origin is None or latest_event is None:",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "            if not next:",
            "                await lock.release()",
            "                return",
            "",
            "            origin, event = next",
            "        else:",
            "            origin = latest_origin",
            "            event = latest_event",
            "",
            "        # We loop round until there are no more events in the room in the",
            "        # staging area, or we fail to get the lock (which means another process",
            "        # has started processing).",
            "        while True:",
            "            async with lock:",
            "                logger.info(\"handling received PDU in room %s: %s\", room_id, event)",
            "                try:",
            "                    with nested_logging_context(event.event_id):",
            "                        await self._federation_event_handler.on_receive_pdu(",
            "                            origin, event",
            "                        )",
            "                except FederationError as e:",
            "                    # XXX: Ideally we'd inform the remote we failed to process",
            "                    # the event, but we can't return an error in the transaction",
            "                    # response (as we've already responded).",
            "                    logger.warning(\"Error handling PDU %s: %s\", event.event_id, e)",
            "                except Exception:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event.event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),  # type: ignore",
            "                    )",
            "",
            "                received_ts = await self.store.remove_received_event_from_staging(",
            "                    origin, event.event_id",
            "                )",
            "                if received_ts is not None:",
            "                    pdu_process_time.observe(",
            "                        (self._clock.time_msec() - received_ts) / 1000",
            "                    )",
            "",
            "            next = await self._get_next_nonspam_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if not next:",
            "                break",
            "",
            "            origin, event = next",
            "",
            "            # Prune the event queue if it's getting large.",
            "            #",
            "            # We do this *after* handling the first event as the common case is",
            "            # that the queue is empty (/has the single event in), and so there's",
            "            # no need to do this check.",
            "            pruned = await self.store.prune_staged_events_in_room(room_id, room_version)",
            "            if pruned:",
            "                # If we have pruned the queue check we need to refetch the next",
            "                # event to handle.",
            "                next = await self.store.get_next_staged_event_for_room(",
            "                    room_id, room_version",
            "                )",
            "                if not next:",
            "                    break",
            "",
            "                origin, event = next",
            "",
            "            new_lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if not new_lock:",
            "                return",
            "            lock = new_lock",
            "",
            "    def __str__(self) -> str:",
            "        return \"<ReplicationLayer(%s)>\" % self.server_name",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ) -> None:",
            "        await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict) -> None:",
            "        await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str) -> None:",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        acl_event = await self._storage_controllers.state.get_current_state_event(",
            "            room_id, EventTypes.ServerACL, \"\"",
            "        )",
            "        if not acl_event or server_matches_acl_event(server_name, acl_event):",
            "            return",
            "",
            "        raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:",
            "    \"\"\"Check if the given server is allowed by the ACL event",
            "",
            "    Args:",
            "        server_name: name of server, without any port part",
            "        acl_event: m.room.server_acl event",
            "",
            "    Returns:",
            "        True if this server is allowed by the ACLs",
            "    \"\"\"",
            "    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)",
            "",
            "    # first of all, check if literal IPs are blocked, and if so, whether the",
            "    # server name is a literal IP",
            "    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)",
            "    if not isinstance(allow_ip_literals, bool):",
            "        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")",
            "        allow_ip_literals = True",
            "    if not allow_ip_literals:",
            "        # check for ipv6 literals. These start with '['.",
            "        if server_name[0] == \"[\":",
            "            return False",
            "",
            "        # check for ipv4 literals. We can just lift the routine from twisted.",
            "        if isIPAddress(server_name):",
            "            return False",
            "",
            "    # next,  check the deny list",
            "    deny = acl_event.content.get(\"deny\", [])",
            "    if not isinstance(deny, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list deny ACL %s\", deny)",
            "        deny = []",
            "    for e in deny:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched deny rule %s\", server_name, e)",
            "            return False",
            "",
            "    # then the allow list.",
            "    allow = acl_event.content.get(\"allow\", [])",
            "    if not isinstance(allow, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list allow ACL %s\", allow)",
            "        allow = []",
            "    for e in allow:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched allow rule %s\", server_name, e)",
            "            return True",
            "",
            "    # everything else should be rejected.",
            "    # logger.info(\"%s fell through\", server_name)",
            "    return False",
            "",
            "",
            "def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:",
            "    if not isinstance(acl_entry, str):",
            "        logger.warning(",
            "            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)",
            "        )",
            "        return False",
            "    regex = glob_to_regex(acl_entry)",
            "    return bool(regex.match(server_name))",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers: Dict[str, Callable[[str, dict], Awaitable[None]]] = {}",
            "        self.query_handlers: Dict[str, Callable[[dict], Awaitable[JsonDict]]] = {}",
            "",
            "        # Map from type to instance names that we should route EDU handling to.",
            "        # We randomly choose one instance from the list to route to for each new",
            "        # EDU received.",
            "        self._edu_type_to_instance: Dict[str, List[str]] = {}",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], Awaitable[JsonDict]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instances_for_edu(",
            "        self, edu_type: str, instance_names: List[str]",
            "    ) -> None:",
            "        \"\"\"Register that the EDU handler is on multiple instances.\"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_names",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict) -> None:",
            "        if not self.config.server.use_presence and edu_type == EduTypes.PRESENCE:",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        instances = self._edu_type_to_instance.get(edu_type, [\"master\"])",
            "        if self._instance_name not in instances:",
            "            # Pick an instance randomly so that we don't overload one.",
            "            route_to = random.choice(instances)",
            "",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict) -> JsonDict:",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))",
            "",
            "",
            "def _get_event_ids_for_partial_state_join(",
            "    join_event: EventBase,",
            "    prev_state_ids: StateMap[str],",
            ") -> Collection[str]:",
            "    \"\"\"Calculate state to be retuned in a partial_state send_join",
            "",
            "    Args:",
            "        join_event: the join event being send_joined",
            "        prev_state_ids: the event ids of the state before the join",
            "",
            "    Returns:",
            "        the event ids to be returned",
            "    \"\"\"",
            "",
            "    # return all non-member events",
            "    state_event_ids = {",
            "        event_id",
            "        for (event_type, state_key), event_id in prev_state_ids.items()",
            "        if event_type != EventTypes.Member",
            "    }",
            "",
            "    # we also need the current state of the current user (it's going to",
            "    # be an auth event for the new join, so we may as well return it)",
            "    current_membership_event_id = prev_state_ids.get(",
            "        (EventTypes.Member, join_event.state_key)",
            "    )",
            "    if current_membership_event_id is not None:",
            "        state_event_ids.add(current_membership_event_id)",
            "",
            "    # TODO: return a few more members:",
            "    #   - those with invites",
            "    #   - those that are kicked? / banned",
            "",
            "    return state_event_ids"
        ],
        "afterPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019-2021 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from matrix_common.regex import glob_to_regex",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.internet.abstract import isIPAddress",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import EduTypes, EventContentFields, EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_base import (",
            "    FederationBase,",
            "    InvalidEventSignatureError,",
            "    event_from_pdu_json,",
            ")",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import (",
            "    log_kv,",
            "    start_active_span_from_edu,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import wrap_as_background_process",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.lock import Lock",
            "from synapse.types import JsonDict, StateMap, get_domain_from_id",
            "from synapse.util import json_decoder, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute, gather_results",
            "from synapse.util.caches.response_cache import ResponseCache",
            "from synapse.util.stringutils import parse_server_name",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\",",
            "    \"Time taken to process an event\",",
            ")",
            "",
            "last_pdu_ts_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_time\",",
            "    \"The timestamp of the last PDU which was successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "# The name of the lock to use when process events in a room received over",
            "# federation.",
            "_INBOUND_EVENT_HANDLING_LOCK_NAME = \"federation_inbound_pdu\"",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.handler = hs.get_federation_handler()",
            "        self._spam_checker = hs.get_spam_checker()",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self.state = hs.get_state_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._room_member_handler = hs.get_room_member_handler()",
            "",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "",
            "        # origins that we are currently processing a transaction from.",
            "        # a dict from origin to txn id.",
            "        self._active_transactions: Dict[str, str] = {}",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"fed_txn_handler\", timeout_ms=30000",
            "        )",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache: ResponseCache[",
            "            Tuple[str, Optional[str]]",
            "        ] = ResponseCache(hs.get_clock(), \"state_resp\", timeout_ms=30000)",
            "        self._state_ids_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"state_ids_resp\", timeout_ms=30000",
            "        )",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.config.federation.federation_metrics_domains",
            "        )",
            "",
            "        self._room_prejoin_state_types = hs.config.api.room_prejoin_state",
            "",
            "        # Whether we have started handling old events in the staging area.",
            "        self._started_handling_of_staged_events = False",
            "",
            "    @wrap_as_background_process(\"_handle_old_staged_events\")",
            "    async def _handle_old_staged_events(self) -> None:",
            "        \"\"\"Handle old staged events by fetching all rooms that have staged",
            "        events and start the processing of each of those rooms.",
            "        \"\"\"",
            "",
            "        # Get all the rooms IDs with staged events.",
            "        room_ids = await self.store.get_all_rooms_with_staged_incoming_events()",
            "",
            "        # We then shuffle them so that if there are multiple instances doing",
            "        # this work they're less likely to collide.",
            "        random.shuffle(room_ids)",
            "",
            "        for room_id in room_ids:",
            "            room_version = await self.store.get_room_version(room_id)",
            "",
            "            # Try and acquire the processing lock for the room, if we get it start a",
            "            # background process for handling the events in the room.",
            "            lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if lock:",
            "                logger.info(\"Handling old staged inbound events in %s\", room_id)",
            "                self._process_incoming_pdus_in_room_inner(",
            "                    room_id,",
            "                    room_version,",
            "                    lock,",
            "                )",
            "",
            "            # We pause a bit so that we don't start handling all rooms at once.",
            "            await self._clock.sleep(random.uniform(0, 0.1))",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_dict_from_pdus(pdus)",
            "",
            "        return 200, res",
            "",
            "    async def on_timestamp_to_event_request(",
            "        self, origin: str, room_id: str, timestamp: int, direction: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"When we receive a federated `/timestamp_to_event` request,",
            "        handle all of the logic for validating and fetching the event.",
            "",
            "        Args:",
            "            origin: The server we received the event from",
            "            room_id: Room to fetch the event from",
            "            timestamp: The point in time (inclusive) we should navigate from in",
            "                the given direction to find the closest event.",
            "            direction: [\"f\"|\"b\"] to indicate whether we should navigate forward",
            "                or backward from the given timestamp to find the closest event.",
            "",
            "        Returns:",
            "            Tuple indicating the response status code and dictionary response",
            "            body including `event_id`.",
            "        \"\"\"",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            # We only try to fetch data from the local database",
            "            event_id = await self.store.get_event_id_for_timestamp(",
            "                room_id, timestamp, direction",
            "            )",
            "            if event_id:",
            "                event = await self.store.get_event(",
            "                    event_id, allow_none=False, allow_rejected=False",
            "                )",
            "",
            "                return 200, {",
            "                    \"event_id\": event_id,",
            "                    \"origin_server_ts\": event.origin_server_ts,",
            "                }",
            "",
            "        raise SynapseError(",
            "            404,",
            "            \"Unable to find event from %s in direction %s\" % (timestamp, direction),",
            "            errcode=Codes.NOT_FOUND,",
            "        )",
            "",
            "    async def on_incoming_transaction(",
            "        self,",
            "        origin: str,",
            "        transaction_id: str,",
            "        destination: str,",
            "        transaction_data: JsonDict,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # If we receive a transaction we should make sure that kick off handling",
            "        # any old events in the staging area.",
            "        if not self._started_handling_of_staged_events:",
            "            self._started_handling_of_staged_events = True",
            "            self._handle_old_staged_events()",
            "",
            "            # Start a periodic check for old staged events. This is to handle",
            "            # the case where locks time out, e.g. if another process gets killed",
            "            # without dropping its locks.",
            "            self._clock.looping_call(self._handle_old_staged_events, 60 * 1000)",
            "",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(",
            "            transaction_id=transaction_id,",
            "            destination=destination,",
            "            origin=origin,",
            "            origin_server_ts=transaction_data.get(\"origin_server_ts\"),  # type: ignore[arg-type]",
            "            pdus=transaction_data.get(\"pdus\"),",
            "            edus=transaction_data.get(\"edus\"),",
            "        )",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # Reject malformed transactions early: reject if too many PDUs/EDUs",
            "        if len(transaction.pdus) > 50 or len(transaction.edus) > 100:",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "            return 400, {}",
            "",
            "        # we only process one transaction from each origin at a time. We need to do",
            "        # this check here, rather than in _on_incoming_transaction_inner so that we",
            "        # don't cache the rejection in _transaction_resp_cache (so that if the txn",
            "        # arrives again later, we can process it).",
            "        current_transaction = self._active_transactions.get(origin)",
            "        if current_transaction and current_transaction != transaction_id:",
            "            logger.warning(",
            "                \"Received another txn %s from %s while still processing %s\",",
            "                transaction_id,",
            "                origin,",
            "                current_transaction,",
            "            )",
            "            return 429, {",
            "                \"errcode\": Codes.UNKNOWN,",
            "                \"error\": \"Too many concurrent transactions\",",
            "            }",
            "",
            "        # CRITICAL SECTION: we must now not await until we populate _active_transactions",
            "        # in _on_incoming_transaction_inner.",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # CRITICAL SECTION: the first thing we must do (before awaiting) is",
            "        # add an entry to _active_transactions.",
            "        assert origin not in self._active_transactions",
            "        self._active_transactions[origin] = transaction.transaction_id",
            "",
            "        try:",
            "            result = await self._handle_incoming_transaction(",
            "                origin, transaction, request_time",
            "            )",
            "            return result",
            "        finally:",
            "            del self._active_transactions[origin]",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        existing_response = await self.transaction_actions.have_responded(",
            "            origin, transaction",
            "        )",
            "",
            "        if existing_response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,",
            "            )",
            "            return existing_response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            gather_results(",
            "                (",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ),",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room: Dict[str, List[EventBase]] = {}",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str) -> None:",
            "            with nested_logging_context(room_id):",
            "                logger.debug(\"Processing PDUs for %s\", room_id)",
            "",
            "                try:",
            "                    await self.check_server_matches_acl(origin_host, room_id)",
            "                except AuthError as e:",
            "                    logger.warning(",
            "                        \"Ignoring PDUs for room %s from banned server\", room_id",
            "                    )",
            "                    for pdu in pdus_by_room[room_id]:",
            "                        event_id = pdu.event_id",
            "                        pdu_results[event_id] = e.error_dict(self.hs.config)",
            "                    return",
            "",
            "                for pdu in pdus_by_room[room_id]:",
            "                    pdu_results[pdu.event_id] = await process_pdu(pdu)",
            "",
            "        async def process_pdu(pdu: EventBase) -> JsonDict:",
            "            event_id = pdu.event_id",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    await self._handle_received_pdu(origin, pdu)",
            "                    return {}",
            "                except FederationError as e:",
            "                    logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                    return {\"error\": str(e)}",
            "                except Exception as e:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),  # type: ignore",
            "                    )",
            "                    return {\"error\": str(e)}",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            last_pdu_ts_metric.labels(server_name=origin).set(newest_pdu_ts / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction) -> None:",
            "        \"\"\"Process the EDUs in a received transaction.\"\"\"",
            "",
            "        async def _process_edu(edu_dict: JsonDict) -> None:",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            transaction.edus,",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            resp = await self._state_resp_cache.wrap(",
            "                (room_id, event_id),",
            "                self._on_context_state_request_compute,",
            "                room_id,",
            "                event_id,",
            "            )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id),",
            "            self._on_state_ids_request_compute,",
            "            room_id,",
            "            event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _on_state_ids_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> JsonDict:",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(room_id, state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": list(auth_chain_ids)}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        pdus: Collection[EventBase]",
            "        event_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        pdus = await self.store.get_events_as_list(event_ids)",
            "",
            "        auth_chain = await self.store.get_auth_chain(",
            "            room_id, [pdu.event_id for pdu in pdus]",
            "        )",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_dict_from_pdus([pdu])",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        # Refuse the request if that room has seen too many joins recently.",
            "        # This is in addition to the HS-level rate limiting applied by",
            "        # BaseFederationServlet.",
            "        # type-ignore: mypy doesn't seem able to deduce the type of the limiter(!?)",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {pdu.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "        caller_supports_partial_state: bool = False,",
            "    ) -> Dict[str, Any]:",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "",
            "        event, context = await self._on_send_membership_event(",
            "            origin, content, Membership.JOIN, room_id",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_event_ids: Collection[str]",
            "        servers_in_room: Optional[Collection[str]]",
            "        if caller_supports_partial_state:",
            "            state_event_ids = _get_event_ids_for_partial_state_join(",
            "                event, prev_state_ids",
            "            )",
            "            servers_in_room = await self.state.get_hosts_in_room_at_events(",
            "                room_id, event_ids=event.prev_event_ids()",
            "            )",
            "        else:",
            "            state_event_ids = prev_state_ids.values()",
            "            servers_in_room = None",
            "",
            "        auth_chain_event_ids = await self.store.get_auth_chain_ids(",
            "            room_id, state_event_ids",
            "        )",
            "",
            "        # if the caller has opted in, we can omit any auth_chain events which are",
            "        # already in state_event_ids",
            "        if caller_supports_partial_state:",
            "            auth_chain_event_ids.difference_update(state_event_ids)",
            "",
            "        auth_chain_events = await self.store.get_events_as_list(auth_chain_event_ids)",
            "        state_events = await self.store.get_events_as_list(state_event_ids)",
            "",
            "        # we try to do all the async stuff before this point, so that time_now is as",
            "        # accurate as possible.",
            "        time_now = self._clock.time_msec()",
            "        event_json = event.get_pdu_json(time_now)",
            "        resp = {",
            "            \"event\": event_json,",
            "            \"state\": [p.get_pdu_json(time_now) for p in state_events],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in auth_chain_events],",
            "            \"org.matrix.msc3706.partial_state\": caller_supports_partial_state,",
            "        }",
            "",
            "        if servers_in_room is not None:",
            "            resp[\"org.matrix.msc3706.servers_in_room\"] = list(servers_in_room)",
            "",
            "        return resp",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(",
            "        self, origin: str, content: JsonDict, room_id: str",
            "    ) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "        await self._on_send_membership_event(origin, content, Membership.LEAVE, room_id)",
            "        return {}",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> JsonDict:",
            "        \"\"\"We've received a /make_knock/ request, so we create a partial knock",
            "        event for the room and hand that back, along with the room version, to the knocking",
            "        homeserver. We do *not* persist or process this event until the other server has",
            "        signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "            supported_versions: The room versions supported by the requesting server.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # Before we do anything: check if the room is partial-stated.",
            "            # Note that at the time this check was added, `on_make_knock_request` would",
            "            # block due to https://github.com/matrix-org/synapse/issues/12997.",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_knock right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        # Check that this room version is supported by the remote homeserver",
            "        if room_version.identifier not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version.identifier, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version.identifier)",
            "",
            "        # Check that this room supports knocking as defined by its room version",
            "        if not room_version.msc2403_knocking:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        pdu = await self.handler.on_make_knock_request(origin, room_id, user_id)",
            "        return {",
            "            \"event\": pdu.get_templated_pdu_json(),",
            "            \"room_version\": room_version.identifier,",
            "        }",
            "",
            "    async def on_send_knock_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "    ) -> Dict[str, List[JsonDict]]:",
            "        \"\"\"",
            "        We have received a knock event for a room. Verify and send the event into the room",
            "        on the knocking homeserver's behalf. Then reply with some stripped state from the",
            "        room for the knockee.",
            "",
            "        Args:",
            "            origin: The remote homeserver of the knocking user.",
            "            content: The content of the request.",
            "            room_id: The ID of the room to knock on.",
            "",
            "        Returns:",
            "            The stripped room state.",
            "        \"\"\"",
            "        _, context = await self._on_send_membership_event(",
            "            origin, content, Membership.KNOCK, room_id",
            "        )",
            "",
            "        # Retrieve stripped state events from the room and send them back to the remote",
            "        # server. This will allow the remote server's clients to display information",
            "        # related to the room while the knock request is pending.",
            "        stripped_room_state = (",
            "            await self.store.get_stripped_room_state_from_event_context(",
            "                context, self._room_prejoin_state_types",
            "            )",
            "        )",
            "        return {\"knock_state_events\": stripped_room_state}",
            "",
            "    async def _on_send_membership_event(",
            "        self, origin: str, content: JsonDict, membership_type: str, room_id: str",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Handle an on_send_{join,leave,knock} request",
            "",
            "        Does some preliminary validation before passing the request on to the",
            "        federation handler.",
            "",
            "        Args:",
            "            origin: The (authenticated) requesting server",
            "            content: The body of the send_* request - a complete membership event",
            "            membership_type: The expected membership type (join or leave, depending",
            "                on the endpoint)",
            "            room_id: The room_id from the request, to be validated against the room_id",
            "                in the event",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            SynapseError if there is a problem with the request, including things like",
            "               the room_id not matching or the event not being authorized.",
            "        \"\"\"",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        if content[\"room_id\"] != room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room ID in body does not match that in request path\",",
            "                Codes.BAD_JSON,",
            "            )",
            "",
            "        # Note that get_room_version throws if the room does not exist here.",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /send_join, /send_knock or /send_leave.",
            "            # This is because we will not be able to provide the server list (for partial",
            "            # joins) or the full state (for full joins).",
            "            # Return a 404 as we would if we weren't in the room at all.",
            "            logger.info(",
            "                f\"Rejecting /send_{membership_type} to %s because it's a partial state room\",",
            "                room_id,",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                f\"Unable to handle /send_{membership_type} right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        if membership_type == Membership.KNOCK and not room_version.msc2403_knocking:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        event = event_from_pdu_json(content, room_version)",
            "",
            "        if event.type != EventTypes.Member or not event.is_state():",
            "            raise SynapseError(400, \"Not an m.room.member event\", Codes.BAD_JSON)",
            "",
            "        if event.content.get(\"membership\") != membership_type:",
            "            raise SynapseError(400, \"Not a %s event\" % membership_type, Codes.BAD_JSON)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, event.room_id)",
            "",
            "        logger.debug(\"_on_send_membership_event: pdu sigs: %s\", event.signatures)",
            "",
            "        # Sign the event since we're vouching on behalf of the remote server that",
            "        # the event is valid to be sent into the room. Currently this is only done",
            "        # if the user is being joined via restricted join rules.",
            "        if (",
            "            room_version.msc3083_join_rules",
            "            and event.membership == Membership.JOIN",
            "            and EventContentFields.AUTHORISING_USER in event.content",
            "        ):",
            "            # We can only authorise our own users.",
            "            authorising_server = get_domain_from_id(",
            "                event.content[EventContentFields.AUTHORISING_USER]",
            "            )",
            "            if authorising_server != self.server_name:",
            "                raise SynapseError(",
            "                    400,",
            "                    f\"Cannot authorise request from resident server: {authorising_server}\",",
            "                )",
            "",
            "            event.signatures.update(",
            "                compute_event_signature(",
            "                    room_version,",
            "                    event.get_pdu_json(),",
            "                    self.hs.hostname,",
            "                    self.hs.signing_key,",
            "                )",
            "            )",
            "",
            "        try:",
            "            event = await self._check_sigs_and_hash(room_version, event)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {event.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "",
            "        try:",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were persisting the event.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated during `on_send_membership_event`, trying again.\",",
            "                room_id,",
            "            )",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, origin: str, content: JsonDict",
            "    ) -> Dict[str, Any]:",
            "        query = []",
            "        for user_id, device_keys in content.get(\"one_time_keys\", {}).items():",
            "            for device_id, algorithm in device_keys.items():",
            "                query.append((user_id, device_id, algorithm))",
            "",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self.store.claim_e2e_one_time_keys(query)",
            "",
            "        json_result: Dict[str, Dict[str, dict]] = {}",
            "        for user_id, device_keys in results.items():",
            "            for device_id, keys in device_keys.items():",
            "                for key_id, json_str in keys.items():",
            "                    json_result.setdefault(user_id, {})[device_id] = {",
            "                        key_id: json_decoder.decode(json_str)",
            "                    }",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_dict_from_pdus(self, pdu_list: List[EventBase]) -> JsonDict:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            # Just need a dummy transaction ID and destination since it won't be used.",
            "            transaction_id=\"\",",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=\"\",",
            "        ).get_dict()",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            logger.warning(\"event id %s: %s\", pdu.event_id, e)",
            "            raise FederationError(\"ERROR\", 403, str(e), affected=pdu.event_id)",
            "",
            "        if await self._spam_checker.should_drop_federated_event(pdu):",
            "            logger.warning(",
            "                \"Unstaged federated event contains spam, dropping %s\", pdu.event_id",
            "            )",
            "            return",
            "",
            "        # Add the event to our staging area",
            "        await self.store.insert_received_event_to_staging(origin, pdu)",
            "",
            "        # Try and acquire the processing lock for the room, if we get it start a",
            "        # background process for handling the events in the room.",
            "        lock = await self.store.try_acquire_lock(",
            "            _INBOUND_EVENT_HANDLING_LOCK_NAME, pdu.room_id",
            "        )",
            "        if lock:",
            "            self._process_incoming_pdus_in_room_inner(",
            "                pdu.room_id, room_version, lock, origin, pdu",
            "            )",
            "",
            "    async def _get_next_nonspam_staged_event_for_room(",
            "        self, room_id: str, room_version: RoomVersion",
            "    ) -> Optional[Tuple[str, EventBase]]:",
            "        \"\"\"Fetch the first non-spam event from staging queue.",
            "",
            "        Args:",
            "            room_id: the room to fetch the first non-spam event in.",
            "            room_version: the version of the room.",
            "",
            "        Returns:",
            "            The first non-spam event in that room.",
            "        \"\"\"",
            "",
            "        while True:",
            "            # We need to do this check outside the lock to avoid a race between",
            "            # a new event being inserted by another instance and it attempting",
            "            # to acquire the lock.",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if next is None:",
            "                return None",
            "",
            "            origin, event = next",
            "",
            "            if await self._spam_checker.should_drop_federated_event(event):",
            "                logger.warning(",
            "                    \"Staged federated event contains spam, dropping %s\",",
            "                    event.event_id,",
            "                )",
            "                continue",
            "",
            "            return next",
            "",
            "    @wrap_as_background_process(\"_process_incoming_pdus_in_room_inner\")",
            "    async def _process_incoming_pdus_in_room_inner(",
            "        self,",
            "        room_id: str,",
            "        room_version: RoomVersion,",
            "        lock: Lock,",
            "        latest_origin: Optional[str] = None,",
            "        latest_event: Optional[EventBase] = None,",
            "    ) -> None:",
            "        \"\"\"Process events in the staging area for the given room.",
            "",
            "        The latest_origin and latest_event args are the latest origin and event",
            "        received (or None to simply pull the next event from the database).",
            "        \"\"\"",
            "",
            "        # The common path is for the event we just received be the only event in",
            "        # the room, so instead of pulling the event out of the DB and parsing",
            "        # the event we just pull out the next event ID and check if that matches.",
            "        if latest_event is not None and latest_origin is not None:",
            "            result = await self.store.get_next_staged_event_id_for_room(room_id)",
            "            if result is None:",
            "                latest_origin = None",
            "                latest_event = None",
            "            else:",
            "                next_origin, next_event_id = result",
            "                if (",
            "                    next_origin != latest_origin",
            "                    or next_event_id != latest_event.event_id",
            "                ):",
            "                    latest_origin = None",
            "                    latest_event = None",
            "",
            "        if latest_origin is None or latest_event is None:",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "            if not next:",
            "                await lock.release()",
            "                return",
            "",
            "            origin, event = next",
            "        else:",
            "            origin = latest_origin",
            "            event = latest_event",
            "",
            "        # We loop round until there are no more events in the room in the",
            "        # staging area, or we fail to get the lock (which means another process",
            "        # has started processing).",
            "        while True:",
            "            async with lock:",
            "                logger.info(\"handling received PDU in room %s: %s\", room_id, event)",
            "                try:",
            "                    with nested_logging_context(event.event_id):",
            "                        await self._federation_event_handler.on_receive_pdu(",
            "                            origin, event",
            "                        )",
            "                except FederationError as e:",
            "                    # XXX: Ideally we'd inform the remote we failed to process",
            "                    # the event, but we can't return an error in the transaction",
            "                    # response (as we've already responded).",
            "                    logger.warning(\"Error handling PDU %s: %s\", event.event_id, e)",
            "                except Exception:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event.event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),  # type: ignore",
            "                    )",
            "",
            "                received_ts = await self.store.remove_received_event_from_staging(",
            "                    origin, event.event_id",
            "                )",
            "                if received_ts is not None:",
            "                    pdu_process_time.observe(",
            "                        (self._clock.time_msec() - received_ts) / 1000",
            "                    )",
            "",
            "            next = await self._get_next_nonspam_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if not next:",
            "                break",
            "",
            "            origin, event = next",
            "",
            "            # Prune the event queue if it's getting large.",
            "            #",
            "            # We do this *after* handling the first event as the common case is",
            "            # that the queue is empty (/has the single event in), and so there's",
            "            # no need to do this check.",
            "            pruned = await self.store.prune_staged_events_in_room(room_id, room_version)",
            "            if pruned:",
            "                # If we have pruned the queue check we need to refetch the next",
            "                # event to handle.",
            "                next = await self.store.get_next_staged_event_for_room(",
            "                    room_id, room_version",
            "                )",
            "                if not next:",
            "                    break",
            "",
            "                origin, event = next",
            "",
            "            new_lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if not new_lock:",
            "                return",
            "            lock = new_lock",
            "",
            "    def __str__(self) -> str:",
            "        return \"<ReplicationLayer(%s)>\" % self.server_name",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ) -> None:",
            "        await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict) -> None:",
            "        await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str) -> None:",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        acl_event = await self._storage_controllers.state.get_current_state_event(",
            "            room_id, EventTypes.ServerACL, \"\"",
            "        )",
            "        if not acl_event or server_matches_acl_event(server_name, acl_event):",
            "            return",
            "",
            "        raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:",
            "    \"\"\"Check if the given server is allowed by the ACL event",
            "",
            "    Args:",
            "        server_name: name of server, without any port part",
            "        acl_event: m.room.server_acl event",
            "",
            "    Returns:",
            "        True if this server is allowed by the ACLs",
            "    \"\"\"",
            "    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)",
            "",
            "    # first of all, check if literal IPs are blocked, and if so, whether the",
            "    # server name is a literal IP",
            "    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)",
            "    if not isinstance(allow_ip_literals, bool):",
            "        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")",
            "        allow_ip_literals = True",
            "    if not allow_ip_literals:",
            "        # check for ipv6 literals. These start with '['.",
            "        if server_name[0] == \"[\":",
            "            return False",
            "",
            "        # check for ipv4 literals. We can just lift the routine from twisted.",
            "        if isIPAddress(server_name):",
            "            return False",
            "",
            "    # next,  check the deny list",
            "    deny = acl_event.content.get(\"deny\", [])",
            "    if not isinstance(deny, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list deny ACL %s\", deny)",
            "        deny = []",
            "    for e in deny:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched deny rule %s\", server_name, e)",
            "            return False",
            "",
            "    # then the allow list.",
            "    allow = acl_event.content.get(\"allow\", [])",
            "    if not isinstance(allow, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list allow ACL %s\", allow)",
            "        allow = []",
            "    for e in allow:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched allow rule %s\", server_name, e)",
            "            return True",
            "",
            "    # everything else should be rejected.",
            "    # logger.info(\"%s fell through\", server_name)",
            "    return False",
            "",
            "",
            "def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:",
            "    if not isinstance(acl_entry, str):",
            "        logger.warning(",
            "            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)",
            "        )",
            "        return False",
            "    regex = glob_to_regex(acl_entry)",
            "    return bool(regex.match(server_name))",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers: Dict[str, Callable[[str, dict], Awaitable[None]]] = {}",
            "        self.query_handlers: Dict[str, Callable[[dict], Awaitable[JsonDict]]] = {}",
            "",
            "        # Map from type to instance names that we should route EDU handling to.",
            "        # We randomly choose one instance from the list to route to for each new",
            "        # EDU received.",
            "        self._edu_type_to_instance: Dict[str, List[str]] = {}",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], Awaitable[JsonDict]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instances_for_edu(",
            "        self, edu_type: str, instance_names: List[str]",
            "    ) -> None:",
            "        \"\"\"Register that the EDU handler is on multiple instances.\"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_names",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict) -> None:",
            "        if not self.config.server.use_presence and edu_type == EduTypes.PRESENCE:",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        instances = self._edu_type_to_instance.get(edu_type, [\"master\"])",
            "        if self._instance_name not in instances:",
            "            # Pick an instance randomly so that we don't overload one.",
            "            route_to = random.choice(instances)",
            "",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict) -> JsonDict:",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))",
            "",
            "",
            "def _get_event_ids_for_partial_state_join(",
            "    join_event: EventBase,",
            "    prev_state_ids: StateMap[str],",
            ") -> Collection[str]:",
            "    \"\"\"Calculate state to be retuned in a partial_state send_join",
            "",
            "    Args:",
            "        join_event: the join event being send_joined",
            "        prev_state_ids: the event ids of the state before the join",
            "",
            "    Returns:",
            "        the event ids to be returned",
            "    \"\"\"",
            "",
            "    # return all non-member events",
            "    state_event_ids = {",
            "        event_id",
            "        for (event_type, state_key), event_id in prev_state_ids.items()",
            "        if event_type != EventTypes.Member",
            "    }",
            "",
            "    # we also need the current state of the current user (it's going to",
            "    # be an auth event for the new join, so we may as well return it)",
            "    current_membership_event_id = prev_state_ids.get(",
            "        (EventTypes.Member, join_event.state_key)",
            "    )",
            "    if current_membership_event_id is not None:",
            "        state_event_ids.add(current_membership_event_id)",
            "",
            "    # TODO: return a few more members:",
            "    #   - those with invites",
            "    #   - those that are kicked? / banned",
            "",
            "    return state_event_ids"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "536": [
                "FederationServer"
            ],
            "537": [
                "FederationServer"
            ],
            "538": [
                "FederationServer"
            ],
            "539": [
                "FederationServer"
            ],
            "566": [
                "FederationServer"
            ],
            "567": [
                "FederationServer"
            ],
            "568": [
                "FederationServer"
            ],
            "569": [
                "FederationServer"
            ]
        },
        "addLocation": [
            "synapse.federation.federation_server.FederationServer.self"
        ]
    },
    "synapse/handlers/event_auth.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from synapse.events.builder import EventBuilder"
            },
            "1": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from synapse.events.snapshot import EventContext"
            },
            "2": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from synapse.types import StateMap, get_domain_from_id"
            },
            "3": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from synapse.util.metrics import Measure"
            },
            "4": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " if TYPE_CHECKING:"
            },
            "6": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "     from synapse.server import HomeServer"
            },
            "7": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "             Codes.UNABLE_TO_GRANT_JOIN,"
            },
            "8": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "         )"
            },
            "9": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 157,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    async def check_host_in_room(self, room_id: str, host: str) -> bool:"
            },
            "11": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        with Measure(self._clock, \"check_host_in_room\"):"
            },
            "12": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return await self._store.is_host_joined(room_id, host)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 158,
                "PatchRowcode": "+    async def is_host_in_room(self, room_id: str, host: str) -> bool:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 159,
                "PatchRowcode": "+        return await self._store.is_host_joined(room_id, host)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 160,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+    async def assert_host_in_room("
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 162,
                "PatchRowcode": "+        self, room_id: str, host: str, allow_partial_state_rooms: bool = False"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 163,
                "PatchRowcode": "+    ) -> None:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 164,
                "PatchRowcode": "+        \"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+        Asserts that the host is in the room, or raises an AuthError."
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 166,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+        If the room is partial-stated, we raise an AuthError with the"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+        UNABLE_DUE_TO_PARTIAL_STATE error code, unless `allow_partial_state_rooms` is true."
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+        If allow_partial_state_rooms is True and the room is partial-stated,"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+        this function may return an incorrect result as we are not able to fully"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 172,
                "PatchRowcode": "+        track server membership in a room without full state."
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 173,
                "PatchRowcode": "+        \"\"\""
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 174,
                "PatchRowcode": "+        if not allow_partial_state_rooms and await self._store.is_partial_state_room("
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 175,
                "PatchRowcode": "+            room_id"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+        ):"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+            raise AuthError("
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+                403,"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 179,
                "PatchRowcode": "+                \"Unable to authorise you right now; room is partial-stated here.\","
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+                errcode=Codes.UNABLE_DUE_TO_PARTIAL_STATE,"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+            )"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+        if not await self.is_host_in_room(room_id, host):"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+            raise AuthError(403, \"Host not in room.\")"
            },
            "40": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 185,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "     async def check_restricted_join_rules("
            },
            "42": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "         self,"
            }
        },
        "frontPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Collection, List, Optional, Union",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventTypes,",
            "    JoinRules,",
            "    Membership,",
            "    RestrictedJoinRuleTypes,",
            ")",
            "from synapse.api.errors import AuthError, Codes, SynapseError",
            "from synapse.api.room_versions import RoomVersion",
            "from synapse.event_auth import (",
            "    check_state_dependent_auth_rules,",
            "    check_state_independent_auth_rules,",
            ")",
            "from synapse.events import EventBase",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.snapshot import EventContext",
            "from synapse.types import StateMap, get_domain_from_id",
            "from synapse.util.metrics import Measure",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class EventAuthHandler:",
            "    \"\"\"",
            "    This class contains methods for authenticating events added to room graphs.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._clock = hs.get_clock()",
            "        self._store = hs.get_datastores().main",
            "        self._server_name = hs.hostname",
            "",
            "    async def check_auth_rules_from_context(",
            "        self,",
            "        event: EventBase,",
            "        context: EventContext,",
            "    ) -> None:",
            "        \"\"\"Check an event passes the auth rules at its own auth events\"\"\"",
            "        await check_state_independent_auth_rules(self._store, event)",
            "        auth_event_ids = event.auth_event_ids()",
            "        auth_events_by_id = await self._store.get_events(auth_event_ids)",
            "        check_state_dependent_auth_rules(event, auth_events_by_id.values())",
            "",
            "    def compute_auth_events(",
            "        self,",
            "        event: Union[EventBase, EventBuilder],",
            "        current_state_ids: StateMap[str],",
            "        for_verification: bool = False,",
            "    ) -> List[str]:",
            "        \"\"\"Given an event and current state return the list of event IDs used",
            "        to auth an event.",
            "",
            "        If `for_verification` is False then only return auth events that",
            "        should be added to the event's `auth_events`.",
            "",
            "        Returns:",
            "            List of event IDs.",
            "        \"\"\"",
            "",
            "        if event.type == EventTypes.Create:",
            "            return []",
            "",
            "        # Currently we ignore the `for_verification` flag even though there are",
            "        # some situations where we can drop particular auth events when adding",
            "        # to the event's `auth_events` (e.g. joins pointing to previous joins",
            "        # when room is publicly joinable). Dropping event IDs has the",
            "        # advantage that the auth chain for the room grows slower, but we use",
            "        # the auth chain in state resolution v2 to order events, which means",
            "        # care must be taken if dropping events to ensure that it doesn't",
            "        # introduce undesirable \"state reset\" behaviour.",
            "        #",
            "        # All of which sounds a bit tricky so we don't bother for now.",
            "        auth_ids = []",
            "        for etype, state_key in event_auth.auth_types_for_event(",
            "            event.room_version, event",
            "        ):",
            "            auth_ev_id = current_state_ids.get((etype, state_key))",
            "            if auth_ev_id:",
            "                auth_ids.append(auth_ev_id)",
            "",
            "        return auth_ids",
            "",
            "    async def get_user_which_could_invite(",
            "        self, room_id: str, current_state_ids: StateMap[str]",
            "    ) -> str:",
            "        \"\"\"",
            "        Searches the room state for a local user who has the power level necessary",
            "        to invite other users.",
            "",
            "        Args:",
            "            room_id: The room ID under search.",
            "            current_state_ids: The current state of the room.",
            "",
            "        Returns:",
            "            The MXID of the user which could issue an invite.",
            "",
            "        Raises:",
            "            SynapseError if no appropriate user is found.",
            "        \"\"\"",
            "        power_level_event_id = current_state_ids.get((EventTypes.PowerLevels, \"\"))",
            "        invite_level = 0",
            "        users_default_level = 0",
            "        if power_level_event_id:",
            "            power_level_event = await self._store.get_event(power_level_event_id)",
            "            invite_level = power_level_event.content.get(\"invite\", invite_level)",
            "            users_default_level = power_level_event.content.get(",
            "                \"users_default\", users_default_level",
            "            )",
            "            users = power_level_event.content.get(\"users\", {})",
            "        else:",
            "            users = {}",
            "",
            "        # Find the user with the highest power level (only interested in local",
            "        # users).",
            "        local_users_in_room = await self._store.get_local_users_in_room(room_id)",
            "        chosen_user = max(",
            "            local_users_in_room,",
            "            key=lambda user: users.get(user, users_default_level),",
            "            default=None,",
            "        )",
            "",
            "        # Return the chosen if they can issue invites.",
            "        user_power_level = users.get(chosen_user, users_default_level)",
            "        if chosen_user and user_power_level >= invite_level:",
            "            logger.debug(",
            "                \"Found a user who can issue invites  %s with power level %d >= invite level %d\",",
            "                chosen_user,",
            "                user_power_level,",
            "                invite_level,",
            "            )",
            "            return chosen_user",
            "",
            "        # No user was found.",
            "        raise SynapseError(",
            "            400,",
            "            \"Unable to find a user which could issue an invite\",",
            "            Codes.UNABLE_TO_GRANT_JOIN,",
            "        )",
            "",
            "    async def check_host_in_room(self, room_id: str, host: str) -> bool:",
            "        with Measure(self._clock, \"check_host_in_room\"):",
            "            return await self._store.is_host_joined(room_id, host)",
            "",
            "    async def check_restricted_join_rules(",
            "        self,",
            "        state_ids: StateMap[str],",
            "        room_version: RoomVersion,",
            "        user_id: str,",
            "        prev_member_event: Optional[EventBase],",
            "    ) -> None:",
            "        \"\"\"",
            "        Check whether a user can join a room without an invite due to restricted join rules.",
            "",
            "        When joining a room with restricted joined rules (as defined in MSC3083),",
            "        the membership of rooms must be checked during a room join.",
            "",
            "        Args:",
            "            state_ids: The state of the room as it currently is.",
            "            room_version: The room version of the room being joined.",
            "            user_id: The user joining the room.",
            "            prev_member_event: The current membership event for this user.",
            "",
            "        Raises:",
            "            AuthError if the user cannot join the room.",
            "        \"\"\"",
            "        # If the member is invited or currently joined, then nothing to do.",
            "        if prev_member_event and (",
            "            prev_member_event.membership in (Membership.JOIN, Membership.INVITE)",
            "        ):",
            "            return",
            "",
            "        # This is not a room with a restricted join rule, so we don't need to do the",
            "        # restricted room specific checks.",
            "        #",
            "        # Note: We'll be applying the standard join rule checks later, which will",
            "        # catch the cases of e.g. trying to join private rooms without an invite.",
            "        if not await self.has_restricted_join_rules(state_ids, room_version):",
            "            return",
            "",
            "        # Get the rooms which allow access to this room and check if the user is",
            "        # in any of them.",
            "        allowed_rooms = await self.get_rooms_that_allow_join(state_ids)",
            "        if not await self.is_user_in_rooms(allowed_rooms, user_id):",
            "",
            "            # If this is a remote request, the user might be in an allowed room",
            "            # that we do not know about.",
            "            if get_domain_from_id(user_id) != self._server_name:",
            "                for room_id in allowed_rooms:",
            "                    if not await self._store.is_host_joined(room_id, self._server_name):",
            "                        raise SynapseError(",
            "                            400,",
            "                            f\"Unable to check if {user_id} is in allowed rooms.\",",
            "                            Codes.UNABLE_AUTHORISE_JOIN,",
            "                        )",
            "",
            "            raise AuthError(",
            "                403,",
            "                \"You do not belong to any of the required rooms/spaces to join this room.\",",
            "            )",
            "",
            "    async def has_restricted_join_rules(",
            "        self, state_ids: StateMap[str], room_version: RoomVersion",
            "    ) -> bool:",
            "        \"\"\"",
            "        Return if the room has the proper join rules set for access via rooms.",
            "",
            "        Args:",
            "            state_ids: The state of the room as it currently is.",
            "            room_version: The room version of the room to query.",
            "",
            "        Returns:",
            "            True if the proper room version and join rules are set for restricted access.",
            "        \"\"\"",
            "        # This only applies to room versions which support the new join rule.",
            "        if not room_version.msc3083_join_rules:",
            "            return False",
            "",
            "        # If there's no join rule, then it defaults to invite (so this doesn't apply).",
            "        join_rules_event_id = state_ids.get((EventTypes.JoinRules, \"\"), None)",
            "        if not join_rules_event_id:",
            "            return False",
            "",
            "        # If the join rule is not restricted, this doesn't apply.",
            "        join_rules_event = await self._store.get_event(join_rules_event_id)",
            "        content_join_rule = join_rules_event.content.get(\"join_rule\")",
            "        if content_join_rule == JoinRules.RESTRICTED:",
            "            return True",
            "",
            "        # also check for MSC3787 behaviour",
            "        if room_version.msc3787_knock_restricted_join_rule:",
            "            return content_join_rule == JoinRules.KNOCK_RESTRICTED",
            "",
            "        return False",
            "",
            "    async def get_rooms_that_allow_join(",
            "        self, state_ids: StateMap[str]",
            "    ) -> Collection[str]:",
            "        \"\"\"",
            "        Generate a list of rooms in which membership allows access to a room.",
            "",
            "        Args:",
            "            state_ids: The current state of the room the user wishes to join",
            "",
            "        Returns:",
            "            A collection of room IDs. Membership in any of the rooms in the list grants the ability to join the target room.",
            "        \"\"\"",
            "        # If there's no join rule, then it defaults to invite (so this doesn't apply).",
            "        join_rules_event_id = state_ids.get((EventTypes.JoinRules, \"\"), None)",
            "        if not join_rules_event_id:",
            "            return ()",
            "",
            "        # If the join rule is not restricted, this doesn't apply.",
            "        join_rules_event = await self._store.get_event(join_rules_event_id)",
            "",
            "        # If allowed is of the wrong form, then only allow invited users.",
            "        allow_list = join_rules_event.content.get(\"allow\", [])",
            "        if not isinstance(allow_list, list):",
            "            return ()",
            "",
            "        # Pull out the other room IDs, invalid data gets filtered.",
            "        result = []",
            "        for allow in allow_list:",
            "            if not isinstance(allow, dict):",
            "                continue",
            "",
            "            # If the type is unexpected, skip it.",
            "            if allow.get(\"type\") != RestrictedJoinRuleTypes.ROOM_MEMBERSHIP:",
            "                continue",
            "",
            "            room_id = allow.get(\"room_id\")",
            "            if not isinstance(room_id, str):",
            "                continue",
            "",
            "            result.append(room_id)",
            "",
            "        return result",
            "",
            "    async def is_user_in_rooms(self, room_ids: Collection[str], user_id: str) -> bool:",
            "        \"\"\"",
            "        Check whether a user is a member of any of the provided rooms.",
            "",
            "        Args:",
            "            room_ids: The rooms to check for membership.",
            "            user_id: The user to check.",
            "",
            "        Returns:",
            "            True if the user is in any of the rooms, false otherwise.",
            "        \"\"\"",
            "        if not room_ids:",
            "            return False",
            "",
            "        # Get the list of joined rooms and see if there's an overlap.",
            "        joined_rooms = await self._store.get_rooms_for_user(user_id)",
            "",
            "        # Check each room and see if the user is in it.",
            "        for room_id in room_ids:",
            "            if room_id in joined_rooms:",
            "                return True",
            "",
            "        # The user was not in any of the rooms.",
            "        return False"
        ],
        "afterPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Collection, List, Optional, Union",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventTypes,",
            "    JoinRules,",
            "    Membership,",
            "    RestrictedJoinRuleTypes,",
            ")",
            "from synapse.api.errors import AuthError, Codes, SynapseError",
            "from synapse.api.room_versions import RoomVersion",
            "from synapse.event_auth import (",
            "    check_state_dependent_auth_rules,",
            "    check_state_independent_auth_rules,",
            ")",
            "from synapse.events import EventBase",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.snapshot import EventContext",
            "from synapse.types import StateMap, get_domain_from_id",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class EventAuthHandler:",
            "    \"\"\"",
            "    This class contains methods for authenticating events added to room graphs.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._clock = hs.get_clock()",
            "        self._store = hs.get_datastores().main",
            "        self._server_name = hs.hostname",
            "",
            "    async def check_auth_rules_from_context(",
            "        self,",
            "        event: EventBase,",
            "        context: EventContext,",
            "    ) -> None:",
            "        \"\"\"Check an event passes the auth rules at its own auth events\"\"\"",
            "        await check_state_independent_auth_rules(self._store, event)",
            "        auth_event_ids = event.auth_event_ids()",
            "        auth_events_by_id = await self._store.get_events(auth_event_ids)",
            "        check_state_dependent_auth_rules(event, auth_events_by_id.values())",
            "",
            "    def compute_auth_events(",
            "        self,",
            "        event: Union[EventBase, EventBuilder],",
            "        current_state_ids: StateMap[str],",
            "        for_verification: bool = False,",
            "    ) -> List[str]:",
            "        \"\"\"Given an event and current state return the list of event IDs used",
            "        to auth an event.",
            "",
            "        If `for_verification` is False then only return auth events that",
            "        should be added to the event's `auth_events`.",
            "",
            "        Returns:",
            "            List of event IDs.",
            "        \"\"\"",
            "",
            "        if event.type == EventTypes.Create:",
            "            return []",
            "",
            "        # Currently we ignore the `for_verification` flag even though there are",
            "        # some situations where we can drop particular auth events when adding",
            "        # to the event's `auth_events` (e.g. joins pointing to previous joins",
            "        # when room is publicly joinable). Dropping event IDs has the",
            "        # advantage that the auth chain for the room grows slower, but we use",
            "        # the auth chain in state resolution v2 to order events, which means",
            "        # care must be taken if dropping events to ensure that it doesn't",
            "        # introduce undesirable \"state reset\" behaviour.",
            "        #",
            "        # All of which sounds a bit tricky so we don't bother for now.",
            "        auth_ids = []",
            "        for etype, state_key in event_auth.auth_types_for_event(",
            "            event.room_version, event",
            "        ):",
            "            auth_ev_id = current_state_ids.get((etype, state_key))",
            "            if auth_ev_id:",
            "                auth_ids.append(auth_ev_id)",
            "",
            "        return auth_ids",
            "",
            "    async def get_user_which_could_invite(",
            "        self, room_id: str, current_state_ids: StateMap[str]",
            "    ) -> str:",
            "        \"\"\"",
            "        Searches the room state for a local user who has the power level necessary",
            "        to invite other users.",
            "",
            "        Args:",
            "            room_id: The room ID under search.",
            "            current_state_ids: The current state of the room.",
            "",
            "        Returns:",
            "            The MXID of the user which could issue an invite.",
            "",
            "        Raises:",
            "            SynapseError if no appropriate user is found.",
            "        \"\"\"",
            "        power_level_event_id = current_state_ids.get((EventTypes.PowerLevels, \"\"))",
            "        invite_level = 0",
            "        users_default_level = 0",
            "        if power_level_event_id:",
            "            power_level_event = await self._store.get_event(power_level_event_id)",
            "            invite_level = power_level_event.content.get(\"invite\", invite_level)",
            "            users_default_level = power_level_event.content.get(",
            "                \"users_default\", users_default_level",
            "            )",
            "            users = power_level_event.content.get(\"users\", {})",
            "        else:",
            "            users = {}",
            "",
            "        # Find the user with the highest power level (only interested in local",
            "        # users).",
            "        local_users_in_room = await self._store.get_local_users_in_room(room_id)",
            "        chosen_user = max(",
            "            local_users_in_room,",
            "            key=lambda user: users.get(user, users_default_level),",
            "            default=None,",
            "        )",
            "",
            "        # Return the chosen if they can issue invites.",
            "        user_power_level = users.get(chosen_user, users_default_level)",
            "        if chosen_user and user_power_level >= invite_level:",
            "            logger.debug(",
            "                \"Found a user who can issue invites  %s with power level %d >= invite level %d\",",
            "                chosen_user,",
            "                user_power_level,",
            "                invite_level,",
            "            )",
            "            return chosen_user",
            "",
            "        # No user was found.",
            "        raise SynapseError(",
            "            400,",
            "            \"Unable to find a user which could issue an invite\",",
            "            Codes.UNABLE_TO_GRANT_JOIN,",
            "        )",
            "",
            "    async def is_host_in_room(self, room_id: str, host: str) -> bool:",
            "        return await self._store.is_host_joined(room_id, host)",
            "",
            "    async def assert_host_in_room(",
            "        self, room_id: str, host: str, allow_partial_state_rooms: bool = False",
            "    ) -> None:",
            "        \"\"\"",
            "        Asserts that the host is in the room, or raises an AuthError.",
            "",
            "        If the room is partial-stated, we raise an AuthError with the",
            "        UNABLE_DUE_TO_PARTIAL_STATE error code, unless `allow_partial_state_rooms` is true.",
            "",
            "        If allow_partial_state_rooms is True and the room is partial-stated,",
            "        this function may return an incorrect result as we are not able to fully",
            "        track server membership in a room without full state.",
            "        \"\"\"",
            "        if not allow_partial_state_rooms and await self._store.is_partial_state_room(",
            "            room_id",
            "        ):",
            "            raise AuthError(",
            "                403,",
            "                \"Unable to authorise you right now; room is partial-stated here.\",",
            "                errcode=Codes.UNABLE_DUE_TO_PARTIAL_STATE,",
            "            )",
            "",
            "        if not await self.is_host_in_room(room_id, host):",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "    async def check_restricted_join_rules(",
            "        self,",
            "        state_ids: StateMap[str],",
            "        room_version: RoomVersion,",
            "        user_id: str,",
            "        prev_member_event: Optional[EventBase],",
            "    ) -> None:",
            "        \"\"\"",
            "        Check whether a user can join a room without an invite due to restricted join rules.",
            "",
            "        When joining a room with restricted joined rules (as defined in MSC3083),",
            "        the membership of rooms must be checked during a room join.",
            "",
            "        Args:",
            "            state_ids: The state of the room as it currently is.",
            "            room_version: The room version of the room being joined.",
            "            user_id: The user joining the room.",
            "            prev_member_event: The current membership event for this user.",
            "",
            "        Raises:",
            "            AuthError if the user cannot join the room.",
            "        \"\"\"",
            "        # If the member is invited or currently joined, then nothing to do.",
            "        if prev_member_event and (",
            "            prev_member_event.membership in (Membership.JOIN, Membership.INVITE)",
            "        ):",
            "            return",
            "",
            "        # This is not a room with a restricted join rule, so we don't need to do the",
            "        # restricted room specific checks.",
            "        #",
            "        # Note: We'll be applying the standard join rule checks later, which will",
            "        # catch the cases of e.g. trying to join private rooms without an invite.",
            "        if not await self.has_restricted_join_rules(state_ids, room_version):",
            "            return",
            "",
            "        # Get the rooms which allow access to this room and check if the user is",
            "        # in any of them.",
            "        allowed_rooms = await self.get_rooms_that_allow_join(state_ids)",
            "        if not await self.is_user_in_rooms(allowed_rooms, user_id):",
            "",
            "            # If this is a remote request, the user might be in an allowed room",
            "            # that we do not know about.",
            "            if get_domain_from_id(user_id) != self._server_name:",
            "                for room_id in allowed_rooms:",
            "                    if not await self._store.is_host_joined(room_id, self._server_name):",
            "                        raise SynapseError(",
            "                            400,",
            "                            f\"Unable to check if {user_id} is in allowed rooms.\",",
            "                            Codes.UNABLE_AUTHORISE_JOIN,",
            "                        )",
            "",
            "            raise AuthError(",
            "                403,",
            "                \"You do not belong to any of the required rooms/spaces to join this room.\",",
            "            )",
            "",
            "    async def has_restricted_join_rules(",
            "        self, state_ids: StateMap[str], room_version: RoomVersion",
            "    ) -> bool:",
            "        \"\"\"",
            "        Return if the room has the proper join rules set for access via rooms.",
            "",
            "        Args:",
            "            state_ids: The state of the room as it currently is.",
            "            room_version: The room version of the room to query.",
            "",
            "        Returns:",
            "            True if the proper room version and join rules are set for restricted access.",
            "        \"\"\"",
            "        # This only applies to room versions which support the new join rule.",
            "        if not room_version.msc3083_join_rules:",
            "            return False",
            "",
            "        # If there's no join rule, then it defaults to invite (so this doesn't apply).",
            "        join_rules_event_id = state_ids.get((EventTypes.JoinRules, \"\"), None)",
            "        if not join_rules_event_id:",
            "            return False",
            "",
            "        # If the join rule is not restricted, this doesn't apply.",
            "        join_rules_event = await self._store.get_event(join_rules_event_id)",
            "        content_join_rule = join_rules_event.content.get(\"join_rule\")",
            "        if content_join_rule == JoinRules.RESTRICTED:",
            "            return True",
            "",
            "        # also check for MSC3787 behaviour",
            "        if room_version.msc3787_knock_restricted_join_rule:",
            "            return content_join_rule == JoinRules.KNOCK_RESTRICTED",
            "",
            "        return False",
            "",
            "    async def get_rooms_that_allow_join(",
            "        self, state_ids: StateMap[str]",
            "    ) -> Collection[str]:",
            "        \"\"\"",
            "        Generate a list of rooms in which membership allows access to a room.",
            "",
            "        Args:",
            "            state_ids: The current state of the room the user wishes to join",
            "",
            "        Returns:",
            "            A collection of room IDs. Membership in any of the rooms in the list grants the ability to join the target room.",
            "        \"\"\"",
            "        # If there's no join rule, then it defaults to invite (so this doesn't apply).",
            "        join_rules_event_id = state_ids.get((EventTypes.JoinRules, \"\"), None)",
            "        if not join_rules_event_id:",
            "            return ()",
            "",
            "        # If the join rule is not restricted, this doesn't apply.",
            "        join_rules_event = await self._store.get_event(join_rules_event_id)",
            "",
            "        # If allowed is of the wrong form, then only allow invited users.",
            "        allow_list = join_rules_event.content.get(\"allow\", [])",
            "        if not isinstance(allow_list, list):",
            "            return ()",
            "",
            "        # Pull out the other room IDs, invalid data gets filtered.",
            "        result = []",
            "        for allow in allow_list:",
            "            if not isinstance(allow, dict):",
            "                continue",
            "",
            "            # If the type is unexpected, skip it.",
            "            if allow.get(\"type\") != RestrictedJoinRuleTypes.ROOM_MEMBERSHIP:",
            "                continue",
            "",
            "            room_id = allow.get(\"room_id\")",
            "            if not isinstance(room_id, str):",
            "                continue",
            "",
            "            result.append(room_id)",
            "",
            "        return result",
            "",
            "    async def is_user_in_rooms(self, room_ids: Collection[str], user_id: str) -> bool:",
            "        \"\"\"",
            "        Check whether a user is a member of any of the provided rooms.",
            "",
            "        Args:",
            "            room_ids: The rooms to check for membership.",
            "            user_id: The user to check.",
            "",
            "        Returns:",
            "            True if the user is in any of the rooms, false otherwise.",
            "        \"\"\"",
            "        if not room_ids:",
            "            return False",
            "",
            "        # Get the list of joined rooms and see if there's an overlap.",
            "        joined_rooms = await self._store.get_rooms_for_user(user_id)",
            "",
            "        # Check each room and see if the user is in it.",
            "        for room_id in room_ids:",
            "            if room_id in joined_rooms:",
            "                return True",
            "",
            "        # The user was not in any of the rooms.",
            "        return False"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "34": [],
            "159": [
                "EventAuthHandler"
            ],
            "160": [
                "EventAuthHandler"
            ],
            "161": [
                "EventAuthHandler"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/federation.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 804,
                "afterPatchRowNumber": 804,
                "PatchRowcode": "             )"
            },
            "1": {
                "beforePatchRowNumber": 805,
                "afterPatchRowNumber": 805,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 806,
                "afterPatchRowNumber": 806,
                "PatchRowcode": "         # now check that we are *still* in the room"
            },
            "3": {
                "beforePatchRowNumber": 807,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        is_in_room = await self._event_auth_handler.check_host_in_room("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 807,
                "PatchRowcode": "+        is_in_room = await self._event_auth_handler.is_host_in_room("
            },
            "5": {
                "beforePatchRowNumber": 808,
                "afterPatchRowNumber": 808,
                "PatchRowcode": "             room_id, self.server_name"
            },
            "6": {
                "beforePatchRowNumber": 809,
                "afterPatchRowNumber": 809,
                "PatchRowcode": "         )"
            },
            "7": {
                "beforePatchRowNumber": 810,
                "afterPatchRowNumber": 810,
                "PatchRowcode": "         if not is_in_room:"
            },
            "8": {
                "beforePatchRowNumber": 1150,
                "afterPatchRowNumber": 1150,
                "PatchRowcode": "     async def on_backfill_request("
            },
            "9": {
                "beforePatchRowNumber": 1151,
                "afterPatchRowNumber": 1151,
                "PatchRowcode": "         self, origin: str, room_id: str, pdu_list: List[str], limit: int"
            },
            "10": {
                "beforePatchRowNumber": 1152,
                "afterPatchRowNumber": 1152,
                "PatchRowcode": "     ) -> List[EventBase]:"
            },
            "11": {
                "beforePatchRowNumber": 1153,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)"
            },
            "12": {
                "beforePatchRowNumber": 1154,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not in_room:"
            },
            "13": {
                "beforePatchRowNumber": 1155,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise AuthError(403, \"Host not in room.\")"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1153,
                "PatchRowcode": "+        await self._event_auth_handler.assert_host_in_room(room_id, origin)"
            },
            "15": {
                "beforePatchRowNumber": 1156,
                "afterPatchRowNumber": 1154,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 1157,
                "afterPatchRowNumber": 1155,
                "PatchRowcode": "         # Synapse asks for 100 events per backfill request. Do not allow more."
            },
            "17": {
                "beforePatchRowNumber": 1158,
                "afterPatchRowNumber": 1156,
                "PatchRowcode": "         limit = min(limit, 100)"
            },
            "18": {
                "beforePatchRowNumber": 1198,
                "afterPatchRowNumber": 1196,
                "PatchRowcode": "             event_id, allow_none=True, allow_rejected=True"
            },
            "19": {
                "beforePatchRowNumber": 1199,
                "afterPatchRowNumber": 1197,
                "PatchRowcode": "         )"
            },
            "20": {
                "beforePatchRowNumber": 1200,
                "afterPatchRowNumber": 1198,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 1201,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if event:"
            },
            "22": {
                "beforePatchRowNumber": 1202,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            in_room = await self._event_auth_handler.check_host_in_room("
            },
            "23": {
                "beforePatchRowNumber": 1203,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                event.room_id, origin"
            },
            "24": {
                "beforePatchRowNumber": 1204,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "25": {
                "beforePatchRowNumber": 1205,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if not in_room:"
            },
            "26": {
                "beforePatchRowNumber": 1206,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                raise AuthError(403, \"Host not in room.\")"
            },
            "27": {
                "beforePatchRowNumber": 1207,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "28": {
                "beforePatchRowNumber": 1208,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            events = await filter_events_for_server("
            },
            "29": {
                "beforePatchRowNumber": 1209,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self._storage_controllers, origin, [event]"
            },
            "30": {
                "beforePatchRowNumber": 1210,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "31": {
                "beforePatchRowNumber": 1211,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            event = events[0]"
            },
            "32": {
                "beforePatchRowNumber": 1212,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return event"
            },
            "33": {
                "beforePatchRowNumber": 1213,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1199,
                "PatchRowcode": "+        if not event:"
            },
            "35": {
                "beforePatchRowNumber": 1214,
                "afterPatchRowNumber": 1200,
                "PatchRowcode": "             return None"
            },
            "36": {
                "beforePatchRowNumber": 1215,
                "afterPatchRowNumber": 1201,
                "PatchRowcode": " "
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1202,
                "PatchRowcode": "+        await self._event_auth_handler.assert_host_in_room(event.room_id, origin)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1203,
                "PatchRowcode": "+"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1204,
                "PatchRowcode": "+        events = await filter_events_for_server("
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1205,
                "PatchRowcode": "+            self._storage_controllers, origin, [event]"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1206,
                "PatchRowcode": "+        )"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1207,
                "PatchRowcode": "+        event = events[0]"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1208,
                "PatchRowcode": "+        return event"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1209,
                "PatchRowcode": "+"
            },
            "45": {
                "beforePatchRowNumber": 1216,
                "afterPatchRowNumber": 1210,
                "PatchRowcode": "     async def on_get_missing_events("
            },
            "46": {
                "beforePatchRowNumber": 1217,
                "afterPatchRowNumber": 1211,
                "PatchRowcode": "         self,"
            },
            "47": {
                "beforePatchRowNumber": 1218,
                "afterPatchRowNumber": 1212,
                "PatchRowcode": "         origin: str,"
            },
            "48": {
                "beforePatchRowNumber": 1221,
                "afterPatchRowNumber": 1215,
                "PatchRowcode": "         latest_events: List[str],"
            },
            "49": {
                "beforePatchRowNumber": 1222,
                "afterPatchRowNumber": 1216,
                "PatchRowcode": "         limit: int,"
            },
            "50": {
                "beforePatchRowNumber": 1223,
                "afterPatchRowNumber": 1217,
                "PatchRowcode": "     ) -> List[EventBase]:"
            },
            "51": {
                "beforePatchRowNumber": 1224,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)"
            },
            "52": {
                "beforePatchRowNumber": 1225,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not in_room:"
            },
            "53": {
                "beforePatchRowNumber": 1226,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise AuthError(403, \"Host not in room.\")"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1218,
                "PatchRowcode": "+        await self._event_auth_handler.assert_host_in_room(room_id, origin)"
            },
            "55": {
                "beforePatchRowNumber": 1227,
                "afterPatchRowNumber": 1219,
                "PatchRowcode": " "
            },
            "56": {
                "beforePatchRowNumber": 1228,
                "afterPatchRowNumber": 1220,
                "PatchRowcode": "         # Only allow up to 20 events to be retrieved per request."
            },
            "57": {
                "beforePatchRowNumber": 1229,
                "afterPatchRowNumber": 1221,
                "PatchRowcode": "         limit = min(limit, 20)"
            },
            "58": {
                "beforePatchRowNumber": 1257,
                "afterPatchRowNumber": 1249,
                "PatchRowcode": "             \"state_key\": target_user_id,"
            },
            "59": {
                "beforePatchRowNumber": 1258,
                "afterPatchRowNumber": 1250,
                "PatchRowcode": "         }"
            },
            "60": {
                "beforePatchRowNumber": 1259,
                "afterPatchRowNumber": 1251,
                "PatchRowcode": " "
            },
            "61": {
                "beforePatchRowNumber": 1260,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if await self._event_auth_handler.check_host_in_room(room_id, self.hs.hostname):"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1252,
                "PatchRowcode": "+        if await self._event_auth_handler.is_host_in_room(room_id, self.hs.hostname):"
            },
            "63": {
                "beforePatchRowNumber": 1261,
                "afterPatchRowNumber": 1253,
                "PatchRowcode": "             room_version_obj = await self.store.get_room_version(room_id)"
            },
            "64": {
                "beforePatchRowNumber": 1262,
                "afterPatchRowNumber": 1254,
                "PatchRowcode": "             builder = self.event_builder_factory.for_room_version("
            },
            "65": {
                "beforePatchRowNumber": 1263,
                "afterPatchRowNumber": 1255,
                "PatchRowcode": "                 room_version_obj, event_dict"
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2022 The Matrix.org Foundation C.I.C.",
            "# Copyright 2020 Sorunome",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Contains handlers for federation events.\"\"\"",
            "",
            "import enum",
            "import itertools",
            "import logging",
            "from enum import Enum",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Collection,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "import attr",
            "from prometheus_client import Histogram",
            "from signedjson.key import decode_verify_key_bytes",
            "from signedjson.sign import verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import EventContentFields, EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    CodeMessageException,",
            "    Codes,",
            "    FederationDeniedError,",
            "    FederationError,",
            "    HttpResponseException,",
            "    LimitExceededError,",
            "    NotFoundError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.events.validator import EventValidator",
            "from synapse.federation.federation_client import InvalidResponseError",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import SynapseTags, set_tag, tag_args, trace",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.module_api import NOT_SPAM",
            "from synapse.replication.http.federation import (",
            "    ReplicationCleanRoomRestServlet,",
            "    ReplicationStoreRoomOnOutlierMembershipRestServlet,",
            ")",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.storage.state import StateFilter",
            "from synapse.types import JsonDict, get_domain_from_id",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.visibility import filter_events_for_server",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_before_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_before_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        30.0,",
            "        40.0,",
            "        60.0,",
            "        80.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "class _BackfillPointType(Enum):",
            "    # a regular backwards extremity (ie, an event which we don't yet have, but which",
            "    # is referred to by other events in the DAG)",
            "    BACKWARDS_EXTREMITY = enum.auto()",
            "",
            "    # an MSC2716 \"insertion event\"",
            "    INSERTION_PONT = enum.auto()",
            "",
            "",
            "@attr.s(slots=True, auto_attribs=True, frozen=True)",
            "class _BackfillPoint:",
            "    \"\"\"A potential point we might backfill from\"\"\"",
            "",
            "    event_id: str",
            "    depth: int",
            "    type: _BackfillPointType",
            "",
            "",
            "class FederationHandler:",
            "    \"\"\"Handles general incoming federation requests",
            "",
            "    Incoming events are *not* handled here, for which see FederationEventHandler.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "",
            "        self.clock = hs.get_clock()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self.federation_client = hs.get_federation_client()",
            "        self.state_handler = hs.get_state_handler()",
            "        self.server_name = hs.hostname",
            "        self.keyring = hs.get_keyring()",
            "        self.is_mine_id = hs.is_mine_id",
            "        self.spam_checker = hs.get_spam_checker()",
            "        self.event_creation_handler = hs.get_event_creation_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._server_notices_mxid = hs.config.servernotices.server_notices_mxid",
            "        self.config = hs.config",
            "        self.http_client = hs.get_proxied_blacklisted_http_client()",
            "        self._replication = hs.get_replication_data_handler()",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "",
            "        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(",
            "            hs",
            "        )",
            "",
            "        if hs.config.worker.worker_app:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                self.store.maybe_store_room_on_outlier_membership",
            "            )",
            "",
            "        self._room_backfill = Linearizer(\"room_backfill\")",
            "",
            "        self.third_party_event_rules = hs.get_third_party_event_rules()",
            "",
            "        # if this is the main process, fire off a background process to resume",
            "        # any partial-state-resync operations which were in flight when we",
            "        # were shut down.",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"resume_sync_partial_state_room\", self._resume_sync_partial_state_room",
            "            )",
            "",
            "    @trace",
            "    async def maybe_backfill(",
            "        self, room_id: str, current_depth: int, limit: int",
            "    ) -> bool:",
            "        \"\"\"Checks the database to see if we should backfill before paginating,",
            "        and if so do.",
            "",
            "        Args:",
            "            room_id",
            "            current_depth: The depth from which we're paginating from. This is",
            "                used to decide if we should backfill and what extremities to",
            "                use.",
            "            limit: The number of events that the pagination request will",
            "                return. This is used as part of the heuristic to decide if we",
            "                should back paginate.",
            "        \"\"\"",
            "        # Starting the processing time here so we can include the room backfill",
            "        # linearizer lock queue in the timing",
            "        processing_start_time = self.clock.time_msec()",
            "",
            "        async with self._room_backfill.queue(room_id):",
            "            return await self._maybe_backfill_inner(",
            "                room_id,",
            "                current_depth,",
            "                limit,",
            "                processing_start_time=processing_start_time,",
            "            )",
            "",
            "    async def _maybe_backfill_inner(",
            "        self,",
            "        room_id: str,",
            "        current_depth: int,",
            "        limit: int,",
            "        *,",
            "        processing_start_time: int,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Checks whether the `current_depth` is at or approaching any backfill",
            "        points in the room and if so, will backfill. We only care about",
            "        checking backfill points that happened before the `current_depth`",
            "        (meaning less than or equal to the `current_depth`).",
            "",
            "        Args:",
            "            room_id: The room to backfill in.",
            "            current_depth: The depth to check at for any upcoming backfill points.",
            "            limit: The max number of events to request from the remote federated server.",
            "            processing_start_time: The time when `maybe_backfill` started",
            "                processing. Only used for timing.",
            "        \"\"\"",
            "        backwards_extremities = [",
            "            _BackfillPoint(event_id, depth, _BackfillPointType.BACKWARDS_EXTREMITY)",
            "            for event_id, depth in await self.store.get_oldest_event_ids_with_depth_in_room(",
            "                room_id",
            "            )",
            "        ]",
            "",
            "        insertion_events_to_be_backfilled: List[_BackfillPoint] = []",
            "        if self.hs.config.experimental.msc2716_enabled:",
            "            insertion_events_to_be_backfilled = [",
            "                _BackfillPoint(event_id, depth, _BackfillPointType.INSERTION_PONT)",
            "                for event_id, depth in await self.store.get_insertion_event_backward_extremities_in_room(",
            "                    room_id",
            "                )",
            "            ]",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: backwards_extremities=%s insertion_events_to_be_backfilled=%s\",",
            "            backwards_extremities,",
            "            insertion_events_to_be_backfilled,",
            "        )",
            "",
            "        if not backwards_extremities and not insertion_events_to_be_backfilled:",
            "            logger.debug(\"Not backfilling as no extremeties found.\")",
            "            return False",
            "",
            "        # we now have a list of potential places to backpaginate from. We prefer to",
            "        # start with the most recent (ie, max depth), so let's sort the list.",
            "        sorted_backfill_points: List[_BackfillPoint] = sorted(",
            "            itertools.chain(",
            "                backwards_extremities,",
            "                insertion_events_to_be_backfilled,",
            "            ),",
            "            key=lambda e: -int(e.depth),",
            "        )",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: room_id: %s: current_depth: %s, limit: %s, \"",
            "            \"backfill points (%d): %s\",",
            "            room_id,",
            "            current_depth,",
            "            limit,",
            "            len(sorted_backfill_points),",
            "            sorted_backfill_points,",
            "        )",
            "",
            "        # If we're approaching an extremity we trigger a backfill, otherwise we",
            "        # no-op.",
            "        #",
            "        # We chose twice the limit here as then clients paginating backwards",
            "        # will send pagination requests that trigger backfill at least twice",
            "        # using the most recent extremity before it gets removed (see below). We",
            "        # chose more than one times the limit in case of failure, but choosing a",
            "        # much larger factor will result in triggering a backfill request much",
            "        # earlier than necessary.",
            "        #",
            "        # XXX: shouldn't we do this *after* the filter by depth below? Again, we don't",
            "        # care about events that have happened after our current position.",
            "        #",
            "        max_depth = sorted_backfill_points[0].depth",
            "        if current_depth - 2 * limit > max_depth:",
            "            logger.debug(",
            "                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",",
            "                max_depth,",
            "                current_depth,",
            "                limit,",
            "            )",
            "            return False",
            "",
            "        # We ignore extremities that have a greater depth than our current depth",
            "        # as:",
            "        #    1. we don't really care about getting events that have happened",
            "        #       after our current position; and",
            "        #    2. we have likely previously tried and failed to backfill from that",
            "        #       extremity, so to avoid getting \"stuck\" requesting the same",
            "        #       backfill repeatedly we drop those extremities.",
            "        #",
            "        # However, we need to check that the filtered extremities are non-empty.",
            "        # If they are empty then either we can a) bail or b) still attempt to",
            "        # backfill. We opt to try backfilling anyway just in case we do get",
            "        # relevant events.",
            "        #",
            "        filtered_sorted_backfill_points = [",
            "            t for t in sorted_backfill_points if t.depth <= current_depth",
            "        ]",
            "        if filtered_sorted_backfill_points:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: backfill points before current depth: %s\",",
            "                filtered_sorted_backfill_points,",
            "            )",
            "            sorted_backfill_points = filtered_sorted_backfill_points",
            "        else:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: all backfill points are *after* current depth. Backfilling anyway.\"",
            "            )",
            "",
            "        # For performance's sake, we only want to paginate from a particular extremity",
            "        # if we can actually see the events we'll get. Otherwise, we'd just spend a lot",
            "        # of resources to get redacted events. We check each extremity in turn and",
            "        # ignore those which users on our server wouldn't be able to see.",
            "        #",
            "        # Additionally, we limit ourselves to backfilling from at most 5 extremities,",
            "        # for two reasons:",
            "        #",
            "        # - The check which determines if we can see an extremity's events can be",
            "        #   expensive (we load the full state for the room at each of the backfill",
            "        #   points, or (worse) their successors)",
            "        # - We want to avoid the server-server API request URI becoming too long.",
            "        #",
            "        # *Note*: the spec wants us to keep backfilling until we reach the start",
            "        # of the room in case we are allowed to see some of the history. However,",
            "        # in practice that causes more issues than its worth, as (a) it's",
            "        # relatively rare for there to be any visible history and (b) even when",
            "        # there is it's often sufficiently long ago that clients would stop",
            "        # attempting to paginate before backfill reached the visible history.",
            "",
            "        extremities_to_request: List[str] = []",
            "        for bp in sorted_backfill_points:",
            "            if len(extremities_to_request) >= 5:",
            "                break",
            "",
            "            # For regular backwards extremities, we don't have the extremity events",
            "            # themselves, so we need to actually check the events that reference them -",
            "            # their \"successor\" events.",
            "            #",
            "            # TODO: Correctly handle the case where we are allowed to see the",
            "            #   successor event but not the backward extremity, e.g. in the case of",
            "            #   initial join of the server where we are allowed to see the join",
            "            #   event but not anything before it. This would require looking at the",
            "            #   state *before* the event, ignoring the special casing certain event",
            "            #   types have.",
            "            if bp.type == _BackfillPointType.INSERTION_PONT:",
            "                event_ids_to_check = [bp.event_id]",
            "            else:",
            "                event_ids_to_check = await self.store.get_successor_events(bp.event_id)",
            "",
            "            events_to_check = await self.store.get_events_as_list(",
            "                event_ids_to_check,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                get_prev_content=False,",
            "            )",
            "",
            "            # We set `check_history_visibility_only` as we might otherwise get false",
            "            # positives from users having been erased.",
            "            filtered_extremities = await filter_events_for_server(",
            "                self._storage_controllers,",
            "                self.server_name,",
            "                events_to_check,",
            "                redact=False,",
            "                check_history_visibility_only=True,",
            "            )",
            "            if filtered_extremities:",
            "                extremities_to_request.append(bp.event_id)",
            "            else:",
            "                logger.debug(",
            "                    \"_maybe_backfill_inner: skipping extremity %s as it would not be visible\",",
            "                    bp,",
            "                )",
            "",
            "        if not extremities_to_request:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: found no extremities which would be visible\"",
            "            )",
            "            return False",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: extremities_to_request %s\", extremities_to_request",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request\",",
            "            str(extremities_to_request),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request.length\",",
            "            str(len(extremities_to_request)),",
            "        )",
            "",
            "        # Now we need to decide which hosts to hit first.",
            "        # First we try hosts that are already in the room.",
            "        # TODO: HEURISTIC ALERT.",
            "        likely_domains = (",
            "            await self._storage_controllers.state.get_current_hosts_in_room(room_id)",
            "        )",
            "",
            "        async def try_backfill(domains: Collection[str]) -> bool:",
            "            # TODO: Should we try multiple of these at a time?",
            "            for dom in domains:",
            "                # We don't want to ask our own server for information we don't have",
            "                if dom == self.server_name:",
            "                    continue",
            "",
            "                try:",
            "                    await self._federation_event_handler.backfill(",
            "                        dom, room_id, limit=100, extremities=extremities_to_request",
            "                    )",
            "                    # If this succeeded then we probably already have the",
            "                    # appropriate stuff.",
            "                    # TODO: We can probably do something more intelligent here.",
            "                    return True",
            "                except (SynapseError, InvalidResponseError) as e:",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except HttpResponseException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise e.to_synapse_error()",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except CodeMessageException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except NotRetryingDestination as e:",
            "                    logger.info(str(e))",
            "                    continue",
            "                except RequestSendFailed as e:",
            "                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except FederationDeniedError as e:",
            "                    logger.info(e)",
            "                    continue",
            "                except Exception as e:",
            "                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "",
            "            return False",
            "",
            "        processing_end_time = self.clock.time_msec()",
            "        backfill_processing_before_timer.observe(",
            "            (processing_end_time - processing_start_time) / 1000",
            "        )",
            "",
            "        success = await try_backfill(likely_domains)",
            "        if success:",
            "            return True",
            "",
            "        # TODO: we could also try servers which were previously in the room, but",
            "        #   are no longer.",
            "",
            "        return False",
            "",
            "    async def send_invite(self, target_host: str, event: EventBase) -> EventBase:",
            "        \"\"\"Sends the invite to the remote server for signing.",
            "",
            "        Invites must be signed by the invitee's server before distribution.",
            "        \"\"\"",
            "        try:",
            "            pdu = await self.federation_client.send_invite(",
            "                destination=target_host,",
            "                room_id=event.room_id,",
            "                event_id=event.event_id,",
            "                pdu=event,",
            "            )",
            "        except RequestSendFailed:",
            "            raise SynapseError(502, f\"Can't connect to server {target_host}\")",
            "",
            "        return pdu",
            "",
            "    async def on_event_auth(self, event_id: str) -> List[EventBase]:",
            "        event = await self.store.get_event(event_id)",
            "        auth = await self.store.get_auth_chain(",
            "            event.room_id, list(event.auth_event_ids()), include_given=True",
            "        )",
            "        return list(auth)",
            "",
            "    async def do_invite_join(",
            "        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Attempts to join the `joinee` to the room `room_id` via the",
            "        servers contained in `target_hosts`.",
            "",
            "        This first triggers a /make_join/ request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via /send_join/ which responds with the state at that",
            "        event and the auth_chains.",
            "",
            "        We suspend processing of any received events from this room until we",
            "        have finished processing the join.",
            "",
            "        Args:",
            "            target_hosts: List of servers to attempt to join the room with.",
            "",
            "            room_id: The ID of the room to join.",
            "",
            "            joinee: The User ID of the joining user.",
            "",
            "            content: The event content to use for the join event.",
            "        \"\"\"",
            "        # TODO: We should be able to call this on workers, but the upgrading of",
            "        # room stuff after join currently doesn't work on workers.",
            "        # TODO: Before we relax this condition, we need to allow re-syncing of",
            "        # partial room state to happen on workers.",
            "        assert self.config.worker.worker_app is None",
            "",
            "        logger.debug(\"Joining %s to %s\", joinee, room_id)",
            "",
            "        origin, event, room_version_obj = await self._make_and_verify_event(",
            "            target_hosts,",
            "            room_id,",
            "            joinee,",
            "            \"join\",",
            "            content,",
            "            params={\"ver\": KNOWN_ROOM_VERSIONS},",
            "        )",
            "",
            "        # This shouldn't happen, because the RoomMemberHandler has a",
            "        # linearizer lock which only allows one operation per user per room",
            "        # at a time - so this is just paranoia.",
            "        assert room_id not in self._federation_event_handler.room_queues",
            "",
            "        self._federation_event_handler.room_queues[room_id] = []",
            "",
            "        await self._clean_room_for_join(room_id)",
            "",
            "        try:",
            "            # Try the host we successfully got a response to /make_join/",
            "            # request first.",
            "            host_list = list(target_hosts)",
            "            try:",
            "                host_list.remove(origin)",
            "                host_list.insert(0, origin)",
            "            except ValueError:",
            "                pass",
            "",
            "            ret = await self.federation_client.send_join(",
            "                host_list, event, room_version_obj",
            "            )",
            "",
            "            event = ret.event",
            "            origin = ret.origin",
            "            state = ret.state",
            "            auth_chain = ret.auth_chain",
            "            auth_chain.sort(key=lambda e: e.depth)",
            "",
            "            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)",
            "            logger.debug(\"do_invite_join state: %s\", state)",
            "",
            "            logger.debug(\"do_invite_join event: %s\", event)",
            "",
            "            # if this is the first time we've joined this room, it's time to add",
            "            # a row to `rooms` with the correct room version. If there's already a",
            "            # row there, we should override it, since it may have been populated",
            "            # based on an invite request which lied about the room version.",
            "            #",
            "            # federation_client.send_join has already checked that the room",
            "            # version in the received create event is the same as room_version_obj,",
            "            # so we can rely on it now.",
            "            #",
            "            await self.store.upsert_room_on_join(",
            "                room_id=room_id,",
            "                room_version=room_version_obj,",
            "                state_events=state,",
            "            )",
            "",
            "            if ret.partial_state:",
            "                # Mark the room as having partial state.",
            "                # The background process is responsible for unmarking this flag,",
            "                # even if the join fails.",
            "                await self.store.store_partial_state_room(room_id, ret.servers_in_room)",
            "",
            "            try:",
            "                max_stream_id = (",
            "                    await self._federation_event_handler.process_remote_join(",
            "                        origin,",
            "                        room_id,",
            "                        auth_chain,",
            "                        state,",
            "                        event,",
            "                        room_version_obj,",
            "                        partial_state=ret.partial_state,",
            "                    )",
            "                )",
            "            except PartialStateConflictError as e:",
            "                # The homeserver was already in the room and it is no longer partial",
            "                # stated. We ought to be doing a local join instead. Turn the error into",
            "                # a 429, as a hint to the client to try again.",
            "                # TODO(faster_joins): `_should_perform_remote_join` suggests that we may",
            "                #   do a remote join for restricted rooms even if we have full state.",
            "                logger.error(",
            "                    \"Room %s was un-partial stated while processing remote join.\",",
            "                    room_id,",
            "                )",
            "                raise LimitExceededError(msg=e.msg, errcode=e.errcode, retry_after_ms=0)",
            "            finally:",
            "                # Always kick off the background process that asynchronously fetches",
            "                # state for the room.",
            "                # If the join failed, the background process is responsible for",
            "                # cleaning up \u2014 including unmarking the room as a partial state room.",
            "                if ret.partial_state:",
            "                    # Kick off the process of asynchronously fetching the state for this",
            "                    # room.",
            "                    run_as_background_process(",
            "                        desc=\"sync_partial_state_room\",",
            "                        func=self._sync_partial_state_room,",
            "                        initial_destination=origin,",
            "                        other_destinations=ret.servers_in_room,",
            "                        room_id=room_id,",
            "                    )",
            "",
            "            # We wait here until this instance has seen the events come down",
            "            # replication (if we're using replication) as the below uses caches.",
            "            await self._replication.wait_for_stream_position(",
            "                self.config.worker.events_shard_config.get_instance(room_id),",
            "                \"events\",",
            "                max_stream_id,",
            "            )",
            "",
            "            # Check whether this room is the result of an upgrade of a room we already know",
            "            # about. If so, migrate over user information",
            "            predecessor = await self.store.get_room_predecessor(room_id)",
            "            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):",
            "                return event.event_id, max_stream_id",
            "            old_room_id = predecessor[\"room_id\"]",
            "            logger.debug(",
            "                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id",
            "            )",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.transfer_room_state_on_room_upgrade(",
            "                old_room_id, room_id",
            "            )",
            "",
            "            logger.debug(\"Finished joining %s to %s\", joinee, room_id)",
            "            return event.event_id, max_stream_id",
            "        finally:",
            "            room_queue = self._federation_event_handler.room_queues[room_id]",
            "            del self._federation_event_handler.room_queues[room_id]",
            "",
            "            # we don't need to wait for the queued events to be processed -",
            "            # it's just a best-effort thing at this point. We do want to do",
            "            # them roughly in order, though, otherwise we'll end up making",
            "            # lots of requests for missing prev_events which we do actually",
            "            # have. Hence we fire off the background task, but don't wait for it.",
            "",
            "            run_as_background_process(",
            "                \"handle_queued_pdus\", self._handle_queued_pdus, room_queue",
            "            )",
            "",
            "    async def do_knock(",
            "        self,",
            "        target_hosts: List[str],",
            "        room_id: str,",
            "        knockee: str,",
            "        content: JsonDict,",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Sends the knock to the remote server.",
            "",
            "        This first triggers a make_knock request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via send_knock.",
            "",
            "        Knock events must be signed by the knockee's server before distributing.",
            "",
            "        Args:",
            "            target_hosts: A list of hosts that we want to try knocking through.",
            "            room_id: The ID of the room to knock on.",
            "            knockee: The ID of the user who is knocking.",
            "            content: The content of the knock event.",
            "",
            "        Returns:",
            "            A tuple of (event ID, stream ID).",
            "",
            "        Raises:",
            "            SynapseError: If the chosen remote server returns a 3xx/4xx code.",
            "            RuntimeError: If no servers were reachable.",
            "        \"\"\"",
            "        logger.debug(\"Knocking on room %s on behalf of user %s\", room_id, knockee)",
            "",
            "        # Inform the remote server of the room versions we support",
            "        supported_room_versions = list(KNOWN_ROOM_VERSIONS.keys())",
            "",
            "        # Ask the remote server to create a valid knock event for us. Once received,",
            "        # we sign the event",
            "        params: Dict[str, Iterable[str]] = {\"ver\": supported_room_versions}",
            "        origin, event, event_format_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, knockee, Membership.KNOCK, content, params=params",
            "        )",
            "",
            "        # Mark the knock as an outlier as we don't yet have the state at this point in",
            "        # the DAG.",
            "        event.internal_metadata.outlier = True",
            "",
            "        # ... but tell /sync to send it to clients anyway.",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Record the room ID and its version so that we have a record of the room",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=event_format_version",
            "        )",
            "",
            "        # Initially try the host that we successfully called /make_knock on",
            "        try:",
            "            target_hosts.remove(origin)",
            "            target_hosts.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        # Send the signed event back to the room, and potentially receive some",
            "        # further information about the room in the form of partial state events",
            "        stripped_room_state = await self.federation_client.send_knock(",
            "            target_hosts, event",
            "        )",
            "",
            "        # Store any stripped room state events in the \"unsigned\" key of the event.",
            "        # This is a bit of a hack and is cribbing off of invites. Basically we",
            "        # store the room state here and retrieve it again when this event appears",
            "        # in the invitee's sync stream. It is stripped out for all other local users.",
            "        event.unsigned[\"knock_room_state\"] = stripped_room_state[\"knock_state_events\"]",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "        return event.event_id, stream_id",
            "",
            "    async def _handle_queued_pdus(",
            "        self, room_queue: List[Tuple[EventBase, str]]",
            "    ) -> None:",
            "        \"\"\"Process PDUs which got queued up while we were busy send_joining.",
            "",
            "        Args:",
            "            room_queue: list of PDUs to be processed and the servers that sent them",
            "        \"\"\"",
            "        for p, origin in room_queue:",
            "            try:",
            "                logger.info(",
            "                    \"Processing queued PDU %s which was received while we were joining\",",
            "                    p,",
            "                )",
            "                with nested_logging_context(p.event_id):",
            "                    await self._federation_event_handler.on_receive_pdu(origin, p)",
            "            except Exception as e:",
            "                logger.warning(",
            "                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e",
            "                )",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_join/ request, so we create a partial",
            "        join event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create join event in",
            "            user_id: The user to create the join for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_join request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        # checking the room version will check that we've actually heard of the room",
            "        # (and return a 404 otherwise)",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /make_join, so return a 404 as we would if we weren't in the",
            "            # room at all.",
            "            # The main reason we can't respond properly is that we need to know about",
            "            # the auth events for the join event that we would return.",
            "            # We also should not bother entertaining the /make_join since we cannot",
            "            # handle the /send_join.",
            "            logger.info(",
            "                \"Rejecting /make_join to %s because it's a partial state room\", room_id",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_join right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        # now check that we are *still* in the room",
            "        is_in_room = await self._event_auth_handler.check_host_in_room(",
            "            room_id, self.server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Got /make_join request for room %s we are no longer in\",",
            "                room_id,",
            "            )",
            "            raise NotFoundError(\"Not an active room on this server\")",
            "",
            "        event_content = {\"membership\": Membership.JOIN}",
            "",
            "        # If the current room is using restricted join rules, additional information",
            "        # may need to be included in the event content in order to efficiently",
            "        # validate the event.",
            "        #",
            "        # Note that this requires the /send_join request to come back to the",
            "        # same server.",
            "        if room_version.msc3083_join_rules:",
            "            state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                room_id",
            "            )",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                prev_member_event_id = state_ids.get((EventTypes.Member, user_id), None)",
            "                # If the user is invited or joined to the room already, then",
            "                # no additional info is needed.",
            "                include_auth_user_id = True",
            "                if prev_member_event_id:",
            "                    prev_member_event = await self.store.get_event(prev_member_event_id)",
            "                    include_auth_user_id = prev_member_event.membership not in (",
            "                        Membership.JOIN,",
            "                        Membership.INVITE,",
            "                    )",
            "",
            "                if include_auth_user_id:",
            "                    event_content[",
            "                        EventContentFields.AUTHORISING_USER",
            "                    ] = await self._event_auth_handler.get_user_which_could_invite(",
            "                        room_id,",
            "                        state_ids,",
            "                    )",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": event_content,",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        try:",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "        except SynapseError as e:",
            "            logger.warning(\"Failed to create join to %s because %s\", room_id, e)",
            "            raise",
            "",
            "        # Ensure the user can even join the room.",
            "        await self._federation_event_handler.check_join_restrictions(context, event)",
            "",
            "        # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "        # when we get the event back in `on_send_join_request`",
            "        await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        return event",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, event: EventBase, room_version: RoomVersion",
            "    ) -> EventBase:",
            "        \"\"\"We've got an invite event. Process and persist it. Sign it.",
            "",
            "        Respond with the now signed event.",
            "        \"\"\"",
            "        if event.state_key is None:",
            "            raise SynapseError(400, \"The invite event did not have a state key\")",
            "",
            "        is_blocked = await self.store.is_room_blocked(event.room_id)",
            "        if is_blocked:",
            "            raise SynapseError(403, \"This room has been blocked on this server\")",
            "",
            "        if self.hs.config.server.block_non_admin_invites:",
            "            raise SynapseError(403, \"This server does not accept room invites\")",
            "",
            "        spam_check = await self.spam_checker.user_may_invite(",
            "            event.sender, event.state_key, event.room_id",
            "        )",
            "        if spam_check != NOT_SPAM:",
            "            raise SynapseError(",
            "                403,",
            "                \"This user is not permitted to send invites to this server/user\",",
            "                errcode=spam_check[0],",
            "                additional_fields=spam_check[1],",
            "            )",
            "",
            "        membership = event.content.get(\"membership\")",
            "        if event.type != EventTypes.Member or membership != Membership.INVITE:",
            "            raise SynapseError(400, \"The event was not an m.room.member invite event\")",
            "",
            "        sender_domain = get_domain_from_id(event.sender)",
            "        if sender_domain != origin:",
            "            raise SynapseError(",
            "                400, \"The invite event was not from the server sending it\"",
            "            )",
            "",
            "        if not self.is_mine_id(event.state_key):",
            "            raise SynapseError(400, \"The invite event must be for this server\")",
            "",
            "        # block any attempts to invite the server notices mxid",
            "        if event.state_key == self._server_notices_mxid:",
            "            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        # We don't rate limit based on room ID, as that should be done by",
            "        # sending server.",
            "        await member_handler.ratelimit_invite(None, None, event.state_key)",
            "",
            "        # keep a record of the room version, if we don't yet know it.",
            "        # (this may get overwritten if we later get a different room version in a",
            "        # join dance).",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=room_version",
            "        )",
            "",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        event.signatures.update(",
            "            compute_event_signature(",
            "                room_version,",
            "                event.get_pdu_json(),",
            "                self.hs.hostname,",
            "                self.hs.signing_key,",
            "            )",
            "        )",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event",
            "",
            "    async def do_remotely_reject_invite(",
            "        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict",
            "    ) -> Tuple[EventBase, int]:",
            "        origin, event, room_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, user_id, \"leave\", content=content",
            "        )",
            "        # Mark as outlier as we don't have any state for this event; we're not",
            "        # even in the room.",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Try the host that we successfully called /make_leave/ on first for",
            "        # the /send_leave/ request.",
            "        host_list = list(target_hosts)",
            "        try:",
            "            host_list.remove(origin)",
            "            host_list.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        await self.federation_client.send_leave(host_list, event)",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event, stream_id",
            "",
            "    async def _make_and_verify_event(",
            "        self,",
            "        target_hosts: Iterable[str],",
            "        room_id: str,",
            "        user_id: str,",
            "        membership: str,",
            "        content: JsonDict,",
            "        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,",
            "    ) -> Tuple[str, EventBase, RoomVersion]:",
            "        (",
            "            origin,",
            "            event,",
            "            room_version,",
            "        ) = await self.federation_client.make_membership_event(",
            "            target_hosts, room_id, user_id, membership, content, params=params",
            "        )",
            "",
            "        logger.debug(\"Got response to make_%s: %s\", membership, event)",
            "",
            "        # We should assert some things.",
            "        # FIXME: Do this in a nicer way",
            "        assert event.type == EventTypes.Member",
            "        assert event.user_id == user_id",
            "        assert event.state_key == user_id",
            "        assert event.room_id == room_id",
            "        return origin, event, room_version",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_leave/ request, so we create a partial",
            "        leave event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create leave event in",
            "            user_id: The user to create the leave for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_leave request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.LEAVE},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_leave_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new leave %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a make_knock request, so we create a partial",
            "        knock event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Get /make_knock request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.KNOCK},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        event_allowed, _ = await self.third_party_event_rules.check_event_allowed(",
            "            event, context",
            "        )",
            "        if not event_allowed:",
            "            logger.warning(\"Creation of knock %s forbidden by third-party rules\", event)",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_knock_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new knock %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.\"\"\"",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "        if event.internal_metadata.outlier:",
            "            raise NotFoundError(\"State not known at event %s\" % (event_id,))",
            "",
            "        state_groups = await self._state_storage_controller.get_state_groups_ids(",
            "            room_id, [event_id]",
            "        )",
            "",
            "        # get_state_groups_ids should return exactly one result",
            "        assert len(state_groups) == 1",
            "",
            "        state_map = next(iter(state_groups.values()))",
            "",
            "        state_key = event.get_state_key()",
            "        if state_key is not None:",
            "            # the event was not rejected (get_event raises a NotFoundError for rejected",
            "            # events) so the state at the event should include the event itself.",
            "            assert (",
            "                state_map.get((event.type, state_key)) == event.event_id",
            "            ), \"State at event did not include event itself\"",
            "",
            "            # ... but we need the state *before* that event",
            "            if \"replaces_state\" in event.unsigned:",
            "                prev_id = event.unsigned[\"replaces_state\"]",
            "                state_map[(event.type, state_key)] = prev_id",
            "            else:",
            "                del state_map[(event.type, state_key)]",
            "",
            "        return list(state_map.values())",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, pdu_list: List[str], limit: int",
            "    ) -> List[EventBase]:",
            "        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # Synapse asks for 100 events per backfill request. Do not allow more.",
            "        limit = min(limit, 100)",
            "",
            "        events = await self.store.get_backfill_events(room_id, pdu_list, limit)",
            "        logger.debug(",
            "            \"on_backfill_request: backfill events=%s\",",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        events = await filter_events_for_server(",
            "            self._storage_controllers, origin, events",
            "        )",
            "",
            "        return events",
            "",
            "    async def get_persisted_pdu(",
            "        self, origin: str, event_id: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get an event from the database for the given server.",
            "",
            "        Args:",
            "            origin: hostname of server which is requesting the event; we",
            "               will check that the server is allowed to see it.",
            "            event_id: id of the event being requested",
            "",
            "        Returns:",
            "            None if we know nothing about the event; otherwise the (possibly-redacted) event.",
            "",
            "        Raises:",
            "            AuthError if the server is not currently in the room",
            "        \"\"\"",
            "        event = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        if event:",
            "            in_room = await self._event_auth_handler.check_host_in_room(",
            "                event.room_id, origin",
            "            )",
            "            if not in_room:",
            "                raise AuthError(403, \"Host not in room.\")",
            "",
            "            events = await filter_events_for_server(",
            "                self._storage_controllers, origin, [event]",
            "            )",
            "            event = events[0]",
            "            return event",
            "        else:",
            "            return None",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> List[EventBase]:",
            "        in_room = await self._event_auth_handler.check_host_in_room(room_id, origin)",
            "        if not in_room:",
            "            raise AuthError(403, \"Host not in room.\")",
            "",
            "        # Only allow up to 20 events to be retrieved per request.",
            "        limit = min(limit, 20)",
            "",
            "        missing_events = await self.store.get_missing_events(",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        missing_events = await filter_events_for_server(",
            "            self._storage_controllers, origin, missing_events",
            "        )",
            "",
            "        return missing_events",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: JsonDict",
            "    ) -> None:",
            "        third_party_invite = {\"signed\": signed}",
            "",
            "        event_dict = {",
            "            \"type\": EventTypes.Member,",
            "            \"content\": {",
            "                \"membership\": Membership.INVITE,",
            "                \"third_party_invite\": third_party_invite,",
            "            },",
            "            \"room_id\": room_id,",
            "            \"sender\": sender_user_id,",
            "            \"state_key\": target_user_id,",
            "        }",
            "",
            "        if await self._event_auth_handler.check_host_in_room(room_id, self.hs.hostname):",
            "            room_version_obj = await self.store.get_room_version(room_id)",
            "            builder = self.event_builder_factory.for_room_version(",
            "                room_version_obj, event_dict",
            "            )",
            "",
            "            EventValidator().validate_builder(builder)",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "",
            "            event, context = await self.add_display_name_to_third_party_invite(",
            "                room_version_obj, event_dict, event, context",
            "            )",
            "",
            "            EventValidator().validate_new(event, self.config)",
            "",
            "            # We need to tell the transaction queue to send this out, even",
            "            # though the sender isn't a local user.",
            "            event.internal_metadata.send_on_behalf_of = self.hs.hostname",
            "",
            "            try:",
            "                validate_event_for_room_version(event)",
            "                await self._event_auth_handler.check_auth_rules_from_context(",
            "                    event, context",
            "                )",
            "            except AuthError as e:",
            "                logger.warning(\"Denying new third party invite %r because %s\", event, e)",
            "                raise e",
            "",
            "            await self._check_signature(event, context)",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.send_membership_event(None, event, context)",
            "        else:",
            "            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}",
            "",
            "            try:",
            "                await self.federation_client.forward_third_party_invite(",
            "                    destinations, room_id, event_dict",
            "                )",
            "            except (RequestSendFailed, HttpResponseException):",
            "                raise SynapseError(502, \"Failed to forward third party invite\")",
            "",
            "    async def on_exchange_third_party_invite_request(",
            "        self, event_dict: JsonDict",
            "    ) -> None:",
            "        \"\"\"Handle an exchange_third_party_invite request from a remote server",
            "",
            "        The remote server will call this when it wants to turn a 3pid invite",
            "        into a normal m.room.member invite.",
            "",
            "        Args:",
            "            event_dict: Dictionary containing the event body.",
            "",
            "        \"\"\"",
            "        assert_params_in_dict(event_dict, [\"room_id\"])",
            "        room_version_obj = await self.store.get_room_version(event_dict[\"room_id\"])",
            "",
            "        # NB: event_dict has a particular specced format we might need to fudge",
            "        # if we change event formats too much.",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        event, context = await self.add_display_name_to_third_party_invite(",
            "            room_version_obj, event_dict, event, context",
            "        )",
            "",
            "        try:",
            "            validate_event_for_room_version(event)",
            "            await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Denying third party invite %r because %s\", event, e)",
            "            raise e",
            "        await self._check_signature(event, context)",
            "",
            "        # We need to tell the transaction queue to send this out, even",
            "        # though the sender isn't a local user.",
            "        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        await member_handler.send_membership_event(None, event, context)",
            "",
            "    async def add_display_name_to_third_party_invite(",
            "        self,",
            "        room_version_obj: RoomVersion,",
            "        event_dict: JsonDict,",
            "        event: EventBase,",
            "        context: EventContext,",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        key = (",
            "            EventTypes.ThirdPartyInvite,",
            "            event.content[\"third_party_invite\"][\"signed\"][\"token\"],",
            "        )",
            "        original_invite = None",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(EventTypes.ThirdPartyInvite, None)])",
            "        )",
            "        original_invite_id = prev_state_ids.get(key)",
            "        if original_invite_id:",
            "            original_invite = await self.store.get_event(",
            "                original_invite_id, allow_none=True",
            "            )",
            "        if original_invite:",
            "            # If the m.room.third_party_invite event's content is empty, it means the",
            "            # invite has been revoked. In this case, we don't have to raise an error here",
            "            # because the auth check will fail on the invite (because it's not able to",
            "            # fetch public keys from the m.room.third_party_invite event's content, which",
            "            # is empty).",
            "            display_name = original_invite.content.get(\"display_name\")",
            "            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name",
            "        else:",
            "            logger.info(",
            "                \"Could not find invite event for third_party_invite: %r\", event_dict",
            "            )",
            "            # We don't discard here as this is not the appropriate place to do",
            "            # auth checks. If we need the invite and don't have it then the",
            "            # auth check code will explode appropriately.",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "        EventValidator().validate_builder(builder)",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        EventValidator().validate_new(event, self.config)",
            "        return event, context",
            "",
            "    async def _check_signature(self, event: EventBase, context: EventContext) -> None:",
            "        \"\"\"",
            "        Checks that the signature in the event is consistent with its invite.",
            "",
            "        Args:",
            "            event: The m.room.member event to check",
            "            context:",
            "",
            "        Raises:",
            "            AuthError: if signature didn't match any keys, or key has been",
            "                revoked,",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        signed = event.content[\"third_party_invite\"][\"signed\"]",
            "        token = signed[\"token\"]",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(EventTypes.ThirdPartyInvite, None)])",
            "        )",
            "        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))",
            "",
            "        invite_event = None",
            "        if invite_event_id:",
            "            invite_event = await self.store.get_event(invite_event_id, allow_none=True)",
            "",
            "        if not invite_event:",
            "            raise AuthError(403, \"Could not find invite\")",
            "",
            "        logger.debug(\"Checking auth on event %r\", event.content)",
            "",
            "        last_exception: Optional[Exception] = None",
            "",
            "        # for each public key in the 3pid invite event",
            "        for public_key_object in event_auth.get_public_keys(invite_event):",
            "            try:",
            "                # for each sig on the third_party_invite block of the actual invite",
            "                for server, signature_block in signed[\"signatures\"].items():",
            "                    for key_name in signature_block.keys():",
            "                        if not key_name.startswith(\"ed25519:\"):",
            "                            continue",
            "",
            "                        logger.debug(",
            "                            \"Attempting to verify sig with key %s from %r \"",
            "                            \"against pubkey %r\",",
            "                            key_name,",
            "                            server,",
            "                            public_key_object,",
            "                        )",
            "",
            "                        try:",
            "                            public_key = public_key_object[\"public_key\"]",
            "                            verify_key = decode_verify_key_bytes(",
            "                                key_name, decode_base64(public_key)",
            "                            )",
            "                            verify_signed_json(signed, server, verify_key)",
            "                            logger.debug(",
            "                                \"Successfully verified sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to verify sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                            raise",
            "                        try:",
            "                            if \"key_validity_url\" in public_key_object:",
            "                                await self._check_key_revocation(",
            "                                    public_key, public_key_object[\"key_validity_url\"]",
            "                                )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to query key_validity_url %s\",",
            "                                public_key_object[\"key_validity_url\"],",
            "                            )",
            "                            raise",
            "                        return",
            "            except Exception as e:",
            "                last_exception = e",
            "",
            "        if last_exception is None:",
            "            # we can only get here if get_public_keys() returned an empty list",
            "            # TODO: make this better",
            "            raise RuntimeError(\"no public key in invite event\")",
            "",
            "        raise last_exception",
            "",
            "    async def _check_key_revocation(self, public_key: str, url: str) -> None:",
            "        \"\"\"",
            "        Checks whether public_key has been revoked.",
            "",
            "        Args:",
            "            public_key: base-64 encoded public key.",
            "            url: Key revocation URL.",
            "",
            "        Raises:",
            "            AuthError: if they key has been revoked.",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        try:",
            "            response = await self.http_client.get_json(url, {\"public_key\": public_key})",
            "        except Exception:",
            "            raise SynapseError(502, \"Third party certificate could not be checked\")",
            "        if \"valid\" not in response or not response[\"valid\"]:",
            "            raise AuthError(403, \"Third party certificate was invalid\")",
            "",
            "    async def _clean_room_for_join(self, room_id: str) -> None:",
            "        \"\"\"Called to clean up any data in DB for a given room, ready for the",
            "        server to join the room.",
            "",
            "        Args:",
            "            room_id",
            "        \"\"\"",
            "        if self.config.worker.worker_app:",
            "            await self._clean_room_for_join_client(room_id)",
            "        else:",
            "            await self.store.clean_room_for_join(room_id)",
            "",
            "    async def get_room_complexity(",
            "        self, remote_room_hosts: List[str], room_id: str",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Fetch the complexity of a remote room over federation.",
            "",
            "        Args:",
            "            remote_room_hosts (list[str]): The remote servers to ask.",
            "            room_id (str): The room ID to ask about.",
            "",
            "        Returns:",
            "            Dict contains the complexity",
            "            metric versions, while None means we could not fetch the complexity.",
            "        \"\"\"",
            "",
            "        for host in remote_room_hosts:",
            "            res = await self.federation_client.get_room_complexity(host, room_id)",
            "",
            "            # We got a result, return it.",
            "            if res:",
            "                return res",
            "",
            "        # We fell off the bottom, couldn't get the complexity from anyone. Oh",
            "        # well.",
            "        return None",
            "",
            "    async def _resume_sync_partial_state_room(self) -> None:",
            "        \"\"\"Resumes resyncing of all partial-state rooms after a restart.\"\"\"",
            "        assert not self.config.worker.worker_app",
            "",
            "        partial_state_rooms = await self.store.get_partial_state_rooms_and_servers()",
            "        for room_id, servers_in_room in partial_state_rooms.items():",
            "            run_as_background_process(",
            "                desc=\"sync_partial_state_room\",",
            "                func=self._sync_partial_state_room,",
            "                initial_destination=None,",
            "                other_destinations=servers_in_room,",
            "                room_id=room_id,",
            "            )",
            "",
            "    async def _sync_partial_state_room(",
            "        self,",
            "        initial_destination: Optional[str],",
            "        other_destinations: Collection[str],",
            "        room_id: str,",
            "    ) -> None:",
            "        \"\"\"Background process to resync the state of a partial-state room",
            "",
            "        Args:",
            "            initial_destination: the initial homeserver to pull the state from",
            "            other_destinations: other homeservers to try to pull the state from, if",
            "                `initial_destination` is unavailable",
            "            room_id: room to be resynced",
            "        \"\"\"",
            "        assert not self.config.worker.worker_app",
            "",
            "        # TODO(faster_joins): do we need to lock to avoid races? What happens if other",
            "        #   worker processes kick off a resync in parallel? Perhaps we should just elect",
            "        #   a single worker to do the resync.",
            "        #   https://github.com/matrix-org/synapse/issues/12994",
            "        #",
            "        # TODO(faster_joins): what happens if we leave the room during a resync? if we",
            "        #   really leave, that might mean we have difficulty getting the room state over",
            "        #   federation.",
            "        #   https://github.com/matrix-org/synapse/issues/12802",
            "        #",
            "        # TODO(faster_joins): we need some way of prioritising which homeservers in",
            "        #   `other_destinations` to try first, otherwise we'll spend ages trying dead",
            "        #   homeservers for large rooms.",
            "        #   https://github.com/matrix-org/synapse/issues/12999",
            "",
            "        if initial_destination is None and len(other_destinations) == 0:",
            "            raise ValueError(",
            "                f\"Cannot resync state of {room_id}: no destinations provided\"",
            "            )",
            "",
            "        # Make an infinite iterator of destinations to try. Once we find a working",
            "        # destination, we'll stick with it until it flakes.",
            "        destinations: Collection[str]",
            "        if initial_destination is not None:",
            "            # Move `initial_destination` to the front of the list.",
            "            destinations = list(other_destinations)",
            "            if initial_destination in destinations:",
            "                destinations.remove(initial_destination)",
            "            destinations = [initial_destination] + destinations",
            "        else:",
            "            destinations = other_destinations",
            "        destination_iter = itertools.cycle(destinations)",
            "",
            "        # `destination` is the current remote homeserver we're pulling from.",
            "        destination = next(destination_iter)",
            "        logger.info(\"Syncing state for room %s via %s\", room_id, destination)",
            "",
            "        # we work through the queue in order of increasing stream ordering.",
            "        while True:",
            "            batch = await self.store.get_partial_state_events_batch(room_id)",
            "            if not batch:",
            "                # all the events are updated, so we can update current state and",
            "                # clear the lazy-loading flag.",
            "                logger.info(\"Updating current state for %s\", room_id)",
            "                # TODO(faster_joins): notify workers in notify_room_un_partial_stated",
            "                #   https://github.com/matrix-org/synapse/issues/12994",
            "                await self.state_handler.update_current_state(room_id)",
            "",
            "                logger.info(\"Clearing partial-state flag for %s\", room_id)",
            "                success = await self.store.clear_partial_state_room(room_id)",
            "                if success:",
            "                    logger.info(\"State resync complete for %s\", room_id)",
            "                    self._storage_controllers.state.notify_room_un_partial_stated(",
            "                        room_id",
            "                    )",
            "",
            "                    # TODO(faster_joins) update room stats and user directory?",
            "                    #   https://github.com/matrix-org/synapse/issues/12814",
            "                    #   https://github.com/matrix-org/synapse/issues/12815",
            "                    return",
            "",
            "                # we raced against more events arriving with partial state. Go round",
            "                # the loop again. We've already logged a warning, so no need for more.",
            "                continue",
            "",
            "            events = await self.store.get_events_as_list(",
            "                batch,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                allow_rejected=True,",
            "            )",
            "            for event in events:",
            "                for attempt in itertools.count():",
            "                    try:",
            "                        await self._federation_event_handler.update_state_for_partial_state_event(",
            "                            destination, event",
            "                        )",
            "                        break",
            "                    except FederationError as e:",
            "                        if attempt == len(destinations) - 1:",
            "                            # We have tried every remote server for this event. Give up.",
            "                            # TODO(faster_joins) giving up isn't the right thing to do",
            "                            #   if there's a temporary network outage. retrying",
            "                            #   indefinitely is also not the right thing to do if we can",
            "                            #   reach all homeservers and they all claim they don't have",
            "                            #   the state we want.",
            "                            #   https://github.com/matrix-org/synapse/issues/13000",
            "                            logger.error(",
            "                                \"Failed to get state for %s at %s from %s because %s, \"",
            "                                \"giving up!\",",
            "                                room_id,",
            "                                event,",
            "                                destination,",
            "                                e,",
            "                            )",
            "                            raise",
            "",
            "                        # Try the next remote server.",
            "                        logger.info(",
            "                            \"Failed to get state for %s at %s from %s because %s\",",
            "                            room_id,",
            "                            event,",
            "                            destination,",
            "                            e,",
            "                        )",
            "                        destination = next(destination_iter)",
            "                        logger.info(",
            "                            \"Syncing state for room %s via %s instead\",",
            "                            room_id,",
            "                            destination,",
            "                        )"
        ],
        "afterPatchFile": [
            "# Copyright 2014-2022 The Matrix.org Foundation C.I.C.",
            "# Copyright 2020 Sorunome",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "\"\"\"Contains handlers for federation events.\"\"\"",
            "",
            "import enum",
            "import itertools",
            "import logging",
            "from enum import Enum",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Collection,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "import attr",
            "from prometheus_client import Histogram",
            "from signedjson.key import decode_verify_key_bytes",
            "from signedjson.sign import verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import EventContentFields, EventTypes, Membership",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    CodeMessageException,",
            "    Codes,",
            "    FederationDeniedError,",
            "    FederationError,",
            "    HttpResponseException,",
            "    LimitExceededError,",
            "    NotFoundError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.events.validator import EventValidator",
            "from synapse.federation.federation_client import InvalidResponseError",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import SynapseTags, set_tag, tag_args, trace",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.module_api import NOT_SPAM",
            "from synapse.replication.http.federation import (",
            "    ReplicationCleanRoomRestServlet,",
            "    ReplicationStoreRoomOnOutlierMembershipRestServlet,",
            ")",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.storage.state import StateFilter",
            "from synapse.types import JsonDict, get_domain_from_id",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.visibility import filter_events_for_server",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_before_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_before_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        30.0,",
            "        40.0,",
            "        60.0,",
            "        80.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "class _BackfillPointType(Enum):",
            "    # a regular backwards extremity (ie, an event which we don't yet have, but which",
            "    # is referred to by other events in the DAG)",
            "    BACKWARDS_EXTREMITY = enum.auto()",
            "",
            "    # an MSC2716 \"insertion event\"",
            "    INSERTION_PONT = enum.auto()",
            "",
            "",
            "@attr.s(slots=True, auto_attribs=True, frozen=True)",
            "class _BackfillPoint:",
            "    \"\"\"A potential point we might backfill from\"\"\"",
            "",
            "    event_id: str",
            "    depth: int",
            "    type: _BackfillPointType",
            "",
            "",
            "class FederationHandler:",
            "    \"\"\"Handles general incoming federation requests",
            "",
            "    Incoming events are *not* handled here, for which see FederationEventHandler.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "",
            "        self.clock = hs.get_clock()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self.federation_client = hs.get_federation_client()",
            "        self.state_handler = hs.get_state_handler()",
            "        self.server_name = hs.hostname",
            "        self.keyring = hs.get_keyring()",
            "        self.is_mine_id = hs.is_mine_id",
            "        self.spam_checker = hs.get_spam_checker()",
            "        self.event_creation_handler = hs.get_event_creation_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._server_notices_mxid = hs.config.servernotices.server_notices_mxid",
            "        self.config = hs.config",
            "        self.http_client = hs.get_proxied_blacklisted_http_client()",
            "        self._replication = hs.get_replication_data_handler()",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "",
            "        self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(",
            "            hs",
            "        )",
            "",
            "        if hs.config.worker.worker_app:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                ReplicationStoreRoomOnOutlierMembershipRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._maybe_store_room_on_outlier_membership = (",
            "                self.store.maybe_store_room_on_outlier_membership",
            "            )",
            "",
            "        self._room_backfill = Linearizer(\"room_backfill\")",
            "",
            "        self.third_party_event_rules = hs.get_third_party_event_rules()",
            "",
            "        # if this is the main process, fire off a background process to resume",
            "        # any partial-state-resync operations which were in flight when we",
            "        # were shut down.",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"resume_sync_partial_state_room\", self._resume_sync_partial_state_room",
            "            )",
            "",
            "    @trace",
            "    async def maybe_backfill(",
            "        self, room_id: str, current_depth: int, limit: int",
            "    ) -> bool:",
            "        \"\"\"Checks the database to see if we should backfill before paginating,",
            "        and if so do.",
            "",
            "        Args:",
            "            room_id",
            "            current_depth: The depth from which we're paginating from. This is",
            "                used to decide if we should backfill and what extremities to",
            "                use.",
            "            limit: The number of events that the pagination request will",
            "                return. This is used as part of the heuristic to decide if we",
            "                should back paginate.",
            "        \"\"\"",
            "        # Starting the processing time here so we can include the room backfill",
            "        # linearizer lock queue in the timing",
            "        processing_start_time = self.clock.time_msec()",
            "",
            "        async with self._room_backfill.queue(room_id):",
            "            return await self._maybe_backfill_inner(",
            "                room_id,",
            "                current_depth,",
            "                limit,",
            "                processing_start_time=processing_start_time,",
            "            )",
            "",
            "    async def _maybe_backfill_inner(",
            "        self,",
            "        room_id: str,",
            "        current_depth: int,",
            "        limit: int,",
            "        *,",
            "        processing_start_time: int,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Checks whether the `current_depth` is at or approaching any backfill",
            "        points in the room and if so, will backfill. We only care about",
            "        checking backfill points that happened before the `current_depth`",
            "        (meaning less than or equal to the `current_depth`).",
            "",
            "        Args:",
            "            room_id: The room to backfill in.",
            "            current_depth: The depth to check at for any upcoming backfill points.",
            "            limit: The max number of events to request from the remote federated server.",
            "            processing_start_time: The time when `maybe_backfill` started",
            "                processing. Only used for timing.",
            "        \"\"\"",
            "        backwards_extremities = [",
            "            _BackfillPoint(event_id, depth, _BackfillPointType.BACKWARDS_EXTREMITY)",
            "            for event_id, depth in await self.store.get_oldest_event_ids_with_depth_in_room(",
            "                room_id",
            "            )",
            "        ]",
            "",
            "        insertion_events_to_be_backfilled: List[_BackfillPoint] = []",
            "        if self.hs.config.experimental.msc2716_enabled:",
            "            insertion_events_to_be_backfilled = [",
            "                _BackfillPoint(event_id, depth, _BackfillPointType.INSERTION_PONT)",
            "                for event_id, depth in await self.store.get_insertion_event_backward_extremities_in_room(",
            "                    room_id",
            "                )",
            "            ]",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: backwards_extremities=%s insertion_events_to_be_backfilled=%s\",",
            "            backwards_extremities,",
            "            insertion_events_to_be_backfilled,",
            "        )",
            "",
            "        if not backwards_extremities and not insertion_events_to_be_backfilled:",
            "            logger.debug(\"Not backfilling as no extremeties found.\")",
            "            return False",
            "",
            "        # we now have a list of potential places to backpaginate from. We prefer to",
            "        # start with the most recent (ie, max depth), so let's sort the list.",
            "        sorted_backfill_points: List[_BackfillPoint] = sorted(",
            "            itertools.chain(",
            "                backwards_extremities,",
            "                insertion_events_to_be_backfilled,",
            "            ),",
            "            key=lambda e: -int(e.depth),",
            "        )",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: room_id: %s: current_depth: %s, limit: %s, \"",
            "            \"backfill points (%d): %s\",",
            "            room_id,",
            "            current_depth,",
            "            limit,",
            "            len(sorted_backfill_points),",
            "            sorted_backfill_points,",
            "        )",
            "",
            "        # If we're approaching an extremity we trigger a backfill, otherwise we",
            "        # no-op.",
            "        #",
            "        # We chose twice the limit here as then clients paginating backwards",
            "        # will send pagination requests that trigger backfill at least twice",
            "        # using the most recent extremity before it gets removed (see below). We",
            "        # chose more than one times the limit in case of failure, but choosing a",
            "        # much larger factor will result in triggering a backfill request much",
            "        # earlier than necessary.",
            "        #",
            "        # XXX: shouldn't we do this *after* the filter by depth below? Again, we don't",
            "        # care about events that have happened after our current position.",
            "        #",
            "        max_depth = sorted_backfill_points[0].depth",
            "        if current_depth - 2 * limit > max_depth:",
            "            logger.debug(",
            "                \"Not backfilling as we don't need to. %d < %d - 2 * %d\",",
            "                max_depth,",
            "                current_depth,",
            "                limit,",
            "            )",
            "            return False",
            "",
            "        # We ignore extremities that have a greater depth than our current depth",
            "        # as:",
            "        #    1. we don't really care about getting events that have happened",
            "        #       after our current position; and",
            "        #    2. we have likely previously tried and failed to backfill from that",
            "        #       extremity, so to avoid getting \"stuck\" requesting the same",
            "        #       backfill repeatedly we drop those extremities.",
            "        #",
            "        # However, we need to check that the filtered extremities are non-empty.",
            "        # If they are empty then either we can a) bail or b) still attempt to",
            "        # backfill. We opt to try backfilling anyway just in case we do get",
            "        # relevant events.",
            "        #",
            "        filtered_sorted_backfill_points = [",
            "            t for t in sorted_backfill_points if t.depth <= current_depth",
            "        ]",
            "        if filtered_sorted_backfill_points:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: backfill points before current depth: %s\",",
            "                filtered_sorted_backfill_points,",
            "            )",
            "            sorted_backfill_points = filtered_sorted_backfill_points",
            "        else:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: all backfill points are *after* current depth. Backfilling anyway.\"",
            "            )",
            "",
            "        # For performance's sake, we only want to paginate from a particular extremity",
            "        # if we can actually see the events we'll get. Otherwise, we'd just spend a lot",
            "        # of resources to get redacted events. We check each extremity in turn and",
            "        # ignore those which users on our server wouldn't be able to see.",
            "        #",
            "        # Additionally, we limit ourselves to backfilling from at most 5 extremities,",
            "        # for two reasons:",
            "        #",
            "        # - The check which determines if we can see an extremity's events can be",
            "        #   expensive (we load the full state for the room at each of the backfill",
            "        #   points, or (worse) their successors)",
            "        # - We want to avoid the server-server API request URI becoming too long.",
            "        #",
            "        # *Note*: the spec wants us to keep backfilling until we reach the start",
            "        # of the room in case we are allowed to see some of the history. However,",
            "        # in practice that causes more issues than its worth, as (a) it's",
            "        # relatively rare for there to be any visible history and (b) even when",
            "        # there is it's often sufficiently long ago that clients would stop",
            "        # attempting to paginate before backfill reached the visible history.",
            "",
            "        extremities_to_request: List[str] = []",
            "        for bp in sorted_backfill_points:",
            "            if len(extremities_to_request) >= 5:",
            "                break",
            "",
            "            # For regular backwards extremities, we don't have the extremity events",
            "            # themselves, so we need to actually check the events that reference them -",
            "            # their \"successor\" events.",
            "            #",
            "            # TODO: Correctly handle the case where we are allowed to see the",
            "            #   successor event but not the backward extremity, e.g. in the case of",
            "            #   initial join of the server where we are allowed to see the join",
            "            #   event but not anything before it. This would require looking at the",
            "            #   state *before* the event, ignoring the special casing certain event",
            "            #   types have.",
            "            if bp.type == _BackfillPointType.INSERTION_PONT:",
            "                event_ids_to_check = [bp.event_id]",
            "            else:",
            "                event_ids_to_check = await self.store.get_successor_events(bp.event_id)",
            "",
            "            events_to_check = await self.store.get_events_as_list(",
            "                event_ids_to_check,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                get_prev_content=False,",
            "            )",
            "",
            "            # We set `check_history_visibility_only` as we might otherwise get false",
            "            # positives from users having been erased.",
            "            filtered_extremities = await filter_events_for_server(",
            "                self._storage_controllers,",
            "                self.server_name,",
            "                events_to_check,",
            "                redact=False,",
            "                check_history_visibility_only=True,",
            "            )",
            "            if filtered_extremities:",
            "                extremities_to_request.append(bp.event_id)",
            "            else:",
            "                logger.debug(",
            "                    \"_maybe_backfill_inner: skipping extremity %s as it would not be visible\",",
            "                    bp,",
            "                )",
            "",
            "        if not extremities_to_request:",
            "            logger.debug(",
            "                \"_maybe_backfill_inner: found no extremities which would be visible\"",
            "            )",
            "            return False",
            "",
            "        logger.debug(",
            "            \"_maybe_backfill_inner: extremities_to_request %s\", extremities_to_request",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request\",",
            "            str(extremities_to_request),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"extremities_to_request.length\",",
            "            str(len(extremities_to_request)),",
            "        )",
            "",
            "        # Now we need to decide which hosts to hit first.",
            "        # First we try hosts that are already in the room.",
            "        # TODO: HEURISTIC ALERT.",
            "        likely_domains = (",
            "            await self._storage_controllers.state.get_current_hosts_in_room(room_id)",
            "        )",
            "",
            "        async def try_backfill(domains: Collection[str]) -> bool:",
            "            # TODO: Should we try multiple of these at a time?",
            "            for dom in domains:",
            "                # We don't want to ask our own server for information we don't have",
            "                if dom == self.server_name:",
            "                    continue",
            "",
            "                try:",
            "                    await self._federation_event_handler.backfill(",
            "                        dom, room_id, limit=100, extremities=extremities_to_request",
            "                    )",
            "                    # If this succeeded then we probably already have the",
            "                    # appropriate stuff.",
            "                    # TODO: We can probably do something more intelligent here.",
            "                    return True",
            "                except (SynapseError, InvalidResponseError) as e:",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except HttpResponseException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise e.to_synapse_error()",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except CodeMessageException as e:",
            "                    if 400 <= e.code < 500:",
            "                        raise",
            "",
            "                    logger.info(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except NotRetryingDestination as e:",
            "                    logger.info(str(e))",
            "                    continue",
            "                except RequestSendFailed as e:",
            "                    logger.info(\"Failed to get backfill from %s because %s\", dom, e)",
            "                    continue",
            "                except FederationDeniedError as e:",
            "                    logger.info(e)",
            "                    continue",
            "                except Exception as e:",
            "                    logger.exception(\"Failed to backfill from %s because %s\", dom, e)",
            "                    continue",
            "",
            "            return False",
            "",
            "        processing_end_time = self.clock.time_msec()",
            "        backfill_processing_before_timer.observe(",
            "            (processing_end_time - processing_start_time) / 1000",
            "        )",
            "",
            "        success = await try_backfill(likely_domains)",
            "        if success:",
            "            return True",
            "",
            "        # TODO: we could also try servers which were previously in the room, but",
            "        #   are no longer.",
            "",
            "        return False",
            "",
            "    async def send_invite(self, target_host: str, event: EventBase) -> EventBase:",
            "        \"\"\"Sends the invite to the remote server for signing.",
            "",
            "        Invites must be signed by the invitee's server before distribution.",
            "        \"\"\"",
            "        try:",
            "            pdu = await self.federation_client.send_invite(",
            "                destination=target_host,",
            "                room_id=event.room_id,",
            "                event_id=event.event_id,",
            "                pdu=event,",
            "            )",
            "        except RequestSendFailed:",
            "            raise SynapseError(502, f\"Can't connect to server {target_host}\")",
            "",
            "        return pdu",
            "",
            "    async def on_event_auth(self, event_id: str) -> List[EventBase]:",
            "        event = await self.store.get_event(event_id)",
            "        auth = await self.store.get_auth_chain(",
            "            event.room_id, list(event.auth_event_ids()), include_given=True",
            "        )",
            "        return list(auth)",
            "",
            "    async def do_invite_join(",
            "        self, target_hosts: Iterable[str], room_id: str, joinee: str, content: JsonDict",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Attempts to join the `joinee` to the room `room_id` via the",
            "        servers contained in `target_hosts`.",
            "",
            "        This first triggers a /make_join/ request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via /send_join/ which responds with the state at that",
            "        event and the auth_chains.",
            "",
            "        We suspend processing of any received events from this room until we",
            "        have finished processing the join.",
            "",
            "        Args:",
            "            target_hosts: List of servers to attempt to join the room with.",
            "",
            "            room_id: The ID of the room to join.",
            "",
            "            joinee: The User ID of the joining user.",
            "",
            "            content: The event content to use for the join event.",
            "        \"\"\"",
            "        # TODO: We should be able to call this on workers, but the upgrading of",
            "        # room stuff after join currently doesn't work on workers.",
            "        # TODO: Before we relax this condition, we need to allow re-syncing of",
            "        # partial room state to happen on workers.",
            "        assert self.config.worker.worker_app is None",
            "",
            "        logger.debug(\"Joining %s to %s\", joinee, room_id)",
            "",
            "        origin, event, room_version_obj = await self._make_and_verify_event(",
            "            target_hosts,",
            "            room_id,",
            "            joinee,",
            "            \"join\",",
            "            content,",
            "            params={\"ver\": KNOWN_ROOM_VERSIONS},",
            "        )",
            "",
            "        # This shouldn't happen, because the RoomMemberHandler has a",
            "        # linearizer lock which only allows one operation per user per room",
            "        # at a time - so this is just paranoia.",
            "        assert room_id not in self._federation_event_handler.room_queues",
            "",
            "        self._federation_event_handler.room_queues[room_id] = []",
            "",
            "        await self._clean_room_for_join(room_id)",
            "",
            "        try:",
            "            # Try the host we successfully got a response to /make_join/",
            "            # request first.",
            "            host_list = list(target_hosts)",
            "            try:",
            "                host_list.remove(origin)",
            "                host_list.insert(0, origin)",
            "            except ValueError:",
            "                pass",
            "",
            "            ret = await self.federation_client.send_join(",
            "                host_list, event, room_version_obj",
            "            )",
            "",
            "            event = ret.event",
            "            origin = ret.origin",
            "            state = ret.state",
            "            auth_chain = ret.auth_chain",
            "            auth_chain.sort(key=lambda e: e.depth)",
            "",
            "            logger.debug(\"do_invite_join auth_chain: %s\", auth_chain)",
            "            logger.debug(\"do_invite_join state: %s\", state)",
            "",
            "            logger.debug(\"do_invite_join event: %s\", event)",
            "",
            "            # if this is the first time we've joined this room, it's time to add",
            "            # a row to `rooms` with the correct room version. If there's already a",
            "            # row there, we should override it, since it may have been populated",
            "            # based on an invite request which lied about the room version.",
            "            #",
            "            # federation_client.send_join has already checked that the room",
            "            # version in the received create event is the same as room_version_obj,",
            "            # so we can rely on it now.",
            "            #",
            "            await self.store.upsert_room_on_join(",
            "                room_id=room_id,",
            "                room_version=room_version_obj,",
            "                state_events=state,",
            "            )",
            "",
            "            if ret.partial_state:",
            "                # Mark the room as having partial state.",
            "                # The background process is responsible for unmarking this flag,",
            "                # even if the join fails.",
            "                await self.store.store_partial_state_room(room_id, ret.servers_in_room)",
            "",
            "            try:",
            "                max_stream_id = (",
            "                    await self._federation_event_handler.process_remote_join(",
            "                        origin,",
            "                        room_id,",
            "                        auth_chain,",
            "                        state,",
            "                        event,",
            "                        room_version_obj,",
            "                        partial_state=ret.partial_state,",
            "                    )",
            "                )",
            "            except PartialStateConflictError as e:",
            "                # The homeserver was already in the room and it is no longer partial",
            "                # stated. We ought to be doing a local join instead. Turn the error into",
            "                # a 429, as a hint to the client to try again.",
            "                # TODO(faster_joins): `_should_perform_remote_join` suggests that we may",
            "                #   do a remote join for restricted rooms even if we have full state.",
            "                logger.error(",
            "                    \"Room %s was un-partial stated while processing remote join.\",",
            "                    room_id,",
            "                )",
            "                raise LimitExceededError(msg=e.msg, errcode=e.errcode, retry_after_ms=0)",
            "            finally:",
            "                # Always kick off the background process that asynchronously fetches",
            "                # state for the room.",
            "                # If the join failed, the background process is responsible for",
            "                # cleaning up \u2014 including unmarking the room as a partial state room.",
            "                if ret.partial_state:",
            "                    # Kick off the process of asynchronously fetching the state for this",
            "                    # room.",
            "                    run_as_background_process(",
            "                        desc=\"sync_partial_state_room\",",
            "                        func=self._sync_partial_state_room,",
            "                        initial_destination=origin,",
            "                        other_destinations=ret.servers_in_room,",
            "                        room_id=room_id,",
            "                    )",
            "",
            "            # We wait here until this instance has seen the events come down",
            "            # replication (if we're using replication) as the below uses caches.",
            "            await self._replication.wait_for_stream_position(",
            "                self.config.worker.events_shard_config.get_instance(room_id),",
            "                \"events\",",
            "                max_stream_id,",
            "            )",
            "",
            "            # Check whether this room is the result of an upgrade of a room we already know",
            "            # about. If so, migrate over user information",
            "            predecessor = await self.store.get_room_predecessor(room_id)",
            "            if not predecessor or not isinstance(predecessor.get(\"room_id\"), str):",
            "                return event.event_id, max_stream_id",
            "            old_room_id = predecessor[\"room_id\"]",
            "            logger.debug(",
            "                \"Found predecessor for %s during remote join: %s\", room_id, old_room_id",
            "            )",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.transfer_room_state_on_room_upgrade(",
            "                old_room_id, room_id",
            "            )",
            "",
            "            logger.debug(\"Finished joining %s to %s\", joinee, room_id)",
            "            return event.event_id, max_stream_id",
            "        finally:",
            "            room_queue = self._federation_event_handler.room_queues[room_id]",
            "            del self._federation_event_handler.room_queues[room_id]",
            "",
            "            # we don't need to wait for the queued events to be processed -",
            "            # it's just a best-effort thing at this point. We do want to do",
            "            # them roughly in order, though, otherwise we'll end up making",
            "            # lots of requests for missing prev_events which we do actually",
            "            # have. Hence we fire off the background task, but don't wait for it.",
            "",
            "            run_as_background_process(",
            "                \"handle_queued_pdus\", self._handle_queued_pdus, room_queue",
            "            )",
            "",
            "    async def do_knock(",
            "        self,",
            "        target_hosts: List[str],",
            "        room_id: str,",
            "        knockee: str,",
            "        content: JsonDict,",
            "    ) -> Tuple[str, int]:",
            "        \"\"\"Sends the knock to the remote server.",
            "",
            "        This first triggers a make_knock request that returns a partial",
            "        event that we can fill out and sign. This is then sent to the",
            "        remote server via send_knock.",
            "",
            "        Knock events must be signed by the knockee's server before distributing.",
            "",
            "        Args:",
            "            target_hosts: A list of hosts that we want to try knocking through.",
            "            room_id: The ID of the room to knock on.",
            "            knockee: The ID of the user who is knocking.",
            "            content: The content of the knock event.",
            "",
            "        Returns:",
            "            A tuple of (event ID, stream ID).",
            "",
            "        Raises:",
            "            SynapseError: If the chosen remote server returns a 3xx/4xx code.",
            "            RuntimeError: If no servers were reachable.",
            "        \"\"\"",
            "        logger.debug(\"Knocking on room %s on behalf of user %s\", room_id, knockee)",
            "",
            "        # Inform the remote server of the room versions we support",
            "        supported_room_versions = list(KNOWN_ROOM_VERSIONS.keys())",
            "",
            "        # Ask the remote server to create a valid knock event for us. Once received,",
            "        # we sign the event",
            "        params: Dict[str, Iterable[str]] = {\"ver\": supported_room_versions}",
            "        origin, event, event_format_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, knockee, Membership.KNOCK, content, params=params",
            "        )",
            "",
            "        # Mark the knock as an outlier as we don't yet have the state at this point in",
            "        # the DAG.",
            "        event.internal_metadata.outlier = True",
            "",
            "        # ... but tell /sync to send it to clients anyway.",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Record the room ID and its version so that we have a record of the room",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=event_format_version",
            "        )",
            "",
            "        # Initially try the host that we successfully called /make_knock on",
            "        try:",
            "            target_hosts.remove(origin)",
            "            target_hosts.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        # Send the signed event back to the room, and potentially receive some",
            "        # further information about the room in the form of partial state events",
            "        stripped_room_state = await self.federation_client.send_knock(",
            "            target_hosts, event",
            "        )",
            "",
            "        # Store any stripped room state events in the \"unsigned\" key of the event.",
            "        # This is a bit of a hack and is cribbing off of invites. Basically we",
            "        # store the room state here and retrieve it again when this event appears",
            "        # in the invitee's sync stream. It is stripped out for all other local users.",
            "        event.unsigned[\"knock_room_state\"] = stripped_room_state[\"knock_state_events\"]",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "        return event.event_id, stream_id",
            "",
            "    async def _handle_queued_pdus(",
            "        self, room_queue: List[Tuple[EventBase, str]]",
            "    ) -> None:",
            "        \"\"\"Process PDUs which got queued up while we were busy send_joining.",
            "",
            "        Args:",
            "            room_queue: list of PDUs to be processed and the servers that sent them",
            "        \"\"\"",
            "        for p, origin in room_queue:",
            "            try:",
            "                logger.info(",
            "                    \"Processing queued PDU %s which was received while we were joining\",",
            "                    p,",
            "                )",
            "                with nested_logging_context(p.event_id):",
            "                    await self._federation_event_handler.on_receive_pdu(origin, p)",
            "            except Exception as e:",
            "                logger.warning(",
            "                    \"Error handling queued PDU %s from %s: %s\", p.event_id, origin, e",
            "                )",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_join/ request, so we create a partial",
            "        join event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create join event in",
            "            user_id: The user to create the join for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_join request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        # checking the room version will check that we've actually heard of the room",
            "        # (and return a 404 otherwise)",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /make_join, so return a 404 as we would if we weren't in the",
            "            # room at all.",
            "            # The main reason we can't respond properly is that we need to know about",
            "            # the auth events for the join event that we would return.",
            "            # We also should not bother entertaining the /make_join since we cannot",
            "            # handle the /send_join.",
            "            logger.info(",
            "                \"Rejecting /make_join to %s because it's a partial state room\", room_id",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_join right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        # now check that we are *still* in the room",
            "        is_in_room = await self._event_auth_handler.is_host_in_room(",
            "            room_id, self.server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Got /make_join request for room %s we are no longer in\",",
            "                room_id,",
            "            )",
            "            raise NotFoundError(\"Not an active room on this server\")",
            "",
            "        event_content = {\"membership\": Membership.JOIN}",
            "",
            "        # If the current room is using restricted join rules, additional information",
            "        # may need to be included in the event content in order to efficiently",
            "        # validate the event.",
            "        #",
            "        # Note that this requires the /send_join request to come back to the",
            "        # same server.",
            "        if room_version.msc3083_join_rules:",
            "            state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                room_id",
            "            )",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                prev_member_event_id = state_ids.get((EventTypes.Member, user_id), None)",
            "                # If the user is invited or joined to the room already, then",
            "                # no additional info is needed.",
            "                include_auth_user_id = True",
            "                if prev_member_event_id:",
            "                    prev_member_event = await self.store.get_event(prev_member_event_id)",
            "                    include_auth_user_id = prev_member_event.membership not in (",
            "                        Membership.JOIN,",
            "                        Membership.INVITE,",
            "                    )",
            "",
            "                if include_auth_user_id:",
            "                    event_content[",
            "                        EventContentFields.AUTHORISING_USER",
            "                    ] = await self._event_auth_handler.get_user_which_could_invite(",
            "                        room_id,",
            "                        state_ids,",
            "                    )",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": event_content,",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        try:",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "        except SynapseError as e:",
            "            logger.warning(\"Failed to create join to %s because %s\", room_id, e)",
            "            raise",
            "",
            "        # Ensure the user can even join the room.",
            "        await self._federation_event_handler.check_join_restrictions(context, event)",
            "",
            "        # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "        # when we get the event back in `on_send_join_request`",
            "        await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        return event",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, event: EventBase, room_version: RoomVersion",
            "    ) -> EventBase:",
            "        \"\"\"We've got an invite event. Process and persist it. Sign it.",
            "",
            "        Respond with the now signed event.",
            "        \"\"\"",
            "        if event.state_key is None:",
            "            raise SynapseError(400, \"The invite event did not have a state key\")",
            "",
            "        is_blocked = await self.store.is_room_blocked(event.room_id)",
            "        if is_blocked:",
            "            raise SynapseError(403, \"This room has been blocked on this server\")",
            "",
            "        if self.hs.config.server.block_non_admin_invites:",
            "            raise SynapseError(403, \"This server does not accept room invites\")",
            "",
            "        spam_check = await self.spam_checker.user_may_invite(",
            "            event.sender, event.state_key, event.room_id",
            "        )",
            "        if spam_check != NOT_SPAM:",
            "            raise SynapseError(",
            "                403,",
            "                \"This user is not permitted to send invites to this server/user\",",
            "                errcode=spam_check[0],",
            "                additional_fields=spam_check[1],",
            "            )",
            "",
            "        membership = event.content.get(\"membership\")",
            "        if event.type != EventTypes.Member or membership != Membership.INVITE:",
            "            raise SynapseError(400, \"The event was not an m.room.member invite event\")",
            "",
            "        sender_domain = get_domain_from_id(event.sender)",
            "        if sender_domain != origin:",
            "            raise SynapseError(",
            "                400, \"The invite event was not from the server sending it\"",
            "            )",
            "",
            "        if not self.is_mine_id(event.state_key):",
            "            raise SynapseError(400, \"The invite event must be for this server\")",
            "",
            "        # block any attempts to invite the server notices mxid",
            "        if event.state_key == self._server_notices_mxid:",
            "            raise SynapseError(HTTPStatus.FORBIDDEN, \"Cannot invite this user\")",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        # We don't rate limit based on room ID, as that should be done by",
            "        # sending server.",
            "        await member_handler.ratelimit_invite(None, None, event.state_key)",
            "",
            "        # keep a record of the room version, if we don't yet know it.",
            "        # (this may get overwritten if we later get a different room version in a",
            "        # join dance).",
            "        await self._maybe_store_room_on_outlier_membership(",
            "            room_id=event.room_id, room_version=room_version",
            "        )",
            "",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        event.signatures.update(",
            "            compute_event_signature(",
            "                room_version,",
            "                event.get_pdu_json(),",
            "                self.hs.hostname,",
            "                self.hs.signing_key,",
            "            )",
            "        )",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event",
            "",
            "    async def do_remotely_reject_invite(",
            "        self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict",
            "    ) -> Tuple[EventBase, int]:",
            "        origin, event, room_version = await self._make_and_verify_event(",
            "            target_hosts, room_id, user_id, \"leave\", content=content",
            "        )",
            "        # Mark as outlier as we don't have any state for this event; we're not",
            "        # even in the room.",
            "        event.internal_metadata.outlier = True",
            "        event.internal_metadata.out_of_band_membership = True",
            "",
            "        # Try the host that we successfully called /make_leave/ on first for",
            "        # the /send_leave/ request.",
            "        host_list = list(target_hosts)",
            "        try:",
            "            host_list.remove(origin)",
            "            host_list.insert(0, origin)",
            "        except ValueError:",
            "            pass",
            "",
            "        await self.federation_client.send_leave(host_list, event)",
            "",
            "        context = EventContext.for_outlier(self._storage_controllers)",
            "        stream_id = await self._federation_event_handler.persist_events_and_notify(",
            "            event.room_id, [(event, context)]",
            "        )",
            "",
            "        return event, stream_id",
            "",
            "    async def _make_and_verify_event(",
            "        self,",
            "        target_hosts: Iterable[str],",
            "        room_id: str,",
            "        user_id: str,",
            "        membership: str,",
            "        content: JsonDict,",
            "        params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,",
            "    ) -> Tuple[str, EventBase, RoomVersion]:",
            "        (",
            "            origin,",
            "            event,",
            "            room_version,",
            "        ) = await self.federation_client.make_membership_event(",
            "            target_hosts, room_id, user_id, membership, content, params=params",
            "        )",
            "",
            "        logger.debug(\"Got response to make_%s: %s\", membership, event)",
            "",
            "        # We should assert some things.",
            "        # FIXME: Do this in a nicer way",
            "        assert event.type == EventTypes.Member",
            "        assert event.user_id == user_id",
            "        assert event.state_key == user_id",
            "        assert event.room_id == room_id",
            "        return origin, event, room_version",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a /make_leave/ request, so we create a partial",
            "        leave event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: Room to create leave event in",
            "            user_id: The user to create the leave for",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Got /make_leave request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.LEAVE},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_leave_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new leave %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> EventBase:",
            "        \"\"\"We've received a make_knock request, so we create a partial",
            "        knock event for the room and return that. We do *not* persist or",
            "        process it until the other server has signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.info(",
            "                \"Get /make_knock request for user %r from different origin %s, ignoring\",",
            "                user_id,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        room_version_obj = await self.store.get_room_version(room_id)",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj,",
            "            {",
            "                \"type\": EventTypes.Member,",
            "                \"content\": {\"membership\": Membership.KNOCK},",
            "                \"room_id\": room_id,",
            "                \"sender\": user_id,",
            "                \"state_key\": user_id,",
            "            },",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "",
            "        event_allowed, _ = await self.third_party_event_rules.check_event_allowed(",
            "            event, context",
            "        )",
            "        if not event_allowed:",
            "            logger.warning(\"Creation of knock %s forbidden by third-party rules\", event)",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "",
            "        try:",
            "            # The remote hasn't signed it yet, obviously. We'll do the full checks",
            "            # when we get the event back in `on_send_knock_request`",
            "            await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Failed to create new knock %r because %s\", event, e)",
            "            raise e",
            "",
            "        return event",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_pdu(self, room_id: str, event_id: str) -> List[str]:",
            "        \"\"\"Returns the state at the event. i.e. not including said event.\"\"\"",
            "        event = await self.store.get_event(event_id, check_room_id=room_id)",
            "        if event.internal_metadata.outlier:",
            "            raise NotFoundError(\"State not known at event %s\" % (event_id,))",
            "",
            "        state_groups = await self._state_storage_controller.get_state_groups_ids(",
            "            room_id, [event_id]",
            "        )",
            "",
            "        # get_state_groups_ids should return exactly one result",
            "        assert len(state_groups) == 1",
            "",
            "        state_map = next(iter(state_groups.values()))",
            "",
            "        state_key = event.get_state_key()",
            "        if state_key is not None:",
            "            # the event was not rejected (get_event raises a NotFoundError for rejected",
            "            # events) so the state at the event should include the event itself.",
            "            assert (",
            "                state_map.get((event.type, state_key)) == event.event_id",
            "            ), \"State at event did not include event itself\"",
            "",
            "            # ... but we need the state *before* that event",
            "            if \"replaces_state\" in event.unsigned:",
            "                prev_id = event.unsigned[\"replaces_state\"]",
            "                state_map[(event.type, state_key)] = prev_id",
            "            else:",
            "                del state_map[(event.type, state_key)]",
            "",
            "        return list(state_map.values())",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, pdu_list: List[str], limit: int",
            "    ) -> List[EventBase]:",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "",
            "        # Synapse asks for 100 events per backfill request. Do not allow more.",
            "        limit = min(limit, 100)",
            "",
            "        events = await self.store.get_backfill_events(room_id, pdu_list, limit)",
            "        logger.debug(",
            "            \"on_backfill_request: backfill events=%s\",",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        events = await filter_events_for_server(",
            "            self._storage_controllers, origin, events",
            "        )",
            "",
            "        return events",
            "",
            "    async def get_persisted_pdu(",
            "        self, origin: str, event_id: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get an event from the database for the given server.",
            "",
            "        Args:",
            "            origin: hostname of server which is requesting the event; we",
            "               will check that the server is allowed to see it.",
            "            event_id: id of the event being requested",
            "",
            "        Returns:",
            "            None if we know nothing about the event; otherwise the (possibly-redacted) event.",
            "",
            "        Raises:",
            "            AuthError if the server is not currently in the room",
            "        \"\"\"",
            "        event = await self.store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        if not event:",
            "            return None",
            "",
            "        await self._event_auth_handler.assert_host_in_room(event.room_id, origin)",
            "",
            "        events = await filter_events_for_server(",
            "            self._storage_controllers, origin, [event]",
            "        )",
            "        event = events[0]",
            "        return event",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> List[EventBase]:",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "",
            "        # Only allow up to 20 events to be retrieved per request.",
            "        limit = min(limit, 20)",
            "",
            "        missing_events = await self.store.get_missing_events(",
            "            room_id=room_id,",
            "            earliest_events=earliest_events,",
            "            latest_events=latest_events,",
            "            limit=limit,",
            "        )",
            "",
            "        missing_events = await filter_events_for_server(",
            "            self._storage_controllers, origin, missing_events",
            "        )",
            "",
            "        return missing_events",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: JsonDict",
            "    ) -> None:",
            "        third_party_invite = {\"signed\": signed}",
            "",
            "        event_dict = {",
            "            \"type\": EventTypes.Member,",
            "            \"content\": {",
            "                \"membership\": Membership.INVITE,",
            "                \"third_party_invite\": third_party_invite,",
            "            },",
            "            \"room_id\": room_id,",
            "            \"sender\": sender_user_id,",
            "            \"state_key\": target_user_id,",
            "        }",
            "",
            "        if await self._event_auth_handler.is_host_in_room(room_id, self.hs.hostname):",
            "            room_version_obj = await self.store.get_room_version(room_id)",
            "            builder = self.event_builder_factory.for_room_version(",
            "                room_version_obj, event_dict",
            "            )",
            "",
            "            EventValidator().validate_builder(builder)",
            "            event, context = await self.event_creation_handler.create_new_client_event(",
            "                builder=builder",
            "            )",
            "",
            "            event, context = await self.add_display_name_to_third_party_invite(",
            "                room_version_obj, event_dict, event, context",
            "            )",
            "",
            "            EventValidator().validate_new(event, self.config)",
            "",
            "            # We need to tell the transaction queue to send this out, even",
            "            # though the sender isn't a local user.",
            "            event.internal_metadata.send_on_behalf_of = self.hs.hostname",
            "",
            "            try:",
            "                validate_event_for_room_version(event)",
            "                await self._event_auth_handler.check_auth_rules_from_context(",
            "                    event, context",
            "                )",
            "            except AuthError as e:",
            "                logger.warning(\"Denying new third party invite %r because %s\", event, e)",
            "                raise e",
            "",
            "            await self._check_signature(event, context)",
            "",
            "            # We retrieve the room member handler here as to not cause a cyclic dependency",
            "            member_handler = self.hs.get_room_member_handler()",
            "            await member_handler.send_membership_event(None, event, context)",
            "        else:",
            "            destinations = {x.split(\":\", 1)[-1] for x in (sender_user_id, room_id)}",
            "",
            "            try:",
            "                await self.federation_client.forward_third_party_invite(",
            "                    destinations, room_id, event_dict",
            "                )",
            "            except (RequestSendFailed, HttpResponseException):",
            "                raise SynapseError(502, \"Failed to forward third party invite\")",
            "",
            "    async def on_exchange_third_party_invite_request(",
            "        self, event_dict: JsonDict",
            "    ) -> None:",
            "        \"\"\"Handle an exchange_third_party_invite request from a remote server",
            "",
            "        The remote server will call this when it wants to turn a 3pid invite",
            "        into a normal m.room.member invite.",
            "",
            "        Args:",
            "            event_dict: Dictionary containing the event body.",
            "",
            "        \"\"\"",
            "        assert_params_in_dict(event_dict, [\"room_id\"])",
            "        room_version_obj = await self.store.get_room_version(event_dict[\"room_id\"])",
            "",
            "        # NB: event_dict has a particular specced format we might need to fudge",
            "        # if we change event formats too much.",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        event, context = await self.add_display_name_to_third_party_invite(",
            "            room_version_obj, event_dict, event, context",
            "        )",
            "",
            "        try:",
            "            validate_event_for_room_version(event)",
            "            await self._event_auth_handler.check_auth_rules_from_context(event, context)",
            "        except AuthError as e:",
            "            logger.warning(\"Denying third party invite %r because %s\", event, e)",
            "            raise e",
            "        await self._check_signature(event, context)",
            "",
            "        # We need to tell the transaction queue to send this out, even",
            "        # though the sender isn't a local user.",
            "        event.internal_metadata.send_on_behalf_of = get_domain_from_id(event.sender)",
            "",
            "        # We retrieve the room member handler here as to not cause a cyclic dependency",
            "        member_handler = self.hs.get_room_member_handler()",
            "        await member_handler.send_membership_event(None, event, context)",
            "",
            "    async def add_display_name_to_third_party_invite(",
            "        self,",
            "        room_version_obj: RoomVersion,",
            "        event_dict: JsonDict,",
            "        event: EventBase,",
            "        context: EventContext,",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        key = (",
            "            EventTypes.ThirdPartyInvite,",
            "            event.content[\"third_party_invite\"][\"signed\"][\"token\"],",
            "        )",
            "        original_invite = None",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(EventTypes.ThirdPartyInvite, None)])",
            "        )",
            "        original_invite_id = prev_state_ids.get(key)",
            "        if original_invite_id:",
            "            original_invite = await self.store.get_event(",
            "                original_invite_id, allow_none=True",
            "            )",
            "        if original_invite:",
            "            # If the m.room.third_party_invite event's content is empty, it means the",
            "            # invite has been revoked. In this case, we don't have to raise an error here",
            "            # because the auth check will fail on the invite (because it's not able to",
            "            # fetch public keys from the m.room.third_party_invite event's content, which",
            "            # is empty).",
            "            display_name = original_invite.content.get(\"display_name\")",
            "            event_dict[\"content\"][\"third_party_invite\"][\"display_name\"] = display_name",
            "        else:",
            "            logger.info(",
            "                \"Could not find invite event for third_party_invite: %r\", event_dict",
            "            )",
            "            # We don't discard here as this is not the appropriate place to do",
            "            # auth checks. If we need the invite and don't have it then the",
            "            # auth check code will explode appropriately.",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "        EventValidator().validate_builder(builder)",
            "        event, context = await self.event_creation_handler.create_new_client_event(",
            "            builder=builder",
            "        )",
            "        EventValidator().validate_new(event, self.config)",
            "        return event, context",
            "",
            "    async def _check_signature(self, event: EventBase, context: EventContext) -> None:",
            "        \"\"\"",
            "        Checks that the signature in the event is consistent with its invite.",
            "",
            "        Args:",
            "            event: The m.room.member event to check",
            "            context:",
            "",
            "        Raises:",
            "            AuthError: if signature didn't match any keys, or key has been",
            "                revoked,",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        signed = event.content[\"third_party_invite\"][\"signed\"]",
            "        token = signed[\"token\"]",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(EventTypes.ThirdPartyInvite, None)])",
            "        )",
            "        invite_event_id = prev_state_ids.get((EventTypes.ThirdPartyInvite, token))",
            "",
            "        invite_event = None",
            "        if invite_event_id:",
            "            invite_event = await self.store.get_event(invite_event_id, allow_none=True)",
            "",
            "        if not invite_event:",
            "            raise AuthError(403, \"Could not find invite\")",
            "",
            "        logger.debug(\"Checking auth on event %r\", event.content)",
            "",
            "        last_exception: Optional[Exception] = None",
            "",
            "        # for each public key in the 3pid invite event",
            "        for public_key_object in event_auth.get_public_keys(invite_event):",
            "            try:",
            "                # for each sig on the third_party_invite block of the actual invite",
            "                for server, signature_block in signed[\"signatures\"].items():",
            "                    for key_name in signature_block.keys():",
            "                        if not key_name.startswith(\"ed25519:\"):",
            "                            continue",
            "",
            "                        logger.debug(",
            "                            \"Attempting to verify sig with key %s from %r \"",
            "                            \"against pubkey %r\",",
            "                            key_name,",
            "                            server,",
            "                            public_key_object,",
            "                        )",
            "",
            "                        try:",
            "                            public_key = public_key_object[\"public_key\"]",
            "                            verify_key = decode_verify_key_bytes(",
            "                                key_name, decode_base64(public_key)",
            "                            )",
            "                            verify_signed_json(signed, server, verify_key)",
            "                            logger.debug(",
            "                                \"Successfully verified sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to verify sig with key %s from %r \"",
            "                                \"against pubkey %r\",",
            "                                key_name,",
            "                                server,",
            "                                public_key_object,",
            "                            )",
            "                            raise",
            "                        try:",
            "                            if \"key_validity_url\" in public_key_object:",
            "                                await self._check_key_revocation(",
            "                                    public_key, public_key_object[\"key_validity_url\"]",
            "                                )",
            "                        except Exception:",
            "                            logger.info(",
            "                                \"Failed to query key_validity_url %s\",",
            "                                public_key_object[\"key_validity_url\"],",
            "                            )",
            "                            raise",
            "                        return",
            "            except Exception as e:",
            "                last_exception = e",
            "",
            "        if last_exception is None:",
            "            # we can only get here if get_public_keys() returned an empty list",
            "            # TODO: make this better",
            "            raise RuntimeError(\"no public key in invite event\")",
            "",
            "        raise last_exception",
            "",
            "    async def _check_key_revocation(self, public_key: str, url: str) -> None:",
            "        \"\"\"",
            "        Checks whether public_key has been revoked.",
            "",
            "        Args:",
            "            public_key: base-64 encoded public key.",
            "            url: Key revocation URL.",
            "",
            "        Raises:",
            "            AuthError: if they key has been revoked.",
            "            SynapseError: if a transient error meant a key couldn't be checked",
            "                for revocation.",
            "        \"\"\"",
            "        try:",
            "            response = await self.http_client.get_json(url, {\"public_key\": public_key})",
            "        except Exception:",
            "            raise SynapseError(502, \"Third party certificate could not be checked\")",
            "        if \"valid\" not in response or not response[\"valid\"]:",
            "            raise AuthError(403, \"Third party certificate was invalid\")",
            "",
            "    async def _clean_room_for_join(self, room_id: str) -> None:",
            "        \"\"\"Called to clean up any data in DB for a given room, ready for the",
            "        server to join the room.",
            "",
            "        Args:",
            "            room_id",
            "        \"\"\"",
            "        if self.config.worker.worker_app:",
            "            await self._clean_room_for_join_client(room_id)",
            "        else:",
            "            await self.store.clean_room_for_join(room_id)",
            "",
            "    async def get_room_complexity(",
            "        self, remote_room_hosts: List[str], room_id: str",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Fetch the complexity of a remote room over federation.",
            "",
            "        Args:",
            "            remote_room_hosts (list[str]): The remote servers to ask.",
            "            room_id (str): The room ID to ask about.",
            "",
            "        Returns:",
            "            Dict contains the complexity",
            "            metric versions, while None means we could not fetch the complexity.",
            "        \"\"\"",
            "",
            "        for host in remote_room_hosts:",
            "            res = await self.federation_client.get_room_complexity(host, room_id)",
            "",
            "            # We got a result, return it.",
            "            if res:",
            "                return res",
            "",
            "        # We fell off the bottom, couldn't get the complexity from anyone. Oh",
            "        # well.",
            "        return None",
            "",
            "    async def _resume_sync_partial_state_room(self) -> None:",
            "        \"\"\"Resumes resyncing of all partial-state rooms after a restart.\"\"\"",
            "        assert not self.config.worker.worker_app",
            "",
            "        partial_state_rooms = await self.store.get_partial_state_rooms_and_servers()",
            "        for room_id, servers_in_room in partial_state_rooms.items():",
            "            run_as_background_process(",
            "                desc=\"sync_partial_state_room\",",
            "                func=self._sync_partial_state_room,",
            "                initial_destination=None,",
            "                other_destinations=servers_in_room,",
            "                room_id=room_id,",
            "            )",
            "",
            "    async def _sync_partial_state_room(",
            "        self,",
            "        initial_destination: Optional[str],",
            "        other_destinations: Collection[str],",
            "        room_id: str,",
            "    ) -> None:",
            "        \"\"\"Background process to resync the state of a partial-state room",
            "",
            "        Args:",
            "            initial_destination: the initial homeserver to pull the state from",
            "            other_destinations: other homeservers to try to pull the state from, if",
            "                `initial_destination` is unavailable",
            "            room_id: room to be resynced",
            "        \"\"\"",
            "        assert not self.config.worker.worker_app",
            "",
            "        # TODO(faster_joins): do we need to lock to avoid races? What happens if other",
            "        #   worker processes kick off a resync in parallel? Perhaps we should just elect",
            "        #   a single worker to do the resync.",
            "        #   https://github.com/matrix-org/synapse/issues/12994",
            "        #",
            "        # TODO(faster_joins): what happens if we leave the room during a resync? if we",
            "        #   really leave, that might mean we have difficulty getting the room state over",
            "        #   federation.",
            "        #   https://github.com/matrix-org/synapse/issues/12802",
            "        #",
            "        # TODO(faster_joins): we need some way of prioritising which homeservers in",
            "        #   `other_destinations` to try first, otherwise we'll spend ages trying dead",
            "        #   homeservers for large rooms.",
            "        #   https://github.com/matrix-org/synapse/issues/12999",
            "",
            "        if initial_destination is None and len(other_destinations) == 0:",
            "            raise ValueError(",
            "                f\"Cannot resync state of {room_id}: no destinations provided\"",
            "            )",
            "",
            "        # Make an infinite iterator of destinations to try. Once we find a working",
            "        # destination, we'll stick with it until it flakes.",
            "        destinations: Collection[str]",
            "        if initial_destination is not None:",
            "            # Move `initial_destination` to the front of the list.",
            "            destinations = list(other_destinations)",
            "            if initial_destination in destinations:",
            "                destinations.remove(initial_destination)",
            "            destinations = [initial_destination] + destinations",
            "        else:",
            "            destinations = other_destinations",
            "        destination_iter = itertools.cycle(destinations)",
            "",
            "        # `destination` is the current remote homeserver we're pulling from.",
            "        destination = next(destination_iter)",
            "        logger.info(\"Syncing state for room %s via %s\", room_id, destination)",
            "",
            "        # we work through the queue in order of increasing stream ordering.",
            "        while True:",
            "            batch = await self.store.get_partial_state_events_batch(room_id)",
            "            if not batch:",
            "                # all the events are updated, so we can update current state and",
            "                # clear the lazy-loading flag.",
            "                logger.info(\"Updating current state for %s\", room_id)",
            "                # TODO(faster_joins): notify workers in notify_room_un_partial_stated",
            "                #   https://github.com/matrix-org/synapse/issues/12994",
            "                await self.state_handler.update_current_state(room_id)",
            "",
            "                logger.info(\"Clearing partial-state flag for %s\", room_id)",
            "                success = await self.store.clear_partial_state_room(room_id)",
            "                if success:",
            "                    logger.info(\"State resync complete for %s\", room_id)",
            "                    self._storage_controllers.state.notify_room_un_partial_stated(",
            "                        room_id",
            "                    )",
            "",
            "                    # TODO(faster_joins) update room stats and user directory?",
            "                    #   https://github.com/matrix-org/synapse/issues/12814",
            "                    #   https://github.com/matrix-org/synapse/issues/12815",
            "                    return",
            "",
            "                # we raced against more events arriving with partial state. Go round",
            "                # the loop again. We've already logged a warning, so no need for more.",
            "                continue",
            "",
            "            events = await self.store.get_events_as_list(",
            "                batch,",
            "                redact_behaviour=EventRedactBehaviour.as_is,",
            "                allow_rejected=True,",
            "            )",
            "            for event in events:",
            "                for attempt in itertools.count():",
            "                    try:",
            "                        await self._federation_event_handler.update_state_for_partial_state_event(",
            "                            destination, event",
            "                        )",
            "                        break",
            "                    except FederationError as e:",
            "                        if attempt == len(destinations) - 1:",
            "                            # We have tried every remote server for this event. Give up.",
            "                            # TODO(faster_joins) giving up isn't the right thing to do",
            "                            #   if there's a temporary network outage. retrying",
            "                            #   indefinitely is also not the right thing to do if we can",
            "                            #   reach all homeservers and they all claim they don't have",
            "                            #   the state we want.",
            "                            #   https://github.com/matrix-org/synapse/issues/13000",
            "                            logger.error(",
            "                                \"Failed to get state for %s at %s from %s because %s, \"",
            "                                \"giving up!\",",
            "                                room_id,",
            "                                event,",
            "                                destination,",
            "                                e,",
            "                            )",
            "                            raise",
            "",
            "                        # Try the next remote server.",
            "                        logger.info(",
            "                            \"Failed to get state for %s at %s from %s because %s\",",
            "                            room_id,",
            "                            event,",
            "                            destination,",
            "                            e,",
            "                        )",
            "                        destination = next(destination_iter)",
            "                        logger.info(",
            "                            \"Syncing state for room %s via %s instead\",",
            "                            room_id,",
            "                            destination,",
            "                        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "807": [
                "FederationHandler"
            ],
            "1153": [
                "FederationHandler"
            ],
            "1154": [
                "FederationHandler"
            ],
            "1155": [
                "FederationHandler"
            ],
            "1201": [
                "FederationHandler"
            ],
            "1202": [
                "FederationHandler"
            ],
            "1203": [
                "FederationHandler"
            ],
            "1204": [
                "FederationHandler"
            ],
            "1205": [
                "FederationHandler"
            ],
            "1206": [
                "FederationHandler"
            ],
            "1207": [
                "FederationHandler"
            ],
            "1208": [
                "FederationHandler"
            ],
            "1209": [
                "FederationHandler"
            ],
            "1210": [
                "FederationHandler"
            ],
            "1211": [
                "FederationHandler"
            ],
            "1212": [
                "FederationHandler"
            ],
            "1213": [
                "FederationHandler"
            ],
            "1224": [
                "FederationHandler"
            ],
            "1225": [
                "FederationHandler"
            ],
            "1226": [
                "FederationHandler"
            ],
            "1260": [
                "FederationHandler"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/federation_event.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": 238,
                "PatchRowcode": "         #"
            },
            "1": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "         # Note that if we were never in the room then we would have already"
            },
            "2": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 240,
                "PatchRowcode": "         # dropped the event, since we wouldn't know the room version."
            },
            "3": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        is_in_room = await self._event_auth_handler.check_host_in_room("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 241,
                "PatchRowcode": "+        is_in_room = await self._event_auth_handler.is_host_in_room("
            },
            "5": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "             room_id, self._server_name"
            },
            "6": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "         )"
            },
            "7": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "         if not is_in_room:"
            }
        },
        "frontPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import collections",
            "import itertools",
            "import logging",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Collection,",
            "    Container,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Sequence,",
            "    Set,",
            "    Tuple,",
            ")",
            "",
            "from prometheus_client import Counter, Histogram",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    Membership,",
            "    RejectedReason,",
            "    RoomEncryptionAlgorithms,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    HttpResponseException,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions",
            "from synapse.event_auth import (",
            "    auth_types_for_event,",
            "    check_state_dependent_auth_rules,",
            "    check_state_independent_auth_rules,",
            "    validate_event_for_room_version,",
            ")",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_client import InvalidResponseError",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    set_tag,",
            "    start_active_span,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEventsRestServlet,",
            ")",
            "from synapse.state import StateResolutionStore",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.storage.state import StateFilter",
            "from synapse.types import (",
            "    PersistedEventPosition,",
            "    RoomStreamToken,",
            "    StateMap,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.iterutils import batch_iter",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import shortstr",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "soft_failed_event_counter = Counter(",
            "    \"synapse_federation_soft_failed_events_total\",",
            "    \"Events received over federation that we marked as soft_failed\",",
            ")",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_after_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_after_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.25,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        25.0,",
            "        30.0,",
            "        40.0,",
            "        50.0,",
            "        60.0,",
            "        80.0,",
            "        100.0,",
            "        120.0,",
            "        150.0,",
            "        180.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "class FederationEventHandler:",
            "    \"\"\"Handles events that originated from federation.",
            "",
            "    Responsible for handing incoming events and passing them on to the rest",
            "    of the homeserver (including auth and state conflict resolutions)",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "",
            "        self._state_handler = hs.get_state_handler()",
            "        self._event_creation_handler = hs.get_event_creation_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._message_handler = hs.get_message_handler()",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "        self._state_resolution_handler = hs.get_state_resolution_handler()",
            "        # avoid a circular dependency by deferring execution here",
            "        self._get_room_member_handler = hs.get_room_member_handler",
            "",
            "        self._federation_client = hs.get_federation_client()",
            "        self._third_party_event_rules = hs.get_third_party_event_rules()",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._server_name = hs.hostname",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        self._config = hs.config",
            "        self._ephemeral_messages_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)",
            "        if hs.config.worker.worker_app:",
            "            self._user_device_resync = (",
            "                ReplicationUserDevicesResyncRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._device_list_updater = hs.get_device_handler().device_list_updater",
            "",
            "        # When joining a room we need to queue any events for that room up.",
            "        # For each room, a list of (pdu, origin) tuples.",
            "        # TODO: replace this with something more elegant, probably based around the",
            "        # federation event staging area.",
            "        self.room_queues: Dict[str, List[Tuple[EventBase, str]]] = {}",
            "",
            "        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")",
            "",
            "    async def on_receive_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received via a federation /send/ transaction",
            "",
            "        Args:",
            "            origin: server which initiated the /send/ transaction. Will",
            "                be used to fetch missing events or state.",
            "            pdu: received PDU",
            "        \"\"\"",
            "",
            "        # We should never see any outliers here.",
            "        assert not pdu.internal_metadata.outlier",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        # We reprocess pdus when we have seen them only as outliers",
            "        existing = await self._store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        # FIXME: Currently we fetch an event again when we already have it",
            "        # if it has been marked as an outlier.",
            "        if existing:",
            "            if not existing.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received event %s which we have already seen\", event_id",
            "                )",
            "                return",
            "            if pdu.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received outlier %s which we already have as an outlier\",",
            "                    event_id,",
            "                )",
            "                return",
            "            logger.info(\"De-outliering event %s\", event_id)",
            "",
            "        # do some initial sanity-checking of the event. In particular, make",
            "        # sure it doesn't have hundreds of prev_events or auth_events, which",
            "        # could cause a huge state resolution or cascade of event fetches.",
            "        try:",
            "            self._sanity_check_event(pdu)",
            "        except SynapseError as err:",
            "            logger.warning(\"Received event failed sanity checks\")",
            "            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)",
            "",
            "        # If we are currently in the process of joining this room, then we",
            "        # queue up events for later processing.",
            "        if room_id in self.room_queues:",
            "            logger.info(",
            "                \"Queuing PDU from %s for now: join in progress\",",
            "                origin,",
            "            )",
            "            self.room_queues[room_id].append((pdu, origin))",
            "            return",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        #",
            "        # Note that if we were never in the room then we would have already",
            "        # dropped the event, since we wouldn't know the room version.",
            "        is_in_room = await self._event_auth_handler.check_host_in_room(",
            "            room_id, self._server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Ignoring PDU from %s as we're not in the room\",",
            "                origin,",
            "            )",
            "            return None",
            "",
            "        # Try to fetch any missing prev events to fill in gaps in the graph",
            "        prevs = set(pdu.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        if missing_prevs:",
            "            # We only backfill backwards to the min depth.",
            "            min_depth = await self._store.get_min_depth(pdu.room_id)",
            "            logger.debug(\"min_depth: %d\", min_depth)",
            "",
            "            if min_depth is not None and pdu.depth > min_depth:",
            "                # If we're missing stuff, ensure we only fetch stuff one",
            "                # at a time.",
            "                logger.info(",
            "                    \"Acquiring room lock to fetch %d missing prev_events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                async with self._room_pdu_linearizer.queue(pdu.room_id):",
            "                    logger.info(",
            "                        \"Acquired room lock to fetch %d missing prev_events\",",
            "                        len(missing_prevs),",
            "                    )",
            "",
            "                    try:",
            "                        await self._get_missing_events_for_pdu(",
            "                            origin, pdu, prevs, min_depth",
            "                        )",
            "                    except Exception as e:",
            "                        raise Exception(",
            "                            \"Error fetching missing prev_events for %s: %s\"",
            "                            % (event_id, e)",
            "                        ) from e",
            "",
            "                # Update the set of things we've seen after trying to",
            "                # fetch the missing stuff",
            "                seen = await self._store.have_events_in_timeline(prevs)",
            "                missing_prevs = prevs - seen",
            "",
            "                if not missing_prevs:",
            "                    logger.info(\"Found all missing prev_events\")",
            "",
            "            if missing_prevs:",
            "                # since this event was pushed to us, it is possible for it to",
            "                # become the only forward-extremity in the room, and we would then",
            "                # trust its state to be the state for the whole room. This is very",
            "                # bad. Further, if the event was pushed to us, there is no excuse",
            "                # for us not to have all the prev_events. (XXX: apart from",
            "                # min_depth?)",
            "                #",
            "                # We therefore reject any such events.",
            "                logger.warning(",
            "                    \"Rejecting: failed to fetch %d prev events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                raise FederationError(",
            "                    \"ERROR\",",
            "                    403,",
            "                    (",
            "                        \"Your server isn't divulging details about prev_events \"",
            "                        \"referenced in this event.\"",
            "                    ),",
            "                    affected=pdu.event_id,",
            "                )",
            "",
            "        try:",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were processing the PDU.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated while processing the PDU, trying again.\",",
            "                room_id,",
            "            )",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "",
            "    async def on_send_membership_event(",
            "        self, origin: str, event: EventBase",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"",
            "        We have received a join/leave/knock event for a room via send_join/leave/knock.",
            "",
            "        Verify that event and send it into the room on the remote homeserver's behalf.",
            "",
            "        This is quite similar to on_receive_pdu, with the following principal",
            "        differences:",
            "          * only membership events are permitted (and only events with",
            "            sender==state_key -- ie, no kicks or bans)",
            "          * *We* send out the event on behalf of the remote server.",
            "          * We enforce the membership restrictions of restricted rooms.",
            "          * Rejected events result in an exception rather than being stored.",
            "",
            "        There are also other differences, however it is not clear if these are by",
            "        design or omission. In particular, we do not attempt to backfill any missing",
            "        prev_events.",
            "",
            "        Args:",
            "            origin: The homeserver of the remote (joining/invited/knocking) user.",
            "            event: The member event that has been signed by the remote homeserver.",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            RuntimeError if any prev_events are missing",
            "            SynapseError if the event is not accepted into the room",
            "            PartialStateConflictError if the room was un-partial stated in between",
            "                computing the state at the event and persisting it. The caller should",
            "                retry exactly once in this case.",
            "        \"\"\"",
            "        logger.debug(",
            "            \"on_send_membership_event: Got event: %s, signatures: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got send_membership request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        if event.sender != event.state_key:",
            "            raise SynapseError(400, \"state_key and sender must match\", Codes.BAD_JSON)",
            "",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # Send this event on behalf of the other server.",
            "        #",
            "        # The remote server isn't a full participant in the room at this point, so",
            "        # may not have an up-to-date list of the other homeservers participating in",
            "        # the room, so we send it on their behalf.",
            "        event.internal_metadata.send_on_behalf_of = origin",
            "",
            "        context = await self._state_handler.compute_event_context(event)",
            "        await self._check_event_auth(origin, event, context)",
            "        if context.rejected:",
            "            raise SynapseError(",
            "                403, f\"{event.membership} event was rejected\", Codes.FORBIDDEN",
            "            )",
            "",
            "        # for joins, we need to check the restrictions of restricted rooms",
            "        if event.membership == Membership.JOIN:",
            "            await self.check_join_restrictions(context, event)",
            "",
            "        # for knock events, we run the third-party event rules. It's not entirely clear",
            "        # why we don't do this for other sorts of membership events.",
            "        if event.membership == Membership.KNOCK:",
            "            event_allowed, _ = await self._third_party_event_rules.check_event_allowed(",
            "                event, context",
            "            )",
            "            if not event_allowed:",
            "                logger.info(\"Sending of knock %s forbidden by third-party rules\", event)",
            "                raise SynapseError(",
            "                    403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "                )",
            "",
            "        # all looks good, we can persist the event.",
            "",
            "        # First, precalculate the joined hosts so that the federation sender doesn't",
            "        # need to.",
            "        await self._event_creation_handler.cache_joined_hosts_for_event(event, context)",
            "",
            "        await self._check_for_soft_fail(event, context=context, origin=origin)",
            "        await self._run_push_actions_and_persist_event(event, context)",
            "        return event, context",
            "",
            "    async def check_join_restrictions(",
            "        self, context: EventContext, event: EventBase",
            "    ) -> None:",
            "        \"\"\"Check that restrictions in restricted join rules are matched",
            "",
            "        Called when we receive a join event via send_join.",
            "",
            "        Raises an auth error if the restrictions are not matched.",
            "        \"\"\"",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        # Check if the user is already in the room or invited to the room.",
            "        user_id = event.state_key",
            "        prev_member_event_id = prev_state_ids.get((EventTypes.Member, user_id), None)",
            "        prev_member_event = None",
            "        if prev_member_event_id:",
            "            prev_member_event = await self._store.get_event(prev_member_event_id)",
            "",
            "        # Check if the member should be allowed access via membership in a space.",
            "        await self._event_auth_handler.check_restricted_join_rules(",
            "            prev_state_ids,",
            "            event.room_version,",
            "            user_id,",
            "            prev_member_event,",
            "        )",
            "",
            "    @trace",
            "    async def process_remote_join(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        auth_events: List[EventBase],",
            "        state: List[EventBase],",
            "        event: EventBase,",
            "        room_version: RoomVersion,",
            "        partial_state: bool,",
            "    ) -> int:",
            "        \"\"\"Persists the events returned by a send_join",
            "",
            "        Checks the auth chain is valid (and passes auth checks) for the",
            "        state and event. Then persists all of the events.",
            "        Notifies about the persisted events where appropriate.",
            "",
            "        Args:",
            "            origin: Where the events came from",
            "            room_id:",
            "            auth_events",
            "            state",
            "            event",
            "            room_version: The room version we expect this room to have, and",
            "                will raise if it doesn't match the version in the create event.",
            "            partial_state: True if the state omits non-critical membership events",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            SynapseError if the response is in some way invalid.",
            "            PartialStateConflictError if the homeserver is already in the room and it",
            "                has been un-partial stated.",
            "        \"\"\"",
            "        create_event = None",
            "        for e in state:",
            "            if (e.type, e.state_key) == (EventTypes.Create, \"\"):",
            "                create_event = e",
            "                break",
            "",
            "        if create_event is None:",
            "            # If the state doesn't have a create event then the room is",
            "            # invalid, and it would fail auth checks anyway.",
            "            raise SynapseError(400, \"No create event in state\")",
            "",
            "        room_version_id = create_event.content.get(",
            "            \"room_version\", RoomVersions.V1.identifier",
            "        )",
            "",
            "        if room_version.identifier != room_version_id:",
            "            raise SynapseError(400, \"Room version mismatch\")",
            "",
            "        # persist the auth chain and state events.",
            "        #",
            "        # any invalid events here will be marked as rejected, and we'll carry on.",
            "        #",
            "        # any events whose auth events are missing (ie, not in the send_join response,",
            "        # and not already in our db) will just be ignored. This is correct behaviour,",
            "        # because the reason that auth_events are missing might be due to us being",
            "        # unable to validate their signatures. The fact that we can't validate their",
            "        # signatures right now doesn't mean that we will *never* be able to, so it",
            "        # is premature to reject them.",
            "        #",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state)",
            "        )",
            "",
            "        # and now persist the join event itself.",
            "        logger.info(",
            "            \"Peristing join-via-remote %s (partial_state: %s)\", event, partial_state",
            "        )",
            "        with nested_logging_context(suffix=event.event_id):",
            "            context = await self._state_handler.compute_event_context(",
            "                event,",
            "                state_ids_before_event={",
            "                    (e.type, e.state_key): e.event_id for e in state",
            "                },",
            "                partial_state=partial_state,",
            "            )",
            "",
            "            await self._check_event_auth(origin, event, context)",
            "            if context.rejected:",
            "                raise SynapseError(400, \"Join event was rejected\")",
            "",
            "            # the remote server is responsible for sending our join event to the rest",
            "            # of the federation. Indeed, attempting to do so will result in problems",
            "            # when we try to look up the state before the join (to get the server list)",
            "            # and discover that we do not have it.",
            "            event.internal_metadata.proactively_send = False",
            "",
            "            stream_id_after_persist = await self.persist_events_and_notify(",
            "                room_id, [(event, context)]",
            "            )",
            "",
            "            # If we're joining the room again, check if there is new marker",
            "            # state indicating that there is new history imported somewhere in",
            "            # the DAG. Multiple markers can exist in the current state with",
            "            # unique state_keys.",
            "            #",
            "            # Do this after the state from the remote join was persisted (via",
            "            # `persist_events_and_notify`). Otherwise we can run into a",
            "            # situation where the create event doesn't exist yet in the",
            "            # `current_state_events`",
            "            for e in state:",
            "                await self._handle_marker_event(origin, e)",
            "",
            "            return stream_id_after_persist",
            "",
            "    async def update_state_for_partial_state_event(",
            "        self, destination: str, event: EventBase",
            "    ) -> None:",
            "        \"\"\"Recalculate the state at an event as part of a de-partial-stating process",
            "",
            "        Args:",
            "            destination: server to request full state from",
            "            event: partial-state event to be de-partial-stated",
            "",
            "        Raises:",
            "            FederationError if we fail to request state from the remote server.",
            "        \"\"\"",
            "        logger.info(\"Updating state for %s\", event.event_id)",
            "        with nested_logging_context(suffix=event.event_id):",
            "            # if we have all the event's prev_events, then we can work out the",
            "            # state based on their states. Otherwise, we request it from the destination",
            "            # server.",
            "            #",
            "            # This is the same operation as we do when we receive a regular event",
            "            # over federation.",
            "            context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                destination, event",
            "            )",
            "            if context.partial_state:",
            "                # this can happen if some or all of the event's prev_events still have",
            "                # partial state. We were careful to only pick events from the db without",
            "                # partial-state prev events, so that implies that a prev event has",
            "                # been persisted (with partial state) since we did the query.",
            "                #",
            "                # So, let's just ignore `event` for now; when we re-run the db query",
            "                # we should instead get its partial-state prev event, which we will",
            "                # de-partial-state, and then come back to event.",
            "                logger.warning(",
            "                    \"%s still has prev_events with partial state: can't de-partial-state it yet\",",
            "                    event.event_id,",
            "                )",
            "                return",
            "",
            "            # since the state at this event has changed, we should now re-evaluate",
            "            # whether it should have been rejected. We must already have all of the",
            "            # auth events (from last time we went round this path), so there is no",
            "            # need to pass the origin.",
            "            await self._check_event_auth(None, event, context)",
            "",
            "            await self._store.update_state_for_partial_state_event(event, context)",
            "            self._state_storage_controller.notify_event_un_partial_stated(",
            "                event.event_id",
            "            )",
            "",
            "    @trace",
            "    async def backfill(",
            "        self, dest: str, room_id: str, limit: int, extremities: Collection[str]",
            "    ) -> None:",
            "        \"\"\"Trigger a backfill request to `dest` for the given `room_id`",
            "",
            "        This will attempt to get more events from the remote. If the other side",
            "        has no new events to offer, this will return an empty list.",
            "",
            "        As the events are received, we check their signatures, and also do some",
            "        sanity-checking on them. If any of the backfilled events are invalid,",
            "        this method throws a SynapseError.",
            "",
            "        We might also raise an InvalidResponseError if the response from the remote",
            "        server is just bogus.",
            "",
            "        TODO: make this more useful to distinguish failures of the remote",
            "        server from invalid events (there is probably no point in trying to",
            "        re-fetch invalid events from every other HS in the room.)",
            "        \"\"\"",
            "        if dest == self._server_name:",
            "            raise SynapseError(400, \"Can't backfill from self.\")",
            "",
            "        events = await self._federation_client.backfill(",
            "            dest, room_id, limit=limit, extremities=extremities",
            "        )",
            "",
            "        if not events:",
            "            return",
            "",
            "        with backfill_processing_after_timer.time():",
            "            # if there are any events in the wrong room, the remote server is buggy and",
            "            # should not be trusted.",
            "            for ev in events:",
            "                if ev.room_id != room_id:",
            "                    raise InvalidResponseError(",
            "                        f\"Remote server {dest} returned event {ev.event_id} which is in \"",
            "                        f\"room {ev.room_id}, when we were backfilling in {room_id}\"",
            "                    )",
            "",
            "            await self._process_pulled_events(",
            "                dest,",
            "                events,",
            "                backfilled=True,",
            "            )",
            "",
            "    @trace",
            "    async def _get_missing_events_for_pdu(",
            "        self, origin: str, pdu: EventBase, prevs: Set[str], min_depth: int",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            origin: Origin of the pdu. Will be called to get the missing events",
            "            pdu: received pdu",
            "            prevs: List of event ids which we are missing",
            "            min_depth: Minimum depth of events to return.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "",
            "        if not prevs - seen:",
            "            return",
            "",
            "        latest_list = await self._store.get_latest_event_ids_in_room(room_id)",
            "",
            "        # We add the prev events that we have seen to the latest",
            "        # list to ensure the remote server doesn't give them to us",
            "        latest = set(latest_list)",
            "        latest |= seen",
            "",
            "        logger.info(",
            "            \"Requesting missing events between %s and %s\",",
            "            shortstr(latest),",
            "            event_id,",
            "        )",
            "",
            "        # XXX: we set timeout to 10s to help workaround",
            "        # https://github.com/matrix-org/synapse/issues/1733.",
            "        # The reason is to avoid holding the linearizer lock",
            "        # whilst processing inbound /send transactions, causing",
            "        # FDs to stack up and block other inbound transactions",
            "        # which empirically can currently take up to 30 minutes.",
            "        #",
            "        # N.B. this explicitly disables retry attempts.",
            "        #",
            "        # N.B. this also increases our chances of falling back to",
            "        # fetching fresh state for the room if the missing event",
            "        # can't be found, which slightly reduces our security.",
            "        # it may also increase our DAG extremity count for the room,",
            "        # causing additional state resolution?  See #1760.",
            "        # However, fetching state doesn't hold the linearizer lock",
            "        # apparently.",
            "        #",
            "        # see https://github.com/matrix-org/synapse/pull/1744",
            "        #",
            "        # ----",
            "        #",
            "        # Update richvdh 2018/09/18: There are a number of problems with timing this",
            "        # request out aggressively on the client side:",
            "        #",
            "        # - it plays badly with the server-side rate-limiter, which starts tarpitting you",
            "        #   if you send too many requests at once, so you end up with the server carefully",
            "        #   working through the backlog of your requests, which you have already timed",
            "        #   out.",
            "        #",
            "        # - for this request in particular, we now (as of",
            "        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the",
            "        #   server can't produce a plausible-looking set of prev_events - so we becone",
            "        #   much more likely to reject the event.",
            "        #",
            "        # - contrary to what it says above, we do *not* fall back to fetching fresh state",
            "        #   for the room if get_missing_events times out. Rather, we give up processing",
            "        #   the PDU whose prevs we are missing, which then makes it much more likely that",
            "        #   we'll end up back here for the *next* PDU in the list, which exacerbates the",
            "        #   problem.",
            "        #",
            "        # - the aggressive 10s timeout was introduced to deal with incoming federation",
            "        #   requests taking 8 hours to process. It's not entirely clear why that was going",
            "        #   on; certainly there were other issues causing traffic storms which are now",
            "        #   resolved, and I think in any case we may be more sensible about our locking",
            "        #   now. We're *certainly* more sensible about our logging.",
            "        #",
            "        # All that said: Let's try increasing the timeout to 60s and see what happens.",
            "",
            "        try:",
            "            missing_events = await self._federation_client.get_missing_events(",
            "                origin,",
            "                room_id,",
            "                earliest_events_ids=list(latest),",
            "                latest_events=[pdu],",
            "                limit=10,",
            "                min_depth=min_depth,",
            "                timeout=60000,",
            "            )",
            "        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:",
            "            # We failed to get the missing events, but since we need to handle",
            "            # the case of `get_missing_events` not returning the necessary",
            "            # events anyway, it is safe to simply log the error and continue.",
            "            logger.warning(\"Failed to get prev_events: %s\", e)",
            "            return",
            "",
            "        logger.info(\"Got %d prev_events\", len(missing_events))",
            "        await self._process_pulled_events(origin, missing_events, backfilled=False)",
            "",
            "    @trace",
            "    async def _process_pulled_events(",
            "        self, origin: str, events: Collection[EventBase], backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a batch of events we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the events, persists the received events,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the events have already had their signatures and hashes checked.",
            "",
            "        Params:",
            "            origin: The server we received these events from",
            "            events: The received events.",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str([event.event_id for event in events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(events)),",
            "        )",
            "        set_tag(SynapseTags.FUNC_ARG_PREFIX + \"backfilled\", str(backfilled))",
            "        logger.debug(",
            "            \"processing pulled backfilled=%s events=%s\",",
            "            backfilled,",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        # We want to sort these by depth so we process them and",
            "        # tell clients about them in order.",
            "        sorted_events = sorted(events, key=lambda x: x.depth)",
            "        for ev in sorted_events:",
            "            with nested_logging_context(ev.event_id):",
            "                await self._process_pulled_event(origin, ev, backfilled=backfilled)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _process_pulled_event(",
            "        self, origin: str, event: EventBase, backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a single event that we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the event, persists the received event,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the event has already had its signatures and hashes checked.",
            "",
            "        This is somewhat equivalent to on_receive_pdu, but applies somewhat different",
            "        logic in the case that we are missing prev_events (in particular, it just",
            "        requests the state at that point, rather than triggering a get_missing_events) -",
            "        so is appropriate when we have pulled the event from a remote server, rather",
            "        than having it pushed to us.",
            "",
            "        Params:",
            "            origin: The server we received this event from",
            "            events: The received event",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        logger.info(\"Processing pulled event %s\", event)",
            "",
            "        # This function should not be used to persist outliers (use something",
            "        # else) because this does a bunch of operations that aren't necessary",
            "        # (extra work; in particular, it makes sure we have all the prev_events",
            "        # and resolves the state across those prev events). If you happen to run",
            "        # into a situation where the event you're trying to process/backfill is",
            "        # marked as an `outlier`, then you should update that spot to return an",
            "        # `EventBase` copy that doesn't have `outlier` flag set.",
            "        #",
            "        # `EventBase` is used to represent both an event we have not yet",
            "        # persisted, and one that we have persisted and now keep in the cache.",
            "        # In an ideal world this method would only be called with the first type",
            "        # of event, but it turns out that's not actually the case and for",
            "        # example, you could get an event from cache that is marked as an",
            "        # `outlier` (fix up that spot though).",
            "        assert not event.internal_metadata.is_outlier(), (",
            "            \"Outlier event passed to _process_pulled_event. \"",
            "            \"To persist an event as a non-outlier, make sure to pass in a copy without `event.internal_metadata.outlier = true`.\"",
            "        )",
            "",
            "        event_id = event.event_id",
            "",
            "        existing = await self._store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "        if existing:",
            "            if not existing.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"_process_pulled_event: Ignoring received event %s which we have already seen\",",
            "                    event_id,",
            "                )",
            "                return",
            "            logger.info(\"De-outliering event %s\", event_id)",
            "",
            "        try:",
            "            self._sanity_check_event(event)",
            "        except SynapseError as err:",
            "            logger.warning(\"Event %s failed sanity check: %s\", event_id, err)",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(err)",
            "            )",
            "            return",
            "        except Exception as exc:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(exc)",
            "            )",
            "            raise exc",
            "",
            "        try:",
            "            try:",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "            except PartialStateConflictError:",
            "                # The room was un-partial stated while we were processing the event.",
            "                # Try once more, with full state this time.",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "",
            "                # We ought to have full state now, barring some unlikely race where we left and",
            "                # rejoned the room in the background.",
            "                if context.partial_state:",
            "                    raise AssertionError(",
            "                        f\"Event {event.event_id} still has a partial resolved state \"",
            "                        f\"after room {event.room_id} was un-partial stated\"",
            "                    )",
            "",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "        except FederationError as e:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(e)",
            "            )",
            "",
            "            if e.code == 403:",
            "                logger.warning(\"Pulled event %s failed history check.\", event_id)",
            "            else:",
            "                raise",
            "        except Exception as exc:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(exc)",
            "            )",
            "            raise exc",
            "",
            "    @trace",
            "    async def _compute_event_context_with_maybe_missing_prevs(",
            "        self, dest: str, event: EventBase",
            "    ) -> EventContext:",
            "        \"\"\"Build an EventContext structure for a non-outlier event whose prev_events may",
            "        be missing.",
            "",
            "        This is used when we have pulled a batch of events from a remote server, and may",
            "        not have all the prev_events.",
            "",
            "        To build an EventContext, we need to calculate the state before the event. If we",
            "        already have all the prev_events for `event`, we can simply use the state after",
            "        the prev_events to calculate the state before `event`.",
            "",
            "        Otherwise, the missing prevs become new backwards extremities, and we fall back",
            "        to asking the remote server for the state after each missing `prev_event`,",
            "        and resolving across them.",
            "",
            "        That's ok provided we then resolve the state against other bits of the DAG",
            "        before using it - in other words, that the received event `event` is not going",
            "        to become the only forwards_extremity in the room (which will ensure that you",
            "        can't just take over a room by sending an event, withholding its prev_events,",
            "        and declaring yourself to be an admin in the subsequent state request).",
            "",
            "        In other words: we should only call this method if `event` has been *pulled*",
            "        as part of a batch of missing prev events, or similar.",
            "",
            "        Params:",
            "            dest: the remote server to ask for state at the missing prevs. Typically,",
            "                this will be the server we got `event` from.",
            "            event: an event to check for missing prevs.",
            "",
            "        Returns:",
            "            The event context.",
            "",
            "        Raises:",
            "            FederationError if we fail to get the state from the remote server after any",
            "                missing `prev_event`s.",
            "        \"\"\"",
            "        room_id = event.room_id",
            "        event_id = event.event_id",
            "",
            "        prevs = set(event.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        if not missing_prevs:",
            "            return await self._state_handler.compute_event_context(event)",
            "",
            "        logger.info(",
            "            \"Event %s is missing prev_events %s: calculating state for a \"",
            "            \"backwards extremity\",",
            "            event_id,",
            "            shortstr(missing_prevs),",
            "        )",
            "        # Calculate the state after each of the previous events, and",
            "        # resolve them to find the correct state at the current event.",
            "",
            "        try:",
            "            # Determine whether we may be about to retrieve partial state",
            "            # Events may be un-partial stated right after we compute the partial state",
            "            # flag, but that's okay, as long as the flag errs on the conservative side.",
            "            partial_state_flags = await self._store.get_partial_state_events(seen)",
            "            partial_state = any(partial_state_flags.values())",
            "",
            "            # Get the state of the events we know about",
            "            ours = await self._state_storage_controller.get_state_groups_ids(",
            "                room_id, seen, await_full_state=False",
            "            )",
            "",
            "            # state_maps is a list of mappings from (type, state_key) to event_id",
            "            state_maps: List[StateMap[str]] = list(ours.values())",
            "",
            "            # we don't need this any more, let's delete it.",
            "            del ours",
            "",
            "            # Ask the remote server for the states we don't",
            "            # know about",
            "            for p in missing_prevs:",
            "                logger.info(\"Requesting state after missing prev_event %s\", p)",
            "",
            "                with nested_logging_context(p):",
            "                    # note that if any of the missing prevs share missing state or",
            "                    # auth events, the requests to fetch those events are deduped",
            "                    # by the get_pdu_cache in federation_client.",
            "                    remote_state_map = (",
            "                        await self._get_state_ids_after_missing_prev_event(",
            "                            dest, room_id, p",
            "                        )",
            "                    )",
            "",
            "                    state_maps.append(remote_state_map)",
            "",
            "            room_version = await self._store.get_room_version_id(room_id)",
            "            state_map = await self._state_resolution_handler.resolve_events_with_store(",
            "                room_id,",
            "                room_version,",
            "                state_maps,",
            "                event_map={event_id: event},",
            "                state_res_store=StateResolutionStore(self._store),",
            "            )",
            "",
            "        except Exception:",
            "            logger.warning(",
            "                \"Error attempting to resolve state at missing prev_events\",",
            "                exc_info=True,",
            "            )",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                403,",
            "                \"We can't get valid state history.\",",
            "                affected=event_id,",
            "            )",
            "        return await self._state_handler.compute_event_context(",
            "            event, state_ids_before_event=state_map, partial_state=partial_state",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_ids_after_missing_prev_event(",
            "        self,",
            "        destination: str,",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> StateMap[str]:",
            "        \"\"\"Requests all of the room state at a given event from a remote homeserver.",
            "",
            "        Args:",
            "            destination: The remote homeserver to query for the state.",
            "            room_id: The id of the room we're interested in.",
            "            event_id: The id of the event we want the state at.",
            "",
            "        Returns:",
            "            The event ids of the state *after* the given event.",
            "",
            "        Raises:",
            "            InvalidResponseError: if the remote homeserver's response contains fields",
            "                of the wrong type.",
            "        \"\"\"",
            "",
            "        # It would be better if we could query the difference from our known",
            "        # state to the given `event_id` so the sending server doesn't have to",
            "        # send as much and we don't have to process as many events. For example",
            "        # in a room like #matrix:matrix.org, we get 200k events (77k state_events, 122k",
            "        # auth_events) from this call.",
            "        #",
            "        # Tracked by https://github.com/matrix-org/synapse/issues/13618",
            "        (",
            "            state_event_ids,",
            "            auth_event_ids,",
            "        ) = await self._federation_client.get_room_state_ids(",
            "            destination, room_id, event_id=event_id",
            "        )",
            "",
            "        logger.debug(",
            "            \"state_ids returned %i state events, %i auth events\",",
            "            len(state_event_ids),",
            "            len(auth_event_ids),",
            "        )",
            "",
            "        # Start by checking events we already have in the DB",
            "        desired_events = set(state_event_ids)",
            "        desired_events.add(event_id)",
            "        logger.debug(\"Fetching %i events from cache/store\", len(desired_events))",
            "        have_events = await self._store.have_seen_events(room_id, desired_events)",
            "",
            "        missing_desired_event_ids = desired_events - have_events",
            "        logger.debug(",
            "            \"We are missing %i events (got %i)\",",
            "            len(missing_desired_event_ids),",
            "            len(have_events),",
            "        )",
            "",
            "        # We probably won't need most of the auth events, so let's just check which",
            "        # we have for now, rather than thrashing the event cache with them all",
            "        # unnecessarily.",
            "",
            "        # TODO: we probably won't actually need all of the auth events, since we",
            "        #   already have a bunch of the state events. It would be nice if the",
            "        #   federation api gave us a way of finding out which we actually need.",
            "",
            "        missing_auth_event_ids = set(auth_event_ids) - have_events",
            "        missing_auth_event_ids.difference_update(",
            "            await self._store.have_seen_events(room_id, missing_auth_event_ids)",
            "        )",
            "        logger.debug(\"We are also missing %i auth events\", len(missing_auth_event_ids))",
            "",
            "        missing_event_ids = missing_desired_event_ids | missing_auth_event_ids",
            "",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids\",",
            "            str(missing_auth_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids.length\",",
            "            str(len(missing_auth_event_ids)),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids\",",
            "            str(missing_desired_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids.length\",",
            "            str(len(missing_desired_event_ids)),",
            "        )",
            "",
            "        # Making an individual request for each of 1000s of events has a lot of",
            "        # overhead. On the other hand, we don't really want to fetch all of the events",
            "        # if we already have most of them.",
            "        #",
            "        # As an arbitrary heuristic, if we are missing more than 10% of the events, then",
            "        # we fetch the whole state.",
            "        #",
            "        # TODO: might it be better to have an API which lets us do an aggregate event",
            "        #   request",
            "        if (len(missing_event_ids) * 10) >= len(auth_event_ids) + len(state_event_ids):",
            "            logger.debug(\"Requesting complete state from remote\")",
            "            await self._get_state_and_persist(destination, room_id, event_id)",
            "        else:",
            "            logger.debug(\"Fetching %i events from remote\", len(missing_event_ids))",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=missing_event_ids",
            "            )",
            "",
            "        # We now need to fill out the state map, which involves fetching the",
            "        # type and state key for each event ID in the state.",
            "        state_map = {}",
            "",
            "        event_metadata = await self._store.get_metadata_for_events(state_event_ids)",
            "        for state_event_id, metadata in event_metadata.items():",
            "            if metadata.room_id != room_id:",
            "                # This is a bogus situation, but since we may only discover it a long time",
            "                # after it happened, we try our best to carry on, by just omitting the",
            "                # bad events from the returned state set.",
            "                #",
            "                # This can happen if a remote server claims that the state or",
            "                # auth_events at an event in room A are actually events in room B",
            "                logger.warning(",
            "                    \"Remote server %s claims event %s in room %s is an auth/state \"",
            "                    \"event in room %s\",",
            "                    destination,",
            "                    state_event_id,",
            "                    metadata.room_id,",
            "                    room_id,",
            "                )",
            "                continue",
            "",
            "            if metadata.state_key is None:",
            "                logger.warning(",
            "                    \"Remote server gave us non-state event in state: %s\", state_event_id",
            "                )",
            "                continue",
            "",
            "            state_map[(metadata.event_type, metadata.state_key)] = state_event_id",
            "",
            "        # if we couldn't get the prev event in question, that's a problem.",
            "        remote_event = await self._store.get_event(",
            "            event_id,",
            "            allow_none=True,",
            "            allow_rejected=True,",
            "            redact_behaviour=EventRedactBehaviour.as_is,",
            "        )",
            "        if not remote_event:",
            "            raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))",
            "",
            "        # missing state at that event is a warning, not a blocker",
            "        # XXX: this doesn't sound right? it means that we'll end up with incomplete",
            "        #   state.",
            "        failed_to_fetch = desired_events - event_metadata.keys()",
            "        # `event_id` could be missing from `event_metadata` because it's not necessarily",
            "        # a state event. We've already checked that we've fetched it above.",
            "        failed_to_fetch.discard(event_id)",
            "        if failed_to_fetch:",
            "            logger.warning(",
            "                \"Failed to fetch missing state events for %s %s\",",
            "                event_id,",
            "                failed_to_fetch,",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch\",",
            "                str(failed_to_fetch),",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch.length\",",
            "                str(len(failed_to_fetch)),",
            "            )",
            "",
            "        if remote_event.is_state() and remote_event.rejected_reason is None:",
            "            state_map[",
            "                (remote_event.type, remote_event.state_key)",
            "            ] = remote_event.event_id",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_and_persist(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"Get the complete room state at a given event, and persist any new events",
            "        as outliers\"\"\"",
            "        room_version = await self._store.get_room_version(room_id)",
            "        auth_events, state_events = await self._federation_client.get_room_state(",
            "            destination, room_id, event_id=event_id, room_version=room_version",
            "        )",
            "        logger.info(\"/state returned %i events\", len(auth_events) + len(state_events))",
            "",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state_events)",
            "        )",
            "",
            "        # we also need the event itself.",
            "        if not await self._store.have_seen_event(room_id, event_id):",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=(event_id,)",
            "            )",
            "",
            "    @trace",
            "    async def _process_received_pdu(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        backfilled: bool = False,",
            "    ) -> None:",
            "        \"\"\"Called when we have a new non-outlier event.",
            "",
            "        This is called when we have a new event to add to the room DAG. This can be",
            "        due to:",
            "           * events received directly via a /send request",
            "           * events retrieved via get_missing_events after a /send request",
            "           * events backfilled after a client request.",
            "",
            "        It's not currently used for events received from incoming send_{join,knock,leave}",
            "        requests (which go via on_send_membership_event), nor for joins created by a",
            "        remote join dance (which go via process_remote_join).",
            "",
            "        We need to do auth checks and put it through the StateHandler.",
            "",
            "        Args:",
            "            origin: server sending the event",
            "",
            "            event: event to be persisted",
            "",
            "            context: The `EventContext` to persist the event with.",
            "",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "",
            "        PartialStateConflictError: if the room was un-partial stated in between",
            "            computing the state at the event and persisting it. The caller should",
            "            recompute `context` and retry exactly once when this happens.",
            "        \"\"\"",
            "        logger.debug(\"Processing event: %s\", event)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        try:",
            "            await self._check_event_auth(origin, event, context)",
            "        except AuthError as e:",
            "            # This happens only if we couldn't find the auth events. We'll already have",
            "            # logged a warning, so now we just convert to a FederationError.",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)",
            "",
            "        if not backfilled and not context.rejected:",
            "            # For new (non-backfilled and non-outlier) events we check if the event",
            "            # passes auth based on the current state. If it doesn't then we",
            "            # \"soft-fail\" the event.",
            "            await self._check_for_soft_fail(event, context=context, origin=origin)",
            "",
            "        await self._run_push_actions_and_persist_event(event, context, backfilled)",
            "",
            "        await self._handle_marker_event(origin, event)",
            "",
            "        if backfilled or context.rejected:",
            "            return",
            "",
            "        await self._maybe_kick_guest_users(event)",
            "",
            "        # For encrypted messages we check that we know about the sending device,",
            "        # if we don't then we mark the device cache for that user as stale.",
            "        if event.type == EventTypes.Encrypted:",
            "            device_id = event.content.get(\"device_id\")",
            "            sender_key = event.content.get(\"sender_key\")",
            "",
            "            cached_devices = await self._store.get_cached_devices_for_user(event.sender)",
            "",
            "            resync = False  # Whether we should resync device lists.",
            "",
            "            device = None",
            "            if device_id is not None:",
            "                device = cached_devices.get(device_id)",
            "                if device is None:",
            "                    logger.info(",
            "                        \"Received event from remote device not in our cache: %s %s\",",
            "                        event.sender,",
            "                        device_id,",
            "                    )",
            "                    resync = True",
            "",
            "            # We also check if the `sender_key` matches what we expect.",
            "            if sender_key is not None:",
            "                # Figure out what sender key we're expecting. If we know the",
            "                # device and recognize the algorithm then we can work out the",
            "                # exact key to expect. Otherwise check it matches any key we",
            "                # have for that device.",
            "",
            "                current_keys: Container[str] = []",
            "",
            "                if device:",
            "                    keys = device.get(\"keys\", {}).get(\"keys\", {})",
            "",
            "                    if (",
            "                        event.content.get(\"algorithm\")",
            "                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2",
            "                    ):",
            "                        # For this algorithm we expect a curve25519 key.",
            "                        key_name = \"curve25519:%s\" % (device_id,)",
            "                        current_keys = [keys.get(key_name)]",
            "                    else:",
            "                        # We don't know understand the algorithm, so we just",
            "                        # check it matches a key for the device.",
            "                        current_keys = keys.values()",
            "                elif device_id:",
            "                    # We don't have any keys for the device ID.",
            "                    pass",
            "                else:",
            "                    # The event didn't include a device ID, so we just look for",
            "                    # keys across all devices.",
            "                    current_keys = [",
            "                        key",
            "                        for device in cached_devices.values()",
            "                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()",
            "                    ]",
            "",
            "                # We now check that the sender key matches (one of) the expected",
            "                # keys.",
            "                if sender_key not in current_keys:",
            "                    logger.info(",
            "                        \"Received event from remote device with unexpected sender key: %s %s: %s\",",
            "                        event.sender,",
            "                        device_id or \"<no device_id>\",",
            "                        sender_key,",
            "                    )",
            "                    resync = True",
            "",
            "            if resync:",
            "                run_as_background_process(",
            "                    \"resync_device_due_to_pdu\",",
            "                    self._resync_device,",
            "                    event.sender,",
            "                )",
            "",
            "    async def _resync_device(self, sender: str) -> None:",
            "        \"\"\"We have detected that the device list for the given user may be out",
            "        of sync, so we try and resync them.",
            "        \"\"\"",
            "",
            "        try:",
            "            await self._store.mark_remote_user_device_cache_as_stale(sender)",
            "",
            "            # Immediately attempt a resync in the background",
            "            if self._config.worker.worker_app:",
            "                await self._user_device_resync(user_id=sender)",
            "            else:",
            "                await self._device_list_updater.user_device_resync(sender)",
            "        except Exception:",
            "            logger.exception(\"Failed to resync device for %s\", sender)",
            "",
            "    @trace",
            "    async def _handle_marker_event(self, origin: str, marker_event: EventBase) -> None:",
            "        \"\"\"Handles backfilling the insertion event when we receive a marker",
            "        event that points to one.",
            "",
            "        Args:",
            "            origin: Origin of the event. Will be called to get the insertion event",
            "            marker_event: The event to process",
            "        \"\"\"",
            "",
            "        if marker_event.type != EventTypes.MSC2716_MARKER:",
            "            # Not a marker event",
            "            return",
            "",
            "        if marker_event.rejected_reason is not None:",
            "            # Rejected event",
            "            return",
            "",
            "        # Skip processing a marker event if the room version doesn't",
            "        # support it or the event is not from the room creator.",
            "        room_version = await self._store.get_room_version(marker_event.room_id)",
            "        create_event = await self._store.get_create_event_for_room(marker_event.room_id)",
            "        room_creator = create_event.content.get(EventContentFields.ROOM_CREATOR)",
            "        if not room_version.msc2716_historical and (",
            "            not self._config.experimental.msc2716_enabled",
            "            or marker_event.sender != room_creator",
            "        ):",
            "            return",
            "",
            "        logger.debug(\"_handle_marker_event: received %s\", marker_event)",
            "",
            "        insertion_event_id = marker_event.content.get(",
            "            EventContentFields.MSC2716_INSERTION_EVENT_REFERENCE",
            "        )",
            "",
            "        if insertion_event_id is None:",
            "            # Nothing to retrieve then (invalid marker)",
            "            return",
            "",
            "        already_seen_insertion_event = await self._store.have_seen_event(",
            "            marker_event.room_id, insertion_event_id",
            "        )",
            "        if already_seen_insertion_event:",
            "            # No need to process a marker again if we have already seen the",
            "            # insertion event that it was pointing to",
            "            return",
            "",
            "        logger.debug(",
            "            \"_handle_marker_event: backfilling insertion event %s\", insertion_event_id",
            "        )",
            "",
            "        await self._get_events_and_persist(",
            "            origin,",
            "            marker_event.room_id,",
            "            [insertion_event_id],",
            "        )",
            "",
            "        insertion_event = await self._store.get_event(",
            "            insertion_event_id, allow_none=True",
            "        )",
            "        if insertion_event is None:",
            "            logger.warning(",
            "                \"_handle_marker_event: server %s didn't return insertion event %s for marker %s\",",
            "                origin,",
            "                insertion_event_id,",
            "                marker_event.event_id,",
            "            )",
            "            return",
            "",
            "        logger.debug(",
            "            \"_handle_marker_event: succesfully backfilled insertion event %s from marker event %s\",",
            "            insertion_event,",
            "            marker_event,",
            "        )",
            "",
            "        await self._store.insert_insertion_extremity(",
            "            insertion_event_id, marker_event.room_id",
            "        )",
            "",
            "        logger.debug(",
            "            \"_handle_marker_event: insertion extremity added for %s from marker event %s\",",
            "            insertion_event,",
            "            marker_event,",
            "        )",
            "",
            "    async def backfill_event_id(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> EventBase:",
            "        \"\"\"Backfill a single event and persist it as a non-outlier which means",
            "        we also pull in all of the state and auth events necessary for it.",
            "",
            "        Args:",
            "            destination: The homeserver to pull the given event_id from.",
            "            room_id: The room where the event is from.",
            "            event_id: The event ID to backfill.",
            "",
            "        Raises:",
            "            FederationError if we are unable to find the event from the destination",
            "        \"\"\"",
            "        logger.info(",
            "            \"backfill_event_id: event_id=%s from destination=%s\", event_id, destination",
            "        )",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        event_from_response = await self._federation_client.get_pdu(",
            "            [destination],",
            "            event_id,",
            "            room_version,",
            "        )",
            "",
            "        if not event_from_response:",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                404,",
            "                \"Unable to find event_id=%s from destination=%s to backfill.\"",
            "                % (event_id, destination),",
            "                affected=event_id,",
            "            )",
            "",
            "        # Persist the event we just fetched, including pulling all of the state",
            "        # and auth events to de-outlier it. This also sets up the necessary",
            "        # `state_groups` for the event.",
            "        await self._process_pulled_events(",
            "            destination,",
            "            [event_from_response],",
            "            # Prevent notifications going to clients",
            "            backfilled=True,",
            "        )",
            "",
            "        return event_from_response",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_events_and_persist(",
            "        self, destination: str, room_id: str, event_ids: Collection[str]",
            "    ) -> None:",
            "        \"\"\"Fetch the given events from a server, and persist them as outliers.",
            "",
            "        This function *does not* recursively get missing auth events of the",
            "        newly fetched events. Callers must include in the `event_ids` argument",
            "        any missing events from the auth chain.",
            "",
            "        Logs a warning if we can't find the given event.",
            "        \"\"\"",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        events: List[EventBase] = []",
            "",
            "        async def get_event(event_id: str) -> None:",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    event = await self._federation_client.get_pdu(",
            "                        [destination],",
            "                        event_id,",
            "                        room_version,",
            "                    )",
            "                    if event is None:",
            "                        logger.warning(",
            "                            \"Server %s didn't return event %s\",",
            "                            destination,",
            "                            event_id,",
            "                        )",
            "                        return",
            "                    events.append(event)",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"Error fetching missing state/auth event %s: %s %s\",",
            "                        event_id,",
            "                        type(e),",
            "                        e,",
            "                    )",
            "",
            "        await concurrently_execute(get_event, event_ids, 5)",
            "        logger.info(\"Fetched %i events of %i requested\", len(events), len(event_ids))",
            "        await self._auth_and_persist_outliers(room_id, events)",
            "",
            "    @trace",
            "    async def _auth_and_persist_outliers(",
            "        self, room_id: str, events: Iterable[EventBase]",
            "    ) -> None:",
            "        \"\"\"Persist a batch of outlier events fetched from remote servers.",
            "",
            "        We first sort the events to make sure that we process each event's auth_events",
            "        before the event itself.",
            "",
            "        We then mark the events as outliers, persist them to the database, and, where",
            "        appropriate (eg, an invite), awake the notifier.",
            "",
            "        Params:",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            events: the events that have been fetched",
            "        \"\"\"",
            "        event_map = {event.event_id: event for event in events}",
            "",
            "        event_ids = event_map.keys()",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str(event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(event_ids)),",
            "        )",
            "",
            "        # filter out any events we have already seen. This might happen because",
            "        # the events were eagerly pushed to us (eg, during a room join), or because",
            "        # another thread has raced against us since we decided to request the event.",
            "        #",
            "        # This is just an optimisation, so it doesn't need to be watertight - the event",
            "        # persister does another round of deduplication.",
            "        seen_remotes = await self._store.have_seen_events(room_id, event_map.keys())",
            "        for s in seen_remotes:",
            "            event_map.pop(s, None)",
            "",
            "        # XXX: it might be possible to kick this process off in parallel with fetching",
            "        # the events.",
            "        while event_map:",
            "            # build a list of events whose auth events are not in the queue.",
            "            roots = tuple(",
            "                ev",
            "                for ev in event_map.values()",
            "                if not any(aid in event_map for aid in ev.auth_event_ids())",
            "            )",
            "",
            "            if not roots:",
            "                # if *none* of the remaining events are ready, that means",
            "                # we have a loop. This either means a bug in our logic, or that",
            "                # somebody has managed to create a loop (which requires finding a",
            "                # hash collision in room v2 and later).",
            "                logger.warning(",
            "                    \"Loop found in auth events while fetching missing state/auth \"",
            "                    \"events: %s\",",
            "                    shortstr(event_map.keys()),",
            "                )",
            "                return",
            "",
            "            logger.info(",
            "                \"Persisting %i of %i remaining outliers: %s\",",
            "                len(roots),",
            "                len(event_map),",
            "                shortstr(e.event_id for e in roots),",
            "            )",
            "",
            "            await self._auth_and_persist_outliers_inner(room_id, roots)",
            "",
            "            for ev in roots:",
            "                del event_map[ev.event_id]",
            "",
            "    async def _auth_and_persist_outliers_inner(",
            "        self, room_id: str, fetched_events: Collection[EventBase]",
            "    ) -> None:",
            "        \"\"\"Helper for _auth_and_persist_outliers",
            "",
            "        Persists a batch of events where we have (theoretically) already persisted all",
            "        of their auth events.",
            "",
            "        Marks the events as outliers, auths them, persists them to the database, and,",
            "        where appropriate (eg, an invite), awakes the notifier.",
            "",
            "        Params:",
            "            origin: where the events came from",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            fetched_events: the events to persist",
            "        \"\"\"",
            "        # get all the auth events for all the events in this batch. By now, they should",
            "        # have been persisted.",
            "        auth_events = {",
            "            aid for event in fetched_events for aid in event.auth_event_ids()",
            "        }",
            "        persisted_events = await self._store.get_events(",
            "            auth_events,",
            "            allow_rejected=True,",
            "        )",
            "",
            "        events_and_contexts_to_persist: List[Tuple[EventBase, EventContext]] = []",
            "",
            "        async def prep(event: EventBase) -> None:",
            "            with nested_logging_context(suffix=event.event_id):",
            "                auth = []",
            "                for auth_event_id in event.auth_event_ids():",
            "                    ae = persisted_events.get(auth_event_id)",
            "                    if not ae:",
            "                        # the fact we can't find the auth event doesn't mean it doesn't",
            "                        # exist, which means it is premature to reject `event`. Instead we",
            "                        # just ignore it for now.",
            "                        logger.warning(",
            "                            \"Dropping event %s, which relies on auth_event %s, which could not be found\",",
            "                            event,",
            "                            auth_event_id,",
            "                        )",
            "                        return",
            "                    auth.append(ae)",
            "",
            "                # we're not bothering about room state, so flag the event as an outlier.",
            "                event.internal_metadata.outlier = True",
            "",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    await check_state_independent_auth_rules(self._store, event)",
            "                    check_state_dependent_auth_rules(event, auth)",
            "                except AuthError as e:",
            "                    logger.warning(\"Rejecting %r because %s\", event, e)",
            "                    context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "            events_and_contexts_to_persist.append((event, context))",
            "",
            "        for event in fetched_events:",
            "            await prep(event)",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            events_and_contexts_to_persist,",
            "            # Mark these events backfilled as they're historic events that will",
            "            # eventually be backfilled. For example, missing events we fetch",
            "            # during backfill should be marked as backfilled as well.",
            "            backfilled=True,",
            "        )",
            "",
            "    @trace",
            "    async def _check_event_auth(",
            "        self, origin: Optional[str], event: EventBase, context: EventContext",
            "    ) -> None:",
            "        \"\"\"",
            "        Checks whether an event should be rejected (for failing auth checks).",
            "",
            "        Args:",
            "            origin: The host the event originates from. This is used to fetch",
            "               any missing auth events. It can be set to None, but only if we are",
            "               sure that we already have all the auth events.",
            "            event: The event itself.",
            "            context:",
            "                The event context.",
            "",
            "        Raises:",
            "            AuthError if we were unable to find copies of the event's auth events.",
            "               (Most other failures just cause us to set `context.rejected`.)",
            "        \"\"\"",
            "        # This method should only be used for non-outliers",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # first of all, check that the event itself is valid.",
            "        try:",
            "            validate_event_for_room_version(event)",
            "        except AuthError as e:",
            "            logger.warning(\"While validating received event %r: %s\", event, e)",
            "            # TODO: use a different rejected reason here?",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "",
            "        # next, check that we have all of the event's auth events.",
            "        #",
            "        # Note that this can raise AuthError, which we want to propagate to the",
            "        # caller rather than swallow with `context.rejected` (since we cannot be",
            "        # certain that there is a permanent problem with the event).",
            "        claimed_auth_events = await self._load_or_fetch_auth_events_for_event(",
            "            origin, event",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events\",",
            "            str([ev.event_id for ev in claimed_auth_events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events.length\",",
            "            str(len(claimed_auth_events)),",
            "        )",
            "",
            "        # ... and check that the event passes auth at those auth events.",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   4. Passes authorization rules based on the event\u2019s auth events,",
            "        #      otherwise it is rejected.",
            "        try:",
            "            await check_state_independent_auth_rules(self._store, event)",
            "            check_state_dependent_auth_rules(event, claimed_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against auth_events: %s\", event, e",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "",
            "        # now check the auth rules pass against the room state before the event",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   5. Passes authorization rules based on the state before the event,",
            "        #      otherwise it is rejected.",
            "        #",
            "        # ... however, if we only have partial state for the room, then there is a good",
            "        # chance that we'll be missing some of the state needed to auth the new event.",
            "        # So, we state-resolve the auth events that we are given against the state that",
            "        # we know about, which ensures things like bans are applied. (Note that we'll",
            "        # already have checked we have all the auth events, in",
            "        # _load_or_fetch_auth_events_for_event above)",
            "        if context.partial_state:",
            "            room_version = await self._store.get_room_version_id(event.room_id)",
            "",
            "            local_state_id_map = await context.get_prev_state_ids()",
            "            claimed_auth_events_id_map = {",
            "                (ev.type, ev.state_key): ev.event_id for ev in claimed_auth_events",
            "            }",
            "",
            "            state_for_auth_id_map = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    [local_state_id_map, claimed_auth_events_id_map],",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "            state_for_auth_id_map = await context.get_prev_state_ids(",
            "                StateFilter.from_types(event_types)",
            "            )",
            "",
            "        calculated_auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "            event, state_for_auth_id_map, for_verification=True",
            "        )",
            "",
            "        # if those are the same, we're done here.",
            "        if collections.Counter(event.auth_event_ids()) == collections.Counter(",
            "            calculated_auth_event_ids",
            "        ):",
            "            return",
            "",
            "        # otherwise, re-run the auth checks based on what we calculated.",
            "        calculated_auth_events = await self._store.get_events_as_list(",
            "            calculated_auth_event_ids",
            "        )",
            "",
            "        # log the differences",
            "",
            "        claimed_auth_event_map = {(e.type, e.state_key): e for e in claimed_auth_events}",
            "        calculated_auth_event_map = {",
            "            (e.type, e.state_key): e for e in calculated_auth_events",
            "        }",
            "        logger.info(",
            "            \"event's auth_events are different to our calculated auth_events. \"",
            "            \"Claimed but not calculated: %s. Calculated but not claimed: %s\",",
            "            [",
            "                ev",
            "                for k, ev in claimed_auth_event_map.items()",
            "                if k not in calculated_auth_event_map",
            "                or calculated_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "            [",
            "                ev",
            "                for k, ev in calculated_auth_event_map.items()",
            "                if k not in claimed_auth_event_map",
            "                or claimed_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, calculated_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against room state before the event: %s\",",
            "                event,",
            "                e,",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "    @trace",
            "    async def _maybe_kick_guest_users(self, event: EventBase) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state = await self._storage_controllers.state.get_current_state(",
            "            event.room_id",
            "        )",
            "        current_state_list = list(current_state.values())",
            "        await self._get_room_member_handler().kick_guest_users(current_state_list)",
            "",
            "    async def _check_for_soft_fail(",
            "        self,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        origin: str,",
            "    ) -> None:",
            "        \"\"\"Checks if we should soft fail the event; if so, marks the event as",
            "        such.",
            "",
            "        Does nothing for events in rooms with partial state, since we may not have an",
            "        accurate membership event for the sender in the current state.",
            "",
            "        Args:",
            "            event",
            "            context: The `EventContext` which we are about to persist the event with.",
            "            origin: The host the event originates from.",
            "        \"\"\"",
            "        if await self._store.is_partial_state_room(event.room_id):",
            "            # We might not know the sender's membership in the current state, so don't",
            "            # soft fail anything. Even if we do have a membership for the sender in the",
            "            # current state, it may have been derived from state resolution between",
            "            # partial and full state and may not be accurate.",
            "            return",
            "",
            "        extrem_ids_list = await self._store.get_latest_event_ids_in_room(event.room_id)",
            "        extrem_ids = set(extrem_ids_list)",
            "        prev_event_ids = set(event.prev_event_ids())",
            "",
            "        if extrem_ids == prev_event_ids:",
            "            # If they're the same then the current state is the same as the",
            "            # state at the event, so no point rechecking auth for soft fail.",
            "            return",
            "",
            "        room_version = await self._store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        # The event types we want to pull from the \"current\" state.",
            "        auth_types = auth_types_for_event(room_version_obj, event)",
            "",
            "        # Calculate the \"current state\".",
            "        seen_event_ids = await self._store.have_events_in_timeline(prev_event_ids)",
            "        has_missing_prevs = bool(prev_event_ids - seen_event_ids)",
            "        if has_missing_prevs:",
            "            # We don't have all the prev_events of this event, which means we have a",
            "            # gap in the graph, and the new event is going to become a new backwards",
            "            # extremity.",
            "            #",
            "            # In this case we want to be a little careful as we might have been",
            "            # down for a while and have an incorrect view of the current state,",
            "            # however we still want to do checks as gaps are easy to",
            "            # maliciously manufacture.",
            "            #",
            "            # So we use a \"current state\" that is actually a state",
            "            # resolution across the current forward extremities and the",
            "            # given state at the event. This should correctly handle cases",
            "            # like bans, especially with state res v2.",
            "",
            "            state_sets_d = await self._state_storage_controller.get_state_groups_ids(",
            "                event.room_id, extrem_ids",
            "            )",
            "            state_sets: List[StateMap[str]] = list(state_sets_d.values())",
            "            state_ids = await context.get_prev_state_ids()",
            "            state_sets.append(state_ids)",
            "            current_state_ids = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    state_sets,",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            current_state_ids = (",
            "                await self._state_storage_controller.get_current_state_ids(",
            "                    event.room_id, StateFilter.from_types(auth_types)",
            "                )",
            "            )",
            "",
            "        logger.debug(",
            "            \"Doing soft-fail check for %s: state %s\",",
            "            event.event_id,",
            "            current_state_ids,",
            "        )",
            "",
            "        # Now check if event pass auth against said current state",
            "        current_state_ids_list = [",
            "            e for k, e in current_state_ids.items() if k in auth_types",
            "        ]",
            "        current_auth_events = await self._store.get_events_as_list(",
            "            current_state_ids_list",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, current_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"Soft-failing %r (from %s) because %s\",",
            "                event,",
            "                e,",
            "                origin,",
            "                extra={",
            "                    \"room_id\": event.room_id,",
            "                    \"mxid\": event.sender,",
            "                    \"hs\": origin,",
            "                },",
            "            )",
            "            soft_failed_event_counter.inc()",
            "            event.internal_metadata.soft_failed = True",
            "",
            "    async def _load_or_fetch_auth_events_for_event(",
            "        self, destination: Optional[str], event: EventBase",
            "    ) -> Collection[EventBase]:",
            "        \"\"\"Fetch this event's auth_events, from database or remote",
            "",
            "        Loads any of the auth_events that we already have from the database/cache. If",
            "        there are any that are missing, calls /event_auth to get the complete auth",
            "        chain for the event (and then attempts to load the auth_events again).",
            "",
            "        If any of the auth_events cannot be found, raises an AuthError. This can happen",
            "        for a number of reasons; eg: the events don't exist, or we were unable to talk",
            "        to `destination`, or we couldn't validate the signature on the event (which",
            "        in turn has multiple potential causes).",
            "",
            "        Args:",
            "            destination: where to send the /event_auth request. Typically the server",
            "               that sent us `event` in the first place.",
            "",
            "               If this is None, no attempt is made to load any missing auth events:",
            "               rather, an AssertionError is raised if there are any missing events.",
            "",
            "            event: the event whose auth_events we want",
            "",
            "        Returns:",
            "            all of the events listed in `event.auth_events_ids`, after deduplication",
            "",
            "        Raises:",
            "            AssertionError if some auth events were missing and no `destination` was",
            "            supplied.",
            "",
            "            AuthError if we were unable to fetch the auth_events for any reason.",
            "        \"\"\"",
            "        event_auth_event_ids = set(event.auth_event_ids())",
            "        event_auth_events = await self._store.get_events(",
            "            event_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids = event_auth_event_ids.difference(",
            "            event_auth_events.keys()",
            "        )",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "        if destination is None:",
            "            # this shouldn't happen: destination must be set unless we know we have already",
            "            # persisted the auth events.",
            "            raise AssertionError(",
            "                \"_load_or_fetch_auth_events_for_event() called with no destination for \"",
            "                \"an event with missing auth_events\"",
            "            )",
            "",
            "        logger.info(",
            "            \"Event %s refers to unknown auth events %s: fetching auth chain\",",
            "            event,",
            "            missing_auth_event_ids,",
            "        )",
            "        try:",
            "            await self._get_remote_auth_chain_for_event(",
            "                destination, event.room_id, event.event_id",
            "            )",
            "        except Exception as e:",
            "            logger.warning(\"Failed to get auth chain for %s: %s\", event, e)",
            "            # in this case, it's very likely we still won't have all the auth",
            "            # events - but we pick that up below.",
            "",
            "        # try to fetch the auth events we missed list time.",
            "        extra_auth_events = await self._store.get_events(",
            "            missing_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids.difference_update(extra_auth_events.keys())",
            "        event_auth_events.update(extra_auth_events)",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "",
            "        # we still don't have all the auth events.",
            "        logger.warning(",
            "            \"Missing auth events for %s: %s\",",
            "            event,",
            "            shortstr(missing_auth_event_ids),",
            "        )",
            "        # the fact we can't find the auth event doesn't mean it doesn't",
            "        # exist, which means it is premature to store `event` as rejected.",
            "        # instead we raise an AuthError, which will make the caller ignore it.",
            "        raise AuthError(code=HTTPStatus.FORBIDDEN, msg=\"Auth events could not be found\")",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_remote_auth_chain_for_event(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"If we are missing some of an event's auth events, attempt to request them",
            "",
            "        Args:",
            "            destination: where to fetch the auth tree from",
            "            room_id: the room in which we are lacking auth events",
            "            event_id: the event for which we are lacking auth events",
            "        \"\"\"",
            "        try:",
            "            remote_events = await self._federation_client.get_event_auth(",
            "                destination, room_id, event_id",
            "            )",
            "",
            "        except RequestSendFailed as e1:",
            "            # The other side isn't around or doesn't implement the",
            "            # endpoint, so lets just bail out.",
            "            logger.info(\"Failed to get event auth from remote: %s\", e1)",
            "            return",
            "",
            "        logger.info(\"/event_auth returned %i events\", len(remote_events))",
            "",
            "        # `event` may be returned, but we should not yet process it.",
            "        remote_auth_events = (e for e in remote_events if e.event_id != event_id)",
            "",
            "        await self._auth_and_persist_outliers(room_id, remote_auth_events)",
            "",
            "    @trace",
            "    async def _run_push_actions_and_persist_event(",
            "        self, event: EventBase, context: EventContext, backfilled: bool = False",
            "    ) -> None:",
            "        \"\"\"Run the push actions for a received event, and persist it.",
            "",
            "        Args:",
            "            event: The event itself.",
            "            context: The event context.",
            "            backfilled: True if the event was backfilled.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "        # this method should not be called on outliers (those code paths call",
            "        # persist_events_and_notify directly.)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        if not backfilled and not context.rejected:",
            "            min_depth = await self._store.get_min_depth(event.room_id)",
            "            if min_depth is None or min_depth > event.depth:",
            "                # XXX richvdh 2021/10/07: I don't really understand what this",
            "                # condition is doing. I think it's trying not to send pushes",
            "                # for events that predate our join - but that's not really what",
            "                # min_depth means, and anyway ancient events are a more general",
            "                # problem.",
            "                #",
            "                # for now I'm just going to log about it.",
            "                logger.info(",
            "                    \"Skipping push actions for old event with depth %s < %s\",",
            "                    event.depth,",
            "                    min_depth,",
            "                )",
            "            else:",
            "                await self._bulk_push_rule_evaluator.action_for_event_by_user(",
            "                    event, context",
            "                )",
            "",
            "        try:",
            "            await self.persist_events_and_notify(",
            "                event.room_id, [(event, context)], backfilled=backfilled",
            "            )",
            "        except Exception:",
            "            await self._store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def persist_events_and_notify(",
            "        self,",
            "        room_id: str,",
            "        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],",
            "        backfilled: bool = False,",
            "    ) -> int:",
            "        \"\"\"Persists events and tells the notifier/pushers about them, if",
            "        necessary.",
            "",
            "        Args:",
            "            room_id: The room ID of events being persisted.",
            "            event_and_contexts: Sequence of events with their associated",
            "                context that should be persisted. All events must belong to",
            "                the same room.",
            "            backfilled: Whether these events are a result of",
            "                backfilling or not",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        if not event_and_contexts:",
            "            return self._store.get_room_max_stream_ordering()",
            "",
            "        instance = self._config.worker.events_shard_config.get_instance(room_id)",
            "        if instance != self._instance_name:",
            "            # Limit the number of events sent over replication. We choose 200",
            "            # here as that is what we default to in `max_request_body_size(..)`",
            "            try:",
            "                for batch in batch_iter(event_and_contexts, 200):",
            "                    result = await self._send_events(",
            "                        instance_name=instance,",
            "                        store=self._store,",
            "                        room_id=room_id,",
            "                        event_and_contexts=batch,",
            "                        backfilled=backfilled,",
            "                    )",
            "            except SynapseError as e:",
            "                if e.code == HTTPStatus.CONFLICT:",
            "                    raise PartialStateConflictError()",
            "                raise",
            "            return result[\"max_stream_id\"]",
            "        else:",
            "            assert self._storage_controllers.persistence",
            "",
            "            # Note that this returns the events that were persisted, which may not be",
            "            # the same as were passed in if some were deduplicated due to transaction IDs.",
            "            (",
            "                events,",
            "                max_stream_token,",
            "            ) = await self._storage_controllers.persistence.persist_events(",
            "                event_and_contexts, backfilled=backfilled",
            "            )",
            "",
            "            if self._ephemeral_messages_enabled:",
            "                for event in events:",
            "                    # If there's an expiry timestamp on the event, schedule its expiry.",
            "                    self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            if not backfilled:  # Never notify for backfilled events",
            "                with start_active_span(\"notify_persisted_events\"):",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids\",",
            "                        str([ev.event_id for ev in events]),",
            "                    )",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids.length\",",
            "                        str(len(events)),",
            "                    )",
            "                    for event in events:",
            "                        await self._notify_persisted_event(event, max_stream_token)",
            "",
            "            return max_stream_token.stream",
            "",
            "    async def _notify_persisted_event(",
            "        self, event: EventBase, max_stream_token: RoomStreamToken",
            "    ) -> None:",
            "        \"\"\"Checks to see if notifier/pushers should be notified about the",
            "        event or not.",
            "",
            "        Args:",
            "            event:",
            "            max_stream_token: The max_stream_id returned by persist_events",
            "        \"\"\"",
            "",
            "        extra_users = []",
            "        if event.type == EventTypes.Member:",
            "            target_user_id = event.state_key",
            "",
            "            # We notify for memberships if its an invite for one of our",
            "            # users",
            "            if event.internal_metadata.is_outlier():",
            "                if event.membership != Membership.INVITE:",
            "                    if not self._is_mine_id(target_user_id):",
            "                        return",
            "",
            "            target_user = UserID.from_string(target_user_id)",
            "            extra_users.append(target_user)",
            "        elif event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        # the event has been persisted so it should have a stream ordering.",
            "        assert event.internal_metadata.stream_ordering",
            "",
            "        event_pos = PersistedEventPosition(",
            "            self._instance_name, event.internal_metadata.stream_ordering",
            "        )",
            "        await self._notifier.on_new_room_event(",
            "            event, event_pos, max_stream_token, extra_users=extra_users",
            "        )",
            "",
            "        if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "            # TODO retrieve the previous state, and exclude join -> join transitions",
            "            self._notifier.notify_user_joined_room(event.event_id, event.room_id)",
            "",
            "    def _sanity_check_event(self, ev: EventBase) -> None:",
            "        \"\"\"",
            "        Do some early sanity checks of a received event",
            "",
            "        In particular, checks it doesn't have an excessive number of",
            "        prev_events or auth_events, which could cause a huge state resolution",
            "        or cascade of event fetches.",
            "",
            "        Args:",
            "            ev: event to be checked",
            "",
            "        Raises:",
            "            SynapseError if the event does not pass muster",
            "        \"\"\"",
            "        if len(ev.prev_event_ids()) > 20:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i prev_events\",",
            "                ev.event_id,",
            "                len(ev.prev_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")",
            "",
            "        if len(ev.auth_event_ids()) > 10:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i auth_events\",",
            "                ev.event_id,",
            "                len(ev.auth_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")"
        ],
        "afterPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import collections",
            "import itertools",
            "import logging",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Collection,",
            "    Container,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Sequence,",
            "    Set,",
            "    Tuple,",
            ")",
            "",
            "from prometheus_client import Counter, Histogram",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    Membership,",
            "    RejectedReason,",
            "    RoomEncryptionAlgorithms,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    HttpResponseException,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions",
            "from synapse.event_auth import (",
            "    auth_types_for_event,",
            "    check_state_dependent_auth_rules,",
            "    check_state_independent_auth_rules,",
            "    validate_event_for_room_version,",
            ")",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_client import InvalidResponseError",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    set_tag,",
            "    start_active_span,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEventsRestServlet,",
            ")",
            "from synapse.state import StateResolutionStore",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.storage.state import StateFilter",
            "from synapse.types import (",
            "    PersistedEventPosition,",
            "    RoomStreamToken,",
            "    StateMap,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.iterutils import batch_iter",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import shortstr",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "soft_failed_event_counter = Counter(",
            "    \"synapse_federation_soft_failed_events_total\",",
            "    \"Events received over federation that we marked as soft_failed\",",
            ")",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_after_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_after_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.25,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        25.0,",
            "        30.0,",
            "        40.0,",
            "        50.0,",
            "        60.0,",
            "        80.0,",
            "        100.0,",
            "        120.0,",
            "        150.0,",
            "        180.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "class FederationEventHandler:",
            "    \"\"\"Handles events that originated from federation.",
            "",
            "    Responsible for handing incoming events and passing them on to the rest",
            "    of the homeserver (including auth and state conflict resolutions)",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "",
            "        self._state_handler = hs.get_state_handler()",
            "        self._event_creation_handler = hs.get_event_creation_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._message_handler = hs.get_message_handler()",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "        self._state_resolution_handler = hs.get_state_resolution_handler()",
            "        # avoid a circular dependency by deferring execution here",
            "        self._get_room_member_handler = hs.get_room_member_handler",
            "",
            "        self._federation_client = hs.get_federation_client()",
            "        self._third_party_event_rules = hs.get_third_party_event_rules()",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._server_name = hs.hostname",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        self._config = hs.config",
            "        self._ephemeral_messages_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)",
            "        if hs.config.worker.worker_app:",
            "            self._user_device_resync = (",
            "                ReplicationUserDevicesResyncRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._device_list_updater = hs.get_device_handler().device_list_updater",
            "",
            "        # When joining a room we need to queue any events for that room up.",
            "        # For each room, a list of (pdu, origin) tuples.",
            "        # TODO: replace this with something more elegant, probably based around the",
            "        # federation event staging area.",
            "        self.room_queues: Dict[str, List[Tuple[EventBase, str]]] = {}",
            "",
            "        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")",
            "",
            "    async def on_receive_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received via a federation /send/ transaction",
            "",
            "        Args:",
            "            origin: server which initiated the /send/ transaction. Will",
            "                be used to fetch missing events or state.",
            "            pdu: received PDU",
            "        \"\"\"",
            "",
            "        # We should never see any outliers here.",
            "        assert not pdu.internal_metadata.outlier",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        # We reprocess pdus when we have seen them only as outliers",
            "        existing = await self._store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        # FIXME: Currently we fetch an event again when we already have it",
            "        # if it has been marked as an outlier.",
            "        if existing:",
            "            if not existing.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received event %s which we have already seen\", event_id",
            "                )",
            "                return",
            "            if pdu.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received outlier %s which we already have as an outlier\",",
            "                    event_id,",
            "                )",
            "                return",
            "            logger.info(\"De-outliering event %s\", event_id)",
            "",
            "        # do some initial sanity-checking of the event. In particular, make",
            "        # sure it doesn't have hundreds of prev_events or auth_events, which",
            "        # could cause a huge state resolution or cascade of event fetches.",
            "        try:",
            "            self._sanity_check_event(pdu)",
            "        except SynapseError as err:",
            "            logger.warning(\"Received event failed sanity checks\")",
            "            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)",
            "",
            "        # If we are currently in the process of joining this room, then we",
            "        # queue up events for later processing.",
            "        if room_id in self.room_queues:",
            "            logger.info(",
            "                \"Queuing PDU from %s for now: join in progress\",",
            "                origin,",
            "            )",
            "            self.room_queues[room_id].append((pdu, origin))",
            "            return",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        #",
            "        # Note that if we were never in the room then we would have already",
            "        # dropped the event, since we wouldn't know the room version.",
            "        is_in_room = await self._event_auth_handler.is_host_in_room(",
            "            room_id, self._server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Ignoring PDU from %s as we're not in the room\",",
            "                origin,",
            "            )",
            "            return None",
            "",
            "        # Try to fetch any missing prev events to fill in gaps in the graph",
            "        prevs = set(pdu.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        if missing_prevs:",
            "            # We only backfill backwards to the min depth.",
            "            min_depth = await self._store.get_min_depth(pdu.room_id)",
            "            logger.debug(\"min_depth: %d\", min_depth)",
            "",
            "            if min_depth is not None and pdu.depth > min_depth:",
            "                # If we're missing stuff, ensure we only fetch stuff one",
            "                # at a time.",
            "                logger.info(",
            "                    \"Acquiring room lock to fetch %d missing prev_events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                async with self._room_pdu_linearizer.queue(pdu.room_id):",
            "                    logger.info(",
            "                        \"Acquired room lock to fetch %d missing prev_events\",",
            "                        len(missing_prevs),",
            "                    )",
            "",
            "                    try:",
            "                        await self._get_missing_events_for_pdu(",
            "                            origin, pdu, prevs, min_depth",
            "                        )",
            "                    except Exception as e:",
            "                        raise Exception(",
            "                            \"Error fetching missing prev_events for %s: %s\"",
            "                            % (event_id, e)",
            "                        ) from e",
            "",
            "                # Update the set of things we've seen after trying to",
            "                # fetch the missing stuff",
            "                seen = await self._store.have_events_in_timeline(prevs)",
            "                missing_prevs = prevs - seen",
            "",
            "                if not missing_prevs:",
            "                    logger.info(\"Found all missing prev_events\")",
            "",
            "            if missing_prevs:",
            "                # since this event was pushed to us, it is possible for it to",
            "                # become the only forward-extremity in the room, and we would then",
            "                # trust its state to be the state for the whole room. This is very",
            "                # bad. Further, if the event was pushed to us, there is no excuse",
            "                # for us not to have all the prev_events. (XXX: apart from",
            "                # min_depth?)",
            "                #",
            "                # We therefore reject any such events.",
            "                logger.warning(",
            "                    \"Rejecting: failed to fetch %d prev events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                raise FederationError(",
            "                    \"ERROR\",",
            "                    403,",
            "                    (",
            "                        \"Your server isn't divulging details about prev_events \"",
            "                        \"referenced in this event.\"",
            "                    ),",
            "                    affected=pdu.event_id,",
            "                )",
            "",
            "        try:",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were processing the PDU.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated while processing the PDU, trying again.\",",
            "                room_id,",
            "            )",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "",
            "    async def on_send_membership_event(",
            "        self, origin: str, event: EventBase",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"",
            "        We have received a join/leave/knock event for a room via send_join/leave/knock.",
            "",
            "        Verify that event and send it into the room on the remote homeserver's behalf.",
            "",
            "        This is quite similar to on_receive_pdu, with the following principal",
            "        differences:",
            "          * only membership events are permitted (and only events with",
            "            sender==state_key -- ie, no kicks or bans)",
            "          * *We* send out the event on behalf of the remote server.",
            "          * We enforce the membership restrictions of restricted rooms.",
            "          * Rejected events result in an exception rather than being stored.",
            "",
            "        There are also other differences, however it is not clear if these are by",
            "        design or omission. In particular, we do not attempt to backfill any missing",
            "        prev_events.",
            "",
            "        Args:",
            "            origin: The homeserver of the remote (joining/invited/knocking) user.",
            "            event: The member event that has been signed by the remote homeserver.",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            RuntimeError if any prev_events are missing",
            "            SynapseError if the event is not accepted into the room",
            "            PartialStateConflictError if the room was un-partial stated in between",
            "                computing the state at the event and persisting it. The caller should",
            "                retry exactly once in this case.",
            "        \"\"\"",
            "        logger.debug(",
            "            \"on_send_membership_event: Got event: %s, signatures: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got send_membership request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        if event.sender != event.state_key:",
            "            raise SynapseError(400, \"state_key and sender must match\", Codes.BAD_JSON)",
            "",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # Send this event on behalf of the other server.",
            "        #",
            "        # The remote server isn't a full participant in the room at this point, so",
            "        # may not have an up-to-date list of the other homeservers participating in",
            "        # the room, so we send it on their behalf.",
            "        event.internal_metadata.send_on_behalf_of = origin",
            "",
            "        context = await self._state_handler.compute_event_context(event)",
            "        await self._check_event_auth(origin, event, context)",
            "        if context.rejected:",
            "            raise SynapseError(",
            "                403, f\"{event.membership} event was rejected\", Codes.FORBIDDEN",
            "            )",
            "",
            "        # for joins, we need to check the restrictions of restricted rooms",
            "        if event.membership == Membership.JOIN:",
            "            await self.check_join_restrictions(context, event)",
            "",
            "        # for knock events, we run the third-party event rules. It's not entirely clear",
            "        # why we don't do this for other sorts of membership events.",
            "        if event.membership == Membership.KNOCK:",
            "            event_allowed, _ = await self._third_party_event_rules.check_event_allowed(",
            "                event, context",
            "            )",
            "            if not event_allowed:",
            "                logger.info(\"Sending of knock %s forbidden by third-party rules\", event)",
            "                raise SynapseError(",
            "                    403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "                )",
            "",
            "        # all looks good, we can persist the event.",
            "",
            "        # First, precalculate the joined hosts so that the federation sender doesn't",
            "        # need to.",
            "        await self._event_creation_handler.cache_joined_hosts_for_event(event, context)",
            "",
            "        await self._check_for_soft_fail(event, context=context, origin=origin)",
            "        await self._run_push_actions_and_persist_event(event, context)",
            "        return event, context",
            "",
            "    async def check_join_restrictions(",
            "        self, context: EventContext, event: EventBase",
            "    ) -> None:",
            "        \"\"\"Check that restrictions in restricted join rules are matched",
            "",
            "        Called when we receive a join event via send_join.",
            "",
            "        Raises an auth error if the restrictions are not matched.",
            "        \"\"\"",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        # Check if the user is already in the room or invited to the room.",
            "        user_id = event.state_key",
            "        prev_member_event_id = prev_state_ids.get((EventTypes.Member, user_id), None)",
            "        prev_member_event = None",
            "        if prev_member_event_id:",
            "            prev_member_event = await self._store.get_event(prev_member_event_id)",
            "",
            "        # Check if the member should be allowed access via membership in a space.",
            "        await self._event_auth_handler.check_restricted_join_rules(",
            "            prev_state_ids,",
            "            event.room_version,",
            "            user_id,",
            "            prev_member_event,",
            "        )",
            "",
            "    @trace",
            "    async def process_remote_join(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        auth_events: List[EventBase],",
            "        state: List[EventBase],",
            "        event: EventBase,",
            "        room_version: RoomVersion,",
            "        partial_state: bool,",
            "    ) -> int:",
            "        \"\"\"Persists the events returned by a send_join",
            "",
            "        Checks the auth chain is valid (and passes auth checks) for the",
            "        state and event. Then persists all of the events.",
            "        Notifies about the persisted events where appropriate.",
            "",
            "        Args:",
            "            origin: Where the events came from",
            "            room_id:",
            "            auth_events",
            "            state",
            "            event",
            "            room_version: The room version we expect this room to have, and",
            "                will raise if it doesn't match the version in the create event.",
            "            partial_state: True if the state omits non-critical membership events",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            SynapseError if the response is in some way invalid.",
            "            PartialStateConflictError if the homeserver is already in the room and it",
            "                has been un-partial stated.",
            "        \"\"\"",
            "        create_event = None",
            "        for e in state:",
            "            if (e.type, e.state_key) == (EventTypes.Create, \"\"):",
            "                create_event = e",
            "                break",
            "",
            "        if create_event is None:",
            "            # If the state doesn't have a create event then the room is",
            "            # invalid, and it would fail auth checks anyway.",
            "            raise SynapseError(400, \"No create event in state\")",
            "",
            "        room_version_id = create_event.content.get(",
            "            \"room_version\", RoomVersions.V1.identifier",
            "        )",
            "",
            "        if room_version.identifier != room_version_id:",
            "            raise SynapseError(400, \"Room version mismatch\")",
            "",
            "        # persist the auth chain and state events.",
            "        #",
            "        # any invalid events here will be marked as rejected, and we'll carry on.",
            "        #",
            "        # any events whose auth events are missing (ie, not in the send_join response,",
            "        # and not already in our db) will just be ignored. This is correct behaviour,",
            "        # because the reason that auth_events are missing might be due to us being",
            "        # unable to validate their signatures. The fact that we can't validate their",
            "        # signatures right now doesn't mean that we will *never* be able to, so it",
            "        # is premature to reject them.",
            "        #",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state)",
            "        )",
            "",
            "        # and now persist the join event itself.",
            "        logger.info(",
            "            \"Peristing join-via-remote %s (partial_state: %s)\", event, partial_state",
            "        )",
            "        with nested_logging_context(suffix=event.event_id):",
            "            context = await self._state_handler.compute_event_context(",
            "                event,",
            "                state_ids_before_event={",
            "                    (e.type, e.state_key): e.event_id for e in state",
            "                },",
            "                partial_state=partial_state,",
            "            )",
            "",
            "            await self._check_event_auth(origin, event, context)",
            "            if context.rejected:",
            "                raise SynapseError(400, \"Join event was rejected\")",
            "",
            "            # the remote server is responsible for sending our join event to the rest",
            "            # of the federation. Indeed, attempting to do so will result in problems",
            "            # when we try to look up the state before the join (to get the server list)",
            "            # and discover that we do not have it.",
            "            event.internal_metadata.proactively_send = False",
            "",
            "            stream_id_after_persist = await self.persist_events_and_notify(",
            "                room_id, [(event, context)]",
            "            )",
            "",
            "            # If we're joining the room again, check if there is new marker",
            "            # state indicating that there is new history imported somewhere in",
            "            # the DAG. Multiple markers can exist in the current state with",
            "            # unique state_keys.",
            "            #",
            "            # Do this after the state from the remote join was persisted (via",
            "            # `persist_events_and_notify`). Otherwise we can run into a",
            "            # situation where the create event doesn't exist yet in the",
            "            # `current_state_events`",
            "            for e in state:",
            "                await self._handle_marker_event(origin, e)",
            "",
            "            return stream_id_after_persist",
            "",
            "    async def update_state_for_partial_state_event(",
            "        self, destination: str, event: EventBase",
            "    ) -> None:",
            "        \"\"\"Recalculate the state at an event as part of a de-partial-stating process",
            "",
            "        Args:",
            "            destination: server to request full state from",
            "            event: partial-state event to be de-partial-stated",
            "",
            "        Raises:",
            "            FederationError if we fail to request state from the remote server.",
            "        \"\"\"",
            "        logger.info(\"Updating state for %s\", event.event_id)",
            "        with nested_logging_context(suffix=event.event_id):",
            "            # if we have all the event's prev_events, then we can work out the",
            "            # state based on their states. Otherwise, we request it from the destination",
            "            # server.",
            "            #",
            "            # This is the same operation as we do when we receive a regular event",
            "            # over federation.",
            "            context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                destination, event",
            "            )",
            "            if context.partial_state:",
            "                # this can happen if some or all of the event's prev_events still have",
            "                # partial state. We were careful to only pick events from the db without",
            "                # partial-state prev events, so that implies that a prev event has",
            "                # been persisted (with partial state) since we did the query.",
            "                #",
            "                # So, let's just ignore `event` for now; when we re-run the db query",
            "                # we should instead get its partial-state prev event, which we will",
            "                # de-partial-state, and then come back to event.",
            "                logger.warning(",
            "                    \"%s still has prev_events with partial state: can't de-partial-state it yet\",",
            "                    event.event_id,",
            "                )",
            "                return",
            "",
            "            # since the state at this event has changed, we should now re-evaluate",
            "            # whether it should have been rejected. We must already have all of the",
            "            # auth events (from last time we went round this path), so there is no",
            "            # need to pass the origin.",
            "            await self._check_event_auth(None, event, context)",
            "",
            "            await self._store.update_state_for_partial_state_event(event, context)",
            "            self._state_storage_controller.notify_event_un_partial_stated(",
            "                event.event_id",
            "            )",
            "",
            "    @trace",
            "    async def backfill(",
            "        self, dest: str, room_id: str, limit: int, extremities: Collection[str]",
            "    ) -> None:",
            "        \"\"\"Trigger a backfill request to `dest` for the given `room_id`",
            "",
            "        This will attempt to get more events from the remote. If the other side",
            "        has no new events to offer, this will return an empty list.",
            "",
            "        As the events are received, we check their signatures, and also do some",
            "        sanity-checking on them. If any of the backfilled events are invalid,",
            "        this method throws a SynapseError.",
            "",
            "        We might also raise an InvalidResponseError if the response from the remote",
            "        server is just bogus.",
            "",
            "        TODO: make this more useful to distinguish failures of the remote",
            "        server from invalid events (there is probably no point in trying to",
            "        re-fetch invalid events from every other HS in the room.)",
            "        \"\"\"",
            "        if dest == self._server_name:",
            "            raise SynapseError(400, \"Can't backfill from self.\")",
            "",
            "        events = await self._federation_client.backfill(",
            "            dest, room_id, limit=limit, extremities=extremities",
            "        )",
            "",
            "        if not events:",
            "            return",
            "",
            "        with backfill_processing_after_timer.time():",
            "            # if there are any events in the wrong room, the remote server is buggy and",
            "            # should not be trusted.",
            "            for ev in events:",
            "                if ev.room_id != room_id:",
            "                    raise InvalidResponseError(",
            "                        f\"Remote server {dest} returned event {ev.event_id} which is in \"",
            "                        f\"room {ev.room_id}, when we were backfilling in {room_id}\"",
            "                    )",
            "",
            "            await self._process_pulled_events(",
            "                dest,",
            "                events,",
            "                backfilled=True,",
            "            )",
            "",
            "    @trace",
            "    async def _get_missing_events_for_pdu(",
            "        self, origin: str, pdu: EventBase, prevs: Set[str], min_depth: int",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            origin: Origin of the pdu. Will be called to get the missing events",
            "            pdu: received pdu",
            "            prevs: List of event ids which we are missing",
            "            min_depth: Minimum depth of events to return.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "",
            "        if not prevs - seen:",
            "            return",
            "",
            "        latest_list = await self._store.get_latest_event_ids_in_room(room_id)",
            "",
            "        # We add the prev events that we have seen to the latest",
            "        # list to ensure the remote server doesn't give them to us",
            "        latest = set(latest_list)",
            "        latest |= seen",
            "",
            "        logger.info(",
            "            \"Requesting missing events between %s and %s\",",
            "            shortstr(latest),",
            "            event_id,",
            "        )",
            "",
            "        # XXX: we set timeout to 10s to help workaround",
            "        # https://github.com/matrix-org/synapse/issues/1733.",
            "        # The reason is to avoid holding the linearizer lock",
            "        # whilst processing inbound /send transactions, causing",
            "        # FDs to stack up and block other inbound transactions",
            "        # which empirically can currently take up to 30 minutes.",
            "        #",
            "        # N.B. this explicitly disables retry attempts.",
            "        #",
            "        # N.B. this also increases our chances of falling back to",
            "        # fetching fresh state for the room if the missing event",
            "        # can't be found, which slightly reduces our security.",
            "        # it may also increase our DAG extremity count for the room,",
            "        # causing additional state resolution?  See #1760.",
            "        # However, fetching state doesn't hold the linearizer lock",
            "        # apparently.",
            "        #",
            "        # see https://github.com/matrix-org/synapse/pull/1744",
            "        #",
            "        # ----",
            "        #",
            "        # Update richvdh 2018/09/18: There are a number of problems with timing this",
            "        # request out aggressively on the client side:",
            "        #",
            "        # - it plays badly with the server-side rate-limiter, which starts tarpitting you",
            "        #   if you send too many requests at once, so you end up with the server carefully",
            "        #   working through the backlog of your requests, which you have already timed",
            "        #   out.",
            "        #",
            "        # - for this request in particular, we now (as of",
            "        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the",
            "        #   server can't produce a plausible-looking set of prev_events - so we becone",
            "        #   much more likely to reject the event.",
            "        #",
            "        # - contrary to what it says above, we do *not* fall back to fetching fresh state",
            "        #   for the room if get_missing_events times out. Rather, we give up processing",
            "        #   the PDU whose prevs we are missing, which then makes it much more likely that",
            "        #   we'll end up back here for the *next* PDU in the list, which exacerbates the",
            "        #   problem.",
            "        #",
            "        # - the aggressive 10s timeout was introduced to deal with incoming federation",
            "        #   requests taking 8 hours to process. It's not entirely clear why that was going",
            "        #   on; certainly there were other issues causing traffic storms which are now",
            "        #   resolved, and I think in any case we may be more sensible about our locking",
            "        #   now. We're *certainly* more sensible about our logging.",
            "        #",
            "        # All that said: Let's try increasing the timeout to 60s and see what happens.",
            "",
            "        try:",
            "            missing_events = await self._federation_client.get_missing_events(",
            "                origin,",
            "                room_id,",
            "                earliest_events_ids=list(latest),",
            "                latest_events=[pdu],",
            "                limit=10,",
            "                min_depth=min_depth,",
            "                timeout=60000,",
            "            )",
            "        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:",
            "            # We failed to get the missing events, but since we need to handle",
            "            # the case of `get_missing_events` not returning the necessary",
            "            # events anyway, it is safe to simply log the error and continue.",
            "            logger.warning(\"Failed to get prev_events: %s\", e)",
            "            return",
            "",
            "        logger.info(\"Got %d prev_events\", len(missing_events))",
            "        await self._process_pulled_events(origin, missing_events, backfilled=False)",
            "",
            "    @trace",
            "    async def _process_pulled_events(",
            "        self, origin: str, events: Collection[EventBase], backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a batch of events we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the events, persists the received events,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the events have already had their signatures and hashes checked.",
            "",
            "        Params:",
            "            origin: The server we received these events from",
            "            events: The received events.",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str([event.event_id for event in events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(events)),",
            "        )",
            "        set_tag(SynapseTags.FUNC_ARG_PREFIX + \"backfilled\", str(backfilled))",
            "        logger.debug(",
            "            \"processing pulled backfilled=%s events=%s\",",
            "            backfilled,",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        # We want to sort these by depth so we process them and",
            "        # tell clients about them in order.",
            "        sorted_events = sorted(events, key=lambda x: x.depth)",
            "        for ev in sorted_events:",
            "            with nested_logging_context(ev.event_id):",
            "                await self._process_pulled_event(origin, ev, backfilled=backfilled)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _process_pulled_event(",
            "        self, origin: str, event: EventBase, backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a single event that we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the event, persists the received event,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the event has already had its signatures and hashes checked.",
            "",
            "        This is somewhat equivalent to on_receive_pdu, but applies somewhat different",
            "        logic in the case that we are missing prev_events (in particular, it just",
            "        requests the state at that point, rather than triggering a get_missing_events) -",
            "        so is appropriate when we have pulled the event from a remote server, rather",
            "        than having it pushed to us.",
            "",
            "        Params:",
            "            origin: The server we received this event from",
            "            events: The received event",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        logger.info(\"Processing pulled event %s\", event)",
            "",
            "        # This function should not be used to persist outliers (use something",
            "        # else) because this does a bunch of operations that aren't necessary",
            "        # (extra work; in particular, it makes sure we have all the prev_events",
            "        # and resolves the state across those prev events). If you happen to run",
            "        # into a situation where the event you're trying to process/backfill is",
            "        # marked as an `outlier`, then you should update that spot to return an",
            "        # `EventBase` copy that doesn't have `outlier` flag set.",
            "        #",
            "        # `EventBase` is used to represent both an event we have not yet",
            "        # persisted, and one that we have persisted and now keep in the cache.",
            "        # In an ideal world this method would only be called with the first type",
            "        # of event, but it turns out that's not actually the case and for",
            "        # example, you could get an event from cache that is marked as an",
            "        # `outlier` (fix up that spot though).",
            "        assert not event.internal_metadata.is_outlier(), (",
            "            \"Outlier event passed to _process_pulled_event. \"",
            "            \"To persist an event as a non-outlier, make sure to pass in a copy without `event.internal_metadata.outlier = true`.\"",
            "        )",
            "",
            "        event_id = event.event_id",
            "",
            "        existing = await self._store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "        if existing:",
            "            if not existing.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"_process_pulled_event: Ignoring received event %s which we have already seen\",",
            "                    event_id,",
            "                )",
            "                return",
            "            logger.info(\"De-outliering event %s\", event_id)",
            "",
            "        try:",
            "            self._sanity_check_event(event)",
            "        except SynapseError as err:",
            "            logger.warning(\"Event %s failed sanity check: %s\", event_id, err)",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(err)",
            "            )",
            "            return",
            "        except Exception as exc:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(exc)",
            "            )",
            "            raise exc",
            "",
            "        try:",
            "            try:",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "            except PartialStateConflictError:",
            "                # The room was un-partial stated while we were processing the event.",
            "                # Try once more, with full state this time.",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "",
            "                # We ought to have full state now, barring some unlikely race where we left and",
            "                # rejoned the room in the background.",
            "                if context.partial_state:",
            "                    raise AssertionError(",
            "                        f\"Event {event.event_id} still has a partial resolved state \"",
            "                        f\"after room {event.room_id} was un-partial stated\"",
            "                    )",
            "",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "        except FederationError as e:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(e)",
            "            )",
            "",
            "            if e.code == 403:",
            "                logger.warning(\"Pulled event %s failed history check.\", event_id)",
            "            else:",
            "                raise",
            "        except Exception as exc:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(exc)",
            "            )",
            "            raise exc",
            "",
            "    @trace",
            "    async def _compute_event_context_with_maybe_missing_prevs(",
            "        self, dest: str, event: EventBase",
            "    ) -> EventContext:",
            "        \"\"\"Build an EventContext structure for a non-outlier event whose prev_events may",
            "        be missing.",
            "",
            "        This is used when we have pulled a batch of events from a remote server, and may",
            "        not have all the prev_events.",
            "",
            "        To build an EventContext, we need to calculate the state before the event. If we",
            "        already have all the prev_events for `event`, we can simply use the state after",
            "        the prev_events to calculate the state before `event`.",
            "",
            "        Otherwise, the missing prevs become new backwards extremities, and we fall back",
            "        to asking the remote server for the state after each missing `prev_event`,",
            "        and resolving across them.",
            "",
            "        That's ok provided we then resolve the state against other bits of the DAG",
            "        before using it - in other words, that the received event `event` is not going",
            "        to become the only forwards_extremity in the room (which will ensure that you",
            "        can't just take over a room by sending an event, withholding its prev_events,",
            "        and declaring yourself to be an admin in the subsequent state request).",
            "",
            "        In other words: we should only call this method if `event` has been *pulled*",
            "        as part of a batch of missing prev events, or similar.",
            "",
            "        Params:",
            "            dest: the remote server to ask for state at the missing prevs. Typically,",
            "                this will be the server we got `event` from.",
            "            event: an event to check for missing prevs.",
            "",
            "        Returns:",
            "            The event context.",
            "",
            "        Raises:",
            "            FederationError if we fail to get the state from the remote server after any",
            "                missing `prev_event`s.",
            "        \"\"\"",
            "        room_id = event.room_id",
            "        event_id = event.event_id",
            "",
            "        prevs = set(event.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        if not missing_prevs:",
            "            return await self._state_handler.compute_event_context(event)",
            "",
            "        logger.info(",
            "            \"Event %s is missing prev_events %s: calculating state for a \"",
            "            \"backwards extremity\",",
            "            event_id,",
            "            shortstr(missing_prevs),",
            "        )",
            "        # Calculate the state after each of the previous events, and",
            "        # resolve them to find the correct state at the current event.",
            "",
            "        try:",
            "            # Determine whether we may be about to retrieve partial state",
            "            # Events may be un-partial stated right after we compute the partial state",
            "            # flag, but that's okay, as long as the flag errs on the conservative side.",
            "            partial_state_flags = await self._store.get_partial_state_events(seen)",
            "            partial_state = any(partial_state_flags.values())",
            "",
            "            # Get the state of the events we know about",
            "            ours = await self._state_storage_controller.get_state_groups_ids(",
            "                room_id, seen, await_full_state=False",
            "            )",
            "",
            "            # state_maps is a list of mappings from (type, state_key) to event_id",
            "            state_maps: List[StateMap[str]] = list(ours.values())",
            "",
            "            # we don't need this any more, let's delete it.",
            "            del ours",
            "",
            "            # Ask the remote server for the states we don't",
            "            # know about",
            "            for p in missing_prevs:",
            "                logger.info(\"Requesting state after missing prev_event %s\", p)",
            "",
            "                with nested_logging_context(p):",
            "                    # note that if any of the missing prevs share missing state or",
            "                    # auth events, the requests to fetch those events are deduped",
            "                    # by the get_pdu_cache in federation_client.",
            "                    remote_state_map = (",
            "                        await self._get_state_ids_after_missing_prev_event(",
            "                            dest, room_id, p",
            "                        )",
            "                    )",
            "",
            "                    state_maps.append(remote_state_map)",
            "",
            "            room_version = await self._store.get_room_version_id(room_id)",
            "            state_map = await self._state_resolution_handler.resolve_events_with_store(",
            "                room_id,",
            "                room_version,",
            "                state_maps,",
            "                event_map={event_id: event},",
            "                state_res_store=StateResolutionStore(self._store),",
            "            )",
            "",
            "        except Exception:",
            "            logger.warning(",
            "                \"Error attempting to resolve state at missing prev_events\",",
            "                exc_info=True,",
            "            )",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                403,",
            "                \"We can't get valid state history.\",",
            "                affected=event_id,",
            "            )",
            "        return await self._state_handler.compute_event_context(",
            "            event, state_ids_before_event=state_map, partial_state=partial_state",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_ids_after_missing_prev_event(",
            "        self,",
            "        destination: str,",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> StateMap[str]:",
            "        \"\"\"Requests all of the room state at a given event from a remote homeserver.",
            "",
            "        Args:",
            "            destination: The remote homeserver to query for the state.",
            "            room_id: The id of the room we're interested in.",
            "            event_id: The id of the event we want the state at.",
            "",
            "        Returns:",
            "            The event ids of the state *after* the given event.",
            "",
            "        Raises:",
            "            InvalidResponseError: if the remote homeserver's response contains fields",
            "                of the wrong type.",
            "        \"\"\"",
            "",
            "        # It would be better if we could query the difference from our known",
            "        # state to the given `event_id` so the sending server doesn't have to",
            "        # send as much and we don't have to process as many events. For example",
            "        # in a room like #matrix:matrix.org, we get 200k events (77k state_events, 122k",
            "        # auth_events) from this call.",
            "        #",
            "        # Tracked by https://github.com/matrix-org/synapse/issues/13618",
            "        (",
            "            state_event_ids,",
            "            auth_event_ids,",
            "        ) = await self._federation_client.get_room_state_ids(",
            "            destination, room_id, event_id=event_id",
            "        )",
            "",
            "        logger.debug(",
            "            \"state_ids returned %i state events, %i auth events\",",
            "            len(state_event_ids),",
            "            len(auth_event_ids),",
            "        )",
            "",
            "        # Start by checking events we already have in the DB",
            "        desired_events = set(state_event_ids)",
            "        desired_events.add(event_id)",
            "        logger.debug(\"Fetching %i events from cache/store\", len(desired_events))",
            "        have_events = await self._store.have_seen_events(room_id, desired_events)",
            "",
            "        missing_desired_event_ids = desired_events - have_events",
            "        logger.debug(",
            "            \"We are missing %i events (got %i)\",",
            "            len(missing_desired_event_ids),",
            "            len(have_events),",
            "        )",
            "",
            "        # We probably won't need most of the auth events, so let's just check which",
            "        # we have for now, rather than thrashing the event cache with them all",
            "        # unnecessarily.",
            "",
            "        # TODO: we probably won't actually need all of the auth events, since we",
            "        #   already have a bunch of the state events. It would be nice if the",
            "        #   federation api gave us a way of finding out which we actually need.",
            "",
            "        missing_auth_event_ids = set(auth_event_ids) - have_events",
            "        missing_auth_event_ids.difference_update(",
            "            await self._store.have_seen_events(room_id, missing_auth_event_ids)",
            "        )",
            "        logger.debug(\"We are also missing %i auth events\", len(missing_auth_event_ids))",
            "",
            "        missing_event_ids = missing_desired_event_ids | missing_auth_event_ids",
            "",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids\",",
            "            str(missing_auth_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids.length\",",
            "            str(len(missing_auth_event_ids)),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids\",",
            "            str(missing_desired_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids.length\",",
            "            str(len(missing_desired_event_ids)),",
            "        )",
            "",
            "        # Making an individual request for each of 1000s of events has a lot of",
            "        # overhead. On the other hand, we don't really want to fetch all of the events",
            "        # if we already have most of them.",
            "        #",
            "        # As an arbitrary heuristic, if we are missing more than 10% of the events, then",
            "        # we fetch the whole state.",
            "        #",
            "        # TODO: might it be better to have an API which lets us do an aggregate event",
            "        #   request",
            "        if (len(missing_event_ids) * 10) >= len(auth_event_ids) + len(state_event_ids):",
            "            logger.debug(\"Requesting complete state from remote\")",
            "            await self._get_state_and_persist(destination, room_id, event_id)",
            "        else:",
            "            logger.debug(\"Fetching %i events from remote\", len(missing_event_ids))",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=missing_event_ids",
            "            )",
            "",
            "        # We now need to fill out the state map, which involves fetching the",
            "        # type and state key for each event ID in the state.",
            "        state_map = {}",
            "",
            "        event_metadata = await self._store.get_metadata_for_events(state_event_ids)",
            "        for state_event_id, metadata in event_metadata.items():",
            "            if metadata.room_id != room_id:",
            "                # This is a bogus situation, but since we may only discover it a long time",
            "                # after it happened, we try our best to carry on, by just omitting the",
            "                # bad events from the returned state set.",
            "                #",
            "                # This can happen if a remote server claims that the state or",
            "                # auth_events at an event in room A are actually events in room B",
            "                logger.warning(",
            "                    \"Remote server %s claims event %s in room %s is an auth/state \"",
            "                    \"event in room %s\",",
            "                    destination,",
            "                    state_event_id,",
            "                    metadata.room_id,",
            "                    room_id,",
            "                )",
            "                continue",
            "",
            "            if metadata.state_key is None:",
            "                logger.warning(",
            "                    \"Remote server gave us non-state event in state: %s\", state_event_id",
            "                )",
            "                continue",
            "",
            "            state_map[(metadata.event_type, metadata.state_key)] = state_event_id",
            "",
            "        # if we couldn't get the prev event in question, that's a problem.",
            "        remote_event = await self._store.get_event(",
            "            event_id,",
            "            allow_none=True,",
            "            allow_rejected=True,",
            "            redact_behaviour=EventRedactBehaviour.as_is,",
            "        )",
            "        if not remote_event:",
            "            raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))",
            "",
            "        # missing state at that event is a warning, not a blocker",
            "        # XXX: this doesn't sound right? it means that we'll end up with incomplete",
            "        #   state.",
            "        failed_to_fetch = desired_events - event_metadata.keys()",
            "        # `event_id` could be missing from `event_metadata` because it's not necessarily",
            "        # a state event. We've already checked that we've fetched it above.",
            "        failed_to_fetch.discard(event_id)",
            "        if failed_to_fetch:",
            "            logger.warning(",
            "                \"Failed to fetch missing state events for %s %s\",",
            "                event_id,",
            "                failed_to_fetch,",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch\",",
            "                str(failed_to_fetch),",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch.length\",",
            "                str(len(failed_to_fetch)),",
            "            )",
            "",
            "        if remote_event.is_state() and remote_event.rejected_reason is None:",
            "            state_map[",
            "                (remote_event.type, remote_event.state_key)",
            "            ] = remote_event.event_id",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_and_persist(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"Get the complete room state at a given event, and persist any new events",
            "        as outliers\"\"\"",
            "        room_version = await self._store.get_room_version(room_id)",
            "        auth_events, state_events = await self._federation_client.get_room_state(",
            "            destination, room_id, event_id=event_id, room_version=room_version",
            "        )",
            "        logger.info(\"/state returned %i events\", len(auth_events) + len(state_events))",
            "",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state_events)",
            "        )",
            "",
            "        # we also need the event itself.",
            "        if not await self._store.have_seen_event(room_id, event_id):",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=(event_id,)",
            "            )",
            "",
            "    @trace",
            "    async def _process_received_pdu(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        backfilled: bool = False,",
            "    ) -> None:",
            "        \"\"\"Called when we have a new non-outlier event.",
            "",
            "        This is called when we have a new event to add to the room DAG. This can be",
            "        due to:",
            "           * events received directly via a /send request",
            "           * events retrieved via get_missing_events after a /send request",
            "           * events backfilled after a client request.",
            "",
            "        It's not currently used for events received from incoming send_{join,knock,leave}",
            "        requests (which go via on_send_membership_event), nor for joins created by a",
            "        remote join dance (which go via process_remote_join).",
            "",
            "        We need to do auth checks and put it through the StateHandler.",
            "",
            "        Args:",
            "            origin: server sending the event",
            "",
            "            event: event to be persisted",
            "",
            "            context: The `EventContext` to persist the event with.",
            "",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "",
            "        PartialStateConflictError: if the room was un-partial stated in between",
            "            computing the state at the event and persisting it. The caller should",
            "            recompute `context` and retry exactly once when this happens.",
            "        \"\"\"",
            "        logger.debug(\"Processing event: %s\", event)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        try:",
            "            await self._check_event_auth(origin, event, context)",
            "        except AuthError as e:",
            "            # This happens only if we couldn't find the auth events. We'll already have",
            "            # logged a warning, so now we just convert to a FederationError.",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)",
            "",
            "        if not backfilled and not context.rejected:",
            "            # For new (non-backfilled and non-outlier) events we check if the event",
            "            # passes auth based on the current state. If it doesn't then we",
            "            # \"soft-fail\" the event.",
            "            await self._check_for_soft_fail(event, context=context, origin=origin)",
            "",
            "        await self._run_push_actions_and_persist_event(event, context, backfilled)",
            "",
            "        await self._handle_marker_event(origin, event)",
            "",
            "        if backfilled or context.rejected:",
            "            return",
            "",
            "        await self._maybe_kick_guest_users(event)",
            "",
            "        # For encrypted messages we check that we know about the sending device,",
            "        # if we don't then we mark the device cache for that user as stale.",
            "        if event.type == EventTypes.Encrypted:",
            "            device_id = event.content.get(\"device_id\")",
            "            sender_key = event.content.get(\"sender_key\")",
            "",
            "            cached_devices = await self._store.get_cached_devices_for_user(event.sender)",
            "",
            "            resync = False  # Whether we should resync device lists.",
            "",
            "            device = None",
            "            if device_id is not None:",
            "                device = cached_devices.get(device_id)",
            "                if device is None:",
            "                    logger.info(",
            "                        \"Received event from remote device not in our cache: %s %s\",",
            "                        event.sender,",
            "                        device_id,",
            "                    )",
            "                    resync = True",
            "",
            "            # We also check if the `sender_key` matches what we expect.",
            "            if sender_key is not None:",
            "                # Figure out what sender key we're expecting. If we know the",
            "                # device and recognize the algorithm then we can work out the",
            "                # exact key to expect. Otherwise check it matches any key we",
            "                # have for that device.",
            "",
            "                current_keys: Container[str] = []",
            "",
            "                if device:",
            "                    keys = device.get(\"keys\", {}).get(\"keys\", {})",
            "",
            "                    if (",
            "                        event.content.get(\"algorithm\")",
            "                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2",
            "                    ):",
            "                        # For this algorithm we expect a curve25519 key.",
            "                        key_name = \"curve25519:%s\" % (device_id,)",
            "                        current_keys = [keys.get(key_name)]",
            "                    else:",
            "                        # We don't know understand the algorithm, so we just",
            "                        # check it matches a key for the device.",
            "                        current_keys = keys.values()",
            "                elif device_id:",
            "                    # We don't have any keys for the device ID.",
            "                    pass",
            "                else:",
            "                    # The event didn't include a device ID, so we just look for",
            "                    # keys across all devices.",
            "                    current_keys = [",
            "                        key",
            "                        for device in cached_devices.values()",
            "                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()",
            "                    ]",
            "",
            "                # We now check that the sender key matches (one of) the expected",
            "                # keys.",
            "                if sender_key not in current_keys:",
            "                    logger.info(",
            "                        \"Received event from remote device with unexpected sender key: %s %s: %s\",",
            "                        event.sender,",
            "                        device_id or \"<no device_id>\",",
            "                        sender_key,",
            "                    )",
            "                    resync = True",
            "",
            "            if resync:",
            "                run_as_background_process(",
            "                    \"resync_device_due_to_pdu\",",
            "                    self._resync_device,",
            "                    event.sender,",
            "                )",
            "",
            "    async def _resync_device(self, sender: str) -> None:",
            "        \"\"\"We have detected that the device list for the given user may be out",
            "        of sync, so we try and resync them.",
            "        \"\"\"",
            "",
            "        try:",
            "            await self._store.mark_remote_user_device_cache_as_stale(sender)",
            "",
            "            # Immediately attempt a resync in the background",
            "            if self._config.worker.worker_app:",
            "                await self._user_device_resync(user_id=sender)",
            "            else:",
            "                await self._device_list_updater.user_device_resync(sender)",
            "        except Exception:",
            "            logger.exception(\"Failed to resync device for %s\", sender)",
            "",
            "    @trace",
            "    async def _handle_marker_event(self, origin: str, marker_event: EventBase) -> None:",
            "        \"\"\"Handles backfilling the insertion event when we receive a marker",
            "        event that points to one.",
            "",
            "        Args:",
            "            origin: Origin of the event. Will be called to get the insertion event",
            "            marker_event: The event to process",
            "        \"\"\"",
            "",
            "        if marker_event.type != EventTypes.MSC2716_MARKER:",
            "            # Not a marker event",
            "            return",
            "",
            "        if marker_event.rejected_reason is not None:",
            "            # Rejected event",
            "            return",
            "",
            "        # Skip processing a marker event if the room version doesn't",
            "        # support it or the event is not from the room creator.",
            "        room_version = await self._store.get_room_version(marker_event.room_id)",
            "        create_event = await self._store.get_create_event_for_room(marker_event.room_id)",
            "        room_creator = create_event.content.get(EventContentFields.ROOM_CREATOR)",
            "        if not room_version.msc2716_historical and (",
            "            not self._config.experimental.msc2716_enabled",
            "            or marker_event.sender != room_creator",
            "        ):",
            "            return",
            "",
            "        logger.debug(\"_handle_marker_event: received %s\", marker_event)",
            "",
            "        insertion_event_id = marker_event.content.get(",
            "            EventContentFields.MSC2716_INSERTION_EVENT_REFERENCE",
            "        )",
            "",
            "        if insertion_event_id is None:",
            "            # Nothing to retrieve then (invalid marker)",
            "            return",
            "",
            "        already_seen_insertion_event = await self._store.have_seen_event(",
            "            marker_event.room_id, insertion_event_id",
            "        )",
            "        if already_seen_insertion_event:",
            "            # No need to process a marker again if we have already seen the",
            "            # insertion event that it was pointing to",
            "            return",
            "",
            "        logger.debug(",
            "            \"_handle_marker_event: backfilling insertion event %s\", insertion_event_id",
            "        )",
            "",
            "        await self._get_events_and_persist(",
            "            origin,",
            "            marker_event.room_id,",
            "            [insertion_event_id],",
            "        )",
            "",
            "        insertion_event = await self._store.get_event(",
            "            insertion_event_id, allow_none=True",
            "        )",
            "        if insertion_event is None:",
            "            logger.warning(",
            "                \"_handle_marker_event: server %s didn't return insertion event %s for marker %s\",",
            "                origin,",
            "                insertion_event_id,",
            "                marker_event.event_id,",
            "            )",
            "            return",
            "",
            "        logger.debug(",
            "            \"_handle_marker_event: succesfully backfilled insertion event %s from marker event %s\",",
            "            insertion_event,",
            "            marker_event,",
            "        )",
            "",
            "        await self._store.insert_insertion_extremity(",
            "            insertion_event_id, marker_event.room_id",
            "        )",
            "",
            "        logger.debug(",
            "            \"_handle_marker_event: insertion extremity added for %s from marker event %s\",",
            "            insertion_event,",
            "            marker_event,",
            "        )",
            "",
            "    async def backfill_event_id(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> EventBase:",
            "        \"\"\"Backfill a single event and persist it as a non-outlier which means",
            "        we also pull in all of the state and auth events necessary for it.",
            "",
            "        Args:",
            "            destination: The homeserver to pull the given event_id from.",
            "            room_id: The room where the event is from.",
            "            event_id: The event ID to backfill.",
            "",
            "        Raises:",
            "            FederationError if we are unable to find the event from the destination",
            "        \"\"\"",
            "        logger.info(",
            "            \"backfill_event_id: event_id=%s from destination=%s\", event_id, destination",
            "        )",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        event_from_response = await self._federation_client.get_pdu(",
            "            [destination],",
            "            event_id,",
            "            room_version,",
            "        )",
            "",
            "        if not event_from_response:",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                404,",
            "                \"Unable to find event_id=%s from destination=%s to backfill.\"",
            "                % (event_id, destination),",
            "                affected=event_id,",
            "            )",
            "",
            "        # Persist the event we just fetched, including pulling all of the state",
            "        # and auth events to de-outlier it. This also sets up the necessary",
            "        # `state_groups` for the event.",
            "        await self._process_pulled_events(",
            "            destination,",
            "            [event_from_response],",
            "            # Prevent notifications going to clients",
            "            backfilled=True,",
            "        )",
            "",
            "        return event_from_response",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_events_and_persist(",
            "        self, destination: str, room_id: str, event_ids: Collection[str]",
            "    ) -> None:",
            "        \"\"\"Fetch the given events from a server, and persist them as outliers.",
            "",
            "        This function *does not* recursively get missing auth events of the",
            "        newly fetched events. Callers must include in the `event_ids` argument",
            "        any missing events from the auth chain.",
            "",
            "        Logs a warning if we can't find the given event.",
            "        \"\"\"",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        events: List[EventBase] = []",
            "",
            "        async def get_event(event_id: str) -> None:",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    event = await self._federation_client.get_pdu(",
            "                        [destination],",
            "                        event_id,",
            "                        room_version,",
            "                    )",
            "                    if event is None:",
            "                        logger.warning(",
            "                            \"Server %s didn't return event %s\",",
            "                            destination,",
            "                            event_id,",
            "                        )",
            "                        return",
            "                    events.append(event)",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"Error fetching missing state/auth event %s: %s %s\",",
            "                        event_id,",
            "                        type(e),",
            "                        e,",
            "                    )",
            "",
            "        await concurrently_execute(get_event, event_ids, 5)",
            "        logger.info(\"Fetched %i events of %i requested\", len(events), len(event_ids))",
            "        await self._auth_and_persist_outliers(room_id, events)",
            "",
            "    @trace",
            "    async def _auth_and_persist_outliers(",
            "        self, room_id: str, events: Iterable[EventBase]",
            "    ) -> None:",
            "        \"\"\"Persist a batch of outlier events fetched from remote servers.",
            "",
            "        We first sort the events to make sure that we process each event's auth_events",
            "        before the event itself.",
            "",
            "        We then mark the events as outliers, persist them to the database, and, where",
            "        appropriate (eg, an invite), awake the notifier.",
            "",
            "        Params:",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            events: the events that have been fetched",
            "        \"\"\"",
            "        event_map = {event.event_id: event for event in events}",
            "",
            "        event_ids = event_map.keys()",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str(event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(event_ids)),",
            "        )",
            "",
            "        # filter out any events we have already seen. This might happen because",
            "        # the events were eagerly pushed to us (eg, during a room join), or because",
            "        # another thread has raced against us since we decided to request the event.",
            "        #",
            "        # This is just an optimisation, so it doesn't need to be watertight - the event",
            "        # persister does another round of deduplication.",
            "        seen_remotes = await self._store.have_seen_events(room_id, event_map.keys())",
            "        for s in seen_remotes:",
            "            event_map.pop(s, None)",
            "",
            "        # XXX: it might be possible to kick this process off in parallel with fetching",
            "        # the events.",
            "        while event_map:",
            "            # build a list of events whose auth events are not in the queue.",
            "            roots = tuple(",
            "                ev",
            "                for ev in event_map.values()",
            "                if not any(aid in event_map for aid in ev.auth_event_ids())",
            "            )",
            "",
            "            if not roots:",
            "                # if *none* of the remaining events are ready, that means",
            "                # we have a loop. This either means a bug in our logic, or that",
            "                # somebody has managed to create a loop (which requires finding a",
            "                # hash collision in room v2 and later).",
            "                logger.warning(",
            "                    \"Loop found in auth events while fetching missing state/auth \"",
            "                    \"events: %s\",",
            "                    shortstr(event_map.keys()),",
            "                )",
            "                return",
            "",
            "            logger.info(",
            "                \"Persisting %i of %i remaining outliers: %s\",",
            "                len(roots),",
            "                len(event_map),",
            "                shortstr(e.event_id for e in roots),",
            "            )",
            "",
            "            await self._auth_and_persist_outliers_inner(room_id, roots)",
            "",
            "            for ev in roots:",
            "                del event_map[ev.event_id]",
            "",
            "    async def _auth_and_persist_outliers_inner(",
            "        self, room_id: str, fetched_events: Collection[EventBase]",
            "    ) -> None:",
            "        \"\"\"Helper for _auth_and_persist_outliers",
            "",
            "        Persists a batch of events where we have (theoretically) already persisted all",
            "        of their auth events.",
            "",
            "        Marks the events as outliers, auths them, persists them to the database, and,",
            "        where appropriate (eg, an invite), awakes the notifier.",
            "",
            "        Params:",
            "            origin: where the events came from",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            fetched_events: the events to persist",
            "        \"\"\"",
            "        # get all the auth events for all the events in this batch. By now, they should",
            "        # have been persisted.",
            "        auth_events = {",
            "            aid for event in fetched_events for aid in event.auth_event_ids()",
            "        }",
            "        persisted_events = await self._store.get_events(",
            "            auth_events,",
            "            allow_rejected=True,",
            "        )",
            "",
            "        events_and_contexts_to_persist: List[Tuple[EventBase, EventContext]] = []",
            "",
            "        async def prep(event: EventBase) -> None:",
            "            with nested_logging_context(suffix=event.event_id):",
            "                auth = []",
            "                for auth_event_id in event.auth_event_ids():",
            "                    ae = persisted_events.get(auth_event_id)",
            "                    if not ae:",
            "                        # the fact we can't find the auth event doesn't mean it doesn't",
            "                        # exist, which means it is premature to reject `event`. Instead we",
            "                        # just ignore it for now.",
            "                        logger.warning(",
            "                            \"Dropping event %s, which relies on auth_event %s, which could not be found\",",
            "                            event,",
            "                            auth_event_id,",
            "                        )",
            "                        return",
            "                    auth.append(ae)",
            "",
            "                # we're not bothering about room state, so flag the event as an outlier.",
            "                event.internal_metadata.outlier = True",
            "",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    await check_state_independent_auth_rules(self._store, event)",
            "                    check_state_dependent_auth_rules(event, auth)",
            "                except AuthError as e:",
            "                    logger.warning(\"Rejecting %r because %s\", event, e)",
            "                    context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "            events_and_contexts_to_persist.append((event, context))",
            "",
            "        for event in fetched_events:",
            "            await prep(event)",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            events_and_contexts_to_persist,",
            "            # Mark these events backfilled as they're historic events that will",
            "            # eventually be backfilled. For example, missing events we fetch",
            "            # during backfill should be marked as backfilled as well.",
            "            backfilled=True,",
            "        )",
            "",
            "    @trace",
            "    async def _check_event_auth(",
            "        self, origin: Optional[str], event: EventBase, context: EventContext",
            "    ) -> None:",
            "        \"\"\"",
            "        Checks whether an event should be rejected (for failing auth checks).",
            "",
            "        Args:",
            "            origin: The host the event originates from. This is used to fetch",
            "               any missing auth events. It can be set to None, but only if we are",
            "               sure that we already have all the auth events.",
            "            event: The event itself.",
            "            context:",
            "                The event context.",
            "",
            "        Raises:",
            "            AuthError if we were unable to find copies of the event's auth events.",
            "               (Most other failures just cause us to set `context.rejected`.)",
            "        \"\"\"",
            "        # This method should only be used for non-outliers",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # first of all, check that the event itself is valid.",
            "        try:",
            "            validate_event_for_room_version(event)",
            "        except AuthError as e:",
            "            logger.warning(\"While validating received event %r: %s\", event, e)",
            "            # TODO: use a different rejected reason here?",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "",
            "        # next, check that we have all of the event's auth events.",
            "        #",
            "        # Note that this can raise AuthError, which we want to propagate to the",
            "        # caller rather than swallow with `context.rejected` (since we cannot be",
            "        # certain that there is a permanent problem with the event).",
            "        claimed_auth_events = await self._load_or_fetch_auth_events_for_event(",
            "            origin, event",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events\",",
            "            str([ev.event_id for ev in claimed_auth_events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events.length\",",
            "            str(len(claimed_auth_events)),",
            "        )",
            "",
            "        # ... and check that the event passes auth at those auth events.",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   4. Passes authorization rules based on the event\u2019s auth events,",
            "        #      otherwise it is rejected.",
            "        try:",
            "            await check_state_independent_auth_rules(self._store, event)",
            "            check_state_dependent_auth_rules(event, claimed_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against auth_events: %s\", event, e",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "",
            "        # now check the auth rules pass against the room state before the event",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   5. Passes authorization rules based on the state before the event,",
            "        #      otherwise it is rejected.",
            "        #",
            "        # ... however, if we only have partial state for the room, then there is a good",
            "        # chance that we'll be missing some of the state needed to auth the new event.",
            "        # So, we state-resolve the auth events that we are given against the state that",
            "        # we know about, which ensures things like bans are applied. (Note that we'll",
            "        # already have checked we have all the auth events, in",
            "        # _load_or_fetch_auth_events_for_event above)",
            "        if context.partial_state:",
            "            room_version = await self._store.get_room_version_id(event.room_id)",
            "",
            "            local_state_id_map = await context.get_prev_state_ids()",
            "            claimed_auth_events_id_map = {",
            "                (ev.type, ev.state_key): ev.event_id for ev in claimed_auth_events",
            "            }",
            "",
            "            state_for_auth_id_map = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    [local_state_id_map, claimed_auth_events_id_map],",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "            state_for_auth_id_map = await context.get_prev_state_ids(",
            "                StateFilter.from_types(event_types)",
            "            )",
            "",
            "        calculated_auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "            event, state_for_auth_id_map, for_verification=True",
            "        )",
            "",
            "        # if those are the same, we're done here.",
            "        if collections.Counter(event.auth_event_ids()) == collections.Counter(",
            "            calculated_auth_event_ids",
            "        ):",
            "            return",
            "",
            "        # otherwise, re-run the auth checks based on what we calculated.",
            "        calculated_auth_events = await self._store.get_events_as_list(",
            "            calculated_auth_event_ids",
            "        )",
            "",
            "        # log the differences",
            "",
            "        claimed_auth_event_map = {(e.type, e.state_key): e for e in claimed_auth_events}",
            "        calculated_auth_event_map = {",
            "            (e.type, e.state_key): e for e in calculated_auth_events",
            "        }",
            "        logger.info(",
            "            \"event's auth_events are different to our calculated auth_events. \"",
            "            \"Claimed but not calculated: %s. Calculated but not claimed: %s\",",
            "            [",
            "                ev",
            "                for k, ev in claimed_auth_event_map.items()",
            "                if k not in calculated_auth_event_map",
            "                or calculated_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "            [",
            "                ev",
            "                for k, ev in calculated_auth_event_map.items()",
            "                if k not in claimed_auth_event_map",
            "                or claimed_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, calculated_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against room state before the event: %s\",",
            "                event,",
            "                e,",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "    @trace",
            "    async def _maybe_kick_guest_users(self, event: EventBase) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state = await self._storage_controllers.state.get_current_state(",
            "            event.room_id",
            "        )",
            "        current_state_list = list(current_state.values())",
            "        await self._get_room_member_handler().kick_guest_users(current_state_list)",
            "",
            "    async def _check_for_soft_fail(",
            "        self,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        origin: str,",
            "    ) -> None:",
            "        \"\"\"Checks if we should soft fail the event; if so, marks the event as",
            "        such.",
            "",
            "        Does nothing for events in rooms with partial state, since we may not have an",
            "        accurate membership event for the sender in the current state.",
            "",
            "        Args:",
            "            event",
            "            context: The `EventContext` which we are about to persist the event with.",
            "            origin: The host the event originates from.",
            "        \"\"\"",
            "        if await self._store.is_partial_state_room(event.room_id):",
            "            # We might not know the sender's membership in the current state, so don't",
            "            # soft fail anything. Even if we do have a membership for the sender in the",
            "            # current state, it may have been derived from state resolution between",
            "            # partial and full state and may not be accurate.",
            "            return",
            "",
            "        extrem_ids_list = await self._store.get_latest_event_ids_in_room(event.room_id)",
            "        extrem_ids = set(extrem_ids_list)",
            "        prev_event_ids = set(event.prev_event_ids())",
            "",
            "        if extrem_ids == prev_event_ids:",
            "            # If they're the same then the current state is the same as the",
            "            # state at the event, so no point rechecking auth for soft fail.",
            "            return",
            "",
            "        room_version = await self._store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        # The event types we want to pull from the \"current\" state.",
            "        auth_types = auth_types_for_event(room_version_obj, event)",
            "",
            "        # Calculate the \"current state\".",
            "        seen_event_ids = await self._store.have_events_in_timeline(prev_event_ids)",
            "        has_missing_prevs = bool(prev_event_ids - seen_event_ids)",
            "        if has_missing_prevs:",
            "            # We don't have all the prev_events of this event, which means we have a",
            "            # gap in the graph, and the new event is going to become a new backwards",
            "            # extremity.",
            "            #",
            "            # In this case we want to be a little careful as we might have been",
            "            # down for a while and have an incorrect view of the current state,",
            "            # however we still want to do checks as gaps are easy to",
            "            # maliciously manufacture.",
            "            #",
            "            # So we use a \"current state\" that is actually a state",
            "            # resolution across the current forward extremities and the",
            "            # given state at the event. This should correctly handle cases",
            "            # like bans, especially with state res v2.",
            "",
            "            state_sets_d = await self._state_storage_controller.get_state_groups_ids(",
            "                event.room_id, extrem_ids",
            "            )",
            "            state_sets: List[StateMap[str]] = list(state_sets_d.values())",
            "            state_ids = await context.get_prev_state_ids()",
            "            state_sets.append(state_ids)",
            "            current_state_ids = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    state_sets,",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            current_state_ids = (",
            "                await self._state_storage_controller.get_current_state_ids(",
            "                    event.room_id, StateFilter.from_types(auth_types)",
            "                )",
            "            )",
            "",
            "        logger.debug(",
            "            \"Doing soft-fail check for %s: state %s\",",
            "            event.event_id,",
            "            current_state_ids,",
            "        )",
            "",
            "        # Now check if event pass auth against said current state",
            "        current_state_ids_list = [",
            "            e for k, e in current_state_ids.items() if k in auth_types",
            "        ]",
            "        current_auth_events = await self._store.get_events_as_list(",
            "            current_state_ids_list",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, current_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"Soft-failing %r (from %s) because %s\",",
            "                event,",
            "                e,",
            "                origin,",
            "                extra={",
            "                    \"room_id\": event.room_id,",
            "                    \"mxid\": event.sender,",
            "                    \"hs\": origin,",
            "                },",
            "            )",
            "            soft_failed_event_counter.inc()",
            "            event.internal_metadata.soft_failed = True",
            "",
            "    async def _load_or_fetch_auth_events_for_event(",
            "        self, destination: Optional[str], event: EventBase",
            "    ) -> Collection[EventBase]:",
            "        \"\"\"Fetch this event's auth_events, from database or remote",
            "",
            "        Loads any of the auth_events that we already have from the database/cache. If",
            "        there are any that are missing, calls /event_auth to get the complete auth",
            "        chain for the event (and then attempts to load the auth_events again).",
            "",
            "        If any of the auth_events cannot be found, raises an AuthError. This can happen",
            "        for a number of reasons; eg: the events don't exist, or we were unable to talk",
            "        to `destination`, or we couldn't validate the signature on the event (which",
            "        in turn has multiple potential causes).",
            "",
            "        Args:",
            "            destination: where to send the /event_auth request. Typically the server",
            "               that sent us `event` in the first place.",
            "",
            "               If this is None, no attempt is made to load any missing auth events:",
            "               rather, an AssertionError is raised if there are any missing events.",
            "",
            "            event: the event whose auth_events we want",
            "",
            "        Returns:",
            "            all of the events listed in `event.auth_events_ids`, after deduplication",
            "",
            "        Raises:",
            "            AssertionError if some auth events were missing and no `destination` was",
            "            supplied.",
            "",
            "            AuthError if we were unable to fetch the auth_events for any reason.",
            "        \"\"\"",
            "        event_auth_event_ids = set(event.auth_event_ids())",
            "        event_auth_events = await self._store.get_events(",
            "            event_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids = event_auth_event_ids.difference(",
            "            event_auth_events.keys()",
            "        )",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "        if destination is None:",
            "            # this shouldn't happen: destination must be set unless we know we have already",
            "            # persisted the auth events.",
            "            raise AssertionError(",
            "                \"_load_or_fetch_auth_events_for_event() called with no destination for \"",
            "                \"an event with missing auth_events\"",
            "            )",
            "",
            "        logger.info(",
            "            \"Event %s refers to unknown auth events %s: fetching auth chain\",",
            "            event,",
            "            missing_auth_event_ids,",
            "        )",
            "        try:",
            "            await self._get_remote_auth_chain_for_event(",
            "                destination, event.room_id, event.event_id",
            "            )",
            "        except Exception as e:",
            "            logger.warning(\"Failed to get auth chain for %s: %s\", event, e)",
            "            # in this case, it's very likely we still won't have all the auth",
            "            # events - but we pick that up below.",
            "",
            "        # try to fetch the auth events we missed list time.",
            "        extra_auth_events = await self._store.get_events(",
            "            missing_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids.difference_update(extra_auth_events.keys())",
            "        event_auth_events.update(extra_auth_events)",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "",
            "        # we still don't have all the auth events.",
            "        logger.warning(",
            "            \"Missing auth events for %s: %s\",",
            "            event,",
            "            shortstr(missing_auth_event_ids),",
            "        )",
            "        # the fact we can't find the auth event doesn't mean it doesn't",
            "        # exist, which means it is premature to store `event` as rejected.",
            "        # instead we raise an AuthError, which will make the caller ignore it.",
            "        raise AuthError(code=HTTPStatus.FORBIDDEN, msg=\"Auth events could not be found\")",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_remote_auth_chain_for_event(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"If we are missing some of an event's auth events, attempt to request them",
            "",
            "        Args:",
            "            destination: where to fetch the auth tree from",
            "            room_id: the room in which we are lacking auth events",
            "            event_id: the event for which we are lacking auth events",
            "        \"\"\"",
            "        try:",
            "            remote_events = await self._federation_client.get_event_auth(",
            "                destination, room_id, event_id",
            "            )",
            "",
            "        except RequestSendFailed as e1:",
            "            # The other side isn't around or doesn't implement the",
            "            # endpoint, so lets just bail out.",
            "            logger.info(\"Failed to get event auth from remote: %s\", e1)",
            "            return",
            "",
            "        logger.info(\"/event_auth returned %i events\", len(remote_events))",
            "",
            "        # `event` may be returned, but we should not yet process it.",
            "        remote_auth_events = (e for e in remote_events if e.event_id != event_id)",
            "",
            "        await self._auth_and_persist_outliers(room_id, remote_auth_events)",
            "",
            "    @trace",
            "    async def _run_push_actions_and_persist_event(",
            "        self, event: EventBase, context: EventContext, backfilled: bool = False",
            "    ) -> None:",
            "        \"\"\"Run the push actions for a received event, and persist it.",
            "",
            "        Args:",
            "            event: The event itself.",
            "            context: The event context.",
            "            backfilled: True if the event was backfilled.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "        # this method should not be called on outliers (those code paths call",
            "        # persist_events_and_notify directly.)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        if not backfilled and not context.rejected:",
            "            min_depth = await self._store.get_min_depth(event.room_id)",
            "            if min_depth is None or min_depth > event.depth:",
            "                # XXX richvdh 2021/10/07: I don't really understand what this",
            "                # condition is doing. I think it's trying not to send pushes",
            "                # for events that predate our join - but that's not really what",
            "                # min_depth means, and anyway ancient events are a more general",
            "                # problem.",
            "                #",
            "                # for now I'm just going to log about it.",
            "                logger.info(",
            "                    \"Skipping push actions for old event with depth %s < %s\",",
            "                    event.depth,",
            "                    min_depth,",
            "                )",
            "            else:",
            "                await self._bulk_push_rule_evaluator.action_for_event_by_user(",
            "                    event, context",
            "                )",
            "",
            "        try:",
            "            await self.persist_events_and_notify(",
            "                event.room_id, [(event, context)], backfilled=backfilled",
            "            )",
            "        except Exception:",
            "            await self._store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def persist_events_and_notify(",
            "        self,",
            "        room_id: str,",
            "        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],",
            "        backfilled: bool = False,",
            "    ) -> int:",
            "        \"\"\"Persists events and tells the notifier/pushers about them, if",
            "        necessary.",
            "",
            "        Args:",
            "            room_id: The room ID of events being persisted.",
            "            event_and_contexts: Sequence of events with their associated",
            "                context that should be persisted. All events must belong to",
            "                the same room.",
            "            backfilled: Whether these events are a result of",
            "                backfilling or not",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        if not event_and_contexts:",
            "            return self._store.get_room_max_stream_ordering()",
            "",
            "        instance = self._config.worker.events_shard_config.get_instance(room_id)",
            "        if instance != self._instance_name:",
            "            # Limit the number of events sent over replication. We choose 200",
            "            # here as that is what we default to in `max_request_body_size(..)`",
            "            try:",
            "                for batch in batch_iter(event_and_contexts, 200):",
            "                    result = await self._send_events(",
            "                        instance_name=instance,",
            "                        store=self._store,",
            "                        room_id=room_id,",
            "                        event_and_contexts=batch,",
            "                        backfilled=backfilled,",
            "                    )",
            "            except SynapseError as e:",
            "                if e.code == HTTPStatus.CONFLICT:",
            "                    raise PartialStateConflictError()",
            "                raise",
            "            return result[\"max_stream_id\"]",
            "        else:",
            "            assert self._storage_controllers.persistence",
            "",
            "            # Note that this returns the events that were persisted, which may not be",
            "            # the same as were passed in if some were deduplicated due to transaction IDs.",
            "            (",
            "                events,",
            "                max_stream_token,",
            "            ) = await self._storage_controllers.persistence.persist_events(",
            "                event_and_contexts, backfilled=backfilled",
            "            )",
            "",
            "            if self._ephemeral_messages_enabled:",
            "                for event in events:",
            "                    # If there's an expiry timestamp on the event, schedule its expiry.",
            "                    self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            if not backfilled:  # Never notify for backfilled events",
            "                with start_active_span(\"notify_persisted_events\"):",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids\",",
            "                        str([ev.event_id for ev in events]),",
            "                    )",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids.length\",",
            "                        str(len(events)),",
            "                    )",
            "                    for event in events:",
            "                        await self._notify_persisted_event(event, max_stream_token)",
            "",
            "            return max_stream_token.stream",
            "",
            "    async def _notify_persisted_event(",
            "        self, event: EventBase, max_stream_token: RoomStreamToken",
            "    ) -> None:",
            "        \"\"\"Checks to see if notifier/pushers should be notified about the",
            "        event or not.",
            "",
            "        Args:",
            "            event:",
            "            max_stream_token: The max_stream_id returned by persist_events",
            "        \"\"\"",
            "",
            "        extra_users = []",
            "        if event.type == EventTypes.Member:",
            "            target_user_id = event.state_key",
            "",
            "            # We notify for memberships if its an invite for one of our",
            "            # users",
            "            if event.internal_metadata.is_outlier():",
            "                if event.membership != Membership.INVITE:",
            "                    if not self._is_mine_id(target_user_id):",
            "                        return",
            "",
            "            target_user = UserID.from_string(target_user_id)",
            "            extra_users.append(target_user)",
            "        elif event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        # the event has been persisted so it should have a stream ordering.",
            "        assert event.internal_metadata.stream_ordering",
            "",
            "        event_pos = PersistedEventPosition(",
            "            self._instance_name, event.internal_metadata.stream_ordering",
            "        )",
            "        await self._notifier.on_new_room_event(",
            "            event, event_pos, max_stream_token, extra_users=extra_users",
            "        )",
            "",
            "        if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "            # TODO retrieve the previous state, and exclude join -> join transitions",
            "            self._notifier.notify_user_joined_room(event.event_id, event.room_id)",
            "",
            "    def _sanity_check_event(self, ev: EventBase) -> None:",
            "        \"\"\"",
            "        Do some early sanity checks of a received event",
            "",
            "        In particular, checks it doesn't have an excessive number of",
            "        prev_events or auth_events, which could cause a huge state resolution",
            "        or cascade of event fetches.",
            "",
            "        Args:",
            "            ev: event to be checked",
            "",
            "        Raises:",
            "            SynapseError if the event does not pass muster",
            "        \"\"\"",
            "        if len(ev.prev_event_ids()) > 20:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i prev_events\",",
            "                ev.event_id,",
            "                len(ev.prev_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")",
            "",
            "        if len(ev.auth_event_ids()) > 10:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i auth_events\",",
            "                ev.event_id,",
            "                len(ev.auth_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "241": [
                "FederationEventHandler"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/receipts.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "             # If we're not in the room just ditch the event entirely. This is"
            },
            "1": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "             # probably an old server that has come back and thinks we're still in"
            },
            "2": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "             # the room (or we've been rejoined to the room by a state reset)."
            },
            "3": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            is_in_room = await self.event_auth_handler.check_host_in_room("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+            is_in_room = await self.event_auth_handler.is_host_in_room("
            },
            "5": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "                 room_id, self.server_name"
            },
            "6": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "             )"
            },
            "7": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "             if not is_in_room:"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Iterable, List, Optional, Tuple",
            "",
            "from synapse.api.constants import EduTypes, ReceiptTypes",
            "from synapse.appservice import ApplicationService",
            "from synapse.streams import EventSource",
            "from synapse.types import (",
            "    JsonDict,",
            "    ReadReceipt,",
            "    StreamKeyType,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class ReceiptsHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.notifier = hs.get_notifier()",
            "        self.server_name = hs.config.server.server_name",
            "        self.store = hs.get_datastores().main",
            "        self.event_auth_handler = hs.get_event_auth_handler()",
            "",
            "        self.hs = hs",
            "",
            "        # We only need to poke the federation sender explicitly if its on the",
            "        # same instance. Other federation sender instances will get notified by",
            "        # `synapse.app.generic_worker.FederationSenderHandler` when it sees it",
            "        # in the receipts stream.",
            "        self.federation_sender = None",
            "        if hs.should_send_federation():",
            "            self.federation_sender = hs.get_federation_sender()",
            "",
            "        # If we can handle the receipt EDUs we do so, otherwise we route them",
            "        # to the appropriate worker.",
            "        if hs.get_instance_name() in hs.config.worker.writers.receipts:",
            "            hs.get_federation_registry().register_edu_handler(",
            "                EduTypes.RECEIPT, self._received_remote_receipt",
            "            )",
            "        else:",
            "            hs.get_federation_registry().register_instances_for_edu(",
            "                EduTypes.RECEIPT,",
            "                hs.config.worker.writers.receipts,",
            "            )",
            "",
            "        self.clock = self.hs.get_clock()",
            "        self.state = hs.get_state_handler()",
            "",
            "    async def _received_remote_receipt(self, origin: str, content: JsonDict) -> None:",
            "        \"\"\"Called when we receive an EDU of type m.receipt from a remote HS.\"\"\"",
            "        receipts = []",
            "        for room_id, room_values in content.items():",
            "            # If we're not in the room just ditch the event entirely. This is",
            "            # probably an old server that has come back and thinks we're still in",
            "            # the room (or we've been rejoined to the room by a state reset).",
            "            is_in_room = await self.event_auth_handler.check_host_in_room(",
            "                room_id, self.server_name",
            "            )",
            "            if not is_in_room:",
            "                logger.info(",
            "                    \"Ignoring receipt for room %r from server %s as we're not in the room\",",
            "                    room_id,",
            "                    origin,",
            "                )",
            "                continue",
            "",
            "            for receipt_type, users in room_values.items():",
            "                for user_id, user_values in users.items():",
            "                    if get_domain_from_id(user_id) != origin:",
            "                        logger.info(",
            "                            \"Received receipt for user %r from server %s, ignoring\",",
            "                            user_id,",
            "                            origin,",
            "                        )",
            "                        continue",
            "",
            "                    receipts.append(",
            "                        ReadReceipt(",
            "                            room_id=room_id,",
            "                            receipt_type=receipt_type,",
            "                            user_id=user_id,",
            "                            event_ids=user_values[\"event_ids\"],",
            "                            data=user_values.get(\"data\", {}),",
            "                        )",
            "                    )",
            "",
            "        await self._handle_new_receipts(receipts)",
            "",
            "    async def _handle_new_receipts(self, receipts: List[ReadReceipt]) -> bool:",
            "        \"\"\"Takes a list of receipts, stores them and informs the notifier.\"\"\"",
            "        min_batch_id: Optional[int] = None",
            "        max_batch_id: Optional[int] = None",
            "",
            "        for receipt in receipts:",
            "            res = await self.store.insert_receipt(",
            "                receipt.room_id,",
            "                receipt.receipt_type,",
            "                receipt.user_id,",
            "                receipt.event_ids,",
            "                receipt.data,",
            "            )",
            "",
            "            if not res:",
            "                # res will be None if this receipt is 'old'",
            "                continue",
            "",
            "            stream_id, max_persisted_id = res",
            "",
            "            if min_batch_id is None or stream_id < min_batch_id:",
            "                min_batch_id = stream_id",
            "            if max_batch_id is None or max_persisted_id > max_batch_id:",
            "                max_batch_id = max_persisted_id",
            "",
            "        # Either both of these should be None or neither.",
            "        if min_batch_id is None or max_batch_id is None:",
            "            # no new receipts",
            "            return False",
            "",
            "        affected_room_ids = list({r.room_id for r in receipts})",
            "",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.RECEIPT, max_batch_id, rooms=affected_room_ids",
            "        )",
            "        # Note that the min here shouldn't be relied upon to be accurate.",
            "        await self.hs.get_pusherpool().on_new_receipts(",
            "            min_batch_id, max_batch_id, affected_room_ids",
            "        )",
            "",
            "        return True",
            "",
            "    async def received_client_receipt(",
            "        self, room_id: str, receipt_type: str, user_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"Called when a client tells us a local user has read up to the given",
            "        event_id in the room.",
            "        \"\"\"",
            "        receipt = ReadReceipt(",
            "            room_id=room_id,",
            "            receipt_type=receipt_type,",
            "            user_id=user_id,",
            "            event_ids=[event_id],",
            "            data={\"ts\": int(self.clock.time_msec())},",
            "        )",
            "",
            "        is_new = await self._handle_new_receipts([receipt])",
            "        if not is_new:",
            "            return",
            "",
            "        if self.federation_sender and receipt_type != ReceiptTypes.READ_PRIVATE:",
            "            await self.federation_sender.send_read_receipt(receipt)",
            "",
            "",
            "class ReceiptEventSource(EventSource[int, JsonDict]):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self.config = hs.config",
            "",
            "    @staticmethod",
            "    def filter_out_private_receipts(",
            "        rooms: List[JsonDict], user_id: str",
            "    ) -> List[JsonDict]:",
            "        \"\"\"",
            "        Filters a list of serialized receipts (as returned by /sync and /initialSync)",
            "        and removes private read receipts of other users.",
            "",
            "        This operates on the return value of get_linearized_receipts_for_rooms(),",
            "        which is wrapped in a cache. Care must be taken to ensure that the input",
            "        values are not modified.",
            "",
            "        Args:",
            "            rooms: A list of mappings, each mapping has a `content` field, which",
            "                is a map of event ID -> receipt type -> user ID -> receipt information.",
            "",
            "        Returns:",
            "            The same as rooms, but filtered.",
            "        \"\"\"",
            "",
            "        result = []",
            "",
            "        # Iterate through each room's receipt content.",
            "        for room in rooms:",
            "            # The receipt content with other user's private read receipts removed.",
            "            content = {}",
            "",
            "            # Iterate over each event ID / receipts for that event.",
            "            for event_id, orig_event_content in room.get(\"content\", {}).items():",
            "                event_content = orig_event_content",
            "                # If there are private read receipts, additional logic is necessary.",
            "                if ReceiptTypes.READ_PRIVATE in event_content:",
            "                    # Make a copy without private read receipts to avoid leaking",
            "                    # other user's private read receipts..",
            "                    event_content = {",
            "                        receipt_type: receipt_value",
            "                        for receipt_type, receipt_value in event_content.items()",
            "                        if receipt_type != ReceiptTypes.READ_PRIVATE",
            "                    }",
            "",
            "                    # Copy the current user's private read receipt from the",
            "                    # original content, if it exists.",
            "                    user_private_read_receipt = orig_event_content[",
            "                        ReceiptTypes.READ_PRIVATE",
            "                    ].get(user_id, None)",
            "                    if user_private_read_receipt:",
            "                        event_content[ReceiptTypes.READ_PRIVATE] = {",
            "                            user_id: user_private_read_receipt",
            "                        }",
            "",
            "                # Include the event if there is at least one non-private read",
            "                # receipt or the current user has a private read receipt.",
            "                if event_content:",
            "                    content[event_id] = event_content",
            "",
            "            # Include the event if there is at least one non-private read receipt",
            "            # or the current user has a private read receipt.",
            "            if content:",
            "                # Build a new event to avoid mutating the cache.",
            "                new_room = {k: v for k, v in room.items() if k != \"content\"}",
            "                new_room[\"content\"] = content",
            "                result.append(new_room)",
            "",
            "        return result",
            "",
            "    async def get_new_events(",
            "        self,",
            "        user: UserID,",
            "        from_key: int,",
            "        limit: Optional[int],",
            "        room_ids: Iterable[str],",
            "        is_guest: bool,",
            "        explicit_room_id: Optional[str] = None,",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        from_key = int(from_key)",
            "        to_key = self.get_current_key()",
            "",
            "        if from_key == to_key:",
            "            return [], to_key",
            "",
            "        events = await self.store.get_linearized_receipts_for_rooms(",
            "            room_ids, from_key=from_key, to_key=to_key",
            "        )",
            "",
            "        events = ReceiptEventSource.filter_out_private_receipts(",
            "            events, user.to_string()",
            "        )",
            "",
            "        return events, to_key",
            "",
            "    async def get_new_events_as(",
            "        self, from_key: int, to_key: int, service: ApplicationService",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        \"\"\"Returns a set of new read receipt events that an appservice",
            "        may be interested in.",
            "",
            "        Args:",
            "            from_key: the stream position at which events should be fetched from",
            "            to_key: the stream position up to which events should be fetched to",
            "            service: The appservice which may be interested",
            "",
            "        Returns:",
            "            A two-tuple containing the following:",
            "                * A list of json dictionaries derived from read receipts that the",
            "                  appservice may be interested in.",
            "                * The current read receipt stream token.",
            "        \"\"\"",
            "        from_key = int(from_key)",
            "",
            "        if from_key == to_key:",
            "            return [], to_key",
            "",
            "        # Fetch all read receipts for all rooms, up to a limit of 100. This is ordered",
            "        # by most recent.",
            "        rooms_to_events = await self.store.get_linearized_receipts_for_all_rooms(",
            "            from_key=from_key, to_key=to_key",
            "        )",
            "",
            "        # Then filter down to rooms that the AS can read",
            "        events = []",
            "        for room_id, event in rooms_to_events.items():",
            "            if not await service.is_interested_in_room(room_id, self.store):",
            "                continue",
            "",
            "            events.append(event)",
            "",
            "        return events, to_key",
            "",
            "    def get_current_key(self, direction: str = \"f\") -> int:",
            "        return self.store.get_max_receipt_stream_id()"
        ],
        "afterPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Iterable, List, Optional, Tuple",
            "",
            "from synapse.api.constants import EduTypes, ReceiptTypes",
            "from synapse.appservice import ApplicationService",
            "from synapse.streams import EventSource",
            "from synapse.types import (",
            "    JsonDict,",
            "    ReadReceipt,",
            "    StreamKeyType,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class ReceiptsHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.notifier = hs.get_notifier()",
            "        self.server_name = hs.config.server.server_name",
            "        self.store = hs.get_datastores().main",
            "        self.event_auth_handler = hs.get_event_auth_handler()",
            "",
            "        self.hs = hs",
            "",
            "        # We only need to poke the federation sender explicitly if its on the",
            "        # same instance. Other federation sender instances will get notified by",
            "        # `synapse.app.generic_worker.FederationSenderHandler` when it sees it",
            "        # in the receipts stream.",
            "        self.federation_sender = None",
            "        if hs.should_send_federation():",
            "            self.federation_sender = hs.get_federation_sender()",
            "",
            "        # If we can handle the receipt EDUs we do so, otherwise we route them",
            "        # to the appropriate worker.",
            "        if hs.get_instance_name() in hs.config.worker.writers.receipts:",
            "            hs.get_federation_registry().register_edu_handler(",
            "                EduTypes.RECEIPT, self._received_remote_receipt",
            "            )",
            "        else:",
            "            hs.get_federation_registry().register_instances_for_edu(",
            "                EduTypes.RECEIPT,",
            "                hs.config.worker.writers.receipts,",
            "            )",
            "",
            "        self.clock = self.hs.get_clock()",
            "        self.state = hs.get_state_handler()",
            "",
            "    async def _received_remote_receipt(self, origin: str, content: JsonDict) -> None:",
            "        \"\"\"Called when we receive an EDU of type m.receipt from a remote HS.\"\"\"",
            "        receipts = []",
            "        for room_id, room_values in content.items():",
            "            # If we're not in the room just ditch the event entirely. This is",
            "            # probably an old server that has come back and thinks we're still in",
            "            # the room (or we've been rejoined to the room by a state reset).",
            "            is_in_room = await self.event_auth_handler.is_host_in_room(",
            "                room_id, self.server_name",
            "            )",
            "            if not is_in_room:",
            "                logger.info(",
            "                    \"Ignoring receipt for room %r from server %s as we're not in the room\",",
            "                    room_id,",
            "                    origin,",
            "                )",
            "                continue",
            "",
            "            for receipt_type, users in room_values.items():",
            "                for user_id, user_values in users.items():",
            "                    if get_domain_from_id(user_id) != origin:",
            "                        logger.info(",
            "                            \"Received receipt for user %r from server %s, ignoring\",",
            "                            user_id,",
            "                            origin,",
            "                        )",
            "                        continue",
            "",
            "                    receipts.append(",
            "                        ReadReceipt(",
            "                            room_id=room_id,",
            "                            receipt_type=receipt_type,",
            "                            user_id=user_id,",
            "                            event_ids=user_values[\"event_ids\"],",
            "                            data=user_values.get(\"data\", {}),",
            "                        )",
            "                    )",
            "",
            "        await self._handle_new_receipts(receipts)",
            "",
            "    async def _handle_new_receipts(self, receipts: List[ReadReceipt]) -> bool:",
            "        \"\"\"Takes a list of receipts, stores them and informs the notifier.\"\"\"",
            "        min_batch_id: Optional[int] = None",
            "        max_batch_id: Optional[int] = None",
            "",
            "        for receipt in receipts:",
            "            res = await self.store.insert_receipt(",
            "                receipt.room_id,",
            "                receipt.receipt_type,",
            "                receipt.user_id,",
            "                receipt.event_ids,",
            "                receipt.data,",
            "            )",
            "",
            "            if not res:",
            "                # res will be None if this receipt is 'old'",
            "                continue",
            "",
            "            stream_id, max_persisted_id = res",
            "",
            "            if min_batch_id is None or stream_id < min_batch_id:",
            "                min_batch_id = stream_id",
            "            if max_batch_id is None or max_persisted_id > max_batch_id:",
            "                max_batch_id = max_persisted_id",
            "",
            "        # Either both of these should be None or neither.",
            "        if min_batch_id is None or max_batch_id is None:",
            "            # no new receipts",
            "            return False",
            "",
            "        affected_room_ids = list({r.room_id for r in receipts})",
            "",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.RECEIPT, max_batch_id, rooms=affected_room_ids",
            "        )",
            "        # Note that the min here shouldn't be relied upon to be accurate.",
            "        await self.hs.get_pusherpool().on_new_receipts(",
            "            min_batch_id, max_batch_id, affected_room_ids",
            "        )",
            "",
            "        return True",
            "",
            "    async def received_client_receipt(",
            "        self, room_id: str, receipt_type: str, user_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"Called when a client tells us a local user has read up to the given",
            "        event_id in the room.",
            "        \"\"\"",
            "        receipt = ReadReceipt(",
            "            room_id=room_id,",
            "            receipt_type=receipt_type,",
            "            user_id=user_id,",
            "            event_ids=[event_id],",
            "            data={\"ts\": int(self.clock.time_msec())},",
            "        )",
            "",
            "        is_new = await self._handle_new_receipts([receipt])",
            "        if not is_new:",
            "            return",
            "",
            "        if self.federation_sender and receipt_type != ReceiptTypes.READ_PRIVATE:",
            "            await self.federation_sender.send_read_receipt(receipt)",
            "",
            "",
            "class ReceiptEventSource(EventSource[int, JsonDict]):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self.config = hs.config",
            "",
            "    @staticmethod",
            "    def filter_out_private_receipts(",
            "        rooms: List[JsonDict], user_id: str",
            "    ) -> List[JsonDict]:",
            "        \"\"\"",
            "        Filters a list of serialized receipts (as returned by /sync and /initialSync)",
            "        and removes private read receipts of other users.",
            "",
            "        This operates on the return value of get_linearized_receipts_for_rooms(),",
            "        which is wrapped in a cache. Care must be taken to ensure that the input",
            "        values are not modified.",
            "",
            "        Args:",
            "            rooms: A list of mappings, each mapping has a `content` field, which",
            "                is a map of event ID -> receipt type -> user ID -> receipt information.",
            "",
            "        Returns:",
            "            The same as rooms, but filtered.",
            "        \"\"\"",
            "",
            "        result = []",
            "",
            "        # Iterate through each room's receipt content.",
            "        for room in rooms:",
            "            # The receipt content with other user's private read receipts removed.",
            "            content = {}",
            "",
            "            # Iterate over each event ID / receipts for that event.",
            "            for event_id, orig_event_content in room.get(\"content\", {}).items():",
            "                event_content = orig_event_content",
            "                # If there are private read receipts, additional logic is necessary.",
            "                if ReceiptTypes.READ_PRIVATE in event_content:",
            "                    # Make a copy without private read receipts to avoid leaking",
            "                    # other user's private read receipts..",
            "                    event_content = {",
            "                        receipt_type: receipt_value",
            "                        for receipt_type, receipt_value in event_content.items()",
            "                        if receipt_type != ReceiptTypes.READ_PRIVATE",
            "                    }",
            "",
            "                    # Copy the current user's private read receipt from the",
            "                    # original content, if it exists.",
            "                    user_private_read_receipt = orig_event_content[",
            "                        ReceiptTypes.READ_PRIVATE",
            "                    ].get(user_id, None)",
            "                    if user_private_read_receipt:",
            "                        event_content[ReceiptTypes.READ_PRIVATE] = {",
            "                            user_id: user_private_read_receipt",
            "                        }",
            "",
            "                # Include the event if there is at least one non-private read",
            "                # receipt or the current user has a private read receipt.",
            "                if event_content:",
            "                    content[event_id] = event_content",
            "",
            "            # Include the event if there is at least one non-private read receipt",
            "            # or the current user has a private read receipt.",
            "            if content:",
            "                # Build a new event to avoid mutating the cache.",
            "                new_room = {k: v for k, v in room.items() if k != \"content\"}",
            "                new_room[\"content\"] = content",
            "                result.append(new_room)",
            "",
            "        return result",
            "",
            "    async def get_new_events(",
            "        self,",
            "        user: UserID,",
            "        from_key: int,",
            "        limit: Optional[int],",
            "        room_ids: Iterable[str],",
            "        is_guest: bool,",
            "        explicit_room_id: Optional[str] = None,",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        from_key = int(from_key)",
            "        to_key = self.get_current_key()",
            "",
            "        if from_key == to_key:",
            "            return [], to_key",
            "",
            "        events = await self.store.get_linearized_receipts_for_rooms(",
            "            room_ids, from_key=from_key, to_key=to_key",
            "        )",
            "",
            "        events = ReceiptEventSource.filter_out_private_receipts(",
            "            events, user.to_string()",
            "        )",
            "",
            "        return events, to_key",
            "",
            "    async def get_new_events_as(",
            "        self, from_key: int, to_key: int, service: ApplicationService",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        \"\"\"Returns a set of new read receipt events that an appservice",
            "        may be interested in.",
            "",
            "        Args:",
            "            from_key: the stream position at which events should be fetched from",
            "            to_key: the stream position up to which events should be fetched to",
            "            service: The appservice which may be interested",
            "",
            "        Returns:",
            "            A two-tuple containing the following:",
            "                * A list of json dictionaries derived from read receipts that the",
            "                  appservice may be interested in.",
            "                * The current read receipt stream token.",
            "        \"\"\"",
            "        from_key = int(from_key)",
            "",
            "        if from_key == to_key:",
            "            return [], to_key",
            "",
            "        # Fetch all read receipts for all rooms, up to a limit of 100. This is ordered",
            "        # by most recent.",
            "        rooms_to_events = await self.store.get_linearized_receipts_for_all_rooms(",
            "            from_key=from_key, to_key=to_key",
            "        )",
            "",
            "        # Then filter down to rooms that the AS can read",
            "        events = []",
            "        for room_id, event in rooms_to_events.items():",
            "            if not await service.is_interested_in_room(room_id, self.store):",
            "                continue",
            "",
            "            events.append(event)",
            "",
            "        return events, to_key",
            "",
            "    def get_current_key(self, direction: str = \"f\") -> int:",
            "        return self.store.get_max_receipt_stream_id()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "73": [
                "ReceiptsHandler"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/room_summary.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 609,
                "afterPatchRowNumber": 609,
                "PatchRowcode": "         # If this is a request over federation, check if the host is in the room or"
            },
            "1": {
                "beforePatchRowNumber": 610,
                "afterPatchRowNumber": 610,
                "PatchRowcode": "         # has a user who could join the room."
            },
            "2": {
                "beforePatchRowNumber": 611,
                "afterPatchRowNumber": 611,
                "PatchRowcode": "         elif origin:"
            },
            "3": {
                "beforePatchRowNumber": 612,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if await self._event_auth_handler.check_host_in_room("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 612,
                "PatchRowcode": "+            if await self._event_auth_handler.is_host_in_room("
            },
            "5": {
                "beforePatchRowNumber": 613,
                "afterPatchRowNumber": 613,
                "PatchRowcode": "                 room_id, origin"
            },
            "6": {
                "beforePatchRowNumber": 614,
                "afterPatchRowNumber": 614,
                "PatchRowcode": "             ) or await self._store.is_host_invited(room_id, origin):"
            },
            "7": {
                "beforePatchRowNumber": 615,
                "afterPatchRowNumber": 615,
                "PatchRowcode": "                 return True"
            },
            "8": {
                "beforePatchRowNumber": 624,
                "afterPatchRowNumber": 624,
                "PatchRowcode": "                     await self._event_auth_handler.get_rooms_that_allow_join(state_ids)"
            },
            "9": {
                "beforePatchRowNumber": 625,
                "afterPatchRowNumber": 625,
                "PatchRowcode": "                 )"
            },
            "10": {
                "beforePatchRowNumber": 626,
                "afterPatchRowNumber": 626,
                "PatchRowcode": "                 for space_id in allowed_rooms:"
            },
            "11": {
                "beforePatchRowNumber": 627,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    if await self._event_auth_handler.check_host_in_room("
            },
            "12": {
                "beforePatchRowNumber": 628,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        space_id, origin"
            },
            "13": {
                "beforePatchRowNumber": 629,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    ):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 627,
                "PatchRowcode": "+                    if await self._event_auth_handler.is_host_in_room(space_id, origin):"
            },
            "15": {
                "beforePatchRowNumber": 630,
                "afterPatchRowNumber": 628,
                "PatchRowcode": "                         return True"
            },
            "16": {
                "beforePatchRowNumber": 631,
                "afterPatchRowNumber": 629,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 632,
                "afterPatchRowNumber": 630,
                "PatchRowcode": "         logger.info("
            }
        },
        "frontPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import itertools",
            "import logging",
            "import re",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Set, Tuple",
            "",
            "import attr",
            "",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    HistoryVisibility,",
            "    JoinRules,",
            "    Membership,",
            "    RoomTypes,",
            ")",
            "from synapse.api.errors import (",
            "    Codes,",
            "    NotFoundError,",
            "    StoreError,",
            "    SynapseError,",
            "    UnstableSpecAuthError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.ratelimiting import Ratelimiter",
            "from synapse.events import EventBase",
            "from synapse.types import JsonDict, Requester",
            "from synapse.util.caches.response_cache import ResponseCache",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# number of rooms to return. We'll stop once we hit this limit.",
            "MAX_ROOMS = 50",
            "",
            "# max number of events to return per room.",
            "MAX_ROOMS_PER_SPACE = 50",
            "",
            "# max number of federation servers to hit per room",
            "MAX_SERVERS_PER_SPACE = 3",
            "",
            "",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class _PaginationKey:",
            "    \"\"\"The key used to find unique pagination session.\"\"\"",
            "",
            "    # The first three entries match the request parameters (and cannot change",
            "    # during a pagination session).",
            "    room_id: str",
            "    suggested_only: bool",
            "    max_depth: Optional[int]",
            "    # The randomly generated token.",
            "    token: str",
            "",
            "",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class _PaginationSession:",
            "    \"\"\"The information that is stored for pagination.\"\"\"",
            "",
            "    # The time the pagination session was created, in milliseconds.",
            "    creation_time_ms: int",
            "    # The queue of rooms which are still to process.",
            "    room_queue: List[\"_RoomQueueEntry\"]",
            "    # A set of rooms which have been processed.",
            "    processed_rooms: Set[str]",
            "",
            "",
            "class RoomSummaryHandler:",
            "    # A unique key used for pagination sessions for the room hierarchy endpoint.",
            "    _PAGINATION_SESSION_TYPE = \"room_hierarchy_pagination\"",
            "",
            "    # The time a pagination session remains valid for.",
            "    _PAGINATION_SESSION_VALIDITY_PERIOD_MS = 5 * 60 * 1000",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._server_name = hs.hostname",
            "        self._federation_client = hs.get_federation_client()",
            "        self._ratelimiter = Ratelimiter(",
            "            store=self._store, clock=hs.get_clock(), rate_hz=5, burst_count=10",
            "        )",
            "",
            "        # If a user tries to fetch the same page multiple times in quick succession,",
            "        # only process the first attempt and return its result to subsequent requests.",
            "        self._pagination_response_cache: ResponseCache[",
            "            Tuple[str, str, bool, Optional[int], Optional[int], Optional[str]]",
            "        ] = ResponseCache(",
            "            hs.get_clock(),",
            "            \"get_room_hierarchy\",",
            "        )",
            "        self._msc3266_enabled = hs.config.experimental.msc3266_enabled",
            "",
            "    async def get_room_hierarchy(",
            "        self,",
            "        requester: Requester,",
            "        requested_room_id: str,",
            "        suggested_only: bool = False,",
            "        max_depth: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        from_token: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Implementation of the room hierarchy C-S API.",
            "",
            "        Args:",
            "            requester: The user ID of the user making this request.",
            "            requested_room_id: The room ID to start the hierarchy at (the \"root\" room).",
            "            suggested_only: Whether we should only return children with the \"suggested\"",
            "                flag set.",
            "            max_depth: The maximum depth in the tree to explore, must be a",
            "                non-negative integer.",
            "",
            "                0 would correspond to just the root room, 1 would include just",
            "                the root room's children, etc.",
            "            limit: An optional limit on the number of rooms to return per",
            "                page. Must be a positive integer.",
            "            from_token: An optional pagination token.",
            "",
            "        Returns:",
            "            The JSON hierarchy dictionary.",
            "        \"\"\"",
            "        await self._ratelimiter.ratelimit(requester)",
            "",
            "        # If a user tries to fetch the same page multiple times in quick succession,",
            "        # only process the first attempt and return its result to subsequent requests.",
            "        #",
            "        # This is due to the pagination process mutating internal state, attempting",
            "        # to process multiple requests for the same page will result in errors.",
            "        return await self._pagination_response_cache.wrap(",
            "            (",
            "                requester.user.to_string(),",
            "                requested_room_id,",
            "                suggested_only,",
            "                max_depth,",
            "                limit,",
            "                from_token,",
            "            ),",
            "            self._get_room_hierarchy,",
            "            requester.user.to_string(),",
            "            requested_room_id,",
            "            suggested_only,",
            "            max_depth,",
            "            limit,",
            "            from_token,",
            "        )",
            "",
            "    async def _get_room_hierarchy(",
            "        self,",
            "        requester: str,",
            "        requested_room_id: str,",
            "        suggested_only: bool = False,",
            "        max_depth: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        from_token: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"See docstring for SpaceSummaryHandler.get_room_hierarchy.\"\"\"",
            "",
            "        # First of all, check that the room is accessible.",
            "        if not await self._is_local_room_accessible(requested_room_id, requester):",
            "            raise UnstableSpecAuthError(",
            "                403,",
            "                \"User %s not in room %s, and room previews are disabled\"",
            "                % (requester, requested_room_id),",
            "                errcode=Codes.NOT_JOINED,",
            "            )",
            "",
            "        # If this is continuing a previous session, pull the persisted data.",
            "        if from_token:",
            "            try:",
            "                pagination_session = await self._store.get_session(",
            "                    session_type=self._PAGINATION_SESSION_TYPE,",
            "                    session_id=from_token,",
            "                )",
            "            except StoreError:",
            "                raise SynapseError(400, \"Unknown pagination token\", Codes.INVALID_PARAM)",
            "",
            "            # If the requester, room ID, suggested-only, or max depth were modified",
            "            # the session is invalid.",
            "            if (",
            "                requester != pagination_session[\"requester\"]",
            "                or requested_room_id != pagination_session[\"room_id\"]",
            "                or suggested_only != pagination_session[\"suggested_only\"]",
            "                or max_depth != pagination_session[\"max_depth\"]",
            "            ):",
            "                raise SynapseError(400, \"Unknown pagination token\", Codes.INVALID_PARAM)",
            "",
            "            # Load the previous state.",
            "            room_queue = [",
            "                _RoomQueueEntry(*fields) for fields in pagination_session[\"room_queue\"]",
            "            ]",
            "            processed_rooms = set(pagination_session[\"processed_rooms\"])",
            "        else:",
            "            # The queue of rooms to process, the next room is last on the stack.",
            "            room_queue = [_RoomQueueEntry(requested_room_id, ())]",
            "",
            "            # Rooms we have already processed.",
            "            processed_rooms = set()",
            "",
            "        rooms_result: List[JsonDict] = []",
            "",
            "        # Cap the limit to a server-side maximum.",
            "        if limit is None:",
            "            limit = MAX_ROOMS",
            "        else:",
            "            limit = min(limit, MAX_ROOMS)",
            "",
            "        # Iterate through the queue until we reach the limit or run out of",
            "        # rooms to include.",
            "        while room_queue and len(rooms_result) < limit:",
            "            queue_entry = room_queue.pop()",
            "            room_id = queue_entry.room_id",
            "            current_depth = queue_entry.depth",
            "            if room_id in processed_rooms:",
            "                # already done this room",
            "                continue",
            "",
            "            logger.debug(\"Processing room %s\", room_id)",
            "",
            "            # A map of summaries for children rooms that might be returned over",
            "            # federation. The rationale for caching these and *maybe* using them",
            "            # is to prefer any information local to the homeserver before trusting",
            "            # data received over federation.",
            "            children_room_entries: Dict[str, JsonDict] = {}",
            "            # A set of room IDs which are children that did not have information",
            "            # returned over federation and are known to be inaccessible to the",
            "            # current server. We should not reach out over federation to try to",
            "            # summarise these rooms.",
            "            inaccessible_children: Set[str] = set()",
            "",
            "            # If the room is known locally, summarise it!",
            "            is_in_room = await self._store.is_host_joined(room_id, self._server_name)",
            "            if is_in_room:",
            "                room_entry = await self._summarize_local_room(",
            "                    requester,",
            "                    None,",
            "                    room_id,",
            "                    suggested_only,",
            "                )",
            "",
            "            # Otherwise, attempt to use information for federation.",
            "            else:",
            "                # A previous call might have included information for this room.",
            "                # It can be used if either:",
            "                #",
            "                # 1. The room is not a space.",
            "                # 2. The maximum depth has been achieved (since no children",
            "                #    information is needed).",
            "                if queue_entry.remote_room and (",
            "                    queue_entry.remote_room.get(\"room_type\") != RoomTypes.SPACE",
            "                    or (max_depth is not None and current_depth >= max_depth)",
            "                ):",
            "                    room_entry = _RoomEntry(",
            "                        queue_entry.room_id, queue_entry.remote_room",
            "                    )",
            "",
            "                # If the above isn't true, attempt to fetch the room",
            "                # information over federation.",
            "                else:",
            "                    (",
            "                        room_entry,",
            "                        children_room_entries,",
            "                        inaccessible_children,",
            "                    ) = await self._summarize_remote_room_hierarchy(",
            "                        queue_entry,",
            "                        suggested_only,",
            "                    )",
            "",
            "                # Ensure this room is accessible to the requester (and not just",
            "                # the homeserver).",
            "                if room_entry and not await self._is_remote_room_accessible(",
            "                    requester, queue_entry.room_id, room_entry.room",
            "                ):",
            "                    room_entry = None",
            "",
            "            # This room has been processed and should be ignored if it appears",
            "            # elsewhere in the hierarchy.",
            "            processed_rooms.add(room_id)",
            "",
            "            # There may or may not be a room entry based on whether it is",
            "            # inaccessible to the requesting user.",
            "            if room_entry:",
            "                # Add the room (including the stripped m.space.child events).",
            "                rooms_result.append(room_entry.as_json(for_client=True))",
            "",
            "                # If this room is not at the max-depth, check if there are any",
            "                # children to process.",
            "                if max_depth is None or current_depth < max_depth:",
            "                    # The children get added in reverse order so that the next",
            "                    # room to process, according to the ordering, is the last",
            "                    # item in the list.",
            "                    room_queue.extend(",
            "                        _RoomQueueEntry(",
            "                            ev[\"state_key\"],",
            "                            ev[\"content\"][\"via\"],",
            "                            current_depth + 1,",
            "                            children_room_entries.get(ev[\"state_key\"]),",
            "                        )",
            "                        for ev in reversed(room_entry.children_state_events)",
            "                        if ev[\"type\"] == EventTypes.SpaceChild",
            "                        and ev[\"state_key\"] not in inaccessible_children",
            "                    )",
            "",
            "        result: JsonDict = {\"rooms\": rooms_result}",
            "",
            "        # If there's additional data, generate a pagination token (and persist state).",
            "        if room_queue:",
            "            result[\"next_batch\"] = await self._store.create_session(",
            "                session_type=self._PAGINATION_SESSION_TYPE,",
            "                value={",
            "                    # Information which must be identical across pagination.",
            "                    \"requester\": requester,",
            "                    \"room_id\": requested_room_id,",
            "                    \"suggested_only\": suggested_only,",
            "                    \"max_depth\": max_depth,",
            "                    # The stored state.",
            "                    \"room_queue\": [",
            "                        attr.astuple(room_entry) for room_entry in room_queue",
            "                    ],",
            "                    \"processed_rooms\": list(processed_rooms),",
            "                },",
            "                expiry_ms=self._PAGINATION_SESSION_VALIDITY_PERIOD_MS,",
            "            )",
            "",
            "        return result",
            "",
            "    async def get_federation_hierarchy(",
            "        self,",
            "        origin: str,",
            "        requested_room_id: str,",
            "        suggested_only: bool,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Implementation of the room hierarchy Federation API.",
            "",
            "        This is similar to get_room_hierarchy, but does not recurse into the space.",
            "        It also considers whether anyone on the server may be able to access the",
            "        room, as opposed to whether a specific user can.",
            "",
            "        Args:",
            "            origin: The server requesting the spaces summary.",
            "            requested_room_id: The room ID to start the hierarchy at (the \"root\" room).",
            "            suggested_only: whether we should only return children with the \"suggested\"",
            "                flag set.",
            "",
            "        Returns:",
            "            The JSON hierarchy dictionary.",
            "        \"\"\"",
            "        root_room_entry = await self._summarize_local_room(",
            "            None, origin, requested_room_id, suggested_only",
            "        )",
            "        if root_room_entry is None:",
            "            # Room is inaccessible to the requesting server.",
            "            raise SynapseError(404, \"Unknown room: %s\" % (requested_room_id,))",
            "",
            "        children_rooms_result: List[JsonDict] = []",
            "        inaccessible_children: List[str] = []",
            "",
            "        # Iterate through each child and potentially add it, but not its children,",
            "        # to the response.",
            "        for child_room in itertools.islice(",
            "            root_room_entry.children_state_events, MAX_ROOMS_PER_SPACE",
            "        ):",
            "            room_id = child_room.get(\"state_key\")",
            "            assert isinstance(room_id, str)",
            "            # If the room is unknown, skip it.",
            "            if not await self._store.is_host_joined(room_id, self._server_name):",
            "                continue",
            "",
            "            room_entry = await self._summarize_local_room(",
            "                None, origin, room_id, suggested_only, include_children=False",
            "            )",
            "            # If the room is accessible, include it in the results.",
            "            #",
            "            # Note that only the room summary (without information on children)",
            "            # is included in the summary.",
            "            if room_entry:",
            "                children_rooms_result.append(room_entry.room)",
            "            #  Otherwise, note that the requesting server shouldn't bother",
            "            #  trying to summarize this room - they do not have access to it.",
            "            else:",
            "                inaccessible_children.append(room_id)",
            "",
            "        return {",
            "            # Include the requested room (including the stripped children events).",
            "            \"room\": root_room_entry.as_json(),",
            "            \"children\": children_rooms_result,",
            "            \"inaccessible_children\": inaccessible_children,",
            "        }",
            "",
            "    async def _summarize_local_room(",
            "        self,",
            "        requester: Optional[str],",
            "        origin: Optional[str],",
            "        room_id: str,",
            "        suggested_only: bool,",
            "        include_children: bool = True,",
            "    ) -> Optional[\"_RoomEntry\"]:",
            "        \"\"\"",
            "        Generate a room entry and a list of event entries for a given room.",
            "",
            "        Args:",
            "            requester:",
            "                The user requesting the summary, if it is a local request. None",
            "                if this is a federation request.",
            "            origin:",
            "                The server requesting the summary, if it is a federation request.",
            "                None if this is a local request.",
            "            room_id: The room ID to summarize.",
            "            suggested_only: True if only suggested children should be returned.",
            "                Otherwise, all children are returned.",
            "            include_children:",
            "                Whether to include the events of any children.",
            "",
            "        Returns:",
            "            A room entry if the room should be returned. None, otherwise.",
            "        \"\"\"",
            "        if not await self._is_local_room_accessible(room_id, requester, origin):",
            "            return None",
            "",
            "        room_entry = await self._build_room_entry(room_id, for_federation=bool(origin))",
            "",
            "        # If the room is not a space return just the room information.",
            "        if room_entry.get(\"room_type\") != RoomTypes.SPACE or not include_children:",
            "            return _RoomEntry(room_id, room_entry)",
            "",
            "        # Otherwise, look for child rooms/spaces.",
            "        child_events = await self._get_child_events(room_id)",
            "",
            "        if suggested_only:",
            "            # we only care about suggested children",
            "            child_events = filter(_is_suggested_child_event, child_events)",
            "",
            "        stripped_events: List[JsonDict] = [",
            "            {",
            "                \"type\": e.type,",
            "                \"state_key\": e.state_key,",
            "                \"content\": e.content,",
            "                \"sender\": e.sender,",
            "                \"origin_server_ts\": e.origin_server_ts,",
            "            }",
            "            for e in child_events",
            "        ]",
            "        return _RoomEntry(room_id, room_entry, stripped_events)",
            "",
            "    async def _summarize_remote_room_hierarchy(",
            "        self, room: \"_RoomQueueEntry\", suggested_only: bool",
            "    ) -> Tuple[Optional[\"_RoomEntry\"], Dict[str, JsonDict], Set[str]]:",
            "        \"\"\"",
            "        Request room entries and a list of event entries for a given room by querying a remote server.",
            "",
            "        Args:",
            "            room: The room to summarize.",
            "            suggested_only: True if only suggested children should be returned.",
            "                Otherwise, all children are returned.",
            "",
            "        Returns:",
            "            A tuple of:",
            "                The room entry.",
            "                Partial room data return over federation.",
            "                A set of inaccessible children room IDs.",
            "        \"\"\"",
            "        room_id = room.room_id",
            "        logger.info(\"Requesting summary for %s via %s\", room_id, room.via)",
            "",
            "        via = itertools.islice(room.via, MAX_SERVERS_PER_SPACE)",
            "        try:",
            "            (",
            "                room_response,",
            "                children_state_events,",
            "                children,",
            "                inaccessible_children,",
            "            ) = await self._federation_client.get_room_hierarchy(",
            "                via,",
            "                room_id,",
            "                suggested_only=suggested_only,",
            "            )",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Unable to get hierarchy of %s via federation: %s\",",
            "                room_id,",
            "                e,",
            "                exc_info=logger.isEnabledFor(logging.DEBUG),",
            "            )",
            "            return None, {}, set()",
            "",
            "        # Map the children to their room ID.",
            "        children_by_room_id = {",
            "            c[\"room_id\"]: c",
            "            for c in children",
            "            if \"room_id\" in c and isinstance(c[\"room_id\"], str)",
            "        }",
            "",
            "        return (",
            "            _RoomEntry(room_id, room_response, children_state_events),",
            "            children_by_room_id,",
            "            set(inaccessible_children),",
            "        )",
            "",
            "    async def _is_local_room_accessible(",
            "        self, room_id: str, requester: Optional[str], origin: Optional[str] = None",
            "    ) -> bool:",
            "        \"\"\"",
            "        Calculate whether the room should be shown to the requester.",
            "",
            "        It should return true if:",
            "",
            "        * The requester is joined or can join the room (per MSC3173).",
            "        * The origin server has any user that is joined or can join the room.",
            "        * The history visibility is set to world readable.",
            "",
            "        Args:",
            "            room_id: The room ID to check accessibility of.",
            "            requester:",
            "                The user making the request, if it is a local request.",
            "                None if this is a federation request.",
            "            origin:",
            "                The server making the request, if it is a federation request.",
            "                None if this is a local request.",
            "",
            "        Returns:",
            "             True if the room is accessible to the requesting user or server.",
            "        \"\"\"",
            "        state_ids = await self._storage_controllers.state.get_current_state_ids(room_id)",
            "",
            "        # If there's no state for the room, it isn't known.",
            "        if not state_ids:",
            "            # The user might have a pending invite for the room.",
            "            if requester and await self._store.get_invite_for_local_user_in_room(",
            "                requester, room_id",
            "            ):",
            "                return True",
            "",
            "            logger.info(\"room %s is unknown, omitting from summary\", room_id)",
            "            return False",
            "",
            "        try:",
            "            room_version = await self._store.get_room_version(room_id)",
            "        except UnsupportedRoomVersionError:",
            "            # If a room with an unsupported room version is encountered, ignore",
            "            # it to avoid breaking the entire summary response.",
            "            return False",
            "",
            "        # Include the room if it has join rules of public or knock.",
            "        join_rules_event_id = state_ids.get((EventTypes.JoinRules, \"\"))",
            "        if join_rules_event_id:",
            "            join_rules_event = await self._store.get_event(join_rules_event_id)",
            "            join_rule = join_rules_event.content.get(\"join_rule\")",
            "            if (",
            "                join_rule == JoinRules.PUBLIC",
            "                or (room_version.msc2403_knocking and join_rule == JoinRules.KNOCK)",
            "                or (",
            "                    room_version.msc3787_knock_restricted_join_rule",
            "                    and join_rule == JoinRules.KNOCK_RESTRICTED",
            "                )",
            "            ):",
            "                return True",
            "",
            "        # Include the room if it is peekable.",
            "        hist_vis_event_id = state_ids.get((EventTypes.RoomHistoryVisibility, \"\"))",
            "        if hist_vis_event_id:",
            "            hist_vis_ev = await self._store.get_event(hist_vis_event_id)",
            "            hist_vis = hist_vis_ev.content.get(\"history_visibility\")",
            "            if hist_vis == HistoryVisibility.WORLD_READABLE:",
            "                return True",
            "",
            "        # Otherwise we need to check information specific to the user or server.",
            "",
            "        # If we have an authenticated requesting user, check if they are a member",
            "        # of the room (or can join the room).",
            "        if requester:",
            "            member_event_id = state_ids.get((EventTypes.Member, requester), None)",
            "",
            "            # If they're in the room they can see info on it.",
            "            if member_event_id:",
            "                member_event = await self._store.get_event(member_event_id)",
            "                if member_event.membership in (Membership.JOIN, Membership.INVITE):",
            "                    return True",
            "",
            "            # Otherwise, check if they should be allowed access via membership in a space.",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                allowed_rooms = (",
            "                    await self._event_auth_handler.get_rooms_that_allow_join(state_ids)",
            "                )",
            "                if await self._event_auth_handler.is_user_in_rooms(",
            "                    allowed_rooms, requester",
            "                ):",
            "                    return True",
            "",
            "        # If this is a request over federation, check if the host is in the room or",
            "        # has a user who could join the room.",
            "        elif origin:",
            "            if await self._event_auth_handler.check_host_in_room(",
            "                room_id, origin",
            "            ) or await self._store.is_host_invited(room_id, origin):",
            "                return True",
            "",
            "            # Alternately, if the host has a user in any of the spaces specified",
            "            # for access, then the host can see this room (and should do filtering",
            "            # if the requester cannot see it).",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                allowed_rooms = (",
            "                    await self._event_auth_handler.get_rooms_that_allow_join(state_ids)",
            "                )",
            "                for space_id in allowed_rooms:",
            "                    if await self._event_auth_handler.check_host_in_room(",
            "                        space_id, origin",
            "                    ):",
            "                        return True",
            "",
            "        logger.info(",
            "            \"room %s is unpeekable and requester %s is not a member / not allowed to join, omitting from summary\",",
            "            room_id,",
            "            requester or origin,",
            "        )",
            "        return False",
            "",
            "    async def _is_remote_room_accessible(",
            "        self, requester: Optional[str], room_id: str, room: JsonDict",
            "    ) -> bool:",
            "        \"\"\"",
            "        Calculate whether the room received over federation should be shown to the requester.",
            "",
            "        It should return true if:",
            "",
            "        * The requester is joined or can join the room (per MSC3173).",
            "        * The history visibility is set to world readable.",
            "",
            "        Note that the local server is not in the requested room (which is why the",
            "        remote call was made in the first place), but the user could have access",
            "        due to an invite, etc.",
            "",
            "        Args:",
            "            requester: The user requesting the summary. If not passed only world",
            "                readability is checked.",
            "            room_id: The room ID returned over federation.",
            "            room: The summary of the room returned over federation.",
            "",
            "        Returns:",
            "            True if the room is accessible to the requesting user.",
            "        \"\"\"",
            "        # The API doesn't return the room version so assume that a",
            "        # join rule of knock is valid.",
            "        if (",
            "            room.get(\"join_rule\")",
            "            in (JoinRules.PUBLIC, JoinRules.KNOCK, JoinRules.KNOCK_RESTRICTED)",
            "            or room.get(\"world_readable\") is True",
            "        ):",
            "            return True",
            "        elif not requester:",
            "            return False",
            "",
            "        # Check if the user is a member of any of the allowed rooms from the response.",
            "        allowed_rooms = room.get(\"allowed_room_ids\")",
            "        if allowed_rooms and isinstance(allowed_rooms, list):",
            "            if await self._event_auth_handler.is_user_in_rooms(",
            "                allowed_rooms, requester",
            "            ):",
            "                return True",
            "",
            "        # Finally, check locally if we can access the room. The user might",
            "        # already be in the room (if it was a child room), or there might be a",
            "        # pending invite, etc.",
            "        return await self._is_local_room_accessible(room_id, requester)",
            "",
            "    async def _build_room_entry(self, room_id: str, for_federation: bool) -> JsonDict:",
            "        \"\"\"",
            "        Generate en entry summarising a single room.",
            "",
            "        Args:",
            "            room_id: The room ID to summarize.",
            "            for_federation: True if this is a summary requested over federation",
            "                (which includes additional fields).",
            "",
            "        Returns:",
            "            The JSON dictionary for the room.",
            "        \"\"\"",
            "        stats = await self._store.get_room_with_stats(room_id)",
            "",
            "        # currently this should be impossible because we call",
            "        # _is_local_room_accessible on the room before we get here, so",
            "        # there should always be an entry",
            "        assert stats is not None, \"unable to retrieve stats for %s\" % (room_id,)",
            "",
            "        current_state_ids = await self._storage_controllers.state.get_current_state_ids(",
            "            room_id",
            "        )",
            "        create_event = await self._store.get_event(",
            "            current_state_ids[(EventTypes.Create, \"\")]",
            "        )",
            "",
            "        entry = {",
            "            \"room_id\": stats[\"room_id\"],",
            "            \"name\": stats[\"name\"],",
            "            \"topic\": stats[\"topic\"],",
            "            \"canonical_alias\": stats[\"canonical_alias\"],",
            "            \"num_joined_members\": stats[\"joined_members\"],",
            "            \"avatar_url\": stats[\"avatar\"],",
            "            \"join_rule\": stats[\"join_rules\"],",
            "            \"world_readable\": (",
            "                stats[\"history_visibility\"] == HistoryVisibility.WORLD_READABLE",
            "            ),",
            "            \"guest_can_join\": stats[\"guest_access\"] == \"can_join\",",
            "            \"room_type\": create_event.content.get(EventContentFields.ROOM_TYPE),",
            "        }",
            "",
            "        if self._msc3266_enabled:",
            "            entry[\"im.nheko.summary.version\"] = stats[\"version\"]",
            "            entry[\"im.nheko.summary.encryption\"] = stats[\"encryption\"]",
            "",
            "        # Federation requests need to provide additional information so the",
            "        # requested server is able to filter the response appropriately.",
            "        if for_federation:",
            "            room_version = await self._store.get_room_version(room_id)",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                current_state_ids, room_version",
            "            ):",
            "                allowed_rooms = (",
            "                    await self._event_auth_handler.get_rooms_that_allow_join(",
            "                        current_state_ids",
            "                    )",
            "                )",
            "                if allowed_rooms:",
            "                    entry[\"allowed_room_ids\"] = allowed_rooms",
            "",
            "        # Filter out Nones \u2013 rather omit the field altogether",
            "        room_entry = {k: v for k, v in entry.items() if v is not None}",
            "",
            "        return room_entry",
            "",
            "    async def _get_child_events(self, room_id: str) -> Iterable[EventBase]:",
            "        \"\"\"",
            "        Get the child events for a given room.",
            "",
            "        The returned results are sorted for stability.",
            "",
            "        Args:",
            "            room_id: The room id to get the children of.",
            "",
            "        Returns:",
            "            An iterable of sorted child events.",
            "        \"\"\"",
            "",
            "        # look for child rooms/spaces.",
            "        current_state_ids = await self._storage_controllers.state.get_current_state_ids(",
            "            room_id",
            "        )",
            "",
            "        events = await self._store.get_events_as_list(",
            "            [",
            "                event_id",
            "                for key, event_id in current_state_ids.items()",
            "                if key[0] == EventTypes.SpaceChild",
            "            ]",
            "        )",
            "",
            "        # filter out any events without a \"via\" (which implies it has been redacted),",
            "        # and order to ensure we return stable results.",
            "        return sorted(filter(_has_valid_via, events), key=_child_events_comparison_key)",
            "",
            "    async def get_room_summary(",
            "        self,",
            "        requester: Optional[str],",
            "        room_id: str,",
            "        remote_room_hosts: Optional[List[str]] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Implementation of the room summary C-S API from MSC3266",
            "",
            "        Args:",
            "            requester:  user id of the user making this request, will be None",
            "                for unauthenticated requests",
            "",
            "            room_id: room id to summarise.",
            "",
            "            remote_room_hosts: a list of homeservers to try fetching data through",
            "                if we don't know it ourselves",
            "",
            "        Returns:",
            "            summary dict to return",
            "        \"\"\"",
            "        is_in_room = await self._store.is_host_joined(room_id, self._server_name)",
            "",
            "        if is_in_room:",
            "            room_entry = await self._summarize_local_room(",
            "                requester,",
            "                None,",
            "                room_id,",
            "                # Suggested-only doesn't matter since no children are requested.",
            "                suggested_only=False,",
            "                include_children=False,",
            "            )",
            "",
            "            if not room_entry:",
            "                raise NotFoundError(\"Room not found or is not accessible\")",
            "",
            "            room_summary = room_entry.room",
            "",
            "            # If there was a requester, add their membership.",
            "            if requester:",
            "                (",
            "                    membership,",
            "                    _,",
            "                ) = await self._store.get_local_current_membership_for_user_in_room(",
            "                    requester, room_id",
            "                )",
            "",
            "                room_summary[\"membership\"] = membership or \"leave\"",
            "        else:",
            "            # Reuse the hierarchy query over federation",
            "            if remote_room_hosts is None:",
            "                raise SynapseError(400, \"Missing via to query remote room\")",
            "",
            "            (",
            "                room_entry,",
            "                children_room_entries,",
            "                inaccessible_children,",
            "            ) = await self._summarize_remote_room_hierarchy(",
            "                _RoomQueueEntry(room_id, remote_room_hosts),",
            "                suggested_only=True,",
            "            )",
            "",
            "            # The results over federation might include rooms that we, as the",
            "            # requesting server, are allowed to see, but the requesting user is",
            "            # not permitted to see.",
            "            #",
            "            # Filter the returned results to only what is accessible to the user.",
            "            if not room_entry or not await self._is_remote_room_accessible(",
            "                requester, room_entry.room_id, room_entry.room",
            "            ):",
            "                raise NotFoundError(\"Room not found or is not accessible\")",
            "",
            "            room = dict(room_entry.room)",
            "            room.pop(\"allowed_room_ids\", None)",
            "",
            "            # If there was a requester, add their membership.",
            "            # We keep the membership in the local membership table unless the",
            "            # room is purged even for remote rooms.",
            "            if requester:",
            "                (",
            "                    membership,",
            "                    _,",
            "                ) = await self._store.get_local_current_membership_for_user_in_room(",
            "                    requester, room_id",
            "                )",
            "                room[\"membership\"] = membership or \"leave\"",
            "",
            "            return room",
            "",
            "        return room_summary",
            "",
            "",
            "@attr.s(frozen=True, slots=True, auto_attribs=True)",
            "class _RoomQueueEntry:",
            "    # The room ID of this entry.",
            "    room_id: str",
            "    # The server to query if the room is not known locally.",
            "    via: Sequence[str]",
            "    # The minimum number of hops necessary to get to this room (compared to the",
            "    # originally requested room).",
            "    depth: int = 0",
            "    # The room summary for this room returned via federation. This will only be",
            "    # used if the room is not known locally (and is not a space).",
            "    remote_room: Optional[JsonDict] = None",
            "",
            "",
            "@attr.s(frozen=True, slots=True, auto_attribs=True)",
            "class _RoomEntry:",
            "    room_id: str",
            "    # The room summary for this room.",
            "    room: JsonDict",
            "    # An iterable of the sorted, stripped children events for children of this room.",
            "    #",
            "    # This may not include all children.",
            "    children_state_events: Sequence[JsonDict] = ()",
            "",
            "    def as_json(self, for_client: bool = False) -> JsonDict:",
            "        \"\"\"",
            "        Returns a JSON dictionary suitable for the room hierarchy endpoint.",
            "",
            "        It returns the room summary including the stripped m.space.child events",
            "        as a sub-key.",
            "",
            "        Args:",
            "            for_client: If true, any server-server only fields are stripped from",
            "                the result.",
            "",
            "        \"\"\"",
            "        result = dict(self.room)",
            "",
            "        # Before returning to the client, remove the allowed_room_ids key, if it",
            "        # exists.",
            "        if for_client:",
            "            result.pop(\"allowed_room_ids\", False)",
            "",
            "        result[\"children_state\"] = self.children_state_events",
            "        return result",
            "",
            "",
            "def _has_valid_via(e: EventBase) -> bool:",
            "    via = e.content.get(\"via\")",
            "    if not via or not isinstance(via, list):",
            "        return False",
            "    for v in via:",
            "        if not isinstance(v, str):",
            "            logger.debug(\"Ignoring edge event %s with invalid via entry\", e.event_id)",
            "            return False",
            "    return True",
            "",
            "",
            "def _is_suggested_child_event(edge_event: EventBase) -> bool:",
            "    suggested = edge_event.content.get(\"suggested\")",
            "    if isinstance(suggested, bool) and suggested:",
            "        return True",
            "    logger.debug(\"Ignorning not-suggested child %s\", edge_event.state_key)",
            "    return False",
            "",
            "",
            "# Order may only contain characters in the range of \\x20 (space) to \\x7E (~) inclusive.",
            "_INVALID_ORDER_CHARS_RE = re.compile(r\"[^\\x20-\\x7E]\")",
            "",
            "",
            "def _child_events_comparison_key(",
            "    child: EventBase,",
            ") -> Tuple[bool, Optional[str], int, str]:",
            "    \"\"\"",
            "    Generate a value for comparing two child events for ordering.",
            "",
            "    The rules for ordering are:",
            "",
            "    1. The 'order' key, if it is valid.",
            "    2. The 'origin_server_ts' of the 'm.space.child' event.",
            "    3. The 'room_id'.",
            "",
            "    Args:",
            "        child: The event for generating a comparison key.",
            "",
            "    Returns:",
            "        The comparison key as a tuple of:",
            "            False if the ordering is valid.",
            "            The 'order' field or None if it is not given or invalid.",
            "            The 'origin_server_ts' field.",
            "            The room ID.",
            "    \"\"\"",
            "    order = child.content.get(\"order\")",
            "    # If order is not a string or doesn't meet the requirements, ignore it.",
            "    if not isinstance(order, str):",
            "        order = None",
            "    elif len(order) > 50 or _INVALID_ORDER_CHARS_RE.search(order):",
            "        order = None",
            "",
            "    # Items without an order come last.",
            "    return order is None, order, child.origin_server_ts, child.room_id"
        ],
        "afterPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import itertools",
            "import logging",
            "import re",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Set, Tuple",
            "",
            "import attr",
            "",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    HistoryVisibility,",
            "    JoinRules,",
            "    Membership,",
            "    RoomTypes,",
            ")",
            "from synapse.api.errors import (",
            "    Codes,",
            "    NotFoundError,",
            "    StoreError,",
            "    SynapseError,",
            "    UnstableSpecAuthError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.ratelimiting import Ratelimiter",
            "from synapse.events import EventBase",
            "from synapse.types import JsonDict, Requester",
            "from synapse.util.caches.response_cache import ResponseCache",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# number of rooms to return. We'll stop once we hit this limit.",
            "MAX_ROOMS = 50",
            "",
            "# max number of events to return per room.",
            "MAX_ROOMS_PER_SPACE = 50",
            "",
            "# max number of federation servers to hit per room",
            "MAX_SERVERS_PER_SPACE = 3",
            "",
            "",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class _PaginationKey:",
            "    \"\"\"The key used to find unique pagination session.\"\"\"",
            "",
            "    # The first three entries match the request parameters (and cannot change",
            "    # during a pagination session).",
            "    room_id: str",
            "    suggested_only: bool",
            "    max_depth: Optional[int]",
            "    # The randomly generated token.",
            "    token: str",
            "",
            "",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class _PaginationSession:",
            "    \"\"\"The information that is stored for pagination.\"\"\"",
            "",
            "    # The time the pagination session was created, in milliseconds.",
            "    creation_time_ms: int",
            "    # The queue of rooms which are still to process.",
            "    room_queue: List[\"_RoomQueueEntry\"]",
            "    # A set of rooms which have been processed.",
            "    processed_rooms: Set[str]",
            "",
            "",
            "class RoomSummaryHandler:",
            "    # A unique key used for pagination sessions for the room hierarchy endpoint.",
            "    _PAGINATION_SESSION_TYPE = \"room_hierarchy_pagination\"",
            "",
            "    # The time a pagination session remains valid for.",
            "    _PAGINATION_SESSION_VALIDITY_PERIOD_MS = 5 * 60 * 1000",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._server_name = hs.hostname",
            "        self._federation_client = hs.get_federation_client()",
            "        self._ratelimiter = Ratelimiter(",
            "            store=self._store, clock=hs.get_clock(), rate_hz=5, burst_count=10",
            "        )",
            "",
            "        # If a user tries to fetch the same page multiple times in quick succession,",
            "        # only process the first attempt and return its result to subsequent requests.",
            "        self._pagination_response_cache: ResponseCache[",
            "            Tuple[str, str, bool, Optional[int], Optional[int], Optional[str]]",
            "        ] = ResponseCache(",
            "            hs.get_clock(),",
            "            \"get_room_hierarchy\",",
            "        )",
            "        self._msc3266_enabled = hs.config.experimental.msc3266_enabled",
            "",
            "    async def get_room_hierarchy(",
            "        self,",
            "        requester: Requester,",
            "        requested_room_id: str,",
            "        suggested_only: bool = False,",
            "        max_depth: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        from_token: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Implementation of the room hierarchy C-S API.",
            "",
            "        Args:",
            "            requester: The user ID of the user making this request.",
            "            requested_room_id: The room ID to start the hierarchy at (the \"root\" room).",
            "            suggested_only: Whether we should only return children with the \"suggested\"",
            "                flag set.",
            "            max_depth: The maximum depth in the tree to explore, must be a",
            "                non-negative integer.",
            "",
            "                0 would correspond to just the root room, 1 would include just",
            "                the root room's children, etc.",
            "            limit: An optional limit on the number of rooms to return per",
            "                page. Must be a positive integer.",
            "            from_token: An optional pagination token.",
            "",
            "        Returns:",
            "            The JSON hierarchy dictionary.",
            "        \"\"\"",
            "        await self._ratelimiter.ratelimit(requester)",
            "",
            "        # If a user tries to fetch the same page multiple times in quick succession,",
            "        # only process the first attempt and return its result to subsequent requests.",
            "        #",
            "        # This is due to the pagination process mutating internal state, attempting",
            "        # to process multiple requests for the same page will result in errors.",
            "        return await self._pagination_response_cache.wrap(",
            "            (",
            "                requester.user.to_string(),",
            "                requested_room_id,",
            "                suggested_only,",
            "                max_depth,",
            "                limit,",
            "                from_token,",
            "            ),",
            "            self._get_room_hierarchy,",
            "            requester.user.to_string(),",
            "            requested_room_id,",
            "            suggested_only,",
            "            max_depth,",
            "            limit,",
            "            from_token,",
            "        )",
            "",
            "    async def _get_room_hierarchy(",
            "        self,",
            "        requester: str,",
            "        requested_room_id: str,",
            "        suggested_only: bool = False,",
            "        max_depth: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        from_token: Optional[str] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"See docstring for SpaceSummaryHandler.get_room_hierarchy.\"\"\"",
            "",
            "        # First of all, check that the room is accessible.",
            "        if not await self._is_local_room_accessible(requested_room_id, requester):",
            "            raise UnstableSpecAuthError(",
            "                403,",
            "                \"User %s not in room %s, and room previews are disabled\"",
            "                % (requester, requested_room_id),",
            "                errcode=Codes.NOT_JOINED,",
            "            )",
            "",
            "        # If this is continuing a previous session, pull the persisted data.",
            "        if from_token:",
            "            try:",
            "                pagination_session = await self._store.get_session(",
            "                    session_type=self._PAGINATION_SESSION_TYPE,",
            "                    session_id=from_token,",
            "                )",
            "            except StoreError:",
            "                raise SynapseError(400, \"Unknown pagination token\", Codes.INVALID_PARAM)",
            "",
            "            # If the requester, room ID, suggested-only, or max depth were modified",
            "            # the session is invalid.",
            "            if (",
            "                requester != pagination_session[\"requester\"]",
            "                or requested_room_id != pagination_session[\"room_id\"]",
            "                or suggested_only != pagination_session[\"suggested_only\"]",
            "                or max_depth != pagination_session[\"max_depth\"]",
            "            ):",
            "                raise SynapseError(400, \"Unknown pagination token\", Codes.INVALID_PARAM)",
            "",
            "            # Load the previous state.",
            "            room_queue = [",
            "                _RoomQueueEntry(*fields) for fields in pagination_session[\"room_queue\"]",
            "            ]",
            "            processed_rooms = set(pagination_session[\"processed_rooms\"])",
            "        else:",
            "            # The queue of rooms to process, the next room is last on the stack.",
            "            room_queue = [_RoomQueueEntry(requested_room_id, ())]",
            "",
            "            # Rooms we have already processed.",
            "            processed_rooms = set()",
            "",
            "        rooms_result: List[JsonDict] = []",
            "",
            "        # Cap the limit to a server-side maximum.",
            "        if limit is None:",
            "            limit = MAX_ROOMS",
            "        else:",
            "            limit = min(limit, MAX_ROOMS)",
            "",
            "        # Iterate through the queue until we reach the limit or run out of",
            "        # rooms to include.",
            "        while room_queue and len(rooms_result) < limit:",
            "            queue_entry = room_queue.pop()",
            "            room_id = queue_entry.room_id",
            "            current_depth = queue_entry.depth",
            "            if room_id in processed_rooms:",
            "                # already done this room",
            "                continue",
            "",
            "            logger.debug(\"Processing room %s\", room_id)",
            "",
            "            # A map of summaries for children rooms that might be returned over",
            "            # federation. The rationale for caching these and *maybe* using them",
            "            # is to prefer any information local to the homeserver before trusting",
            "            # data received over federation.",
            "            children_room_entries: Dict[str, JsonDict] = {}",
            "            # A set of room IDs which are children that did not have information",
            "            # returned over federation and are known to be inaccessible to the",
            "            # current server. We should not reach out over federation to try to",
            "            # summarise these rooms.",
            "            inaccessible_children: Set[str] = set()",
            "",
            "            # If the room is known locally, summarise it!",
            "            is_in_room = await self._store.is_host_joined(room_id, self._server_name)",
            "            if is_in_room:",
            "                room_entry = await self._summarize_local_room(",
            "                    requester,",
            "                    None,",
            "                    room_id,",
            "                    suggested_only,",
            "                )",
            "",
            "            # Otherwise, attempt to use information for federation.",
            "            else:",
            "                # A previous call might have included information for this room.",
            "                # It can be used if either:",
            "                #",
            "                # 1. The room is not a space.",
            "                # 2. The maximum depth has been achieved (since no children",
            "                #    information is needed).",
            "                if queue_entry.remote_room and (",
            "                    queue_entry.remote_room.get(\"room_type\") != RoomTypes.SPACE",
            "                    or (max_depth is not None and current_depth >= max_depth)",
            "                ):",
            "                    room_entry = _RoomEntry(",
            "                        queue_entry.room_id, queue_entry.remote_room",
            "                    )",
            "",
            "                # If the above isn't true, attempt to fetch the room",
            "                # information over federation.",
            "                else:",
            "                    (",
            "                        room_entry,",
            "                        children_room_entries,",
            "                        inaccessible_children,",
            "                    ) = await self._summarize_remote_room_hierarchy(",
            "                        queue_entry,",
            "                        suggested_only,",
            "                    )",
            "",
            "                # Ensure this room is accessible to the requester (and not just",
            "                # the homeserver).",
            "                if room_entry and not await self._is_remote_room_accessible(",
            "                    requester, queue_entry.room_id, room_entry.room",
            "                ):",
            "                    room_entry = None",
            "",
            "            # This room has been processed and should be ignored if it appears",
            "            # elsewhere in the hierarchy.",
            "            processed_rooms.add(room_id)",
            "",
            "            # There may or may not be a room entry based on whether it is",
            "            # inaccessible to the requesting user.",
            "            if room_entry:",
            "                # Add the room (including the stripped m.space.child events).",
            "                rooms_result.append(room_entry.as_json(for_client=True))",
            "",
            "                # If this room is not at the max-depth, check if there are any",
            "                # children to process.",
            "                if max_depth is None or current_depth < max_depth:",
            "                    # The children get added in reverse order so that the next",
            "                    # room to process, according to the ordering, is the last",
            "                    # item in the list.",
            "                    room_queue.extend(",
            "                        _RoomQueueEntry(",
            "                            ev[\"state_key\"],",
            "                            ev[\"content\"][\"via\"],",
            "                            current_depth + 1,",
            "                            children_room_entries.get(ev[\"state_key\"]),",
            "                        )",
            "                        for ev in reversed(room_entry.children_state_events)",
            "                        if ev[\"type\"] == EventTypes.SpaceChild",
            "                        and ev[\"state_key\"] not in inaccessible_children",
            "                    )",
            "",
            "        result: JsonDict = {\"rooms\": rooms_result}",
            "",
            "        # If there's additional data, generate a pagination token (and persist state).",
            "        if room_queue:",
            "            result[\"next_batch\"] = await self._store.create_session(",
            "                session_type=self._PAGINATION_SESSION_TYPE,",
            "                value={",
            "                    # Information which must be identical across pagination.",
            "                    \"requester\": requester,",
            "                    \"room_id\": requested_room_id,",
            "                    \"suggested_only\": suggested_only,",
            "                    \"max_depth\": max_depth,",
            "                    # The stored state.",
            "                    \"room_queue\": [",
            "                        attr.astuple(room_entry) for room_entry in room_queue",
            "                    ],",
            "                    \"processed_rooms\": list(processed_rooms),",
            "                },",
            "                expiry_ms=self._PAGINATION_SESSION_VALIDITY_PERIOD_MS,",
            "            )",
            "",
            "        return result",
            "",
            "    async def get_federation_hierarchy(",
            "        self,",
            "        origin: str,",
            "        requested_room_id: str,",
            "        suggested_only: bool,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Implementation of the room hierarchy Federation API.",
            "",
            "        This is similar to get_room_hierarchy, but does not recurse into the space.",
            "        It also considers whether anyone on the server may be able to access the",
            "        room, as opposed to whether a specific user can.",
            "",
            "        Args:",
            "            origin: The server requesting the spaces summary.",
            "            requested_room_id: The room ID to start the hierarchy at (the \"root\" room).",
            "            suggested_only: whether we should only return children with the \"suggested\"",
            "                flag set.",
            "",
            "        Returns:",
            "            The JSON hierarchy dictionary.",
            "        \"\"\"",
            "        root_room_entry = await self._summarize_local_room(",
            "            None, origin, requested_room_id, suggested_only",
            "        )",
            "        if root_room_entry is None:",
            "            # Room is inaccessible to the requesting server.",
            "            raise SynapseError(404, \"Unknown room: %s\" % (requested_room_id,))",
            "",
            "        children_rooms_result: List[JsonDict] = []",
            "        inaccessible_children: List[str] = []",
            "",
            "        # Iterate through each child and potentially add it, but not its children,",
            "        # to the response.",
            "        for child_room in itertools.islice(",
            "            root_room_entry.children_state_events, MAX_ROOMS_PER_SPACE",
            "        ):",
            "            room_id = child_room.get(\"state_key\")",
            "            assert isinstance(room_id, str)",
            "            # If the room is unknown, skip it.",
            "            if not await self._store.is_host_joined(room_id, self._server_name):",
            "                continue",
            "",
            "            room_entry = await self._summarize_local_room(",
            "                None, origin, room_id, suggested_only, include_children=False",
            "            )",
            "            # If the room is accessible, include it in the results.",
            "            #",
            "            # Note that only the room summary (without information on children)",
            "            # is included in the summary.",
            "            if room_entry:",
            "                children_rooms_result.append(room_entry.room)",
            "            #  Otherwise, note that the requesting server shouldn't bother",
            "            #  trying to summarize this room - they do not have access to it.",
            "            else:",
            "                inaccessible_children.append(room_id)",
            "",
            "        return {",
            "            # Include the requested room (including the stripped children events).",
            "            \"room\": root_room_entry.as_json(),",
            "            \"children\": children_rooms_result,",
            "            \"inaccessible_children\": inaccessible_children,",
            "        }",
            "",
            "    async def _summarize_local_room(",
            "        self,",
            "        requester: Optional[str],",
            "        origin: Optional[str],",
            "        room_id: str,",
            "        suggested_only: bool,",
            "        include_children: bool = True,",
            "    ) -> Optional[\"_RoomEntry\"]:",
            "        \"\"\"",
            "        Generate a room entry and a list of event entries for a given room.",
            "",
            "        Args:",
            "            requester:",
            "                The user requesting the summary, if it is a local request. None",
            "                if this is a federation request.",
            "            origin:",
            "                The server requesting the summary, if it is a federation request.",
            "                None if this is a local request.",
            "            room_id: The room ID to summarize.",
            "            suggested_only: True if only suggested children should be returned.",
            "                Otherwise, all children are returned.",
            "            include_children:",
            "                Whether to include the events of any children.",
            "",
            "        Returns:",
            "            A room entry if the room should be returned. None, otherwise.",
            "        \"\"\"",
            "        if not await self._is_local_room_accessible(room_id, requester, origin):",
            "            return None",
            "",
            "        room_entry = await self._build_room_entry(room_id, for_federation=bool(origin))",
            "",
            "        # If the room is not a space return just the room information.",
            "        if room_entry.get(\"room_type\") != RoomTypes.SPACE or not include_children:",
            "            return _RoomEntry(room_id, room_entry)",
            "",
            "        # Otherwise, look for child rooms/spaces.",
            "        child_events = await self._get_child_events(room_id)",
            "",
            "        if suggested_only:",
            "            # we only care about suggested children",
            "            child_events = filter(_is_suggested_child_event, child_events)",
            "",
            "        stripped_events: List[JsonDict] = [",
            "            {",
            "                \"type\": e.type,",
            "                \"state_key\": e.state_key,",
            "                \"content\": e.content,",
            "                \"sender\": e.sender,",
            "                \"origin_server_ts\": e.origin_server_ts,",
            "            }",
            "            for e in child_events",
            "        ]",
            "        return _RoomEntry(room_id, room_entry, stripped_events)",
            "",
            "    async def _summarize_remote_room_hierarchy(",
            "        self, room: \"_RoomQueueEntry\", suggested_only: bool",
            "    ) -> Tuple[Optional[\"_RoomEntry\"], Dict[str, JsonDict], Set[str]]:",
            "        \"\"\"",
            "        Request room entries and a list of event entries for a given room by querying a remote server.",
            "",
            "        Args:",
            "            room: The room to summarize.",
            "            suggested_only: True if only suggested children should be returned.",
            "                Otherwise, all children are returned.",
            "",
            "        Returns:",
            "            A tuple of:",
            "                The room entry.",
            "                Partial room data return over federation.",
            "                A set of inaccessible children room IDs.",
            "        \"\"\"",
            "        room_id = room.room_id",
            "        logger.info(\"Requesting summary for %s via %s\", room_id, room.via)",
            "",
            "        via = itertools.islice(room.via, MAX_SERVERS_PER_SPACE)",
            "        try:",
            "            (",
            "                room_response,",
            "                children_state_events,",
            "                children,",
            "                inaccessible_children,",
            "            ) = await self._federation_client.get_room_hierarchy(",
            "                via,",
            "                room_id,",
            "                suggested_only=suggested_only,",
            "            )",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Unable to get hierarchy of %s via federation: %s\",",
            "                room_id,",
            "                e,",
            "                exc_info=logger.isEnabledFor(logging.DEBUG),",
            "            )",
            "            return None, {}, set()",
            "",
            "        # Map the children to their room ID.",
            "        children_by_room_id = {",
            "            c[\"room_id\"]: c",
            "            for c in children",
            "            if \"room_id\" in c and isinstance(c[\"room_id\"], str)",
            "        }",
            "",
            "        return (",
            "            _RoomEntry(room_id, room_response, children_state_events),",
            "            children_by_room_id,",
            "            set(inaccessible_children),",
            "        )",
            "",
            "    async def _is_local_room_accessible(",
            "        self, room_id: str, requester: Optional[str], origin: Optional[str] = None",
            "    ) -> bool:",
            "        \"\"\"",
            "        Calculate whether the room should be shown to the requester.",
            "",
            "        It should return true if:",
            "",
            "        * The requester is joined or can join the room (per MSC3173).",
            "        * The origin server has any user that is joined or can join the room.",
            "        * The history visibility is set to world readable.",
            "",
            "        Args:",
            "            room_id: The room ID to check accessibility of.",
            "            requester:",
            "                The user making the request, if it is a local request.",
            "                None if this is a federation request.",
            "            origin:",
            "                The server making the request, if it is a federation request.",
            "                None if this is a local request.",
            "",
            "        Returns:",
            "             True if the room is accessible to the requesting user or server.",
            "        \"\"\"",
            "        state_ids = await self._storage_controllers.state.get_current_state_ids(room_id)",
            "",
            "        # If there's no state for the room, it isn't known.",
            "        if not state_ids:",
            "            # The user might have a pending invite for the room.",
            "            if requester and await self._store.get_invite_for_local_user_in_room(",
            "                requester, room_id",
            "            ):",
            "                return True",
            "",
            "            logger.info(\"room %s is unknown, omitting from summary\", room_id)",
            "            return False",
            "",
            "        try:",
            "            room_version = await self._store.get_room_version(room_id)",
            "        except UnsupportedRoomVersionError:",
            "            # If a room with an unsupported room version is encountered, ignore",
            "            # it to avoid breaking the entire summary response.",
            "            return False",
            "",
            "        # Include the room if it has join rules of public or knock.",
            "        join_rules_event_id = state_ids.get((EventTypes.JoinRules, \"\"))",
            "        if join_rules_event_id:",
            "            join_rules_event = await self._store.get_event(join_rules_event_id)",
            "            join_rule = join_rules_event.content.get(\"join_rule\")",
            "            if (",
            "                join_rule == JoinRules.PUBLIC",
            "                or (room_version.msc2403_knocking and join_rule == JoinRules.KNOCK)",
            "                or (",
            "                    room_version.msc3787_knock_restricted_join_rule",
            "                    and join_rule == JoinRules.KNOCK_RESTRICTED",
            "                )",
            "            ):",
            "                return True",
            "",
            "        # Include the room if it is peekable.",
            "        hist_vis_event_id = state_ids.get((EventTypes.RoomHistoryVisibility, \"\"))",
            "        if hist_vis_event_id:",
            "            hist_vis_ev = await self._store.get_event(hist_vis_event_id)",
            "            hist_vis = hist_vis_ev.content.get(\"history_visibility\")",
            "            if hist_vis == HistoryVisibility.WORLD_READABLE:",
            "                return True",
            "",
            "        # Otherwise we need to check information specific to the user or server.",
            "",
            "        # If we have an authenticated requesting user, check if they are a member",
            "        # of the room (or can join the room).",
            "        if requester:",
            "            member_event_id = state_ids.get((EventTypes.Member, requester), None)",
            "",
            "            # If they're in the room they can see info on it.",
            "            if member_event_id:",
            "                member_event = await self._store.get_event(member_event_id)",
            "                if member_event.membership in (Membership.JOIN, Membership.INVITE):",
            "                    return True",
            "",
            "            # Otherwise, check if they should be allowed access via membership in a space.",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                allowed_rooms = (",
            "                    await self._event_auth_handler.get_rooms_that_allow_join(state_ids)",
            "                )",
            "                if await self._event_auth_handler.is_user_in_rooms(",
            "                    allowed_rooms, requester",
            "                ):",
            "                    return True",
            "",
            "        # If this is a request over federation, check if the host is in the room or",
            "        # has a user who could join the room.",
            "        elif origin:",
            "            if await self._event_auth_handler.is_host_in_room(",
            "                room_id, origin",
            "            ) or await self._store.is_host_invited(room_id, origin):",
            "                return True",
            "",
            "            # Alternately, if the host has a user in any of the spaces specified",
            "            # for access, then the host can see this room (and should do filtering",
            "            # if the requester cannot see it).",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                state_ids, room_version",
            "            ):",
            "                allowed_rooms = (",
            "                    await self._event_auth_handler.get_rooms_that_allow_join(state_ids)",
            "                )",
            "                for space_id in allowed_rooms:",
            "                    if await self._event_auth_handler.is_host_in_room(space_id, origin):",
            "                        return True",
            "",
            "        logger.info(",
            "            \"room %s is unpeekable and requester %s is not a member / not allowed to join, omitting from summary\",",
            "            room_id,",
            "            requester or origin,",
            "        )",
            "        return False",
            "",
            "    async def _is_remote_room_accessible(",
            "        self, requester: Optional[str], room_id: str, room: JsonDict",
            "    ) -> bool:",
            "        \"\"\"",
            "        Calculate whether the room received over federation should be shown to the requester.",
            "",
            "        It should return true if:",
            "",
            "        * The requester is joined or can join the room (per MSC3173).",
            "        * The history visibility is set to world readable.",
            "",
            "        Note that the local server is not in the requested room (which is why the",
            "        remote call was made in the first place), but the user could have access",
            "        due to an invite, etc.",
            "",
            "        Args:",
            "            requester: The user requesting the summary. If not passed only world",
            "                readability is checked.",
            "            room_id: The room ID returned over federation.",
            "            room: The summary of the room returned over federation.",
            "",
            "        Returns:",
            "            True if the room is accessible to the requesting user.",
            "        \"\"\"",
            "        # The API doesn't return the room version so assume that a",
            "        # join rule of knock is valid.",
            "        if (",
            "            room.get(\"join_rule\")",
            "            in (JoinRules.PUBLIC, JoinRules.KNOCK, JoinRules.KNOCK_RESTRICTED)",
            "            or room.get(\"world_readable\") is True",
            "        ):",
            "            return True",
            "        elif not requester:",
            "            return False",
            "",
            "        # Check if the user is a member of any of the allowed rooms from the response.",
            "        allowed_rooms = room.get(\"allowed_room_ids\")",
            "        if allowed_rooms and isinstance(allowed_rooms, list):",
            "            if await self._event_auth_handler.is_user_in_rooms(",
            "                allowed_rooms, requester",
            "            ):",
            "                return True",
            "",
            "        # Finally, check locally if we can access the room. The user might",
            "        # already be in the room (if it was a child room), or there might be a",
            "        # pending invite, etc.",
            "        return await self._is_local_room_accessible(room_id, requester)",
            "",
            "    async def _build_room_entry(self, room_id: str, for_federation: bool) -> JsonDict:",
            "        \"\"\"",
            "        Generate en entry summarising a single room.",
            "",
            "        Args:",
            "            room_id: The room ID to summarize.",
            "            for_federation: True if this is a summary requested over federation",
            "                (which includes additional fields).",
            "",
            "        Returns:",
            "            The JSON dictionary for the room.",
            "        \"\"\"",
            "        stats = await self._store.get_room_with_stats(room_id)",
            "",
            "        # currently this should be impossible because we call",
            "        # _is_local_room_accessible on the room before we get here, so",
            "        # there should always be an entry",
            "        assert stats is not None, \"unable to retrieve stats for %s\" % (room_id,)",
            "",
            "        current_state_ids = await self._storage_controllers.state.get_current_state_ids(",
            "            room_id",
            "        )",
            "        create_event = await self._store.get_event(",
            "            current_state_ids[(EventTypes.Create, \"\")]",
            "        )",
            "",
            "        entry = {",
            "            \"room_id\": stats[\"room_id\"],",
            "            \"name\": stats[\"name\"],",
            "            \"topic\": stats[\"topic\"],",
            "            \"canonical_alias\": stats[\"canonical_alias\"],",
            "            \"num_joined_members\": stats[\"joined_members\"],",
            "            \"avatar_url\": stats[\"avatar\"],",
            "            \"join_rule\": stats[\"join_rules\"],",
            "            \"world_readable\": (",
            "                stats[\"history_visibility\"] == HistoryVisibility.WORLD_READABLE",
            "            ),",
            "            \"guest_can_join\": stats[\"guest_access\"] == \"can_join\",",
            "            \"room_type\": create_event.content.get(EventContentFields.ROOM_TYPE),",
            "        }",
            "",
            "        if self._msc3266_enabled:",
            "            entry[\"im.nheko.summary.version\"] = stats[\"version\"]",
            "            entry[\"im.nheko.summary.encryption\"] = stats[\"encryption\"]",
            "",
            "        # Federation requests need to provide additional information so the",
            "        # requested server is able to filter the response appropriately.",
            "        if for_federation:",
            "            room_version = await self._store.get_room_version(room_id)",
            "            if await self._event_auth_handler.has_restricted_join_rules(",
            "                current_state_ids, room_version",
            "            ):",
            "                allowed_rooms = (",
            "                    await self._event_auth_handler.get_rooms_that_allow_join(",
            "                        current_state_ids",
            "                    )",
            "                )",
            "                if allowed_rooms:",
            "                    entry[\"allowed_room_ids\"] = allowed_rooms",
            "",
            "        # Filter out Nones \u2013 rather omit the field altogether",
            "        room_entry = {k: v for k, v in entry.items() if v is not None}",
            "",
            "        return room_entry",
            "",
            "    async def _get_child_events(self, room_id: str) -> Iterable[EventBase]:",
            "        \"\"\"",
            "        Get the child events for a given room.",
            "",
            "        The returned results are sorted for stability.",
            "",
            "        Args:",
            "            room_id: The room id to get the children of.",
            "",
            "        Returns:",
            "            An iterable of sorted child events.",
            "        \"\"\"",
            "",
            "        # look for child rooms/spaces.",
            "        current_state_ids = await self._storage_controllers.state.get_current_state_ids(",
            "            room_id",
            "        )",
            "",
            "        events = await self._store.get_events_as_list(",
            "            [",
            "                event_id",
            "                for key, event_id in current_state_ids.items()",
            "                if key[0] == EventTypes.SpaceChild",
            "            ]",
            "        )",
            "",
            "        # filter out any events without a \"via\" (which implies it has been redacted),",
            "        # and order to ensure we return stable results.",
            "        return sorted(filter(_has_valid_via, events), key=_child_events_comparison_key)",
            "",
            "    async def get_room_summary(",
            "        self,",
            "        requester: Optional[str],",
            "        room_id: str,",
            "        remote_room_hosts: Optional[List[str]] = None,",
            "    ) -> JsonDict:",
            "        \"\"\"",
            "        Implementation of the room summary C-S API from MSC3266",
            "",
            "        Args:",
            "            requester:  user id of the user making this request, will be None",
            "                for unauthenticated requests",
            "",
            "            room_id: room id to summarise.",
            "",
            "            remote_room_hosts: a list of homeservers to try fetching data through",
            "                if we don't know it ourselves",
            "",
            "        Returns:",
            "            summary dict to return",
            "        \"\"\"",
            "        is_in_room = await self._store.is_host_joined(room_id, self._server_name)",
            "",
            "        if is_in_room:",
            "            room_entry = await self._summarize_local_room(",
            "                requester,",
            "                None,",
            "                room_id,",
            "                # Suggested-only doesn't matter since no children are requested.",
            "                suggested_only=False,",
            "                include_children=False,",
            "            )",
            "",
            "            if not room_entry:",
            "                raise NotFoundError(\"Room not found or is not accessible\")",
            "",
            "            room_summary = room_entry.room",
            "",
            "            # If there was a requester, add their membership.",
            "            if requester:",
            "                (",
            "                    membership,",
            "                    _,",
            "                ) = await self._store.get_local_current_membership_for_user_in_room(",
            "                    requester, room_id",
            "                )",
            "",
            "                room_summary[\"membership\"] = membership or \"leave\"",
            "        else:",
            "            # Reuse the hierarchy query over federation",
            "            if remote_room_hosts is None:",
            "                raise SynapseError(400, \"Missing via to query remote room\")",
            "",
            "            (",
            "                room_entry,",
            "                children_room_entries,",
            "                inaccessible_children,",
            "            ) = await self._summarize_remote_room_hierarchy(",
            "                _RoomQueueEntry(room_id, remote_room_hosts),",
            "                suggested_only=True,",
            "            )",
            "",
            "            # The results over federation might include rooms that we, as the",
            "            # requesting server, are allowed to see, but the requesting user is",
            "            # not permitted to see.",
            "            #",
            "            # Filter the returned results to only what is accessible to the user.",
            "            if not room_entry or not await self._is_remote_room_accessible(",
            "                requester, room_entry.room_id, room_entry.room",
            "            ):",
            "                raise NotFoundError(\"Room not found or is not accessible\")",
            "",
            "            room = dict(room_entry.room)",
            "            room.pop(\"allowed_room_ids\", None)",
            "",
            "            # If there was a requester, add their membership.",
            "            # We keep the membership in the local membership table unless the",
            "            # room is purged even for remote rooms.",
            "            if requester:",
            "                (",
            "                    membership,",
            "                    _,",
            "                ) = await self._store.get_local_current_membership_for_user_in_room(",
            "                    requester, room_id",
            "                )",
            "                room[\"membership\"] = membership or \"leave\"",
            "",
            "            return room",
            "",
            "        return room_summary",
            "",
            "",
            "@attr.s(frozen=True, slots=True, auto_attribs=True)",
            "class _RoomQueueEntry:",
            "    # The room ID of this entry.",
            "    room_id: str",
            "    # The server to query if the room is not known locally.",
            "    via: Sequence[str]",
            "    # The minimum number of hops necessary to get to this room (compared to the",
            "    # originally requested room).",
            "    depth: int = 0",
            "    # The room summary for this room returned via federation. This will only be",
            "    # used if the room is not known locally (and is not a space).",
            "    remote_room: Optional[JsonDict] = None",
            "",
            "",
            "@attr.s(frozen=True, slots=True, auto_attribs=True)",
            "class _RoomEntry:",
            "    room_id: str",
            "    # The room summary for this room.",
            "    room: JsonDict",
            "    # An iterable of the sorted, stripped children events for children of this room.",
            "    #",
            "    # This may not include all children.",
            "    children_state_events: Sequence[JsonDict] = ()",
            "",
            "    def as_json(self, for_client: bool = False) -> JsonDict:",
            "        \"\"\"",
            "        Returns a JSON dictionary suitable for the room hierarchy endpoint.",
            "",
            "        It returns the room summary including the stripped m.space.child events",
            "        as a sub-key.",
            "",
            "        Args:",
            "            for_client: If true, any server-server only fields are stripped from",
            "                the result.",
            "",
            "        \"\"\"",
            "        result = dict(self.room)",
            "",
            "        # Before returning to the client, remove the allowed_room_ids key, if it",
            "        # exists.",
            "        if for_client:",
            "            result.pop(\"allowed_room_ids\", False)",
            "",
            "        result[\"children_state\"] = self.children_state_events",
            "        return result",
            "",
            "",
            "def _has_valid_via(e: EventBase) -> bool:",
            "    via = e.content.get(\"via\")",
            "    if not via or not isinstance(via, list):",
            "        return False",
            "    for v in via:",
            "        if not isinstance(v, str):",
            "            logger.debug(\"Ignoring edge event %s with invalid via entry\", e.event_id)",
            "            return False",
            "    return True",
            "",
            "",
            "def _is_suggested_child_event(edge_event: EventBase) -> bool:",
            "    suggested = edge_event.content.get(\"suggested\")",
            "    if isinstance(suggested, bool) and suggested:",
            "        return True",
            "    logger.debug(\"Ignorning not-suggested child %s\", edge_event.state_key)",
            "    return False",
            "",
            "",
            "# Order may only contain characters in the range of \\x20 (space) to \\x7E (~) inclusive.",
            "_INVALID_ORDER_CHARS_RE = re.compile(r\"[^\\x20-\\x7E]\")",
            "",
            "",
            "def _child_events_comparison_key(",
            "    child: EventBase,",
            ") -> Tuple[bool, Optional[str], int, str]:",
            "    \"\"\"",
            "    Generate a value for comparing two child events for ordering.",
            "",
            "    The rules for ordering are:",
            "",
            "    1. The 'order' key, if it is valid.",
            "    2. The 'origin_server_ts' of the 'm.space.child' event.",
            "    3. The 'room_id'.",
            "",
            "    Args:",
            "        child: The event for generating a comparison key.",
            "",
            "    Returns:",
            "        The comparison key as a tuple of:",
            "            False if the ordering is valid.",
            "            The 'order' field or None if it is not given or invalid.",
            "            The 'origin_server_ts' field.",
            "            The room ID.",
            "    \"\"\"",
            "    order = child.content.get(\"order\")",
            "    # If order is not a string or doesn't meet the requirements, ignore it.",
            "    if not isinstance(order, str):",
            "        order = None",
            "    elif len(order) > 50 or _INVALID_ORDER_CHARS_RE.search(order):",
            "        order = None",
            "",
            "    # Items without an order come last.",
            "    return order is None, order, child.origin_server_ts, child.room_id"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "612": [
                "RoomSummaryHandler"
            ],
            "627": [
                "RoomSummaryHandler"
            ],
            "628": [
                "RoomSummaryHandler"
            ],
            "629": [
                "RoomSummaryHandler"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/typing.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": 340,
                "PatchRowcode": "         # If we're not in the room just ditch the event entirely. This is"
            },
            "1": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "         # probably an old server that has come back and thinks we're still in"
            },
            "2": {
                "beforePatchRowNumber": 342,
                "afterPatchRowNumber": 342,
                "PatchRowcode": "         # the room (or we've been rejoined to the room by a state reset)."
            },
            "3": {
                "beforePatchRowNumber": 343,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        is_in_room = await self.event_auth_handler.check_host_in_room("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+        is_in_room = await self.event_auth_handler.is_host_in_room("
            },
            "5": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": 344,
                "PatchRowcode": "             room_id, self.server_name"
            },
            "6": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 345,
                "PatchRowcode": "         )"
            },
            "7": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "         if not is_in_room:"
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Set, Tuple",
            "",
            "import attr",
            "",
            "from synapse.api.constants import EduTypes",
            "from synapse.api.errors import AuthError, ShadowBanError, SynapseError",
            "from synapse.appservice import ApplicationService",
            "from synapse.metrics.background_process_metrics import (",
            "    run_as_background_process,",
            "    wrap_as_background_process,",
            ")",
            "from synapse.replication.tcp.streams import TypingStream",
            "from synapse.streams import EventSource",
            "from synapse.types import JsonDict, Requester, StreamKeyType, UserID",
            "from synapse.util.caches.stream_change_cache import StreamChangeCache",
            "from synapse.util.metrics import Measure",
            "from synapse.util.wheel_timer import WheelTimer",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "# A tiny object useful for storing a user's membership in a room, as a mapping",
            "# key",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class RoomMember:",
            "    room_id: str",
            "    user_id: str",
            "",
            "",
            "# How often we expect remote servers to resend us presence.",
            "FEDERATION_TIMEOUT = 60 * 1000",
            "",
            "# How often to resend typing across federation.",
            "FEDERATION_PING_INTERVAL = 40 * 1000",
            "",
            "",
            "class FollowerTypingHandler:",
            "    \"\"\"A typing handler on a different process than the writer that is updated",
            "    via replication.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.server_name = hs.config.server.server_name",
            "        self.clock = hs.get_clock()",
            "        self.is_mine_id = hs.is_mine_id",
            "",
            "        self.federation = None",
            "        if hs.should_send_federation():",
            "            self.federation = hs.get_federation_sender()",
            "",
            "        if hs.get_instance_name() not in hs.config.worker.writers.typing:",
            "            hs.get_federation_registry().register_instances_for_edu(",
            "                EduTypes.TYPING,",
            "                hs.config.worker.writers.typing,",
            "            )",
            "",
            "        # map room IDs to serial numbers",
            "        self._room_serials: Dict[str, int] = {}",
            "        # map room IDs to sets of users currently typing",
            "        self._room_typing: Dict[str, Set[str]] = {}",
            "",
            "        self._member_last_federation_poke: Dict[RoomMember, int] = {}",
            "        self.wheel_timer: WheelTimer[RoomMember] = WheelTimer(bucket_size=5000)",
            "        self._latest_room_serial = 0",
            "",
            "        self.clock.looping_call(self._handle_timeouts, 5000)",
            "",
            "    def _reset(self) -> None:",
            "        \"\"\"Reset the typing handler's data caches.\"\"\"",
            "        # map room IDs to serial numbers",
            "        self._room_serials = {}",
            "        # map room IDs to sets of users currently typing",
            "        self._room_typing = {}",
            "",
            "        self._member_last_federation_poke = {}",
            "        self.wheel_timer = WheelTimer(bucket_size=5000)",
            "",
            "    @wrap_as_background_process(\"typing._handle_timeouts\")",
            "    async def _handle_timeouts(self) -> None:",
            "        logger.debug(\"Checking for typing timeouts\")",
            "",
            "        now = self.clock.time_msec()",
            "",
            "        members = set(self.wheel_timer.fetch(now))",
            "",
            "        for member in members:",
            "            self._handle_timeout_for_member(now, member)",
            "",
            "    def _handle_timeout_for_member(self, now: int, member: RoomMember) -> None:",
            "        if not self.is_typing(member):",
            "            # Nothing to do if they're no longer typing",
            "            return",
            "",
            "        # Check if we need to resend a keep alive over federation for this",
            "        # user.",
            "        if self.federation and self.is_mine_id(member.user_id):",
            "            last_fed_poke = self._member_last_federation_poke.get(member, None)",
            "            if not last_fed_poke or last_fed_poke + FEDERATION_PING_INTERVAL <= now:",
            "                run_as_background_process(",
            "                    \"typing._push_remote\", self._push_remote, member=member, typing=True",
            "                )",
            "",
            "        # Add a paranoia timer to ensure that we always have a timer for",
            "        # each person typing.",
            "        self.wheel_timer.insert(now=now, obj=member, then=now + 60 * 1000)",
            "",
            "    def is_typing(self, member: RoomMember) -> bool:",
            "        return member.user_id in self._room_typing.get(member.room_id, set())",
            "",
            "    async def _push_remote(self, member: RoomMember, typing: bool) -> None:",
            "        if not self.federation:",
            "            return",
            "",
            "        try:",
            "            self._member_last_federation_poke[member] = self.clock.time_msec()",
            "",
            "            now = self.clock.time_msec()",
            "            self.wheel_timer.insert(",
            "                now=now, obj=member, then=now + FEDERATION_PING_INTERVAL",
            "            )",
            "",
            "            hosts = await self._storage_controllers.state.get_current_hosts_in_room(",
            "                member.room_id",
            "            )",
            "            for domain in hosts:",
            "                if domain != self.server_name:",
            "                    logger.debug(\"sending typing update to %s\", domain)",
            "                    self.federation.build_and_send_edu(",
            "                        destination=domain,",
            "                        edu_type=EduTypes.TYPING,",
            "                        content={",
            "                            \"room_id\": member.room_id,",
            "                            \"user_id\": member.user_id,",
            "                            \"typing\": typing,",
            "                        },",
            "                        key=member,",
            "                    )",
            "        except Exception:",
            "            logger.exception(\"Error pushing typing notif to remotes\")",
            "",
            "    def process_replication_rows(",
            "        self, token: int, rows: List[TypingStream.TypingStreamRow]",
            "    ) -> None:",
            "        \"\"\"Should be called whenever we receive updates for typing stream.\"\"\"",
            "",
            "        if self._latest_room_serial > token:",
            "            # The typing worker has gone backwards (e.g. it may have restarted).",
            "            # To prevent inconsistent data, just clear everything.",
            "            logger.info(\"Typing handler stream went backwards; resetting\")",
            "            self._reset()",
            "",
            "        # Set the latest serial token to whatever the server gave us.",
            "        self._latest_room_serial = token",
            "",
            "        for row in rows:",
            "            self._room_serials[row.room_id] = token",
            "",
            "            prev_typing = self._room_typing.get(row.room_id, set())",
            "            now_typing = set(row.user_ids)",
            "            self._room_typing[row.room_id] = now_typing",
            "",
            "            if self.federation:",
            "                run_as_background_process(",
            "                    \"_send_changes_in_typing_to_remotes\",",
            "                    self._send_changes_in_typing_to_remotes,",
            "                    row.room_id,",
            "                    prev_typing,",
            "                    now_typing,",
            "                )",
            "",
            "    async def _send_changes_in_typing_to_remotes(",
            "        self, room_id: str, prev_typing: Set[str], now_typing: Set[str]",
            "    ) -> None:",
            "        \"\"\"Process a change in typing of a room from replication, sending EDUs",
            "        for any local users.",
            "        \"\"\"",
            "",
            "        if not self.federation:",
            "            return",
            "",
            "        for user_id in now_typing - prev_typing:",
            "            if self.is_mine_id(user_id):",
            "                await self._push_remote(RoomMember(room_id, user_id), True)",
            "",
            "        for user_id in prev_typing - now_typing:",
            "            if self.is_mine_id(user_id):",
            "                await self._push_remote(RoomMember(room_id, user_id), False)",
            "",
            "    def get_current_token(self) -> int:",
            "        return self._latest_room_serial",
            "",
            "",
            "class TypingWriterHandler(FollowerTypingHandler):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        assert hs.get_instance_name() in hs.config.worker.writers.typing",
            "",
            "        self.auth = hs.get_auth()",
            "        self.notifier = hs.get_notifier()",
            "        self.event_auth_handler = hs.get_event_auth_handler()",
            "",
            "        self.hs = hs",
            "",
            "        hs.get_federation_registry().register_edu_handler(",
            "            EduTypes.TYPING, self._recv_edu",
            "        )",
            "",
            "        hs.get_distributor().observe(\"user_left_room\", self.user_left_room)",
            "",
            "        # clock time we expect to stop",
            "        self._member_typing_until: Dict[RoomMember, int] = {}",
            "",
            "        # caches which room_ids changed at which serials",
            "        self._typing_stream_change_cache = StreamChangeCache(",
            "            \"TypingStreamChangeCache\", self._latest_room_serial",
            "        )",
            "",
            "    def _handle_timeout_for_member(self, now: int, member: RoomMember) -> None:",
            "        super()._handle_timeout_for_member(now, member)",
            "",
            "        if not self.is_typing(member):",
            "            # Nothing to do if they're no longer typing",
            "            return",
            "",
            "        until = self._member_typing_until.get(member, None)",
            "        if not until or until <= now:",
            "            logger.info(\"Timing out typing for: %s\", member.user_id)",
            "            self._stopped_typing(member)",
            "            return",
            "",
            "    async def started_typing(",
            "        self, target_user: UserID, requester: Requester, room_id: str, timeout: int",
            "    ) -> None:",
            "        target_user_id = target_user.to_string()",
            "",
            "        if not self.is_mine_id(target_user_id):",
            "            raise SynapseError(400, \"User is not hosted on this homeserver\")",
            "",
            "        if target_user != requester.user:",
            "            raise AuthError(400, \"Cannot set another user's typing state\")",
            "",
            "        if requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        await self.auth.check_user_in_room(room_id, requester)",
            "",
            "        logger.debug(\"%s has started typing in %s\", target_user_id, room_id)",
            "",
            "        member = RoomMember(room_id=room_id, user_id=target_user_id)",
            "",
            "        was_present = member.user_id in self._room_typing.get(room_id, set())",
            "",
            "        now = self.clock.time_msec()",
            "        self._member_typing_until[member] = now + timeout",
            "",
            "        self.wheel_timer.insert(now=now, obj=member, then=now + timeout)",
            "",
            "        if was_present:",
            "            # No point sending another notification",
            "            return",
            "",
            "        self._push_update(member=member, typing=True)",
            "",
            "    async def stopped_typing(",
            "        self, target_user: UserID, requester: Requester, room_id: str",
            "    ) -> None:",
            "        target_user_id = target_user.to_string()",
            "",
            "        if not self.is_mine_id(target_user_id):",
            "            raise SynapseError(400, \"User is not hosted on this homeserver\")",
            "",
            "        if target_user != requester.user:",
            "            raise AuthError(400, \"Cannot set another user's typing state\")",
            "",
            "        if requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        await self.auth.check_user_in_room(room_id, requester)",
            "",
            "        logger.debug(\"%s has stopped typing in %s\", target_user_id, room_id)",
            "",
            "        member = RoomMember(room_id=room_id, user_id=target_user_id)",
            "",
            "        self._stopped_typing(member)",
            "",
            "    def user_left_room(self, user: UserID, room_id: str) -> None:",
            "        user_id = user.to_string()",
            "        if self.is_mine_id(user_id):",
            "            member = RoomMember(room_id=room_id, user_id=user_id)",
            "            self._stopped_typing(member)",
            "",
            "    def _stopped_typing(self, member: RoomMember) -> None:",
            "        if member.user_id not in self._room_typing.get(member.room_id, set()):",
            "            # No point",
            "            return",
            "",
            "        self._member_typing_until.pop(member, None)",
            "        self._member_last_federation_poke.pop(member, None)",
            "",
            "        self._push_update(member=member, typing=False)",
            "",
            "    def _push_update(self, member: RoomMember, typing: bool) -> None:",
            "        if self.hs.is_mine_id(member.user_id):",
            "            # Only send updates for changes to our own users.",
            "            run_as_background_process(",
            "                \"typing._push_remote\", self._push_remote, member, typing",
            "            )",
            "",
            "        self._push_update_local(member=member, typing=typing)",
            "",
            "    async def _recv_edu(self, origin: str, content: JsonDict) -> None:",
            "        room_id = content[\"room_id\"]",
            "        user_id = content[\"user_id\"]",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        is_in_room = await self.event_auth_handler.check_host_in_room(",
            "            room_id, self.server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Ignoring typing update for room %r from server %s as we're not in the room\",",
            "                room_id,",
            "                origin,",
            "            )",
            "            return",
            "",
            "        member = RoomMember(user_id=user_id, room_id=room_id)",
            "",
            "        # Check that the string is a valid user id",
            "        user = UserID.from_string(user_id)",
            "",
            "        if user.domain != origin:",
            "            logger.info(",
            "                \"Got typing update from %r with bad 'user_id': %r\", origin, user_id",
            "            )",
            "            return",
            "",
            "        domains = await self._storage_controllers.state.get_current_hosts_in_room(",
            "            room_id",
            "        )",
            "",
            "        if self.server_name in domains:",
            "            logger.info(\"Got typing update from %s: %r\", user_id, content)",
            "            now = self.clock.time_msec()",
            "            self._member_typing_until[member] = now + FEDERATION_TIMEOUT",
            "            self.wheel_timer.insert(now=now, obj=member, then=now + FEDERATION_TIMEOUT)",
            "            self._push_update_local(member=member, typing=content[\"typing\"])",
            "",
            "    def _push_update_local(self, member: RoomMember, typing: bool) -> None:",
            "        room_set = self._room_typing.setdefault(member.room_id, set())",
            "        if typing:",
            "            room_set.add(member.user_id)",
            "        else:",
            "            room_set.discard(member.user_id)",
            "",
            "        self._latest_room_serial += 1",
            "        self._room_serials[member.room_id] = self._latest_room_serial",
            "        self._typing_stream_change_cache.entity_has_changed(",
            "            member.room_id, self._latest_room_serial",
            "        )",
            "",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.TYPING, self._latest_room_serial, rooms=[member.room_id]",
            "        )",
            "",
            "    async def get_all_typing_updates(",
            "        self, instance_name: str, last_id: int, current_id: int, limit: int",
            "    ) -> Tuple[List[Tuple[int, list]], int, bool]:",
            "        \"\"\"Get updates for typing replication stream.",
            "",
            "        Args:",
            "            instance_name: The writer we want to fetch updates from. Unused",
            "                here since there is only ever one writer.",
            "            last_id: The token to fetch updates from. Exclusive.",
            "            current_id: The token to fetch updates up to. Inclusive.",
            "            limit: The requested limit for the number of rows to return. The",
            "                function may return more or fewer rows.",
            "",
            "        Returns:",
            "            A tuple consisting of: the updates, a token to use to fetch",
            "            subsequent updates, and whether we returned fewer rows than exists",
            "            between the requested tokens due to the limit.",
            "",
            "            The token returned can be used in a subsequent call to this",
            "            function to get further updates.",
            "",
            "            The updates are a list of 2-tuples of stream ID and the row data",
            "        \"\"\"",
            "",
            "        if last_id == current_id:",
            "            return [], current_id, False",
            "",
            "        changed_rooms: Optional[",
            "            Iterable[str]",
            "        ] = self._typing_stream_change_cache.get_all_entities_changed(last_id)",
            "",
            "        if changed_rooms is None:",
            "            changed_rooms = self._room_serials",
            "",
            "        rows = []",
            "        for room_id in changed_rooms:",
            "            serial = self._room_serials[room_id]",
            "            if last_id < serial <= current_id:",
            "                typing = self._room_typing[room_id]",
            "                rows.append((serial, [room_id, list(typing)]))",
            "        rows.sort()",
            "",
            "        limited = False",
            "        # We, unusually, use a strict limit here as we have all the rows in",
            "        # memory rather than pulling them out of the database with a `LIMIT ?`",
            "        # clause.",
            "        if len(rows) > limit:",
            "            rows = rows[:limit]",
            "            current_id = rows[-1][0]",
            "            limited = True",
            "",
            "        return rows, current_id, limited",
            "",
            "    def process_replication_rows(",
            "        self, token: int, rows: List[TypingStream.TypingStreamRow]",
            "    ) -> None:",
            "        # The writing process should never get updates from replication.",
            "        raise Exception(\"Typing writer instance got typing info over replication\")",
            "",
            "",
            "class TypingNotificationEventSource(EventSource[int, JsonDict]):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._main_store = hs.get_datastores().main",
            "        self.clock = hs.get_clock()",
            "        # We can't call get_typing_handler here because there's a cycle:",
            "        #",
            "        #   Typing -> Notifier -> TypingNotificationEventSource -> Typing",
            "        #",
            "        self.get_typing_handler = hs.get_typing_handler",
            "",
            "    def _make_event_for(self, room_id: str) -> JsonDict:",
            "        typing = self.get_typing_handler()._room_typing[room_id]",
            "        return {",
            "            \"type\": EduTypes.TYPING,",
            "            \"room_id\": room_id,",
            "            \"content\": {\"user_ids\": list(typing)},",
            "        }",
            "",
            "    async def get_new_events_as(",
            "        self, from_key: int, service: ApplicationService",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        \"\"\"Returns a set of new typing events that an appservice",
            "        may be interested in.",
            "",
            "        Args:",
            "            from_key: the stream position at which events should be fetched from.",
            "            service: The appservice which may be interested.",
            "",
            "        Returns:",
            "            A two-tuple containing the following:",
            "                * A list of json dictionaries derived from typing events that the",
            "                  appservice may be interested in.",
            "                * The latest known room serial.",
            "        \"\"\"",
            "        with Measure(self.clock, \"typing.get_new_events_as\"):",
            "            handler = self.get_typing_handler()",
            "",
            "            events = []",
            "",
            "            # Work on a copy of things here as these may change in the handler while",
            "            # waiting for the AS `is_interested_in_room` call to complete.",
            "            # Shallow copy is safe as no nested data is present.",
            "            latest_room_serial = handler._latest_room_serial",
            "            room_serials = handler._room_serials.copy()",
            "",
            "            for room_id, serial in room_serials.items():",
            "                if serial <= from_key:",
            "                    continue",
            "",
            "                if not await service.is_interested_in_room(room_id, self._main_store):",
            "                    continue",
            "",
            "                events.append(self._make_event_for(room_id))",
            "",
            "            return events, latest_room_serial",
            "",
            "    async def get_new_events(",
            "        self,",
            "        user: UserID,",
            "        from_key: int,",
            "        limit: Optional[int],",
            "        room_ids: Iterable[str],",
            "        is_guest: bool,",
            "        explicit_room_id: Optional[str] = None,",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        with Measure(self.clock, \"typing.get_new_events\"):",
            "            from_key = int(from_key)",
            "            handler = self.get_typing_handler()",
            "",
            "            events = []",
            "            for room_id in room_ids:",
            "                if room_id not in handler._room_serials:",
            "                    continue",
            "                if handler._room_serials[room_id] <= from_key:",
            "                    continue",
            "",
            "                events.append(self._make_event_for(room_id))",
            "",
            "            return events, handler._latest_room_serial",
            "",
            "    def get_current_key(self) -> int:",
            "        return self.get_typing_handler()._latest_room_serial"
        ],
        "afterPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Set, Tuple",
            "",
            "import attr",
            "",
            "from synapse.api.constants import EduTypes",
            "from synapse.api.errors import AuthError, ShadowBanError, SynapseError",
            "from synapse.appservice import ApplicationService",
            "from synapse.metrics.background_process_metrics import (",
            "    run_as_background_process,",
            "    wrap_as_background_process,",
            ")",
            "from synapse.replication.tcp.streams import TypingStream",
            "from synapse.streams import EventSource",
            "from synapse.types import JsonDict, Requester, StreamKeyType, UserID",
            "from synapse.util.caches.stream_change_cache import StreamChangeCache",
            "from synapse.util.metrics import Measure",
            "from synapse.util.wheel_timer import WheelTimer",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "# A tiny object useful for storing a user's membership in a room, as a mapping",
            "# key",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class RoomMember:",
            "    room_id: str",
            "    user_id: str",
            "",
            "",
            "# How often we expect remote servers to resend us presence.",
            "FEDERATION_TIMEOUT = 60 * 1000",
            "",
            "# How often to resend typing across federation.",
            "FEDERATION_PING_INTERVAL = 40 * 1000",
            "",
            "",
            "class FollowerTypingHandler:",
            "    \"\"\"A typing handler on a different process than the writer that is updated",
            "    via replication.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.server_name = hs.config.server.server_name",
            "        self.clock = hs.get_clock()",
            "        self.is_mine_id = hs.is_mine_id",
            "",
            "        self.federation = None",
            "        if hs.should_send_federation():",
            "            self.federation = hs.get_federation_sender()",
            "",
            "        if hs.get_instance_name() not in hs.config.worker.writers.typing:",
            "            hs.get_federation_registry().register_instances_for_edu(",
            "                EduTypes.TYPING,",
            "                hs.config.worker.writers.typing,",
            "            )",
            "",
            "        # map room IDs to serial numbers",
            "        self._room_serials: Dict[str, int] = {}",
            "        # map room IDs to sets of users currently typing",
            "        self._room_typing: Dict[str, Set[str]] = {}",
            "",
            "        self._member_last_federation_poke: Dict[RoomMember, int] = {}",
            "        self.wheel_timer: WheelTimer[RoomMember] = WheelTimer(bucket_size=5000)",
            "        self._latest_room_serial = 0",
            "",
            "        self.clock.looping_call(self._handle_timeouts, 5000)",
            "",
            "    def _reset(self) -> None:",
            "        \"\"\"Reset the typing handler's data caches.\"\"\"",
            "        # map room IDs to serial numbers",
            "        self._room_serials = {}",
            "        # map room IDs to sets of users currently typing",
            "        self._room_typing = {}",
            "",
            "        self._member_last_federation_poke = {}",
            "        self.wheel_timer = WheelTimer(bucket_size=5000)",
            "",
            "    @wrap_as_background_process(\"typing._handle_timeouts\")",
            "    async def _handle_timeouts(self) -> None:",
            "        logger.debug(\"Checking for typing timeouts\")",
            "",
            "        now = self.clock.time_msec()",
            "",
            "        members = set(self.wheel_timer.fetch(now))",
            "",
            "        for member in members:",
            "            self._handle_timeout_for_member(now, member)",
            "",
            "    def _handle_timeout_for_member(self, now: int, member: RoomMember) -> None:",
            "        if not self.is_typing(member):",
            "            # Nothing to do if they're no longer typing",
            "            return",
            "",
            "        # Check if we need to resend a keep alive over federation for this",
            "        # user.",
            "        if self.federation and self.is_mine_id(member.user_id):",
            "            last_fed_poke = self._member_last_federation_poke.get(member, None)",
            "            if not last_fed_poke or last_fed_poke + FEDERATION_PING_INTERVAL <= now:",
            "                run_as_background_process(",
            "                    \"typing._push_remote\", self._push_remote, member=member, typing=True",
            "                )",
            "",
            "        # Add a paranoia timer to ensure that we always have a timer for",
            "        # each person typing.",
            "        self.wheel_timer.insert(now=now, obj=member, then=now + 60 * 1000)",
            "",
            "    def is_typing(self, member: RoomMember) -> bool:",
            "        return member.user_id in self._room_typing.get(member.room_id, set())",
            "",
            "    async def _push_remote(self, member: RoomMember, typing: bool) -> None:",
            "        if not self.federation:",
            "            return",
            "",
            "        try:",
            "            self._member_last_federation_poke[member] = self.clock.time_msec()",
            "",
            "            now = self.clock.time_msec()",
            "            self.wheel_timer.insert(",
            "                now=now, obj=member, then=now + FEDERATION_PING_INTERVAL",
            "            )",
            "",
            "            hosts = await self._storage_controllers.state.get_current_hosts_in_room(",
            "                member.room_id",
            "            )",
            "            for domain in hosts:",
            "                if domain != self.server_name:",
            "                    logger.debug(\"sending typing update to %s\", domain)",
            "                    self.federation.build_and_send_edu(",
            "                        destination=domain,",
            "                        edu_type=EduTypes.TYPING,",
            "                        content={",
            "                            \"room_id\": member.room_id,",
            "                            \"user_id\": member.user_id,",
            "                            \"typing\": typing,",
            "                        },",
            "                        key=member,",
            "                    )",
            "        except Exception:",
            "            logger.exception(\"Error pushing typing notif to remotes\")",
            "",
            "    def process_replication_rows(",
            "        self, token: int, rows: List[TypingStream.TypingStreamRow]",
            "    ) -> None:",
            "        \"\"\"Should be called whenever we receive updates for typing stream.\"\"\"",
            "",
            "        if self._latest_room_serial > token:",
            "            # The typing worker has gone backwards (e.g. it may have restarted).",
            "            # To prevent inconsistent data, just clear everything.",
            "            logger.info(\"Typing handler stream went backwards; resetting\")",
            "            self._reset()",
            "",
            "        # Set the latest serial token to whatever the server gave us.",
            "        self._latest_room_serial = token",
            "",
            "        for row in rows:",
            "            self._room_serials[row.room_id] = token",
            "",
            "            prev_typing = self._room_typing.get(row.room_id, set())",
            "            now_typing = set(row.user_ids)",
            "            self._room_typing[row.room_id] = now_typing",
            "",
            "            if self.federation:",
            "                run_as_background_process(",
            "                    \"_send_changes_in_typing_to_remotes\",",
            "                    self._send_changes_in_typing_to_remotes,",
            "                    row.room_id,",
            "                    prev_typing,",
            "                    now_typing,",
            "                )",
            "",
            "    async def _send_changes_in_typing_to_remotes(",
            "        self, room_id: str, prev_typing: Set[str], now_typing: Set[str]",
            "    ) -> None:",
            "        \"\"\"Process a change in typing of a room from replication, sending EDUs",
            "        for any local users.",
            "        \"\"\"",
            "",
            "        if not self.federation:",
            "            return",
            "",
            "        for user_id in now_typing - prev_typing:",
            "            if self.is_mine_id(user_id):",
            "                await self._push_remote(RoomMember(room_id, user_id), True)",
            "",
            "        for user_id in prev_typing - now_typing:",
            "            if self.is_mine_id(user_id):",
            "                await self._push_remote(RoomMember(room_id, user_id), False)",
            "",
            "    def get_current_token(self) -> int:",
            "        return self._latest_room_serial",
            "",
            "",
            "class TypingWriterHandler(FollowerTypingHandler):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        assert hs.get_instance_name() in hs.config.worker.writers.typing",
            "",
            "        self.auth = hs.get_auth()",
            "        self.notifier = hs.get_notifier()",
            "        self.event_auth_handler = hs.get_event_auth_handler()",
            "",
            "        self.hs = hs",
            "",
            "        hs.get_federation_registry().register_edu_handler(",
            "            EduTypes.TYPING, self._recv_edu",
            "        )",
            "",
            "        hs.get_distributor().observe(\"user_left_room\", self.user_left_room)",
            "",
            "        # clock time we expect to stop",
            "        self._member_typing_until: Dict[RoomMember, int] = {}",
            "",
            "        # caches which room_ids changed at which serials",
            "        self._typing_stream_change_cache = StreamChangeCache(",
            "            \"TypingStreamChangeCache\", self._latest_room_serial",
            "        )",
            "",
            "    def _handle_timeout_for_member(self, now: int, member: RoomMember) -> None:",
            "        super()._handle_timeout_for_member(now, member)",
            "",
            "        if not self.is_typing(member):",
            "            # Nothing to do if they're no longer typing",
            "            return",
            "",
            "        until = self._member_typing_until.get(member, None)",
            "        if not until or until <= now:",
            "            logger.info(\"Timing out typing for: %s\", member.user_id)",
            "            self._stopped_typing(member)",
            "            return",
            "",
            "    async def started_typing(",
            "        self, target_user: UserID, requester: Requester, room_id: str, timeout: int",
            "    ) -> None:",
            "        target_user_id = target_user.to_string()",
            "",
            "        if not self.is_mine_id(target_user_id):",
            "            raise SynapseError(400, \"User is not hosted on this homeserver\")",
            "",
            "        if target_user != requester.user:",
            "            raise AuthError(400, \"Cannot set another user's typing state\")",
            "",
            "        if requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        await self.auth.check_user_in_room(room_id, requester)",
            "",
            "        logger.debug(\"%s has started typing in %s\", target_user_id, room_id)",
            "",
            "        member = RoomMember(room_id=room_id, user_id=target_user_id)",
            "",
            "        was_present = member.user_id in self._room_typing.get(room_id, set())",
            "",
            "        now = self.clock.time_msec()",
            "        self._member_typing_until[member] = now + timeout",
            "",
            "        self.wheel_timer.insert(now=now, obj=member, then=now + timeout)",
            "",
            "        if was_present:",
            "            # No point sending another notification",
            "            return",
            "",
            "        self._push_update(member=member, typing=True)",
            "",
            "    async def stopped_typing(",
            "        self, target_user: UserID, requester: Requester, room_id: str",
            "    ) -> None:",
            "        target_user_id = target_user.to_string()",
            "",
            "        if not self.is_mine_id(target_user_id):",
            "            raise SynapseError(400, \"User is not hosted on this homeserver\")",
            "",
            "        if target_user != requester.user:",
            "            raise AuthError(400, \"Cannot set another user's typing state\")",
            "",
            "        if requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        await self.auth.check_user_in_room(room_id, requester)",
            "",
            "        logger.debug(\"%s has stopped typing in %s\", target_user_id, room_id)",
            "",
            "        member = RoomMember(room_id=room_id, user_id=target_user_id)",
            "",
            "        self._stopped_typing(member)",
            "",
            "    def user_left_room(self, user: UserID, room_id: str) -> None:",
            "        user_id = user.to_string()",
            "        if self.is_mine_id(user_id):",
            "            member = RoomMember(room_id=room_id, user_id=user_id)",
            "            self._stopped_typing(member)",
            "",
            "    def _stopped_typing(self, member: RoomMember) -> None:",
            "        if member.user_id not in self._room_typing.get(member.room_id, set()):",
            "            # No point",
            "            return",
            "",
            "        self._member_typing_until.pop(member, None)",
            "        self._member_last_federation_poke.pop(member, None)",
            "",
            "        self._push_update(member=member, typing=False)",
            "",
            "    def _push_update(self, member: RoomMember, typing: bool) -> None:",
            "        if self.hs.is_mine_id(member.user_id):",
            "            # Only send updates for changes to our own users.",
            "            run_as_background_process(",
            "                \"typing._push_remote\", self._push_remote, member, typing",
            "            )",
            "",
            "        self._push_update_local(member=member, typing=typing)",
            "",
            "    async def _recv_edu(self, origin: str, content: JsonDict) -> None:",
            "        room_id = content[\"room_id\"]",
            "        user_id = content[\"user_id\"]",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        is_in_room = await self.event_auth_handler.is_host_in_room(",
            "            room_id, self.server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Ignoring typing update for room %r from server %s as we're not in the room\",",
            "                room_id,",
            "                origin,",
            "            )",
            "            return",
            "",
            "        member = RoomMember(user_id=user_id, room_id=room_id)",
            "",
            "        # Check that the string is a valid user id",
            "        user = UserID.from_string(user_id)",
            "",
            "        if user.domain != origin:",
            "            logger.info(",
            "                \"Got typing update from %r with bad 'user_id': %r\", origin, user_id",
            "            )",
            "            return",
            "",
            "        domains = await self._storage_controllers.state.get_current_hosts_in_room(",
            "            room_id",
            "        )",
            "",
            "        if self.server_name in domains:",
            "            logger.info(\"Got typing update from %s: %r\", user_id, content)",
            "            now = self.clock.time_msec()",
            "            self._member_typing_until[member] = now + FEDERATION_TIMEOUT",
            "            self.wheel_timer.insert(now=now, obj=member, then=now + FEDERATION_TIMEOUT)",
            "            self._push_update_local(member=member, typing=content[\"typing\"])",
            "",
            "    def _push_update_local(self, member: RoomMember, typing: bool) -> None:",
            "        room_set = self._room_typing.setdefault(member.room_id, set())",
            "        if typing:",
            "            room_set.add(member.user_id)",
            "        else:",
            "            room_set.discard(member.user_id)",
            "",
            "        self._latest_room_serial += 1",
            "        self._room_serials[member.room_id] = self._latest_room_serial",
            "        self._typing_stream_change_cache.entity_has_changed(",
            "            member.room_id, self._latest_room_serial",
            "        )",
            "",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.TYPING, self._latest_room_serial, rooms=[member.room_id]",
            "        )",
            "",
            "    async def get_all_typing_updates(",
            "        self, instance_name: str, last_id: int, current_id: int, limit: int",
            "    ) -> Tuple[List[Tuple[int, list]], int, bool]:",
            "        \"\"\"Get updates for typing replication stream.",
            "",
            "        Args:",
            "            instance_name: The writer we want to fetch updates from. Unused",
            "                here since there is only ever one writer.",
            "            last_id: The token to fetch updates from. Exclusive.",
            "            current_id: The token to fetch updates up to. Inclusive.",
            "            limit: The requested limit for the number of rows to return. The",
            "                function may return more or fewer rows.",
            "",
            "        Returns:",
            "            A tuple consisting of: the updates, a token to use to fetch",
            "            subsequent updates, and whether we returned fewer rows than exists",
            "            between the requested tokens due to the limit.",
            "",
            "            The token returned can be used in a subsequent call to this",
            "            function to get further updates.",
            "",
            "            The updates are a list of 2-tuples of stream ID and the row data",
            "        \"\"\"",
            "",
            "        if last_id == current_id:",
            "            return [], current_id, False",
            "",
            "        changed_rooms: Optional[",
            "            Iterable[str]",
            "        ] = self._typing_stream_change_cache.get_all_entities_changed(last_id)",
            "",
            "        if changed_rooms is None:",
            "            changed_rooms = self._room_serials",
            "",
            "        rows = []",
            "        for room_id in changed_rooms:",
            "            serial = self._room_serials[room_id]",
            "            if last_id < serial <= current_id:",
            "                typing = self._room_typing[room_id]",
            "                rows.append((serial, [room_id, list(typing)]))",
            "        rows.sort()",
            "",
            "        limited = False",
            "        # We, unusually, use a strict limit here as we have all the rows in",
            "        # memory rather than pulling them out of the database with a `LIMIT ?`",
            "        # clause.",
            "        if len(rows) > limit:",
            "            rows = rows[:limit]",
            "            current_id = rows[-1][0]",
            "            limited = True",
            "",
            "        return rows, current_id, limited",
            "",
            "    def process_replication_rows(",
            "        self, token: int, rows: List[TypingStream.TypingStreamRow]",
            "    ) -> None:",
            "        # The writing process should never get updates from replication.",
            "        raise Exception(\"Typing writer instance got typing info over replication\")",
            "",
            "",
            "class TypingNotificationEventSource(EventSource[int, JsonDict]):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._main_store = hs.get_datastores().main",
            "        self.clock = hs.get_clock()",
            "        # We can't call get_typing_handler here because there's a cycle:",
            "        #",
            "        #   Typing -> Notifier -> TypingNotificationEventSource -> Typing",
            "        #",
            "        self.get_typing_handler = hs.get_typing_handler",
            "",
            "    def _make_event_for(self, room_id: str) -> JsonDict:",
            "        typing = self.get_typing_handler()._room_typing[room_id]",
            "        return {",
            "            \"type\": EduTypes.TYPING,",
            "            \"room_id\": room_id,",
            "            \"content\": {\"user_ids\": list(typing)},",
            "        }",
            "",
            "    async def get_new_events_as(",
            "        self, from_key: int, service: ApplicationService",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        \"\"\"Returns a set of new typing events that an appservice",
            "        may be interested in.",
            "",
            "        Args:",
            "            from_key: the stream position at which events should be fetched from.",
            "            service: The appservice which may be interested.",
            "",
            "        Returns:",
            "            A two-tuple containing the following:",
            "                * A list of json dictionaries derived from typing events that the",
            "                  appservice may be interested in.",
            "                * The latest known room serial.",
            "        \"\"\"",
            "        with Measure(self.clock, \"typing.get_new_events_as\"):",
            "            handler = self.get_typing_handler()",
            "",
            "            events = []",
            "",
            "            # Work on a copy of things here as these may change in the handler while",
            "            # waiting for the AS `is_interested_in_room` call to complete.",
            "            # Shallow copy is safe as no nested data is present.",
            "            latest_room_serial = handler._latest_room_serial",
            "            room_serials = handler._room_serials.copy()",
            "",
            "            for room_id, serial in room_serials.items():",
            "                if serial <= from_key:",
            "                    continue",
            "",
            "                if not await service.is_interested_in_room(room_id, self._main_store):",
            "                    continue",
            "",
            "                events.append(self._make_event_for(room_id))",
            "",
            "            return events, latest_room_serial",
            "",
            "    async def get_new_events(",
            "        self,",
            "        user: UserID,",
            "        from_key: int,",
            "        limit: Optional[int],",
            "        room_ids: Iterable[str],",
            "        is_guest: bool,",
            "        explicit_room_id: Optional[str] = None,",
            "    ) -> Tuple[List[JsonDict], int]:",
            "        with Measure(self.clock, \"typing.get_new_events\"):",
            "            from_key = int(from_key)",
            "            handler = self.get_typing_handler()",
            "",
            "            events = []",
            "            for room_id in room_ids:",
            "                if room_id not in handler._room_serials:",
            "                    continue",
            "                if handler._room_serials[room_id] <= from_key:",
            "                    continue",
            "",
            "                events.append(self._make_event_for(room_id))",
            "",
            "            return events, handler._latest_room_serial",
            "",
            "    def get_current_key(self) -> int:",
            "        return self.get_typing_handler()._latest_room_serial"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "343": [
                "TypingWriterHandler"
            ]
        },
        "addLocation": []
    }
}