{
    "ansible_runner/interface.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from ansible_runner import output"
            },
            "1": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from ansible_runner.runner_config import RunnerConfig"
            },
            "2": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from ansible_runner.runner import Runner"
            },
            "3": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from ansible_runner.streaming import StreamWorker"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+from ansible_runner.streaming import StreamController, StreamWorker"
            },
            "5": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from ansible_runner.utils import ("
            },
            "6": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": "     dump_artifacts,"
            },
            "7": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": "     check_isolation_executable_installed,"
            },
            "8": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     event_callback_handler = kwargs.pop('event_handler', None)"
            },
            "10": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     status_callback_handler = kwargs.pop('status_handler', None)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+    artifacts_handler = kwargs.pop('artifacts_handler', None)"
            },
            "12": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "     cancel_callback = kwargs.pop('cancel_callback', None)"
            },
            "13": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    artifacts_callback = kwargs.pop('artifacts_callback', None)  # Currently not expected"
            },
            "14": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "     finished_callback = kwargs.pop('finished_callback', None)"
            },
            "15": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+    control_in = kwargs.pop('control_in', None)"
            },
            "17": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "     control_out = kwargs.pop('control_out', None)"
            },
            "18": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if control_out is not None:"
            },
            "19": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        stream_worker = StreamWorker(control_out)"
            },
            "20": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        status_callback_handler = stream_worker.status_handler"
            },
            "21": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        event_callback_handler = stream_worker.event_handler"
            },
            "22": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        artifacts_callback = stream_worker.artifacts_callback"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+    worker_in = kwargs.pop('worker_in', None)"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+    worker_out = kwargs.pop('worker_out', None)"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    if worker_in is not None and worker_out is not None:"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+        stream_worker = StreamWorker(worker_in, worker_out, **kwargs)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+        return stream_worker"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+    if control_in is not None and control_out is not None:"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        stream_controller = StreamController(control_in, control_out,"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+                                             event_handler=event_callback_handler,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+                                             status_handler=status_callback_handler,"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+                                             artifacts_handler=artifacts_handler,"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+                                             cancel_callback=cancel_callback,"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+                                             finished_callback=finished_callback,"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+                                             **kwargs)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+        return stream_controller"
            },
            "39": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 89,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "     rc = RunnerConfig(**kwargs)"
            },
            "41": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "     rc.prepare()"
            },
            "42": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "43": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "     return Runner(rc,"
            },
            "44": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "                   event_handler=event_callback_handler,"
            },
            "45": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 95,
                "PatchRowcode": "                   status_handler=status_callback_handler,"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+                  artifacts_handler=artifacts_handler,"
            },
            "47": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "                   cancel_callback=cancel_callback,"
            },
            "48": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                  artifacts_callback=artifacts_callback,"
            },
            "49": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "                   finished_callback=finished_callback)"
            },
            "50": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 99,
                "PatchRowcode": " "
            },
            "51": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 100,
                "PatchRowcode": " "
            },
            "52": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "     :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir"
            },
            "53": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "     :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir"
            },
            "54": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "     :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default"
            },
            "55": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    :param control_out: A file-like object used for streaming information back to a control instance of Runner"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+    :param control_in: A file object used for receiving streamed data back from a worker instance of Runner"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+    :param control_out: A file object used for streaming project data to a worker instance of Runner"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+    :param worker_in: A file object used for streaming project data to a worker instance of Runner"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+    :param worker_out: A file object used for streaming information back to a control instance of Runner"
            },
            "60": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "     :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event"
            },
            "61": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 144,
                "PatchRowcode": "     :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)"
            },
            "62": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 145,
                "PatchRowcode": "     :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup."
            },
            "63": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 146,
                "PatchRowcode": "     :param status_handler: An optional callback that will be invoked any time the status changes (e.g...started, running, failed, successful, timeout)"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+    :param artifacts_handler: An optional callback that will be invoked at the end of the run to deal with the artifacts from the run."
            },
            "65": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": 148,
                "PatchRowcode": "     :param process_isolation: Enable process isolation, using either a container engine (e.g. podman) or a sandbox (e.g. bwrap)."
            },
            "66": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": 149,
                "PatchRowcode": "     :param process_isolation_executable: Process isolation executable or container engine used to isolate execution. (default: podman)"
            },
            "67": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 150,
                "PatchRowcode": "     :param process_isolation_path: Path that an isolated playbook run will use for staging. (default: /tmp)"
            },
            "68": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "     :type forks: int"
            },
            "69": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "     :type quiet: bool"
            },
            "70": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "     :type verbosity: int"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+    :type control_in: file"
            },
            "72": {
                "beforePatchRowNumber": 173,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "     :type control_out: file"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 191,
                "PatchRowcode": "+    :type worker_in: file"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+    :type worker_out: file"
            },
            "75": {
                "beforePatchRowNumber": 174,
                "afterPatchRowNumber": 193,
                "PatchRowcode": "     :type event_handler: function"
            },
            "76": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": 194,
                "PatchRowcode": "     :type cancel_callback: function"
            },
            "77": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 195,
                "PatchRowcode": "     :type finished_callback: function"
            },
            "78": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": 196,
                "PatchRowcode": "     :type status_handler: function"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+    :type artifacts_handler: function"
            },
            "80": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 198,
                "PatchRowcode": "     :type process_isolation: bool"
            },
            "81": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 199,
                "PatchRowcode": "     :type process_isolation_executable: str"
            },
            "82": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 200,
                "PatchRowcode": "     :type process_isolation_path: str"
            }
        },
        "frontPatchFile": [
            "# Copyright (c) 2016 Ansible by Red Hat, Inc.",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "import sys",
            "import threading",
            "import logging",
            "",
            "from ansible_runner import output",
            "from ansible_runner.runner_config import RunnerConfig",
            "from ansible_runner.runner import Runner",
            "from ansible_runner.streaming import StreamWorker",
            "from ansible_runner.utils import (",
            "    dump_artifacts,",
            "    check_isolation_executable_installed,",
            ")",
            "",
            "logging.getLogger('ansible-runner').addHandler(logging.NullHandler())",
            "",
            "",
            "def init_runner(**kwargs):",
            "    '''",
            "    Initialize the Runner() instance",
            "",
            "    This function will properly initialize both run() and run_async()",
            "    functions in the same way and return a value instance of Runner.",
            "",
            "    See parameters given to :py:func:`ansible_runner.interface.run`",
            "    '''",
            "    if not kwargs.get('cli_execenv_cmd'):",
            "        dump_artifacts(kwargs)",
            "",
            "    debug = kwargs.pop('debug', None)",
            "    logfile = kwargs.pop('logfile', None)",
            "",
            "    if not kwargs.pop(\"ignore_logging\", True):",
            "        output.configure()",
            "        if debug in (True, False):",
            "            output.set_debug('enable' if debug is True else 'disable')",
            "",
            "        if logfile:",
            "            output.set_logfile(logfile)",
            "",
            "    if kwargs.get(\"process_isolation\", False):",
            "        pi_executable = kwargs.get(\"process_isolation_executable\", \"podman\")",
            "        if not check_isolation_executable_installed(pi_executable):",
            "            print(f'Unable to find process isolation executable: {pi_executable}')",
            "            sys.exit(1)",
            "",
            "    event_callback_handler = kwargs.pop('event_handler', None)",
            "    status_callback_handler = kwargs.pop('status_handler', None)",
            "    cancel_callback = kwargs.pop('cancel_callback', None)",
            "    artifacts_callback = kwargs.pop('artifacts_callback', None)  # Currently not expected",
            "    finished_callback = kwargs.pop('finished_callback', None)",
            "",
            "    control_out = kwargs.pop('control_out', None)",
            "    if control_out is not None:",
            "        stream_worker = StreamWorker(control_out)",
            "        status_callback_handler = stream_worker.status_handler",
            "        event_callback_handler = stream_worker.event_handler",
            "        artifacts_callback = stream_worker.artifacts_callback",
            "",
            "    rc = RunnerConfig(**kwargs)",
            "    rc.prepare()",
            "",
            "    return Runner(rc,",
            "                  event_handler=event_callback_handler,",
            "                  status_handler=status_callback_handler,",
            "                  cancel_callback=cancel_callback,",
            "                  artifacts_callback=artifacts_callback,",
            "                  finished_callback=finished_callback)",
            "",
            "",
            "def run(**kwargs):",
            "    '''",
            "    Run an Ansible Runner task in the foreground and return a Runner object when complete.",
            "",
            "    :param private_data_dir: The directory containing all runner metadata needed to invoke the runner",
            "                             module. Output artifacts will also be stored here for later consumption.",
            "    :param ident: The run identifier for this invocation of Runner. Will be used to create and name",
            "                  the artifact directory holding the results of the invocation.",
            "    :param json_mode: Store event data in place of stdout on the console and in the stdout file",
            "    :param playbook: The playbook (either supplied here as a list or string... or as a path relative to",
            "                     ``private_data_dir/project``) that will be invoked by runner when executing Ansible.",
            "    :param module: The module that will be invoked in ad-hoc mode by runner when executing Ansible.",
            "    :param module_args: The module arguments that will be supplied to ad-hoc mode.",
            "    :param host_pattern: The host pattern to match when running in ad-hoc mode.",
            "    :param inventory: Overridees the inventory directory/file (supplied at ``private_data_dir/inventory``) with",
            "                      a specific host or list of hosts. This can take the form of",
            "      - Path to the inventory file in the ``private_data_dir``",
            "      - Native python dict supporting the YAML/json inventory structure",
            "      - A text INI formatted string",
            "      - A list of inventory sources, or an empty list to disable passing inventory",
            "    :param roles_path: Directory or list of directories to assign to ANSIBLE_ROLES_PATH",
            "    :param envvars: Environment variables to be used when running Ansible. Environment variables will also be",
            "                    read from ``env/envvars`` in ``private_data_dir``",
            "    :param extravars: Extra variables to be passed to Ansible at runtime using ``-e``. Extra vars will also be",
            "                      read from ``env/extravars`` in ``private_data_dir``.",
            "    :param passwords: A dictionary containing password prompt patterns and response values used when processing output from",
            "                      Ansible. Passwords will also be read from ``env/passwords`` in ``private_data_dir``.",
            "    :param settings: A dictionary containing settings values for the ``ansible-runner`` runtime environment. These will also",
            "                     be read from ``env/settings`` in ``private_data_dir``.",
            "    :param ssh_key: The ssh private key passed to ``ssh-agent`` as part of the ansible-playbook run.",
            "    :param cmdline: Command line options passed to Ansible read from ``env/cmdline`` in ``private_data_dir``",
            "    :param limit: Matches ansible's ``--limit`` parameter to further constrain the inventory to be used",
            "    :param forks: Control Ansible parallel concurrency",
            "    :param verbosity: Control how verbose the output of ansible-playbook is",
            "    :param quiet: Disable all output",
            "    :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir",
            "    :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir",
            "    :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default",
            "    :param control_out: A file-like object used for streaming information back to a control instance of Runner",
            "    :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event",
            "    :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)",
            "    :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup.",
            "    :param status_handler: An optional callback that will be invoked any time the status changes (e.g...started, running, failed, successful, timeout)",
            "    :param process_isolation: Enable process isolation, using either a container engine (e.g. podman) or a sandbox (e.g. bwrap).",
            "    :param process_isolation_executable: Process isolation executable or container engine used to isolate execution. (default: podman)",
            "    :param process_isolation_path: Path that an isolated playbook run will use for staging. (default: /tmp)",
            "    :param process_isolation_hide_paths: A path or list of paths on the system that should be hidden from the playbook run.",
            "    :param process_isolation_show_paths: A path or list of paths on the system that should be exposed to the playbook run.",
            "    :param process_isolation_ro_paths: A path or list of paths on the system that should be exposed to the playbook run as read-only.",
            "    :param container_image: Container image to use when running an ansible task (default: quay.io/ansible/ansible-runner:devel)",
            "    :param container_volume_mounts: List of bind mounts in the form 'host_dir:/container_dir. (default: None)",
            "    :param container_options: List of container options to pass to execution engine.",
            "    :param resource_profiling: Enable collection of resource utilization data during playbook execution.",
            "    :param resource_profiling_base_cgroup: Name of existing cgroup which will be sub-grouped in order to measure resource utilization (default: ansible-runner)",
            "    :param resource_profiling_cpu_poll_interval: Interval (in seconds) between CPU polling for determining CPU usage (default: 0.25)",
            "    :param resource_profiling_memory_poll_interval: Interval (in seconds) between memory polling for determining memory usage (default: 0.25)",
            "    :param resource_profiling_pid_poll_interval: Interval (in seconds) between polling PID count for determining number of processes used (default: 0.25)",
            "    :param resource_profiling_results_dir: Directory where profiling data files should be saved (defaults to profiling_data folder inside private data dir)",
            "    :param directory_isolation_base_path: An optional path will be used as the base path to create a temp directory, the project contents will be",
            "                                          copied to this location which will then be used as the working directory during playbook execution.",
            "    :param fact_cache: A string that will be used as the name for the subdirectory of the fact cache in artifacts directory.",
            "                       This is only used for 'jsonfile' type fact caches.",
            "    :param fact_cache_type: A string of the type of fact cache to use.  Defaults to 'jsonfile'.",
            "    :param omit_event_data: Omits extra ansible event data from event payload (stdout and event still included)",
            "    :param only_failed_event_data: Omits extra ansible event data unless it's a failed event (stdout and event still included)",
            "    :param cli_execenv_cmd: Tells Ansible Runner to emulate the CLI of Ansible by prepping an Execution Environment and then passing the user provided cmdline",
            "    :type private_data_dir: str",
            "    :type ident: str",
            "    :type json_mode: bool",
            "    :type playbook: str or filename or list",
            "    :type inventory: str or dict or list",
            "    :type envvars: dict",
            "    :type extravars: dict",
            "    :type passwords: dict",
            "    :type settings: dict",
            "    :type ssh_key: str",
            "    :type artifact_dir: str",
            "    :type project_dir: str",
            "    :type rotate_artifacts: int",
            "    :type cmdline: str",
            "    :type limit: str",
            "    :type forks: int",
            "    :type quiet: bool",
            "    :type verbosity: int",
            "    :type control_out: file",
            "    :type event_handler: function",
            "    :type cancel_callback: function",
            "    :type finished_callback: function",
            "    :type status_handler: function",
            "    :type process_isolation: bool",
            "    :type process_isolation_executable: str",
            "    :type process_isolation_path: str",
            "    :type process_isolation_hide_paths: str or list",
            "    :type process_isolation_show_paths: str or list",
            "    :type process_isolation_ro_paths: str or list",
            "    :type container_image: str",
            "    :type container_volume_mounts: list",
            "    :type container_options: list",
            "    :type resource_profiling: bool",
            "    :type resource_profiling_base_cgroup: str",
            "    :type resource_profiling_cpu_poll_interval: float",
            "    :type resource_profiling_memory_poll_interval: float",
            "    :type resource_profiling_pid_poll_interval: float",
            "    :type resource_profiling_results_dir: str",
            "    :type directory_isolation_base_path: str",
            "    :type fact_cache: str",
            "    :type fact_cache_type: str",
            "    :type omit_event_data: bool",
            "    :type only_failed_event_data: bool",
            "    :type cli_execenv_cmd: str",
            "",
            "    :returns: A :py:class:`ansible_runner.runner.Runner` object, or a simple object containing `rc` if run remotely",
            "    '''",
            "    r = init_runner(**kwargs)",
            "    r.run()",
            "    return r",
            "",
            "",
            "def run_async(**kwargs):",
            "    '''",
            "    Runs an Ansible Runner task in the background which will start immediately. Returns the thread object and a Runner object.",
            "",
            "    This uses the same parameters as :py:func:`ansible_runner.interface.run`",
            "",
            "    :returns: A tuple containing a :py:class:`threading.Thread` object and a :py:class:`ansible_runner.runner.Runner` object",
            "    '''",
            "    r = init_runner(**kwargs)",
            "    runner_thread = threading.Thread(target=r.run)",
            "    runner_thread.start()",
            "    return runner_thread, r"
        ],
        "afterPatchFile": [
            "# Copyright (c) 2016 Ansible by Red Hat, Inc.",
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "#",
            "import sys",
            "import threading",
            "import logging",
            "",
            "from ansible_runner import output",
            "from ansible_runner.runner_config import RunnerConfig",
            "from ansible_runner.runner import Runner",
            "from ansible_runner.streaming import StreamController, StreamWorker",
            "from ansible_runner.utils import (",
            "    dump_artifacts,",
            "    check_isolation_executable_installed,",
            ")",
            "",
            "logging.getLogger('ansible-runner').addHandler(logging.NullHandler())",
            "",
            "",
            "def init_runner(**kwargs):",
            "    '''",
            "    Initialize the Runner() instance",
            "",
            "    This function will properly initialize both run() and run_async()",
            "    functions in the same way and return a value instance of Runner.",
            "",
            "    See parameters given to :py:func:`ansible_runner.interface.run`",
            "    '''",
            "    if not kwargs.get('cli_execenv_cmd'):",
            "        dump_artifacts(kwargs)",
            "",
            "    debug = kwargs.pop('debug', None)",
            "    logfile = kwargs.pop('logfile', None)",
            "",
            "    if not kwargs.pop(\"ignore_logging\", True):",
            "        output.configure()",
            "        if debug in (True, False):",
            "            output.set_debug('enable' if debug is True else 'disable')",
            "",
            "        if logfile:",
            "            output.set_logfile(logfile)",
            "",
            "    if kwargs.get(\"process_isolation\", False):",
            "        pi_executable = kwargs.get(\"process_isolation_executable\", \"podman\")",
            "        if not check_isolation_executable_installed(pi_executable):",
            "            print(f'Unable to find process isolation executable: {pi_executable}')",
            "            sys.exit(1)",
            "",
            "    event_callback_handler = kwargs.pop('event_handler', None)",
            "    status_callback_handler = kwargs.pop('status_handler', None)",
            "    artifacts_handler = kwargs.pop('artifacts_handler', None)",
            "    cancel_callback = kwargs.pop('cancel_callback', None)",
            "    finished_callback = kwargs.pop('finished_callback', None)",
            "",
            "    control_in = kwargs.pop('control_in', None)",
            "    control_out = kwargs.pop('control_out', None)",
            "    worker_in = kwargs.pop('worker_in', None)",
            "    worker_out = kwargs.pop('worker_out', None)",
            "",
            "    if worker_in is not None and worker_out is not None:",
            "        stream_worker = StreamWorker(worker_in, worker_out, **kwargs)",
            "        return stream_worker",
            "",
            "    if control_in is not None and control_out is not None:",
            "        stream_controller = StreamController(control_in, control_out,",
            "                                             event_handler=event_callback_handler,",
            "                                             status_handler=status_callback_handler,",
            "                                             artifacts_handler=artifacts_handler,",
            "                                             cancel_callback=cancel_callback,",
            "                                             finished_callback=finished_callback,",
            "                                             **kwargs)",
            "        return stream_controller",
            "",
            "    rc = RunnerConfig(**kwargs)",
            "    rc.prepare()",
            "",
            "    return Runner(rc,",
            "                  event_handler=event_callback_handler,",
            "                  status_handler=status_callback_handler,",
            "                  artifacts_handler=artifacts_handler,",
            "                  cancel_callback=cancel_callback,",
            "                  finished_callback=finished_callback)",
            "",
            "",
            "def run(**kwargs):",
            "    '''",
            "    Run an Ansible Runner task in the foreground and return a Runner object when complete.",
            "",
            "    :param private_data_dir: The directory containing all runner metadata needed to invoke the runner",
            "                             module. Output artifacts will also be stored here for later consumption.",
            "    :param ident: The run identifier for this invocation of Runner. Will be used to create and name",
            "                  the artifact directory holding the results of the invocation.",
            "    :param json_mode: Store event data in place of stdout on the console and in the stdout file",
            "    :param playbook: The playbook (either supplied here as a list or string... or as a path relative to",
            "                     ``private_data_dir/project``) that will be invoked by runner when executing Ansible.",
            "    :param module: The module that will be invoked in ad-hoc mode by runner when executing Ansible.",
            "    :param module_args: The module arguments that will be supplied to ad-hoc mode.",
            "    :param host_pattern: The host pattern to match when running in ad-hoc mode.",
            "    :param inventory: Overridees the inventory directory/file (supplied at ``private_data_dir/inventory``) with",
            "                      a specific host or list of hosts. This can take the form of",
            "      - Path to the inventory file in the ``private_data_dir``",
            "      - Native python dict supporting the YAML/json inventory structure",
            "      - A text INI formatted string",
            "      - A list of inventory sources, or an empty list to disable passing inventory",
            "    :param roles_path: Directory or list of directories to assign to ANSIBLE_ROLES_PATH",
            "    :param envvars: Environment variables to be used when running Ansible. Environment variables will also be",
            "                    read from ``env/envvars`` in ``private_data_dir``",
            "    :param extravars: Extra variables to be passed to Ansible at runtime using ``-e``. Extra vars will also be",
            "                      read from ``env/extravars`` in ``private_data_dir``.",
            "    :param passwords: A dictionary containing password prompt patterns and response values used when processing output from",
            "                      Ansible. Passwords will also be read from ``env/passwords`` in ``private_data_dir``.",
            "    :param settings: A dictionary containing settings values for the ``ansible-runner`` runtime environment. These will also",
            "                     be read from ``env/settings`` in ``private_data_dir``.",
            "    :param ssh_key: The ssh private key passed to ``ssh-agent`` as part of the ansible-playbook run.",
            "    :param cmdline: Command line options passed to Ansible read from ``env/cmdline`` in ``private_data_dir``",
            "    :param limit: Matches ansible's ``--limit`` parameter to further constrain the inventory to be used",
            "    :param forks: Control Ansible parallel concurrency",
            "    :param verbosity: Control how verbose the output of ansible-playbook is",
            "    :param quiet: Disable all output",
            "    :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir",
            "    :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir",
            "    :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default",
            "    :param control_in: A file object used for receiving streamed data back from a worker instance of Runner",
            "    :param control_out: A file object used for streaming project data to a worker instance of Runner",
            "    :param worker_in: A file object used for streaming project data to a worker instance of Runner",
            "    :param worker_out: A file object used for streaming information back to a control instance of Runner",
            "    :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event",
            "    :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)",
            "    :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup.",
            "    :param status_handler: An optional callback that will be invoked any time the status changes (e.g...started, running, failed, successful, timeout)",
            "    :param artifacts_handler: An optional callback that will be invoked at the end of the run to deal with the artifacts from the run.",
            "    :param process_isolation: Enable process isolation, using either a container engine (e.g. podman) or a sandbox (e.g. bwrap).",
            "    :param process_isolation_executable: Process isolation executable or container engine used to isolate execution. (default: podman)",
            "    :param process_isolation_path: Path that an isolated playbook run will use for staging. (default: /tmp)",
            "    :param process_isolation_hide_paths: A path or list of paths on the system that should be hidden from the playbook run.",
            "    :param process_isolation_show_paths: A path or list of paths on the system that should be exposed to the playbook run.",
            "    :param process_isolation_ro_paths: A path or list of paths on the system that should be exposed to the playbook run as read-only.",
            "    :param container_image: Container image to use when running an ansible task (default: quay.io/ansible/ansible-runner:devel)",
            "    :param container_volume_mounts: List of bind mounts in the form 'host_dir:/container_dir. (default: None)",
            "    :param container_options: List of container options to pass to execution engine.",
            "    :param resource_profiling: Enable collection of resource utilization data during playbook execution.",
            "    :param resource_profiling_base_cgroup: Name of existing cgroup which will be sub-grouped in order to measure resource utilization (default: ansible-runner)",
            "    :param resource_profiling_cpu_poll_interval: Interval (in seconds) between CPU polling for determining CPU usage (default: 0.25)",
            "    :param resource_profiling_memory_poll_interval: Interval (in seconds) between memory polling for determining memory usage (default: 0.25)",
            "    :param resource_profiling_pid_poll_interval: Interval (in seconds) between polling PID count for determining number of processes used (default: 0.25)",
            "    :param resource_profiling_results_dir: Directory where profiling data files should be saved (defaults to profiling_data folder inside private data dir)",
            "    :param directory_isolation_base_path: An optional path will be used as the base path to create a temp directory, the project contents will be",
            "                                          copied to this location which will then be used as the working directory during playbook execution.",
            "    :param fact_cache: A string that will be used as the name for the subdirectory of the fact cache in artifacts directory.",
            "                       This is only used for 'jsonfile' type fact caches.",
            "    :param fact_cache_type: A string of the type of fact cache to use.  Defaults to 'jsonfile'.",
            "    :param omit_event_data: Omits extra ansible event data from event payload (stdout and event still included)",
            "    :param only_failed_event_data: Omits extra ansible event data unless it's a failed event (stdout and event still included)",
            "    :param cli_execenv_cmd: Tells Ansible Runner to emulate the CLI of Ansible by prepping an Execution Environment and then passing the user provided cmdline",
            "    :type private_data_dir: str",
            "    :type ident: str",
            "    :type json_mode: bool",
            "    :type playbook: str or filename or list",
            "    :type inventory: str or dict or list",
            "    :type envvars: dict",
            "    :type extravars: dict",
            "    :type passwords: dict",
            "    :type settings: dict",
            "    :type ssh_key: str",
            "    :type artifact_dir: str",
            "    :type project_dir: str",
            "    :type rotate_artifacts: int",
            "    :type cmdline: str",
            "    :type limit: str",
            "    :type forks: int",
            "    :type quiet: bool",
            "    :type verbosity: int",
            "    :type control_in: file",
            "    :type control_out: file",
            "    :type worker_in: file",
            "    :type worker_out: file",
            "    :type event_handler: function",
            "    :type cancel_callback: function",
            "    :type finished_callback: function",
            "    :type status_handler: function",
            "    :type artifacts_handler: function",
            "    :type process_isolation: bool",
            "    :type process_isolation_executable: str",
            "    :type process_isolation_path: str",
            "    :type process_isolation_hide_paths: str or list",
            "    :type process_isolation_show_paths: str or list",
            "    :type process_isolation_ro_paths: str or list",
            "    :type container_image: str",
            "    :type container_volume_mounts: list",
            "    :type container_options: list",
            "    :type resource_profiling: bool",
            "    :type resource_profiling_base_cgroup: str",
            "    :type resource_profiling_cpu_poll_interval: float",
            "    :type resource_profiling_memory_poll_interval: float",
            "    :type resource_profiling_pid_poll_interval: float",
            "    :type resource_profiling_results_dir: str",
            "    :type directory_isolation_base_path: str",
            "    :type fact_cache: str",
            "    :type fact_cache_type: str",
            "    :type omit_event_data: bool",
            "    :type only_failed_event_data: bool",
            "    :type cli_execenv_cmd: str",
            "",
            "    :returns: A :py:class:`ansible_runner.runner.Runner` object, or a simple object containing `rc` if run remotely",
            "    '''",
            "    r = init_runner(**kwargs)",
            "    r.run()",
            "    return r",
            "",
            "",
            "def run_async(**kwargs):",
            "    '''",
            "    Runs an Ansible Runner task in the background which will start immediately. Returns the thread object and a Runner object.",
            "",
            "    This uses the same parameters as :py:func:`ansible_runner.interface.run`",
            "",
            "    :returns: A tuple containing a :py:class:`threading.Thread` object and a :py:class:`ansible_runner.runner.Runner` object",
            "    '''",
            "    r = init_runner(**kwargs)",
            "    runner_thread = threading.Thread(target=r.run)",
            "    runner_thread.start()",
            "    return runner_thread, r"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "27": [],
            "68": [
                "init_runner"
            ],
            "72": [
                "init_runner"
            ],
            "73": [
                "init_runner"
            ],
            "74": [
                "init_runner"
            ],
            "75": [
                "init_runner"
            ],
            "76": [
                "init_runner"
            ],
            "85": [
                "init_runner"
            ],
            "127": [
                "run"
            ]
        },
        "addLocation": []
    },
    "ansible_runner/runner.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " class Runner(object):"
            },
            "1": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": "     def __init__(self, config, cancel_callback=None, remove_partials=True, event_handler=None,"
            },
            "3": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                 artifacts_callback=None, finished_callback=None, status_handler=None):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+                 artifacts_handler=None, finished_callback=None, status_handler=None):"
            },
            "5": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": "         self.config = config"
            },
            "6": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "         self.cancel_callback = cancel_callback"
            },
            "7": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "         self.event_handler = event_handler"
            },
            "8": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.artifacts_callback = artifacts_callback"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+        self.artifacts_handler = artifacts_handler"
            },
            "10": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "         self.finished_callback = finished_callback"
            },
            "11": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "         self.status_handler = status_handler"
            },
            "12": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "         self.canceled = False"
            },
            "13": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 284,
                "PatchRowcode": "                 logger.error('Failed to delete cgroup: {}'.format(stderr))"
            },
            "14": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": 285,
                "PatchRowcode": "                 raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))"
            },
            "15": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 286,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if self.artifacts_callback is not None:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 287,
                "PatchRowcode": "+        if self.artifacts_handler is not None:"
            },
            "18": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 288,
                "PatchRowcode": "             try:"
            },
            "19": {
                "beforePatchRowNumber": 289,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.artifacts_callback(self.config.artifact_dir)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 289,
                "PatchRowcode": "+                self.artifacts_handler(self.config.artifact_dir)"
            },
            "21": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": 290,
                "PatchRowcode": "             except Exception as e:"
            },
            "22": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": 291,
                "PatchRowcode": "                 raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))"
            },
            "23": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 292,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import os",
            "import stat",
            "import time",
            "import json",
            "import errno",
            "import signal",
            "from subprocess import Popen, PIPE",
            "import shutil",
            "import codecs",
            "import collections",
            "import datetime",
            "import logging",
            "",
            "import six",
            "import pexpect",
            "import psutil",
            "",
            "import ansible_runner.plugins",
            "",
            "from .utils import OutputEventFilter, cleanup_artifact_dir, ensure_str, collect_new_events",
            "from .exceptions import CallbackError, AnsibleRunnerException",
            "from ansible_runner.output import debug",
            "",
            "logger = logging.getLogger('ansible-runner')",
            "",
            "",
            "class Runner(object):",
            "",
            "    def __init__(self, config, cancel_callback=None, remove_partials=True, event_handler=None,",
            "                 artifacts_callback=None, finished_callback=None, status_handler=None):",
            "        self.config = config",
            "        self.cancel_callback = cancel_callback",
            "        self.event_handler = event_handler",
            "        self.artifacts_callback = artifacts_callback",
            "        self.finished_callback = finished_callback",
            "        self.status_handler = status_handler",
            "        self.canceled = False",
            "        self.timed_out = False",
            "        self.errored = False",
            "        self.status = \"unstarted\"",
            "        self.rc = None",
            "        self.remove_partials = remove_partials",
            "",
            "    def event_callback(self, event_data):",
            "        '''",
            "        Invoked for every Ansible event to collect stdout with the event data and store it for",
            "        later use",
            "        '''",
            "        self.last_stdout_update = time.time()",
            "        if 'uuid' in event_data:",
            "            filename = '{}-partial.json'.format(event_data['uuid'])",
            "            partial_filename = os.path.join(self.config.artifact_dir,",
            "                                            'job_events',",
            "                                            filename)",
            "            full_filename = os.path.join(self.config.artifact_dir,",
            "                                         'job_events',",
            "                                         '{}-{}.json'.format(event_data['counter'],",
            "                                                             event_data['uuid']))",
            "            try:",
            "                event_data.update(dict(runner_ident=str(self.config.ident)))",
            "                try:",
            "                    with codecs.open(partial_filename, 'r', encoding='utf-8') as read_file:",
            "                        partial_event_data = json.load(read_file)",
            "                    event_data.update(partial_event_data)",
            "                    if self.remove_partials:",
            "                        os.remove(partial_filename)",
            "                except IOError:",
            "                    debug(\"Failed to open ansible stdout callback plugin partial data file {}\".format(partial_filename))",
            "                if self.event_handler is not None:",
            "                    should_write = self.event_handler(event_data)",
            "                else:",
            "                    should_write = True",
            "                for plugin in ansible_runner.plugins:",
            "                    ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
            "                if should_write:",
            "                    with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
            "                        os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
            "                        json.dump(event_data, write_file)",
            "            except IOError as e:",
            "                debug(\"Failed writing event data: {}\".format(e))",
            "",
            "    def status_callback(self, status):",
            "        self.status = status",
            "        status_data = dict(status=status, runner_ident=str(self.config.ident))",
            "        for plugin in ansible_runner.plugins:",
            "            ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
            "        if self.status_handler is not None:",
            "            self.status_handler(status_data, runner_config=self.config)",
            "",
            "    def run(self):",
            "        '''",
            "        Launch the Ansible task configured in self.config (A RunnerConfig object), returns once the",
            "        invocation is complete",
            "        '''",
            "        self.status_callback('starting')",
            "        stdout_filename = os.path.join(self.config.artifact_dir, 'stdout')",
            "        command_filename = os.path.join(self.config.artifact_dir, 'command')",
            "",
            "        try:",
            "            os.makedirs(self.config.artifact_dir, mode=0o700)",
            "        except OSError as exc:",
            "            if exc.errno == errno.EEXIST and os.path.isdir(self.config.artifact_dir):",
            "                pass",
            "            else:",
            "                raise",
            "        os.close(os.open(stdout_filename, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))",
            "",
            "        job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
            "        if not os.path.exists(job_events_path):",
            "            os.mkdir(job_events_path, 0o700)",
            "",
            "        command = self.config.command",
            "        with codecs.open(command_filename, 'w', encoding='utf-8') as f:",
            "            os.chmod(command_filename, stat.S_IRUSR | stat.S_IWUSR)",
            "            json.dump(",
            "                {'command': command,",
            "                 'cwd': self.config.cwd,",
            "                 'env': self.config.env}, f, ensure_ascii=False",
            "            )",
            "",
            "        if self.config.ident is not None:",
            "            cleanup_artifact_dir(os.path.join(self.config.artifact_dir, \"..\"), self.config.rotate_artifacts)",
            "",
            "        stdout_handle = codecs.open(stdout_filename, 'w', encoding='utf-8')",
            "        stdout_handle = OutputEventFilter(stdout_handle, self.event_callback, self.config.suppress_ansible_output, output_json=self.config.json_mode)",
            "",
            "        if not isinstance(self.config.expect_passwords, collections.OrderedDict):",
            "            # We iterate over `expect_passwords.keys()` and",
            "            # `expect_passwords.values()` separately to map matched inputs to",
            "            # patterns and choose the proper string to send to the subprocess;",
            "            # enforce usage of an OrderedDict so that the ordering of elements in",
            "            # `keys()` matches `values()`.",
            "            expect_passwords = collections.OrderedDict(self.config.expect_passwords)",
            "        password_patterns = list(expect_passwords.keys())",
            "        password_values = list(expect_passwords.values())",
            "",
            "        # pexpect needs all env vars to be utf-8 encoded bytes",
            "        # https://github.com/pexpect/pexpect/issues/512",
            "",
            "        # Use a copy so as not to cause problems when serializing the job_env.",
            "        if self.config.containerized:",
            "            # If this is containerized, the shell environment calling podman has little",
            "            # to do with the actual job environment, but still needs PATH, auth, etc.",
            "            pexpect_env = os.environ.copy()",
            "            # But we still rely on env vars to pass secrets",
            "            pexpect_env.update(self.config.env)",
            "            # Write the keys to pass into container to expected file in artifacts dir",
            "            # option expecting should have already been written in ansible_runner.runner_config",
            "            env_file_host = os.path.join(self.config.artifact_dir, 'env.list')",
            "            with open(env_file_host, 'w') as f:",
            "                f.write('\\n'.join(list(self.config.env.keys())))",
            "        else:",
            "            pexpect_env = self.config.env",
            "        env = {",
            "            ensure_str(k): ensure_str(v) if k != 'PATH' and isinstance(v, six.text_type) else v",
            "            for k, v in pexpect_env.items()",
            "        }",
            "",
            "        # Prepare to collect performance data",
            "        if self.config.resource_profiling:",
            "            cgroup_path = '{0}/{1}'.format(self.config.resource_profiling_base_cgroup, self.config.ident)",
            "",
            "            import getpass",
            "            import grp",
            "            user = getpass.getuser()",
            "            group = grp.getgrgid(os.getgid()).gr_name",
            "",
            "            cmd = 'cgcreate -a {user}:{group} -t {user}:{group} -g cpuacct,memory,pids:{}'.format(cgroup_path, user=user, group=group)",
            "            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)",
            "            _, stderr = proc.communicate()",
            "            if proc.returncode:",
            "                # Unable to create cgroup",
            "                logger.error('Unable to create cgroup: {}'.format(stderr))",
            "                raise RuntimeError('Unable to create cgroup: {}'.format(stderr))",
            "            else:",
            "                logger.info(\"Created cgroup '{}'\".format(cgroup_path))",
            "",
            "",
            "        self.status_callback('running')",
            "        self.last_stdout_update = time.time()",
            "        try:",
            "            child = pexpect.spawn(",
            "                command[0],",
            "                command[1:],",
            "                cwd=self.config.cwd,",
            "                env=env,",
            "                ignore_sighup=True,",
            "                encoding='utf-8',",
            "                echo=False,",
            "                use_poll=self.config.pexpect_use_poll,",
            "            )",
            "            child.logfile_read = stdout_handle",
            "        except pexpect.exceptions.ExceptionPexpect as e:",
            "            child = collections.namedtuple(",
            "                'MissingProcess', 'exitstatus isalive close'",
            "            )(",
            "                exitstatus=127,",
            "                isalive=lambda: False,",
            "                close=lambda: None,",
            "            )",
            "",
            "            def _decode(x):",
            "                return x.decode('utf-8') if six.PY2 else x",
            "",
            "            # create the events directory (the callback plugin won't run, so it",
            "            # won't get created)",
            "            events_directory = os.path.join(self.config.artifact_dir, 'job_events')",
            "            if not os.path.exists(events_directory):",
            "                os.mkdir(events_directory, 0o700)",
            "            stdout_handle.write(_decode(str(e)))",
            "            stdout_handle.write(_decode('\\n'))",
            "",
            "        job_start = time.time()",
            "        while child.isalive():",
            "            result_id = child.expect(password_patterns,",
            "                                     timeout=self.config.pexpect_timeout,",
            "                                     searchwindowsize=100)",
            "            password = password_values[result_id]",
            "            if password is not None:",
            "                child.sendline(password)",
            "                self.last_stdout_update = time.time()",
            "            if self.cancel_callback:",
            "                try:",
            "                    self.canceled = self.cancel_callback()",
            "                except Exception as e:",
            "                    # TODO: logger.exception('Could not check cancel callback - cancelling immediately')",
            "                    #if isinstance(extra_update_fields, dict):",
            "                    #    extra_update_fields['job_explanation'] = \"System error during job execution, check system logs\"",
            "                    raise CallbackError(\"Exception in Cancel Callback: {}\".format(e))",
            "            if self.config.job_timeout and not self.canceled and (time.time() - job_start) > self.config.job_timeout:",
            "                self.timed_out = True",
            "                # if isinstance(extra_update_fields, dict):",
            "                #     extra_update_fields['job_explanation'] = \"Job terminated due to timeout\"",
            "            if self.canceled or self.timed_out or self.errored:",
            "                self.kill_container()",
            "                Runner.handle_termination(child.pid, is_cancel=self.canceled)",
            "            if self.config.idle_timeout and (time.time() - self.last_stdout_update) > self.config.idle_timeout:",
            "                self.kill_container()",
            "                Runner.handle_termination(child.pid, is_cancel=False)",
            "                self.timed_out = True",
            "",
            "        stdout_handle.flush()",
            "        stdout_handle.close()",
            "        child.close()",
            "",
            "        if self.canceled:",
            "            self.status_callback('canceled')",
            "        elif child.exitstatus == 0 and not self.timed_out:",
            "            self.status_callback('successful')",
            "        elif self.timed_out:",
            "            self.status_callback('timeout')",
            "        else:",
            "            self.status_callback('failed')",
            "        self.rc = child.exitstatus if not (self.timed_out or self.canceled) else 254",
            "        for filename, data in [",
            "            ('status', self.status),",
            "            ('rc', self.rc),",
            "        ]:",
            "            artifact_path = os.path.join(self.config.artifact_dir, filename)",
            "            if not os.path.exists(artifact_path):",
            "                os.close(os.open(artifact_path, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))",
            "            with open(artifact_path, 'w') as f:",
            "                f.write(str(data))",
            "        if self.config.directory_isolation_path and self.config.directory_isolation_cleanup:",
            "            shutil.rmtree(self.config.directory_isolation_path)",
            "        if self.config.process_isolation and self.config.process_isolation_path_actual:",
            "            def _delete(retries=15):",
            "                try:",
            "                    shutil.rmtree(self.config.process_isolation_path_actual)",
            "                except OSError as e:",
            "                    res = False",
            "                    if e.errno == 16 and retries > 0:",
            "                        time.sleep(1)",
            "                        res = _delete(retries=retries - 1)",
            "                    if not res:",
            "                        raise",
            "                return True",
            "            _delete()",
            "        if self.config.resource_profiling:",
            "            cmd = 'cgdelete -g cpuacct,memory,pids:{}'.format(cgroup_path)",
            "            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)",
            "            _, stderr = proc.communicate()",
            "            if proc.returncode:",
            "                logger.error('Failed to delete cgroup: {}'.format(stderr))",
            "                raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))",
            "",
            "        if self.artifacts_callback is not None:",
            "            try:",
            "                self.artifacts_callback(self.config.artifact_dir)",
            "            except Exception as e:",
            "                raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))",
            "",
            "        if self.finished_callback is not None:",
            "            try:",
            "                self.finished_callback(self)",
            "            except Exception as e:",
            "                raise CallbackError(\"Exception in Finished Callback: {}\".format(e))",
            "        return self.status, self.rc",
            "",
            "    @property",
            "    def stdout(self):",
            "        '''",
            "        Returns an open file handle to the stdout representing the Ansible run",
            "        '''",
            "        stdout_path = os.path.join(self.config.artifact_dir, 'stdout')",
            "        if not os.path.exists(stdout_path):",
            "            raise AnsibleRunnerException(\"stdout missing\")",
            "        return open(os.path.join(self.config.artifact_dir, 'stdout'), 'r')",
            "",
            "    @property",
            "    def events(self):",
            "        '''",
            "        A generator that will return all ansible job events in the order that they were emitted from Ansible",
            "",
            "        Example:",
            "",
            "            {",
            "               \"event\":\"runner_on_ok\",",
            "               \"uuid\":\"00a50d9c-161a-4b74-b978-9f60becaf209\",",
            "               \"stdout\":\"ok: [localhost] => {\\\\r\\\\n    \\\\\"   msg\\\\\":\\\\\"Test!\\\\\"\\\\r\\\\n}\",",
            "               \"counter\":6,",
            "               \"pid\":740,",
            "               \"created\":\"2018-04-05T18:24:36.096725\",",
            "               \"end_line\":10,",
            "               \"start_line\":7,",
            "               \"event_data\":{",
            "                  \"play_pattern\":\"all\",",
            "                  \"play\":\"all\",",
            "                  \"task\":\"debug\",",
            "                  \"task_args\":\"msg=Test!\",",
            "                  \"remote_addr\":\"localhost\",",
            "                  \"res\":{",
            "                     \"msg\":\"Test!\",",
            "                     \"changed\":false,",
            "                     \"_ansible_verbose_always\":true,",
            "                     \"_ansible_no_log\":false",
            "                  },",
            "                  \"pid\":740,",
            "                  \"play_uuid\":\"0242ac11-0002-443b-cdb1-000000000006\",",
            "                  \"task_uuid\":\"0242ac11-0002-443b-cdb1-000000000008\",",
            "                  \"event_loop\":null,",
            "                  \"playbook_uuid\":\"634edeee-3228-4c17-a1b4-f010fdd42eb2\",",
            "                  \"playbook\":\"test.yml\",",
            "                  \"task_action\":\"debug\",",
            "                  \"host\":\"localhost\",",
            "                  \"task_path\":\"/tmp/demo/project/test.yml:3\"",
            "               }",
            "           }",
            "        '''",
            "        # collection of all the events that were yielded",
            "        old_events = {}",
            "        event_path = os.path.join(self.config.artifact_dir, 'job_events')",
            "",
            "        # Wait for events dir to be created",
            "        now = datetime.datetime.now()",
            "        while not os.path.exists(event_path):",
            "            time.sleep(0.05)",
            "            wait_time = datetime.datetime.now() - now",
            "            if wait_time.total_seconds() > 60:",
            "                raise AnsibleRunnerException(\"events directory is missing: %s\" % event_path)",
            "",
            "        while self.status == \"running\":",
            "            for event, old_evnts in collect_new_events(event_path, old_events):",
            "                old_events = old_evnts",
            "                yield event",
            "",
            "        # collect new events that were written after the playbook has finished",
            "        for event, old_evnts in collect_new_events(event_path, old_events):",
            "            old_events = old_evnts",
            "            yield event",
            "",
            "    @property",
            "    def stats(self):",
            "        '''",
            "        Returns the final high level stats from the Ansible run",
            "",
            "        Example:",
            "            {'dark': {}, 'failures': {}, 'skipped': {}, 'ok': {u'localhost': 2}, 'processed': {u'localhost': 1}}",
            "        '''",
            "        last_event = list(filter(lambda x: 'event' in x and x['event'] == 'playbook_on_stats',",
            "                                 self.events))",
            "        if not last_event:",
            "            return None",
            "        last_event = last_event[0]['event_data']",
            "        return dict(skipped=last_event.get('skipped',{}),",
            "                    ok=last_event.get('ok',{}),",
            "                    dark=last_event.get('dark',{}),",
            "                    failures=last_event.get('failures',{}),",
            "                    ignored=last_event.get('ignored', {}),",
            "                    rescued=last_event.get('rescued', {}),",
            "                    processed=last_event.get('processed',{}),",
            "                    changed=last_event.get('changed',{}))",
            "",
            "",
            "    def host_events(self, host):",
            "        '''",
            "        Given a host name, this will return all task events executed on that host",
            "        '''",
            "        all_host_events = filter(lambda x: 'event_data' in x and 'host' in x['event_data'] and x['event_data']['host'] == host,",
            "                                 self.events)",
            "        return all_host_events",
            "",
            "    def kill_container(self):",
            "        '''",
            "        Internal method to terminate a container being used for job isolation",
            "        '''",
            "        container_name = self.config.container_name",
            "        if container_name:",
            "            container_cli = self.config.process_isolation_executable",
            "            cmd = '{} kill {}'.format(container_cli, container_name)",
            "            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)",
            "            _, stderr = proc.communicate()",
            "            if proc.returncode:",
            "                logger.info('Error from {} kill {} command:\\n{}'.format(container_cli, container_name, stderr))",
            "            else:",
            "                logger.info(\"Killed container {}\".format(container_name))",
            "",
            "    @classmethod",
            "    def handle_termination(cls, pid, pidfile=None, is_cancel=True):",
            "        '''",
            "        Internal method to terminate a subprocess spawned by `pexpect` representing an invocation of runner.",
            "",
            "        :param pid:       the process id of the running the job.",
            "        :param pidfile:   the daemon's PID file",
            "        :param is_cancel: flag showing whether this termination is caused by",
            "                          instance's cancel_flag.",
            "        '''",
            "",
            "        try:",
            "            main_proc = psutil.Process(pid=pid)",
            "            child_procs = main_proc.children(recursive=True)",
            "            for child_proc in child_procs:",
            "                try:",
            "                    os.kill(child_proc.pid, signal.SIGKILL)",
            "                except (TypeError, OSError):",
            "                    pass",
            "            os.kill(main_proc.pid, signal.SIGKILL)",
            "            try:",
            "                os.remove(pidfile)",
            "            except (OSError):",
            "                pass",
            "        except (TypeError, psutil.Error, OSError):",
            "            try:",
            "                os.kill(pid, signal.SIGKILL)",
            "            except (OSError):",
            "                pass",
            "",
            "    def get_fact_cache(self, host):",
            "        '''",
            "        Get the entire fact cache only if the fact_cache_type is 'jsonfile'",
            "        '''",
            "        if self.config.fact_cache_type != 'jsonfile':",
            "            raise Exception('Unsupported fact cache type.  Only \"jsonfile\" is supported for reading and writing facts from ansible-runner')",
            "        fact_cache = os.path.join(self.config.fact_cache, host)",
            "        if os.path.exists(fact_cache):",
            "            with open(fact_cache) as f:",
            "                return json.loads(f.read())",
            "        return {}",
            "",
            "    def set_fact_cache(self, host, data):",
            "        '''",
            "        Set the entire fact cache data only if the fact_cache_type is 'jsonfile'",
            "        '''",
            "        if self.config.fact_cache_type != 'jsonfile':",
            "            raise Exception('Unsupported fact cache type.  Only \"jsonfile\" is supported for reading and writing facts from ansible-runner')",
            "        fact_cache = os.path.join(self.config.fact_cache, host)",
            "        if not os.path.exists(os.path.dirname(fact_cache)):",
            "            os.makedirs(os.path.dirname(fact_cache), mode=0o700)",
            "        with open(fact_cache, 'w') as f:",
            "            return f.write(json.dumps(data))"
        ],
        "afterPatchFile": [
            "import os",
            "import stat",
            "import time",
            "import json",
            "import errno",
            "import signal",
            "from subprocess import Popen, PIPE",
            "import shutil",
            "import codecs",
            "import collections",
            "import datetime",
            "import logging",
            "",
            "import six",
            "import pexpect",
            "import psutil",
            "",
            "import ansible_runner.plugins",
            "",
            "from .utils import OutputEventFilter, cleanup_artifact_dir, ensure_str, collect_new_events",
            "from .exceptions import CallbackError, AnsibleRunnerException",
            "from ansible_runner.output import debug",
            "",
            "logger = logging.getLogger('ansible-runner')",
            "",
            "",
            "class Runner(object):",
            "",
            "    def __init__(self, config, cancel_callback=None, remove_partials=True, event_handler=None,",
            "                 artifacts_handler=None, finished_callback=None, status_handler=None):",
            "        self.config = config",
            "        self.cancel_callback = cancel_callback",
            "        self.event_handler = event_handler",
            "        self.artifacts_handler = artifacts_handler",
            "        self.finished_callback = finished_callback",
            "        self.status_handler = status_handler",
            "        self.canceled = False",
            "        self.timed_out = False",
            "        self.errored = False",
            "        self.status = \"unstarted\"",
            "        self.rc = None",
            "        self.remove_partials = remove_partials",
            "",
            "    def event_callback(self, event_data):",
            "        '''",
            "        Invoked for every Ansible event to collect stdout with the event data and store it for",
            "        later use",
            "        '''",
            "        self.last_stdout_update = time.time()",
            "        if 'uuid' in event_data:",
            "            filename = '{}-partial.json'.format(event_data['uuid'])",
            "            partial_filename = os.path.join(self.config.artifact_dir,",
            "                                            'job_events',",
            "                                            filename)",
            "            full_filename = os.path.join(self.config.artifact_dir,",
            "                                         'job_events',",
            "                                         '{}-{}.json'.format(event_data['counter'],",
            "                                                             event_data['uuid']))",
            "            try:",
            "                event_data.update(dict(runner_ident=str(self.config.ident)))",
            "                try:",
            "                    with codecs.open(partial_filename, 'r', encoding='utf-8') as read_file:",
            "                        partial_event_data = json.load(read_file)",
            "                    event_data.update(partial_event_data)",
            "                    if self.remove_partials:",
            "                        os.remove(partial_filename)",
            "                except IOError:",
            "                    debug(\"Failed to open ansible stdout callback plugin partial data file {}\".format(partial_filename))",
            "                if self.event_handler is not None:",
            "                    should_write = self.event_handler(event_data)",
            "                else:",
            "                    should_write = True",
            "                for plugin in ansible_runner.plugins:",
            "                    ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
            "                if should_write:",
            "                    with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
            "                        os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
            "                        json.dump(event_data, write_file)",
            "            except IOError as e:",
            "                debug(\"Failed writing event data: {}\".format(e))",
            "",
            "    def status_callback(self, status):",
            "        self.status = status",
            "        status_data = dict(status=status, runner_ident=str(self.config.ident))",
            "        for plugin in ansible_runner.plugins:",
            "            ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
            "        if self.status_handler is not None:",
            "            self.status_handler(status_data, runner_config=self.config)",
            "",
            "    def run(self):",
            "        '''",
            "        Launch the Ansible task configured in self.config (A RunnerConfig object), returns once the",
            "        invocation is complete",
            "        '''",
            "        self.status_callback('starting')",
            "        stdout_filename = os.path.join(self.config.artifact_dir, 'stdout')",
            "        command_filename = os.path.join(self.config.artifact_dir, 'command')",
            "",
            "        try:",
            "            os.makedirs(self.config.artifact_dir, mode=0o700)",
            "        except OSError as exc:",
            "            if exc.errno == errno.EEXIST and os.path.isdir(self.config.artifact_dir):",
            "                pass",
            "            else:",
            "                raise",
            "        os.close(os.open(stdout_filename, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))",
            "",
            "        job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
            "        if not os.path.exists(job_events_path):",
            "            os.mkdir(job_events_path, 0o700)",
            "",
            "        command = self.config.command",
            "        with codecs.open(command_filename, 'w', encoding='utf-8') as f:",
            "            os.chmod(command_filename, stat.S_IRUSR | stat.S_IWUSR)",
            "            json.dump(",
            "                {'command': command,",
            "                 'cwd': self.config.cwd,",
            "                 'env': self.config.env}, f, ensure_ascii=False",
            "            )",
            "",
            "        if self.config.ident is not None:",
            "            cleanup_artifact_dir(os.path.join(self.config.artifact_dir, \"..\"), self.config.rotate_artifacts)",
            "",
            "        stdout_handle = codecs.open(stdout_filename, 'w', encoding='utf-8')",
            "        stdout_handle = OutputEventFilter(stdout_handle, self.event_callback, self.config.suppress_ansible_output, output_json=self.config.json_mode)",
            "",
            "        if not isinstance(self.config.expect_passwords, collections.OrderedDict):",
            "            # We iterate over `expect_passwords.keys()` and",
            "            # `expect_passwords.values()` separately to map matched inputs to",
            "            # patterns and choose the proper string to send to the subprocess;",
            "            # enforce usage of an OrderedDict so that the ordering of elements in",
            "            # `keys()` matches `values()`.",
            "            expect_passwords = collections.OrderedDict(self.config.expect_passwords)",
            "        password_patterns = list(expect_passwords.keys())",
            "        password_values = list(expect_passwords.values())",
            "",
            "        # pexpect needs all env vars to be utf-8 encoded bytes",
            "        # https://github.com/pexpect/pexpect/issues/512",
            "",
            "        # Use a copy so as not to cause problems when serializing the job_env.",
            "        if self.config.containerized:",
            "            # If this is containerized, the shell environment calling podman has little",
            "            # to do with the actual job environment, but still needs PATH, auth, etc.",
            "            pexpect_env = os.environ.copy()",
            "            # But we still rely on env vars to pass secrets",
            "            pexpect_env.update(self.config.env)",
            "            # Write the keys to pass into container to expected file in artifacts dir",
            "            # option expecting should have already been written in ansible_runner.runner_config",
            "            env_file_host = os.path.join(self.config.artifact_dir, 'env.list')",
            "            with open(env_file_host, 'w') as f:",
            "                f.write('\\n'.join(list(self.config.env.keys())))",
            "        else:",
            "            pexpect_env = self.config.env",
            "        env = {",
            "            ensure_str(k): ensure_str(v) if k != 'PATH' and isinstance(v, six.text_type) else v",
            "            for k, v in pexpect_env.items()",
            "        }",
            "",
            "        # Prepare to collect performance data",
            "        if self.config.resource_profiling:",
            "            cgroup_path = '{0}/{1}'.format(self.config.resource_profiling_base_cgroup, self.config.ident)",
            "",
            "            import getpass",
            "            import grp",
            "            user = getpass.getuser()",
            "            group = grp.getgrgid(os.getgid()).gr_name",
            "",
            "            cmd = 'cgcreate -a {user}:{group} -t {user}:{group} -g cpuacct,memory,pids:{}'.format(cgroup_path, user=user, group=group)",
            "            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)",
            "            _, stderr = proc.communicate()",
            "            if proc.returncode:",
            "                # Unable to create cgroup",
            "                logger.error('Unable to create cgroup: {}'.format(stderr))",
            "                raise RuntimeError('Unable to create cgroup: {}'.format(stderr))",
            "            else:",
            "                logger.info(\"Created cgroup '{}'\".format(cgroup_path))",
            "",
            "",
            "        self.status_callback('running')",
            "        self.last_stdout_update = time.time()",
            "        try:",
            "            child = pexpect.spawn(",
            "                command[0],",
            "                command[1:],",
            "                cwd=self.config.cwd,",
            "                env=env,",
            "                ignore_sighup=True,",
            "                encoding='utf-8',",
            "                echo=False,",
            "                use_poll=self.config.pexpect_use_poll,",
            "            )",
            "            child.logfile_read = stdout_handle",
            "        except pexpect.exceptions.ExceptionPexpect as e:",
            "            child = collections.namedtuple(",
            "                'MissingProcess', 'exitstatus isalive close'",
            "            )(",
            "                exitstatus=127,",
            "                isalive=lambda: False,",
            "                close=lambda: None,",
            "            )",
            "",
            "            def _decode(x):",
            "                return x.decode('utf-8') if six.PY2 else x",
            "",
            "            # create the events directory (the callback plugin won't run, so it",
            "            # won't get created)",
            "            events_directory = os.path.join(self.config.artifact_dir, 'job_events')",
            "            if not os.path.exists(events_directory):",
            "                os.mkdir(events_directory, 0o700)",
            "            stdout_handle.write(_decode(str(e)))",
            "            stdout_handle.write(_decode('\\n'))",
            "",
            "        job_start = time.time()",
            "        while child.isalive():",
            "            result_id = child.expect(password_patterns,",
            "                                     timeout=self.config.pexpect_timeout,",
            "                                     searchwindowsize=100)",
            "            password = password_values[result_id]",
            "            if password is not None:",
            "                child.sendline(password)",
            "                self.last_stdout_update = time.time()",
            "            if self.cancel_callback:",
            "                try:",
            "                    self.canceled = self.cancel_callback()",
            "                except Exception as e:",
            "                    # TODO: logger.exception('Could not check cancel callback - cancelling immediately')",
            "                    #if isinstance(extra_update_fields, dict):",
            "                    #    extra_update_fields['job_explanation'] = \"System error during job execution, check system logs\"",
            "                    raise CallbackError(\"Exception in Cancel Callback: {}\".format(e))",
            "            if self.config.job_timeout and not self.canceled and (time.time() - job_start) > self.config.job_timeout:",
            "                self.timed_out = True",
            "                # if isinstance(extra_update_fields, dict):",
            "                #     extra_update_fields['job_explanation'] = \"Job terminated due to timeout\"",
            "            if self.canceled or self.timed_out or self.errored:",
            "                self.kill_container()",
            "                Runner.handle_termination(child.pid, is_cancel=self.canceled)",
            "            if self.config.idle_timeout and (time.time() - self.last_stdout_update) > self.config.idle_timeout:",
            "                self.kill_container()",
            "                Runner.handle_termination(child.pid, is_cancel=False)",
            "                self.timed_out = True",
            "",
            "        stdout_handle.flush()",
            "        stdout_handle.close()",
            "        child.close()",
            "",
            "        if self.canceled:",
            "            self.status_callback('canceled')",
            "        elif child.exitstatus == 0 and not self.timed_out:",
            "            self.status_callback('successful')",
            "        elif self.timed_out:",
            "            self.status_callback('timeout')",
            "        else:",
            "            self.status_callback('failed')",
            "        self.rc = child.exitstatus if not (self.timed_out or self.canceled) else 254",
            "        for filename, data in [",
            "            ('status', self.status),",
            "            ('rc', self.rc),",
            "        ]:",
            "            artifact_path = os.path.join(self.config.artifact_dir, filename)",
            "            if not os.path.exists(artifact_path):",
            "                os.close(os.open(artifact_path, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))",
            "            with open(artifact_path, 'w') as f:",
            "                f.write(str(data))",
            "        if self.config.directory_isolation_path and self.config.directory_isolation_cleanup:",
            "            shutil.rmtree(self.config.directory_isolation_path)",
            "        if self.config.process_isolation and self.config.process_isolation_path_actual:",
            "            def _delete(retries=15):",
            "                try:",
            "                    shutil.rmtree(self.config.process_isolation_path_actual)",
            "                except OSError as e:",
            "                    res = False",
            "                    if e.errno == 16 and retries > 0:",
            "                        time.sleep(1)",
            "                        res = _delete(retries=retries - 1)",
            "                    if not res:",
            "                        raise",
            "                return True",
            "            _delete()",
            "        if self.config.resource_profiling:",
            "            cmd = 'cgdelete -g cpuacct,memory,pids:{}'.format(cgroup_path)",
            "            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)",
            "            _, stderr = proc.communicate()",
            "            if proc.returncode:",
            "                logger.error('Failed to delete cgroup: {}'.format(stderr))",
            "                raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))",
            "",
            "        if self.artifacts_handler is not None:",
            "            try:",
            "                self.artifacts_handler(self.config.artifact_dir)",
            "            except Exception as e:",
            "                raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))",
            "",
            "        if self.finished_callback is not None:",
            "            try:",
            "                self.finished_callback(self)",
            "            except Exception as e:",
            "                raise CallbackError(\"Exception in Finished Callback: {}\".format(e))",
            "        return self.status, self.rc",
            "",
            "    @property",
            "    def stdout(self):",
            "        '''",
            "        Returns an open file handle to the stdout representing the Ansible run",
            "        '''",
            "        stdout_path = os.path.join(self.config.artifact_dir, 'stdout')",
            "        if not os.path.exists(stdout_path):",
            "            raise AnsibleRunnerException(\"stdout missing\")",
            "        return open(os.path.join(self.config.artifact_dir, 'stdout'), 'r')",
            "",
            "    @property",
            "    def events(self):",
            "        '''",
            "        A generator that will return all ansible job events in the order that they were emitted from Ansible",
            "",
            "        Example:",
            "",
            "            {",
            "               \"event\":\"runner_on_ok\",",
            "               \"uuid\":\"00a50d9c-161a-4b74-b978-9f60becaf209\",",
            "               \"stdout\":\"ok: [localhost] => {\\\\r\\\\n    \\\\\"   msg\\\\\":\\\\\"Test!\\\\\"\\\\r\\\\n}\",",
            "               \"counter\":6,",
            "               \"pid\":740,",
            "               \"created\":\"2018-04-05T18:24:36.096725\",",
            "               \"end_line\":10,",
            "               \"start_line\":7,",
            "               \"event_data\":{",
            "                  \"play_pattern\":\"all\",",
            "                  \"play\":\"all\",",
            "                  \"task\":\"debug\",",
            "                  \"task_args\":\"msg=Test!\",",
            "                  \"remote_addr\":\"localhost\",",
            "                  \"res\":{",
            "                     \"msg\":\"Test!\",",
            "                     \"changed\":false,",
            "                     \"_ansible_verbose_always\":true,",
            "                     \"_ansible_no_log\":false",
            "                  },",
            "                  \"pid\":740,",
            "                  \"play_uuid\":\"0242ac11-0002-443b-cdb1-000000000006\",",
            "                  \"task_uuid\":\"0242ac11-0002-443b-cdb1-000000000008\",",
            "                  \"event_loop\":null,",
            "                  \"playbook_uuid\":\"634edeee-3228-4c17-a1b4-f010fdd42eb2\",",
            "                  \"playbook\":\"test.yml\",",
            "                  \"task_action\":\"debug\",",
            "                  \"host\":\"localhost\",",
            "                  \"task_path\":\"/tmp/demo/project/test.yml:3\"",
            "               }",
            "           }",
            "        '''",
            "        # collection of all the events that were yielded",
            "        old_events = {}",
            "        event_path = os.path.join(self.config.artifact_dir, 'job_events')",
            "",
            "        # Wait for events dir to be created",
            "        now = datetime.datetime.now()",
            "        while not os.path.exists(event_path):",
            "            time.sleep(0.05)",
            "            wait_time = datetime.datetime.now() - now",
            "            if wait_time.total_seconds() > 60:",
            "                raise AnsibleRunnerException(\"events directory is missing: %s\" % event_path)",
            "",
            "        while self.status == \"running\":",
            "            for event, old_evnts in collect_new_events(event_path, old_events):",
            "                old_events = old_evnts",
            "                yield event",
            "",
            "        # collect new events that were written after the playbook has finished",
            "        for event, old_evnts in collect_new_events(event_path, old_events):",
            "            old_events = old_evnts",
            "            yield event",
            "",
            "    @property",
            "    def stats(self):",
            "        '''",
            "        Returns the final high level stats from the Ansible run",
            "",
            "        Example:",
            "            {'dark': {}, 'failures': {}, 'skipped': {}, 'ok': {u'localhost': 2}, 'processed': {u'localhost': 1}}",
            "        '''",
            "        last_event = list(filter(lambda x: 'event' in x and x['event'] == 'playbook_on_stats',",
            "                                 self.events))",
            "        if not last_event:",
            "            return None",
            "        last_event = last_event[0]['event_data']",
            "        return dict(skipped=last_event.get('skipped',{}),",
            "                    ok=last_event.get('ok',{}),",
            "                    dark=last_event.get('dark',{}),",
            "                    failures=last_event.get('failures',{}),",
            "                    ignored=last_event.get('ignored', {}),",
            "                    rescued=last_event.get('rescued', {}),",
            "                    processed=last_event.get('processed',{}),",
            "                    changed=last_event.get('changed',{}))",
            "",
            "",
            "    def host_events(self, host):",
            "        '''",
            "        Given a host name, this will return all task events executed on that host",
            "        '''",
            "        all_host_events = filter(lambda x: 'event_data' in x and 'host' in x['event_data'] and x['event_data']['host'] == host,",
            "                                 self.events)",
            "        return all_host_events",
            "",
            "    def kill_container(self):",
            "        '''",
            "        Internal method to terminate a container being used for job isolation",
            "        '''",
            "        container_name = self.config.container_name",
            "        if container_name:",
            "            container_cli = self.config.process_isolation_executable",
            "            cmd = '{} kill {}'.format(container_cli, container_name)",
            "            proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)",
            "            _, stderr = proc.communicate()",
            "            if proc.returncode:",
            "                logger.info('Error from {} kill {} command:\\n{}'.format(container_cli, container_name, stderr))",
            "            else:",
            "                logger.info(\"Killed container {}\".format(container_name))",
            "",
            "    @classmethod",
            "    def handle_termination(cls, pid, pidfile=None, is_cancel=True):",
            "        '''",
            "        Internal method to terminate a subprocess spawned by `pexpect` representing an invocation of runner.",
            "",
            "        :param pid:       the process id of the running the job.",
            "        :param pidfile:   the daemon's PID file",
            "        :param is_cancel: flag showing whether this termination is caused by",
            "                          instance's cancel_flag.",
            "        '''",
            "",
            "        try:",
            "            main_proc = psutil.Process(pid=pid)",
            "            child_procs = main_proc.children(recursive=True)",
            "            for child_proc in child_procs:",
            "                try:",
            "                    os.kill(child_proc.pid, signal.SIGKILL)",
            "                except (TypeError, OSError):",
            "                    pass",
            "            os.kill(main_proc.pid, signal.SIGKILL)",
            "            try:",
            "                os.remove(pidfile)",
            "            except (OSError):",
            "                pass",
            "        except (TypeError, psutil.Error, OSError):",
            "            try:",
            "                os.kill(pid, signal.SIGKILL)",
            "            except (OSError):",
            "                pass",
            "",
            "    def get_fact_cache(self, host):",
            "        '''",
            "        Get the entire fact cache only if the fact_cache_type is 'jsonfile'",
            "        '''",
            "        if self.config.fact_cache_type != 'jsonfile':",
            "            raise Exception('Unsupported fact cache type.  Only \"jsonfile\" is supported for reading and writing facts from ansible-runner')",
            "        fact_cache = os.path.join(self.config.fact_cache, host)",
            "        if os.path.exists(fact_cache):",
            "            with open(fact_cache) as f:",
            "                return json.loads(f.read())",
            "        return {}",
            "",
            "    def set_fact_cache(self, host, data):",
            "        '''",
            "        Set the entire fact cache data only if the fact_cache_type is 'jsonfile'",
            "        '''",
            "        if self.config.fact_cache_type != 'jsonfile':",
            "            raise Exception('Unsupported fact cache type.  Only \"jsonfile\" is supported for reading and writing facts from ansible-runner')",
            "        fact_cache = os.path.join(self.config.fact_cache, host)",
            "        if not os.path.exists(os.path.dirname(fact_cache)):",
            "            os.makedirs(os.path.dirname(fact_cache), mode=0o700)",
            "        with open(fact_cache, 'w') as f:",
            "            return f.write(json.dumps(data))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "30": [
                "Runner",
                "__init__"
            ],
            "34": [
                "Runner",
                "__init__"
            ],
            "287": [
                "Runner",
                "run"
            ],
            "289": [
                "Runner",
                "run"
            ]
        },
        "addLocation": []
    },
    "ansible_runner/streaming.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import base64"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+import codecs"
            },
            "2": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import io"
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import json"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import os"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 6,
                "PatchRowcode": "+import stat"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+import tempfile"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+import uuid"
            },
            "8": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " import zipfile"
            },
            "9": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 11,
                "PatchRowcode": "+import ansible_runner"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 12,
                "PatchRowcode": "+import ansible_runner.plugins"
            },
            "12": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-class StreamWorker(object):"
            },
            "14": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __init__(self, control_out):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 14,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 15,
                "PatchRowcode": "+class UUIDEncoder(json.JSONEncoder):"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+    def default(self, obj):"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+        if isinstance(obj, uuid.UUID):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+            return obj.hex"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+        return json.JSONEncoder.default(self, obj)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+# List of kwargs options to the run method that should be sent to the remote executor."
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+remote_run_options = ("
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+    'forks',"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+    'host_pattern',"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+    'ident',"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+    'ignore_logging',"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+    'inventory',"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+    'limit',"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+    'module',"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+    'module_args',"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+    'omit_event_data',"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+    'only_failed_event_data',"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+    'playbook',"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+    'verbosity',"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+class StreamController(object):"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+    def __init__(self, control_in, control_out, status_handler=None, event_handler=None,"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+                 artifacts_handler=None, cancel_callback=None, finished_callback=None, **kwargs):"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+        self.control_in = control_in"
            },
            "44": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "         self.control_out = control_out"
            },
            "45": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 44,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def status_handler(self, status, runner_config):"
            },
            "47": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.write(json.dumps(status).encode('utf-8'))"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+        self.kwargs = kwargs"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        self.config = ansible_runner.RunnerConfig(**kwargs)"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+        self.status_handler = status_handler"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+        self.event_handler = event_handler"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+        self.artifacts_handler = artifacts_handler"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        self.cancel_callback = cancel_callback"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+        self.finished_callback = finished_callback"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+        self.status = \"unstarted\""
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+        self.rc = None"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    def run(self):"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+        self.send_job()"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+        job_events_path = os.path.join(self.config.artifact_dir, 'job_events')"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+        if not os.path.exists(job_events_path):"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+            os.mkdir(job_events_path, 0o700)"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+        for line in self.control_in:"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+            data = json.loads(line)"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+            if 'status' in data:"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+                self.status_callback(data)"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+            elif 'artifacts' in data:"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+                self.artifacts_callback(data)"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+            elif 'eof' in data:"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+                break"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+            else:"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+                self.event_callback(data)"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        if self.finished_callback is not None:"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+            self.finished_callback(self)"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+        return self.status, self.rc"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+    def send_job(self):"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+        self.config.prepare()"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        remote_options = {key: value for key, value in self.kwargs.items() if key in remote_run_options}"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+        buf = io.BytesIO()"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+        with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+            private_data_dir = self.kwargs.get('private_data_dir', None)"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+            if private_data_dir:"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+                for dirpath, dirs, files in os.walk(private_data_dir):"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+                    relpath = os.path.relpath(dirpath, private_data_dir)"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+                    if relpath == \".\":"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+                        relpath = \"\""
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+                    for fname in files:"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+                        archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+            kwargs = json.dumps(remote_options, cls=UUIDEncoder)"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+            archive.writestr('kwargs', kwargs)"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+            archive.close()"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+        buf.flush()"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+        data = {"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+            'private_data_dir': True,"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+            'payload': base64.b64encode(buf.getvalue()).decode('ascii'),"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+        }"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+        self.control_out.write(json.dumps(data).encode('utf-8'))"
            },
            "107": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "         self.control_out.write(b'\\n')"
            },
            "108": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "         self.control_out.flush()"
            },
            "109": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+        self.control_out.close()"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+    def status_callback(self, status_data):"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+        self.status = status_data['status']"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+"
            },
            "114": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+        for plugin in ansible_runner.plugins:"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+            ansible_runner.plugins[plugin].status_handler(self.config, status_data)"
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+        if self.status_handler is not None:"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+            self.status_handler(status_data, runner_config=self.config)"
            },
            "118": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+"
            },
            "119": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+    def event_callback(self, event_data):"
            },
            "120": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+        full_filename = os.path.join(self.config.artifact_dir,"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+                                     'job_events',"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+                                     '{}-{}.json'.format(event_data['counter'],"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+                                                         event_data['uuid']))"
            },
            "124": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+"
            },
            "125": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+        if self.event_handler is not None:"
            },
            "126": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+            should_write = self.event_handler(event_data)"
            },
            "127": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        else:"
            },
            "128": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+            should_write = True"
            },
            "129": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+        for plugin in ansible_runner.plugins:"
            },
            "130": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+            ansible_runner.plugins[plugin].event_handler(self.config, event_data)"
            },
            "131": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        if should_write:"
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+            with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:"
            },
            "133": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+                os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)"
            },
            "134": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+                json.dump(event_data, write_file)"
            },
            "135": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+"
            },
            "136": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+    def artifacts_callback(self, artifacts_data):  # FIXME"
            },
            "137": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+        if self.artifacts_handler is not None:"
            },
            "138": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+            self.artifacts_handler()"
            },
            "139": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+"
            },
            "140": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+class StreamWorker(object):"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+    def __init__(self, worker_in, worker_out, **kwargs):"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+        self.worker_in = worker_in"
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+        self.worker_out = worker_out"
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+"
            },
            "146": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+        self.kwargs = kwargs"
            },
            "147": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 144,
                "PatchRowcode": "+"
            },
            "148": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 145,
                "PatchRowcode": "+        self.private_data_dir = tempfile.TemporaryDirectory().name"
            },
            "149": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+"
            },
            "150": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+    def run(self):"
            },
            "151": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 148,
                "PatchRowcode": "+        for line in self.worker_in:"
            },
            "152": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+            data = json.loads(line)"
            },
            "153": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+            if data.get('private_data_dir'):"
            },
            "154": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+                buf = io.BytesIO(base64.b64decode(data['payload']))"
            },
            "155": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 152,
                "PatchRowcode": "+                with zipfile.ZipFile(buf, 'r') as archive:"
            },
            "156": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+                    archive.extractall(path=self.private_data_dir)"
            },
            "157": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+"
            },
            "158": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 155,
                "PatchRowcode": "+        kwargs_path = os.path.join(self.private_data_dir, 'kwargs')"
            },
            "159": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 156,
                "PatchRowcode": "+        if os.path.exists(kwargs_path):"
            },
            "160": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 157,
                "PatchRowcode": "+            with open(kwargs_path, \"r\") as kwf:"
            },
            "161": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 158,
                "PatchRowcode": "+                kwargs = json.load(kwf)"
            },
            "162": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 159,
                "PatchRowcode": "+            if not isinstance(kwargs, dict):"
            },
            "163": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 160,
                "PatchRowcode": "+                raise ValueError(\"Invalid kwargs data\")"
            },
            "164": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+        else:"
            },
            "165": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 162,
                "PatchRowcode": "+            kwargs = {}"
            },
            "166": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 163,
                "PatchRowcode": "+"
            },
            "167": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 164,
                "PatchRowcode": "+        self.kwargs.update(kwargs)"
            },
            "168": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+"
            },
            "169": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 166,
                "PatchRowcode": "+        self.kwargs['quiet'] = True"
            },
            "170": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+        self.kwargs['private_data_dir'] = self.private_data_dir"
            },
            "171": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+        self.kwargs['status_handler'] = self.status_handler"
            },
            "172": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+        self.kwargs['event_handler'] = self.event_handler"
            },
            "173": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+        self.kwargs['artifacts_handler'] = self.artifacts_handler"
            },
            "174": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+        self.kwargs['finished_callback'] = self.finished_callback"
            },
            "175": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 172,
                "PatchRowcode": "+"
            },
            "176": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 173,
                "PatchRowcode": "+        ansible_runner.interface.run(**self.kwargs)"
            },
            "177": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 174,
                "PatchRowcode": "+"
            },
            "178": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 175,
                "PatchRowcode": "+        # FIXME: do cleanup on the tempdir"
            },
            "179": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+"
            },
            "180": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+    def status_handler(self, status, runner_config):"
            },
            "181": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+        self.worker_out.write(json.dumps(status).encode('utf-8'))"
            },
            "182": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 179,
                "PatchRowcode": "+        self.worker_out.write(b'\\n')"
            },
            "183": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+        self.worker_out.flush()"
            },
            "184": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 181,
                "PatchRowcode": " "
            },
            "185": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "     def event_handler(self, event_data):"
            },
            "186": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.write(json.dumps(event_data).encode('utf-8'))"
            },
            "187": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.write(b'\\n')"
            },
            "188": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.flush()"
            },
            "189": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+        self.worker_out.write(json.dumps(event_data).encode('utf-8'))"
            },
            "190": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+        self.worker_out.write(b'\\n')"
            },
            "191": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+        self.worker_out.flush()"
            },
            "192": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 186,
                "PatchRowcode": " "
            },
            "193": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def artifacts_callback(self, artifact_dir):"
            },
            "194": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+    def artifacts_handler(self, artifact_dir):"
            },
            "195": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "         buf = io.BytesIO()"
            },
            "196": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:"
            },
            "197": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "             for dirpath, dirs, files in os.walk(artifact_dir):"
            },
            "198": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 199,
                "PatchRowcode": "             'artifacts': True,"
            },
            "199": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 200,
                "PatchRowcode": "             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),"
            },
            "200": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 201,
                "PatchRowcode": "         }"
            },
            "201": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.write(json.dumps(data).encode('utf-8'))"
            },
            "202": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.write(b'\\n')"
            },
            "203": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.flush()"
            },
            "204": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.control_out.close()"
            },
            "205": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+        self.worker_out.write(json.dumps(data).encode('utf-8'))"
            },
            "206": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+        self.worker_out.write(b'\\n')"
            },
            "207": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+        self.worker_out.flush()"
            },
            "208": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+"
            },
            "209": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+    def finished_callback(self, runner_obj):"
            },
            "210": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+        self.worker_out.write(json.dumps({'eof': True}).encode('utf-8'))"
            },
            "211": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+        self.worker_out.write(b'\\n')"
            },
            "212": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+        self.worker_out.flush()"
            },
            "213": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+        self.worker_out.close()"
            }
        },
        "frontPatchFile": [
            "import base64",
            "import io",
            "import json",
            "import os",
            "import zipfile",
            "",
            "",
            "class StreamWorker(object):",
            "    def __init__(self, control_out):",
            "        self.control_out = control_out",
            "",
            "    def status_handler(self, status, runner_config):",
            "        self.control_out.write(json.dumps(status).encode('utf-8'))",
            "        self.control_out.write(b'\\n')",
            "        self.control_out.flush()",
            "",
            "    def event_handler(self, event_data):",
            "        self.control_out.write(json.dumps(event_data).encode('utf-8'))",
            "        self.control_out.write(b'\\n')",
            "        self.control_out.flush()",
            "",
            "    def artifacts_callback(self, artifact_dir):",
            "        buf = io.BytesIO()",
            "        with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
            "            for dirpath, dirs, files in os.walk(artifact_dir):",
            "                relpath = os.path.relpath(dirpath, artifact_dir)",
            "                if relpath == \".\":",
            "                    relpath = \"\"",
            "                for fname in files:",
            "                    archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
            "            archive.close()",
            "",
            "        data = {",
            "            'artifacts': True,",
            "            'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
            "        }",
            "        self.control_out.write(json.dumps(data).encode('utf-8'))",
            "        self.control_out.write(b'\\n')",
            "        self.control_out.flush()",
            "        self.control_out.close()"
        ],
        "afterPatchFile": [
            "import base64",
            "import codecs",
            "import io",
            "import json",
            "import os",
            "import stat",
            "import tempfile",
            "import uuid",
            "import zipfile",
            "",
            "import ansible_runner",
            "import ansible_runner.plugins",
            "",
            "",
            "class UUIDEncoder(json.JSONEncoder):",
            "    def default(self, obj):",
            "        if isinstance(obj, uuid.UUID):",
            "            return obj.hex",
            "        return json.JSONEncoder.default(self, obj)",
            "",
            "",
            "# List of kwargs options to the run method that should be sent to the remote executor.",
            "remote_run_options = (",
            "    'forks',",
            "    'host_pattern',",
            "    'ident',",
            "    'ignore_logging',",
            "    'inventory',",
            "    'limit',",
            "    'module',",
            "    'module_args',",
            "    'omit_event_data',",
            "    'only_failed_event_data',",
            "    'playbook',",
            "    'verbosity',",
            ")",
            "",
            "",
            "class StreamController(object):",
            "    def __init__(self, control_in, control_out, status_handler=None, event_handler=None,",
            "                 artifacts_handler=None, cancel_callback=None, finished_callback=None, **kwargs):",
            "        self.control_in = control_in",
            "        self.control_out = control_out",
            "",
            "        self.kwargs = kwargs",
            "        self.config = ansible_runner.RunnerConfig(**kwargs)",
            "        self.status_handler = status_handler",
            "        self.event_handler = event_handler",
            "        self.artifacts_handler = artifacts_handler",
            "",
            "        self.cancel_callback = cancel_callback",
            "        self.finished_callback = finished_callback",
            "",
            "        self.status = \"unstarted\"",
            "        self.rc = None",
            "",
            "    def run(self):",
            "        self.send_job()",
            "",
            "        job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
            "        if not os.path.exists(job_events_path):",
            "            os.mkdir(job_events_path, 0o700)",
            "",
            "        for line in self.control_in:",
            "            data = json.loads(line)",
            "            if 'status' in data:",
            "                self.status_callback(data)",
            "            elif 'artifacts' in data:",
            "                self.artifacts_callback(data)",
            "            elif 'eof' in data:",
            "                break",
            "            else:",
            "                self.event_callback(data)",
            "",
            "        if self.finished_callback is not None:",
            "            self.finished_callback(self)",
            "        return self.status, self.rc",
            "",
            "    def send_job(self):",
            "        self.config.prepare()",
            "        remote_options = {key: value for key, value in self.kwargs.items() if key in remote_run_options}",
            "",
            "        buf = io.BytesIO()",
            "        with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
            "            private_data_dir = self.kwargs.get('private_data_dir', None)",
            "            if private_data_dir:",
            "                for dirpath, dirs, files in os.walk(private_data_dir):",
            "                    relpath = os.path.relpath(dirpath, private_data_dir)",
            "                    if relpath == \".\":",
            "                        relpath = \"\"",
            "                    for fname in files:",
            "                        archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
            "",
            "            kwargs = json.dumps(remote_options, cls=UUIDEncoder)",
            "            archive.writestr('kwargs', kwargs)",
            "            archive.close()",
            "        buf.flush()",
            "",
            "        data = {",
            "            'private_data_dir': True,",
            "            'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
            "        }",
            "        self.control_out.write(json.dumps(data).encode('utf-8'))",
            "        self.control_out.write(b'\\n')",
            "        self.control_out.flush()",
            "        self.control_out.close()",
            "",
            "    def status_callback(self, status_data):",
            "        self.status = status_data['status']",
            "",
            "        for plugin in ansible_runner.plugins:",
            "            ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
            "        if self.status_handler is not None:",
            "            self.status_handler(status_data, runner_config=self.config)",
            "",
            "    def event_callback(self, event_data):",
            "        full_filename = os.path.join(self.config.artifact_dir,",
            "                                     'job_events',",
            "                                     '{}-{}.json'.format(event_data['counter'],",
            "                                                         event_data['uuid']))",
            "",
            "        if self.event_handler is not None:",
            "            should_write = self.event_handler(event_data)",
            "        else:",
            "            should_write = True",
            "        for plugin in ansible_runner.plugins:",
            "            ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
            "        if should_write:",
            "            with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
            "                os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
            "                json.dump(event_data, write_file)",
            "",
            "    def artifacts_callback(self, artifacts_data):  # FIXME",
            "        if self.artifacts_handler is not None:",
            "            self.artifacts_handler()",
            "",
            "",
            "class StreamWorker(object):",
            "    def __init__(self, worker_in, worker_out, **kwargs):",
            "        self.worker_in = worker_in",
            "        self.worker_out = worker_out",
            "",
            "        self.kwargs = kwargs",
            "",
            "        self.private_data_dir = tempfile.TemporaryDirectory().name",
            "",
            "    def run(self):",
            "        for line in self.worker_in:",
            "            data = json.loads(line)",
            "            if data.get('private_data_dir'):",
            "                buf = io.BytesIO(base64.b64decode(data['payload']))",
            "                with zipfile.ZipFile(buf, 'r') as archive:",
            "                    archive.extractall(path=self.private_data_dir)",
            "",
            "        kwargs_path = os.path.join(self.private_data_dir, 'kwargs')",
            "        if os.path.exists(kwargs_path):",
            "            with open(kwargs_path, \"r\") as kwf:",
            "                kwargs = json.load(kwf)",
            "            if not isinstance(kwargs, dict):",
            "                raise ValueError(\"Invalid kwargs data\")",
            "        else:",
            "            kwargs = {}",
            "",
            "        self.kwargs.update(kwargs)",
            "",
            "        self.kwargs['quiet'] = True",
            "        self.kwargs['private_data_dir'] = self.private_data_dir",
            "        self.kwargs['status_handler'] = self.status_handler",
            "        self.kwargs['event_handler'] = self.event_handler",
            "        self.kwargs['artifacts_handler'] = self.artifacts_handler",
            "        self.kwargs['finished_callback'] = self.finished_callback",
            "",
            "        ansible_runner.interface.run(**self.kwargs)",
            "",
            "        # FIXME: do cleanup on the tempdir",
            "",
            "    def status_handler(self, status, runner_config):",
            "        self.worker_out.write(json.dumps(status).encode('utf-8'))",
            "        self.worker_out.write(b'\\n')",
            "        self.worker_out.flush()",
            "",
            "    def event_handler(self, event_data):",
            "        self.worker_out.write(json.dumps(event_data).encode('utf-8'))",
            "        self.worker_out.write(b'\\n')",
            "        self.worker_out.flush()",
            "",
            "    def artifacts_handler(self, artifact_dir):",
            "        buf = io.BytesIO()",
            "        with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
            "            for dirpath, dirs, files in os.walk(artifact_dir):",
            "                relpath = os.path.relpath(dirpath, artifact_dir)",
            "                if relpath == \".\":",
            "                    relpath = \"\"",
            "                for fname in files:",
            "                    archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
            "            archive.close()",
            "",
            "        data = {",
            "            'artifacts': True,",
            "            'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
            "        }",
            "        self.worker_out.write(json.dumps(data).encode('utf-8'))",
            "        self.worker_out.write(b'\\n')",
            "        self.worker_out.flush()",
            "",
            "    def finished_callback(self, runner_obj):",
            "        self.worker_out.write(json.dumps({'eof': True}).encode('utf-8'))",
            "        self.worker_out.write(b'\\n')",
            "        self.worker_out.flush()",
            "        self.worker_out.close()"
        ],
        "action": [
            "0",
            "-1",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "8": [
                "StreamWorker"
            ],
            "9": [
                "StreamWorker",
                "__init__"
            ],
            "12": [
                "StreamWorker",
                "status_handler"
            ],
            "13": [
                "StreamWorker",
                "status_handler"
            ],
            "18": [
                "StreamWorker",
                "event_handler"
            ],
            "19": [
                "StreamWorker",
                "event_handler"
            ],
            "20": [
                "StreamWorker",
                "event_handler"
            ],
            "22": [
                "StreamWorker",
                "artifacts_callback"
            ],
            "37": [
                "StreamWorker",
                "artifacts_callback"
            ],
            "38": [
                "StreamWorker",
                "artifacts_callback"
            ],
            "39": [
                "StreamWorker",
                "artifacts_callback"
            ],
            "40": [
                "StreamWorker",
                "artifacts_callback"
            ]
        },
        "addLocation": []
    }
}