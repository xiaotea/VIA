{
    "src/prefect/client/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import copy"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " import sys"
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import threading"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 4,
                "PatchRowcode": "+import uuid"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from collections import defaultdict"
            },
            "5": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from contextlib import asynccontextmanager"
            },
            "6": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from functools import partial"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+from datetime import datetime, timezone"
            },
            "8": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from typing import ("
            },
            "9": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": "     Any,"
            },
            "10": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": "     AsyncGenerator,"
            },
            "11": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 11,
                "PatchRowcode": "     Awaitable,"
            },
            "12": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 12,
                "PatchRowcode": "     Callable,"
            },
            "13": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 13,
                "PatchRowcode": "     Dict,"
            },
            "14": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 14,
                "PatchRowcode": "     MutableMapping,"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 15,
                "PatchRowcode": "+    Optional,"
            },
            "16": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 16,
                "PatchRowcode": "     Protocol,"
            },
            "17": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 17,
                "PatchRowcode": "     Set,"
            },
            "18": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 18,
                "PatchRowcode": "     Tuple,"
            },
            "19": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " import anyio"
            },
            "20": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " import httpx"
            },
            "21": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from asgi_lifespan import LifespanManager"
            },
            "22": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from httpx import HTTPStatusError, Response"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+from httpx import HTTPStatusError, Request, Response"
            },
            "24": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from prefect._vendor.starlette import status"
            },
            "25": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from typing_extensions import Self"
            },
            "26": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+from prefect.client.schemas.objects import CsrfToken"
            },
            "28": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from prefect.exceptions import PrefectHTTPStatusError"
            },
            "29": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from prefect.logging import get_logger"
            },
            "30": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from prefect.settings import ("
            },
            "31": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "     [Configuring Cloudflare Rate Limiting](https://support.cloudflare.com/hc/en-us/articles/115001635128-Configuring-Rate-Limiting-from-UI)"
            },
            "32": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "     \"\"\""
            },
            "33": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 193,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 194,
                "PatchRowcode": "+    def __init__(self, *args, enable_csrf_support: bool = False, **kwargs):"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 195,
                "PatchRowcode": "+        self.enable_csrf_support: bool = enable_csrf_support"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+        self.csrf_token: Optional[str] = None"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+        self.csrf_token_expiration: Optional[datetime] = None"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+        self.csrf_client_id: uuid.UUID = uuid.uuid4()"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+        super().__init__(*args, **kwargs)"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+"
            },
            "42": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 202,
                "PatchRowcode": "     async def _send_with_retry("
            },
            "43": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 203,
                "PatchRowcode": "         self,"
            },
            "44": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        request: Callable,"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+        request: Request,"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+        send: Callable[[Request], Awaitable[Response]],"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+        send_args: Tuple,"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 207,
                "PatchRowcode": "+        send_kwargs: Dict,"
            },
            "49": {
                "beforePatchRowNumber": 194,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "         retry_codes: Set[int] = set(),"
            },
            "50": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": 209,
                "PatchRowcode": "         retry_exceptions: Tuple[Exception, ...] = tuple(),"
            },
            "51": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": 210,
                "PatchRowcode": "     ):"
            },
            "52": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 221,
                "PatchRowcode": "         try_count = 0"
            },
            "53": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 222,
                "PatchRowcode": "         response = None"
            },
            "54": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 223,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 224,
                "PatchRowcode": "+        is_change_request = request.method.lower() in {\"post\", \"put\", \"patch\", \"delete\"}"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+        if self.enable_csrf_support and is_change_request:"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 227,
                "PatchRowcode": "+            await self._add_csrf_headers(request=request)"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 228,
                "PatchRowcode": "+"
            },
            "60": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "         while try_count <= PREFECT_CLIENT_MAX_RETRIES.value():"
            },
            "61": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "             try_count += 1"
            },
            "62": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 231,
                "PatchRowcode": "             retry_seconds = None"
            },
            "63": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 232,
                "PatchRowcode": "             exc_info = None"
            },
            "64": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 233,
                "PatchRowcode": " "
            },
            "65": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "             try:"
            },
            "66": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                response = await request()"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 235,
                "PatchRowcode": "+                response = await send(request, *send_args, **send_kwargs)"
            },
            "68": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 236,
                "PatchRowcode": "             except retry_exceptions:  # type: ignore"
            },
            "69": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 237,
                "PatchRowcode": "                 if try_count > PREFECT_CLIENT_MAX_RETRIES.value():"
            },
            "70": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 238,
                "PatchRowcode": "                     raise"
            },
            "71": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "                 # Otherwise, we will ignore this error but capture the info for logging"
            },
            "72": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": 240,
                "PatchRowcode": "                 exc_info = sys.exc_info()"
            },
            "73": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "             else:"
            },
            "74": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # We got a response; return immediately if it is not retryable"
            },
            "75": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                if response.status_code not in retry_codes:"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 242,
                "PatchRowcode": "+                # We got a response; check if it's a CSRF error, otherwise"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 243,
                "PatchRowcode": "+                # return immediately if it is not retryable"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+                if ("
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 245,
                "PatchRowcode": "+                    response.status_code == status.HTTP_403_FORBIDDEN"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+                    and \"Invalid CSRF token\" in response.text"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+                ):"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+                    # We got a CSRF error, clear the token and try again"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 249,
                "PatchRowcode": "+                    self.csrf_token = None"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 250,
                "PatchRowcode": "+                    await self._add_csrf_headers(request)"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 251,
                "PatchRowcode": "+                elif response.status_code not in retry_codes:"
            },
            "86": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": 252,
                "PatchRowcode": "                     return response"
            },
            "87": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 253,
                "PatchRowcode": " "
            },
            "88": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 254,
                "PatchRowcode": "                 if \"Retry-After\" in response.headers:"
            },
            "89": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 295,
                "PatchRowcode": "         # We ran out of retries, return the failed response"
            },
            "90": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": 296,
                "PatchRowcode": "         return response"
            },
            "91": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 297,
                "PatchRowcode": " "
            },
            "92": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    async def send(self, *args, **kwargs) -> Response:"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+    async def send(self, request: Request, *args, **kwargs) -> Response:"
            },
            "94": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 299,
                "PatchRowcode": "         \"\"\""
            },
            "95": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "         Send a request with automatic retry behavior for the following status codes:"
            },
            "96": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": 301,
                "PatchRowcode": " "
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+        - 403 Forbidden, if the request failed due to CSRF protection"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+        - 408 Request Timeout"
            },
            "99": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "         - 429 CloudFlare-style rate limiting"
            },
            "100": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "         - 502 Bad Gateway"
            },
            "101": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "         - 503 Service unavailable"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 307,
                "PatchRowcode": "+        - Any additional status codes provided in `PREFECT_CLIENT_RETRY_EXTRA_CODES`"
            },
            "103": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": 308,
                "PatchRowcode": "         \"\"\""
            },
            "104": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 309,
                "PatchRowcode": " "
            },
            "105": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        api_request = partial(super().send, *args, **kwargs)"
            },
            "106": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+        super_send = super().send"
            },
            "108": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 311,
                "PatchRowcode": "         response = await self._send_with_retry("
            },
            "109": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            request=api_request,"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+            request=request,"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 313,
                "PatchRowcode": "+            send=super_send,"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 314,
                "PatchRowcode": "+            send_args=args,"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 315,
                "PatchRowcode": "+            send_kwargs=kwargs,"
            },
            "114": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 316,
                "PatchRowcode": "             retry_codes={"
            },
            "115": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": 317,
                "PatchRowcode": "                 status.HTTP_429_TOO_MANY_REQUESTS,"
            },
            "116": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": 318,
                "PatchRowcode": "                 status.HTTP_503_SERVICE_UNAVAILABLE,"
            },
            "117": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 344,
                "PatchRowcode": "         response.raise_for_status()"
            },
            "118": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 345,
                "PatchRowcode": " "
            },
            "119": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "         return response"
            },
            "120": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 347,
                "PatchRowcode": "+"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 348,
                "PatchRowcode": "+    async def _add_csrf_headers(self, request: Request):"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 349,
                "PatchRowcode": "+        now = datetime.now(timezone.utc)"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 350,
                "PatchRowcode": "+"
            },
            "124": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 351,
                "PatchRowcode": "+        if not self.enable_csrf_support:"
            },
            "125": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 352,
                "PatchRowcode": "+            return"
            },
            "126": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 353,
                "PatchRowcode": "+"
            },
            "127": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 354,
                "PatchRowcode": "+        if not self.csrf_token or ("
            },
            "128": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 355,
                "PatchRowcode": "+            self.csrf_token_expiration and now > self.csrf_token_expiration"
            },
            "129": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 356,
                "PatchRowcode": "+        ):"
            },
            "130": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 357,
                "PatchRowcode": "+            token_request = self.build_request("
            },
            "131": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 358,
                "PatchRowcode": "+                \"GET\", f\"/csrf-token?client={self.csrf_client_id}\""
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 359,
                "PatchRowcode": "+            )"
            },
            "133": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 360,
                "PatchRowcode": "+"
            },
            "134": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 361,
                "PatchRowcode": "+            try:"
            },
            "135": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 362,
                "PatchRowcode": "+                token_response = await self.send(token_request)"
            },
            "136": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 363,
                "PatchRowcode": "+            except PrefectHTTPStatusError as exc:"
            },
            "137": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 364,
                "PatchRowcode": "+                if exc.response.status_code == status.HTTP_404_NOT_FOUND:"
            },
            "138": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 365,
                "PatchRowcode": "+                    # The token endpoint is not available, likely an older"
            },
            "139": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 366,
                "PatchRowcode": "+                    # server version; disable CSRF support"
            },
            "140": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 367,
                "PatchRowcode": "+                    self.enable_csrf_support = False"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 368,
                "PatchRowcode": "+                    return"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 369,
                "PatchRowcode": "+"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 370,
                "PatchRowcode": "+                raise"
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 371,
                "PatchRowcode": "+"
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 372,
                "PatchRowcode": "+            token: CsrfToken = CsrfToken.parse_obj(token_response.json())"
            },
            "146": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 373,
                "PatchRowcode": "+            self.csrf_token = token.token"
            },
            "147": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 374,
                "PatchRowcode": "+            self.csrf_token_expiration = token.expiration"
            },
            "148": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 375,
                "PatchRowcode": "+"
            },
            "149": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 376,
                "PatchRowcode": "+        request.headers[\"Prefect-Csrf-Token\"] = self.csrf_token"
            },
            "150": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 377,
                "PatchRowcode": "+        request.headers[\"Prefect-Csrf-Client\"] = str(self.csrf_client_id)"
            }
        },
        "frontPatchFile": [
            "import copy",
            "import sys",
            "import threading",
            "from collections import defaultdict",
            "from contextlib import asynccontextmanager",
            "from functools import partial",
            "from typing import (",
            "    Any,",
            "    AsyncGenerator,",
            "    Awaitable,",
            "    Callable,",
            "    Dict,",
            "    MutableMapping,",
            "    Protocol,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    runtime_checkable,",
            ")",
            "",
            "import anyio",
            "import httpx",
            "from asgi_lifespan import LifespanManager",
            "from httpx import HTTPStatusError, Response",
            "from prefect._vendor.starlette import status",
            "from typing_extensions import Self",
            "",
            "from prefect.exceptions import PrefectHTTPStatusError",
            "from prefect.logging import get_logger",
            "from prefect.settings import (",
            "    PREFECT_CLIENT_MAX_RETRIES,",
            "    PREFECT_CLIENT_RETRY_EXTRA_CODES,",
            "    PREFECT_CLIENT_RETRY_JITTER_FACTOR,",
            ")",
            "from prefect.utilities.math import bounded_poisson_interval, clamped_poisson_interval",
            "",
            "# Datastores for lifespan management, keys should be a tuple of thread and app",
            "# identities.",
            "APP_LIFESPANS: Dict[Tuple[int, int], LifespanManager] = {}",
            "APP_LIFESPANS_REF_COUNTS: Dict[Tuple[int, int], int] = {}",
            "# Blocks concurrent access to the above dicts per thread. The index should be the thread",
            "# identity.",
            "APP_LIFESPANS_LOCKS: Dict[int, anyio.Lock] = defaultdict(anyio.Lock)",
            "",
            "",
            "logger = get_logger(\"client\")",
            "",
            "",
            "# Define ASGI application types for type checking",
            "Scope = MutableMapping[str, Any]",
            "Message = MutableMapping[str, Any]",
            "",
            "Receive = Callable[[], Awaitable[Message]]",
            "Send = Callable[[Message], Awaitable[None]]",
            "",
            "",
            "@runtime_checkable",
            "class ASGIApp(Protocol):",
            "    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:",
            "        ...",
            "",
            "",
            "@asynccontextmanager",
            "async def app_lifespan_context(app: ASGIApp) -> AsyncGenerator[None, None]:",
            "    \"\"\"",
            "    A context manager that calls startup/shutdown hooks for the given application.",
            "",
            "    Lifespan contexts are cached per application to avoid calling the lifespan hooks",
            "    more than once if the context is entered in nested code. A no-op context will be",
            "    returned if the context for the given application is already being managed.",
            "",
            "    This manager is robust to concurrent access within the event loop. For example,",
            "    if you have concurrent contexts for the same application, it is guaranteed that",
            "    startup hooks will be called before their context starts and shutdown hooks will",
            "    only be called after their context exits.",
            "",
            "    A reference count is used to support nested use of clients without running",
            "    lifespan hooks excessively. The first client context entered will create and enter",
            "    a lifespan context. Each subsequent client will increment a reference count but will",
            "    not create a new lifespan context. When each client context exits, the reference",
            "    count is decremented. When the last client context exits, the lifespan will be",
            "    closed.",
            "",
            "    In simple nested cases, the first client context will be the one to exit the",
            "    lifespan. However, if client contexts are entered concurrently they may not exit",
            "    in a consistent order. If the first client context was responsible for closing",
            "    the lifespan, it would have to wait until all other client contexts to exit to",
            "    avoid firing shutdown hooks while the application is in use. Waiting for the other",
            "    clients to exit can introduce deadlocks, so, instead, the first client will exit",
            "    without closing the lifespan context and reference counts will be used to ensure",
            "    the lifespan is closed once all of the clients are done.",
            "    \"\"\"",
            "    # TODO: A deadlock has been observed during multithreaded use of clients while this",
            "    #       lifespan context is being used. This has only been reproduced on Python 3.7",
            "    #       and while we hope to discourage using multiple event loops in threads, this",
            "    #       bug may emerge again.",
            "    #       See https://github.com/PrefectHQ/orion/pull/1696",
            "    thread_id = threading.get_ident()",
            "",
            "    # The id of the application is used instead of the hash so each application instance",
            "    # is managed independently even if they share the same settings. We include the",
            "    # thread id since applications are managed separately per thread.",
            "    key = (thread_id, id(app))",
            "",
            "    # On exception, this will be populated with exception details",
            "    exc_info = (None, None, None)",
            "",
            "    # Get a lock unique to this thread since anyio locks are not threadsafe",
            "    lock = APP_LIFESPANS_LOCKS[thread_id]",
            "",
            "    async with lock:",
            "        if key in APP_LIFESPANS:",
            "            # The lifespan is already being managed, just increment the reference count",
            "            APP_LIFESPANS_REF_COUNTS[key] += 1",
            "        else:",
            "            # Create a new lifespan manager",
            "            APP_LIFESPANS[key] = context = LifespanManager(",
            "                app, startup_timeout=30, shutdown_timeout=30",
            "            )",
            "            APP_LIFESPANS_REF_COUNTS[key] = 1",
            "",
            "            # Ensure we enter the context before releasing the lock so startup hooks",
            "            # are complete before another client can be used",
            "            await context.__aenter__()",
            "",
            "    try:",
            "        yield",
            "    except BaseException:",
            "        exc_info = sys.exc_info()",
            "        raise",
            "    finally:",
            "        # If we do not shield against anyio cancellation, the lock will return",
            "        # immediately and the code in its context will not run, leaving the lifespan",
            "        # open",
            "        with anyio.CancelScope(shield=True):",
            "            async with lock:",
            "                # After the consumer exits the context, decrement the reference count",
            "                APP_LIFESPANS_REF_COUNTS[key] -= 1",
            "",
            "                # If this the last context to exit, close the lifespan",
            "                if APP_LIFESPANS_REF_COUNTS[key] <= 0:",
            "                    APP_LIFESPANS_REF_COUNTS.pop(key)",
            "                    context = APP_LIFESPANS.pop(key)",
            "                    await context.__aexit__(*exc_info)",
            "",
            "",
            "class PrefectResponse(httpx.Response):",
            "    \"\"\"",
            "    A Prefect wrapper for the `httpx.Response` class.",
            "",
            "    Provides more informative error messages.",
            "    \"\"\"",
            "",
            "    def raise_for_status(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the response contains an HTTPStatusError.",
            "",
            "        The `PrefectHTTPStatusError` contains useful additional information that",
            "        is not contained in the `HTTPStatusError`.",
            "        \"\"\"",
            "        try:",
            "            return super().raise_for_status()",
            "        except HTTPStatusError as exc:",
            "            raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__",
            "",
            "    @classmethod",
            "    def from_httpx_response(cls: Type[Self], response: httpx.Response) -> Self:",
            "        \"\"\"",
            "        Create a `PrefectReponse` from an `httpx.Response`.",
            "",
            "        By changing the `__class__` attribute of the Response, we change the method",
            "        resolution order to look for methods defined in PrefectResponse, while leaving",
            "        everything else about the original Response instance intact.",
            "        \"\"\"",
            "        new_response = copy.copy(response)",
            "        new_response.__class__ = cls",
            "        return new_response",
            "",
            "",
            "class PrefectHttpxClient(httpx.AsyncClient):",
            "    \"\"\"",
            "    A Prefect wrapper for the async httpx client with support for retry-after headers",
            "    for the provided status codes (typically 429, 502 and 503).",
            "",
            "    Additionally, this client will always call `raise_for_status` on responses.",
            "",
            "    For more details on rate limit headers, see:",
            "    [Configuring Cloudflare Rate Limiting](https://support.cloudflare.com/hc/en-us/articles/115001635128-Configuring-Rate-Limiting-from-UI)",
            "    \"\"\"",
            "",
            "    async def _send_with_retry(",
            "        self,",
            "        request: Callable,",
            "        retry_codes: Set[int] = set(),",
            "        retry_exceptions: Tuple[Exception, ...] = tuple(),",
            "    ):",
            "        \"\"\"",
            "        Send a request and retry it if it fails.",
            "",
            "        Sends the provided request and retries it up to PREFECT_CLIENT_MAX_RETRIES times",
            "        if the request either raises an exception listed in `retry_exceptions` or",
            "        receives a response with a status code listed in `retry_codes`.",
            "",
            "        Retries will be delayed based on either the retry header (preferred) or",
            "        exponential backoff if a retry header is not provided.",
            "        \"\"\"",
            "        try_count = 0",
            "        response = None",
            "",
            "        while try_count <= PREFECT_CLIENT_MAX_RETRIES.value():",
            "            try_count += 1",
            "            retry_seconds = None",
            "            exc_info = None",
            "",
            "            try:",
            "                response = await request()",
            "            except retry_exceptions:  # type: ignore",
            "                if try_count > PREFECT_CLIENT_MAX_RETRIES.value():",
            "                    raise",
            "                # Otherwise, we will ignore this error but capture the info for logging",
            "                exc_info = sys.exc_info()",
            "            else:",
            "                # We got a response; return immediately if it is not retryable",
            "                if response.status_code not in retry_codes:",
            "                    return response",
            "",
            "                if \"Retry-After\" in response.headers:",
            "                    retry_seconds = float(response.headers[\"Retry-After\"])",
            "",
            "            # Use an exponential back-off if not set in a header",
            "            if retry_seconds is None:",
            "                retry_seconds = 2**try_count",
            "",
            "            # Add jitter",
            "            jitter_factor = PREFECT_CLIENT_RETRY_JITTER_FACTOR.value()",
            "            if retry_seconds > 0 and jitter_factor > 0:",
            "                if response is not None and \"Retry-After\" in response.headers:",
            "                    # Always wait for _at least_ retry seconds if requested by the API",
            "                    retry_seconds = bounded_poisson_interval(",
            "                        retry_seconds, retry_seconds * (1 + jitter_factor)",
            "                    )",
            "                else:",
            "                    # Otherwise, use a symmetrical jitter",
            "                    retry_seconds = clamped_poisson_interval(",
            "                        retry_seconds, jitter_factor",
            "                    )",
            "",
            "            logger.debug(",
            "                (",
            "                    \"Encountered retryable exception during request. \"",
            "                    if exc_info",
            "                    else (",
            "                        \"Received response with retryable status code\"",
            "                        f\" {response.status_code}. \"",
            "                    )",
            "                )",
            "                + f\"Another attempt will be made in {retry_seconds}s. \"",
            "                \"This is attempt\"",
            "                f\" {try_count}/{PREFECT_CLIENT_MAX_RETRIES.value() + 1}.\",",
            "                exc_info=exc_info,",
            "            )",
            "            await anyio.sleep(retry_seconds)",
            "",
            "        assert (",
            "            response is not None",
            "        ), \"Retry handling ended without response or exception\"",
            "",
            "        # We ran out of retries, return the failed response",
            "        return response",
            "",
            "    async def send(self, *args, **kwargs) -> Response:",
            "        \"\"\"",
            "        Send a request with automatic retry behavior for the following status codes:",
            "",
            "        - 429 CloudFlare-style rate limiting",
            "        - 502 Bad Gateway",
            "        - 503 Service unavailable",
            "        \"\"\"",
            "",
            "        api_request = partial(super().send, *args, **kwargs)",
            "",
            "        response = await self._send_with_retry(",
            "            request=api_request,",
            "            retry_codes={",
            "                status.HTTP_429_TOO_MANY_REQUESTS,",
            "                status.HTTP_503_SERVICE_UNAVAILABLE,",
            "                status.HTTP_502_BAD_GATEWAY,",
            "                status.HTTP_408_REQUEST_TIMEOUT,",
            "                *PREFECT_CLIENT_RETRY_EXTRA_CODES.value(),",
            "            },",
            "            retry_exceptions=(",
            "                httpx.ReadTimeout,",
            "                httpx.PoolTimeout,",
            "                httpx.ConnectTimeout,",
            "                # `ConnectionResetError` when reading socket raises as a `ReadError`",
            "                httpx.ReadError,",
            "                # Sockets can be closed during writes resulting in a `WriteError`",
            "                httpx.WriteError,",
            "                # Uvicorn bug, see https://github.com/PrefectHQ/prefect/issues/7512",
            "                httpx.RemoteProtocolError,",
            "                # HTTP2 bug, see https://github.com/PrefectHQ/prefect/issues/7442",
            "                httpx.LocalProtocolError,",
            "            ),",
            "        )",
            "",
            "        # Convert to a Prefect response to add nicer errors messages",
            "        response = PrefectResponse.from_httpx_response(response)",
            "",
            "        # Always raise bad responses",
            "        # NOTE: We may want to remove this and handle responses per route in the",
            "        #       `PrefectClient`",
            "        response.raise_for_status()",
            "",
            "        return response"
        ],
        "afterPatchFile": [
            "import copy",
            "import sys",
            "import threading",
            "import uuid",
            "from collections import defaultdict",
            "from contextlib import asynccontextmanager",
            "from datetime import datetime, timezone",
            "from typing import (",
            "    Any,",
            "    AsyncGenerator,",
            "    Awaitable,",
            "    Callable,",
            "    Dict,",
            "    MutableMapping,",
            "    Optional,",
            "    Protocol,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    runtime_checkable,",
            ")",
            "",
            "import anyio",
            "import httpx",
            "from asgi_lifespan import LifespanManager",
            "from httpx import HTTPStatusError, Request, Response",
            "from prefect._vendor.starlette import status",
            "from typing_extensions import Self",
            "",
            "from prefect.client.schemas.objects import CsrfToken",
            "from prefect.exceptions import PrefectHTTPStatusError",
            "from prefect.logging import get_logger",
            "from prefect.settings import (",
            "    PREFECT_CLIENT_MAX_RETRIES,",
            "    PREFECT_CLIENT_RETRY_EXTRA_CODES,",
            "    PREFECT_CLIENT_RETRY_JITTER_FACTOR,",
            ")",
            "from prefect.utilities.math import bounded_poisson_interval, clamped_poisson_interval",
            "",
            "# Datastores for lifespan management, keys should be a tuple of thread and app",
            "# identities.",
            "APP_LIFESPANS: Dict[Tuple[int, int], LifespanManager] = {}",
            "APP_LIFESPANS_REF_COUNTS: Dict[Tuple[int, int], int] = {}",
            "# Blocks concurrent access to the above dicts per thread. The index should be the thread",
            "# identity.",
            "APP_LIFESPANS_LOCKS: Dict[int, anyio.Lock] = defaultdict(anyio.Lock)",
            "",
            "",
            "logger = get_logger(\"client\")",
            "",
            "",
            "# Define ASGI application types for type checking",
            "Scope = MutableMapping[str, Any]",
            "Message = MutableMapping[str, Any]",
            "",
            "Receive = Callable[[], Awaitable[Message]]",
            "Send = Callable[[Message], Awaitable[None]]",
            "",
            "",
            "@runtime_checkable",
            "class ASGIApp(Protocol):",
            "    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:",
            "        ...",
            "",
            "",
            "@asynccontextmanager",
            "async def app_lifespan_context(app: ASGIApp) -> AsyncGenerator[None, None]:",
            "    \"\"\"",
            "    A context manager that calls startup/shutdown hooks for the given application.",
            "",
            "    Lifespan contexts are cached per application to avoid calling the lifespan hooks",
            "    more than once if the context is entered in nested code. A no-op context will be",
            "    returned if the context for the given application is already being managed.",
            "",
            "    This manager is robust to concurrent access within the event loop. For example,",
            "    if you have concurrent contexts for the same application, it is guaranteed that",
            "    startup hooks will be called before their context starts and shutdown hooks will",
            "    only be called after their context exits.",
            "",
            "    A reference count is used to support nested use of clients without running",
            "    lifespan hooks excessively. The first client context entered will create and enter",
            "    a lifespan context. Each subsequent client will increment a reference count but will",
            "    not create a new lifespan context. When each client context exits, the reference",
            "    count is decremented. When the last client context exits, the lifespan will be",
            "    closed.",
            "",
            "    In simple nested cases, the first client context will be the one to exit the",
            "    lifespan. However, if client contexts are entered concurrently they may not exit",
            "    in a consistent order. If the first client context was responsible for closing",
            "    the lifespan, it would have to wait until all other client contexts to exit to",
            "    avoid firing shutdown hooks while the application is in use. Waiting for the other",
            "    clients to exit can introduce deadlocks, so, instead, the first client will exit",
            "    without closing the lifespan context and reference counts will be used to ensure",
            "    the lifespan is closed once all of the clients are done.",
            "    \"\"\"",
            "    # TODO: A deadlock has been observed during multithreaded use of clients while this",
            "    #       lifespan context is being used. This has only been reproduced on Python 3.7",
            "    #       and while we hope to discourage using multiple event loops in threads, this",
            "    #       bug may emerge again.",
            "    #       See https://github.com/PrefectHQ/orion/pull/1696",
            "    thread_id = threading.get_ident()",
            "",
            "    # The id of the application is used instead of the hash so each application instance",
            "    # is managed independently even if they share the same settings. We include the",
            "    # thread id since applications are managed separately per thread.",
            "    key = (thread_id, id(app))",
            "",
            "    # On exception, this will be populated with exception details",
            "    exc_info = (None, None, None)",
            "",
            "    # Get a lock unique to this thread since anyio locks are not threadsafe",
            "    lock = APP_LIFESPANS_LOCKS[thread_id]",
            "",
            "    async with lock:",
            "        if key in APP_LIFESPANS:",
            "            # The lifespan is already being managed, just increment the reference count",
            "            APP_LIFESPANS_REF_COUNTS[key] += 1",
            "        else:",
            "            # Create a new lifespan manager",
            "            APP_LIFESPANS[key] = context = LifespanManager(",
            "                app, startup_timeout=30, shutdown_timeout=30",
            "            )",
            "            APP_LIFESPANS_REF_COUNTS[key] = 1",
            "",
            "            # Ensure we enter the context before releasing the lock so startup hooks",
            "            # are complete before another client can be used",
            "            await context.__aenter__()",
            "",
            "    try:",
            "        yield",
            "    except BaseException:",
            "        exc_info = sys.exc_info()",
            "        raise",
            "    finally:",
            "        # If we do not shield against anyio cancellation, the lock will return",
            "        # immediately and the code in its context will not run, leaving the lifespan",
            "        # open",
            "        with anyio.CancelScope(shield=True):",
            "            async with lock:",
            "                # After the consumer exits the context, decrement the reference count",
            "                APP_LIFESPANS_REF_COUNTS[key] -= 1",
            "",
            "                # If this the last context to exit, close the lifespan",
            "                if APP_LIFESPANS_REF_COUNTS[key] <= 0:",
            "                    APP_LIFESPANS_REF_COUNTS.pop(key)",
            "                    context = APP_LIFESPANS.pop(key)",
            "                    await context.__aexit__(*exc_info)",
            "",
            "",
            "class PrefectResponse(httpx.Response):",
            "    \"\"\"",
            "    A Prefect wrapper for the `httpx.Response` class.",
            "",
            "    Provides more informative error messages.",
            "    \"\"\"",
            "",
            "    def raise_for_status(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the response contains an HTTPStatusError.",
            "",
            "        The `PrefectHTTPStatusError` contains useful additional information that",
            "        is not contained in the `HTTPStatusError`.",
            "        \"\"\"",
            "        try:",
            "            return super().raise_for_status()",
            "        except HTTPStatusError as exc:",
            "            raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__",
            "",
            "    @classmethod",
            "    def from_httpx_response(cls: Type[Self], response: httpx.Response) -> Self:",
            "        \"\"\"",
            "        Create a `PrefectReponse` from an `httpx.Response`.",
            "",
            "        By changing the `__class__` attribute of the Response, we change the method",
            "        resolution order to look for methods defined in PrefectResponse, while leaving",
            "        everything else about the original Response instance intact.",
            "        \"\"\"",
            "        new_response = copy.copy(response)",
            "        new_response.__class__ = cls",
            "        return new_response",
            "",
            "",
            "class PrefectHttpxClient(httpx.AsyncClient):",
            "    \"\"\"",
            "    A Prefect wrapper for the async httpx client with support for retry-after headers",
            "    for the provided status codes (typically 429, 502 and 503).",
            "",
            "    Additionally, this client will always call `raise_for_status` on responses.",
            "",
            "    For more details on rate limit headers, see:",
            "    [Configuring Cloudflare Rate Limiting](https://support.cloudflare.com/hc/en-us/articles/115001635128-Configuring-Rate-Limiting-from-UI)",
            "    \"\"\"",
            "",
            "    def __init__(self, *args, enable_csrf_support: bool = False, **kwargs):",
            "        self.enable_csrf_support: bool = enable_csrf_support",
            "        self.csrf_token: Optional[str] = None",
            "        self.csrf_token_expiration: Optional[datetime] = None",
            "        self.csrf_client_id: uuid.UUID = uuid.uuid4()",
            "",
            "        super().__init__(*args, **kwargs)",
            "",
            "    async def _send_with_retry(",
            "        self,",
            "        request: Request,",
            "        send: Callable[[Request], Awaitable[Response]],",
            "        send_args: Tuple,",
            "        send_kwargs: Dict,",
            "        retry_codes: Set[int] = set(),",
            "        retry_exceptions: Tuple[Exception, ...] = tuple(),",
            "    ):",
            "        \"\"\"",
            "        Send a request and retry it if it fails.",
            "",
            "        Sends the provided request and retries it up to PREFECT_CLIENT_MAX_RETRIES times",
            "        if the request either raises an exception listed in `retry_exceptions` or",
            "        receives a response with a status code listed in `retry_codes`.",
            "",
            "        Retries will be delayed based on either the retry header (preferred) or",
            "        exponential backoff if a retry header is not provided.",
            "        \"\"\"",
            "        try_count = 0",
            "        response = None",
            "",
            "        is_change_request = request.method.lower() in {\"post\", \"put\", \"patch\", \"delete\"}",
            "",
            "        if self.enable_csrf_support and is_change_request:",
            "            await self._add_csrf_headers(request=request)",
            "",
            "        while try_count <= PREFECT_CLIENT_MAX_RETRIES.value():",
            "            try_count += 1",
            "            retry_seconds = None",
            "            exc_info = None",
            "",
            "            try:",
            "                response = await send(request, *send_args, **send_kwargs)",
            "            except retry_exceptions:  # type: ignore",
            "                if try_count > PREFECT_CLIENT_MAX_RETRIES.value():",
            "                    raise",
            "                # Otherwise, we will ignore this error but capture the info for logging",
            "                exc_info = sys.exc_info()",
            "            else:",
            "                # We got a response; check if it's a CSRF error, otherwise",
            "                # return immediately if it is not retryable",
            "                if (",
            "                    response.status_code == status.HTTP_403_FORBIDDEN",
            "                    and \"Invalid CSRF token\" in response.text",
            "                ):",
            "                    # We got a CSRF error, clear the token and try again",
            "                    self.csrf_token = None",
            "                    await self._add_csrf_headers(request)",
            "                elif response.status_code not in retry_codes:",
            "                    return response",
            "",
            "                if \"Retry-After\" in response.headers:",
            "                    retry_seconds = float(response.headers[\"Retry-After\"])",
            "",
            "            # Use an exponential back-off if not set in a header",
            "            if retry_seconds is None:",
            "                retry_seconds = 2**try_count",
            "",
            "            # Add jitter",
            "            jitter_factor = PREFECT_CLIENT_RETRY_JITTER_FACTOR.value()",
            "            if retry_seconds > 0 and jitter_factor > 0:",
            "                if response is not None and \"Retry-After\" in response.headers:",
            "                    # Always wait for _at least_ retry seconds if requested by the API",
            "                    retry_seconds = bounded_poisson_interval(",
            "                        retry_seconds, retry_seconds * (1 + jitter_factor)",
            "                    )",
            "                else:",
            "                    # Otherwise, use a symmetrical jitter",
            "                    retry_seconds = clamped_poisson_interval(",
            "                        retry_seconds, jitter_factor",
            "                    )",
            "",
            "            logger.debug(",
            "                (",
            "                    \"Encountered retryable exception during request. \"",
            "                    if exc_info",
            "                    else (",
            "                        \"Received response with retryable status code\"",
            "                        f\" {response.status_code}. \"",
            "                    )",
            "                )",
            "                + f\"Another attempt will be made in {retry_seconds}s. \"",
            "                \"This is attempt\"",
            "                f\" {try_count}/{PREFECT_CLIENT_MAX_RETRIES.value() + 1}.\",",
            "                exc_info=exc_info,",
            "            )",
            "            await anyio.sleep(retry_seconds)",
            "",
            "        assert (",
            "            response is not None",
            "        ), \"Retry handling ended without response or exception\"",
            "",
            "        # We ran out of retries, return the failed response",
            "        return response",
            "",
            "    async def send(self, request: Request, *args, **kwargs) -> Response:",
            "        \"\"\"",
            "        Send a request with automatic retry behavior for the following status codes:",
            "",
            "        - 403 Forbidden, if the request failed due to CSRF protection",
            "        - 408 Request Timeout",
            "        - 429 CloudFlare-style rate limiting",
            "        - 502 Bad Gateway",
            "        - 503 Service unavailable",
            "        - Any additional status codes provided in `PREFECT_CLIENT_RETRY_EXTRA_CODES`",
            "        \"\"\"",
            "",
            "        super_send = super().send",
            "        response = await self._send_with_retry(",
            "            request=request,",
            "            send=super_send,",
            "            send_args=args,",
            "            send_kwargs=kwargs,",
            "            retry_codes={",
            "                status.HTTP_429_TOO_MANY_REQUESTS,",
            "                status.HTTP_503_SERVICE_UNAVAILABLE,",
            "                status.HTTP_502_BAD_GATEWAY,",
            "                status.HTTP_408_REQUEST_TIMEOUT,",
            "                *PREFECT_CLIENT_RETRY_EXTRA_CODES.value(),",
            "            },",
            "            retry_exceptions=(",
            "                httpx.ReadTimeout,",
            "                httpx.PoolTimeout,",
            "                httpx.ConnectTimeout,",
            "                # `ConnectionResetError` when reading socket raises as a `ReadError`",
            "                httpx.ReadError,",
            "                # Sockets can be closed during writes resulting in a `WriteError`",
            "                httpx.WriteError,",
            "                # Uvicorn bug, see https://github.com/PrefectHQ/prefect/issues/7512",
            "                httpx.RemoteProtocolError,",
            "                # HTTP2 bug, see https://github.com/PrefectHQ/prefect/issues/7442",
            "                httpx.LocalProtocolError,",
            "            ),",
            "        )",
            "",
            "        # Convert to a Prefect response to add nicer errors messages",
            "        response = PrefectResponse.from_httpx_response(response)",
            "",
            "        # Always raise bad responses",
            "        # NOTE: We may want to remove this and handle responses per route in the",
            "        #       `PrefectClient`",
            "        response.raise_for_status()",
            "",
            "        return response",
            "",
            "    async def _add_csrf_headers(self, request: Request):",
            "        now = datetime.now(timezone.utc)",
            "",
            "        if not self.enable_csrf_support:",
            "            return",
            "",
            "        if not self.csrf_token or (",
            "            self.csrf_token_expiration and now > self.csrf_token_expiration",
            "        ):",
            "            token_request = self.build_request(",
            "                \"GET\", f\"/csrf-token?client={self.csrf_client_id}\"",
            "            )",
            "",
            "            try:",
            "                token_response = await self.send(token_request)",
            "            except PrefectHTTPStatusError as exc:",
            "                if exc.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                    # The token endpoint is not available, likely an older",
            "                    # server version; disable CSRF support",
            "                    self.enable_csrf_support = False",
            "                    return",
            "",
            "                raise",
            "",
            "            token: CsrfToken = CsrfToken.parse_obj(token_response.json())",
            "            self.csrf_token = token.token",
            "            self.csrf_token_expiration = token.expiration",
            "",
            "        request.headers[\"Prefect-Csrf-Token\"] = self.csrf_token",
            "        request.headers[\"Prefect-Csrf-Client\"] = str(self.csrf_client_id)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "6": [],
            "24": [],
            "193": [
                "PrefectHttpxClient"
            ],
            "216": [
                "PrefectHttpxClient"
            ],
            "223": [
                "PrefectHttpxClient"
            ],
            "224": [
                "PrefectHttpxClient"
            ],
            "271": [
                "PrefectHttpxClient"
            ],
            "280": [
                "PrefectHttpxClient"
            ],
            "281": [
                "PrefectHttpxClient"
            ],
            "283": [
                "PrefectHttpxClient"
            ]
        },
        "addLocation": []
    },
    "src/prefect/client/cloud.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "         httpx_settings.setdefault(\"base_url\", host)"
            },
            "1": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "         if not PREFECT_UNIT_TEST_MODE.value():"
            },
            "2": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "             httpx_settings.setdefault(\"follow_redirects\", True)"
            },
            "3": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._client = PrefectHttpxClient(**httpx_settings)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        self._client = PrefectHttpxClient(**httpx_settings, enable_csrf_support=False)"
            },
            "5": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 76,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     async def api_healthcheck(self):"
            },
            "7": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "         \"\"\""
            }
        },
        "frontPatchFile": [
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "import anyio",
            "import httpx",
            "",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "",
            "if HAS_PYDANTIC_V2:",
            "    import pydantic.v1 as pydantic",
            "else:",
            "    import pydantic",
            "",
            "from prefect._vendor.starlette import status",
            "",
            "import prefect.context",
            "import prefect.settings",
            "from prefect.client.base import PrefectHttpxClient",
            "from prefect.client.schemas import Workspace",
            "from prefect.exceptions import PrefectException",
            "from prefect.settings import (",
            "    PREFECT_API_KEY,",
            "    PREFECT_CLOUD_API_URL,",
            "    PREFECT_UNIT_TEST_MODE,",
            ")",
            "",
            "PARSE_API_URL_REGEX = re.compile(r\"accounts/(.{36})/workspaces/(.{36})\")",
            "",
            "",
            "def get_cloud_client(",
            "    host: Optional[str] = None,",
            "    api_key: Optional[str] = None,",
            "    httpx_settings: Optional[dict] = None,",
            "    infer_cloud_url: bool = False,",
            ") -> \"CloudClient\":",
            "    \"\"\"",
            "    Needs a docstring.",
            "    \"\"\"",
            "    if httpx_settings is not None:",
            "        httpx_settings = httpx_settings.copy()",
            "",
            "    if infer_cloud_url is False:",
            "        host = host or PREFECT_CLOUD_API_URL.value()",
            "    else:",
            "        configured_url = prefect.settings.PREFECT_API_URL.value()",
            "        host = re.sub(PARSE_API_URL_REGEX, \"\", configured_url)",
            "",
            "    return CloudClient(",
            "        host=host,",
            "        api_key=api_key or PREFECT_API_KEY.value(),",
            "        httpx_settings=httpx_settings,",
            "    )",
            "",
            "",
            "class CloudUnauthorizedError(PrefectException):",
            "    \"\"\"",
            "    Raised when the CloudClient receives a 401 or 403 from the Cloud API.",
            "    \"\"\"",
            "",
            "",
            "class CloudClient:",
            "    def __init__(",
            "        self,",
            "        host: str,",
            "        api_key: str,",
            "        httpx_settings: dict = None,",
            "    ) -> None:",
            "        httpx_settings = httpx_settings or dict()",
            "        httpx_settings.setdefault(\"headers\", dict())",
            "        httpx_settings[\"headers\"].setdefault(\"Authorization\", f\"Bearer {api_key}\")",
            "",
            "        httpx_settings.setdefault(\"base_url\", host)",
            "        if not PREFECT_UNIT_TEST_MODE.value():",
            "            httpx_settings.setdefault(\"follow_redirects\", True)",
            "        self._client = PrefectHttpxClient(**httpx_settings)",
            "",
            "    async def api_healthcheck(self):",
            "        \"\"\"",
            "        Attempts to connect to the Cloud API and raises the encountered exception if not",
            "        successful.",
            "",
            "        If successful, returns `None`.",
            "        \"\"\"",
            "        with anyio.fail_after(10):",
            "            await self.read_workspaces()",
            "",
            "    async def read_workspaces(self) -> List[Workspace]:",
            "        return pydantic.parse_obj_as(List[Workspace], await self.get(\"/me/workspaces\"))",
            "",
            "    async def read_worker_metadata(self) -> Dict[str, Any]:",
            "        configured_url = prefect.settings.PREFECT_API_URL.value()",
            "        account_id, workspace_id = re.findall(PARSE_API_URL_REGEX, configured_url)[0]",
            "        return await self.get(",
            "            f\"accounts/{account_id}/workspaces/{workspace_id}/collections/work_pool_types\"",
            "        )",
            "",
            "    async def __aenter__(self):",
            "        await self._client.__aenter__()",
            "        return self",
            "",
            "    async def __aexit__(self, *exc_info):",
            "        return await self._client.__aexit__(*exc_info)",
            "",
            "    def __enter__(self):",
            "        raise RuntimeError(",
            "            \"The `CloudClient` must be entered with an async context. Use 'async \"",
            "            \"with CloudClient(...)' not 'with CloudClient(...)'\"",
            "        )",
            "",
            "    def __exit__(self, *_):",
            "        assert False, \"This should never be called but must be defined for __enter__\"",
            "",
            "    async def get(self, route, **kwargs):",
            "        return await self.request(\"GET\", route, **kwargs)",
            "",
            "    async def request(self, method, route, **kwargs):",
            "        try:",
            "            res = await self._client.request(method, route, **kwargs)",
            "            res.raise_for_status()",
            "        except httpx.HTTPStatusError as exc:",
            "            if exc.response.status_code in (",
            "                status.HTTP_401_UNAUTHORIZED,",
            "                status.HTTP_403_FORBIDDEN,",
            "            ):",
            "                raise CloudUnauthorizedError",
            "            else:",
            "                raise exc",
            "",
            "        if res.status_code == status.HTTP_204_NO_CONTENT:",
            "            return",
            "",
            "        return res.json()"
        ],
        "afterPatchFile": [
            "import re",
            "from typing import Any, Dict, List, Optional",
            "",
            "import anyio",
            "import httpx",
            "",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "",
            "if HAS_PYDANTIC_V2:",
            "    import pydantic.v1 as pydantic",
            "else:",
            "    import pydantic",
            "",
            "from prefect._vendor.starlette import status",
            "",
            "import prefect.context",
            "import prefect.settings",
            "from prefect.client.base import PrefectHttpxClient",
            "from prefect.client.schemas import Workspace",
            "from prefect.exceptions import PrefectException",
            "from prefect.settings import (",
            "    PREFECT_API_KEY,",
            "    PREFECT_CLOUD_API_URL,",
            "    PREFECT_UNIT_TEST_MODE,",
            ")",
            "",
            "PARSE_API_URL_REGEX = re.compile(r\"accounts/(.{36})/workspaces/(.{36})\")",
            "",
            "",
            "def get_cloud_client(",
            "    host: Optional[str] = None,",
            "    api_key: Optional[str] = None,",
            "    httpx_settings: Optional[dict] = None,",
            "    infer_cloud_url: bool = False,",
            ") -> \"CloudClient\":",
            "    \"\"\"",
            "    Needs a docstring.",
            "    \"\"\"",
            "    if httpx_settings is not None:",
            "        httpx_settings = httpx_settings.copy()",
            "",
            "    if infer_cloud_url is False:",
            "        host = host or PREFECT_CLOUD_API_URL.value()",
            "    else:",
            "        configured_url = prefect.settings.PREFECT_API_URL.value()",
            "        host = re.sub(PARSE_API_URL_REGEX, \"\", configured_url)",
            "",
            "    return CloudClient(",
            "        host=host,",
            "        api_key=api_key or PREFECT_API_KEY.value(),",
            "        httpx_settings=httpx_settings,",
            "    )",
            "",
            "",
            "class CloudUnauthorizedError(PrefectException):",
            "    \"\"\"",
            "    Raised when the CloudClient receives a 401 or 403 from the Cloud API.",
            "    \"\"\"",
            "",
            "",
            "class CloudClient:",
            "    def __init__(",
            "        self,",
            "        host: str,",
            "        api_key: str,",
            "        httpx_settings: dict = None,",
            "    ) -> None:",
            "        httpx_settings = httpx_settings or dict()",
            "        httpx_settings.setdefault(\"headers\", dict())",
            "        httpx_settings[\"headers\"].setdefault(\"Authorization\", f\"Bearer {api_key}\")",
            "",
            "        httpx_settings.setdefault(\"base_url\", host)",
            "        if not PREFECT_UNIT_TEST_MODE.value():",
            "            httpx_settings.setdefault(\"follow_redirects\", True)",
            "        self._client = PrefectHttpxClient(**httpx_settings, enable_csrf_support=False)",
            "",
            "    async def api_healthcheck(self):",
            "        \"\"\"",
            "        Attempts to connect to the Cloud API and raises the encountered exception if not",
            "        successful.",
            "",
            "        If successful, returns `None`.",
            "        \"\"\"",
            "        with anyio.fail_after(10):",
            "            await self.read_workspaces()",
            "",
            "    async def read_workspaces(self) -> List[Workspace]:",
            "        return pydantic.parse_obj_as(List[Workspace], await self.get(\"/me/workspaces\"))",
            "",
            "    async def read_worker_metadata(self) -> Dict[str, Any]:",
            "        configured_url = prefect.settings.PREFECT_API_URL.value()",
            "        account_id, workspace_id = re.findall(PARSE_API_URL_REGEX, configured_url)[0]",
            "        return await self.get(",
            "            f\"accounts/{account_id}/workspaces/{workspace_id}/collections/work_pool_types\"",
            "        )",
            "",
            "    async def __aenter__(self):",
            "        await self._client.__aenter__()",
            "        return self",
            "",
            "    async def __aexit__(self, *exc_info):",
            "        return await self._client.__aexit__(*exc_info)",
            "",
            "    def __enter__(self):",
            "        raise RuntimeError(",
            "            \"The `CloudClient` must be entered with an async context. Use 'async \"",
            "            \"with CloudClient(...)' not 'with CloudClient(...)'\"",
            "        )",
            "",
            "    def __exit__(self, *_):",
            "        assert False, \"This should never be called but must be defined for __enter__\"",
            "",
            "    async def get(self, route, **kwargs):",
            "        return await self.request(\"GET\", route, **kwargs)",
            "",
            "    async def request(self, method, route, **kwargs):",
            "        try:",
            "            res = await self._client.request(method, route, **kwargs)",
            "            res.raise_for_status()",
            "        except httpx.HTTPStatusError as exc:",
            "            if exc.response.status_code in (",
            "                status.HTTP_401_UNAUTHORIZED,",
            "                status.HTTP_403_FORBIDDEN,",
            "            ):",
            "                raise CloudUnauthorizedError",
            "            else:",
            "                raise exc",
            "",
            "        if res.status_code == status.HTTP_204_NO_CONTENT:",
            "            return",
            "",
            "        return res.json()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "75": [
                "CloudClient",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "src/prefect/client/orchestration.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "     PREFECT_API_REQUEST_TIMEOUT,"
            },
            "1": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "     PREFECT_API_TLS_INSECURE_SKIP_VERIFY,"
            },
            "2": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "     PREFECT_API_URL,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+    PREFECT_CLIENT_CSRF_SUPPORT_ENABLED,"
            },
            "4": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 140,
                "PatchRowcode": "     PREFECT_CLOUD_API_URL,"
            },
            "5": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "     PREFECT_UNIT_TEST_MODE,"
            },
            "6": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 142,
                "PatchRowcode": " )"
            },
            "7": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": 317,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 317,
                "afterPatchRowNumber": 318,
                "PatchRowcode": "         if not PREFECT_UNIT_TEST_MODE:"
            },
            "9": {
                "beforePatchRowNumber": 318,
                "afterPatchRowNumber": 319,
                "PatchRowcode": "             httpx_settings.setdefault(\"follow_redirects\", True)"
            },
            "10": {
                "beforePatchRowNumber": 319,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._client = PrefectHttpxClient(**httpx_settings)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 320,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 321,
                "PatchRowcode": "+        enable_csrf_support = ("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 322,
                "PatchRowcode": "+            self.server_type != ServerType.CLOUD"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 323,
                "PatchRowcode": "+            and PREFECT_CLIENT_CSRF_SUPPORT_ENABLED.value()"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 324,
                "PatchRowcode": "+        )"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 325,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 326,
                "PatchRowcode": "+        self._client = PrefectHttpxClient("
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 327,
                "PatchRowcode": "+            **httpx_settings, enable_csrf_support=enable_csrf_support"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 328,
                "PatchRowcode": "+        )"
            },
            "20": {
                "beforePatchRowNumber": 320,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "         self._loop = None"
            },
            "21": {
                "beforePatchRowNumber": 321,
                "afterPatchRowNumber": 330,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 322,
                "afterPatchRowNumber": 331,
                "PatchRowcode": "         # See https://www.python-httpx.org/advanced/#custom-transports"
            }
        },
        "frontPatchFile": [
            "import asyncio",
            "import datetime",
            "import warnings",
            "from contextlib import AsyncExitStack",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Union,",
            ")",
            "from uuid import UUID, uuid4",
            "",
            "import httpcore",
            "import httpx",
            "import pendulum",
            "",
            "from prefect._internal.compatibility.experimental import (",
            "    EXPERIMENTAL_WARNING,",
            "    ExperimentalFeature,",
            "    experiment_enabled,",
            ")",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "from prefect.settings import (",
            "    PREFECT_EXPERIMENTAL_WARN,",
            "    PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES,",
            ")",
            "",
            "if HAS_PYDANTIC_V2:",
            "    import pydantic.v1 as pydantic",
            "else:",
            "    import pydantic",
            "",
            "from asgi_lifespan import LifespanManager",
            "from prefect._vendor.starlette import status",
            "",
            "import prefect",
            "import prefect.exceptions",
            "import prefect.settings",
            "import prefect.states",
            "from prefect.client.constants import SERVER_API_VERSION",
            "from prefect.client.schemas import FlowRun, OrchestrationResult, TaskRun",
            "from prefect.client.schemas.actions import (",
            "    ArtifactCreate,",
            "    BlockDocumentCreate,",
            "    BlockDocumentUpdate,",
            "    BlockSchemaCreate,",
            "    BlockTypeCreate,",
            "    BlockTypeUpdate,",
            "    ConcurrencyLimitCreate,",
            "    DeploymentCreate,",
            "    DeploymentFlowRunCreate,",
            "    DeploymentScheduleCreate,",
            "    DeploymentScheduleUpdate,",
            "    DeploymentUpdate,",
            "    FlowCreate,",
            "    FlowRunCreate,",
            "    FlowRunNotificationPolicyCreate,",
            "    FlowRunNotificationPolicyUpdate,",
            "    FlowRunUpdate,",
            "    GlobalConcurrencyLimitCreate,",
            "    GlobalConcurrencyLimitUpdate,",
            "    LogCreate,",
            "    TaskRunCreate,",
            "    TaskRunUpdate,",
            "    WorkPoolCreate,",
            "    WorkPoolUpdate,",
            "    WorkQueueCreate,",
            "    WorkQueueUpdate,",
            ")",
            "from prefect.client.schemas.filters import (",
            "    ArtifactCollectionFilter,",
            "    ArtifactFilter,",
            "    DeploymentFilter,",
            "    FlowFilter,",
            "    FlowRunFilter,",
            "    FlowRunNotificationPolicyFilter,",
            "    LogFilter,",
            "    TaskRunFilter,",
            "    WorkerFilter,",
            "    WorkPoolFilter,",
            "    WorkQueueFilter,",
            "    WorkQueueFilterName,",
            ")",
            "from prefect.client.schemas.objects import (",
            "    Artifact,",
            "    ArtifactCollection,",
            "    BlockDocument,",
            "    BlockSchema,",
            "    BlockType,",
            "    ConcurrencyLimit,",
            "    Constant,",
            "    Deployment,",
            "    DeploymentSchedule,",
            "    Flow,",
            "    FlowRunInput,",
            "    FlowRunNotificationPolicy,",
            "    FlowRunPolicy,",
            "    Log,",
            "    Parameter,",
            "    QueueFilter,",
            "    TaskRunPolicy,",
            "    TaskRunResult,",
            "    Variable,",
            "    Worker,",
            "    WorkPool,",
            "    WorkQueue,",
            "    WorkQueueStatusDetail,",
            ")",
            "from prefect.client.schemas.responses import (",
            "    DeploymentResponse,",
            "    FlowRunResponse,",
            "    WorkerFlowRunResponse,",
            ")",
            "from prefect.client.schemas.schedules import SCHEDULE_TYPES",
            "from prefect.client.schemas.sorting import (",
            "    ArtifactCollectionSort,",
            "    ArtifactSort,",
            "    DeploymentSort,",
            "    FlowRunSort,",
            "    FlowSort,",
            "    LogSort,",
            "    TaskRunSort,",
            ")",
            "from prefect.deprecated.data_documents import DataDocument",
            "from prefect.events.schemas import Automation, ExistingAutomation",
            "from prefect.logging import get_logger",
            "from prefect.settings import (",
            "    PREFECT_API_DATABASE_CONNECTION_URL,",
            "    PREFECT_API_ENABLE_HTTP2,",
            "    PREFECT_API_KEY,",
            "    PREFECT_API_REQUEST_TIMEOUT,",
            "    PREFECT_API_TLS_INSECURE_SKIP_VERIFY,",
            "    PREFECT_API_URL,",
            "    PREFECT_CLOUD_API_URL,",
            "    PREFECT_UNIT_TEST_MODE,",
            ")",
            "from prefect.utilities.collections import AutoEnum",
            "",
            "if TYPE_CHECKING:",
            "    from prefect.flows import Flow as FlowObject",
            "    from prefect.tasks import Task as TaskObject",
            "",
            "from prefect.client.base import ASGIApp, PrefectHttpxClient, app_lifespan_context",
            "",
            "",
            "class ServerType(AutoEnum):",
            "    EPHEMERAL = AutoEnum.auto()",
            "    SERVER = AutoEnum.auto()",
            "    CLOUD = AutoEnum.auto()",
            "",
            "",
            "def get_client(httpx_settings: Optional[dict] = None) -> \"PrefectClient\":",
            "    \"\"\"",
            "    Retrieve a HTTP client for communicating with the Prefect REST API.",
            "",
            "    The client must be context managed; for example:",
            "",
            "    ```python",
            "    async with get_client() as client:",
            "        await client.hello()",
            "    ```",
            "    \"\"\"",
            "    ctx = prefect.context.get_settings_context()",
            "    api = PREFECT_API_URL.value()",
            "",
            "    if not api:",
            "        # create an ephemeral API if none was provided",
            "        from prefect.server.api.server import create_app",
            "",
            "        api = create_app(ctx.settings, ephemeral=True)",
            "",
            "    return PrefectClient(",
            "        api,",
            "        api_key=PREFECT_API_KEY.value(),",
            "        httpx_settings=httpx_settings,",
            "    )",
            "",
            "",
            "class PrefectClient:",
            "    \"\"\"",
            "    An asynchronous client for interacting with the [Prefect REST API](/api-ref/rest-api/).",
            "",
            "    Args:",
            "        api: the REST API URL or FastAPI application to connect to",
            "        api_key: An optional API key for authentication.",
            "        api_version: The API version this client is compatible with.",
            "        httpx_settings: An optional dictionary of settings to pass to the underlying",
            "            `httpx.AsyncClient`",
            "",
            "    Examples:",
            "",
            "        Say hello to a Prefect REST API",
            "",
            "        <div class=\"terminal\">",
            "        ```",
            "        >>> async with get_client() as client:",
            "        >>>     response = await client.hello()",
            "        >>>",
            "        >>> print(response.json())",
            "        \ud83d\udc4b",
            "        ```",
            "        </div>",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        api: Union[str, ASGIApp],",
            "        *,",
            "        api_key: str = None,",
            "        api_version: str = None,",
            "        httpx_settings: dict = None,",
            "    ) -> None:",
            "        httpx_settings = httpx_settings.copy() if httpx_settings else {}",
            "        httpx_settings.setdefault(\"headers\", {})",
            "",
            "        if PREFECT_API_TLS_INSECURE_SKIP_VERIFY:",
            "            httpx_settings.setdefault(\"verify\", False)",
            "",
            "        if api_version is None:",
            "            api_version = SERVER_API_VERSION",
            "        httpx_settings[\"headers\"].setdefault(\"X-PREFECT-API-VERSION\", api_version)",
            "        if api_key:",
            "            httpx_settings[\"headers\"].setdefault(\"Authorization\", f\"Bearer {api_key}\")",
            "",
            "        # Context management",
            "        self._exit_stack = AsyncExitStack()",
            "        self._ephemeral_app: Optional[ASGIApp] = None",
            "        self.manage_lifespan = True",
            "        self.server_type: ServerType",
            "",
            "        # Only set if this client started the lifespan of the application",
            "        self._ephemeral_lifespan: Optional[LifespanManager] = None",
            "",
            "        self._closed = False",
            "        self._started = False",
            "",
            "        # Connect to an external application",
            "        if isinstance(api, str):",
            "            if httpx_settings.get(\"app\"):",
            "                raise ValueError(",
            "                    \"Invalid httpx settings: `app` cannot be set when providing an \"",
            "                    \"api url. `app` is only for use with ephemeral instances. Provide \"",
            "                    \"it as the `api` parameter instead.\"",
            "                )",
            "            httpx_settings.setdefault(\"base_url\", api)",
            "",
            "            # See https://www.python-httpx.org/advanced/#pool-limit-configuration",
            "            httpx_settings.setdefault(",
            "                \"limits\",",
            "                httpx.Limits(",
            "                    # We see instability when allowing the client to open many connections at once.",
            "                    # Limiting concurrency results in more stable performance.",
            "                    max_connections=16,",
            "                    max_keepalive_connections=8,",
            "                    # The Prefect Cloud LB will keep connections alive for 30s.",
            "                    # Only allow the client to keep connections alive for 25s.",
            "                    keepalive_expiry=25,",
            "                ),",
            "            )",
            "",
            "            # See https://www.python-httpx.org/http2/",
            "            # Enabling HTTP/2 support on the client does not necessarily mean that your requests",
            "            # and responses will be transported over HTTP/2, since both the client and the server",
            "            # need to support HTTP/2. If you connect to a server that only supports HTTP/1.1 the",
            "            # client will use a standard HTTP/1.1 connection instead.",
            "            httpx_settings.setdefault(\"http2\", PREFECT_API_ENABLE_HTTP2.value())",
            "",
            "            self.server_type = (",
            "                ServerType.CLOUD",
            "                if api.startswith(PREFECT_CLOUD_API_URL.value())",
            "                else ServerType.SERVER",
            "            )",
            "",
            "        # Connect to an in-process application",
            "        elif isinstance(api, ASGIApp):",
            "            self._ephemeral_app = api",
            "            self.server_type = ServerType.EPHEMERAL",
            "",
            "            # When using an ephemeral server, server-side exceptions can be raised",
            "            # client-side breaking all of our response error code handling. To work",
            "            # around this, we create an ASGI transport with application exceptions",
            "            # disabled instead of using the application directly.",
            "            # refs:",
            "            # - https://github.com/PrefectHQ/prefect/pull/9637",
            "            # - https://github.com/encode/starlette/blob/d3a11205ed35f8e5a58a711db0ff59c86fa7bb31/starlette/middleware/errors.py#L184",
            "            # - https://github.com/tiangolo/fastapi/blob/8cc967a7605d3883bd04ceb5d25cc94ae079612f/fastapi/applications.py#L163-L164",
            "            httpx_settings.setdefault(",
            "                \"transport\",",
            "                httpx.ASGITransport(",
            "                    app=self._ephemeral_app, raise_app_exceptions=False",
            "                ),",
            "            )",
            "            httpx_settings.setdefault(\"base_url\", \"http://ephemeral-prefect/api\")",
            "",
            "        else:",
            "            raise TypeError(",
            "                f\"Unexpected type {type(api).__name__!r} for argument `api`. Expected\"",
            "                \" 'str' or 'ASGIApp/FastAPI'\"",
            "            )",
            "",
            "        # See https://www.python-httpx.org/advanced/#timeout-configuration",
            "        httpx_settings.setdefault(",
            "            \"timeout\",",
            "            httpx.Timeout(",
            "                connect=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "                read=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "                write=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "                pool=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "            ),",
            "        )",
            "",
            "        if not PREFECT_UNIT_TEST_MODE:",
            "            httpx_settings.setdefault(\"follow_redirects\", True)",
            "        self._client = PrefectHttpxClient(**httpx_settings)",
            "        self._loop = None",
            "",
            "        # See https://www.python-httpx.org/advanced/#custom-transports",
            "        #",
            "        # If we're using an HTTP/S client (not the ephemeral client), adjust the",
            "        # transport to add retries _after_ it is instantiated. If we alter the transport",
            "        # before instantiation, the transport will not be aware of proxies unless we",
            "        # reproduce all of the logic to make it so.",
            "        #",
            "        # Only alter the transport to set our default of 3 retries, don't modify any",
            "        # transport a user may have provided via httpx_settings.",
            "        #",
            "        # Making liberal use of getattr and isinstance checks here to avoid any",
            "        # surprises if the internals of httpx or httpcore change on us",
            "        if isinstance(api, str) and not httpx_settings.get(\"transport\"):",
            "            transport_for_url = getattr(self._client, \"_transport_for_url\", None)",
            "            if callable(transport_for_url):",
            "                server_transport = transport_for_url(httpx.URL(api))",
            "                if isinstance(server_transport, httpx.AsyncHTTPTransport):",
            "                    pool = getattr(server_transport, \"_pool\", None)",
            "                    if isinstance(pool, httpcore.AsyncConnectionPool):",
            "                        pool._retries = 3",
            "",
            "        self.logger = get_logger(\"client\")",
            "",
            "    @property",
            "    def api_url(self) -> httpx.URL:",
            "        \"\"\"",
            "        Get the base URL for the API.",
            "        \"\"\"",
            "        return self._client.base_url",
            "",
            "    # API methods ----------------------------------------------------------------------",
            "",
            "    async def api_healthcheck(self) -> Optional[Exception]:",
            "        \"\"\"",
            "        Attempts to connect to the API and returns the encountered exception if not",
            "        successful.",
            "",
            "        If successful, returns `None`.",
            "        \"\"\"",
            "        try:",
            "            await self._client.get(\"/health\")",
            "            return None",
            "        except Exception as exc:",
            "            return exc",
            "",
            "    async def hello(self) -> httpx.Response:",
            "        \"\"\"",
            "        Send a GET request to /hello for testing purposes.",
            "        \"\"\"",
            "        return await self._client.get(\"/hello\")",
            "",
            "    async def create_flow(self, flow: \"FlowObject\") -> UUID:",
            "        \"\"\"",
            "        Create a flow in the Prefect API.",
            "",
            "        Args:",
            "            flow: a [Flow][prefect.flows.Flow] object",
            "",
            "        Raises:",
            "            httpx.RequestError: if a flow was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the flow in the backend",
            "        \"\"\"",
            "        return await self.create_flow_from_name(flow.name)",
            "",
            "    async def create_flow_from_name(self, flow_name: str) -> UUID:",
            "        \"\"\"",
            "        Create a flow in the Prefect API.",
            "",
            "        Args:",
            "            flow_name: the name of the new flow",
            "",
            "        Raises:",
            "            httpx.RequestError: if a flow was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the flow in the backend",
            "        \"\"\"",
            "        flow_data = FlowCreate(name=flow_name)",
            "        response = await self._client.post(",
            "            \"/flows/\", json=flow_data.dict(json_compatible=True)",
            "        )",
            "",
            "        flow_id = response.json().get(\"id\")",
            "        if not flow_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        # Return the id of the created flow",
            "        return UUID(flow_id)",
            "",
            "    async def read_flow(self, flow_id: UUID) -> Flow:",
            "        \"\"\"",
            "        Query the Prefect API for a flow by id.",
            "",
            "        Args:",
            "            flow_id: the flow ID of interest",
            "",
            "        Returns:",
            "            a [Flow model][prefect.client.schemas.objects.Flow] representation of the flow",
            "        \"\"\"",
            "        response = await self._client.get(f\"/flows/{flow_id}\")",
            "        return Flow.parse_obj(response.json())",
            "",
            "    async def read_flows(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        work_pool_filter: WorkPoolFilter = None,",
            "        work_queue_filter: WorkQueueFilter = None,",
            "        sort: FlowSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[Flow]:",
            "        \"\"\"",
            "        Query the Prefect API for flows. Only flows matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            work_pool_filter: filter criteria for work pools",
            "            work_queue_filter: filter criteria for work pool queues",
            "            sort: sort criteria for the flows",
            "            limit: limit for the flow query",
            "            offset: offset for the flow query",
            "",
            "        Returns:",
            "            a list of Flow model representations of the flows",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "            \"work_queues\": (",
            "                work_queue_filter.dict(json_compatible=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        response = await self._client.post(\"/flows/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[Flow], response.json())",
            "",
            "    async def read_flow_by_name(",
            "        self,",
            "        flow_name: str,",
            "    ) -> Flow:",
            "        \"\"\"",
            "        Query the Prefect API for a flow by name.",
            "",
            "        Args:",
            "            flow_name: the name of a flow",
            "",
            "        Returns:",
            "            a fully hydrated Flow model",
            "        \"\"\"",
            "        response = await self._client.get(f\"/flows/name/{flow_name}\")",
            "        return Flow.parse_obj(response.json())",
            "",
            "    async def create_flow_run_from_deployment(",
            "        self,",
            "        deployment_id: UUID,",
            "        *,",
            "        parameters: Dict[str, Any] = None,",
            "        context: dict = None,",
            "        state: prefect.states.State = None,",
            "        name: str = None,",
            "        tags: Iterable[str] = None,",
            "        idempotency_key: str = None,",
            "        parent_task_run_id: UUID = None,",
            "        work_queue_name: str = None,",
            "        job_variables: Optional[Dict[str, Any]] = None,",
            "    ) -> FlowRun:",
            "        \"\"\"",
            "        Create a flow run for a deployment.",
            "",
            "        Args:",
            "            deployment_id: The deployment ID to create the flow run from",
            "            parameters: Parameter overrides for this flow run. Merged with the",
            "                deployment defaults",
            "            context: Optional run context data",
            "            state: The initial state for the run. If not provided, defaults to",
            "                `Scheduled` for now. Should always be a `Scheduled` type.",
            "            name: An optional name for the flow run. If not provided, the server will",
            "                generate a name.",
            "            tags: An optional iterable of tags to apply to the flow run; these tags",
            "                are merged with the deployment's tags.",
            "            idempotency_key: Optional idempotency key for creation of the flow run.",
            "                If the key matches the key of an existing flow run, the existing run will",
            "                be returned instead of creating a new one.",
            "            parent_task_run_id: if a subflow run is being created, the placeholder task",
            "                run identifier in the parent flow",
            "            work_queue_name: An optional work queue name to add this run to. If not provided,",
            "                will default to the deployment's set work queue.  If one is provided that does not",
            "                exist, a new work queue will be created within the deployment's work pool.",
            "            job_variables: Optional variables that will be supplied to the flow run job.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the Prefect API does not successfully create a run for any reason",
            "",
            "        Returns:",
            "            The flow run model",
            "        \"\"\"",
            "        if job_variables is not None and experiment_enabled(\"flow_run_infra_overrides\"):",
            "            if (",
            "                PREFECT_EXPERIMENTAL_WARN",
            "                and PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES",
            "            ):",
            "                warnings.warn(",
            "                    EXPERIMENTAL_WARNING.format(",
            "                        feature=\"Flow run job variables\",",
            "                        group=\"flow_run_infra_overrides\",",
            "                        help=\"To use this feature, update your workers to Prefect 2.16.4 or later. \",",
            "                    ),",
            "                    ExperimentalFeature,",
            "                    stacklevel=3,",
            "                )",
            "",
            "        parameters = parameters or {}",
            "        context = context or {}",
            "        state = state or prefect.states.Scheduled()",
            "        tags = tags or []",
            "",
            "        flow_run_create = DeploymentFlowRunCreate(",
            "            parameters=parameters,",
            "            context=context,",
            "            state=state.to_state_create(),",
            "            tags=tags,",
            "            name=name,",
            "            idempotency_key=idempotency_key,",
            "            parent_task_run_id=parent_task_run_id,",
            "            job_variables=job_variables,",
            "        )",
            "",
            "        # done separately to avoid including this field in payloads sent to older API versions",
            "        if work_queue_name:",
            "            flow_run_create.work_queue_name = work_queue_name",
            "",
            "        response = await self._client.post(",
            "            f\"/deployments/{deployment_id}/create_flow_run\",",
            "            json=flow_run_create.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "        return FlowRun.parse_obj(response.json())",
            "",
            "    async def create_flow_run(",
            "        self,",
            "        flow: \"FlowObject\",",
            "        name: str = None,",
            "        parameters: Dict[str, Any] = None,",
            "        context: dict = None,",
            "        tags: Iterable[str] = None,",
            "        parent_task_run_id: UUID = None,",
            "        state: \"prefect.states.State\" = None,",
            "    ) -> FlowRun:",
            "        \"\"\"",
            "        Create a flow run for a flow.",
            "",
            "        Args:",
            "            flow: The flow model to create the flow run for",
            "            name: An optional name for the flow run",
            "            parameters: Parameter overrides for this flow run.",
            "            context: Optional run context data",
            "            tags: a list of tags to apply to this flow run",
            "            parent_task_run_id: if a subflow run is being created, the placeholder task",
            "                run identifier in the parent flow",
            "            state: The initial state for the run. If not provided, defaults to",
            "                `Scheduled` for now. Should always be a `Scheduled` type.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the Prefect API does not successfully create a run for any reason",
            "",
            "        Returns:",
            "            The flow run model",
            "        \"\"\"",
            "        parameters = parameters or {}",
            "        context = context or {}",
            "",
            "        if state is None:",
            "            state = prefect.states.Pending()",
            "",
            "        # Retrieve the flow id",
            "        flow_id = await self.create_flow(flow)",
            "",
            "        flow_run_create = FlowRunCreate(",
            "            flow_id=flow_id,",
            "            flow_version=flow.version,",
            "            name=name,",
            "            parameters=parameters,",
            "            context=context,",
            "            tags=list(tags or []),",
            "            parent_task_run_id=parent_task_run_id,",
            "            state=state.to_state_create(),",
            "            empirical_policy=FlowRunPolicy(",
            "                retries=flow.retries,",
            "                retry_delay=flow.retry_delay_seconds,",
            "            ),",
            "        )",
            "",
            "        flow_run_create_json = flow_run_create.dict(json_compatible=True)",
            "        response = await self._client.post(\"/flow_runs/\", json=flow_run_create_json)",
            "        flow_run = FlowRun.parse_obj(response.json())",
            "",
            "        # Restore the parameters to the local objects to retain expectations about",
            "        # Python objects",
            "        flow_run.parameters = parameters",
            "",
            "        return flow_run",
            "",
            "    async def update_flow_run(",
            "        self,",
            "        flow_run_id: UUID,",
            "        flow_version: Optional[str] = None,",
            "        parameters: Optional[dict] = None,",
            "        name: Optional[str] = None,",
            "        tags: Optional[Iterable[str]] = None,",
            "        empirical_policy: Optional[FlowRunPolicy] = None,",
            "        infrastructure_pid: Optional[str] = None,",
            "        job_variables: Optional[dict] = None,",
            "    ) -> httpx.Response:",
            "        \"\"\"",
            "        Update a flow run's details.",
            "",
            "        Args:",
            "            flow_run_id: The identifier for the flow run to update.",
            "            flow_version: A new version string for the flow run.",
            "            parameters: A dictionary of parameter values for the flow run. This will not",
            "                be merged with any existing parameters.",
            "            name: A new name for the flow run.",
            "            empirical_policy: A new flow run orchestration policy. This will not be",
            "                merged with any existing policy.",
            "            tags: An iterable of new tags for the flow run. These will not be merged with",
            "                any existing tags.",
            "            infrastructure_pid: The id of flow run as returned by an",
            "                infrastructure block.",
            "",
            "        Returns:",
            "            an `httpx.Response` object from the PATCH request",
            "        \"\"\"",
            "        if job_variables is not None and experiment_enabled(\"flow_run_infra_overrides\"):",
            "            if (",
            "                PREFECT_EXPERIMENTAL_WARN",
            "                and PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES",
            "            ):",
            "                warnings.warn(",
            "                    EXPERIMENTAL_WARNING.format(",
            "                        feature=\"Flow run job variables\",",
            "                        group=\"flow_run_infra_overrides\",",
            "                        help=\"To use this feature, update your workers to Prefect 2.16.4 or later. \",",
            "                    ),",
            "                    ExperimentalFeature,",
            "                    stacklevel=3,",
            "                )",
            "",
            "        params = {}",
            "        if flow_version is not None:",
            "            params[\"flow_version\"] = flow_version",
            "        if parameters is not None:",
            "            params[\"parameters\"] = parameters",
            "        if name is not None:",
            "            params[\"name\"] = name",
            "        if tags is not None:",
            "            params[\"tags\"] = tags",
            "        if empirical_policy is not None:",
            "            params[\"empirical_policy\"] = empirical_policy",
            "        if infrastructure_pid:",
            "            params[\"infrastructure_pid\"] = infrastructure_pid",
            "        if job_variables is not None:",
            "            params[\"job_variables\"] = job_variables",
            "",
            "        flow_run_data = FlowRunUpdate(**params)",
            "",
            "        return await self._client.patch(",
            "            f\"/flow_runs/{flow_run_id}\",",
            "            json=flow_run_data.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "",
            "    async def delete_flow_run(",
            "        self,",
            "        flow_run_id: UUID,",
            "    ) -> None:",
            "        \"\"\"",
            "        Delete a flow run by UUID.",
            "",
            "        Args:",
            "            flow_run_id: The flow run UUID of interest.",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/flow_runs/{flow_run_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_concurrency_limit(",
            "        self,",
            "        tag: str,",
            "        concurrency_limit: int,",
            "    ) -> UUID:",
            "        \"\"\"",
            "        Create a tag concurrency limit in the Prefect API. These limits govern concurrently",
            "        running tasks.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "            concurrency_limit: the maximum number of concurrent task runs for a given tag",
            "",
            "        Raises:",
            "            httpx.RequestError: if the concurrency limit was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the concurrency limit in the backend",
            "        \"\"\"",
            "",
            "        concurrency_limit_create = ConcurrencyLimitCreate(",
            "            tag=tag,",
            "            concurrency_limit=concurrency_limit,",
            "        )",
            "        response = await self._client.post(",
            "            \"/concurrency_limits/\",",
            "            json=concurrency_limit_create.dict(json_compatible=True),",
            "        )",
            "",
            "        concurrency_limit_id = response.json().get(\"id\")",
            "",
            "        if not concurrency_limit_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(concurrency_limit_id)",
            "",
            "    async def read_concurrency_limit_by_tag(",
            "        self,",
            "        tag: str,",
            "    ):",
            "        \"\"\"",
            "        Read the concurrency limit set on a specific tag.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: if the concurrency limit was not created for any reason",
            "",
            "        Returns:",
            "            the concurrency limit set on a specific tag",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(",
            "                f\"/concurrency_limits/tag/{tag}\",",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        concurrency_limit_id = response.json().get(\"id\")",
            "",
            "        if not concurrency_limit_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        concurrency_limit = ConcurrencyLimit.parse_obj(response.json())",
            "        return concurrency_limit",
            "",
            "    async def read_concurrency_limits(",
            "        self,",
            "        limit: int,",
            "        offset: int,",
            "    ):",
            "        \"\"\"",
            "        Lists concurrency limits set on task run tags.",
            "",
            "        Args:",
            "            limit: the maximum number of concurrency limits returned",
            "            offset: the concurrency limit query offset",
            "",
            "        Returns:",
            "            a list of concurrency limits",
            "        \"\"\"",
            "",
            "        body = {",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        response = await self._client.post(\"/concurrency_limits/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[ConcurrencyLimit], response.json())",
            "",
            "    async def reset_concurrency_limit_by_tag(",
            "        self,",
            "        tag: str,",
            "        slot_override: Optional[List[Union[UUID, str]]] = None,",
            "    ):",
            "        \"\"\"",
            "        Resets the concurrency limit slots set on a specific tag.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "            slot_override: a list of task run IDs that are currently using a",
            "                concurrency slot, please check that any task run IDs included in",
            "                `slot_override` are currently running, otherwise those concurrency",
            "                slots will never be released.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        \"\"\"",
            "        if slot_override is not None:",
            "            slot_override = [str(slot) for slot in slot_override]",
            "",
            "        try:",
            "            await self._client.post(",
            "                f\"/concurrency_limits/tag/{tag}/reset\",",
            "                json=dict(slot_override=slot_override),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_concurrency_limit_by_tag(",
            "        self,",
            "        tag: str,",
            "    ):",
            "        \"\"\"",
            "        Delete the concurrency limit set on a specific tag.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(",
            "                f\"/concurrency_limits/tag/{tag}\",",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_work_queue(",
            "        self,",
            "        name: str,",
            "        tags: Optional[List[str]] = None,",
            "        description: Optional[str] = None,",
            "        is_paused: Optional[bool] = None,",
            "        concurrency_limit: Optional[int] = None,",
            "        priority: Optional[int] = None,",
            "        work_pool_name: Optional[str] = None,",
            "    ) -> WorkQueue:",
            "        \"\"\"",
            "        Create a work queue.",
            "",
            "        Args:",
            "            name: a unique name for the work queue",
            "            tags: DEPRECATED: an optional list of tags to filter on; only work scheduled with these tags",
            "                will be included in the queue. This option will be removed on 2023-02-23.",
            "            description: An optional description for the work queue.",
            "            is_paused: Whether or not the work queue is paused.",
            "            concurrency_limit: An optional concurrency limit for the work queue.",
            "            priority: The queue's priority. Lower values are higher priority (1 is the highest).",
            "            work_pool_name: The name of the work pool to use for this queue.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectAlreadyExists: If request returns 409",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            The created work queue",
            "        \"\"\"",
            "        if tags:",
            "            warnings.warn(",
            "                (",
            "                    \"The use of tags for creating work queue filters is deprecated.\"",
            "                    \" This option will be removed on 2023-02-23.\"",
            "                ),",
            "                DeprecationWarning,",
            "            )",
            "            filter = QueueFilter(tags=tags)",
            "        else:",
            "            filter = None",
            "        create_model = WorkQueueCreate(name=name, filter=filter)",
            "        if description is not None:",
            "            create_model.description = description",
            "        if is_paused is not None:",
            "            create_model.is_paused = is_paused",
            "        if concurrency_limit is not None:",
            "            create_model.concurrency_limit = concurrency_limit",
            "        if priority is not None:",
            "            create_model.priority = priority",
            "",
            "        data = create_model.dict(json_compatible=True)",
            "        try:",
            "            if work_pool_name is not None:",
            "                response = await self._client.post(",
            "                    f\"/work_pools/{work_pool_name}/queues\", json=data",
            "                )",
            "            else:",
            "                response = await self._client.post(\"/work_queues/\", json=data)",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            elif e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return WorkQueue.parse_obj(response.json())",
            "",
            "    async def read_work_queue_by_name(",
            "        self,",
            "        name: str,",
            "        work_pool_name: Optional[str] = None,",
            "    ) -> WorkQueue:",
            "        \"\"\"",
            "        Read a work queue by name.",
            "",
            "        Args:",
            "            name (str): a unique name for the work queue",
            "            work_pool_name (str, optional): the name of the work pool",
            "                the queue belongs to.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: if no work queue is found",
            "            httpx.HTTPStatusError: other status errors",
            "",
            "        Returns:",
            "            WorkQueue: a work queue API object",
            "        \"\"\"",
            "        try:",
            "            if work_pool_name is not None:",
            "                response = await self._client.get(",
            "                    f\"/work_pools/{work_pool_name}/queues/{name}\"",
            "                )",
            "            else:",
            "                response = await self._client.get(f\"/work_queues/name/{name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return WorkQueue.parse_obj(response.json())",
            "",
            "    async def update_work_queue(self, id: UUID, **kwargs):",
            "        \"\"\"",
            "        Update properties of a work queue.",
            "",
            "        Args:",
            "            id: the ID of the work queue to update",
            "            **kwargs: the fields to update",
            "",
            "        Raises:",
            "            ValueError: if no kwargs are provided",
            "            prefect.exceptions.ObjectNotFound: if request returns 404",
            "            httpx.RequestError: if the request fails",
            "",
            "        \"\"\"",
            "        if not kwargs:",
            "            raise ValueError(\"No fields provided to update.\")",
            "",
            "        data = WorkQueueUpdate(**kwargs).dict(json_compatible=True, exclude_unset=True)",
            "        try:",
            "            await self._client.patch(f\"/work_queues/{id}\", json=data)",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def get_runs_in_work_queue(",
            "        self,",
            "        id: UUID,",
            "        limit: int = 10,",
            "        scheduled_before: datetime.datetime = None,",
            "    ) -> List[FlowRun]:",
            "        \"\"\"",
            "        Read flow runs off a work queue.",
            "",
            "        Args:",
            "            id: the id of the work queue to read from",
            "            limit: a limit on the number of runs to return",
            "            scheduled_before: a timestamp; only runs scheduled before this time will be returned.",
            "                Defaults to now.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            List[FlowRun]: a list of FlowRun objects read from the queue",
            "        \"\"\"",
            "        if scheduled_before is None:",
            "            scheduled_before = pendulum.now(\"UTC\")",
            "",
            "        try:",
            "            response = await self._client.post(",
            "                f\"/work_queues/{id}/get_runs\",",
            "                json={",
            "                    \"limit\": limit,",
            "                    \"scheduled_before\": scheduled_before.isoformat(),",
            "                },",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return pydantic.parse_obj_as(List[FlowRun], response.json())",
            "",
            "    async def read_work_queue(",
            "        self,",
            "        id: UUID,",
            "    ) -> WorkQueue:",
            "        \"\"\"",
            "        Read a work queue.",
            "",
            "        Args:",
            "            id: the id of the work queue to load",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            WorkQueue: an instantiated WorkQueue object",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/work_queues/{id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return WorkQueue.parse_obj(response.json())",
            "",
            "    async def read_work_queue_status(",
            "        self,",
            "        id: UUID,",
            "    ) -> WorkQueueStatusDetail:",
            "        \"\"\"",
            "        Read a work queue status.",
            "",
            "        Args:",
            "            id: the id of the work queue to load",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            WorkQueueStatus: an instantiated WorkQueueStatus object",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/work_queues/{id}/status\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return WorkQueueStatusDetail.parse_obj(response.json())",
            "",
            "    async def match_work_queues(",
            "        self,",
            "        prefixes: List[str],",
            "        work_pool_name: Optional[str] = None,",
            "    ) -> List[WorkQueue]:",
            "        \"\"\"",
            "        Query the Prefect API for work queues with names with a specific prefix.",
            "",
            "        Args:",
            "            prefixes: a list of strings used to match work queue name prefixes",
            "            work_pool_name: an optional work pool name to scope the query to",
            "",
            "        Returns:",
            "            a list of WorkQueue model representations",
            "                of the work queues",
            "        \"\"\"",
            "        page_length = 100",
            "        current_page = 0",
            "        work_queues = []",
            "",
            "        while True:",
            "            new_queues = await self.read_work_queues(",
            "                work_pool_name=work_pool_name,",
            "                offset=current_page * page_length,",
            "                limit=page_length,",
            "                work_queue_filter=WorkQueueFilter(",
            "                    name=WorkQueueFilterName(startswith_=prefixes)",
            "                ),",
            "            )",
            "            if not new_queues:",
            "                break",
            "            work_queues += new_queues",
            "            current_page += 1",
            "",
            "        return work_queues",
            "",
            "    async def delete_work_queue_by_id(",
            "        self,",
            "        id: UUID,",
            "    ):",
            "        \"\"\"",
            "        Delete a work queue by its ID.",
            "",
            "        Args:",
            "            id: the id of the work queue to delete",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(",
            "                f\"/work_queues/{id}\",",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_block_type(self, block_type: BlockTypeCreate) -> BlockType:",
            "        \"\"\"",
            "        Create a block type in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_types/\",",
            "                json=block_type.dict(",
            "                    json_compatible=True, exclude_unset=True, exclude={\"id\"}",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockType.parse_obj(response.json())",
            "",
            "    async def create_block_schema(self, block_schema: BlockSchemaCreate) -> BlockSchema:",
            "        \"\"\"",
            "        Create a block schema in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_schemas/\",",
            "                json=block_schema.dict(",
            "                    json_compatible=True,",
            "                    exclude_unset=True,",
            "                    exclude={\"id\", \"block_type\", \"checksum\"},",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockSchema.parse_obj(response.json())",
            "",
            "    async def create_block_document(",
            "        self,",
            "        block_document: Union[BlockDocument, BlockDocumentCreate],",
            "        include_secrets: bool = True,",
            "    ) -> BlockDocument:",
            "        \"\"\"",
            "        Create a block document in the Prefect API. This data is used to configure a",
            "        corresponding Block.",
            "",
            "        Args:",
            "            include_secrets (bool): whether to include secret values",
            "                on the stored Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. Note Blocks may not work as expected if",
            "                this is set to `False`.",
            "        \"\"\"",
            "        if isinstance(block_document, BlockDocument):",
            "            block_document = BlockDocumentCreate.parse_obj(",
            "                block_document.dict(",
            "                    json_compatible=True,",
            "                    include_secrets=include_secrets,",
            "                    exclude_unset=True,",
            "                    exclude={\"id\", \"block_schema\", \"block_type\"},",
            "                ),",
            "            )",
            "",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_documents/\",",
            "                json=block_document.dict(",
            "                    json_compatible=True,",
            "                    include_secrets=include_secrets,",
            "                    exclude_unset=True,",
            "                    exclude={\"id\", \"block_schema\", \"block_type\"},",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockDocument.parse_obj(response.json())",
            "",
            "    async def update_block_document(",
            "        self,",
            "        block_document_id: UUID,",
            "        block_document: BlockDocumentUpdate,",
            "    ):",
            "        \"\"\"",
            "        Update a block document in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            await self._client.patch(",
            "                f\"/block_documents/{block_document_id}\",",
            "                json=block_document.dict(",
            "                    json_compatible=True,",
            "                    exclude_unset=True,",
            "                    include={\"data\", \"merge_existing_data\", \"block_schema_id\"},",
            "                    include_secrets=True,",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_block_document(self, block_document_id: UUID):",
            "        \"\"\"",
            "        Delete a block document.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/block_documents/{block_document_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_block_type_by_slug(self, slug: str) -> BlockType:",
            "        \"\"\"",
            "        Read a block type by its slug.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/block_types/slug/{slug}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockType.parse_obj(response.json())",
            "",
            "    async def read_block_schema_by_checksum(",
            "        self, checksum: str, version: Optional[str] = None",
            "    ) -> BlockSchema:",
            "        \"\"\"",
            "        Look up a block schema checksum",
            "        \"\"\"",
            "        try:",
            "            url = f\"/block_schemas/checksum/{checksum}\"",
            "            if version is not None:",
            "                url = f\"{url}?version={version}\"",
            "            response = await self._client.get(url)",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockSchema.parse_obj(response.json())",
            "",
            "    async def update_block_type(self, block_type_id: UUID, block_type: BlockTypeUpdate):",
            "        \"\"\"",
            "        Update a block document in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            await self._client.patch(",
            "                f\"/block_types/{block_type_id}\",",
            "                json=block_type.dict(",
            "                    json_compatible=True,",
            "                    exclude_unset=True,",
            "                    include=BlockTypeUpdate.updatable_fields(),",
            "                    include_secrets=True,",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_block_type(self, block_type_id: UUID):",
            "        \"\"\"",
            "        Delete a block type.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/block_types/{block_type_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            elif (",
            "                e.response.status_code == status.HTTP_403_FORBIDDEN",
            "                and e.response.json()[\"detail\"]",
            "                == \"protected block types cannot be deleted.\"",
            "            ):",
            "                raise prefect.exceptions.ProtectedBlockError(",
            "                    \"Protected block types cannot be deleted.\"",
            "                ) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_block_types(self) -> List[BlockType]:",
            "        \"\"\"",
            "        Read all block types",
            "        Raises:",
            "            httpx.RequestError: if the block types were not found",
            "",
            "        Returns:",
            "            List of BlockTypes.",
            "        \"\"\"",
            "        response = await self._client.post(\"/block_types/filter\", json={})",
            "        return pydantic.parse_obj_as(List[BlockType], response.json())",
            "",
            "    async def read_block_schemas(self) -> List[BlockSchema]:",
            "        \"\"\"",
            "        Read all block schemas",
            "        Raises:",
            "            httpx.RequestError: if a valid block schema was not found",
            "",
            "        Returns:",
            "            A BlockSchema.",
            "        \"\"\"",
            "        response = await self._client.post(\"/block_schemas/filter\", json={})",
            "        return pydantic.parse_obj_as(List[BlockSchema], response.json())",
            "",
            "    async def get_most_recent_block_schema_for_block_type(",
            "        self,",
            "        block_type_id: UUID,",
            "    ) -> Optional[BlockSchema]:",
            "        \"\"\"",
            "        Fetches the most recent block schema for a specified block type ID.",
            "",
            "        Args:",
            "            block_type_id: The ID of the block type.",
            "",
            "        Raises:",
            "            httpx.RequestError: If the request fails for any reason.",
            "",
            "        Returns:",
            "            The most recent block schema or None.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_schemas/filter\",",
            "                json={",
            "                    \"block_schemas\": {\"block_type_id\": {\"any_\": [str(block_type_id)]}},",
            "                    \"limit\": 1,",
            "                },",
            "            )",
            "        except httpx.HTTPStatusError:",
            "            raise",
            "        return BlockSchema.parse_obj(response.json()[0]) if response.json() else None",
            "",
            "    async def read_block_document(",
            "        self,",
            "        block_document_id: UUID,",
            "        include_secrets: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Read the block document with the specified ID.",
            "",
            "        Args:",
            "            block_document_id: the block document id",
            "            include_secrets (bool): whether to include secret values",
            "                on the Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. These fields are automatically obfuscated",
            "                by Pydantic, but users can additionally choose not to receive",
            "                their values from the API. Note that any business logic on the",
            "                Block may not work if this is `False`.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the block document was not found for any reason",
            "",
            "        Returns:",
            "            A block document or None.",
            "        \"\"\"",
            "        assert (",
            "            block_document_id is not None",
            "        ), \"Unexpected ID on block document. Was it persisted?\"",
            "        try:",
            "            response = await self._client.get(",
            "                f\"/block_documents/{block_document_id}\",",
            "                params=dict(include_secrets=include_secrets),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockDocument.parse_obj(response.json())",
            "",
            "    async def read_block_document_by_name(",
            "        self,",
            "        name: str,",
            "        block_type_slug: str,",
            "        include_secrets: bool = True,",
            "    ) -> BlockDocument:",
            "        \"\"\"",
            "        Read the block document with the specified name that corresponds to a",
            "        specific block type name.",
            "",
            "        Args:",
            "            name: The block document name.",
            "            block_type_slug: The block type slug.",
            "            include_secrets (bool): whether to include secret values",
            "                on the Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. These fields are automatically obfuscated",
            "                by Pydantic, but users can additionally choose not to receive",
            "                their values from the API. Note that any business logic on the",
            "                Block may not work if this is `False`.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the block document was not found for any reason",
            "",
            "        Returns:",
            "            A block document or None.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(",
            "                f\"/block_types/slug/{block_type_slug}/block_documents/name/{name}\",",
            "                params=dict(include_secrets=include_secrets),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockDocument.parse_obj(response.json())",
            "",
            "    async def read_block_documents(",
            "        self,",
            "        block_schema_type: Optional[str] = None,",
            "        offset: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        include_secrets: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Read block documents",
            "",
            "        Args:",
            "            block_schema_type: an optional block schema type",
            "            offset: an offset",
            "            limit: the number of blocks to return",
            "            include_secrets (bool): whether to include secret values",
            "                on the Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. These fields are automatically obfuscated",
            "                by Pydantic, but users can additionally choose not to receive",
            "                their values from the API. Note that any business logic on the",
            "                Block may not work if this is `False`.",
            "",
            "        Returns:",
            "            A list of block documents",
            "        \"\"\"",
            "        response = await self._client.post(",
            "            \"/block_documents/filter\",",
            "            json=dict(",
            "                block_schema_type=block_schema_type,",
            "                offset=offset,",
            "                limit=limit,",
            "                include_secrets=include_secrets,",
            "            ),",
            "        )",
            "        return pydantic.parse_obj_as(List[BlockDocument], response.json())",
            "",
            "    async def read_block_documents_by_type(",
            "        self,",
            "        block_type_slug: str,",
            "        offset: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        include_secrets: bool = True,",
            "    ) -> List[BlockDocument]:",
            "        \"\"\"Retrieve block documents by block type slug.",
            "",
            "        Args:",
            "            block_type_slug: The block type slug.",
            "            offset: an offset",
            "            limit: the number of blocks to return",
            "            include_secrets: whether to include secret values",
            "",
            "        Returns:",
            "            A list of block documents",
            "        \"\"\"",
            "        response = await self._client.get(",
            "            f\"/block_types/slug/{block_type_slug}/block_documents\",",
            "            params=dict(",
            "                offset=offset,",
            "                limit=limit,",
            "                include_secrets=include_secrets,",
            "            ),",
            "        )",
            "",
            "        return pydantic.parse_obj_as(List[BlockDocument], response.json())",
            "",
            "    async def create_deployment(",
            "        self,",
            "        flow_id: UUID,",
            "        name: str,",
            "        version: str = None,",
            "        schedule: SCHEDULE_TYPES = None,",
            "        schedules: List[DeploymentScheduleCreate] = None,",
            "        parameters: Dict[str, Any] = None,",
            "        description: str = None,",
            "        work_queue_name: str = None,",
            "        work_pool_name: str = None,",
            "        tags: List[str] = None,",
            "        storage_document_id: UUID = None,",
            "        manifest_path: str = None,",
            "        path: str = None,",
            "        entrypoint: str = None,",
            "        infrastructure_document_id: UUID = None,",
            "        infra_overrides: Dict[str, Any] = None,",
            "        parameter_openapi_schema: dict = None,",
            "        is_schedule_active: Optional[bool] = None,",
            "        paused: Optional[bool] = None,",
            "        pull_steps: Optional[List[dict]] = None,",
            "        enforce_parameter_schema: Optional[bool] = None,",
            "    ) -> UUID:",
            "        \"\"\"",
            "        Create a deployment.",
            "",
            "        Args:",
            "            flow_id: the flow ID to create a deployment for",
            "            name: the name of the deployment",
            "            version: an optional version string for the deployment",
            "            schedule: an optional schedule to apply to the deployment",
            "            tags: an optional list of tags to apply to the deployment",
            "            storage_document_id: an reference to the storage block document",
            "                used for the deployed flow",
            "            infrastructure_document_id: an reference to the infrastructure block document",
            "                to use for this deployment",
            "",
            "        Raises:",
            "            httpx.RequestError: if the deployment was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the deployment in the backend",
            "        \"\"\"",
            "",
            "        deployment_create = DeploymentCreate(",
            "            flow_id=flow_id,",
            "            name=name,",
            "            version=version,",
            "            parameters=dict(parameters or {}),",
            "            tags=list(tags or []),",
            "            work_queue_name=work_queue_name,",
            "            description=description,",
            "            storage_document_id=storage_document_id,",
            "            path=path,",
            "            entrypoint=entrypoint,",
            "            manifest_path=manifest_path,  # for backwards compat",
            "            infrastructure_document_id=infrastructure_document_id,",
            "            infra_overrides=infra_overrides or {},",
            "            parameter_openapi_schema=parameter_openapi_schema,",
            "            is_schedule_active=is_schedule_active,",
            "            paused=paused,",
            "            schedule=schedule,",
            "            schedules=schedules or [],",
            "            pull_steps=pull_steps,",
            "            enforce_parameter_schema=enforce_parameter_schema,",
            "        )",
            "",
            "        if work_pool_name is not None:",
            "            deployment_create.work_pool_name = work_pool_name",
            "",
            "        # Exclude newer fields that are not set to avoid compatibility issues",
            "        exclude = {",
            "            field",
            "            for field in [\"work_pool_name\", \"work_queue_name\"]",
            "            if field not in deployment_create.__fields_set__",
            "        }",
            "",
            "        if deployment_create.is_schedule_active is None:",
            "            exclude.add(\"is_schedule_active\")",
            "",
            "        if deployment_create.paused is None:",
            "            exclude.add(\"paused\")",
            "",
            "        if deployment_create.pull_steps is None:",
            "            exclude.add(\"pull_steps\")",
            "",
            "        if deployment_create.enforce_parameter_schema is None:",
            "            exclude.add(\"enforce_parameter_schema\")",
            "",
            "        json = deployment_create.dict(json_compatible=True, exclude=exclude)",
            "        response = await self._client.post(",
            "            \"/deployments/\",",
            "            json=json,",
            "        )",
            "        deployment_id = response.json().get(\"id\")",
            "        if not deployment_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(deployment_id)",
            "",
            "    async def update_schedule(self, deployment_id: UUID, active: bool = True):",
            "        path = \"set_schedule_active\" if active else \"set_schedule_inactive\"",
            "        await self._client.post(",
            "            f\"/deployments/{deployment_id}/{path}\",",
            "        )",
            "",
            "    async def set_deployment_paused_state(self, deployment_id: UUID, paused: bool):",
            "        await self._client.patch(",
            "            f\"/deployments/{deployment_id}\", json={\"paused\": paused}",
            "        )",
            "",
            "    async def update_deployment(",
            "        self,",
            "        deployment: Deployment,",
            "        schedule: SCHEDULE_TYPES = None,",
            "        is_schedule_active: bool = None,",
            "    ):",
            "        deployment_update = DeploymentUpdate(",
            "            version=deployment.version,",
            "            schedule=schedule if schedule is not None else deployment.schedule,",
            "            is_schedule_active=(",
            "                is_schedule_active",
            "                if is_schedule_active is not None",
            "                else deployment.is_schedule_active",
            "            ),",
            "            description=deployment.description,",
            "            work_queue_name=deployment.work_queue_name,",
            "            tags=deployment.tags,",
            "            manifest_path=deployment.manifest_path,",
            "            path=deployment.path,",
            "            entrypoint=deployment.entrypoint,",
            "            parameters=deployment.parameters,",
            "            storage_document_id=deployment.storage_document_id,",
            "            infrastructure_document_id=deployment.infrastructure_document_id,",
            "            infra_overrides=deployment.infra_overrides,",
            "            enforce_parameter_schema=deployment.enforce_parameter_schema,",
            "        )",
            "",
            "        if getattr(deployment, \"work_pool_name\", None) is not None:",
            "            deployment_update.work_pool_name = deployment.work_pool_name",
            "",
            "        exclude = set()",
            "        if deployment.enforce_parameter_schema is None:",
            "            exclude.add(\"enforce_parameter_schema\")",
            "",
            "        await self._client.patch(",
            "            f\"/deployments/{deployment.id}\",",
            "            json=deployment_update.dict(json_compatible=True, exclude=exclude),",
            "        )",
            "",
            "    async def _create_deployment_from_schema(self, schema: DeploymentCreate) -> UUID:",
            "        \"\"\"",
            "        Create a deployment from a prepared `DeploymentCreate` schema.",
            "        \"\"\"",
            "        # TODO: We are likely to remove this method once we have considered the",
            "        #       packaging interface for deployments further.",
            "        response = await self._client.post(",
            "            \"/deployments/\", json=schema.dict(json_compatible=True)",
            "        )",
            "        deployment_id = response.json().get(\"id\")",
            "        if not deployment_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(deployment_id)",
            "",
            "    async def read_deployment(",
            "        self,",
            "        deployment_id: UUID,",
            "    ) -> DeploymentResponse:",
            "        \"\"\"",
            "        Query the Prefect API for a deployment by id.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID of interest",
            "",
            "        Returns:",
            "            a [Deployment model][prefect.client.schemas.objects.Deployment] representation of the deployment",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/deployments/{deployment_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return DeploymentResponse.parse_obj(response.json())",
            "",
            "    async def read_deployment_by_name(",
            "        self,",
            "        name: str,",
            "    ) -> DeploymentResponse:",
            "        \"\"\"",
            "        Query the Prefect API for a deployment by name.",
            "",
            "        Args:",
            "            name: A deployed flow's name: <FLOW_NAME>/<DEPLOYMENT_NAME>",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            a Deployment model representation of the deployment",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/deployments/name/{name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return DeploymentResponse.parse_obj(response.json())",
            "",
            "    async def read_deployments(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        work_pool_filter: WorkPoolFilter = None,",
            "        work_queue_filter: WorkQueueFilter = None,",
            "        limit: int = None,",
            "        sort: DeploymentSort = None,",
            "        offset: int = 0,",
            "    ) -> List[DeploymentResponse]:",
            "        \"\"\"",
            "        Query the Prefect API for deployments. Only deployments matching all",
            "        the provided criteria will be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            work_pool_filter: filter criteria for work pools",
            "            work_queue_filter: filter criteria for work pool queues",
            "            limit: a limit for the deployment query",
            "            offset: an offset for the deployment query",
            "",
            "        Returns:",
            "            a list of Deployment model representations",
            "                of the deployments",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "            \"work_pool_queues\": (",
            "                work_queue_filter.dict(json_compatible=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "            \"sort\": sort,",
            "        }",
            "",
            "        response = await self._client.post(\"/deployments/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[DeploymentResponse], response.json())",
            "",
            "    async def delete_deployment(",
            "        self,",
            "        deployment_id: UUID,",
            "    ):",
            "        \"\"\"",
            "        Delete deployment by id.",
            "",
            "        Args:",
            "            deployment_id: The deployment id of interest.",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/deployments/{deployment_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_deployment_schedules(",
            "        self,",
            "        deployment_id: UUID,",
            "        schedules: List[Tuple[SCHEDULE_TYPES, bool]],",
            "    ) -> List[DeploymentSchedule]:",
            "        \"\"\"",
            "        Create deployment schedules.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "            schedules: a list of tuples containing the schedule to create",
            "                       and whether or not it should be active.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the schedules were not created for any reason",
            "",
            "        Returns:",
            "            the list of schedules created in the backend",
            "        \"\"\"",
            "        deployment_schedule_create = [",
            "            DeploymentScheduleCreate(schedule=schedule[0], active=schedule[1])",
            "            for schedule in schedules",
            "        ]",
            "",
            "        json = [",
            "            deployment_schedule_create.dict(json_compatible=True)",
            "            for deployment_schedule_create in deployment_schedule_create",
            "        ]",
            "        response = await self._client.post(",
            "            f\"/deployments/{deployment_id}/schedules\", json=json",
            "        )",
            "        return pydantic.parse_obj_as(List[DeploymentSchedule], response.json())",
            "",
            "    async def read_deployment_schedules(",
            "        self,",
            "        deployment_id: UUID,",
            "    ) -> List[DeploymentSchedule]:",
            "        \"\"\"",
            "        Query the Prefect API for a deployment's schedules.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "",
            "        Returns:",
            "            a list of DeploymentSchedule model representations of the deployment schedules",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/deployments/{deployment_id}/schedules\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return pydantic.parse_obj_as(List[DeploymentSchedule], response.json())",
            "",
            "    async def update_deployment_schedule(",
            "        self,",
            "        deployment_id: UUID,",
            "        schedule_id: UUID,",
            "        active: Optional[bool] = None,",
            "        schedule: Optional[SCHEDULE_TYPES] = None,",
            "    ):",
            "        \"\"\"",
            "        Update a deployment schedule by ID.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "            schedule_id: the deployment schedule ID of interest",
            "            active: whether or not the schedule should be active",
            "            schedule: the cron, rrule, or interval schedule this deployment schedule should use",
            "        \"\"\"",
            "        kwargs = {}",
            "        if active is not None:",
            "            kwargs[\"active\"] = active",
            "        elif schedule is not None:",
            "            kwargs[\"schedule\"] = schedule",
            "",
            "        deployment_schedule_update = DeploymentScheduleUpdate(**kwargs)",
            "        json = deployment_schedule_update.dict(json_compatible=True, exclude_unset=True)",
            "",
            "        try:",
            "            await self._client.patch(",
            "                f\"/deployments/{deployment_id}/schedules/{schedule_id}\", json=json",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_deployment_schedule(",
            "        self,",
            "        deployment_id: UUID,",
            "        schedule_id: UUID,",
            "    ) -> None:",
            "        \"\"\"",
            "        Delete a deployment schedule.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "            schedule_id: the ID of the deployment schedule to delete.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the schedules were not deleted for any reason",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(",
            "                f\"/deployments/{deployment_id}/schedules/{schedule_id}\"",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_flow_run(self, flow_run_id: UUID) -> FlowRun:",
            "        \"\"\"",
            "        Query the Prefect API for a flow run by id.",
            "",
            "        Args:",
            "            flow_run_id: the flow run ID of interest",
            "",
            "        Returns:",
            "            a Flow Run model representation of the flow run",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/flow_runs/{flow_run_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return FlowRun.parse_obj(response.json())",
            "",
            "    async def resume_flow_run(",
            "        self, flow_run_id: UUID, run_input: Optional[Dict] = None",
            "    ) -> OrchestrationResult:",
            "        \"\"\"",
            "        Resumes a paused flow run.",
            "",
            "        Args:",
            "            flow_run_id: the flow run ID of interest",
            "            run_input: the input to resume the flow run with",
            "",
            "        Returns:",
            "            an OrchestrationResult model representation of state orchestration output",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                f\"/flow_runs/{flow_run_id}/resume\", json={\"run_input\": run_input}",
            "            )",
            "        except httpx.HTTPStatusError:",
            "            raise",
            "",
            "        return OrchestrationResult.parse_obj(response.json())",
            "",
            "    async def read_flow_runs(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        work_pool_filter: WorkPoolFilter = None,",
            "        work_queue_filter: WorkQueueFilter = None,",
            "        sort: FlowRunSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[FlowRun]:",
            "        \"\"\"",
            "        Query the Prefect API for flow runs. Only flow runs matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            work_pool_filter: filter criteria for work pools",
            "            work_queue_filter: filter criteria for work pool queues",
            "            sort: sort criteria for the flow runs",
            "            limit: limit for the flow run query",
            "            offset: offset for the flow run query",
            "",
            "        Returns:",
            "            a list of Flow Run model representations",
            "                of the flow runs",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "            \"work_pool_queues\": (",
            "                work_queue_filter.dict(json_compatible=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        response = await self._client.post(\"/flow_runs/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[FlowRun], response.json())",
            "",
            "    async def set_flow_run_state(",
            "        self,",
            "        flow_run_id: UUID,",
            "        state: \"prefect.states.State\",",
            "        force: bool = False,",
            "    ) -> OrchestrationResult:",
            "        \"\"\"",
            "        Set the state of a flow run.",
            "",
            "        Args:",
            "            flow_run_id: the id of the flow run",
            "            state: the state to set",
            "            force: if True, disregard orchestration logic when setting the state,",
            "                forcing the Prefect API to accept the state",
            "",
            "        Returns:",
            "            an OrchestrationResult model representation of state orchestration output",
            "        \"\"\"",
            "        state_create = state.to_state_create()",
            "        state_create.state_details.flow_run_id = flow_run_id",
            "        state_create.state_details.transition_id = uuid4()",
            "        try:",
            "            response = await self._client.post(",
            "                f\"/flow_runs/{flow_run_id}/set_state\",",
            "                json=dict(state=state_create.dict(json_compatible=True), force=force),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return OrchestrationResult.parse_obj(response.json())",
            "",
            "    async def read_flow_run_states(",
            "        self, flow_run_id: UUID",
            "    ) -> List[prefect.states.State]:",
            "        \"\"\"",
            "        Query for the states of a flow run",
            "",
            "        Args:",
            "            flow_run_id: the id of the flow run",
            "",
            "        Returns:",
            "            a list of State model representations",
            "                of the flow run states",
            "        \"\"\"",
            "        response = await self._client.get(",
            "            \"/flow_run_states/\", params=dict(flow_run_id=str(flow_run_id))",
            "        )",
            "        return pydantic.parse_obj_as(List[prefect.states.State], response.json())",
            "",
            "    async def set_task_run_name(self, task_run_id: UUID, name: str):",
            "        task_run_data = TaskRunUpdate(name=name)",
            "        return await self._client.patch(",
            "            f\"/task_runs/{task_run_id}\",",
            "            json=task_run_data.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "",
            "    async def create_task_run(",
            "        self,",
            "        task: \"TaskObject\",",
            "        flow_run_id: Optional[UUID],",
            "        dynamic_key: str,",
            "        name: str = None,",
            "        extra_tags: Iterable[str] = None,",
            "        state: prefect.states.State = None,",
            "        task_inputs: Dict[",
            "            str,",
            "            List[",
            "                Union[",
            "                    TaskRunResult,",
            "                    Parameter,",
            "                    Constant,",
            "                ]",
            "            ],",
            "        ] = None,",
            "    ) -> TaskRun:",
            "        \"\"\"",
            "        Create a task run",
            "",
            "        Args:",
            "            task: The Task to run",
            "            flow_run_id: The flow run id with which to associate the task run",
            "            dynamic_key: A key unique to this particular run of a Task within the flow",
            "            name: An optional name for the task run",
            "            extra_tags: an optional list of extra tags to apply to the task run in",
            "                addition to `task.tags`",
            "            state: The initial state for the run. If not provided, defaults to",
            "                `Pending` for now. Should always be a `Scheduled` type.",
            "            task_inputs: the set of inputs passed to the task",
            "",
            "        Returns:",
            "            The created task run.",
            "        \"\"\"",
            "        tags = set(task.tags).union(extra_tags or [])",
            "",
            "        if state is None:",
            "            state = prefect.states.Pending()",
            "",
            "        task_run_data = TaskRunCreate(",
            "            name=name,",
            "            flow_run_id=flow_run_id,",
            "            task_key=task.task_key,",
            "            dynamic_key=dynamic_key,",
            "            tags=list(tags),",
            "            task_version=task.version,",
            "            empirical_policy=TaskRunPolicy(",
            "                retries=task.retries,",
            "                retry_delay=task.retry_delay_seconds,",
            "                retry_jitter_factor=task.retry_jitter_factor,",
            "            ),",
            "            state=state.to_state_create(),",
            "            task_inputs=task_inputs or {},",
            "        )",
            "",
            "        response = await self._client.post(",
            "            \"/task_runs/\", json=task_run_data.dict(json_compatible=True)",
            "        )",
            "        return TaskRun.parse_obj(response.json())",
            "",
            "    async def read_task_run(self, task_run_id: UUID) -> TaskRun:",
            "        \"\"\"",
            "        Query the Prefect API for a task run by id.",
            "",
            "        Args:",
            "            task_run_id: the task run ID of interest",
            "",
            "        Returns:",
            "            a Task Run model representation of the task run",
            "        \"\"\"",
            "        response = await self._client.get(f\"/task_runs/{task_run_id}\")",
            "        return TaskRun.parse_obj(response.json())",
            "",
            "    async def read_task_runs(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        sort: TaskRunSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[TaskRun]:",
            "        \"\"\"",
            "        Query the Prefect API for task runs. Only task runs matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            sort: sort criteria for the task runs",
            "            limit: a limit for the task run query",
            "            offset: an offset for the task run query",
            "",
            "        Returns:",
            "            a list of Task Run model representations",
            "                of the task runs",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(\"/task_runs/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[TaskRun], response.json())",
            "",
            "    async def delete_task_run(self, task_run_id: UUID) -> None:",
            "        \"\"\"",
            "        Delete a task run by id.",
            "",
            "        Args:",
            "            task_run_id: the task run ID of interest",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/task_runs/{task_run_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def set_task_run_state(",
            "        self,",
            "        task_run_id: UUID,",
            "        state: prefect.states.State,",
            "        force: bool = False,",
            "    ) -> OrchestrationResult:",
            "        \"\"\"",
            "        Set the state of a task run.",
            "",
            "        Args:",
            "            task_run_id: the id of the task run",
            "            state: the state to set",
            "            force: if True, disregard orchestration logic when setting the state,",
            "                forcing the Prefect API to accept the state",
            "",
            "        Returns:",
            "            an OrchestrationResult model representation of state orchestration output",
            "        \"\"\"",
            "        state_create = state.to_state_create()",
            "        state_create.state_details.task_run_id = task_run_id",
            "        response = await self._client.post(",
            "            f\"/task_runs/{task_run_id}/set_state\",",
            "            json=dict(state=state_create.dict(json_compatible=True), force=force),",
            "        )",
            "        return OrchestrationResult.parse_obj(response.json())",
            "",
            "    async def read_task_run_states(",
            "        self, task_run_id: UUID",
            "    ) -> List[prefect.states.State]:",
            "        \"\"\"",
            "        Query for the states of a task run",
            "",
            "        Args:",
            "            task_run_id: the id of the task run",
            "",
            "        Returns:",
            "            a list of State model representations of the task run states",
            "        \"\"\"",
            "        response = await self._client.get(",
            "            \"/task_run_states/\", params=dict(task_run_id=str(task_run_id))",
            "        )",
            "        return pydantic.parse_obj_as(List[prefect.states.State], response.json())",
            "",
            "    async def create_logs(self, logs: Iterable[Union[LogCreate, dict]]) -> None:",
            "        \"\"\"",
            "        Create logs for a flow or task run",
            "",
            "        Args:",
            "            logs: An iterable of `LogCreate` objects or already json-compatible dicts",
            "        \"\"\"",
            "        serialized_logs = [",
            "            log.dict(json_compatible=True) if isinstance(log, LogCreate) else log",
            "            for log in logs",
            "        ]",
            "        await self._client.post(\"/logs/\", json=serialized_logs)",
            "",
            "    async def create_flow_run_notification_policy(",
            "        self,",
            "        block_document_id: UUID,",
            "        is_active: bool = True,",
            "        tags: List[str] = None,",
            "        state_names: List[str] = None,",
            "        message_template: Optional[str] = None,",
            "    ) -> UUID:",
            "        \"\"\"",
            "        Create a notification policy for flow runs",
            "",
            "        Args:",
            "            block_document_id: The block document UUID",
            "            is_active: Whether the notification policy is active",
            "            tags: List of flow tags",
            "            state_names: List of state names",
            "            message_template: Notification message template",
            "        \"\"\"",
            "        if tags is None:",
            "            tags = []",
            "        if state_names is None:",
            "            state_names = []",
            "",
            "        policy = FlowRunNotificationPolicyCreate(",
            "            block_document_id=block_document_id,",
            "            is_active=is_active,",
            "            tags=tags,",
            "            state_names=state_names,",
            "            message_template=message_template,",
            "        )",
            "        response = await self._client.post(",
            "            \"/flow_run_notification_policies/\",",
            "            json=policy.dict(json_compatible=True),",
            "        )",
            "",
            "        policy_id = response.json().get(\"id\")",
            "        if not policy_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(policy_id)",
            "",
            "    async def delete_flow_run_notification_policy(",
            "        self,",
            "        id: UUID,",
            "    ) -> None:",
            "        \"\"\"",
            "        Delete a flow run notification policy by id.",
            "",
            "        Args:",
            "            id: UUID of the flow run notification policy to delete.",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/flow_run_notification_policies/{id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def update_flow_run_notification_policy(",
            "        self,",
            "        id: UUID,",
            "        block_document_id: Optional[UUID] = None,",
            "        is_active: Optional[bool] = None,",
            "        tags: Optional[List[str]] = None,",
            "        state_names: Optional[List[str]] = None,",
            "        message_template: Optional[str] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a notification policy for flow runs",
            "",
            "        Args:",
            "            id: UUID of the notification policy",
            "            block_document_id: The block document UUID",
            "            is_active: Whether the notification policy is active",
            "            tags: List of flow tags",
            "            state_names: List of state names",
            "            message_template: Notification message template",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        params = {}",
            "        if block_document_id is not None:",
            "            params[\"block_document_id\"] = block_document_id",
            "        if is_active is not None:",
            "            params[\"is_active\"] = is_active",
            "        if tags is not None:",
            "            params[\"tags\"] = tags",
            "        if state_names is not None:",
            "            params[\"state_names\"] = state_names",
            "        if message_template is not None:",
            "            params[\"message_template\"] = message_template",
            "",
            "        policy = FlowRunNotificationPolicyUpdate(**params)",
            "",
            "        try:",
            "            await self._client.patch(",
            "                f\"/flow_run_notification_policies/{id}\",",
            "                json=policy.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_flow_run_notification_policies(",
            "        self,",
            "        flow_run_notification_policy_filter: FlowRunNotificationPolicyFilter,",
            "        limit: Optional[int] = None,",
            "        offset: int = 0,",
            "    ) -> List[FlowRunNotificationPolicy]:",
            "        \"\"\"",
            "        Query the Prefect API for flow run notification policies. Only policies matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_run_notification_policy_filter: filter criteria for notification policies",
            "            limit: a limit for the notification policies query",
            "            offset: an offset for the notification policies query",
            "",
            "        Returns:",
            "            a list of FlowRunNotificationPolicy model representations",
            "                of the notification policies",
            "        \"\"\"",
            "        body = {",
            "            \"flow_run_notification_policy_filter\": (",
            "                flow_run_notification_policy_filter.dict(json_compatible=True)",
            "                if flow_run_notification_policy_filter",
            "                else None",
            "            ),",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(",
            "            \"/flow_run_notification_policies/filter\", json=body",
            "        )",
            "        return pydantic.parse_obj_as(List[FlowRunNotificationPolicy], response.json())",
            "",
            "    async def read_logs(",
            "        self,",
            "        log_filter: LogFilter = None,",
            "        limit: int = None,",
            "        offset: int = None,",
            "        sort: LogSort = LogSort.TIMESTAMP_ASC,",
            "    ) -> List[Log]:",
            "        \"\"\"",
            "        Read flow and task run logs.",
            "        \"\"\"",
            "        body = {",
            "            \"logs\": log_filter.dict(json_compatible=True) if log_filter else None,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "            \"sort\": sort,",
            "        }",
            "",
            "        response = await self._client.post(\"/logs/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[Log], response.json())",
            "",
            "    async def resolve_datadoc(self, datadoc: DataDocument) -> Any:",
            "        \"\"\"",
            "        Recursively decode possibly nested data documents.",
            "",
            "        \"server\" encoded documents will be retrieved from the server.",
            "",
            "        Args:",
            "            datadoc: The data document to resolve",
            "",
            "        Returns:",
            "            a decoded object, the innermost data",
            "        \"\"\"",
            "        if not isinstance(datadoc, DataDocument):",
            "            raise TypeError(",
            "                f\"`resolve_datadoc` received invalid type {type(datadoc).__name__}\"",
            "            )",
            "",
            "        async def resolve_inner(data):",
            "            if isinstance(data, bytes):",
            "                try:",
            "                    data = DataDocument.parse_raw(data)",
            "                except pydantic.ValidationError:",
            "                    return data",
            "",
            "            if isinstance(data, DataDocument):",
            "                return await resolve_inner(data.decode())",
            "",
            "            return data",
            "",
            "        return await resolve_inner(datadoc)",
            "",
            "    async def send_worker_heartbeat(",
            "        self,",
            "        work_pool_name: str,",
            "        worker_name: str,",
            "        heartbeat_interval_seconds: Optional[float] = None,",
            "    ):",
            "        \"\"\"",
            "        Sends a worker heartbeat for a given work pool.",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool to heartbeat against.",
            "            worker_name: The name of the worker sending the heartbeat.",
            "        \"\"\"",
            "        await self._client.post(",
            "            f\"/work_pools/{work_pool_name}/workers/heartbeat\",",
            "            json={",
            "                \"name\": worker_name,",
            "                \"heartbeat_interval_seconds\": heartbeat_interval_seconds,",
            "            },",
            "        )",
            "",
            "    async def read_workers_for_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "        worker_filter: Optional[WorkerFilter] = None,",
            "        offset: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "    ) -> List[Worker]:",
            "        \"\"\"",
            "        Reads workers for a given work pool.",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool for which to get",
            "                member workers.",
            "            worker_filter: Criteria by which to filter workers.",
            "            limit: Limit for the worker query.",
            "            offset: Limit for the worker query.",
            "        \"\"\"",
            "        response = await self._client.post(",
            "            f\"/work_pools/{work_pool_name}/workers/filter\",",
            "            json={",
            "                \"worker_filter\": (",
            "                    worker_filter.dict(json_compatible=True, exclude_unset=True)",
            "                    if worker_filter",
            "                    else None",
            "                ),",
            "                \"offset\": offset,",
            "                \"limit\": limit,",
            "            },",
            "        )",
            "",
            "        return pydantic.parse_obj_as(List[Worker], response.json())",
            "",
            "    async def read_work_pool(self, work_pool_name: str) -> WorkPool:",
            "        \"\"\"",
            "        Reads information for a given work pool",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool to for which to get",
            "                information.",
            "",
            "        Returns:",
            "            Information about the requested work pool.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/work_pools/{work_pool_name}\")",
            "            return pydantic.parse_obj_as(WorkPool, response.json())",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_work_pools(",
            "        self,",
            "        limit: Optional[int] = None,",
            "        offset: int = 0,",
            "        work_pool_filter: Optional[WorkPoolFilter] = None,",
            "    ) -> List[WorkPool]:",
            "        \"\"\"",
            "        Reads work pools.",
            "",
            "        Args:",
            "            limit: Limit for the work pool query.",
            "            offset: Offset for the work pool query.",
            "            work_pool_filter: Criteria by which to filter work pools.",
            "",
            "        Returns:",
            "            A list of work pools.",
            "        \"\"\"",
            "",
            "        body = {",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "        }",
            "        response = await self._client.post(\"/work_pools/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[WorkPool], response.json())",
            "",
            "    async def create_work_pool(",
            "        self,",
            "        work_pool: WorkPoolCreate,",
            "    ) -> WorkPool:",
            "        \"\"\"",
            "        Creates a work pool with the provided configuration.",
            "",
            "        Args:",
            "            work_pool: Desired configuration for the new work pool.",
            "",
            "        Returns:",
            "            Information about the newly created work pool.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/work_pools/\",",
            "                json=work_pool.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return pydantic.parse_obj_as(WorkPool, response.json())",
            "",
            "    async def update_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "        work_pool: WorkPoolUpdate,",
            "    ):",
            "        \"\"\"",
            "        Updates a work pool.",
            "",
            "        Args:",
            "            work_pool_name: Name of the work pool to update.",
            "            work_pool: Fields to update in the work pool.",
            "        \"\"\"",
            "        try:",
            "            await self._client.patch(",
            "                f\"/work_pools/{work_pool_name}\",",
            "                json=work_pool.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "    ):",
            "        \"\"\"",
            "        Deletes a work pool.",
            "",
            "        Args:",
            "            work_pool_name: Name of the work pool to delete.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/work_pools/{work_pool_name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_work_queues(",
            "        self,",
            "        work_pool_name: Optional[str] = None,",
            "        work_queue_filter: Optional[WorkQueueFilter] = None,",
            "        limit: Optional[int] = None,",
            "        offset: Optional[int] = None,",
            "    ) -> List[WorkQueue]:",
            "        \"\"\"",
            "        Retrieves queues for a work pool.",
            "",
            "        Args:",
            "            work_pool_name: Name of the work pool for which to get queues.",
            "            work_queue_filter: Criteria by which to filter queues.",
            "            limit: Limit for the queue query.",
            "            offset: Limit for the queue query.",
            "",
            "        Returns:",
            "            List of queues for the specified work pool.",
            "        \"\"\"",
            "        json = {",
            "            \"work_queues\": (",
            "                work_queue_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        if work_pool_name:",
            "            try:",
            "                response = await self._client.post(",
            "                    f\"/work_pools/{work_pool_name}/queues/filter\",",
            "                    json=json,",
            "                )",
            "            except httpx.HTTPStatusError as e:",
            "                if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                    raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "                else:",
            "                    raise",
            "        else:",
            "            response = await self._client.post(\"/work_queues/filter\", json=json)",
            "",
            "        return pydantic.parse_obj_as(List[WorkQueue], response.json())",
            "",
            "    async def get_scheduled_flow_runs_for_deployments(",
            "        self,",
            "        deployment_ids: List[UUID],",
            "        scheduled_before: Optional[datetime.datetime] = None,",
            "        limit: Optional[int] = None,",
            "    ):",
            "        body: Dict[str, Any] = dict(deployment_ids=[str(id) for id in deployment_ids])",
            "        if scheduled_before:",
            "            body[\"scheduled_before\"] = str(scheduled_before)",
            "        if limit:",
            "            body[\"limit\"] = limit",
            "",
            "        response = await self._client.post(",
            "            \"/deployments/get_scheduled_flow_runs\",",
            "            json=body,",
            "        )",
            "",
            "        return pydantic.parse_obj_as(List[FlowRunResponse], response.json())",
            "",
            "    async def get_scheduled_flow_runs_for_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "        work_queue_names: Optional[List[str]] = None,",
            "        scheduled_before: Optional[datetime.datetime] = None,",
            "    ) -> List[WorkerFlowRunResponse]:",
            "        \"\"\"",
            "        Retrieves scheduled flow runs for the provided set of work pool queues.",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool that the work pool",
            "                queues are associated with.",
            "            work_queue_names: The names of the work pool queues from which",
            "                to get scheduled flow runs.",
            "            scheduled_before: Datetime used to filter returned flow runs. Flow runs",
            "                scheduled for after the given datetime string will not be returned.",
            "",
            "        Returns:",
            "            A list of worker flow run responses containing information about the",
            "            retrieved flow runs.",
            "        \"\"\"",
            "        body: Dict[str, Any] = {}",
            "        if work_queue_names is not None:",
            "            body[\"work_queue_names\"] = list(work_queue_names)",
            "        if scheduled_before:",
            "            body[\"scheduled_before\"] = str(scheduled_before)",
            "",
            "        response = await self._client.post(",
            "            f\"/work_pools/{work_pool_name}/get_scheduled_flow_runs\",",
            "            json=body,",
            "        )",
            "        return pydantic.parse_obj_as(List[WorkerFlowRunResponse], response.json())",
            "",
            "    async def create_artifact(",
            "        self,",
            "        artifact: ArtifactCreate,",
            "    ) -> Artifact:",
            "        \"\"\"",
            "        Creates an artifact with the provided configuration.",
            "",
            "        Args:",
            "            artifact: Desired configuration for the new artifact.",
            "        Returns:",
            "            Information about the newly created artifact.",
            "        \"\"\"",
            "",
            "        response = await self._client.post(",
            "            \"/artifacts/\",",
            "            json=artifact.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "",
            "        return pydantic.parse_obj_as(Artifact, response.json())",
            "",
            "    async def read_artifacts(",
            "        self,",
            "        *,",
            "        artifact_filter: ArtifactFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        sort: ArtifactSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[Artifact]:",
            "        \"\"\"",
            "        Query the Prefect API for artifacts. Only artifacts matching all criteria will",
            "        be returned.",
            "        Args:",
            "            artifact_filter: filter criteria for artifacts",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            sort: sort criteria for the artifacts",
            "            limit: limit for the artifact query",
            "            offset: offset for the artifact query",
            "        Returns:",
            "            a list of Artifact model representations of the artifacts",
            "        \"\"\"",
            "        body = {",
            "            \"artifacts\": (",
            "                artifact_filter.dict(json_compatible=True) if artifact_filter else None",
            "            ),",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(\"/artifacts/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[Artifact], response.json())",
            "",
            "    async def read_latest_artifacts(",
            "        self,",
            "        *,",
            "        artifact_filter: ArtifactCollectionFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        sort: ArtifactCollectionSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[ArtifactCollection]:",
            "        \"\"\"",
            "        Query the Prefect API for artifacts. Only artifacts matching all criteria will",
            "        be returned.",
            "        Args:",
            "            artifact_filter: filter criteria for artifacts",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            sort: sort criteria for the artifacts",
            "            limit: limit for the artifact query",
            "            offset: offset for the artifact query",
            "        Returns:",
            "            a list of Artifact model representations of the artifacts",
            "        \"\"\"",
            "        body = {",
            "            \"artifacts\": (",
            "                artifact_filter.dict(json_compatible=True) if artifact_filter else None",
            "            ),",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(\"/artifacts/latest/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[ArtifactCollection], response.json())",
            "",
            "    async def delete_artifact(self, artifact_id: UUID) -> None:",
            "        \"\"\"",
            "        Deletes an artifact with the provided id.",
            "",
            "        Args:",
            "            artifact_id: The id of the artifact to delete.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/artifacts/{artifact_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_variable_by_name(self, name: str) -> Optional[Variable]:",
            "        \"\"\"Reads a variable by name. Returns None if no variable is found.\"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/variables/name/{name}\")",
            "            return pydantic.parse_obj_as(Variable, response.json())",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                return None",
            "            else:",
            "                raise",
            "",
            "    async def delete_variable_by_name(self, name: str):",
            "        \"\"\"Deletes a variable by name.\"\"\"",
            "        try:",
            "            await self._client.delete(f\"/variables/name/{name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_variables(self, limit: int = None) -> List[Variable]:",
            "        \"\"\"Reads all variables.\"\"\"",
            "        response = await self._client.post(\"/variables/filter\", json={\"limit\": limit})",
            "        return pydantic.parse_obj_as(List[Variable], response.json())",
            "",
            "    async def read_worker_metadata(self) -> Dict[str, Any]:",
            "        \"\"\"Reads worker metadata stored in Prefect collection registry.\"\"\"",
            "        response = await self._client.get(\"collections/views/aggregate-worker-metadata\")",
            "        response.raise_for_status()",
            "        return response.json()",
            "",
            "    async def create_automation(self, automation: Automation) -> UUID:",
            "        \"\"\"Creates an automation in Prefect Cloud.\"\"\"",
            "        if self.server_type != ServerType.CLOUD:",
            "            raise RuntimeError(\"Automations are only supported for Prefect Cloud.\")",
            "",
            "        response = await self._client.post(",
            "            \"/automations/\",",
            "            json=automation.dict(json_compatible=True),",
            "        )",
            "",
            "        return UUID(response.json()[\"id\"])",
            "",
            "    async def read_resource_related_automations(",
            "        self, resource_id: str",
            "    ) -> List[ExistingAutomation]:",
            "        if self.server_type != ServerType.CLOUD:",
            "            raise RuntimeError(\"Automations are only supported for Prefect Cloud.\")",
            "",
            "        response = await self._client.get(f\"/automations/related-to/{resource_id}\")",
            "        response.raise_for_status()",
            "        return pydantic.parse_obj_as(List[ExistingAutomation], response.json())",
            "",
            "    async def delete_resource_owned_automations(self, resource_id: str):",
            "        if self.server_type != ServerType.CLOUD:",
            "            raise RuntimeError(\"Automations are only supported for Prefect Cloud.\")",
            "",
            "        await self._client.delete(f\"/automations/owned-by/{resource_id}\")",
            "",
            "    async def increment_concurrency_slots(",
            "        self, names: List[str], slots: int, mode: str",
            "    ) -> httpx.Response:",
            "        return await self._client.post(",
            "            \"/v2/concurrency_limits/increment\",",
            "            json={\"names\": names, \"slots\": slots, \"mode\": mode},",
            "        )",
            "",
            "    async def release_concurrency_slots(",
            "        self, names: List[str], slots: int, occupancy_seconds: float",
            "    ) -> httpx.Response:",
            "        return await self._client.post(",
            "            \"/v2/concurrency_limits/decrement\",",
            "            json={",
            "                \"names\": names,",
            "                \"slots\": slots,",
            "                \"occupancy_seconds\": occupancy_seconds,",
            "            },",
            "        )",
            "",
            "    async def create_global_concurrency_limit(",
            "        self, concurrency_limit: GlobalConcurrencyLimitCreate",
            "    ) -> UUID:",
            "        response = await self._client.post(",
            "            \"/v2/concurrency_limits/\",",
            "            json=concurrency_limit.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "        return UUID(response.json()[\"id\"])",
            "",
            "    async def update_global_concurrency_limit(",
            "        self, name: str, concurrency_limit: GlobalConcurrencyLimitUpdate",
            "    ) -> httpx.Response:",
            "        try:",
            "            response = await self._client.patch(",
            "                f\"/v2/concurrency_limits/{name}\",",
            "                json=concurrency_limit.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "            return response",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_global_concurrency_limit_by_name(",
            "        self, name: str",
            "    ) -> httpx.Response:",
            "        try:",
            "            response = await self._client.delete(f\"/v2/concurrency_limits/{name}\")",
            "            return response",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_global_concurrency_limit_by_name(",
            "        self, name: str",
            "    ) -> Dict[str, object]:",
            "        try:",
            "            response = await self._client.get(f\"/v2/concurrency_limits/{name}\")",
            "            return response.json()",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_global_concurrency_limits(",
            "        self, limit: int = 10, offset: int = 0",
            "    ) -> List[Dict[str, object]]:",
            "        response = await self._client.post(",
            "            \"/v2/concurrency_limits/filter\",",
            "            json={",
            "                \"limit\": limit,",
            "                \"offset\": offset,",
            "            },",
            "        )",
            "        return response.json()",
            "",
            "    async def create_flow_run_input(",
            "        self, flow_run_id: UUID, key: str, value: str, sender: Optional[str] = None",
            "    ):",
            "        \"\"\"",
            "        Creates a flow run input.",
            "",
            "        Args:",
            "            flow_run_id: The flow run id.",
            "            key: The input key.",
            "            value: The input value.",
            "            sender: The sender of the input.",
            "        \"\"\"",
            "",
            "        # Initialize the input to ensure that the key is valid.",
            "        FlowRunInput(flow_run_id=flow_run_id, key=key, value=value)",
            "",
            "        response = await self._client.post(",
            "            f\"/flow_runs/{flow_run_id}/input\",",
            "            json={\"key\": key, \"value\": value, \"sender\": sender},",
            "        )",
            "        response.raise_for_status()",
            "",
            "    async def filter_flow_run_input(",
            "        self, flow_run_id: UUID, key_prefix: str, limit: int, exclude_keys: Set[str]",
            "    ) -> List[FlowRunInput]:",
            "        response = await self._client.post(",
            "            f\"/flow_runs/{flow_run_id}/input/filter\",",
            "            json={",
            "                \"prefix\": key_prefix,",
            "                \"limit\": limit,",
            "                \"exclude_keys\": list(exclude_keys),",
            "            },",
            "        )",
            "        response.raise_for_status()",
            "        return pydantic.parse_obj_as(List[FlowRunInput], response.json())",
            "",
            "    async def read_flow_run_input(self, flow_run_id: UUID, key: str) -> str:",
            "        \"\"\"",
            "        Reads a flow run input.",
            "",
            "        Args:",
            "            flow_run_id: The flow run id.",
            "            key: The input key.",
            "        \"\"\"",
            "        response = await self._client.get(f\"/flow_runs/{flow_run_id}/input/{key}\")",
            "        response.raise_for_status()",
            "        return response.content.decode()",
            "",
            "    async def delete_flow_run_input(self, flow_run_id: UUID, key: str):",
            "        \"\"\"",
            "        Deletes a flow run input.",
            "",
            "        Args:",
            "            flow_run_id: The flow run id.",
            "            key: The input key.",
            "        \"\"\"",
            "        response = await self._client.delete(f\"/flow_runs/{flow_run_id}/input/{key}\")",
            "        response.raise_for_status()",
            "",
            "    async def __aenter__(self):",
            "        \"\"\"",
            "        Start the client.",
            "",
            "        If the client is already started, this will raise an exception.",
            "",
            "        If the client is already closed, this will raise an exception. Use a new client",
            "        instance instead.",
            "        \"\"\"",
            "        if self._closed:",
            "            # httpx.AsyncClient does not allow reuse so we will not either.",
            "            raise RuntimeError(",
            "                \"The client cannot be started again after closing. \"",
            "                \"Retrieve a new client with `get_client()` instead.\"",
            "            )",
            "",
            "        if self._started:",
            "            # httpx.AsyncClient does not allow reentrancy so we will not either.",
            "            raise RuntimeError(\"The client cannot be started more than once.\")",
            "",
            "        self._loop = asyncio.get_running_loop()",
            "        await self._exit_stack.__aenter__()",
            "",
            "        # Enter a lifespan context if using an ephemeral application.",
            "        # See https://github.com/encode/httpx/issues/350",
            "        if self._ephemeral_app and self.manage_lifespan:",
            "            self._ephemeral_lifespan = await self._exit_stack.enter_async_context(",
            "                app_lifespan_context(self._ephemeral_app)",
            "            )",
            "",
            "        if self._ephemeral_app:",
            "            self.logger.debug(",
            "                \"Using ephemeral application with database at \"",
            "                f\"{PREFECT_API_DATABASE_CONNECTION_URL.value()}\"",
            "            )",
            "        else:",
            "            self.logger.debug(f\"Connecting to API at {self.api_url}\")",
            "",
            "        # Enter the httpx client's context",
            "        await self._exit_stack.enter_async_context(self._client)",
            "",
            "        self._started = True",
            "",
            "        return self",
            "",
            "    async def __aexit__(self, *exc_info):",
            "        \"\"\"",
            "        Shutdown the client.",
            "        \"\"\"",
            "        self._closed = True",
            "        return await self._exit_stack.__aexit__(*exc_info)",
            "",
            "    def __enter__(self):",
            "        raise RuntimeError(",
            "            \"The `PrefectClient` must be entered with an async context. Use 'async \"",
            "            \"with PrefectClient(...)' not 'with PrefectClient(...)'\"",
            "        )",
            "",
            "    def __exit__(self, *_):",
            "        assert False, \"This should never be called but must be defined for __enter__\""
        ],
        "afterPatchFile": [
            "import asyncio",
            "import datetime",
            "import warnings",
            "from contextlib import AsyncExitStack",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Union,",
            ")",
            "from uuid import UUID, uuid4",
            "",
            "import httpcore",
            "import httpx",
            "import pendulum",
            "",
            "from prefect._internal.compatibility.experimental import (",
            "    EXPERIMENTAL_WARNING,",
            "    ExperimentalFeature,",
            "    experiment_enabled,",
            ")",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "from prefect.settings import (",
            "    PREFECT_EXPERIMENTAL_WARN,",
            "    PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES,",
            ")",
            "",
            "if HAS_PYDANTIC_V2:",
            "    import pydantic.v1 as pydantic",
            "else:",
            "    import pydantic",
            "",
            "from asgi_lifespan import LifespanManager",
            "from prefect._vendor.starlette import status",
            "",
            "import prefect",
            "import prefect.exceptions",
            "import prefect.settings",
            "import prefect.states",
            "from prefect.client.constants import SERVER_API_VERSION",
            "from prefect.client.schemas import FlowRun, OrchestrationResult, TaskRun",
            "from prefect.client.schemas.actions import (",
            "    ArtifactCreate,",
            "    BlockDocumentCreate,",
            "    BlockDocumentUpdate,",
            "    BlockSchemaCreate,",
            "    BlockTypeCreate,",
            "    BlockTypeUpdate,",
            "    ConcurrencyLimitCreate,",
            "    DeploymentCreate,",
            "    DeploymentFlowRunCreate,",
            "    DeploymentScheduleCreate,",
            "    DeploymentScheduleUpdate,",
            "    DeploymentUpdate,",
            "    FlowCreate,",
            "    FlowRunCreate,",
            "    FlowRunNotificationPolicyCreate,",
            "    FlowRunNotificationPolicyUpdate,",
            "    FlowRunUpdate,",
            "    GlobalConcurrencyLimitCreate,",
            "    GlobalConcurrencyLimitUpdate,",
            "    LogCreate,",
            "    TaskRunCreate,",
            "    TaskRunUpdate,",
            "    WorkPoolCreate,",
            "    WorkPoolUpdate,",
            "    WorkQueueCreate,",
            "    WorkQueueUpdate,",
            ")",
            "from prefect.client.schemas.filters import (",
            "    ArtifactCollectionFilter,",
            "    ArtifactFilter,",
            "    DeploymentFilter,",
            "    FlowFilter,",
            "    FlowRunFilter,",
            "    FlowRunNotificationPolicyFilter,",
            "    LogFilter,",
            "    TaskRunFilter,",
            "    WorkerFilter,",
            "    WorkPoolFilter,",
            "    WorkQueueFilter,",
            "    WorkQueueFilterName,",
            ")",
            "from prefect.client.schemas.objects import (",
            "    Artifact,",
            "    ArtifactCollection,",
            "    BlockDocument,",
            "    BlockSchema,",
            "    BlockType,",
            "    ConcurrencyLimit,",
            "    Constant,",
            "    Deployment,",
            "    DeploymentSchedule,",
            "    Flow,",
            "    FlowRunInput,",
            "    FlowRunNotificationPolicy,",
            "    FlowRunPolicy,",
            "    Log,",
            "    Parameter,",
            "    QueueFilter,",
            "    TaskRunPolicy,",
            "    TaskRunResult,",
            "    Variable,",
            "    Worker,",
            "    WorkPool,",
            "    WorkQueue,",
            "    WorkQueueStatusDetail,",
            ")",
            "from prefect.client.schemas.responses import (",
            "    DeploymentResponse,",
            "    FlowRunResponse,",
            "    WorkerFlowRunResponse,",
            ")",
            "from prefect.client.schemas.schedules import SCHEDULE_TYPES",
            "from prefect.client.schemas.sorting import (",
            "    ArtifactCollectionSort,",
            "    ArtifactSort,",
            "    DeploymentSort,",
            "    FlowRunSort,",
            "    FlowSort,",
            "    LogSort,",
            "    TaskRunSort,",
            ")",
            "from prefect.deprecated.data_documents import DataDocument",
            "from prefect.events.schemas import Automation, ExistingAutomation",
            "from prefect.logging import get_logger",
            "from prefect.settings import (",
            "    PREFECT_API_DATABASE_CONNECTION_URL,",
            "    PREFECT_API_ENABLE_HTTP2,",
            "    PREFECT_API_KEY,",
            "    PREFECT_API_REQUEST_TIMEOUT,",
            "    PREFECT_API_TLS_INSECURE_SKIP_VERIFY,",
            "    PREFECT_API_URL,",
            "    PREFECT_CLIENT_CSRF_SUPPORT_ENABLED,",
            "    PREFECT_CLOUD_API_URL,",
            "    PREFECT_UNIT_TEST_MODE,",
            ")",
            "from prefect.utilities.collections import AutoEnum",
            "",
            "if TYPE_CHECKING:",
            "    from prefect.flows import Flow as FlowObject",
            "    from prefect.tasks import Task as TaskObject",
            "",
            "from prefect.client.base import ASGIApp, PrefectHttpxClient, app_lifespan_context",
            "",
            "",
            "class ServerType(AutoEnum):",
            "    EPHEMERAL = AutoEnum.auto()",
            "    SERVER = AutoEnum.auto()",
            "    CLOUD = AutoEnum.auto()",
            "",
            "",
            "def get_client(httpx_settings: Optional[dict] = None) -> \"PrefectClient\":",
            "    \"\"\"",
            "    Retrieve a HTTP client for communicating with the Prefect REST API.",
            "",
            "    The client must be context managed; for example:",
            "",
            "    ```python",
            "    async with get_client() as client:",
            "        await client.hello()",
            "    ```",
            "    \"\"\"",
            "    ctx = prefect.context.get_settings_context()",
            "    api = PREFECT_API_URL.value()",
            "",
            "    if not api:",
            "        # create an ephemeral API if none was provided",
            "        from prefect.server.api.server import create_app",
            "",
            "        api = create_app(ctx.settings, ephemeral=True)",
            "",
            "    return PrefectClient(",
            "        api,",
            "        api_key=PREFECT_API_KEY.value(),",
            "        httpx_settings=httpx_settings,",
            "    )",
            "",
            "",
            "class PrefectClient:",
            "    \"\"\"",
            "    An asynchronous client for interacting with the [Prefect REST API](/api-ref/rest-api/).",
            "",
            "    Args:",
            "        api: the REST API URL or FastAPI application to connect to",
            "        api_key: An optional API key for authentication.",
            "        api_version: The API version this client is compatible with.",
            "        httpx_settings: An optional dictionary of settings to pass to the underlying",
            "            `httpx.AsyncClient`",
            "",
            "    Examples:",
            "",
            "        Say hello to a Prefect REST API",
            "",
            "        <div class=\"terminal\">",
            "        ```",
            "        >>> async with get_client() as client:",
            "        >>>     response = await client.hello()",
            "        >>>",
            "        >>> print(response.json())",
            "        \ud83d\udc4b",
            "        ```",
            "        </div>",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        api: Union[str, ASGIApp],",
            "        *,",
            "        api_key: str = None,",
            "        api_version: str = None,",
            "        httpx_settings: dict = None,",
            "    ) -> None:",
            "        httpx_settings = httpx_settings.copy() if httpx_settings else {}",
            "        httpx_settings.setdefault(\"headers\", {})",
            "",
            "        if PREFECT_API_TLS_INSECURE_SKIP_VERIFY:",
            "            httpx_settings.setdefault(\"verify\", False)",
            "",
            "        if api_version is None:",
            "            api_version = SERVER_API_VERSION",
            "        httpx_settings[\"headers\"].setdefault(\"X-PREFECT-API-VERSION\", api_version)",
            "        if api_key:",
            "            httpx_settings[\"headers\"].setdefault(\"Authorization\", f\"Bearer {api_key}\")",
            "",
            "        # Context management",
            "        self._exit_stack = AsyncExitStack()",
            "        self._ephemeral_app: Optional[ASGIApp] = None",
            "        self.manage_lifespan = True",
            "        self.server_type: ServerType",
            "",
            "        # Only set if this client started the lifespan of the application",
            "        self._ephemeral_lifespan: Optional[LifespanManager] = None",
            "",
            "        self._closed = False",
            "        self._started = False",
            "",
            "        # Connect to an external application",
            "        if isinstance(api, str):",
            "            if httpx_settings.get(\"app\"):",
            "                raise ValueError(",
            "                    \"Invalid httpx settings: `app` cannot be set when providing an \"",
            "                    \"api url. `app` is only for use with ephemeral instances. Provide \"",
            "                    \"it as the `api` parameter instead.\"",
            "                )",
            "            httpx_settings.setdefault(\"base_url\", api)",
            "",
            "            # See https://www.python-httpx.org/advanced/#pool-limit-configuration",
            "            httpx_settings.setdefault(",
            "                \"limits\",",
            "                httpx.Limits(",
            "                    # We see instability when allowing the client to open many connections at once.",
            "                    # Limiting concurrency results in more stable performance.",
            "                    max_connections=16,",
            "                    max_keepalive_connections=8,",
            "                    # The Prefect Cloud LB will keep connections alive for 30s.",
            "                    # Only allow the client to keep connections alive for 25s.",
            "                    keepalive_expiry=25,",
            "                ),",
            "            )",
            "",
            "            # See https://www.python-httpx.org/http2/",
            "            # Enabling HTTP/2 support on the client does not necessarily mean that your requests",
            "            # and responses will be transported over HTTP/2, since both the client and the server",
            "            # need to support HTTP/2. If you connect to a server that only supports HTTP/1.1 the",
            "            # client will use a standard HTTP/1.1 connection instead.",
            "            httpx_settings.setdefault(\"http2\", PREFECT_API_ENABLE_HTTP2.value())",
            "",
            "            self.server_type = (",
            "                ServerType.CLOUD",
            "                if api.startswith(PREFECT_CLOUD_API_URL.value())",
            "                else ServerType.SERVER",
            "            )",
            "",
            "        # Connect to an in-process application",
            "        elif isinstance(api, ASGIApp):",
            "            self._ephemeral_app = api",
            "            self.server_type = ServerType.EPHEMERAL",
            "",
            "            # When using an ephemeral server, server-side exceptions can be raised",
            "            # client-side breaking all of our response error code handling. To work",
            "            # around this, we create an ASGI transport with application exceptions",
            "            # disabled instead of using the application directly.",
            "            # refs:",
            "            # - https://github.com/PrefectHQ/prefect/pull/9637",
            "            # - https://github.com/encode/starlette/blob/d3a11205ed35f8e5a58a711db0ff59c86fa7bb31/starlette/middleware/errors.py#L184",
            "            # - https://github.com/tiangolo/fastapi/blob/8cc967a7605d3883bd04ceb5d25cc94ae079612f/fastapi/applications.py#L163-L164",
            "            httpx_settings.setdefault(",
            "                \"transport\",",
            "                httpx.ASGITransport(",
            "                    app=self._ephemeral_app, raise_app_exceptions=False",
            "                ),",
            "            )",
            "            httpx_settings.setdefault(\"base_url\", \"http://ephemeral-prefect/api\")",
            "",
            "        else:",
            "            raise TypeError(",
            "                f\"Unexpected type {type(api).__name__!r} for argument `api`. Expected\"",
            "                \" 'str' or 'ASGIApp/FastAPI'\"",
            "            )",
            "",
            "        # See https://www.python-httpx.org/advanced/#timeout-configuration",
            "        httpx_settings.setdefault(",
            "            \"timeout\",",
            "            httpx.Timeout(",
            "                connect=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "                read=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "                write=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "                pool=PREFECT_API_REQUEST_TIMEOUT.value(),",
            "            ),",
            "        )",
            "",
            "        if not PREFECT_UNIT_TEST_MODE:",
            "            httpx_settings.setdefault(\"follow_redirects\", True)",
            "",
            "        enable_csrf_support = (",
            "            self.server_type != ServerType.CLOUD",
            "            and PREFECT_CLIENT_CSRF_SUPPORT_ENABLED.value()",
            "        )",
            "",
            "        self._client = PrefectHttpxClient(",
            "            **httpx_settings, enable_csrf_support=enable_csrf_support",
            "        )",
            "        self._loop = None",
            "",
            "        # See https://www.python-httpx.org/advanced/#custom-transports",
            "        #",
            "        # If we're using an HTTP/S client (not the ephemeral client), adjust the",
            "        # transport to add retries _after_ it is instantiated. If we alter the transport",
            "        # before instantiation, the transport will not be aware of proxies unless we",
            "        # reproduce all of the logic to make it so.",
            "        #",
            "        # Only alter the transport to set our default of 3 retries, don't modify any",
            "        # transport a user may have provided via httpx_settings.",
            "        #",
            "        # Making liberal use of getattr and isinstance checks here to avoid any",
            "        # surprises if the internals of httpx or httpcore change on us",
            "        if isinstance(api, str) and not httpx_settings.get(\"transport\"):",
            "            transport_for_url = getattr(self._client, \"_transport_for_url\", None)",
            "            if callable(transport_for_url):",
            "                server_transport = transport_for_url(httpx.URL(api))",
            "                if isinstance(server_transport, httpx.AsyncHTTPTransport):",
            "                    pool = getattr(server_transport, \"_pool\", None)",
            "                    if isinstance(pool, httpcore.AsyncConnectionPool):",
            "                        pool._retries = 3",
            "",
            "        self.logger = get_logger(\"client\")",
            "",
            "    @property",
            "    def api_url(self) -> httpx.URL:",
            "        \"\"\"",
            "        Get the base URL for the API.",
            "        \"\"\"",
            "        return self._client.base_url",
            "",
            "    # API methods ----------------------------------------------------------------------",
            "",
            "    async def api_healthcheck(self) -> Optional[Exception]:",
            "        \"\"\"",
            "        Attempts to connect to the API and returns the encountered exception if not",
            "        successful.",
            "",
            "        If successful, returns `None`.",
            "        \"\"\"",
            "        try:",
            "            await self._client.get(\"/health\")",
            "            return None",
            "        except Exception as exc:",
            "            return exc",
            "",
            "    async def hello(self) -> httpx.Response:",
            "        \"\"\"",
            "        Send a GET request to /hello for testing purposes.",
            "        \"\"\"",
            "        return await self._client.get(\"/hello\")",
            "",
            "    async def create_flow(self, flow: \"FlowObject\") -> UUID:",
            "        \"\"\"",
            "        Create a flow in the Prefect API.",
            "",
            "        Args:",
            "            flow: a [Flow][prefect.flows.Flow] object",
            "",
            "        Raises:",
            "            httpx.RequestError: if a flow was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the flow in the backend",
            "        \"\"\"",
            "        return await self.create_flow_from_name(flow.name)",
            "",
            "    async def create_flow_from_name(self, flow_name: str) -> UUID:",
            "        \"\"\"",
            "        Create a flow in the Prefect API.",
            "",
            "        Args:",
            "            flow_name: the name of the new flow",
            "",
            "        Raises:",
            "            httpx.RequestError: if a flow was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the flow in the backend",
            "        \"\"\"",
            "        flow_data = FlowCreate(name=flow_name)",
            "        response = await self._client.post(",
            "            \"/flows/\", json=flow_data.dict(json_compatible=True)",
            "        )",
            "",
            "        flow_id = response.json().get(\"id\")",
            "        if not flow_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        # Return the id of the created flow",
            "        return UUID(flow_id)",
            "",
            "    async def read_flow(self, flow_id: UUID) -> Flow:",
            "        \"\"\"",
            "        Query the Prefect API for a flow by id.",
            "",
            "        Args:",
            "            flow_id: the flow ID of interest",
            "",
            "        Returns:",
            "            a [Flow model][prefect.client.schemas.objects.Flow] representation of the flow",
            "        \"\"\"",
            "        response = await self._client.get(f\"/flows/{flow_id}\")",
            "        return Flow.parse_obj(response.json())",
            "",
            "    async def read_flows(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        work_pool_filter: WorkPoolFilter = None,",
            "        work_queue_filter: WorkQueueFilter = None,",
            "        sort: FlowSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[Flow]:",
            "        \"\"\"",
            "        Query the Prefect API for flows. Only flows matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            work_pool_filter: filter criteria for work pools",
            "            work_queue_filter: filter criteria for work pool queues",
            "            sort: sort criteria for the flows",
            "            limit: limit for the flow query",
            "            offset: offset for the flow query",
            "",
            "        Returns:",
            "            a list of Flow model representations of the flows",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "            \"work_queues\": (",
            "                work_queue_filter.dict(json_compatible=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        response = await self._client.post(\"/flows/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[Flow], response.json())",
            "",
            "    async def read_flow_by_name(",
            "        self,",
            "        flow_name: str,",
            "    ) -> Flow:",
            "        \"\"\"",
            "        Query the Prefect API for a flow by name.",
            "",
            "        Args:",
            "            flow_name: the name of a flow",
            "",
            "        Returns:",
            "            a fully hydrated Flow model",
            "        \"\"\"",
            "        response = await self._client.get(f\"/flows/name/{flow_name}\")",
            "        return Flow.parse_obj(response.json())",
            "",
            "    async def create_flow_run_from_deployment(",
            "        self,",
            "        deployment_id: UUID,",
            "        *,",
            "        parameters: Dict[str, Any] = None,",
            "        context: dict = None,",
            "        state: prefect.states.State = None,",
            "        name: str = None,",
            "        tags: Iterable[str] = None,",
            "        idempotency_key: str = None,",
            "        parent_task_run_id: UUID = None,",
            "        work_queue_name: str = None,",
            "        job_variables: Optional[Dict[str, Any]] = None,",
            "    ) -> FlowRun:",
            "        \"\"\"",
            "        Create a flow run for a deployment.",
            "",
            "        Args:",
            "            deployment_id: The deployment ID to create the flow run from",
            "            parameters: Parameter overrides for this flow run. Merged with the",
            "                deployment defaults",
            "            context: Optional run context data",
            "            state: The initial state for the run. If not provided, defaults to",
            "                `Scheduled` for now. Should always be a `Scheduled` type.",
            "            name: An optional name for the flow run. If not provided, the server will",
            "                generate a name.",
            "            tags: An optional iterable of tags to apply to the flow run; these tags",
            "                are merged with the deployment's tags.",
            "            idempotency_key: Optional idempotency key for creation of the flow run.",
            "                If the key matches the key of an existing flow run, the existing run will",
            "                be returned instead of creating a new one.",
            "            parent_task_run_id: if a subflow run is being created, the placeholder task",
            "                run identifier in the parent flow",
            "            work_queue_name: An optional work queue name to add this run to. If not provided,",
            "                will default to the deployment's set work queue.  If one is provided that does not",
            "                exist, a new work queue will be created within the deployment's work pool.",
            "            job_variables: Optional variables that will be supplied to the flow run job.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the Prefect API does not successfully create a run for any reason",
            "",
            "        Returns:",
            "            The flow run model",
            "        \"\"\"",
            "        if job_variables is not None and experiment_enabled(\"flow_run_infra_overrides\"):",
            "            if (",
            "                PREFECT_EXPERIMENTAL_WARN",
            "                and PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES",
            "            ):",
            "                warnings.warn(",
            "                    EXPERIMENTAL_WARNING.format(",
            "                        feature=\"Flow run job variables\",",
            "                        group=\"flow_run_infra_overrides\",",
            "                        help=\"To use this feature, update your workers to Prefect 2.16.4 or later. \",",
            "                    ),",
            "                    ExperimentalFeature,",
            "                    stacklevel=3,",
            "                )",
            "",
            "        parameters = parameters or {}",
            "        context = context or {}",
            "        state = state or prefect.states.Scheduled()",
            "        tags = tags or []",
            "",
            "        flow_run_create = DeploymentFlowRunCreate(",
            "            parameters=parameters,",
            "            context=context,",
            "            state=state.to_state_create(),",
            "            tags=tags,",
            "            name=name,",
            "            idempotency_key=idempotency_key,",
            "            parent_task_run_id=parent_task_run_id,",
            "            job_variables=job_variables,",
            "        )",
            "",
            "        # done separately to avoid including this field in payloads sent to older API versions",
            "        if work_queue_name:",
            "            flow_run_create.work_queue_name = work_queue_name",
            "",
            "        response = await self._client.post(",
            "            f\"/deployments/{deployment_id}/create_flow_run\",",
            "            json=flow_run_create.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "        return FlowRun.parse_obj(response.json())",
            "",
            "    async def create_flow_run(",
            "        self,",
            "        flow: \"FlowObject\",",
            "        name: str = None,",
            "        parameters: Dict[str, Any] = None,",
            "        context: dict = None,",
            "        tags: Iterable[str] = None,",
            "        parent_task_run_id: UUID = None,",
            "        state: \"prefect.states.State\" = None,",
            "    ) -> FlowRun:",
            "        \"\"\"",
            "        Create a flow run for a flow.",
            "",
            "        Args:",
            "            flow: The flow model to create the flow run for",
            "            name: An optional name for the flow run",
            "            parameters: Parameter overrides for this flow run.",
            "            context: Optional run context data",
            "            tags: a list of tags to apply to this flow run",
            "            parent_task_run_id: if a subflow run is being created, the placeholder task",
            "                run identifier in the parent flow",
            "            state: The initial state for the run. If not provided, defaults to",
            "                `Scheduled` for now. Should always be a `Scheduled` type.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the Prefect API does not successfully create a run for any reason",
            "",
            "        Returns:",
            "            The flow run model",
            "        \"\"\"",
            "        parameters = parameters or {}",
            "        context = context or {}",
            "",
            "        if state is None:",
            "            state = prefect.states.Pending()",
            "",
            "        # Retrieve the flow id",
            "        flow_id = await self.create_flow(flow)",
            "",
            "        flow_run_create = FlowRunCreate(",
            "            flow_id=flow_id,",
            "            flow_version=flow.version,",
            "            name=name,",
            "            parameters=parameters,",
            "            context=context,",
            "            tags=list(tags or []),",
            "            parent_task_run_id=parent_task_run_id,",
            "            state=state.to_state_create(),",
            "            empirical_policy=FlowRunPolicy(",
            "                retries=flow.retries,",
            "                retry_delay=flow.retry_delay_seconds,",
            "            ),",
            "        )",
            "",
            "        flow_run_create_json = flow_run_create.dict(json_compatible=True)",
            "        response = await self._client.post(\"/flow_runs/\", json=flow_run_create_json)",
            "        flow_run = FlowRun.parse_obj(response.json())",
            "",
            "        # Restore the parameters to the local objects to retain expectations about",
            "        # Python objects",
            "        flow_run.parameters = parameters",
            "",
            "        return flow_run",
            "",
            "    async def update_flow_run(",
            "        self,",
            "        flow_run_id: UUID,",
            "        flow_version: Optional[str] = None,",
            "        parameters: Optional[dict] = None,",
            "        name: Optional[str] = None,",
            "        tags: Optional[Iterable[str]] = None,",
            "        empirical_policy: Optional[FlowRunPolicy] = None,",
            "        infrastructure_pid: Optional[str] = None,",
            "        job_variables: Optional[dict] = None,",
            "    ) -> httpx.Response:",
            "        \"\"\"",
            "        Update a flow run's details.",
            "",
            "        Args:",
            "            flow_run_id: The identifier for the flow run to update.",
            "            flow_version: A new version string for the flow run.",
            "            parameters: A dictionary of parameter values for the flow run. This will not",
            "                be merged with any existing parameters.",
            "            name: A new name for the flow run.",
            "            empirical_policy: A new flow run orchestration policy. This will not be",
            "                merged with any existing policy.",
            "            tags: An iterable of new tags for the flow run. These will not be merged with",
            "                any existing tags.",
            "            infrastructure_pid: The id of flow run as returned by an",
            "                infrastructure block.",
            "",
            "        Returns:",
            "            an `httpx.Response` object from the PATCH request",
            "        \"\"\"",
            "        if job_variables is not None and experiment_enabled(\"flow_run_infra_overrides\"):",
            "            if (",
            "                PREFECT_EXPERIMENTAL_WARN",
            "                and PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES",
            "            ):",
            "                warnings.warn(",
            "                    EXPERIMENTAL_WARNING.format(",
            "                        feature=\"Flow run job variables\",",
            "                        group=\"flow_run_infra_overrides\",",
            "                        help=\"To use this feature, update your workers to Prefect 2.16.4 or later. \",",
            "                    ),",
            "                    ExperimentalFeature,",
            "                    stacklevel=3,",
            "                )",
            "",
            "        params = {}",
            "        if flow_version is not None:",
            "            params[\"flow_version\"] = flow_version",
            "        if parameters is not None:",
            "            params[\"parameters\"] = parameters",
            "        if name is not None:",
            "            params[\"name\"] = name",
            "        if tags is not None:",
            "            params[\"tags\"] = tags",
            "        if empirical_policy is not None:",
            "            params[\"empirical_policy\"] = empirical_policy",
            "        if infrastructure_pid:",
            "            params[\"infrastructure_pid\"] = infrastructure_pid",
            "        if job_variables is not None:",
            "            params[\"job_variables\"] = job_variables",
            "",
            "        flow_run_data = FlowRunUpdate(**params)",
            "",
            "        return await self._client.patch(",
            "            f\"/flow_runs/{flow_run_id}\",",
            "            json=flow_run_data.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "",
            "    async def delete_flow_run(",
            "        self,",
            "        flow_run_id: UUID,",
            "    ) -> None:",
            "        \"\"\"",
            "        Delete a flow run by UUID.",
            "",
            "        Args:",
            "            flow_run_id: The flow run UUID of interest.",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/flow_runs/{flow_run_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_concurrency_limit(",
            "        self,",
            "        tag: str,",
            "        concurrency_limit: int,",
            "    ) -> UUID:",
            "        \"\"\"",
            "        Create a tag concurrency limit in the Prefect API. These limits govern concurrently",
            "        running tasks.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "            concurrency_limit: the maximum number of concurrent task runs for a given tag",
            "",
            "        Raises:",
            "            httpx.RequestError: if the concurrency limit was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the concurrency limit in the backend",
            "        \"\"\"",
            "",
            "        concurrency_limit_create = ConcurrencyLimitCreate(",
            "            tag=tag,",
            "            concurrency_limit=concurrency_limit,",
            "        )",
            "        response = await self._client.post(",
            "            \"/concurrency_limits/\",",
            "            json=concurrency_limit_create.dict(json_compatible=True),",
            "        )",
            "",
            "        concurrency_limit_id = response.json().get(\"id\")",
            "",
            "        if not concurrency_limit_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(concurrency_limit_id)",
            "",
            "    async def read_concurrency_limit_by_tag(",
            "        self,",
            "        tag: str,",
            "    ):",
            "        \"\"\"",
            "        Read the concurrency limit set on a specific tag.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: if the concurrency limit was not created for any reason",
            "",
            "        Returns:",
            "            the concurrency limit set on a specific tag",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(",
            "                f\"/concurrency_limits/tag/{tag}\",",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        concurrency_limit_id = response.json().get(\"id\")",
            "",
            "        if not concurrency_limit_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        concurrency_limit = ConcurrencyLimit.parse_obj(response.json())",
            "        return concurrency_limit",
            "",
            "    async def read_concurrency_limits(",
            "        self,",
            "        limit: int,",
            "        offset: int,",
            "    ):",
            "        \"\"\"",
            "        Lists concurrency limits set on task run tags.",
            "",
            "        Args:",
            "            limit: the maximum number of concurrency limits returned",
            "            offset: the concurrency limit query offset",
            "",
            "        Returns:",
            "            a list of concurrency limits",
            "        \"\"\"",
            "",
            "        body = {",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        response = await self._client.post(\"/concurrency_limits/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[ConcurrencyLimit], response.json())",
            "",
            "    async def reset_concurrency_limit_by_tag(",
            "        self,",
            "        tag: str,",
            "        slot_override: Optional[List[Union[UUID, str]]] = None,",
            "    ):",
            "        \"\"\"",
            "        Resets the concurrency limit slots set on a specific tag.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "            slot_override: a list of task run IDs that are currently using a",
            "                concurrency slot, please check that any task run IDs included in",
            "                `slot_override` are currently running, otherwise those concurrency",
            "                slots will never be released.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        \"\"\"",
            "        if slot_override is not None:",
            "            slot_override = [str(slot) for slot in slot_override]",
            "",
            "        try:",
            "            await self._client.post(",
            "                f\"/concurrency_limits/tag/{tag}/reset\",",
            "                json=dict(slot_override=slot_override),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_concurrency_limit_by_tag(",
            "        self,",
            "        tag: str,",
            "    ):",
            "        \"\"\"",
            "        Delete the concurrency limit set on a specific tag.",
            "",
            "        Args:",
            "            tag: a tag the concurrency limit is applied to",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(",
            "                f\"/concurrency_limits/tag/{tag}\",",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_work_queue(",
            "        self,",
            "        name: str,",
            "        tags: Optional[List[str]] = None,",
            "        description: Optional[str] = None,",
            "        is_paused: Optional[bool] = None,",
            "        concurrency_limit: Optional[int] = None,",
            "        priority: Optional[int] = None,",
            "        work_pool_name: Optional[str] = None,",
            "    ) -> WorkQueue:",
            "        \"\"\"",
            "        Create a work queue.",
            "",
            "        Args:",
            "            name: a unique name for the work queue",
            "            tags: DEPRECATED: an optional list of tags to filter on; only work scheduled with these tags",
            "                will be included in the queue. This option will be removed on 2023-02-23.",
            "            description: An optional description for the work queue.",
            "            is_paused: Whether or not the work queue is paused.",
            "            concurrency_limit: An optional concurrency limit for the work queue.",
            "            priority: The queue's priority. Lower values are higher priority (1 is the highest).",
            "            work_pool_name: The name of the work pool to use for this queue.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectAlreadyExists: If request returns 409",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            The created work queue",
            "        \"\"\"",
            "        if tags:",
            "            warnings.warn(",
            "                (",
            "                    \"The use of tags for creating work queue filters is deprecated.\"",
            "                    \" This option will be removed on 2023-02-23.\"",
            "                ),",
            "                DeprecationWarning,",
            "            )",
            "            filter = QueueFilter(tags=tags)",
            "        else:",
            "            filter = None",
            "        create_model = WorkQueueCreate(name=name, filter=filter)",
            "        if description is not None:",
            "            create_model.description = description",
            "        if is_paused is not None:",
            "            create_model.is_paused = is_paused",
            "        if concurrency_limit is not None:",
            "            create_model.concurrency_limit = concurrency_limit",
            "        if priority is not None:",
            "            create_model.priority = priority",
            "",
            "        data = create_model.dict(json_compatible=True)",
            "        try:",
            "            if work_pool_name is not None:",
            "                response = await self._client.post(",
            "                    f\"/work_pools/{work_pool_name}/queues\", json=data",
            "                )",
            "            else:",
            "                response = await self._client.post(\"/work_queues/\", json=data)",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            elif e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return WorkQueue.parse_obj(response.json())",
            "",
            "    async def read_work_queue_by_name(",
            "        self,",
            "        name: str,",
            "        work_pool_name: Optional[str] = None,",
            "    ) -> WorkQueue:",
            "        \"\"\"",
            "        Read a work queue by name.",
            "",
            "        Args:",
            "            name (str): a unique name for the work queue",
            "            work_pool_name (str, optional): the name of the work pool",
            "                the queue belongs to.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: if no work queue is found",
            "            httpx.HTTPStatusError: other status errors",
            "",
            "        Returns:",
            "            WorkQueue: a work queue API object",
            "        \"\"\"",
            "        try:",
            "            if work_pool_name is not None:",
            "                response = await self._client.get(",
            "                    f\"/work_pools/{work_pool_name}/queues/{name}\"",
            "                )",
            "            else:",
            "                response = await self._client.get(f\"/work_queues/name/{name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return WorkQueue.parse_obj(response.json())",
            "",
            "    async def update_work_queue(self, id: UUID, **kwargs):",
            "        \"\"\"",
            "        Update properties of a work queue.",
            "",
            "        Args:",
            "            id: the ID of the work queue to update",
            "            **kwargs: the fields to update",
            "",
            "        Raises:",
            "            ValueError: if no kwargs are provided",
            "            prefect.exceptions.ObjectNotFound: if request returns 404",
            "            httpx.RequestError: if the request fails",
            "",
            "        \"\"\"",
            "        if not kwargs:",
            "            raise ValueError(\"No fields provided to update.\")",
            "",
            "        data = WorkQueueUpdate(**kwargs).dict(json_compatible=True, exclude_unset=True)",
            "        try:",
            "            await self._client.patch(f\"/work_queues/{id}\", json=data)",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def get_runs_in_work_queue(",
            "        self,",
            "        id: UUID,",
            "        limit: int = 10,",
            "        scheduled_before: datetime.datetime = None,",
            "    ) -> List[FlowRun]:",
            "        \"\"\"",
            "        Read flow runs off a work queue.",
            "",
            "        Args:",
            "            id: the id of the work queue to read from",
            "            limit: a limit on the number of runs to return",
            "            scheduled_before: a timestamp; only runs scheduled before this time will be returned.",
            "                Defaults to now.",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            List[FlowRun]: a list of FlowRun objects read from the queue",
            "        \"\"\"",
            "        if scheduled_before is None:",
            "            scheduled_before = pendulum.now(\"UTC\")",
            "",
            "        try:",
            "            response = await self._client.post(",
            "                f\"/work_queues/{id}/get_runs\",",
            "                json={",
            "                    \"limit\": limit,",
            "                    \"scheduled_before\": scheduled_before.isoformat(),",
            "                },",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return pydantic.parse_obj_as(List[FlowRun], response.json())",
            "",
            "    async def read_work_queue(",
            "        self,",
            "        id: UUID,",
            "    ) -> WorkQueue:",
            "        \"\"\"",
            "        Read a work queue.",
            "",
            "        Args:",
            "            id: the id of the work queue to load",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            WorkQueue: an instantiated WorkQueue object",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/work_queues/{id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return WorkQueue.parse_obj(response.json())",
            "",
            "    async def read_work_queue_status(",
            "        self,",
            "        id: UUID,",
            "    ) -> WorkQueueStatusDetail:",
            "        \"\"\"",
            "        Read a work queue status.",
            "",
            "        Args:",
            "            id: the id of the work queue to load",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            WorkQueueStatus: an instantiated WorkQueueStatus object",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/work_queues/{id}/status\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return WorkQueueStatusDetail.parse_obj(response.json())",
            "",
            "    async def match_work_queues(",
            "        self,",
            "        prefixes: List[str],",
            "        work_pool_name: Optional[str] = None,",
            "    ) -> List[WorkQueue]:",
            "        \"\"\"",
            "        Query the Prefect API for work queues with names with a specific prefix.",
            "",
            "        Args:",
            "            prefixes: a list of strings used to match work queue name prefixes",
            "            work_pool_name: an optional work pool name to scope the query to",
            "",
            "        Returns:",
            "            a list of WorkQueue model representations",
            "                of the work queues",
            "        \"\"\"",
            "        page_length = 100",
            "        current_page = 0",
            "        work_queues = []",
            "",
            "        while True:",
            "            new_queues = await self.read_work_queues(",
            "                work_pool_name=work_pool_name,",
            "                offset=current_page * page_length,",
            "                limit=page_length,",
            "                work_queue_filter=WorkQueueFilter(",
            "                    name=WorkQueueFilterName(startswith_=prefixes)",
            "                ),",
            "            )",
            "            if not new_queues:",
            "                break",
            "            work_queues += new_queues",
            "            current_page += 1",
            "",
            "        return work_queues",
            "",
            "    async def delete_work_queue_by_id(",
            "        self,",
            "        id: UUID,",
            "    ):",
            "        \"\"\"",
            "        Delete a work queue by its ID.",
            "",
            "        Args:",
            "            id: the id of the work queue to delete",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(",
            "                f\"/work_queues/{id}\",",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_block_type(self, block_type: BlockTypeCreate) -> BlockType:",
            "        \"\"\"",
            "        Create a block type in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_types/\",",
            "                json=block_type.dict(",
            "                    json_compatible=True, exclude_unset=True, exclude={\"id\"}",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockType.parse_obj(response.json())",
            "",
            "    async def create_block_schema(self, block_schema: BlockSchemaCreate) -> BlockSchema:",
            "        \"\"\"",
            "        Create a block schema in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_schemas/\",",
            "                json=block_schema.dict(",
            "                    json_compatible=True,",
            "                    exclude_unset=True,",
            "                    exclude={\"id\", \"block_type\", \"checksum\"},",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockSchema.parse_obj(response.json())",
            "",
            "    async def create_block_document(",
            "        self,",
            "        block_document: Union[BlockDocument, BlockDocumentCreate],",
            "        include_secrets: bool = True,",
            "    ) -> BlockDocument:",
            "        \"\"\"",
            "        Create a block document in the Prefect API. This data is used to configure a",
            "        corresponding Block.",
            "",
            "        Args:",
            "            include_secrets (bool): whether to include secret values",
            "                on the stored Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. Note Blocks may not work as expected if",
            "                this is set to `False`.",
            "        \"\"\"",
            "        if isinstance(block_document, BlockDocument):",
            "            block_document = BlockDocumentCreate.parse_obj(",
            "                block_document.dict(",
            "                    json_compatible=True,",
            "                    include_secrets=include_secrets,",
            "                    exclude_unset=True,",
            "                    exclude={\"id\", \"block_schema\", \"block_type\"},",
            "                ),",
            "            )",
            "",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_documents/\",",
            "                json=block_document.dict(",
            "                    json_compatible=True,",
            "                    include_secrets=include_secrets,",
            "                    exclude_unset=True,",
            "                    exclude={\"id\", \"block_schema\", \"block_type\"},",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockDocument.parse_obj(response.json())",
            "",
            "    async def update_block_document(",
            "        self,",
            "        block_document_id: UUID,",
            "        block_document: BlockDocumentUpdate,",
            "    ):",
            "        \"\"\"",
            "        Update a block document in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            await self._client.patch(",
            "                f\"/block_documents/{block_document_id}\",",
            "                json=block_document.dict(",
            "                    json_compatible=True,",
            "                    exclude_unset=True,",
            "                    include={\"data\", \"merge_existing_data\", \"block_schema_id\"},",
            "                    include_secrets=True,",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_block_document(self, block_document_id: UUID):",
            "        \"\"\"",
            "        Delete a block document.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/block_documents/{block_document_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_block_type_by_slug(self, slug: str) -> BlockType:",
            "        \"\"\"",
            "        Read a block type by its slug.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/block_types/slug/{slug}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockType.parse_obj(response.json())",
            "",
            "    async def read_block_schema_by_checksum(",
            "        self, checksum: str, version: Optional[str] = None",
            "    ) -> BlockSchema:",
            "        \"\"\"",
            "        Look up a block schema checksum",
            "        \"\"\"",
            "        try:",
            "            url = f\"/block_schemas/checksum/{checksum}\"",
            "            if version is not None:",
            "                url = f\"{url}?version={version}\"",
            "            response = await self._client.get(url)",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockSchema.parse_obj(response.json())",
            "",
            "    async def update_block_type(self, block_type_id: UUID, block_type: BlockTypeUpdate):",
            "        \"\"\"",
            "        Update a block document in the Prefect API.",
            "        \"\"\"",
            "        try:",
            "            await self._client.patch(",
            "                f\"/block_types/{block_type_id}\",",
            "                json=block_type.dict(",
            "                    json_compatible=True,",
            "                    exclude_unset=True,",
            "                    include=BlockTypeUpdate.updatable_fields(),",
            "                    include_secrets=True,",
            "                ),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_block_type(self, block_type_id: UUID):",
            "        \"\"\"",
            "        Delete a block type.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/block_types/{block_type_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            elif (",
            "                e.response.status_code == status.HTTP_403_FORBIDDEN",
            "                and e.response.json()[\"detail\"]",
            "                == \"protected block types cannot be deleted.\"",
            "            ):",
            "                raise prefect.exceptions.ProtectedBlockError(",
            "                    \"Protected block types cannot be deleted.\"",
            "                ) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_block_types(self) -> List[BlockType]:",
            "        \"\"\"",
            "        Read all block types",
            "        Raises:",
            "            httpx.RequestError: if the block types were not found",
            "",
            "        Returns:",
            "            List of BlockTypes.",
            "        \"\"\"",
            "        response = await self._client.post(\"/block_types/filter\", json={})",
            "        return pydantic.parse_obj_as(List[BlockType], response.json())",
            "",
            "    async def read_block_schemas(self) -> List[BlockSchema]:",
            "        \"\"\"",
            "        Read all block schemas",
            "        Raises:",
            "            httpx.RequestError: if a valid block schema was not found",
            "",
            "        Returns:",
            "            A BlockSchema.",
            "        \"\"\"",
            "        response = await self._client.post(\"/block_schemas/filter\", json={})",
            "        return pydantic.parse_obj_as(List[BlockSchema], response.json())",
            "",
            "    async def get_most_recent_block_schema_for_block_type(",
            "        self,",
            "        block_type_id: UUID,",
            "    ) -> Optional[BlockSchema]:",
            "        \"\"\"",
            "        Fetches the most recent block schema for a specified block type ID.",
            "",
            "        Args:",
            "            block_type_id: The ID of the block type.",
            "",
            "        Raises:",
            "            httpx.RequestError: If the request fails for any reason.",
            "",
            "        Returns:",
            "            The most recent block schema or None.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/block_schemas/filter\",",
            "                json={",
            "                    \"block_schemas\": {\"block_type_id\": {\"any_\": [str(block_type_id)]}},",
            "                    \"limit\": 1,",
            "                },",
            "            )",
            "        except httpx.HTTPStatusError:",
            "            raise",
            "        return BlockSchema.parse_obj(response.json()[0]) if response.json() else None",
            "",
            "    async def read_block_document(",
            "        self,",
            "        block_document_id: UUID,",
            "        include_secrets: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Read the block document with the specified ID.",
            "",
            "        Args:",
            "            block_document_id: the block document id",
            "            include_secrets (bool): whether to include secret values",
            "                on the Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. These fields are automatically obfuscated",
            "                by Pydantic, but users can additionally choose not to receive",
            "                their values from the API. Note that any business logic on the",
            "                Block may not work if this is `False`.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the block document was not found for any reason",
            "",
            "        Returns:",
            "            A block document or None.",
            "        \"\"\"",
            "        assert (",
            "            block_document_id is not None",
            "        ), \"Unexpected ID on block document. Was it persisted?\"",
            "        try:",
            "            response = await self._client.get(",
            "                f\"/block_documents/{block_document_id}\",",
            "                params=dict(include_secrets=include_secrets),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockDocument.parse_obj(response.json())",
            "",
            "    async def read_block_document_by_name(",
            "        self,",
            "        name: str,",
            "        block_type_slug: str,",
            "        include_secrets: bool = True,",
            "    ) -> BlockDocument:",
            "        \"\"\"",
            "        Read the block document with the specified name that corresponds to a",
            "        specific block type name.",
            "",
            "        Args:",
            "            name: The block document name.",
            "            block_type_slug: The block type slug.",
            "            include_secrets (bool): whether to include secret values",
            "                on the Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. These fields are automatically obfuscated",
            "                by Pydantic, but users can additionally choose not to receive",
            "                their values from the API. Note that any business logic on the",
            "                Block may not work if this is `False`.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the block document was not found for any reason",
            "",
            "        Returns:",
            "            A block document or None.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(",
            "                f\"/block_types/slug/{block_type_slug}/block_documents/name/{name}\",",
            "                params=dict(include_secrets=include_secrets),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return BlockDocument.parse_obj(response.json())",
            "",
            "    async def read_block_documents(",
            "        self,",
            "        block_schema_type: Optional[str] = None,",
            "        offset: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        include_secrets: bool = True,",
            "    ):",
            "        \"\"\"",
            "        Read block documents",
            "",
            "        Args:",
            "            block_schema_type: an optional block schema type",
            "            offset: an offset",
            "            limit: the number of blocks to return",
            "            include_secrets (bool): whether to include secret values",
            "                on the Block, corresponding to Pydantic's `SecretStr` and",
            "                `SecretBytes` fields. These fields are automatically obfuscated",
            "                by Pydantic, but users can additionally choose not to receive",
            "                their values from the API. Note that any business logic on the",
            "                Block may not work if this is `False`.",
            "",
            "        Returns:",
            "            A list of block documents",
            "        \"\"\"",
            "        response = await self._client.post(",
            "            \"/block_documents/filter\",",
            "            json=dict(",
            "                block_schema_type=block_schema_type,",
            "                offset=offset,",
            "                limit=limit,",
            "                include_secrets=include_secrets,",
            "            ),",
            "        )",
            "        return pydantic.parse_obj_as(List[BlockDocument], response.json())",
            "",
            "    async def read_block_documents_by_type(",
            "        self,",
            "        block_type_slug: str,",
            "        offset: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "        include_secrets: bool = True,",
            "    ) -> List[BlockDocument]:",
            "        \"\"\"Retrieve block documents by block type slug.",
            "",
            "        Args:",
            "            block_type_slug: The block type slug.",
            "            offset: an offset",
            "            limit: the number of blocks to return",
            "            include_secrets: whether to include secret values",
            "",
            "        Returns:",
            "            A list of block documents",
            "        \"\"\"",
            "        response = await self._client.get(",
            "            f\"/block_types/slug/{block_type_slug}/block_documents\",",
            "            params=dict(",
            "                offset=offset,",
            "                limit=limit,",
            "                include_secrets=include_secrets,",
            "            ),",
            "        )",
            "",
            "        return pydantic.parse_obj_as(List[BlockDocument], response.json())",
            "",
            "    async def create_deployment(",
            "        self,",
            "        flow_id: UUID,",
            "        name: str,",
            "        version: str = None,",
            "        schedule: SCHEDULE_TYPES = None,",
            "        schedules: List[DeploymentScheduleCreate] = None,",
            "        parameters: Dict[str, Any] = None,",
            "        description: str = None,",
            "        work_queue_name: str = None,",
            "        work_pool_name: str = None,",
            "        tags: List[str] = None,",
            "        storage_document_id: UUID = None,",
            "        manifest_path: str = None,",
            "        path: str = None,",
            "        entrypoint: str = None,",
            "        infrastructure_document_id: UUID = None,",
            "        infra_overrides: Dict[str, Any] = None,",
            "        parameter_openapi_schema: dict = None,",
            "        is_schedule_active: Optional[bool] = None,",
            "        paused: Optional[bool] = None,",
            "        pull_steps: Optional[List[dict]] = None,",
            "        enforce_parameter_schema: Optional[bool] = None,",
            "    ) -> UUID:",
            "        \"\"\"",
            "        Create a deployment.",
            "",
            "        Args:",
            "            flow_id: the flow ID to create a deployment for",
            "            name: the name of the deployment",
            "            version: an optional version string for the deployment",
            "            schedule: an optional schedule to apply to the deployment",
            "            tags: an optional list of tags to apply to the deployment",
            "            storage_document_id: an reference to the storage block document",
            "                used for the deployed flow",
            "            infrastructure_document_id: an reference to the infrastructure block document",
            "                to use for this deployment",
            "",
            "        Raises:",
            "            httpx.RequestError: if the deployment was not created for any reason",
            "",
            "        Returns:",
            "            the ID of the deployment in the backend",
            "        \"\"\"",
            "",
            "        deployment_create = DeploymentCreate(",
            "            flow_id=flow_id,",
            "            name=name,",
            "            version=version,",
            "            parameters=dict(parameters or {}),",
            "            tags=list(tags or []),",
            "            work_queue_name=work_queue_name,",
            "            description=description,",
            "            storage_document_id=storage_document_id,",
            "            path=path,",
            "            entrypoint=entrypoint,",
            "            manifest_path=manifest_path,  # for backwards compat",
            "            infrastructure_document_id=infrastructure_document_id,",
            "            infra_overrides=infra_overrides or {},",
            "            parameter_openapi_schema=parameter_openapi_schema,",
            "            is_schedule_active=is_schedule_active,",
            "            paused=paused,",
            "            schedule=schedule,",
            "            schedules=schedules or [],",
            "            pull_steps=pull_steps,",
            "            enforce_parameter_schema=enforce_parameter_schema,",
            "        )",
            "",
            "        if work_pool_name is not None:",
            "            deployment_create.work_pool_name = work_pool_name",
            "",
            "        # Exclude newer fields that are not set to avoid compatibility issues",
            "        exclude = {",
            "            field",
            "            for field in [\"work_pool_name\", \"work_queue_name\"]",
            "            if field not in deployment_create.__fields_set__",
            "        }",
            "",
            "        if deployment_create.is_schedule_active is None:",
            "            exclude.add(\"is_schedule_active\")",
            "",
            "        if deployment_create.paused is None:",
            "            exclude.add(\"paused\")",
            "",
            "        if deployment_create.pull_steps is None:",
            "            exclude.add(\"pull_steps\")",
            "",
            "        if deployment_create.enforce_parameter_schema is None:",
            "            exclude.add(\"enforce_parameter_schema\")",
            "",
            "        json = deployment_create.dict(json_compatible=True, exclude=exclude)",
            "        response = await self._client.post(",
            "            \"/deployments/\",",
            "            json=json,",
            "        )",
            "        deployment_id = response.json().get(\"id\")",
            "        if not deployment_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(deployment_id)",
            "",
            "    async def update_schedule(self, deployment_id: UUID, active: bool = True):",
            "        path = \"set_schedule_active\" if active else \"set_schedule_inactive\"",
            "        await self._client.post(",
            "            f\"/deployments/{deployment_id}/{path}\",",
            "        )",
            "",
            "    async def set_deployment_paused_state(self, deployment_id: UUID, paused: bool):",
            "        await self._client.patch(",
            "            f\"/deployments/{deployment_id}\", json={\"paused\": paused}",
            "        )",
            "",
            "    async def update_deployment(",
            "        self,",
            "        deployment: Deployment,",
            "        schedule: SCHEDULE_TYPES = None,",
            "        is_schedule_active: bool = None,",
            "    ):",
            "        deployment_update = DeploymentUpdate(",
            "            version=deployment.version,",
            "            schedule=schedule if schedule is not None else deployment.schedule,",
            "            is_schedule_active=(",
            "                is_schedule_active",
            "                if is_schedule_active is not None",
            "                else deployment.is_schedule_active",
            "            ),",
            "            description=deployment.description,",
            "            work_queue_name=deployment.work_queue_name,",
            "            tags=deployment.tags,",
            "            manifest_path=deployment.manifest_path,",
            "            path=deployment.path,",
            "            entrypoint=deployment.entrypoint,",
            "            parameters=deployment.parameters,",
            "            storage_document_id=deployment.storage_document_id,",
            "            infrastructure_document_id=deployment.infrastructure_document_id,",
            "            infra_overrides=deployment.infra_overrides,",
            "            enforce_parameter_schema=deployment.enforce_parameter_schema,",
            "        )",
            "",
            "        if getattr(deployment, \"work_pool_name\", None) is not None:",
            "            deployment_update.work_pool_name = deployment.work_pool_name",
            "",
            "        exclude = set()",
            "        if deployment.enforce_parameter_schema is None:",
            "            exclude.add(\"enforce_parameter_schema\")",
            "",
            "        await self._client.patch(",
            "            f\"/deployments/{deployment.id}\",",
            "            json=deployment_update.dict(json_compatible=True, exclude=exclude),",
            "        )",
            "",
            "    async def _create_deployment_from_schema(self, schema: DeploymentCreate) -> UUID:",
            "        \"\"\"",
            "        Create a deployment from a prepared `DeploymentCreate` schema.",
            "        \"\"\"",
            "        # TODO: We are likely to remove this method once we have considered the",
            "        #       packaging interface for deployments further.",
            "        response = await self._client.post(",
            "            \"/deployments/\", json=schema.dict(json_compatible=True)",
            "        )",
            "        deployment_id = response.json().get(\"id\")",
            "        if not deployment_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(deployment_id)",
            "",
            "    async def read_deployment(",
            "        self,",
            "        deployment_id: UUID,",
            "    ) -> DeploymentResponse:",
            "        \"\"\"",
            "        Query the Prefect API for a deployment by id.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID of interest",
            "",
            "        Returns:",
            "            a [Deployment model][prefect.client.schemas.objects.Deployment] representation of the deployment",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/deployments/{deployment_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return DeploymentResponse.parse_obj(response.json())",
            "",
            "    async def read_deployment_by_name(",
            "        self,",
            "        name: str,",
            "    ) -> DeploymentResponse:",
            "        \"\"\"",
            "        Query the Prefect API for a deployment by name.",
            "",
            "        Args:",
            "            name: A deployed flow's name: <FLOW_NAME>/<DEPLOYMENT_NAME>",
            "",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If request fails",
            "",
            "        Returns:",
            "            a Deployment model representation of the deployment",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/deployments/name/{name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return DeploymentResponse.parse_obj(response.json())",
            "",
            "    async def read_deployments(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        work_pool_filter: WorkPoolFilter = None,",
            "        work_queue_filter: WorkQueueFilter = None,",
            "        limit: int = None,",
            "        sort: DeploymentSort = None,",
            "        offset: int = 0,",
            "    ) -> List[DeploymentResponse]:",
            "        \"\"\"",
            "        Query the Prefect API for deployments. Only deployments matching all",
            "        the provided criteria will be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            work_pool_filter: filter criteria for work pools",
            "            work_queue_filter: filter criteria for work pool queues",
            "            limit: a limit for the deployment query",
            "            offset: an offset for the deployment query",
            "",
            "        Returns:",
            "            a list of Deployment model representations",
            "                of the deployments",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "            \"work_pool_queues\": (",
            "                work_queue_filter.dict(json_compatible=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "            \"sort\": sort,",
            "        }",
            "",
            "        response = await self._client.post(\"/deployments/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[DeploymentResponse], response.json())",
            "",
            "    async def delete_deployment(",
            "        self,",
            "        deployment_id: UUID,",
            "    ):",
            "        \"\"\"",
            "        Delete deployment by id.",
            "",
            "        Args:",
            "            deployment_id: The deployment id of interest.",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/deployments/{deployment_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def create_deployment_schedules(",
            "        self,",
            "        deployment_id: UUID,",
            "        schedules: List[Tuple[SCHEDULE_TYPES, bool]],",
            "    ) -> List[DeploymentSchedule]:",
            "        \"\"\"",
            "        Create deployment schedules.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "            schedules: a list of tuples containing the schedule to create",
            "                       and whether or not it should be active.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the schedules were not created for any reason",
            "",
            "        Returns:",
            "            the list of schedules created in the backend",
            "        \"\"\"",
            "        deployment_schedule_create = [",
            "            DeploymentScheduleCreate(schedule=schedule[0], active=schedule[1])",
            "            for schedule in schedules",
            "        ]",
            "",
            "        json = [",
            "            deployment_schedule_create.dict(json_compatible=True)",
            "            for deployment_schedule_create in deployment_schedule_create",
            "        ]",
            "        response = await self._client.post(",
            "            f\"/deployments/{deployment_id}/schedules\", json=json",
            "        )",
            "        return pydantic.parse_obj_as(List[DeploymentSchedule], response.json())",
            "",
            "    async def read_deployment_schedules(",
            "        self,",
            "        deployment_id: UUID,",
            "    ) -> List[DeploymentSchedule]:",
            "        \"\"\"",
            "        Query the Prefect API for a deployment's schedules.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "",
            "        Returns:",
            "            a list of DeploymentSchedule model representations of the deployment schedules",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/deployments/{deployment_id}/schedules\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return pydantic.parse_obj_as(List[DeploymentSchedule], response.json())",
            "",
            "    async def update_deployment_schedule(",
            "        self,",
            "        deployment_id: UUID,",
            "        schedule_id: UUID,",
            "        active: Optional[bool] = None,",
            "        schedule: Optional[SCHEDULE_TYPES] = None,",
            "    ):",
            "        \"\"\"",
            "        Update a deployment schedule by ID.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "            schedule_id: the deployment schedule ID of interest",
            "            active: whether or not the schedule should be active",
            "            schedule: the cron, rrule, or interval schedule this deployment schedule should use",
            "        \"\"\"",
            "        kwargs = {}",
            "        if active is not None:",
            "            kwargs[\"active\"] = active",
            "        elif schedule is not None:",
            "            kwargs[\"schedule\"] = schedule",
            "",
            "        deployment_schedule_update = DeploymentScheduleUpdate(**kwargs)",
            "        json = deployment_schedule_update.dict(json_compatible=True, exclude_unset=True)",
            "",
            "        try:",
            "            await self._client.patch(",
            "                f\"/deployments/{deployment_id}/schedules/{schedule_id}\", json=json",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_deployment_schedule(",
            "        self,",
            "        deployment_id: UUID,",
            "        schedule_id: UUID,",
            "    ) -> None:",
            "        \"\"\"",
            "        Delete a deployment schedule.",
            "",
            "        Args:",
            "            deployment_id: the deployment ID",
            "            schedule_id: the ID of the deployment schedule to delete.",
            "",
            "        Raises:",
            "            httpx.RequestError: if the schedules were not deleted for any reason",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(",
            "                f\"/deployments/{deployment_id}/schedules/{schedule_id}\"",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_flow_run(self, flow_run_id: UUID) -> FlowRun:",
            "        \"\"\"",
            "        Query the Prefect API for a flow run by id.",
            "",
            "        Args:",
            "            flow_run_id: the flow run ID of interest",
            "",
            "        Returns:",
            "            a Flow Run model representation of the flow run",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/flow_runs/{flow_run_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "        return FlowRun.parse_obj(response.json())",
            "",
            "    async def resume_flow_run(",
            "        self, flow_run_id: UUID, run_input: Optional[Dict] = None",
            "    ) -> OrchestrationResult:",
            "        \"\"\"",
            "        Resumes a paused flow run.",
            "",
            "        Args:",
            "            flow_run_id: the flow run ID of interest",
            "            run_input: the input to resume the flow run with",
            "",
            "        Returns:",
            "            an OrchestrationResult model representation of state orchestration output",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                f\"/flow_runs/{flow_run_id}/resume\", json={\"run_input\": run_input}",
            "            )",
            "        except httpx.HTTPStatusError:",
            "            raise",
            "",
            "        return OrchestrationResult.parse_obj(response.json())",
            "",
            "    async def read_flow_runs(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        work_pool_filter: WorkPoolFilter = None,",
            "        work_queue_filter: WorkQueueFilter = None,",
            "        sort: FlowRunSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[FlowRun]:",
            "        \"\"\"",
            "        Query the Prefect API for flow runs. Only flow runs matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            work_pool_filter: filter criteria for work pools",
            "            work_queue_filter: filter criteria for work pool queues",
            "            sort: sort criteria for the flow runs",
            "            limit: limit for the flow run query",
            "            offset: offset for the flow run query",
            "",
            "        Returns:",
            "            a list of Flow Run model representations",
            "                of the flow runs",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "            \"work_pool_queues\": (",
            "                work_queue_filter.dict(json_compatible=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        response = await self._client.post(\"/flow_runs/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[FlowRun], response.json())",
            "",
            "    async def set_flow_run_state(",
            "        self,",
            "        flow_run_id: UUID,",
            "        state: \"prefect.states.State\",",
            "        force: bool = False,",
            "    ) -> OrchestrationResult:",
            "        \"\"\"",
            "        Set the state of a flow run.",
            "",
            "        Args:",
            "            flow_run_id: the id of the flow run",
            "            state: the state to set",
            "            force: if True, disregard orchestration logic when setting the state,",
            "                forcing the Prefect API to accept the state",
            "",
            "        Returns:",
            "            an OrchestrationResult model representation of state orchestration output",
            "        \"\"\"",
            "        state_create = state.to_state_create()",
            "        state_create.state_details.flow_run_id = flow_run_id",
            "        state_create.state_details.transition_id = uuid4()",
            "        try:",
            "            response = await self._client.post(",
            "                f\"/flow_runs/{flow_run_id}/set_state\",",
            "                json=dict(state=state_create.dict(json_compatible=True), force=force),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return OrchestrationResult.parse_obj(response.json())",
            "",
            "    async def read_flow_run_states(",
            "        self, flow_run_id: UUID",
            "    ) -> List[prefect.states.State]:",
            "        \"\"\"",
            "        Query for the states of a flow run",
            "",
            "        Args:",
            "            flow_run_id: the id of the flow run",
            "",
            "        Returns:",
            "            a list of State model representations",
            "                of the flow run states",
            "        \"\"\"",
            "        response = await self._client.get(",
            "            \"/flow_run_states/\", params=dict(flow_run_id=str(flow_run_id))",
            "        )",
            "        return pydantic.parse_obj_as(List[prefect.states.State], response.json())",
            "",
            "    async def set_task_run_name(self, task_run_id: UUID, name: str):",
            "        task_run_data = TaskRunUpdate(name=name)",
            "        return await self._client.patch(",
            "            f\"/task_runs/{task_run_id}\",",
            "            json=task_run_data.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "",
            "    async def create_task_run(",
            "        self,",
            "        task: \"TaskObject\",",
            "        flow_run_id: Optional[UUID],",
            "        dynamic_key: str,",
            "        name: str = None,",
            "        extra_tags: Iterable[str] = None,",
            "        state: prefect.states.State = None,",
            "        task_inputs: Dict[",
            "            str,",
            "            List[",
            "                Union[",
            "                    TaskRunResult,",
            "                    Parameter,",
            "                    Constant,",
            "                ]",
            "            ],",
            "        ] = None,",
            "    ) -> TaskRun:",
            "        \"\"\"",
            "        Create a task run",
            "",
            "        Args:",
            "            task: The Task to run",
            "            flow_run_id: The flow run id with which to associate the task run",
            "            dynamic_key: A key unique to this particular run of a Task within the flow",
            "            name: An optional name for the task run",
            "            extra_tags: an optional list of extra tags to apply to the task run in",
            "                addition to `task.tags`",
            "            state: The initial state for the run. If not provided, defaults to",
            "                `Pending` for now. Should always be a `Scheduled` type.",
            "            task_inputs: the set of inputs passed to the task",
            "",
            "        Returns:",
            "            The created task run.",
            "        \"\"\"",
            "        tags = set(task.tags).union(extra_tags or [])",
            "",
            "        if state is None:",
            "            state = prefect.states.Pending()",
            "",
            "        task_run_data = TaskRunCreate(",
            "            name=name,",
            "            flow_run_id=flow_run_id,",
            "            task_key=task.task_key,",
            "            dynamic_key=dynamic_key,",
            "            tags=list(tags),",
            "            task_version=task.version,",
            "            empirical_policy=TaskRunPolicy(",
            "                retries=task.retries,",
            "                retry_delay=task.retry_delay_seconds,",
            "                retry_jitter_factor=task.retry_jitter_factor,",
            "            ),",
            "            state=state.to_state_create(),",
            "            task_inputs=task_inputs or {},",
            "        )",
            "",
            "        response = await self._client.post(",
            "            \"/task_runs/\", json=task_run_data.dict(json_compatible=True)",
            "        )",
            "        return TaskRun.parse_obj(response.json())",
            "",
            "    async def read_task_run(self, task_run_id: UUID) -> TaskRun:",
            "        \"\"\"",
            "        Query the Prefect API for a task run by id.",
            "",
            "        Args:",
            "            task_run_id: the task run ID of interest",
            "",
            "        Returns:",
            "            a Task Run model representation of the task run",
            "        \"\"\"",
            "        response = await self._client.get(f\"/task_runs/{task_run_id}\")",
            "        return TaskRun.parse_obj(response.json())",
            "",
            "    async def read_task_runs(",
            "        self,",
            "        *,",
            "        flow_filter: FlowFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        deployment_filter: DeploymentFilter = None,",
            "        sort: TaskRunSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[TaskRun]:",
            "        \"\"\"",
            "        Query the Prefect API for task runs. Only task runs matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_filter: filter criteria for flows",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            deployment_filter: filter criteria for deployments",
            "            sort: sort criteria for the task runs",
            "            limit: a limit for the task run query",
            "            offset: an offset for the task run query",
            "",
            "        Returns:",
            "            a list of Task Run model representations",
            "                of the task runs",
            "        \"\"\"",
            "        body = {",
            "            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if flow_run_filter",
            "                else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"deployments\": (",
            "                deployment_filter.dict(json_compatible=True)",
            "                if deployment_filter",
            "                else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(\"/task_runs/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[TaskRun], response.json())",
            "",
            "    async def delete_task_run(self, task_run_id: UUID) -> None:",
            "        \"\"\"",
            "        Delete a task run by id.",
            "",
            "        Args:",
            "            task_run_id: the task run ID of interest",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/task_runs/{task_run_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def set_task_run_state(",
            "        self,",
            "        task_run_id: UUID,",
            "        state: prefect.states.State,",
            "        force: bool = False,",
            "    ) -> OrchestrationResult:",
            "        \"\"\"",
            "        Set the state of a task run.",
            "",
            "        Args:",
            "            task_run_id: the id of the task run",
            "            state: the state to set",
            "            force: if True, disregard orchestration logic when setting the state,",
            "                forcing the Prefect API to accept the state",
            "",
            "        Returns:",
            "            an OrchestrationResult model representation of state orchestration output",
            "        \"\"\"",
            "        state_create = state.to_state_create()",
            "        state_create.state_details.task_run_id = task_run_id",
            "        response = await self._client.post(",
            "            f\"/task_runs/{task_run_id}/set_state\",",
            "            json=dict(state=state_create.dict(json_compatible=True), force=force),",
            "        )",
            "        return OrchestrationResult.parse_obj(response.json())",
            "",
            "    async def read_task_run_states(",
            "        self, task_run_id: UUID",
            "    ) -> List[prefect.states.State]:",
            "        \"\"\"",
            "        Query for the states of a task run",
            "",
            "        Args:",
            "            task_run_id: the id of the task run",
            "",
            "        Returns:",
            "            a list of State model representations of the task run states",
            "        \"\"\"",
            "        response = await self._client.get(",
            "            \"/task_run_states/\", params=dict(task_run_id=str(task_run_id))",
            "        )",
            "        return pydantic.parse_obj_as(List[prefect.states.State], response.json())",
            "",
            "    async def create_logs(self, logs: Iterable[Union[LogCreate, dict]]) -> None:",
            "        \"\"\"",
            "        Create logs for a flow or task run",
            "",
            "        Args:",
            "            logs: An iterable of `LogCreate` objects or already json-compatible dicts",
            "        \"\"\"",
            "        serialized_logs = [",
            "            log.dict(json_compatible=True) if isinstance(log, LogCreate) else log",
            "            for log in logs",
            "        ]",
            "        await self._client.post(\"/logs/\", json=serialized_logs)",
            "",
            "    async def create_flow_run_notification_policy(",
            "        self,",
            "        block_document_id: UUID,",
            "        is_active: bool = True,",
            "        tags: List[str] = None,",
            "        state_names: List[str] = None,",
            "        message_template: Optional[str] = None,",
            "    ) -> UUID:",
            "        \"\"\"",
            "        Create a notification policy for flow runs",
            "",
            "        Args:",
            "            block_document_id: The block document UUID",
            "            is_active: Whether the notification policy is active",
            "            tags: List of flow tags",
            "            state_names: List of state names",
            "            message_template: Notification message template",
            "        \"\"\"",
            "        if tags is None:",
            "            tags = []",
            "        if state_names is None:",
            "            state_names = []",
            "",
            "        policy = FlowRunNotificationPolicyCreate(",
            "            block_document_id=block_document_id,",
            "            is_active=is_active,",
            "            tags=tags,",
            "            state_names=state_names,",
            "            message_template=message_template,",
            "        )",
            "        response = await self._client.post(",
            "            \"/flow_run_notification_policies/\",",
            "            json=policy.dict(json_compatible=True),",
            "        )",
            "",
            "        policy_id = response.json().get(\"id\")",
            "        if not policy_id:",
            "            raise httpx.RequestError(f\"Malformed response: {response}\")",
            "",
            "        return UUID(policy_id)",
            "",
            "    async def delete_flow_run_notification_policy(",
            "        self,",
            "        id: UUID,",
            "    ) -> None:",
            "        \"\"\"",
            "        Delete a flow run notification policy by id.",
            "",
            "        Args:",
            "            id: UUID of the flow run notification policy to delete.",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/flow_run_notification_policies/{id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def update_flow_run_notification_policy(",
            "        self,",
            "        id: UUID,",
            "        block_document_id: Optional[UUID] = None,",
            "        is_active: Optional[bool] = None,",
            "        tags: Optional[List[str]] = None,",
            "        state_names: Optional[List[str]] = None,",
            "        message_template: Optional[str] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a notification policy for flow runs",
            "",
            "        Args:",
            "            id: UUID of the notification policy",
            "            block_document_id: The block document UUID",
            "            is_active: Whether the notification policy is active",
            "            tags: List of flow tags",
            "            state_names: List of state names",
            "            message_template: Notification message template",
            "        Raises:",
            "            prefect.exceptions.ObjectNotFound: If request returns 404",
            "            httpx.RequestError: If requests fails",
            "        \"\"\"",
            "        params = {}",
            "        if block_document_id is not None:",
            "            params[\"block_document_id\"] = block_document_id",
            "        if is_active is not None:",
            "            params[\"is_active\"] = is_active",
            "        if tags is not None:",
            "            params[\"tags\"] = tags",
            "        if state_names is not None:",
            "            params[\"state_names\"] = state_names",
            "        if message_template is not None:",
            "            params[\"message_template\"] = message_template",
            "",
            "        policy = FlowRunNotificationPolicyUpdate(**params)",
            "",
            "        try:",
            "            await self._client.patch(",
            "                f\"/flow_run_notification_policies/{id}\",",
            "                json=policy.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_flow_run_notification_policies(",
            "        self,",
            "        flow_run_notification_policy_filter: FlowRunNotificationPolicyFilter,",
            "        limit: Optional[int] = None,",
            "        offset: int = 0,",
            "    ) -> List[FlowRunNotificationPolicy]:",
            "        \"\"\"",
            "        Query the Prefect API for flow run notification policies. Only policies matching all criteria will",
            "        be returned.",
            "",
            "        Args:",
            "            flow_run_notification_policy_filter: filter criteria for notification policies",
            "            limit: a limit for the notification policies query",
            "            offset: an offset for the notification policies query",
            "",
            "        Returns:",
            "            a list of FlowRunNotificationPolicy model representations",
            "                of the notification policies",
            "        \"\"\"",
            "        body = {",
            "            \"flow_run_notification_policy_filter\": (",
            "                flow_run_notification_policy_filter.dict(json_compatible=True)",
            "                if flow_run_notification_policy_filter",
            "                else None",
            "            ),",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(",
            "            \"/flow_run_notification_policies/filter\", json=body",
            "        )",
            "        return pydantic.parse_obj_as(List[FlowRunNotificationPolicy], response.json())",
            "",
            "    async def read_logs(",
            "        self,",
            "        log_filter: LogFilter = None,",
            "        limit: int = None,",
            "        offset: int = None,",
            "        sort: LogSort = LogSort.TIMESTAMP_ASC,",
            "    ) -> List[Log]:",
            "        \"\"\"",
            "        Read flow and task run logs.",
            "        \"\"\"",
            "        body = {",
            "            \"logs\": log_filter.dict(json_compatible=True) if log_filter else None,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "            \"sort\": sort,",
            "        }",
            "",
            "        response = await self._client.post(\"/logs/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[Log], response.json())",
            "",
            "    async def resolve_datadoc(self, datadoc: DataDocument) -> Any:",
            "        \"\"\"",
            "        Recursively decode possibly nested data documents.",
            "",
            "        \"server\" encoded documents will be retrieved from the server.",
            "",
            "        Args:",
            "            datadoc: The data document to resolve",
            "",
            "        Returns:",
            "            a decoded object, the innermost data",
            "        \"\"\"",
            "        if not isinstance(datadoc, DataDocument):",
            "            raise TypeError(",
            "                f\"`resolve_datadoc` received invalid type {type(datadoc).__name__}\"",
            "            )",
            "",
            "        async def resolve_inner(data):",
            "            if isinstance(data, bytes):",
            "                try:",
            "                    data = DataDocument.parse_raw(data)",
            "                except pydantic.ValidationError:",
            "                    return data",
            "",
            "            if isinstance(data, DataDocument):",
            "                return await resolve_inner(data.decode())",
            "",
            "            return data",
            "",
            "        return await resolve_inner(datadoc)",
            "",
            "    async def send_worker_heartbeat(",
            "        self,",
            "        work_pool_name: str,",
            "        worker_name: str,",
            "        heartbeat_interval_seconds: Optional[float] = None,",
            "    ):",
            "        \"\"\"",
            "        Sends a worker heartbeat for a given work pool.",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool to heartbeat against.",
            "            worker_name: The name of the worker sending the heartbeat.",
            "        \"\"\"",
            "        await self._client.post(",
            "            f\"/work_pools/{work_pool_name}/workers/heartbeat\",",
            "            json={",
            "                \"name\": worker_name,",
            "                \"heartbeat_interval_seconds\": heartbeat_interval_seconds,",
            "            },",
            "        )",
            "",
            "    async def read_workers_for_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "        worker_filter: Optional[WorkerFilter] = None,",
            "        offset: Optional[int] = None,",
            "        limit: Optional[int] = None,",
            "    ) -> List[Worker]:",
            "        \"\"\"",
            "        Reads workers for a given work pool.",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool for which to get",
            "                member workers.",
            "            worker_filter: Criteria by which to filter workers.",
            "            limit: Limit for the worker query.",
            "            offset: Limit for the worker query.",
            "        \"\"\"",
            "        response = await self._client.post(",
            "            f\"/work_pools/{work_pool_name}/workers/filter\",",
            "            json={",
            "                \"worker_filter\": (",
            "                    worker_filter.dict(json_compatible=True, exclude_unset=True)",
            "                    if worker_filter",
            "                    else None",
            "                ),",
            "                \"offset\": offset,",
            "                \"limit\": limit,",
            "            },",
            "        )",
            "",
            "        return pydantic.parse_obj_as(List[Worker], response.json())",
            "",
            "    async def read_work_pool(self, work_pool_name: str) -> WorkPool:",
            "        \"\"\"",
            "        Reads information for a given work pool",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool to for which to get",
            "                information.",
            "",
            "        Returns:",
            "            Information about the requested work pool.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/work_pools/{work_pool_name}\")",
            "            return pydantic.parse_obj_as(WorkPool, response.json())",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_work_pools(",
            "        self,",
            "        limit: Optional[int] = None,",
            "        offset: int = 0,",
            "        work_pool_filter: Optional[WorkPoolFilter] = None,",
            "    ) -> List[WorkPool]:",
            "        \"\"\"",
            "        Reads work pools.",
            "",
            "        Args:",
            "            limit: Limit for the work pool query.",
            "            offset: Offset for the work pool query.",
            "            work_pool_filter: Criteria by which to filter work pools.",
            "",
            "        Returns:",
            "            A list of work pools.",
            "        \"\"\"",
            "",
            "        body = {",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "            \"work_pools\": (",
            "                work_pool_filter.dict(json_compatible=True)",
            "                if work_pool_filter",
            "                else None",
            "            ),",
            "        }",
            "        response = await self._client.post(\"/work_pools/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[WorkPool], response.json())",
            "",
            "    async def create_work_pool(",
            "        self,",
            "        work_pool: WorkPoolCreate,",
            "    ) -> WorkPool:",
            "        \"\"\"",
            "        Creates a work pool with the provided configuration.",
            "",
            "        Args:",
            "            work_pool: Desired configuration for the new work pool.",
            "",
            "        Returns:",
            "            Information about the newly created work pool.",
            "        \"\"\"",
            "        try:",
            "            response = await self._client.post(",
            "                \"/work_pools/\",",
            "                json=work_pool.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_409_CONFLICT:",
            "                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "        return pydantic.parse_obj_as(WorkPool, response.json())",
            "",
            "    async def update_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "        work_pool: WorkPoolUpdate,",
            "    ):",
            "        \"\"\"",
            "        Updates a work pool.",
            "",
            "        Args:",
            "            work_pool_name: Name of the work pool to update.",
            "            work_pool: Fields to update in the work pool.",
            "        \"\"\"",
            "        try:",
            "            await self._client.patch(",
            "                f\"/work_pools/{work_pool_name}\",",
            "                json=work_pool.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "    ):",
            "        \"\"\"",
            "        Deletes a work pool.",
            "",
            "        Args:",
            "            work_pool_name: Name of the work pool to delete.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/work_pools/{work_pool_name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_work_queues(",
            "        self,",
            "        work_pool_name: Optional[str] = None,",
            "        work_queue_filter: Optional[WorkQueueFilter] = None,",
            "        limit: Optional[int] = None,",
            "        offset: Optional[int] = None,",
            "    ) -> List[WorkQueue]:",
            "        \"\"\"",
            "        Retrieves queues for a work pool.",
            "",
            "        Args:",
            "            work_pool_name: Name of the work pool for which to get queues.",
            "            work_queue_filter: Criteria by which to filter queues.",
            "            limit: Limit for the queue query.",
            "            offset: Limit for the queue query.",
            "",
            "        Returns:",
            "            List of queues for the specified work pool.",
            "        \"\"\"",
            "        json = {",
            "            \"work_queues\": (",
            "                work_queue_filter.dict(json_compatible=True, exclude_unset=True)",
            "                if work_queue_filter",
            "                else None",
            "            ),",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "",
            "        if work_pool_name:",
            "            try:",
            "                response = await self._client.post(",
            "                    f\"/work_pools/{work_pool_name}/queues/filter\",",
            "                    json=json,",
            "                )",
            "            except httpx.HTTPStatusError as e:",
            "                if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                    raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "                else:",
            "                    raise",
            "        else:",
            "            response = await self._client.post(\"/work_queues/filter\", json=json)",
            "",
            "        return pydantic.parse_obj_as(List[WorkQueue], response.json())",
            "",
            "    async def get_scheduled_flow_runs_for_deployments(",
            "        self,",
            "        deployment_ids: List[UUID],",
            "        scheduled_before: Optional[datetime.datetime] = None,",
            "        limit: Optional[int] = None,",
            "    ):",
            "        body: Dict[str, Any] = dict(deployment_ids=[str(id) for id in deployment_ids])",
            "        if scheduled_before:",
            "            body[\"scheduled_before\"] = str(scheduled_before)",
            "        if limit:",
            "            body[\"limit\"] = limit",
            "",
            "        response = await self._client.post(",
            "            \"/deployments/get_scheduled_flow_runs\",",
            "            json=body,",
            "        )",
            "",
            "        return pydantic.parse_obj_as(List[FlowRunResponse], response.json())",
            "",
            "    async def get_scheduled_flow_runs_for_work_pool(",
            "        self,",
            "        work_pool_name: str,",
            "        work_queue_names: Optional[List[str]] = None,",
            "        scheduled_before: Optional[datetime.datetime] = None,",
            "    ) -> List[WorkerFlowRunResponse]:",
            "        \"\"\"",
            "        Retrieves scheduled flow runs for the provided set of work pool queues.",
            "",
            "        Args:",
            "            work_pool_name: The name of the work pool that the work pool",
            "                queues are associated with.",
            "            work_queue_names: The names of the work pool queues from which",
            "                to get scheduled flow runs.",
            "            scheduled_before: Datetime used to filter returned flow runs. Flow runs",
            "                scheduled for after the given datetime string will not be returned.",
            "",
            "        Returns:",
            "            A list of worker flow run responses containing information about the",
            "            retrieved flow runs.",
            "        \"\"\"",
            "        body: Dict[str, Any] = {}",
            "        if work_queue_names is not None:",
            "            body[\"work_queue_names\"] = list(work_queue_names)",
            "        if scheduled_before:",
            "            body[\"scheduled_before\"] = str(scheduled_before)",
            "",
            "        response = await self._client.post(",
            "            f\"/work_pools/{work_pool_name}/get_scheduled_flow_runs\",",
            "            json=body,",
            "        )",
            "        return pydantic.parse_obj_as(List[WorkerFlowRunResponse], response.json())",
            "",
            "    async def create_artifact(",
            "        self,",
            "        artifact: ArtifactCreate,",
            "    ) -> Artifact:",
            "        \"\"\"",
            "        Creates an artifact with the provided configuration.",
            "",
            "        Args:",
            "            artifact: Desired configuration for the new artifact.",
            "        Returns:",
            "            Information about the newly created artifact.",
            "        \"\"\"",
            "",
            "        response = await self._client.post(",
            "            \"/artifacts/\",",
            "            json=artifact.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "",
            "        return pydantic.parse_obj_as(Artifact, response.json())",
            "",
            "    async def read_artifacts(",
            "        self,",
            "        *,",
            "        artifact_filter: ArtifactFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        sort: ArtifactSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[Artifact]:",
            "        \"\"\"",
            "        Query the Prefect API for artifacts. Only artifacts matching all criteria will",
            "        be returned.",
            "        Args:",
            "            artifact_filter: filter criteria for artifacts",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            sort: sort criteria for the artifacts",
            "            limit: limit for the artifact query",
            "            offset: offset for the artifact query",
            "        Returns:",
            "            a list of Artifact model representations of the artifacts",
            "        \"\"\"",
            "        body = {",
            "            \"artifacts\": (",
            "                artifact_filter.dict(json_compatible=True) if artifact_filter else None",
            "            ),",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(\"/artifacts/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[Artifact], response.json())",
            "",
            "    async def read_latest_artifacts(",
            "        self,",
            "        *,",
            "        artifact_filter: ArtifactCollectionFilter = None,",
            "        flow_run_filter: FlowRunFilter = None,",
            "        task_run_filter: TaskRunFilter = None,",
            "        sort: ArtifactCollectionSort = None,",
            "        limit: int = None,",
            "        offset: int = 0,",
            "    ) -> List[ArtifactCollection]:",
            "        \"\"\"",
            "        Query the Prefect API for artifacts. Only artifacts matching all criteria will",
            "        be returned.",
            "        Args:",
            "            artifact_filter: filter criteria for artifacts",
            "            flow_run_filter: filter criteria for flow runs",
            "            task_run_filter: filter criteria for task runs",
            "            sort: sort criteria for the artifacts",
            "            limit: limit for the artifact query",
            "            offset: offset for the artifact query",
            "        Returns:",
            "            a list of Artifact model representations of the artifacts",
            "        \"\"\"",
            "        body = {",
            "            \"artifacts\": (",
            "                artifact_filter.dict(json_compatible=True) if artifact_filter else None",
            "            ),",
            "            \"flow_runs\": (",
            "                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None",
            "            ),",
            "            \"task_runs\": (",
            "                task_run_filter.dict(json_compatible=True) if task_run_filter else None",
            "            ),",
            "            \"sort\": sort,",
            "            \"limit\": limit,",
            "            \"offset\": offset,",
            "        }",
            "        response = await self._client.post(\"/artifacts/latest/filter\", json=body)",
            "        return pydantic.parse_obj_as(List[ArtifactCollection], response.json())",
            "",
            "    async def delete_artifact(self, artifact_id: UUID) -> None:",
            "        \"\"\"",
            "        Deletes an artifact with the provided id.",
            "",
            "        Args:",
            "            artifact_id: The id of the artifact to delete.",
            "        \"\"\"",
            "        try:",
            "            await self._client.delete(f\"/artifacts/{artifact_id}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_variable_by_name(self, name: str) -> Optional[Variable]:",
            "        \"\"\"Reads a variable by name. Returns None if no variable is found.\"\"\"",
            "        try:",
            "            response = await self._client.get(f\"/variables/name/{name}\")",
            "            return pydantic.parse_obj_as(Variable, response.json())",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                return None",
            "            else:",
            "                raise",
            "",
            "    async def delete_variable_by_name(self, name: str):",
            "        \"\"\"Deletes a variable by name.\"\"\"",
            "        try:",
            "            await self._client.delete(f\"/variables/name/{name}\")",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == 404:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_variables(self, limit: int = None) -> List[Variable]:",
            "        \"\"\"Reads all variables.\"\"\"",
            "        response = await self._client.post(\"/variables/filter\", json={\"limit\": limit})",
            "        return pydantic.parse_obj_as(List[Variable], response.json())",
            "",
            "    async def read_worker_metadata(self) -> Dict[str, Any]:",
            "        \"\"\"Reads worker metadata stored in Prefect collection registry.\"\"\"",
            "        response = await self._client.get(\"collections/views/aggregate-worker-metadata\")",
            "        response.raise_for_status()",
            "        return response.json()",
            "",
            "    async def create_automation(self, automation: Automation) -> UUID:",
            "        \"\"\"Creates an automation in Prefect Cloud.\"\"\"",
            "        if self.server_type != ServerType.CLOUD:",
            "            raise RuntimeError(\"Automations are only supported for Prefect Cloud.\")",
            "",
            "        response = await self._client.post(",
            "            \"/automations/\",",
            "            json=automation.dict(json_compatible=True),",
            "        )",
            "",
            "        return UUID(response.json()[\"id\"])",
            "",
            "    async def read_resource_related_automations(",
            "        self, resource_id: str",
            "    ) -> List[ExistingAutomation]:",
            "        if self.server_type != ServerType.CLOUD:",
            "            raise RuntimeError(\"Automations are only supported for Prefect Cloud.\")",
            "",
            "        response = await self._client.get(f\"/automations/related-to/{resource_id}\")",
            "        response.raise_for_status()",
            "        return pydantic.parse_obj_as(List[ExistingAutomation], response.json())",
            "",
            "    async def delete_resource_owned_automations(self, resource_id: str):",
            "        if self.server_type != ServerType.CLOUD:",
            "            raise RuntimeError(\"Automations are only supported for Prefect Cloud.\")",
            "",
            "        await self._client.delete(f\"/automations/owned-by/{resource_id}\")",
            "",
            "    async def increment_concurrency_slots(",
            "        self, names: List[str], slots: int, mode: str",
            "    ) -> httpx.Response:",
            "        return await self._client.post(",
            "            \"/v2/concurrency_limits/increment\",",
            "            json={\"names\": names, \"slots\": slots, \"mode\": mode},",
            "        )",
            "",
            "    async def release_concurrency_slots(",
            "        self, names: List[str], slots: int, occupancy_seconds: float",
            "    ) -> httpx.Response:",
            "        return await self._client.post(",
            "            \"/v2/concurrency_limits/decrement\",",
            "            json={",
            "                \"names\": names,",
            "                \"slots\": slots,",
            "                \"occupancy_seconds\": occupancy_seconds,",
            "            },",
            "        )",
            "",
            "    async def create_global_concurrency_limit(",
            "        self, concurrency_limit: GlobalConcurrencyLimitCreate",
            "    ) -> UUID:",
            "        response = await self._client.post(",
            "            \"/v2/concurrency_limits/\",",
            "            json=concurrency_limit.dict(json_compatible=True, exclude_unset=True),",
            "        )",
            "        return UUID(response.json()[\"id\"])",
            "",
            "    async def update_global_concurrency_limit(",
            "        self, name: str, concurrency_limit: GlobalConcurrencyLimitUpdate",
            "    ) -> httpx.Response:",
            "        try:",
            "            response = await self._client.patch(",
            "                f\"/v2/concurrency_limits/{name}\",",
            "                json=concurrency_limit.dict(json_compatible=True, exclude_unset=True),",
            "            )",
            "            return response",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def delete_global_concurrency_limit_by_name(",
            "        self, name: str",
            "    ) -> httpx.Response:",
            "        try:",
            "            response = await self._client.delete(f\"/v2/concurrency_limits/{name}\")",
            "            return response",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_global_concurrency_limit_by_name(",
            "        self, name: str",
            "    ) -> Dict[str, object]:",
            "        try:",
            "            response = await self._client.get(f\"/v2/concurrency_limits/{name}\")",
            "            return response.json()",
            "        except httpx.HTTPStatusError as e:",
            "            if e.response.status_code == status.HTTP_404_NOT_FOUND:",
            "                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e",
            "            else:",
            "                raise",
            "",
            "    async def read_global_concurrency_limits(",
            "        self, limit: int = 10, offset: int = 0",
            "    ) -> List[Dict[str, object]]:",
            "        response = await self._client.post(",
            "            \"/v2/concurrency_limits/filter\",",
            "            json={",
            "                \"limit\": limit,",
            "                \"offset\": offset,",
            "            },",
            "        )",
            "        return response.json()",
            "",
            "    async def create_flow_run_input(",
            "        self, flow_run_id: UUID, key: str, value: str, sender: Optional[str] = None",
            "    ):",
            "        \"\"\"",
            "        Creates a flow run input.",
            "",
            "        Args:",
            "            flow_run_id: The flow run id.",
            "            key: The input key.",
            "            value: The input value.",
            "            sender: The sender of the input.",
            "        \"\"\"",
            "",
            "        # Initialize the input to ensure that the key is valid.",
            "        FlowRunInput(flow_run_id=flow_run_id, key=key, value=value)",
            "",
            "        response = await self._client.post(",
            "            f\"/flow_runs/{flow_run_id}/input\",",
            "            json={\"key\": key, \"value\": value, \"sender\": sender},",
            "        )",
            "        response.raise_for_status()",
            "",
            "    async def filter_flow_run_input(",
            "        self, flow_run_id: UUID, key_prefix: str, limit: int, exclude_keys: Set[str]",
            "    ) -> List[FlowRunInput]:",
            "        response = await self._client.post(",
            "            f\"/flow_runs/{flow_run_id}/input/filter\",",
            "            json={",
            "                \"prefix\": key_prefix,",
            "                \"limit\": limit,",
            "                \"exclude_keys\": list(exclude_keys),",
            "            },",
            "        )",
            "        response.raise_for_status()",
            "        return pydantic.parse_obj_as(List[FlowRunInput], response.json())",
            "",
            "    async def read_flow_run_input(self, flow_run_id: UUID, key: str) -> str:",
            "        \"\"\"",
            "        Reads a flow run input.",
            "",
            "        Args:",
            "            flow_run_id: The flow run id.",
            "            key: The input key.",
            "        \"\"\"",
            "        response = await self._client.get(f\"/flow_runs/{flow_run_id}/input/{key}\")",
            "        response.raise_for_status()",
            "        return response.content.decode()",
            "",
            "    async def delete_flow_run_input(self, flow_run_id: UUID, key: str):",
            "        \"\"\"",
            "        Deletes a flow run input.",
            "",
            "        Args:",
            "            flow_run_id: The flow run id.",
            "            key: The input key.",
            "        \"\"\"",
            "        response = await self._client.delete(f\"/flow_runs/{flow_run_id}/input/{key}\")",
            "        response.raise_for_status()",
            "",
            "    async def __aenter__(self):",
            "        \"\"\"",
            "        Start the client.",
            "",
            "        If the client is already started, this will raise an exception.",
            "",
            "        If the client is already closed, this will raise an exception. Use a new client",
            "        instance instead.",
            "        \"\"\"",
            "        if self._closed:",
            "            # httpx.AsyncClient does not allow reuse so we will not either.",
            "            raise RuntimeError(",
            "                \"The client cannot be started again after closing. \"",
            "                \"Retrieve a new client with `get_client()` instead.\"",
            "            )",
            "",
            "        if self._started:",
            "            # httpx.AsyncClient does not allow reentrancy so we will not either.",
            "            raise RuntimeError(\"The client cannot be started more than once.\")",
            "",
            "        self._loop = asyncio.get_running_loop()",
            "        await self._exit_stack.__aenter__()",
            "",
            "        # Enter a lifespan context if using an ephemeral application.",
            "        # See https://github.com/encode/httpx/issues/350",
            "        if self._ephemeral_app and self.manage_lifespan:",
            "            self._ephemeral_lifespan = await self._exit_stack.enter_async_context(",
            "                app_lifespan_context(self._ephemeral_app)",
            "            )",
            "",
            "        if self._ephemeral_app:",
            "            self.logger.debug(",
            "                \"Using ephemeral application with database at \"",
            "                f\"{PREFECT_API_DATABASE_CONNECTION_URL.value()}\"",
            "            )",
            "        else:",
            "            self.logger.debug(f\"Connecting to API at {self.api_url}\")",
            "",
            "        # Enter the httpx client's context",
            "        await self._exit_stack.enter_async_context(self._client)",
            "",
            "        self._started = True",
            "",
            "        return self",
            "",
            "    async def __aexit__(self, *exc_info):",
            "        \"\"\"",
            "        Shutdown the client.",
            "        \"\"\"",
            "        self._closed = True",
            "        return await self._exit_stack.__aexit__(*exc_info)",
            "",
            "    def __enter__(self):",
            "        raise RuntimeError(",
            "            \"The `PrefectClient` must be entered with an async context. Use 'async \"",
            "            \"with PrefectClient(...)' not 'with PrefectClient(...)'\"",
            "        )",
            "",
            "    def __exit__(self, *_):",
            "        assert False, \"This should never be called but must be defined for __enter__\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "319": [
                "PrefectClient",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "src/prefect/client/schemas/objects.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1632,
                "afterPatchRowNumber": 1632,
                "PatchRowcode": "             \" is used as a rate limit.\""
            },
            "1": {
                "beforePatchRowNumber": 1633,
                "afterPatchRowNumber": 1633,
                "PatchRowcode": "         ),"
            },
            "2": {
                "beforePatchRowNumber": 1634,
                "afterPatchRowNumber": 1634,
                "PatchRowcode": "     )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1635,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1636,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1637,
                "PatchRowcode": "+class CsrfToken(ObjectBaseModel):"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1638,
                "PatchRowcode": "+    token: str = Field("
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1639,
                "PatchRowcode": "+        default=...,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1640,
                "PatchRowcode": "+        description=\"The CSRF token\","
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1641,
                "PatchRowcode": "+    )"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1642,
                "PatchRowcode": "+    client: str = Field("
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1643,
                "PatchRowcode": "+        default=..., description=\"The client id associated with the CSRF token\""
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1644,
                "PatchRowcode": "+    )"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1645,
                "PatchRowcode": "+    expiration: DateTimeTZ = Field("
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1646,
                "PatchRowcode": "+        default=..., description=\"The expiration time of the CSRF token\""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1647,
                "PatchRowcode": "+    )"
            }
        },
        "frontPatchFile": [
            "import datetime",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Dict,",
            "    Generic,",
            "    List,",
            "    Optional,",
            "    TypeVar,",
            "    Union,",
            "    overload,",
            ")",
            "from uuid import UUID",
            "",
            "import orjson",
            "import pendulum",
            "",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "",
            "if HAS_PYDANTIC_V2:",
            "    from pydantic.v1 import Field, HttpUrl, conint, root_validator, validator",
            "else:",
            "    from pydantic import Field, HttpUrl, conint, root_validator, validator",
            "",
            "from typing_extensions import Literal",
            "",
            "from prefect._internal.schemas.bases import ObjectBaseModel, PrefectBaseModel",
            "from prefect._internal.schemas.fields import CreatedBy, DateTimeTZ, UpdatedBy",
            "from prefect._internal.schemas.validators import (",
            "    raise_on_name_alphanumeric_dashes_only,",
            "    raise_on_name_with_banned_characters,",
            ")",
            "from prefect.client.schemas.schedules import SCHEDULE_TYPES",
            "from prefect.settings import PREFECT_CLOUD_API_URL, PREFECT_CLOUD_UI_URL",
            "from prefect.utilities.collections import AutoEnum, listrepr",
            "from prefect.utilities.names import generate_slug",
            "",
            "if TYPE_CHECKING:",
            "    from prefect.deprecated.data_documents import DataDocument",
            "    from prefect.results import BaseResult",
            "",
            "R = TypeVar(\"R\")",
            "",
            "",
            "DEFAULT_BLOCK_SCHEMA_VERSION = \"non-versioned\"",
            "DEFAULT_AGENT_WORK_POOL_NAME = \"default-agent-pool\"",
            "FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS = [",
            "    \"flow_run_notification_policy_id\",",
            "    \"flow_id\",",
            "    \"flow_name\",",
            "    \"flow_run_url\",",
            "    \"flow_run_id\",",
            "    \"flow_run_name\",",
            "    \"flow_run_parameters\",",
            "    \"flow_run_state_type\",",
            "    \"flow_run_state_name\",",
            "    \"flow_run_state_timestamp\",",
            "    \"flow_run_state_message\",",
            "]",
            "MAX_VARIABLE_NAME_LENGTH = 255",
            "MAX_VARIABLE_VALUE_LENGTH = 5000",
            "",
            "",
            "class StateType(AutoEnum):",
            "    \"\"\"Enumeration of state types.\"\"\"",
            "",
            "    SCHEDULED = AutoEnum.auto()",
            "    PENDING = AutoEnum.auto()",
            "    RUNNING = AutoEnum.auto()",
            "    COMPLETED = AutoEnum.auto()",
            "    FAILED = AutoEnum.auto()",
            "    CANCELLED = AutoEnum.auto()",
            "    CRASHED = AutoEnum.auto()",
            "    PAUSED = AutoEnum.auto()",
            "    CANCELLING = AutoEnum.auto()",
            "",
            "",
            "class WorkPoolStatus(AutoEnum):",
            "    \"\"\"Enumeration of work pool statuses.\"\"\"",
            "",
            "    READY = AutoEnum.auto()",
            "    NOT_READY = AutoEnum.auto()",
            "    PAUSED = AutoEnum.auto()",
            "",
            "",
            "class WorkerStatus(AutoEnum):",
            "    \"\"\"Enumeration of worker statuses.\"\"\"",
            "",
            "    ONLINE = AutoEnum.auto()",
            "    OFFLINE = AutoEnum.auto()",
            "",
            "",
            "class DeploymentStatus(AutoEnum):",
            "    \"\"\"Enumeration of deployment statuses.\"\"\"",
            "",
            "    READY = AutoEnum.auto()",
            "    NOT_READY = AutoEnum.auto()",
            "",
            "",
            "class WorkQueueStatus(AutoEnum):",
            "    \"\"\"Enumeration of work queue statuses.\"\"\"",
            "",
            "    READY = AutoEnum.auto()",
            "    NOT_READY = AutoEnum.auto()",
            "    PAUSED = AutoEnum.auto()",
            "",
            "",
            "class StateDetails(PrefectBaseModel):",
            "    flow_run_id: UUID = None",
            "    task_run_id: UUID = None",
            "    # for task runs that represent subflows, the subflow's run ID",
            "    child_flow_run_id: UUID = None",
            "    scheduled_time: DateTimeTZ = None",
            "    cache_key: str = None",
            "    cache_expiration: DateTimeTZ = None",
            "    untrackable_result: bool = False",
            "    pause_timeout: DateTimeTZ = None",
            "    pause_reschedule: bool = False",
            "    pause_key: str = None",
            "    run_input_keyset: Optional[Dict[str, str]] = None",
            "    refresh_cache: bool = None",
            "    retriable: bool = None",
            "    transition_id: Optional[UUID] = None",
            "    task_parameters_id: Optional[UUID] = None",
            "",
            "",
            "class State(ObjectBaseModel, Generic[R]):",
            "    \"\"\"",
            "    The state of a run.",
            "    \"\"\"",
            "",
            "    type: StateType",
            "    name: Optional[str] = Field(default=None)",
            "    timestamp: DateTimeTZ = Field(default_factory=lambda: pendulum.now(\"UTC\"))",
            "    message: Optional[str] = Field(default=None, example=\"Run started\")",
            "    state_details: StateDetails = Field(default_factory=StateDetails)",
            "    data: Union[\"BaseResult[R]\", \"DataDocument[R]\", Any] = Field(",
            "        default=None,",
            "    )",
            "",
            "    @overload",
            "    def result(self: \"State[R]\", raise_on_failure: bool = True) -> R:",
            "        ...",
            "",
            "    @overload",
            "    def result(self: \"State[R]\", raise_on_failure: bool = False) -> Union[R, Exception]:",
            "        ...",
            "",
            "    def result(self, raise_on_failure: bool = True, fetch: Optional[bool] = None):",
            "        \"\"\"",
            "        Retrieve the result attached to this state.",
            "",
            "        Args:",
            "            raise_on_failure: a boolean specifying whether to raise an exception",
            "                if the state is of type `FAILED` and the underlying data is an exception",
            "            fetch: a boolean specifying whether to resolve references to persisted",
            "                results into data. For synchronous users, this defaults to `True`.",
            "                For asynchronous users, this defaults to `False` for backwards",
            "                compatibility.",
            "",
            "        Raises:",
            "            TypeError: If the state is failed but the result is not an exception.",
            "",
            "        Returns:",
            "            The result of the run",
            "",
            "        Examples:",
            "            >>> from prefect import flow, task",
            "            >>> @task",
            "            >>> def my_task(x):",
            "            >>>     return x",
            "",
            "            Get the result from a task future in a flow",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     future = my_task(\"hello\")",
            "            >>>     state = future.wait()",
            "            >>>     result = state.result()",
            "            >>>     print(result)",
            "            >>> my_flow()",
            "            hello",
            "",
            "            Get the result from a flow state",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     return \"hello\"",
            "            >>> my_flow(return_state=True).result()",
            "            hello",
            "",
            "            Get the result from a failed state",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     raise ValueError(\"oh no!\")",
            "            >>> state = my_flow(return_state=True)  # Error is wrapped in FAILED state",
            "            >>> state.result()  # Raises `ValueError`",
            "",
            "            Get the result from a failed state without erroring",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     raise ValueError(\"oh no!\")",
            "            >>> state = my_flow(return_state=True)",
            "            >>> result = state.result(raise_on_failure=False)",
            "            >>> print(result)",
            "            ValueError(\"oh no!\")",
            "",
            "",
            "            Get the result from a flow state in an async context",
            "",
            "            >>> @flow",
            "            >>> async def my_flow():",
            "            >>>     return \"hello\"",
            "            >>> state = await my_flow(return_state=True)",
            "            >>> await state.result()",
            "            hello",
            "        \"\"\"",
            "        from prefect.states import get_state_result",
            "",
            "        return get_state_result(self, raise_on_failure=raise_on_failure, fetch=fetch)",
            "",
            "    def to_state_create(self):",
            "        \"\"\"",
            "        Convert this state to a `StateCreate` type which can be used to set the state of",
            "        a run in the API.",
            "",
            "        This method will drop this state's `data` if it is not a result type. Only",
            "        results should be sent to the API. Other data is only available locally.",
            "        \"\"\"",
            "        from prefect.client.schemas.actions import StateCreate",
            "        from prefect.results import BaseResult",
            "",
            "        return StateCreate(",
            "            type=self.type,",
            "            name=self.name,",
            "            message=self.message,",
            "            data=self.data if isinstance(self.data, BaseResult) else None,",
            "            state_details=self.state_details,",
            "        )",
            "",
            "    @validator(\"name\", always=True)",
            "    def default_name_from_type(cls, v, *, values, **kwargs):",
            "        \"\"\"If a name is not provided, use the type\"\"\"",
            "",
            "        # if `type` is not in `values` it means the `type` didn't pass its own",
            "        # validation check and an error will be raised after this function is called",
            "        if v is None and values.get(\"type\"):",
            "            v = \" \".join([v.capitalize() for v in values.get(\"type\").value.split(\"_\")])",
            "        return v",
            "",
            "    @root_validator",
            "    def default_scheduled_start_time(cls, values):",
            "        \"\"\"",
            "        TODO: This should throw an error instead of setting a default but is out of",
            "              scope for https://github.com/PrefectHQ/orion/pull/174/ and can be rolled",
            "              into work refactoring state initialization",
            "        \"\"\"",
            "        if values.get(\"type\") == StateType.SCHEDULED:",
            "            state_details = values.setdefault(",
            "                \"state_details\", cls.__fields__[\"state_details\"].get_default()",
            "            )",
            "            if not state_details.scheduled_time:",
            "                state_details.scheduled_time = pendulum.now(\"utc\")",
            "        return values",
            "",
            "    def is_scheduled(self) -> bool:",
            "        return self.type == StateType.SCHEDULED",
            "",
            "    def is_pending(self) -> bool:",
            "        return self.type == StateType.PENDING",
            "",
            "    def is_running(self) -> bool:",
            "        return self.type == StateType.RUNNING",
            "",
            "    def is_completed(self) -> bool:",
            "        return self.type == StateType.COMPLETED",
            "",
            "    def is_failed(self) -> bool:",
            "        return self.type == StateType.FAILED",
            "",
            "    def is_crashed(self) -> bool:",
            "        return self.type == StateType.CRASHED",
            "",
            "    def is_cancelled(self) -> bool:",
            "        return self.type == StateType.CANCELLED",
            "",
            "    def is_cancelling(self) -> bool:",
            "        return self.type == StateType.CANCELLING",
            "",
            "    def is_final(self) -> bool:",
            "        return self.type in {",
            "            StateType.CANCELLED,",
            "            StateType.FAILED,",
            "            StateType.COMPLETED,",
            "            StateType.CRASHED,",
            "        }",
            "",
            "    def is_paused(self) -> bool:",
            "        return self.type == StateType.PAUSED",
            "",
            "    def copy(self, *, update: dict = None, reset_fields: bool = False, **kwargs):",
            "        \"\"\"",
            "        Copying API models should return an object that could be inserted into the",
            "        database again. The 'timestamp' is reset using the default factory.",
            "        \"\"\"",
            "        update = update or {}",
            "        update.setdefault(\"timestamp\", self.__fields__[\"timestamp\"].get_default())",
            "        return super().copy(reset_fields=reset_fields, update=update, **kwargs)",
            "",
            "    def __repr__(self) -> str:",
            "        \"\"\"",
            "        Generates a complete state representation appropriate for introspection",
            "        and debugging, including the result:",
            "",
            "        `MyCompletedState(message=\"my message\", type=COMPLETED, result=...)`",
            "        \"\"\"",
            "        from prefect.deprecated.data_documents import DataDocument",
            "",
            "        if isinstance(self.data, DataDocument):",
            "            result = self.data.decode()",
            "        else:",
            "            result = self.data",
            "",
            "        display = dict(",
            "            message=repr(self.message),",
            "            type=str(self.type.value),",
            "            result=repr(result),",
            "        )",
            "",
            "        return f\"{self.name}({', '.join(f'{k}={v}' for k, v in display.items())})\"",
            "",
            "    def __str__(self) -> str:",
            "        \"\"\"",
            "        Generates a simple state representation appropriate for logging:",
            "",
            "        `MyCompletedState(\"my message\", type=COMPLETED)`",
            "        \"\"\"",
            "",
            "        display = []",
            "",
            "        if self.message:",
            "            display.append(repr(self.message))",
            "",
            "        if self.type.value.lower() != self.name.lower():",
            "            display.append(f\"type={self.type.value}\")",
            "",
            "        return f\"{self.name}({', '.join(display)})\"",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(",
            "            (",
            "                getattr(self.state_details, \"flow_run_id\", None),",
            "                getattr(self.state_details, \"task_run_id\", None),",
            "                self.timestamp,",
            "                self.type,",
            "            )",
            "        )",
            "",
            "",
            "class FlowRunPolicy(PrefectBaseModel):",
            "    \"\"\"Defines of how a flow run should be orchestrated.\"\"\"",
            "",
            "    max_retries: int = Field(",
            "        default=0,",
            "        description=(",
            "            \"The maximum number of retries. Field is not used. Please use `retries`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retry_delay_seconds: float = Field(",
            "        default=0,",
            "        description=(",
            "            \"The delay between retries. Field is not used. Please use `retry_delay`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retries: Optional[int] = Field(default=None, description=\"The number of retries.\")",
            "    retry_delay: Optional[int] = Field(",
            "        default=None, description=\"The delay time between retries, in seconds.\"",
            "    )",
            "    pause_keys: Optional[set] = Field(",
            "        default_factory=set, description=\"Tracks pauses this run has observed.\"",
            "    )",
            "    resuming: Optional[bool] = Field(",
            "        default=False, description=\"Indicates if this run is resuming from a pause.\"",
            "    )",
            "",
            "    @root_validator",
            "    def populate_deprecated_fields(cls, values):",
            "        \"\"\"",
            "        If deprecated fields are provided, populate the corresponding new fields",
            "        to preserve orchestration behavior.",
            "        \"\"\"",
            "        if not values.get(\"retries\", None) and values.get(\"max_retries\", 0) != 0:",
            "            values[\"retries\"] = values[\"max_retries\"]",
            "        if (",
            "            not values.get(\"retry_delay\", None)",
            "            and values.get(\"retry_delay_seconds\", 0) != 0",
            "        ):",
            "            values[\"retry_delay\"] = values[\"retry_delay_seconds\"]",
            "        return values",
            "",
            "",
            "class FlowRun(ObjectBaseModel):",
            "    name: str = Field(",
            "        default_factory=lambda: generate_slug(2),",
            "        description=(",
            "            \"The name of the flow run. Defaults to a random slug if not specified.\"",
            "        ),",
            "        example=\"my-flow-run\",",
            "    )",
            "    flow_id: UUID = Field(default=..., description=\"The id of the flow being run.\")",
            "    state_id: Optional[UUID] = Field(",
            "        default=None, description=\"The id of the flow run's current state.\"",
            "    )",
            "    deployment_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The id of the deployment associated with this flow run, if available.\"",
            "        ),",
            "    )",
            "    work_queue_name: Optional[str] = Field(",
            "        default=None, description=\"The work queue that handled this flow run.\"",
            "    )",
            "    flow_version: Optional[str] = Field(",
            "        default=None,",
            "        description=\"The version of the flow executed in this flow run.\",",
            "        example=\"1.0\",",
            "    )",
            "    parameters: dict = Field(",
            "        default_factory=dict, description=\"Parameters for the flow run.\"",
            "    )",
            "    idempotency_key: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An optional idempotency key for the flow run. Used to ensure the same flow\"",
            "            \" run is not created multiple times.\"",
            "        ),",
            "    )",
            "    context: dict = Field(",
            "        default_factory=dict,",
            "        description=\"Additional context for the flow run.\",",
            "        example={\"my_var\": \"my_val\"},",
            "    )",
            "    empirical_policy: FlowRunPolicy = Field(",
            "        default_factory=FlowRunPolicy,",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of tags on the flow run\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "    parent_task_run_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=(",
            "            \"If the flow run is a subflow, the id of the 'dummy' task in the parent\"",
            "            \" flow used to track subflow state.\"",
            "        ),",
            "    )",
            "    run_count: int = Field(",
            "        default=0, description=\"The number of times the flow run was executed.\"",
            "    )",
            "    expected_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The flow run's expected start time.\",",
            "    )",
            "    next_scheduled_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The next time the flow run is scheduled to start.\",",
            "    )",
            "    start_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual start time.\"",
            "    )",
            "    end_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual end time.\"",
            "    )",
            "    total_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=(",
            "            \"Total run time. If the flow run was executed multiple times, the time of\"",
            "            \" each run will be summed.\"",
            "        ),",
            "    )",
            "    estimated_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"A real-time estimate of the total run time.\",",
            "    )",
            "    estimated_start_time_delta: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"The difference between actual and expected start time.\",",
            "    )",
            "    auto_scheduled: bool = Field(",
            "        default=False,",
            "        description=\"Whether or not the flow run was automatically scheduled.\",",
            "    )",
            "    infrastructure_document_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The block document defining infrastructure to use this flow run.\",",
            "    )",
            "    infrastructure_pid: Optional[str] = Field(",
            "        default=None,",
            "        description=\"The id of the flow run as returned by an infrastructure block.\",",
            "    )",
            "    created_by: Optional[CreatedBy] = Field(",
            "        default=None,",
            "        description=\"Optional information about the creator of this flow run.\",",
            "    )",
            "    work_queue_id: Optional[UUID] = Field(",
            "        default=None, description=\"The id of the run's work pool queue.\"",
            "    )",
            "",
            "    work_pool_id: Optional[UUID] = Field(",
            "        description=\"The work pool with which the queue is associated.\"",
            "    )",
            "    work_pool_name: Optional[str] = Field(",
            "        default=None,",
            "        description=\"The name of the flow run's work pool.\",",
            "        example=\"my-work-pool\",",
            "    )",
            "    state: Optional[State] = Field(",
            "        default=None,",
            "        description=\"The state of the flow run.\",",
            "        example=State(type=StateType.COMPLETED),",
            "    )",
            "    job_variables: Optional[dict] = Field(",
            "        default=None, description=\"Job variables for the flow run.\"",
            "    )",
            "",
            "    def __eq__(self, other: Any) -> bool:",
            "        \"\"\"",
            "        Check for \"equality\" to another flow run schema",
            "",
            "        Estimates times are rolling and will always change with repeated queries for",
            "        a flow run so we ignore them during equality checks.",
            "        \"\"\"",
            "        if isinstance(other, FlowRun):",
            "            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}",
            "            return self.dict(exclude=exclude_fields) == other.dict(",
            "                exclude=exclude_fields",
            "            )",
            "        return super().__eq__(other)",
            "",
            "    @validator(\"name\", pre=True)",
            "    def set_default_name(cls, name):",
            "        return name or generate_slug(2)",
            "",
            "    # These are server-side optimizations and should not be present on client models",
            "    # TODO: Deprecate these fields",
            "",
            "    state_type: Optional[StateType] = Field(",
            "        default=None, description=\"The type of the current flow run state.\"",
            "    )",
            "    state_name: Optional[str] = Field(",
            "        default=None, description=\"The name of the current flow run state.\"",
            "    )",
            "",
            "",
            "class TaskRunPolicy(PrefectBaseModel):",
            "    \"\"\"Defines of how a task run should retry.\"\"\"",
            "",
            "    max_retries: int = Field(",
            "        default=0,",
            "        description=(",
            "            \"The maximum number of retries. Field is not used. Please use `retries`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retry_delay_seconds: float = Field(",
            "        default=0,",
            "        description=(",
            "            \"The delay between retries. Field is not used. Please use `retry_delay`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retries: Optional[int] = Field(default=None, description=\"The number of retries.\")",
            "    retry_delay: Union[None, int, List[int]] = Field(",
            "        default=None,",
            "        description=\"A delay time or list of delay times between retries, in seconds.\",",
            "    )",
            "    retry_jitter_factor: Optional[float] = Field(",
            "        default=None, description=\"Determines the amount a retry should jitter\"",
            "    )",
            "",
            "    @root_validator",
            "    def populate_deprecated_fields(cls, values):",
            "        \"\"\"",
            "        If deprecated fields are provided, populate the corresponding new fields",
            "        to preserve orchestration behavior.",
            "        \"\"\"",
            "        if not values.get(\"retries\", None) and values.get(\"max_retries\", 0) != 0:",
            "            values[\"retries\"] = values[\"max_retries\"]",
            "",
            "        if (",
            "            not values.get(\"retry_delay\", None)",
            "            and values.get(\"retry_delay_seconds\", 0) != 0",
            "        ):",
            "            values[\"retry_delay\"] = values[\"retry_delay_seconds\"]",
            "",
            "        return values",
            "",
            "    @validator(\"retry_delay\")",
            "    def validate_configured_retry_delays(cls, v):",
            "        if isinstance(v, list) and (len(v) > 50):",
            "            raise ValueError(\"Can not configure more than 50 retry delays per task.\")",
            "        return v",
            "",
            "    @validator(\"retry_jitter_factor\")",
            "    def validate_jitter_factor(cls, v):",
            "        if v is not None and v < 0:",
            "            raise ValueError(\"`retry_jitter_factor` must be >= 0.\")",
            "        return v",
            "",
            "",
            "class TaskRunInput(PrefectBaseModel):",
            "    \"\"\"",
            "    Base class for classes that represent inputs to task runs, which",
            "    could include, constants, parameters, or other task runs.",
            "    \"\"\"",
            "",
            "    # freeze TaskRunInputs to allow them to be placed in sets",
            "    class Config:",
            "        frozen = True",
            "",
            "    input_type: str",
            "",
            "",
            "class TaskRunResult(TaskRunInput):",
            "    \"\"\"Represents a task run result input to another task run.\"\"\"",
            "",
            "    input_type: Literal[\"task_run\"] = \"task_run\"",
            "    id: UUID",
            "",
            "",
            "class Parameter(TaskRunInput):",
            "    \"\"\"Represents a parameter input to a task run.\"\"\"",
            "",
            "    input_type: Literal[\"parameter\"] = \"parameter\"",
            "    name: str",
            "",
            "",
            "class Constant(TaskRunInput):",
            "    \"\"\"Represents constant input value to a task run.\"\"\"",
            "",
            "    input_type: Literal[\"constant\"] = \"constant\"",
            "    type: str",
            "",
            "",
            "class TaskRun(ObjectBaseModel):",
            "    name: str = Field(default_factory=lambda: generate_slug(2), example=\"my-task-run\")",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run id of the task run.\"",
            "    )",
            "    task_key: str = Field(",
            "        default=..., description=\"A unique identifier for the task being run.\"",
            "    )",
            "    dynamic_key: str = Field(",
            "        default=...,",
            "        description=(",
            "            \"A dynamic key used to differentiate between multiple runs of the same task\"",
            "            \" within the same flow run.\"",
            "        ),",
            "    )",
            "    cache_key: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An optional cache key. If a COMPLETED state associated with this cache key\"",
            "            \" is found, the cached COMPLETED state will be used instead of executing\"",
            "            \" the task run.\"",
            "        ),",
            "    )",
            "    cache_expiration: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"Specifies when the cached state should expire.\"",
            "    )",
            "    task_version: Optional[str] = Field(",
            "        default=None, description=\"The version of the task being run.\"",
            "    )",
            "    empirical_policy: TaskRunPolicy = Field(",
            "        default_factory=TaskRunPolicy,",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of tags for the task run.\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "    state_id: Optional[UUID] = Field(",
            "        default=None, description=\"The id of the current task run state.\"",
            "    )",
            "    task_inputs: Dict[str, List[Union[TaskRunResult, Parameter, Constant]]] = Field(",
            "        default_factory=dict,",
            "        description=(",
            "            \"Tracks the source of inputs to a task run. Used for internal bookkeeping.\"",
            "        ),",
            "    )",
            "    state_type: Optional[StateType] = Field(",
            "        default=None, description=\"The type of the current task run state.\"",
            "    )",
            "    state_name: Optional[str] = Field(",
            "        default=None, description=\"The name of the current task run state.\"",
            "    )",
            "    run_count: int = Field(",
            "        default=0, description=\"The number of times the task run has been executed.\"",
            "    )",
            "    flow_run_run_count: int = Field(",
            "        default=0,",
            "        description=(",
            "            \"If the parent flow has retried, this indicates the flow retry this run is\"",
            "            \" associated with.\"",
            "        ),",
            "    )",
            "    expected_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The task run's expected start time.\",",
            "    )",
            "",
            "    # the next scheduled start time will be populated",
            "    # whenever the run is in a scheduled state",
            "    next_scheduled_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The next time the task run is scheduled to start.\",",
            "    )",
            "    start_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual start time.\"",
            "    )",
            "    end_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual end time.\"",
            "    )",
            "    total_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=(",
            "            \"Total run time. If the task run was executed multiple times, the time of\"",
            "            \" each run will be summed.\"",
            "        ),",
            "    )",
            "    estimated_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"A real-time estimate of total run time.\",",
            "    )",
            "    estimated_start_time_delta: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"The difference between actual and expected start time.\",",
            "    )",
            "",
            "    state: Optional[State] = Field(",
            "        default=None,",
            "        description=\"The state of the flow run.\",",
            "        example=State(type=StateType.COMPLETED),",
            "    )",
            "",
            "    @validator(\"name\", pre=True)",
            "    def set_default_name(cls, name):",
            "        return name or generate_slug(2)",
            "",
            "",
            "class Workspace(PrefectBaseModel):",
            "    \"\"\"",
            "    A Prefect Cloud workspace.",
            "",
            "    Expected payload for each workspace returned by the `me/workspaces` route.",
            "    \"\"\"",
            "",
            "    account_id: UUID = Field(..., description=\"The account id of the workspace.\")",
            "    account_name: str = Field(..., description=\"The account name.\")",
            "    account_handle: str = Field(..., description=\"The account's unique handle.\")",
            "    workspace_id: UUID = Field(..., description=\"The workspace id.\")",
            "    workspace_name: str = Field(..., description=\"The workspace name.\")",
            "    workspace_description: str = Field(..., description=\"Description of the workspace.\")",
            "    workspace_handle: str = Field(..., description=\"The workspace's unique handle.\")",
            "",
            "    class Config:",
            "        extra = \"ignore\"",
            "",
            "    @property",
            "    def handle(self) -> str:",
            "        \"\"\"",
            "        The full handle of the workspace as `account_handle` / `workspace_handle`",
            "        \"\"\"",
            "        return self.account_handle + \"/\" + self.workspace_handle",
            "",
            "    def api_url(self) -> str:",
            "        \"\"\"",
            "        Generate the API URL for accessing this workspace",
            "        \"\"\"",
            "        return (",
            "            f\"{PREFECT_CLOUD_API_URL.value()}\"",
            "            f\"/accounts/{self.account_id}\"",
            "            f\"/workspaces/{self.workspace_id}\"",
            "        )",
            "",
            "    def ui_url(self) -> str:",
            "        \"\"\"",
            "        Generate the UI URL for accessing this workspace",
            "        \"\"\"",
            "        return (",
            "            f\"{PREFECT_CLOUD_UI_URL.value()}\"",
            "            f\"/account/{self.account_id}\"",
            "            f\"/workspace/{self.workspace_id}\"",
            "        )",
            "",
            "    def __hash__(self):",
            "        return hash(self.handle)",
            "",
            "",
            "class BlockType(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block type\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"A block type's name\")",
            "    slug: str = Field(default=..., description=\"A block type's slug\")",
            "    logo_url: Optional[HttpUrl] = Field(",
            "        default=None, description=\"Web URL for the block type's logo\"",
            "    )",
            "    documentation_url: Optional[HttpUrl] = Field(",
            "        default=None, description=\"Web URL for the block type's documentation\"",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None,",
            "        description=\"A short blurb about the corresponding block's intended use\",",
            "    )",
            "    code_example: Optional[str] = Field(",
            "        default=None,",
            "        description=\"A code snippet demonstrating use of the corresponding block\",",
            "    )",
            "    is_protected: bool = Field(",
            "        default=False, description=\"Protected block types cannot be modified via API.\"",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class BlockSchema(ObjectBaseModel):",
            "    \"\"\"A representation of a block schema.\"\"\"",
            "",
            "    checksum: str = Field(default=..., description=\"The block schema's unique checksum\")",
            "    fields: dict = Field(",
            "        default_factory=dict, description=\"The block schema's field schema\"",
            "    )",
            "    block_type_id: Optional[UUID] = Field(default=..., description=\"A block type ID\")",
            "    block_type: Optional[BlockType] = Field(",
            "        default=None, description=\"The associated block type\"",
            "    )",
            "    capabilities: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of Block capabilities\",",
            "    )",
            "    version: str = Field(",
            "        default=DEFAULT_BLOCK_SCHEMA_VERSION,",
            "        description=\"Human readable identifier for the block schema\",",
            "    )",
            "",
            "",
            "class BlockDocument(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block document.\"\"\"",
            "",
            "    name: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The block document's name. Not required for anonymous block documents.\"",
            "        ),",
            "    )",
            "    data: dict = Field(default_factory=dict, description=\"The block document's data\")",
            "    block_schema_id: UUID = Field(default=..., description=\"A block schema ID\")",
            "    block_schema: Optional[BlockSchema] = Field(",
            "        default=None, description=\"The associated block schema\"",
            "    )",
            "    block_type_id: UUID = Field(default=..., description=\"A block type ID\")",
            "    block_type_name: Optional[str] = Field(None, description=\"A block type name\")",
            "    block_type: Optional[BlockType] = Field(",
            "        default=None, description=\"The associated block type\"",
            "    )",
            "    block_document_references: Dict[str, Dict[str, Any]] = Field(",
            "        default_factory=dict, description=\"Record of the block document's references\"",
            "    )",
            "    is_anonymous: bool = Field(",
            "        default=False,",
            "        description=(",
            "            \"Whether the block is anonymous (anonymous blocks are usually created by\"",
            "            \" Prefect automatically)\"",
            "        ),",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        # the BlockDocumentCreate subclass allows name=None",
            "        # and will inherit this validator",
            "        if v is not None:",
            "            raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "    @root_validator",
            "    def validate_name_is_present_if_not_anonymous(cls, values):",
            "        # anonymous blocks may have no name prior to actually being",
            "        # stored in the database",
            "        if not values.get(\"is_anonymous\") and not values.get(\"name\"):",
            "            raise ValueError(\"Names must be provided for block documents.\")",
            "        return values",
            "",
            "",
            "class Flow(ObjectBaseModel):",
            "    \"\"\"An ORM representation of flow data.\"\"\"",
            "",
            "    name: str = Field(",
            "        default=..., description=\"The name of the flow\", example=\"my-flow\"",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of flow tags\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class FlowRunnerSettings(PrefectBaseModel):",
            "    \"\"\"",
            "    An API schema for passing details about the flow runner.",
            "",
            "    This schema is agnostic to the types and configuration provided by clients",
            "    \"\"\"",
            "",
            "    type: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The type of the flow runner which can be used by the client for\"",
            "            \" dispatching.\"",
            "        ),",
            "    )",
            "    config: Optional[dict] = Field(",
            "        default=None, description=\"The configuration for the given flow runner type.\"",
            "    )",
            "",
            "    # The following is required for composite compatibility in the ORM",
            "",
            "    def __init__(self, type: str = None, config: dict = None, **kwargs) -> None:",
            "        # Pydantic does not support positional arguments so they must be converted to",
            "        # keyword arguments",
            "        super().__init__(type=type, config=config, **kwargs)",
            "",
            "    def __composite_values__(self):",
            "        return self.type, self.config",
            "",
            "",
            "class DeploymentSchedule(ObjectBaseModel):",
            "    deployment_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The deployment id associated with this schedule.\",",
            "    )",
            "    schedule: SCHEDULE_TYPES = Field(",
            "        default=..., description=\"The schedule for the deployment.\"",
            "    )",
            "    active: bool = Field(",
            "        default=True, description=\"Whether or not the schedule is active.\"",
            "    )",
            "",
            "",
            "class MinimalDeploymentSchedule(PrefectBaseModel):",
            "    schedule: SCHEDULE_TYPES = Field(",
            "        default=..., description=\"The schedule for the deployment.\"",
            "    )",
            "    active: bool = Field(",
            "        default=True, description=\"Whether or not the schedule is active.\"",
            "    )",
            "",
            "",
            "class Deployment(ObjectBaseModel):",
            "    \"\"\"An ORM representation of deployment data.\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The name of the deployment.\")",
            "    version: Optional[str] = Field(",
            "        default=None, description=\"An optional version for the deployment.\"",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A description for the deployment.\"",
            "    )",
            "    flow_id: UUID = Field(",
            "        default=..., description=\"The flow id associated with the deployment.\"",
            "    )",
            "    schedule: Optional[SCHEDULE_TYPES] = Field(",
            "        default=None, description=\"A schedule for the deployment.\"",
            "    )",
            "    is_schedule_active: bool = Field(",
            "        default=True, description=\"Whether or not the deployment schedule is active.\"",
            "    )",
            "    paused: bool = Field(",
            "        default=False, description=\"Whether or not the deployment is paused.\"",
            "    )",
            "    schedules: List[DeploymentSchedule] = Field(",
            "        default_factory=list, description=\"A list of schedules for the deployment.\"",
            "    )",
            "    infra_overrides: Dict[str, Any] = Field(",
            "        default_factory=dict,",
            "        description=\"Overrides to apply to the base infrastructure block at runtime.\",",
            "    )",
            "    parameters: Dict[str, Any] = Field(",
            "        default_factory=dict,",
            "        description=\"Parameters for flow runs scheduled by the deployment.\",",
            "    )",
            "    pull_steps: Optional[List[dict]] = Field(",
            "        default=None,",
            "        description=\"Pull steps for cloning and running this deployment.\",",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of tags for the deployment\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "    work_queue_name: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The work queue for the deployment. If no work queue is set, work will not\"",
            "            \" be scheduled.\"",
            "        ),",
            "    )",
            "    last_polled: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The last time the deployment was polled for status updates.\",",
            "    )",
            "    parameter_openapi_schema: Optional[Dict[str, Any]] = Field(",
            "        default=None,",
            "        description=\"The parameter schema of the flow, including defaults.\",",
            "    )",
            "    path: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The path to the working directory for the workflow, relative to remote\"",
            "            \" storage or an absolute path.\"",
            "        ),",
            "    )",
            "    entrypoint: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The path to the entrypoint for the workflow, relative to the `path`.\"",
            "        ),",
            "    )",
            "    manifest_path: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The path to the flow's manifest file, relative to the chosen storage.\"",
            "        ),",
            "    )",
            "    storage_document_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The block document defining storage used for this flow.\",",
            "    )",
            "    infrastructure_document_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The block document defining infrastructure to use for flow runs.\",",
            "    )",
            "    created_by: Optional[CreatedBy] = Field(",
            "        default=None,",
            "        description=\"Optional information about the creator of this deployment.\",",
            "    )",
            "    updated_by: Optional[UpdatedBy] = Field(",
            "        default=None,",
            "        description=\"Optional information about the updater of this deployment.\",",
            "    )",
            "    work_queue_id: UUID = Field(",
            "        default=None,",
            "        description=(",
            "            \"The id of the work pool queue to which this deployment is assigned.\"",
            "        ),",
            "    )",
            "    enforce_parameter_schema: bool = Field(",
            "        default=False,",
            "        description=(",
            "            \"Whether or not the deployment should enforce the parameter schema.\"",
            "        ),",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class ConcurrencyLimit(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a concurrency limit.\"\"\"",
            "",
            "    tag: str = Field(",
            "        default=..., description=\"A tag the concurrency limit is applied to.\"",
            "    )",
            "    concurrency_limit: int = Field(default=..., description=\"The concurrency limit.\")",
            "    active_slots: List[UUID] = Field(",
            "        default_factory=list,",
            "        description=\"A list of active run ids using a concurrency slot\",",
            "    )",
            "",
            "",
            "class BlockSchema(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block schema.\"\"\"",
            "",
            "    checksum: str = Field(default=..., description=\"The block schema's unique checksum\")",
            "    fields: dict = Field(",
            "        default_factory=dict, description=\"The block schema's field schema\"",
            "    )",
            "    block_type_id: Optional[UUID] = Field(default=..., description=\"A block type ID\")",
            "    block_type: Optional[BlockType] = Field(",
            "        default=None, description=\"The associated block type\"",
            "    )",
            "    capabilities: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of Block capabilities\",",
            "    )",
            "    version: str = Field(",
            "        default=DEFAULT_BLOCK_SCHEMA_VERSION,",
            "        description=\"Human readable identifier for the block schema\",",
            "    )",
            "",
            "",
            "class BlockSchemaReference(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block schema reference.\"\"\"",
            "",
            "    parent_block_schema_id: UUID = Field(",
            "        default=..., description=\"ID of block schema the reference is nested within\"",
            "    )",
            "    parent_block_schema: Optional[BlockSchema] = Field(",
            "        default=None, description=\"The block schema the reference is nested within\"",
            "    )",
            "    reference_block_schema_id: UUID = Field(",
            "        default=..., description=\"ID of the nested block schema\"",
            "    )",
            "    reference_block_schema: Optional[BlockSchema] = Field(",
            "        default=None, description=\"The nested block schema\"",
            "    )",
            "    name: str = Field(",
            "        default=..., description=\"The name that the reference is nested under\"",
            "    )",
            "",
            "",
            "class BlockDocumentReference(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block document reference.\"\"\"",
            "",
            "    parent_block_document_id: UUID = Field(",
            "        default=..., description=\"ID of block document the reference is nested within\"",
            "    )",
            "    parent_block_document: Optional[BlockDocument] = Field(",
            "        default=None, description=\"The block document the reference is nested within\"",
            "    )",
            "    reference_block_document_id: UUID = Field(",
            "        default=..., description=\"ID of the nested block document\"",
            "    )",
            "    reference_block_document: Optional[BlockDocument] = Field(",
            "        default=None, description=\"The nested block document\"",
            "    )",
            "    name: str = Field(",
            "        default=..., description=\"The name that the reference is nested under\"",
            "    )",
            "",
            "    @root_validator",
            "    def validate_parent_and_ref_are_different(cls, values):",
            "        parent_id = values.get(\"parent_block_document_id\")",
            "        ref_id = values.get(\"reference_block_document_id\")",
            "        if parent_id and ref_id and parent_id == ref_id:",
            "            raise ValueError(",
            "                \"`parent_block_document_id` and `reference_block_document_id` cannot be\"",
            "                \" the same\"",
            "            )",
            "        return values",
            "",
            "",
            "class Configuration(ObjectBaseModel):",
            "    \"\"\"An ORM representation of account info.\"\"\"",
            "",
            "    key: str = Field(default=..., description=\"Account info key\")",
            "    value: dict = Field(default=..., description=\"Account info\")",
            "",
            "",
            "class SavedSearchFilter(PrefectBaseModel):",
            "    \"\"\"A filter for a saved search model. Intended for use by the Prefect UI.\"\"\"",
            "",
            "    object: str = Field(default=..., description=\"The object over which to filter.\")",
            "    property: str = Field(",
            "        default=..., description=\"The property of the object on which to filter.\"",
            "    )",
            "    type: str = Field(default=..., description=\"The type of the property.\")",
            "    operation: str = Field(",
            "        default=...,",
            "        description=\"The operator to apply to the object. For example, `equals`.\",",
            "    )",
            "    value: Any = Field(",
            "        default=..., description=\"A JSON-compatible value for the filter.\"",
            "    )",
            "",
            "",
            "class SavedSearch(ObjectBaseModel):",
            "    \"\"\"An ORM representation of saved search data. Represents a set of filter criteria.\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The name of the saved search.\")",
            "    filters: List[SavedSearchFilter] = Field(",
            "        default_factory=list, description=\"The filter set for the saved search.\"",
            "    )",
            "",
            "",
            "class Log(ObjectBaseModel):",
            "    \"\"\"An ORM representation of log data.\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The logger name.\")",
            "    level: int = Field(default=..., description=\"The log level.\")",
            "    message: str = Field(default=..., description=\"The log message.\")",
            "    timestamp: DateTimeTZ = Field(default=..., description=\"The log timestamp.\")",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run ID associated with the log.\"",
            "    )",
            "    task_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The task run ID associated with the log.\"",
            "    )",
            "",
            "",
            "class QueueFilter(PrefectBaseModel):",
            "    \"\"\"Filter criteria definition for a work queue.\"\"\"",
            "",
            "    tags: Optional[List[str]] = Field(",
            "        default=None,",
            "        description=\"Only include flow runs with these tags in the work queue.\",",
            "    )",
            "    deployment_ids: Optional[List[UUID]] = Field(",
            "        default=None,",
            "        description=\"Only include flow runs from these deployments in the work queue.\",",
            "    )",
            "",
            "",
            "class WorkQueue(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a work queue\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The name of the work queue.\")",
            "    description: Optional[str] = Field(",
            "        default=\"\", description=\"An optional description for the work queue.\"",
            "    )",
            "    is_paused: bool = Field(",
            "        default=False, description=\"Whether or not the work queue is paused.\"",
            "    )",
            "    concurrency_limit: Optional[conint(ge=0)] = Field(",
            "        default=None, description=\"An optional concurrency limit for the work queue.\"",
            "    )",
            "    priority: conint(ge=1) = Field(",
            "        default=1,",
            "        description=(",
            "            \"The queue's priority. Lower values are higher priority (1 is the highest).\"",
            "        ),",
            "    )",
            "    work_pool_name: Optional[str] = Field(default=None)",
            "    # Will be required after a future migration",
            "    work_pool_id: Optional[UUID] = Field(",
            "        description=\"The work pool with which the queue is associated.\"",
            "    )",
            "    filter: Optional[QueueFilter] = Field(",
            "        default=None,",
            "        description=\"DEPRECATED: Filter criteria for the work queue.\",",
            "        deprecated=True,",
            "    )",
            "    last_polled: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The last time an agent polled this queue for work.\"",
            "    )",
            "    status: Optional[WorkQueueStatus] = Field(",
            "        default=None, description=\"The queue status.\"",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class WorkQueueHealthPolicy(PrefectBaseModel):",
            "    maximum_late_runs: Optional[int] = Field(",
            "        default=0,",
            "        description=(",
            "            \"The maximum number of late runs in the work queue before it is deemed\"",
            "            \" unhealthy. Defaults to `0`.\"",
            "        ),",
            "    )",
            "    maximum_seconds_since_last_polled: Optional[int] = Field(",
            "        default=60,",
            "        description=(",
            "            \"The maximum number of time in seconds elapsed since work queue has been\"",
            "            \" polled before it is deemed unhealthy. Defaults to `60`.\"",
            "        ),",
            "    )",
            "",
            "    def evaluate_health_status(",
            "        self, late_runs_count: int, last_polled: Optional[DateTimeTZ] = None",
            "    ) -> bool:",
            "        \"\"\"",
            "        Given empirical information about the state of the work queue, evaluate its health status.",
            "",
            "        Args:",
            "            late_runs: the count of late runs for the work queue.",
            "            last_polled: the last time the work queue was polled, if available.",
            "",
            "        Returns:",
            "            bool: whether or not the work queue is healthy.",
            "        \"\"\"",
            "        healthy = True",
            "        if (",
            "            self.maximum_late_runs is not None",
            "            and late_runs_count > self.maximum_late_runs",
            "        ):",
            "            healthy = False",
            "",
            "        if self.maximum_seconds_since_last_polled is not None:",
            "            if (",
            "                last_polled is None",
            "                or pendulum.now(\"UTC\").diff(last_polled).in_seconds()",
            "                > self.maximum_seconds_since_last_polled",
            "            ):",
            "                healthy = False",
            "",
            "        return healthy",
            "",
            "",
            "class WorkQueueStatusDetail(PrefectBaseModel):",
            "    healthy: bool = Field(..., description=\"Whether or not the work queue is healthy.\")",
            "    late_runs_count: int = Field(",
            "        default=0, description=\"The number of late flow runs in the work queue.\"",
            "    )",
            "    last_polled: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The last time an agent polled this queue for work.\"",
            "    )",
            "    health_check_policy: WorkQueueHealthPolicy = Field(",
            "        ...,",
            "        description=(",
            "            \"The policy used to determine whether or not the work queue is healthy.\"",
            "        ),",
            "    )",
            "",
            "",
            "class FlowRunNotificationPolicy(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a flow run notification.\"\"\"",
            "",
            "    is_active: bool = Field(",
            "        default=True, description=\"Whether the policy is currently active\"",
            "    )",
            "    state_names: List[str] = Field(",
            "        default=..., description=\"The flow run states that trigger notifications\"",
            "    )",
            "    tags: List[str] = Field(",
            "        default=...,",
            "        description=\"The flow run tags that trigger notifications (set [] to disable)\",",
            "    )",
            "    block_document_id: UUID = Field(",
            "        default=..., description=\"The block document ID used for sending notifications\"",
            "    )",
            "    message_template: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"A templatable notification message. Use {braces} to add variables.\"",
            "            \" Valid variables include:\"",
            "            f\" {listrepr(sorted(FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS), sep=', ')}\"",
            "        ),",
            "        example=(",
            "            \"Flow run {flow_run_name} with id {flow_run_id} entered state\"",
            "            \" {flow_run_state_name}.\"",
            "        ),",
            "    )",
            "",
            "    @validator(\"message_template\")",
            "    def validate_message_template_variables(cls, v):",
            "        if v is not None:",
            "            try:",
            "                v.format(**{k: \"test\" for k in FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS})",
            "            except KeyError as exc:",
            "                raise ValueError(f\"Invalid template variable provided: '{exc.args[0]}'\")",
            "        return v",
            "",
            "",
            "class Agent(ObjectBaseModel):",
            "    \"\"\"An ORM representation of an agent\"\"\"",
            "",
            "    name: str = Field(",
            "        default_factory=lambda: generate_slug(2),",
            "        description=(",
            "            \"The name of the agent. If a name is not provided, it will be\"",
            "            \" auto-generated.\"",
            "        ),",
            "    )",
            "    work_queue_id: UUID = Field(",
            "        default=..., description=\"The work queue with which the agent is associated.\"",
            "    )",
            "    last_activity_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The last time this agent polled for work.\"",
            "    )",
            "",
            "",
            "class WorkPool(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a work pool\"\"\"",
            "",
            "    name: str = Field(",
            "        description=\"The name of the work pool.\",",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A description of the work pool.\"",
            "    )",
            "    type: str = Field(description=\"The work pool type.\")",
            "    base_job_template: Dict[str, Any] = Field(",
            "        default_factory=dict, description=\"The work pool's base job template.\"",
            "    )",
            "    is_paused: bool = Field(",
            "        default=False,",
            "        description=\"Pausing the work pool stops the delivery of all work.\",",
            "    )",
            "    concurrency_limit: Optional[conint(ge=0)] = Field(",
            "        default=None, description=\"A concurrency limit for the work pool.\"",
            "    )",
            "    status: Optional[WorkPoolStatus] = Field(",
            "        default=None, description=\"The current status of the work pool.\"",
            "    )",
            "",
            "    # this required field has a default of None so that the custom validator",
            "    # below will be called and produce a more helpful error message",
            "    default_queue_id: UUID = Field(",
            "        None, description=\"The id of the pool's default queue.\"",
            "    )",
            "",
            "    @property",
            "    def is_push_pool(self) -> bool:",
            "        return self.type.endswith(\":push\")",
            "",
            "    @property",
            "    def is_managed_pool(self) -> bool:",
            "        return self.type.endswith(\":managed\")",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "    @validator(\"default_queue_id\", always=True)",
            "    def helpful_error_for_missing_default_queue_id(cls, v):",
            "        \"\"\"",
            "        Default queue ID is required because all pools must have a default queue",
            "        ID, but it represents a circular foreign key relationship to a",
            "        WorkQueue (which can't be created until the work pool exists).",
            "        Therefore, while this field can *technically* be null, it shouldn't be.",
            "        This should only be an issue when creating new pools, as reading",
            "        existing ones will always have this field populated. This custom error",
            "        message will help users understand that they should use the",
            "        `actions.WorkPoolCreate` model in that case.",
            "        \"\"\"",
            "        if v is None:",
            "            raise ValueError(",
            "                \"`default_queue_id` is a required field. If you are \"",
            "                \"creating a new WorkPool and don't have a queue \"",
            "                \"ID yet, use the `actions.WorkPoolCreate` model instead.\"",
            "            )",
            "        return v",
            "",
            "",
            "class Worker(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a worker\"\"\"",
            "",
            "    name: str = Field(description=\"The name of the worker.\")",
            "    work_pool_id: UUID = Field(",
            "        description=\"The work pool with which the queue is associated.\"",
            "    )",
            "    last_heartbeat_time: datetime.datetime = Field(",
            "        None, description=\"The last time the worker process sent a heartbeat.\"",
            "    )",
            "    heartbeat_interval_seconds: Optional[int] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The number of seconds to expect between heartbeats sent by the worker.\"",
            "        ),",
            "    )",
            "    status: WorkerStatus = Field(",
            "        WorkerStatus.OFFLINE,",
            "        description=\"Current status of the worker.\",",
            "    )",
            "",
            "",
            "Flow.update_forward_refs()",
            "FlowRun.update_forward_refs()",
            "",
            "",
            "class Artifact(ObjectBaseModel):",
            "    key: Optional[str] = Field(",
            "        default=None, description=\"An optional unique reference key for this artifact.\"",
            "    )",
            "    type: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An identifier that describes the shape of the data field. e.g. 'result',\"",
            "            \" 'table', 'markdown'\"",
            "        ),",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A markdown-enabled description of the artifact.\"",
            "    )",
            "    # data will eventually be typed as `Optional[Union[Result, Any]]`",
            "    data: Optional[Union[Dict[str, Any], Any]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"Data associated with the artifact, e.g. a result.; structure depends on\"",
            "            \" the artifact type.\"",
            "        ),",
            "    )",
            "    metadata_: Optional[Dict[str, str]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"User-defined artifact metadata. Content must be string key and value\"",
            "            \" pairs.\"",
            "        ),",
            "    )",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run associated with the artifact.\"",
            "    )",
            "    task_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The task run associated with the artifact.\"",
            "    )",
            "",
            "    @validator(\"metadata_\")",
            "    def validate_metadata_length(cls, v):",
            "        max_metadata_length = 500",
            "        if not isinstance(v, dict):",
            "            return v",
            "        for key in v.keys():",
            "            if len(str(v[key])) > max_metadata_length:",
            "                v[key] = str(v[key])[:max_metadata_length] + \"...\"",
            "        return v",
            "",
            "",
            "class ArtifactCollection(ObjectBaseModel):",
            "    key: str = Field(description=\"An optional unique reference key for this artifact.\")",
            "    latest_id: UUID = Field(",
            "        description=\"The latest artifact ID associated with the key.\"",
            "    )",
            "    type: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An identifier that describes the shape of the data field. e.g. 'result',\"",
            "            \" 'table', 'markdown'\"",
            "        ),",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A markdown-enabled description of the artifact.\"",
            "    )",
            "    data: Optional[Union[Dict[str, Any], Any]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"Data associated with the artifact, e.g. a result.; structure depends on\"",
            "            \" the artifact type.\"",
            "        ),",
            "    )",
            "    metadata_: Optional[Dict[str, str]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"User-defined artifact metadata. Content must be string key and value\"",
            "            \" pairs.\"",
            "        ),",
            "    )",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run associated with the artifact.\"",
            "    )",
            "    task_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The task run associated with the artifact.\"",
            "    )",
            "",
            "",
            "class Variable(ObjectBaseModel):",
            "    name: str = Field(",
            "        default=...,",
            "        description=\"The name of the variable\",",
            "        example=\"my_variable\",",
            "        max_length=MAX_VARIABLE_NAME_LENGTH,",
            "    )",
            "    value: str = Field(",
            "        default=...,",
            "        description=\"The value of the variable\",",
            "        example=\"my-value\",",
            "        max_length=MAX_VARIABLE_VALUE_LENGTH,",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of variable tags\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "",
            "",
            "class FlowRunInput(ObjectBaseModel):",
            "    flow_run_id: UUID = Field(description=\"The flow run ID associated with the input.\")",
            "    key: str = Field(description=\"The key of the input.\")",
            "    value: str = Field(description=\"The value of the input.\")",
            "    sender: Optional[str] = Field(description=\"The sender of the input.\")",
            "",
            "    @property",
            "    def decoded_value(self) -> Any:",
            "        \"\"\"",
            "        Decode the value of the input.",
            "",
            "        Returns:",
            "            Any: the decoded value",
            "        \"\"\"",
            "        return orjson.loads(self.value)",
            "",
            "    @validator(\"key\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_alphanumeric_dashes_only(v)",
            "        return v",
            "",
            "",
            "class GlobalConcurrencyLimit(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a global concurrency limit\"\"\"",
            "",
            "    name: str = Field(description=\"The name of the global concurrency limit.\")",
            "    limit: int = Field(",
            "        description=(",
            "            \"The maximum number of slots that can be occupied on this concurrency\"",
            "            \" limit.\"",
            "        )",
            "    )",
            "    active: Optional[bool] = Field(",
            "        default=True,",
            "        description=\"Whether or not the concurrency limit is in an active state.\",",
            "    )",
            "    active_slots: Optional[int] = Field(",
            "        default=0,",
            "        description=\"Number of tasks currently using a concurrency slot.\",",
            "    )",
            "    slot_decay_per_second: Optional[int] = Field(",
            "        default=0,",
            "        description=(",
            "            \"Controls the rate at which slots are released when the concurrency limit\"",
            "            \" is used as a rate limit.\"",
            "        ),",
            "    )"
        ],
        "afterPatchFile": [
            "import datetime",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Dict,",
            "    Generic,",
            "    List,",
            "    Optional,",
            "    TypeVar,",
            "    Union,",
            "    overload,",
            ")",
            "from uuid import UUID",
            "",
            "import orjson",
            "import pendulum",
            "",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "",
            "if HAS_PYDANTIC_V2:",
            "    from pydantic.v1 import Field, HttpUrl, conint, root_validator, validator",
            "else:",
            "    from pydantic import Field, HttpUrl, conint, root_validator, validator",
            "",
            "from typing_extensions import Literal",
            "",
            "from prefect._internal.schemas.bases import ObjectBaseModel, PrefectBaseModel",
            "from prefect._internal.schemas.fields import CreatedBy, DateTimeTZ, UpdatedBy",
            "from prefect._internal.schemas.validators import (",
            "    raise_on_name_alphanumeric_dashes_only,",
            "    raise_on_name_with_banned_characters,",
            ")",
            "from prefect.client.schemas.schedules import SCHEDULE_TYPES",
            "from prefect.settings import PREFECT_CLOUD_API_URL, PREFECT_CLOUD_UI_URL",
            "from prefect.utilities.collections import AutoEnum, listrepr",
            "from prefect.utilities.names import generate_slug",
            "",
            "if TYPE_CHECKING:",
            "    from prefect.deprecated.data_documents import DataDocument",
            "    from prefect.results import BaseResult",
            "",
            "R = TypeVar(\"R\")",
            "",
            "",
            "DEFAULT_BLOCK_SCHEMA_VERSION = \"non-versioned\"",
            "DEFAULT_AGENT_WORK_POOL_NAME = \"default-agent-pool\"",
            "FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS = [",
            "    \"flow_run_notification_policy_id\",",
            "    \"flow_id\",",
            "    \"flow_name\",",
            "    \"flow_run_url\",",
            "    \"flow_run_id\",",
            "    \"flow_run_name\",",
            "    \"flow_run_parameters\",",
            "    \"flow_run_state_type\",",
            "    \"flow_run_state_name\",",
            "    \"flow_run_state_timestamp\",",
            "    \"flow_run_state_message\",",
            "]",
            "MAX_VARIABLE_NAME_LENGTH = 255",
            "MAX_VARIABLE_VALUE_LENGTH = 5000",
            "",
            "",
            "class StateType(AutoEnum):",
            "    \"\"\"Enumeration of state types.\"\"\"",
            "",
            "    SCHEDULED = AutoEnum.auto()",
            "    PENDING = AutoEnum.auto()",
            "    RUNNING = AutoEnum.auto()",
            "    COMPLETED = AutoEnum.auto()",
            "    FAILED = AutoEnum.auto()",
            "    CANCELLED = AutoEnum.auto()",
            "    CRASHED = AutoEnum.auto()",
            "    PAUSED = AutoEnum.auto()",
            "    CANCELLING = AutoEnum.auto()",
            "",
            "",
            "class WorkPoolStatus(AutoEnum):",
            "    \"\"\"Enumeration of work pool statuses.\"\"\"",
            "",
            "    READY = AutoEnum.auto()",
            "    NOT_READY = AutoEnum.auto()",
            "    PAUSED = AutoEnum.auto()",
            "",
            "",
            "class WorkerStatus(AutoEnum):",
            "    \"\"\"Enumeration of worker statuses.\"\"\"",
            "",
            "    ONLINE = AutoEnum.auto()",
            "    OFFLINE = AutoEnum.auto()",
            "",
            "",
            "class DeploymentStatus(AutoEnum):",
            "    \"\"\"Enumeration of deployment statuses.\"\"\"",
            "",
            "    READY = AutoEnum.auto()",
            "    NOT_READY = AutoEnum.auto()",
            "",
            "",
            "class WorkQueueStatus(AutoEnum):",
            "    \"\"\"Enumeration of work queue statuses.\"\"\"",
            "",
            "    READY = AutoEnum.auto()",
            "    NOT_READY = AutoEnum.auto()",
            "    PAUSED = AutoEnum.auto()",
            "",
            "",
            "class StateDetails(PrefectBaseModel):",
            "    flow_run_id: UUID = None",
            "    task_run_id: UUID = None",
            "    # for task runs that represent subflows, the subflow's run ID",
            "    child_flow_run_id: UUID = None",
            "    scheduled_time: DateTimeTZ = None",
            "    cache_key: str = None",
            "    cache_expiration: DateTimeTZ = None",
            "    untrackable_result: bool = False",
            "    pause_timeout: DateTimeTZ = None",
            "    pause_reschedule: bool = False",
            "    pause_key: str = None",
            "    run_input_keyset: Optional[Dict[str, str]] = None",
            "    refresh_cache: bool = None",
            "    retriable: bool = None",
            "    transition_id: Optional[UUID] = None",
            "    task_parameters_id: Optional[UUID] = None",
            "",
            "",
            "class State(ObjectBaseModel, Generic[R]):",
            "    \"\"\"",
            "    The state of a run.",
            "    \"\"\"",
            "",
            "    type: StateType",
            "    name: Optional[str] = Field(default=None)",
            "    timestamp: DateTimeTZ = Field(default_factory=lambda: pendulum.now(\"UTC\"))",
            "    message: Optional[str] = Field(default=None, example=\"Run started\")",
            "    state_details: StateDetails = Field(default_factory=StateDetails)",
            "    data: Union[\"BaseResult[R]\", \"DataDocument[R]\", Any] = Field(",
            "        default=None,",
            "    )",
            "",
            "    @overload",
            "    def result(self: \"State[R]\", raise_on_failure: bool = True) -> R:",
            "        ...",
            "",
            "    @overload",
            "    def result(self: \"State[R]\", raise_on_failure: bool = False) -> Union[R, Exception]:",
            "        ...",
            "",
            "    def result(self, raise_on_failure: bool = True, fetch: Optional[bool] = None):",
            "        \"\"\"",
            "        Retrieve the result attached to this state.",
            "",
            "        Args:",
            "            raise_on_failure: a boolean specifying whether to raise an exception",
            "                if the state is of type `FAILED` and the underlying data is an exception",
            "            fetch: a boolean specifying whether to resolve references to persisted",
            "                results into data. For synchronous users, this defaults to `True`.",
            "                For asynchronous users, this defaults to `False` for backwards",
            "                compatibility.",
            "",
            "        Raises:",
            "            TypeError: If the state is failed but the result is not an exception.",
            "",
            "        Returns:",
            "            The result of the run",
            "",
            "        Examples:",
            "            >>> from prefect import flow, task",
            "            >>> @task",
            "            >>> def my_task(x):",
            "            >>>     return x",
            "",
            "            Get the result from a task future in a flow",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     future = my_task(\"hello\")",
            "            >>>     state = future.wait()",
            "            >>>     result = state.result()",
            "            >>>     print(result)",
            "            >>> my_flow()",
            "            hello",
            "",
            "            Get the result from a flow state",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     return \"hello\"",
            "            >>> my_flow(return_state=True).result()",
            "            hello",
            "",
            "            Get the result from a failed state",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     raise ValueError(\"oh no!\")",
            "            >>> state = my_flow(return_state=True)  # Error is wrapped in FAILED state",
            "            >>> state.result()  # Raises `ValueError`",
            "",
            "            Get the result from a failed state without erroring",
            "",
            "            >>> @flow",
            "            >>> def my_flow():",
            "            >>>     raise ValueError(\"oh no!\")",
            "            >>> state = my_flow(return_state=True)",
            "            >>> result = state.result(raise_on_failure=False)",
            "            >>> print(result)",
            "            ValueError(\"oh no!\")",
            "",
            "",
            "            Get the result from a flow state in an async context",
            "",
            "            >>> @flow",
            "            >>> async def my_flow():",
            "            >>>     return \"hello\"",
            "            >>> state = await my_flow(return_state=True)",
            "            >>> await state.result()",
            "            hello",
            "        \"\"\"",
            "        from prefect.states import get_state_result",
            "",
            "        return get_state_result(self, raise_on_failure=raise_on_failure, fetch=fetch)",
            "",
            "    def to_state_create(self):",
            "        \"\"\"",
            "        Convert this state to a `StateCreate` type which can be used to set the state of",
            "        a run in the API.",
            "",
            "        This method will drop this state's `data` if it is not a result type. Only",
            "        results should be sent to the API. Other data is only available locally.",
            "        \"\"\"",
            "        from prefect.client.schemas.actions import StateCreate",
            "        from prefect.results import BaseResult",
            "",
            "        return StateCreate(",
            "            type=self.type,",
            "            name=self.name,",
            "            message=self.message,",
            "            data=self.data if isinstance(self.data, BaseResult) else None,",
            "            state_details=self.state_details,",
            "        )",
            "",
            "    @validator(\"name\", always=True)",
            "    def default_name_from_type(cls, v, *, values, **kwargs):",
            "        \"\"\"If a name is not provided, use the type\"\"\"",
            "",
            "        # if `type` is not in `values` it means the `type` didn't pass its own",
            "        # validation check and an error will be raised after this function is called",
            "        if v is None and values.get(\"type\"):",
            "            v = \" \".join([v.capitalize() for v in values.get(\"type\").value.split(\"_\")])",
            "        return v",
            "",
            "    @root_validator",
            "    def default_scheduled_start_time(cls, values):",
            "        \"\"\"",
            "        TODO: This should throw an error instead of setting a default but is out of",
            "              scope for https://github.com/PrefectHQ/orion/pull/174/ and can be rolled",
            "              into work refactoring state initialization",
            "        \"\"\"",
            "        if values.get(\"type\") == StateType.SCHEDULED:",
            "            state_details = values.setdefault(",
            "                \"state_details\", cls.__fields__[\"state_details\"].get_default()",
            "            )",
            "            if not state_details.scheduled_time:",
            "                state_details.scheduled_time = pendulum.now(\"utc\")",
            "        return values",
            "",
            "    def is_scheduled(self) -> bool:",
            "        return self.type == StateType.SCHEDULED",
            "",
            "    def is_pending(self) -> bool:",
            "        return self.type == StateType.PENDING",
            "",
            "    def is_running(self) -> bool:",
            "        return self.type == StateType.RUNNING",
            "",
            "    def is_completed(self) -> bool:",
            "        return self.type == StateType.COMPLETED",
            "",
            "    def is_failed(self) -> bool:",
            "        return self.type == StateType.FAILED",
            "",
            "    def is_crashed(self) -> bool:",
            "        return self.type == StateType.CRASHED",
            "",
            "    def is_cancelled(self) -> bool:",
            "        return self.type == StateType.CANCELLED",
            "",
            "    def is_cancelling(self) -> bool:",
            "        return self.type == StateType.CANCELLING",
            "",
            "    def is_final(self) -> bool:",
            "        return self.type in {",
            "            StateType.CANCELLED,",
            "            StateType.FAILED,",
            "            StateType.COMPLETED,",
            "            StateType.CRASHED,",
            "        }",
            "",
            "    def is_paused(self) -> bool:",
            "        return self.type == StateType.PAUSED",
            "",
            "    def copy(self, *, update: dict = None, reset_fields: bool = False, **kwargs):",
            "        \"\"\"",
            "        Copying API models should return an object that could be inserted into the",
            "        database again. The 'timestamp' is reset using the default factory.",
            "        \"\"\"",
            "        update = update or {}",
            "        update.setdefault(\"timestamp\", self.__fields__[\"timestamp\"].get_default())",
            "        return super().copy(reset_fields=reset_fields, update=update, **kwargs)",
            "",
            "    def __repr__(self) -> str:",
            "        \"\"\"",
            "        Generates a complete state representation appropriate for introspection",
            "        and debugging, including the result:",
            "",
            "        `MyCompletedState(message=\"my message\", type=COMPLETED, result=...)`",
            "        \"\"\"",
            "        from prefect.deprecated.data_documents import DataDocument",
            "",
            "        if isinstance(self.data, DataDocument):",
            "            result = self.data.decode()",
            "        else:",
            "            result = self.data",
            "",
            "        display = dict(",
            "            message=repr(self.message),",
            "            type=str(self.type.value),",
            "            result=repr(result),",
            "        )",
            "",
            "        return f\"{self.name}({', '.join(f'{k}={v}' for k, v in display.items())})\"",
            "",
            "    def __str__(self) -> str:",
            "        \"\"\"",
            "        Generates a simple state representation appropriate for logging:",
            "",
            "        `MyCompletedState(\"my message\", type=COMPLETED)`",
            "        \"\"\"",
            "",
            "        display = []",
            "",
            "        if self.message:",
            "            display.append(repr(self.message))",
            "",
            "        if self.type.value.lower() != self.name.lower():",
            "            display.append(f\"type={self.type.value}\")",
            "",
            "        return f\"{self.name}({', '.join(display)})\"",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(",
            "            (",
            "                getattr(self.state_details, \"flow_run_id\", None),",
            "                getattr(self.state_details, \"task_run_id\", None),",
            "                self.timestamp,",
            "                self.type,",
            "            )",
            "        )",
            "",
            "",
            "class FlowRunPolicy(PrefectBaseModel):",
            "    \"\"\"Defines of how a flow run should be orchestrated.\"\"\"",
            "",
            "    max_retries: int = Field(",
            "        default=0,",
            "        description=(",
            "            \"The maximum number of retries. Field is not used. Please use `retries`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retry_delay_seconds: float = Field(",
            "        default=0,",
            "        description=(",
            "            \"The delay between retries. Field is not used. Please use `retry_delay`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retries: Optional[int] = Field(default=None, description=\"The number of retries.\")",
            "    retry_delay: Optional[int] = Field(",
            "        default=None, description=\"The delay time between retries, in seconds.\"",
            "    )",
            "    pause_keys: Optional[set] = Field(",
            "        default_factory=set, description=\"Tracks pauses this run has observed.\"",
            "    )",
            "    resuming: Optional[bool] = Field(",
            "        default=False, description=\"Indicates if this run is resuming from a pause.\"",
            "    )",
            "",
            "    @root_validator",
            "    def populate_deprecated_fields(cls, values):",
            "        \"\"\"",
            "        If deprecated fields are provided, populate the corresponding new fields",
            "        to preserve orchestration behavior.",
            "        \"\"\"",
            "        if not values.get(\"retries\", None) and values.get(\"max_retries\", 0) != 0:",
            "            values[\"retries\"] = values[\"max_retries\"]",
            "        if (",
            "            not values.get(\"retry_delay\", None)",
            "            and values.get(\"retry_delay_seconds\", 0) != 0",
            "        ):",
            "            values[\"retry_delay\"] = values[\"retry_delay_seconds\"]",
            "        return values",
            "",
            "",
            "class FlowRun(ObjectBaseModel):",
            "    name: str = Field(",
            "        default_factory=lambda: generate_slug(2),",
            "        description=(",
            "            \"The name of the flow run. Defaults to a random slug if not specified.\"",
            "        ),",
            "        example=\"my-flow-run\",",
            "    )",
            "    flow_id: UUID = Field(default=..., description=\"The id of the flow being run.\")",
            "    state_id: Optional[UUID] = Field(",
            "        default=None, description=\"The id of the flow run's current state.\"",
            "    )",
            "    deployment_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The id of the deployment associated with this flow run, if available.\"",
            "        ),",
            "    )",
            "    work_queue_name: Optional[str] = Field(",
            "        default=None, description=\"The work queue that handled this flow run.\"",
            "    )",
            "    flow_version: Optional[str] = Field(",
            "        default=None,",
            "        description=\"The version of the flow executed in this flow run.\",",
            "        example=\"1.0\",",
            "    )",
            "    parameters: dict = Field(",
            "        default_factory=dict, description=\"Parameters for the flow run.\"",
            "    )",
            "    idempotency_key: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An optional idempotency key for the flow run. Used to ensure the same flow\"",
            "            \" run is not created multiple times.\"",
            "        ),",
            "    )",
            "    context: dict = Field(",
            "        default_factory=dict,",
            "        description=\"Additional context for the flow run.\",",
            "        example={\"my_var\": \"my_val\"},",
            "    )",
            "    empirical_policy: FlowRunPolicy = Field(",
            "        default_factory=FlowRunPolicy,",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of tags on the flow run\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "    parent_task_run_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=(",
            "            \"If the flow run is a subflow, the id of the 'dummy' task in the parent\"",
            "            \" flow used to track subflow state.\"",
            "        ),",
            "    )",
            "    run_count: int = Field(",
            "        default=0, description=\"The number of times the flow run was executed.\"",
            "    )",
            "    expected_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The flow run's expected start time.\",",
            "    )",
            "    next_scheduled_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The next time the flow run is scheduled to start.\",",
            "    )",
            "    start_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual start time.\"",
            "    )",
            "    end_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual end time.\"",
            "    )",
            "    total_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=(",
            "            \"Total run time. If the flow run was executed multiple times, the time of\"",
            "            \" each run will be summed.\"",
            "        ),",
            "    )",
            "    estimated_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"A real-time estimate of the total run time.\",",
            "    )",
            "    estimated_start_time_delta: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"The difference between actual and expected start time.\",",
            "    )",
            "    auto_scheduled: bool = Field(",
            "        default=False,",
            "        description=\"Whether or not the flow run was automatically scheduled.\",",
            "    )",
            "    infrastructure_document_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The block document defining infrastructure to use this flow run.\",",
            "    )",
            "    infrastructure_pid: Optional[str] = Field(",
            "        default=None,",
            "        description=\"The id of the flow run as returned by an infrastructure block.\",",
            "    )",
            "    created_by: Optional[CreatedBy] = Field(",
            "        default=None,",
            "        description=\"Optional information about the creator of this flow run.\",",
            "    )",
            "    work_queue_id: Optional[UUID] = Field(",
            "        default=None, description=\"The id of the run's work pool queue.\"",
            "    )",
            "",
            "    work_pool_id: Optional[UUID] = Field(",
            "        description=\"The work pool with which the queue is associated.\"",
            "    )",
            "    work_pool_name: Optional[str] = Field(",
            "        default=None,",
            "        description=\"The name of the flow run's work pool.\",",
            "        example=\"my-work-pool\",",
            "    )",
            "    state: Optional[State] = Field(",
            "        default=None,",
            "        description=\"The state of the flow run.\",",
            "        example=State(type=StateType.COMPLETED),",
            "    )",
            "    job_variables: Optional[dict] = Field(",
            "        default=None, description=\"Job variables for the flow run.\"",
            "    )",
            "",
            "    def __eq__(self, other: Any) -> bool:",
            "        \"\"\"",
            "        Check for \"equality\" to another flow run schema",
            "",
            "        Estimates times are rolling and will always change with repeated queries for",
            "        a flow run so we ignore them during equality checks.",
            "        \"\"\"",
            "        if isinstance(other, FlowRun):",
            "            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}",
            "            return self.dict(exclude=exclude_fields) == other.dict(",
            "                exclude=exclude_fields",
            "            )",
            "        return super().__eq__(other)",
            "",
            "    @validator(\"name\", pre=True)",
            "    def set_default_name(cls, name):",
            "        return name or generate_slug(2)",
            "",
            "    # These are server-side optimizations and should not be present on client models",
            "    # TODO: Deprecate these fields",
            "",
            "    state_type: Optional[StateType] = Field(",
            "        default=None, description=\"The type of the current flow run state.\"",
            "    )",
            "    state_name: Optional[str] = Field(",
            "        default=None, description=\"The name of the current flow run state.\"",
            "    )",
            "",
            "",
            "class TaskRunPolicy(PrefectBaseModel):",
            "    \"\"\"Defines of how a task run should retry.\"\"\"",
            "",
            "    max_retries: int = Field(",
            "        default=0,",
            "        description=(",
            "            \"The maximum number of retries. Field is not used. Please use `retries`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retry_delay_seconds: float = Field(",
            "        default=0,",
            "        description=(",
            "            \"The delay between retries. Field is not used. Please use `retry_delay`\"",
            "            \" instead.\"",
            "        ),",
            "        deprecated=True,",
            "    )",
            "    retries: Optional[int] = Field(default=None, description=\"The number of retries.\")",
            "    retry_delay: Union[None, int, List[int]] = Field(",
            "        default=None,",
            "        description=\"A delay time or list of delay times between retries, in seconds.\",",
            "    )",
            "    retry_jitter_factor: Optional[float] = Field(",
            "        default=None, description=\"Determines the amount a retry should jitter\"",
            "    )",
            "",
            "    @root_validator",
            "    def populate_deprecated_fields(cls, values):",
            "        \"\"\"",
            "        If deprecated fields are provided, populate the corresponding new fields",
            "        to preserve orchestration behavior.",
            "        \"\"\"",
            "        if not values.get(\"retries\", None) and values.get(\"max_retries\", 0) != 0:",
            "            values[\"retries\"] = values[\"max_retries\"]",
            "",
            "        if (",
            "            not values.get(\"retry_delay\", None)",
            "            and values.get(\"retry_delay_seconds\", 0) != 0",
            "        ):",
            "            values[\"retry_delay\"] = values[\"retry_delay_seconds\"]",
            "",
            "        return values",
            "",
            "    @validator(\"retry_delay\")",
            "    def validate_configured_retry_delays(cls, v):",
            "        if isinstance(v, list) and (len(v) > 50):",
            "            raise ValueError(\"Can not configure more than 50 retry delays per task.\")",
            "        return v",
            "",
            "    @validator(\"retry_jitter_factor\")",
            "    def validate_jitter_factor(cls, v):",
            "        if v is not None and v < 0:",
            "            raise ValueError(\"`retry_jitter_factor` must be >= 0.\")",
            "        return v",
            "",
            "",
            "class TaskRunInput(PrefectBaseModel):",
            "    \"\"\"",
            "    Base class for classes that represent inputs to task runs, which",
            "    could include, constants, parameters, or other task runs.",
            "    \"\"\"",
            "",
            "    # freeze TaskRunInputs to allow them to be placed in sets",
            "    class Config:",
            "        frozen = True",
            "",
            "    input_type: str",
            "",
            "",
            "class TaskRunResult(TaskRunInput):",
            "    \"\"\"Represents a task run result input to another task run.\"\"\"",
            "",
            "    input_type: Literal[\"task_run\"] = \"task_run\"",
            "    id: UUID",
            "",
            "",
            "class Parameter(TaskRunInput):",
            "    \"\"\"Represents a parameter input to a task run.\"\"\"",
            "",
            "    input_type: Literal[\"parameter\"] = \"parameter\"",
            "    name: str",
            "",
            "",
            "class Constant(TaskRunInput):",
            "    \"\"\"Represents constant input value to a task run.\"\"\"",
            "",
            "    input_type: Literal[\"constant\"] = \"constant\"",
            "    type: str",
            "",
            "",
            "class TaskRun(ObjectBaseModel):",
            "    name: str = Field(default_factory=lambda: generate_slug(2), example=\"my-task-run\")",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run id of the task run.\"",
            "    )",
            "    task_key: str = Field(",
            "        default=..., description=\"A unique identifier for the task being run.\"",
            "    )",
            "    dynamic_key: str = Field(",
            "        default=...,",
            "        description=(",
            "            \"A dynamic key used to differentiate between multiple runs of the same task\"",
            "            \" within the same flow run.\"",
            "        ),",
            "    )",
            "    cache_key: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An optional cache key. If a COMPLETED state associated with this cache key\"",
            "            \" is found, the cached COMPLETED state will be used instead of executing\"",
            "            \" the task run.\"",
            "        ),",
            "    )",
            "    cache_expiration: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"Specifies when the cached state should expire.\"",
            "    )",
            "    task_version: Optional[str] = Field(",
            "        default=None, description=\"The version of the task being run.\"",
            "    )",
            "    empirical_policy: TaskRunPolicy = Field(",
            "        default_factory=TaskRunPolicy,",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of tags for the task run.\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "    state_id: Optional[UUID] = Field(",
            "        default=None, description=\"The id of the current task run state.\"",
            "    )",
            "    task_inputs: Dict[str, List[Union[TaskRunResult, Parameter, Constant]]] = Field(",
            "        default_factory=dict,",
            "        description=(",
            "            \"Tracks the source of inputs to a task run. Used for internal bookkeeping.\"",
            "        ),",
            "    )",
            "    state_type: Optional[StateType] = Field(",
            "        default=None, description=\"The type of the current task run state.\"",
            "    )",
            "    state_name: Optional[str] = Field(",
            "        default=None, description=\"The name of the current task run state.\"",
            "    )",
            "    run_count: int = Field(",
            "        default=0, description=\"The number of times the task run has been executed.\"",
            "    )",
            "    flow_run_run_count: int = Field(",
            "        default=0,",
            "        description=(",
            "            \"If the parent flow has retried, this indicates the flow retry this run is\"",
            "            \" associated with.\"",
            "        ),",
            "    )",
            "    expected_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The task run's expected start time.\",",
            "    )",
            "",
            "    # the next scheduled start time will be populated",
            "    # whenever the run is in a scheduled state",
            "    next_scheduled_start_time: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The next time the task run is scheduled to start.\",",
            "    )",
            "    start_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual start time.\"",
            "    )",
            "    end_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The actual end time.\"",
            "    )",
            "    total_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=(",
            "            \"Total run time. If the task run was executed multiple times, the time of\"",
            "            \" each run will be summed.\"",
            "        ),",
            "    )",
            "    estimated_run_time: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"A real-time estimate of total run time.\",",
            "    )",
            "    estimated_start_time_delta: datetime.timedelta = Field(",
            "        default=datetime.timedelta(0),",
            "        description=\"The difference between actual and expected start time.\",",
            "    )",
            "",
            "    state: Optional[State] = Field(",
            "        default=None,",
            "        description=\"The state of the flow run.\",",
            "        example=State(type=StateType.COMPLETED),",
            "    )",
            "",
            "    @validator(\"name\", pre=True)",
            "    def set_default_name(cls, name):",
            "        return name or generate_slug(2)",
            "",
            "",
            "class Workspace(PrefectBaseModel):",
            "    \"\"\"",
            "    A Prefect Cloud workspace.",
            "",
            "    Expected payload for each workspace returned by the `me/workspaces` route.",
            "    \"\"\"",
            "",
            "    account_id: UUID = Field(..., description=\"The account id of the workspace.\")",
            "    account_name: str = Field(..., description=\"The account name.\")",
            "    account_handle: str = Field(..., description=\"The account's unique handle.\")",
            "    workspace_id: UUID = Field(..., description=\"The workspace id.\")",
            "    workspace_name: str = Field(..., description=\"The workspace name.\")",
            "    workspace_description: str = Field(..., description=\"Description of the workspace.\")",
            "    workspace_handle: str = Field(..., description=\"The workspace's unique handle.\")",
            "",
            "    class Config:",
            "        extra = \"ignore\"",
            "",
            "    @property",
            "    def handle(self) -> str:",
            "        \"\"\"",
            "        The full handle of the workspace as `account_handle` / `workspace_handle`",
            "        \"\"\"",
            "        return self.account_handle + \"/\" + self.workspace_handle",
            "",
            "    def api_url(self) -> str:",
            "        \"\"\"",
            "        Generate the API URL for accessing this workspace",
            "        \"\"\"",
            "        return (",
            "            f\"{PREFECT_CLOUD_API_URL.value()}\"",
            "            f\"/accounts/{self.account_id}\"",
            "            f\"/workspaces/{self.workspace_id}\"",
            "        )",
            "",
            "    def ui_url(self) -> str:",
            "        \"\"\"",
            "        Generate the UI URL for accessing this workspace",
            "        \"\"\"",
            "        return (",
            "            f\"{PREFECT_CLOUD_UI_URL.value()}\"",
            "            f\"/account/{self.account_id}\"",
            "            f\"/workspace/{self.workspace_id}\"",
            "        )",
            "",
            "    def __hash__(self):",
            "        return hash(self.handle)",
            "",
            "",
            "class BlockType(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block type\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"A block type's name\")",
            "    slug: str = Field(default=..., description=\"A block type's slug\")",
            "    logo_url: Optional[HttpUrl] = Field(",
            "        default=None, description=\"Web URL for the block type's logo\"",
            "    )",
            "    documentation_url: Optional[HttpUrl] = Field(",
            "        default=None, description=\"Web URL for the block type's documentation\"",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None,",
            "        description=\"A short blurb about the corresponding block's intended use\",",
            "    )",
            "    code_example: Optional[str] = Field(",
            "        default=None,",
            "        description=\"A code snippet demonstrating use of the corresponding block\",",
            "    )",
            "    is_protected: bool = Field(",
            "        default=False, description=\"Protected block types cannot be modified via API.\"",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class BlockSchema(ObjectBaseModel):",
            "    \"\"\"A representation of a block schema.\"\"\"",
            "",
            "    checksum: str = Field(default=..., description=\"The block schema's unique checksum\")",
            "    fields: dict = Field(",
            "        default_factory=dict, description=\"The block schema's field schema\"",
            "    )",
            "    block_type_id: Optional[UUID] = Field(default=..., description=\"A block type ID\")",
            "    block_type: Optional[BlockType] = Field(",
            "        default=None, description=\"The associated block type\"",
            "    )",
            "    capabilities: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of Block capabilities\",",
            "    )",
            "    version: str = Field(",
            "        default=DEFAULT_BLOCK_SCHEMA_VERSION,",
            "        description=\"Human readable identifier for the block schema\",",
            "    )",
            "",
            "",
            "class BlockDocument(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block document.\"\"\"",
            "",
            "    name: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The block document's name. Not required for anonymous block documents.\"",
            "        ),",
            "    )",
            "    data: dict = Field(default_factory=dict, description=\"The block document's data\")",
            "    block_schema_id: UUID = Field(default=..., description=\"A block schema ID\")",
            "    block_schema: Optional[BlockSchema] = Field(",
            "        default=None, description=\"The associated block schema\"",
            "    )",
            "    block_type_id: UUID = Field(default=..., description=\"A block type ID\")",
            "    block_type_name: Optional[str] = Field(None, description=\"A block type name\")",
            "    block_type: Optional[BlockType] = Field(",
            "        default=None, description=\"The associated block type\"",
            "    )",
            "    block_document_references: Dict[str, Dict[str, Any]] = Field(",
            "        default_factory=dict, description=\"Record of the block document's references\"",
            "    )",
            "    is_anonymous: bool = Field(",
            "        default=False,",
            "        description=(",
            "            \"Whether the block is anonymous (anonymous blocks are usually created by\"",
            "            \" Prefect automatically)\"",
            "        ),",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        # the BlockDocumentCreate subclass allows name=None",
            "        # and will inherit this validator",
            "        if v is not None:",
            "            raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "    @root_validator",
            "    def validate_name_is_present_if_not_anonymous(cls, values):",
            "        # anonymous blocks may have no name prior to actually being",
            "        # stored in the database",
            "        if not values.get(\"is_anonymous\") and not values.get(\"name\"):",
            "            raise ValueError(\"Names must be provided for block documents.\")",
            "        return values",
            "",
            "",
            "class Flow(ObjectBaseModel):",
            "    \"\"\"An ORM representation of flow data.\"\"\"",
            "",
            "    name: str = Field(",
            "        default=..., description=\"The name of the flow\", example=\"my-flow\"",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of flow tags\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class FlowRunnerSettings(PrefectBaseModel):",
            "    \"\"\"",
            "    An API schema for passing details about the flow runner.",
            "",
            "    This schema is agnostic to the types and configuration provided by clients",
            "    \"\"\"",
            "",
            "    type: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The type of the flow runner which can be used by the client for\"",
            "            \" dispatching.\"",
            "        ),",
            "    )",
            "    config: Optional[dict] = Field(",
            "        default=None, description=\"The configuration for the given flow runner type.\"",
            "    )",
            "",
            "    # The following is required for composite compatibility in the ORM",
            "",
            "    def __init__(self, type: str = None, config: dict = None, **kwargs) -> None:",
            "        # Pydantic does not support positional arguments so they must be converted to",
            "        # keyword arguments",
            "        super().__init__(type=type, config=config, **kwargs)",
            "",
            "    def __composite_values__(self):",
            "        return self.type, self.config",
            "",
            "",
            "class DeploymentSchedule(ObjectBaseModel):",
            "    deployment_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The deployment id associated with this schedule.\",",
            "    )",
            "    schedule: SCHEDULE_TYPES = Field(",
            "        default=..., description=\"The schedule for the deployment.\"",
            "    )",
            "    active: bool = Field(",
            "        default=True, description=\"Whether or not the schedule is active.\"",
            "    )",
            "",
            "",
            "class MinimalDeploymentSchedule(PrefectBaseModel):",
            "    schedule: SCHEDULE_TYPES = Field(",
            "        default=..., description=\"The schedule for the deployment.\"",
            "    )",
            "    active: bool = Field(",
            "        default=True, description=\"Whether or not the schedule is active.\"",
            "    )",
            "",
            "",
            "class Deployment(ObjectBaseModel):",
            "    \"\"\"An ORM representation of deployment data.\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The name of the deployment.\")",
            "    version: Optional[str] = Field(",
            "        default=None, description=\"An optional version for the deployment.\"",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A description for the deployment.\"",
            "    )",
            "    flow_id: UUID = Field(",
            "        default=..., description=\"The flow id associated with the deployment.\"",
            "    )",
            "    schedule: Optional[SCHEDULE_TYPES] = Field(",
            "        default=None, description=\"A schedule for the deployment.\"",
            "    )",
            "    is_schedule_active: bool = Field(",
            "        default=True, description=\"Whether or not the deployment schedule is active.\"",
            "    )",
            "    paused: bool = Field(",
            "        default=False, description=\"Whether or not the deployment is paused.\"",
            "    )",
            "    schedules: List[DeploymentSchedule] = Field(",
            "        default_factory=list, description=\"A list of schedules for the deployment.\"",
            "    )",
            "    infra_overrides: Dict[str, Any] = Field(",
            "        default_factory=dict,",
            "        description=\"Overrides to apply to the base infrastructure block at runtime.\",",
            "    )",
            "    parameters: Dict[str, Any] = Field(",
            "        default_factory=dict,",
            "        description=\"Parameters for flow runs scheduled by the deployment.\",",
            "    )",
            "    pull_steps: Optional[List[dict]] = Field(",
            "        default=None,",
            "        description=\"Pull steps for cloning and running this deployment.\",",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of tags for the deployment\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "    work_queue_name: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The work queue for the deployment. If no work queue is set, work will not\"",
            "            \" be scheduled.\"",
            "        ),",
            "    )",
            "    last_polled: Optional[DateTimeTZ] = Field(",
            "        default=None,",
            "        description=\"The last time the deployment was polled for status updates.\",",
            "    )",
            "    parameter_openapi_schema: Optional[Dict[str, Any]] = Field(",
            "        default=None,",
            "        description=\"The parameter schema of the flow, including defaults.\",",
            "    )",
            "    path: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The path to the working directory for the workflow, relative to remote\"",
            "            \" storage or an absolute path.\"",
            "        ),",
            "    )",
            "    entrypoint: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The path to the entrypoint for the workflow, relative to the `path`.\"",
            "        ),",
            "    )",
            "    manifest_path: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The path to the flow's manifest file, relative to the chosen storage.\"",
            "        ),",
            "    )",
            "    storage_document_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The block document defining storage used for this flow.\",",
            "    )",
            "    infrastructure_document_id: Optional[UUID] = Field(",
            "        default=None,",
            "        description=\"The block document defining infrastructure to use for flow runs.\",",
            "    )",
            "    created_by: Optional[CreatedBy] = Field(",
            "        default=None,",
            "        description=\"Optional information about the creator of this deployment.\",",
            "    )",
            "    updated_by: Optional[UpdatedBy] = Field(",
            "        default=None,",
            "        description=\"Optional information about the updater of this deployment.\",",
            "    )",
            "    work_queue_id: UUID = Field(",
            "        default=None,",
            "        description=(",
            "            \"The id of the work pool queue to which this deployment is assigned.\"",
            "        ),",
            "    )",
            "    enforce_parameter_schema: bool = Field(",
            "        default=False,",
            "        description=(",
            "            \"Whether or not the deployment should enforce the parameter schema.\"",
            "        ),",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class ConcurrencyLimit(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a concurrency limit.\"\"\"",
            "",
            "    tag: str = Field(",
            "        default=..., description=\"A tag the concurrency limit is applied to.\"",
            "    )",
            "    concurrency_limit: int = Field(default=..., description=\"The concurrency limit.\")",
            "    active_slots: List[UUID] = Field(",
            "        default_factory=list,",
            "        description=\"A list of active run ids using a concurrency slot\",",
            "    )",
            "",
            "",
            "class BlockSchema(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block schema.\"\"\"",
            "",
            "    checksum: str = Field(default=..., description=\"The block schema's unique checksum\")",
            "    fields: dict = Field(",
            "        default_factory=dict, description=\"The block schema's field schema\"",
            "    )",
            "    block_type_id: Optional[UUID] = Field(default=..., description=\"A block type ID\")",
            "    block_type: Optional[BlockType] = Field(",
            "        default=None, description=\"The associated block type\"",
            "    )",
            "    capabilities: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of Block capabilities\",",
            "    )",
            "    version: str = Field(",
            "        default=DEFAULT_BLOCK_SCHEMA_VERSION,",
            "        description=\"Human readable identifier for the block schema\",",
            "    )",
            "",
            "",
            "class BlockSchemaReference(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block schema reference.\"\"\"",
            "",
            "    parent_block_schema_id: UUID = Field(",
            "        default=..., description=\"ID of block schema the reference is nested within\"",
            "    )",
            "    parent_block_schema: Optional[BlockSchema] = Field(",
            "        default=None, description=\"The block schema the reference is nested within\"",
            "    )",
            "    reference_block_schema_id: UUID = Field(",
            "        default=..., description=\"ID of the nested block schema\"",
            "    )",
            "    reference_block_schema: Optional[BlockSchema] = Field(",
            "        default=None, description=\"The nested block schema\"",
            "    )",
            "    name: str = Field(",
            "        default=..., description=\"The name that the reference is nested under\"",
            "    )",
            "",
            "",
            "class BlockDocumentReference(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a block document reference.\"\"\"",
            "",
            "    parent_block_document_id: UUID = Field(",
            "        default=..., description=\"ID of block document the reference is nested within\"",
            "    )",
            "    parent_block_document: Optional[BlockDocument] = Field(",
            "        default=None, description=\"The block document the reference is nested within\"",
            "    )",
            "    reference_block_document_id: UUID = Field(",
            "        default=..., description=\"ID of the nested block document\"",
            "    )",
            "    reference_block_document: Optional[BlockDocument] = Field(",
            "        default=None, description=\"The nested block document\"",
            "    )",
            "    name: str = Field(",
            "        default=..., description=\"The name that the reference is nested under\"",
            "    )",
            "",
            "    @root_validator",
            "    def validate_parent_and_ref_are_different(cls, values):",
            "        parent_id = values.get(\"parent_block_document_id\")",
            "        ref_id = values.get(\"reference_block_document_id\")",
            "        if parent_id and ref_id and parent_id == ref_id:",
            "            raise ValueError(",
            "                \"`parent_block_document_id` and `reference_block_document_id` cannot be\"",
            "                \" the same\"",
            "            )",
            "        return values",
            "",
            "",
            "class Configuration(ObjectBaseModel):",
            "    \"\"\"An ORM representation of account info.\"\"\"",
            "",
            "    key: str = Field(default=..., description=\"Account info key\")",
            "    value: dict = Field(default=..., description=\"Account info\")",
            "",
            "",
            "class SavedSearchFilter(PrefectBaseModel):",
            "    \"\"\"A filter for a saved search model. Intended for use by the Prefect UI.\"\"\"",
            "",
            "    object: str = Field(default=..., description=\"The object over which to filter.\")",
            "    property: str = Field(",
            "        default=..., description=\"The property of the object on which to filter.\"",
            "    )",
            "    type: str = Field(default=..., description=\"The type of the property.\")",
            "    operation: str = Field(",
            "        default=...,",
            "        description=\"The operator to apply to the object. For example, `equals`.\",",
            "    )",
            "    value: Any = Field(",
            "        default=..., description=\"A JSON-compatible value for the filter.\"",
            "    )",
            "",
            "",
            "class SavedSearch(ObjectBaseModel):",
            "    \"\"\"An ORM representation of saved search data. Represents a set of filter criteria.\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The name of the saved search.\")",
            "    filters: List[SavedSearchFilter] = Field(",
            "        default_factory=list, description=\"The filter set for the saved search.\"",
            "    )",
            "",
            "",
            "class Log(ObjectBaseModel):",
            "    \"\"\"An ORM representation of log data.\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The logger name.\")",
            "    level: int = Field(default=..., description=\"The log level.\")",
            "    message: str = Field(default=..., description=\"The log message.\")",
            "    timestamp: DateTimeTZ = Field(default=..., description=\"The log timestamp.\")",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run ID associated with the log.\"",
            "    )",
            "    task_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The task run ID associated with the log.\"",
            "    )",
            "",
            "",
            "class QueueFilter(PrefectBaseModel):",
            "    \"\"\"Filter criteria definition for a work queue.\"\"\"",
            "",
            "    tags: Optional[List[str]] = Field(",
            "        default=None,",
            "        description=\"Only include flow runs with these tags in the work queue.\",",
            "    )",
            "    deployment_ids: Optional[List[UUID]] = Field(",
            "        default=None,",
            "        description=\"Only include flow runs from these deployments in the work queue.\",",
            "    )",
            "",
            "",
            "class WorkQueue(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a work queue\"\"\"",
            "",
            "    name: str = Field(default=..., description=\"The name of the work queue.\")",
            "    description: Optional[str] = Field(",
            "        default=\"\", description=\"An optional description for the work queue.\"",
            "    )",
            "    is_paused: bool = Field(",
            "        default=False, description=\"Whether or not the work queue is paused.\"",
            "    )",
            "    concurrency_limit: Optional[conint(ge=0)] = Field(",
            "        default=None, description=\"An optional concurrency limit for the work queue.\"",
            "    )",
            "    priority: conint(ge=1) = Field(",
            "        default=1,",
            "        description=(",
            "            \"The queue's priority. Lower values are higher priority (1 is the highest).\"",
            "        ),",
            "    )",
            "    work_pool_name: Optional[str] = Field(default=None)",
            "    # Will be required after a future migration",
            "    work_pool_id: Optional[UUID] = Field(",
            "        description=\"The work pool with which the queue is associated.\"",
            "    )",
            "    filter: Optional[QueueFilter] = Field(",
            "        default=None,",
            "        description=\"DEPRECATED: Filter criteria for the work queue.\",",
            "        deprecated=True,",
            "    )",
            "    last_polled: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The last time an agent polled this queue for work.\"",
            "    )",
            "    status: Optional[WorkQueueStatus] = Field(",
            "        default=None, description=\"The queue status.\"",
            "    )",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "",
            "class WorkQueueHealthPolicy(PrefectBaseModel):",
            "    maximum_late_runs: Optional[int] = Field(",
            "        default=0,",
            "        description=(",
            "            \"The maximum number of late runs in the work queue before it is deemed\"",
            "            \" unhealthy. Defaults to `0`.\"",
            "        ),",
            "    )",
            "    maximum_seconds_since_last_polled: Optional[int] = Field(",
            "        default=60,",
            "        description=(",
            "            \"The maximum number of time in seconds elapsed since work queue has been\"",
            "            \" polled before it is deemed unhealthy. Defaults to `60`.\"",
            "        ),",
            "    )",
            "",
            "    def evaluate_health_status(",
            "        self, late_runs_count: int, last_polled: Optional[DateTimeTZ] = None",
            "    ) -> bool:",
            "        \"\"\"",
            "        Given empirical information about the state of the work queue, evaluate its health status.",
            "",
            "        Args:",
            "            late_runs: the count of late runs for the work queue.",
            "            last_polled: the last time the work queue was polled, if available.",
            "",
            "        Returns:",
            "            bool: whether or not the work queue is healthy.",
            "        \"\"\"",
            "        healthy = True",
            "        if (",
            "            self.maximum_late_runs is not None",
            "            and late_runs_count > self.maximum_late_runs",
            "        ):",
            "            healthy = False",
            "",
            "        if self.maximum_seconds_since_last_polled is not None:",
            "            if (",
            "                last_polled is None",
            "                or pendulum.now(\"UTC\").diff(last_polled).in_seconds()",
            "                > self.maximum_seconds_since_last_polled",
            "            ):",
            "                healthy = False",
            "",
            "        return healthy",
            "",
            "",
            "class WorkQueueStatusDetail(PrefectBaseModel):",
            "    healthy: bool = Field(..., description=\"Whether or not the work queue is healthy.\")",
            "    late_runs_count: int = Field(",
            "        default=0, description=\"The number of late flow runs in the work queue.\"",
            "    )",
            "    last_polled: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The last time an agent polled this queue for work.\"",
            "    )",
            "    health_check_policy: WorkQueueHealthPolicy = Field(",
            "        ...,",
            "        description=(",
            "            \"The policy used to determine whether or not the work queue is healthy.\"",
            "        ),",
            "    )",
            "",
            "",
            "class FlowRunNotificationPolicy(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a flow run notification.\"\"\"",
            "",
            "    is_active: bool = Field(",
            "        default=True, description=\"Whether the policy is currently active\"",
            "    )",
            "    state_names: List[str] = Field(",
            "        default=..., description=\"The flow run states that trigger notifications\"",
            "    )",
            "    tags: List[str] = Field(",
            "        default=...,",
            "        description=\"The flow run tags that trigger notifications (set [] to disable)\",",
            "    )",
            "    block_document_id: UUID = Field(",
            "        default=..., description=\"The block document ID used for sending notifications\"",
            "    )",
            "    message_template: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"A templatable notification message. Use {braces} to add variables.\"",
            "            \" Valid variables include:\"",
            "            f\" {listrepr(sorted(FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS), sep=', ')}\"",
            "        ),",
            "        example=(",
            "            \"Flow run {flow_run_name} with id {flow_run_id} entered state\"",
            "            \" {flow_run_state_name}.\"",
            "        ),",
            "    )",
            "",
            "    @validator(\"message_template\")",
            "    def validate_message_template_variables(cls, v):",
            "        if v is not None:",
            "            try:",
            "                v.format(**{k: \"test\" for k in FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS})",
            "            except KeyError as exc:",
            "                raise ValueError(f\"Invalid template variable provided: '{exc.args[0]}'\")",
            "        return v",
            "",
            "",
            "class Agent(ObjectBaseModel):",
            "    \"\"\"An ORM representation of an agent\"\"\"",
            "",
            "    name: str = Field(",
            "        default_factory=lambda: generate_slug(2),",
            "        description=(",
            "            \"The name of the agent. If a name is not provided, it will be\"",
            "            \" auto-generated.\"",
            "        ),",
            "    )",
            "    work_queue_id: UUID = Field(",
            "        default=..., description=\"The work queue with which the agent is associated.\"",
            "    )",
            "    last_activity_time: Optional[DateTimeTZ] = Field(",
            "        default=None, description=\"The last time this agent polled for work.\"",
            "    )",
            "",
            "",
            "class WorkPool(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a work pool\"\"\"",
            "",
            "    name: str = Field(",
            "        description=\"The name of the work pool.\",",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A description of the work pool.\"",
            "    )",
            "    type: str = Field(description=\"The work pool type.\")",
            "    base_job_template: Dict[str, Any] = Field(",
            "        default_factory=dict, description=\"The work pool's base job template.\"",
            "    )",
            "    is_paused: bool = Field(",
            "        default=False,",
            "        description=\"Pausing the work pool stops the delivery of all work.\",",
            "    )",
            "    concurrency_limit: Optional[conint(ge=0)] = Field(",
            "        default=None, description=\"A concurrency limit for the work pool.\"",
            "    )",
            "    status: Optional[WorkPoolStatus] = Field(",
            "        default=None, description=\"The current status of the work pool.\"",
            "    )",
            "",
            "    # this required field has a default of None so that the custom validator",
            "    # below will be called and produce a more helpful error message",
            "    default_queue_id: UUID = Field(",
            "        None, description=\"The id of the pool's default queue.\"",
            "    )",
            "",
            "    @property",
            "    def is_push_pool(self) -> bool:",
            "        return self.type.endswith(\":push\")",
            "",
            "    @property",
            "    def is_managed_pool(self) -> bool:",
            "        return self.type.endswith(\":managed\")",
            "",
            "    @validator(\"name\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_with_banned_characters(v)",
            "        return v",
            "",
            "    @validator(\"default_queue_id\", always=True)",
            "    def helpful_error_for_missing_default_queue_id(cls, v):",
            "        \"\"\"",
            "        Default queue ID is required because all pools must have a default queue",
            "        ID, but it represents a circular foreign key relationship to a",
            "        WorkQueue (which can't be created until the work pool exists).",
            "        Therefore, while this field can *technically* be null, it shouldn't be.",
            "        This should only be an issue when creating new pools, as reading",
            "        existing ones will always have this field populated. This custom error",
            "        message will help users understand that they should use the",
            "        `actions.WorkPoolCreate` model in that case.",
            "        \"\"\"",
            "        if v is None:",
            "            raise ValueError(",
            "                \"`default_queue_id` is a required field. If you are \"",
            "                \"creating a new WorkPool and don't have a queue \"",
            "                \"ID yet, use the `actions.WorkPoolCreate` model instead.\"",
            "            )",
            "        return v",
            "",
            "",
            "class Worker(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a worker\"\"\"",
            "",
            "    name: str = Field(description=\"The name of the worker.\")",
            "    work_pool_id: UUID = Field(",
            "        description=\"The work pool with which the queue is associated.\"",
            "    )",
            "    last_heartbeat_time: datetime.datetime = Field(",
            "        None, description=\"The last time the worker process sent a heartbeat.\"",
            "    )",
            "    heartbeat_interval_seconds: Optional[int] = Field(",
            "        default=None,",
            "        description=(",
            "            \"The number of seconds to expect between heartbeats sent by the worker.\"",
            "        ),",
            "    )",
            "    status: WorkerStatus = Field(",
            "        WorkerStatus.OFFLINE,",
            "        description=\"Current status of the worker.\",",
            "    )",
            "",
            "",
            "Flow.update_forward_refs()",
            "FlowRun.update_forward_refs()",
            "",
            "",
            "class Artifact(ObjectBaseModel):",
            "    key: Optional[str] = Field(",
            "        default=None, description=\"An optional unique reference key for this artifact.\"",
            "    )",
            "    type: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An identifier that describes the shape of the data field. e.g. 'result',\"",
            "            \" 'table', 'markdown'\"",
            "        ),",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A markdown-enabled description of the artifact.\"",
            "    )",
            "    # data will eventually be typed as `Optional[Union[Result, Any]]`",
            "    data: Optional[Union[Dict[str, Any], Any]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"Data associated with the artifact, e.g. a result.; structure depends on\"",
            "            \" the artifact type.\"",
            "        ),",
            "    )",
            "    metadata_: Optional[Dict[str, str]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"User-defined artifact metadata. Content must be string key and value\"",
            "            \" pairs.\"",
            "        ),",
            "    )",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run associated with the artifact.\"",
            "    )",
            "    task_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The task run associated with the artifact.\"",
            "    )",
            "",
            "    @validator(\"metadata_\")",
            "    def validate_metadata_length(cls, v):",
            "        max_metadata_length = 500",
            "        if not isinstance(v, dict):",
            "            return v",
            "        for key in v.keys():",
            "            if len(str(v[key])) > max_metadata_length:",
            "                v[key] = str(v[key])[:max_metadata_length] + \"...\"",
            "        return v",
            "",
            "",
            "class ArtifactCollection(ObjectBaseModel):",
            "    key: str = Field(description=\"An optional unique reference key for this artifact.\")",
            "    latest_id: UUID = Field(",
            "        description=\"The latest artifact ID associated with the key.\"",
            "    )",
            "    type: Optional[str] = Field(",
            "        default=None,",
            "        description=(",
            "            \"An identifier that describes the shape of the data field. e.g. 'result',\"",
            "            \" 'table', 'markdown'\"",
            "        ),",
            "    )",
            "    description: Optional[str] = Field(",
            "        default=None, description=\"A markdown-enabled description of the artifact.\"",
            "    )",
            "    data: Optional[Union[Dict[str, Any], Any]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"Data associated with the artifact, e.g. a result.; structure depends on\"",
            "            \" the artifact type.\"",
            "        ),",
            "    )",
            "    metadata_: Optional[Dict[str, str]] = Field(",
            "        default=None,",
            "        description=(",
            "            \"User-defined artifact metadata. Content must be string key and value\"",
            "            \" pairs.\"",
            "        ),",
            "    )",
            "    flow_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The flow run associated with the artifact.\"",
            "    )",
            "    task_run_id: Optional[UUID] = Field(",
            "        default=None, description=\"The task run associated with the artifact.\"",
            "    )",
            "",
            "",
            "class Variable(ObjectBaseModel):",
            "    name: str = Field(",
            "        default=...,",
            "        description=\"The name of the variable\",",
            "        example=\"my_variable\",",
            "        max_length=MAX_VARIABLE_NAME_LENGTH,",
            "    )",
            "    value: str = Field(",
            "        default=...,",
            "        description=\"The value of the variable\",",
            "        example=\"my-value\",",
            "        max_length=MAX_VARIABLE_VALUE_LENGTH,",
            "    )",
            "    tags: List[str] = Field(",
            "        default_factory=list,",
            "        description=\"A list of variable tags\",",
            "        example=[\"tag-1\", \"tag-2\"],",
            "    )",
            "",
            "",
            "class FlowRunInput(ObjectBaseModel):",
            "    flow_run_id: UUID = Field(description=\"The flow run ID associated with the input.\")",
            "    key: str = Field(description=\"The key of the input.\")",
            "    value: str = Field(description=\"The value of the input.\")",
            "    sender: Optional[str] = Field(description=\"The sender of the input.\")",
            "",
            "    @property",
            "    def decoded_value(self) -> Any:",
            "        \"\"\"",
            "        Decode the value of the input.",
            "",
            "        Returns:",
            "            Any: the decoded value",
            "        \"\"\"",
            "        return orjson.loads(self.value)",
            "",
            "    @validator(\"key\", check_fields=False)",
            "    def validate_name_characters(cls, v):",
            "        raise_on_name_alphanumeric_dashes_only(v)",
            "        return v",
            "",
            "",
            "class GlobalConcurrencyLimit(ObjectBaseModel):",
            "    \"\"\"An ORM representation of a global concurrency limit\"\"\"",
            "",
            "    name: str = Field(description=\"The name of the global concurrency limit.\")",
            "    limit: int = Field(",
            "        description=(",
            "            \"The maximum number of slots that can be occupied on this concurrency\"",
            "            \" limit.\"",
            "        )",
            "    )",
            "    active: Optional[bool] = Field(",
            "        default=True,",
            "        description=\"Whether or not the concurrency limit is in an active state.\",",
            "    )",
            "    active_slots: Optional[int] = Field(",
            "        default=0,",
            "        description=\"Number of tasks currently using a concurrency slot.\",",
            "    )",
            "    slot_decay_per_second: Optional[int] = Field(",
            "        default=0,",
            "        description=(",
            "            \"Controls the rate at which slots are released when the concurrency limit\"",
            "            \" is used as a rate limit.\"",
            "        ),",
            "    )",
            "",
            "",
            "class CsrfToken(ObjectBaseModel):",
            "    token: str = Field(",
            "        default=...,",
            "        description=\"The CSRF token\",",
            "    )",
            "    client: str = Field(",
            "        default=..., description=\"The client id associated with the CSRF token\"",
            "    )",
            "    expiration: DateTimeTZ = Field(",
            "        default=..., description=\"The expiration time of the CSRF token\"",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "src/prefect/settings.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 657,
                "afterPatchRowNumber": 657,
                "PatchRowcode": " may result in unexpected behavior."
            },
            "1": {
                "beforePatchRowNumber": 658,
                "afterPatchRowNumber": 658,
                "PatchRowcode": " \"\"\""
            },
            "2": {
                "beforePatchRowNumber": 659,
                "afterPatchRowNumber": 659,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 660,
                "PatchRowcode": "+PREFECT_CLIENT_CSRF_SUPPORT_ENABLED = Setting(bool, default=True)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 661,
                "PatchRowcode": "+\"\"\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 662,
                "PatchRowcode": "+Determines if CSRF token handling is active in the Prefect client for API"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 663,
                "PatchRowcode": "+requests."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 664,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 665,
                "PatchRowcode": "+When enabled (`True`), the client automatically manages CSRF tokens by"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 666,
                "PatchRowcode": "+retrieving, storing, and including them in applicable state-changing requests"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 667,
                "PatchRowcode": "+(POST, PUT, PATCH, DELETE) to the API."
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 668,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 669,
                "PatchRowcode": "+Disabling this setting (`False`) means the client will not handle CSRF tokens,"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 670,
                "PatchRowcode": "+which might be suitable for environments where CSRF protection is disabled."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 671,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 672,
                "PatchRowcode": "+Defaults to `True`, ensuring CSRF protection is enabled by default."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 673,
                "PatchRowcode": "+\"\"\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 674,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": 660,
                "afterPatchRowNumber": 675,
                "PatchRowcode": " PREFECT_CLOUD_API_URL = Setting("
            },
            "19": {
                "beforePatchRowNumber": 661,
                "afterPatchRowNumber": 676,
                "PatchRowcode": "     str,"
            },
            "20": {
                "beforePatchRowNumber": 662,
                "afterPatchRowNumber": 677,
                "PatchRowcode": "     default=\"https://api.prefect.cloud/api\","
            },
            "21": {
                "beforePatchRowNumber": 1208,
                "afterPatchRowNumber": 1223,
                "PatchRowcode": " \"\"\""
            },
            "22": {
                "beforePatchRowNumber": 1209,
                "afterPatchRowNumber": 1224,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 1210,
                "afterPatchRowNumber": 1225,
                "PatchRowcode": " PREFECT_SERVER_CSRF_PROTECTION_ENABLED = Setting(bool, default=False)"
            },
            "24": {
                "beforePatchRowNumber": 1211,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-\"\"\"Whether or not to protect the API from CSRF attacks. Experimental and"
            },
            "25": {
                "beforePatchRowNumber": 1212,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-currently defaults to `False`.\"\"\""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1226,
                "PatchRowcode": "+\"\"\""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1227,
                "PatchRowcode": "+Controls the activation of CSRF protection for the Prefect server API."
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1228,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1229,
                "PatchRowcode": "+When enabled (`True`), the server enforces CSRF validation checks on incoming"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1230,
                "PatchRowcode": "+state-changing requests (POST, PUT, PATCH, DELETE), requiring a valid CSRF"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1231,
                "PatchRowcode": "+token to be included in the request headers or body. This adds a layer of"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1232,
                "PatchRowcode": "+security by preventing unauthorized or malicious sites from making requests on"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1233,
                "PatchRowcode": "+behalf of authenticated users."
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1234,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1235,
                "PatchRowcode": "+It is recommended to enable this setting in production environments where the"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1236,
                "PatchRowcode": "+API is exposed to web clients to safeguard against CSRF attacks."
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1237,
                "PatchRowcode": "+"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1238,
                "PatchRowcode": "+Note: Enabling this setting requires corresponding support in the client for"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1239,
                "PatchRowcode": "+CSRF token management. See PREFECT_CLIENT_CSRF_SUPPORT_ENABLED for more."
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1240,
                "PatchRowcode": "+\"\"\""
            },
            "41": {
                "beforePatchRowNumber": 1213,
                "afterPatchRowNumber": 1241,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 1214,
                "afterPatchRowNumber": 1242,
                "PatchRowcode": " PREFECT_SERVER_CSRF_TOKEN_EXPIRATION = Setting(timedelta, default=timedelta(hours=1))"
            },
            "43": {
                "beforePatchRowNumber": 1215,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-\"\"\"How long a CSRF token is valid for. Defaults to 1 hour.\"\"\""
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1243,
                "PatchRowcode": "+\"\"\""
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1244,
                "PatchRowcode": "+Specifies the duration for which a CSRF token remains valid after being issued"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1245,
                "PatchRowcode": "+by the server."
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1246,
                "PatchRowcode": "+"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1247,
                "PatchRowcode": "+The default expiration time is set to 1 hour, which offers a reasonable"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1248,
                "PatchRowcode": "+compromise. Adjust this setting based on your specific security requirements"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1249,
                "PatchRowcode": "+and usage patterns."
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1250,
                "PatchRowcode": "+\"\"\""
            },
            "52": {
                "beforePatchRowNumber": 1216,
                "afterPatchRowNumber": 1251,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 1217,
                "afterPatchRowNumber": 1252,
                "PatchRowcode": " PREFECT_UI_ENABLED = Setting("
            },
            "54": {
                "beforePatchRowNumber": 1218,
                "afterPatchRowNumber": 1253,
                "PatchRowcode": "     bool,"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Prefect settings management.",
            "",
            "Each setting is defined as a `Setting` type. The name of each setting is stylized in all",
            "caps, matching the environment variable that can be used to change the setting.",
            "",
            "All settings defined in this file are used to generate a dynamic Pydantic settings class",
            "called `Settings`. When instantiated, this class will load settings from environment",
            "variables and pull default values from the setting definitions.",
            "",
            "The current instance of `Settings` being used by the application is stored in a",
            "`SettingsContext` model which allows each instance of the `Settings` class to be",
            "accessed in an async-safe manner.",
            "",
            "Aside from environment variables, we allow settings to be changed during the runtime of",
            "the process using profiles. Profiles contain setting overrides that the user may",
            "persist without setting environment variables. Profiles are also used internally for",
            "managing settings during task run execution where differing settings may be used",
            "concurrently in the same process and during testing where we need to override settings",
            "to ensure their value is respected as intended.",
            "",
            "The `SettingsContext` is set when the `prefect` module is imported. This context is",
            "referred to as the \"root\" settings context for clarity. Generally, this is the only",
            "settings context that will be used. When this context is entered, we will instantiate",
            "a `Settings` object, loading settings from environment variables and defaults, then we",
            "will load the active profile and use it to override settings. See  `enter_root_settings_context`",
            "for details on determining the active profile.",
            "",
            "Another `SettingsContext` may be entered at any time to change the settings being",
            "used by the code within the context. Generally, users should not use this. Settings",
            "management should be left to Prefect application internals.",
            "",
            "Generally, settings should be accessed with `SETTING_VARIABLE.value()` which will",
            "pull the current `Settings` instance from the current `SettingsContext` and retrieve",
            "the value of the relevant setting.",
            "",
            "Accessing a setting's value will also call the `Setting.value_callback` which allows",
            "settings to be dynamically modified on retrieval. This allows us to make settings",
            "dependent on the value of other settings or perform other dynamic effects.",
            "",
            "\"\"\"",
            "",
            "import logging",
            "import os",
            "import string",
            "import warnings",
            "from contextlib import contextmanager",
            "from datetime import timedelta",
            "from pathlib import Path",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    Generic,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    TypeVar,",
            "    Union,",
            ")",
            "from urllib.parse import urlparse",
            "",
            "import toml",
            "",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "",
            "if HAS_PYDANTIC_V2:",
            "    from pydantic.v1 import (",
            "        BaseModel,",
            "        BaseSettings,",
            "        Field,",
            "        create_model,",
            "        fields,",
            "        root_validator,",
            "        validator,",
            "    )",
            "else:",
            "    from pydantic import (",
            "        BaseModel,",
            "        BaseSettings,",
            "        Field,",
            "        create_model,",
            "        fields,",
            "        root_validator,",
            "        validator,",
            "    )",
            "",
            "from typing_extensions import Literal",
            "",
            "from prefect._internal.compatibility.deprecated import generate_deprecation_message",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "from prefect.exceptions import MissingProfileError",
            "from prefect.utilities.names import OBFUSCATED_PREFIX, obfuscate",
            "from prefect.utilities.pydantic import add_cloudpickle_reduction",
            "",
            "T = TypeVar(\"T\")",
            "",
            "",
            "DEFAULT_PROFILES_PATH = Path(__file__).parent.joinpath(\"profiles.toml\")",
            "",
            "REMOVED_EXPERIMENTAL_FLAGS = {\"PREFECT_EXPERIMENTAL_ENABLE_ENHANCED_SCHEDULING_UI\"}",
            "",
            "",
            "class Setting(Generic[T]):",
            "    \"\"\"",
            "    Setting definition type.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        type: Type[T],",
            "        *,",
            "        deprecated: bool = False,",
            "        deprecated_start_date: Optional[str] = None,",
            "        deprecated_end_date: Optional[str] = None,",
            "        deprecated_help: str = \"\",",
            "        deprecated_when_message: str = \"\",",
            "        deprecated_when: Optional[Callable[[Any], bool]] = None,",
            "        deprecated_renamed_to: Optional[\"Setting[T]\"] = None,",
            "        value_callback: Optional[Callable[[\"Settings\", T], T]] = None,",
            "        is_secret: bool = False,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        self.field: fields.FieldInfo = Field(**kwargs)",
            "        self.type = type",
            "        self.value_callback = value_callback",
            "        self._name = None",
            "        self.is_secret = is_secret",
            "        self.deprecated = deprecated",
            "        self.deprecated_start_date = deprecated_start_date",
            "        self.deprecated_end_date = deprecated_end_date",
            "        self.deprecated_help = deprecated_help",
            "        self.deprecated_when = deprecated_when or (lambda _: True)",
            "        self.deprecated_when_message = deprecated_when_message",
            "        self.deprecated_renamed_to = deprecated_renamed_to",
            "        self.deprecated_renamed_from = None",
            "        self.__doc__ = self.field.description",
            "",
            "        # Validate the deprecation settings, will throw an error at setting definition",
            "        # time if the developer has not configured it correctly",
            "        if deprecated:",
            "            generate_deprecation_message(",
            "                name=\"...\",  # setting names not populated until after init",
            "                start_date=self.deprecated_start_date,",
            "                end_date=self.deprecated_end_date,",
            "                help=self.deprecated_help,",
            "                when=self.deprecated_when_message,",
            "            )",
            "",
            "        if deprecated_renamed_to is not None:",
            "            # Track the deprecation both ways",
            "            deprecated_renamed_to.deprecated_renamed_from = self",
            "",
            "    def value(self, bypass_callback: bool = False) -> T:",
            "        \"\"\"",
            "        Get the current value of a setting.",
            "",
            "        Example:",
            "        ```python",
            "        from prefect.settings import PREFECT_API_URL",
            "        PREFECT_API_URL.value()",
            "        ```",
            "        \"\"\"",
            "        return self.value_from(get_current_settings(), bypass_callback=bypass_callback)",
            "",
            "    def value_from(self, settings: \"Settings\", bypass_callback: bool = False) -> T:",
            "        \"\"\"",
            "        Get the value of a setting from a settings object",
            "",
            "        Example:",
            "        ```python",
            "        from prefect.settings import get_default_settings",
            "        PREFECT_API_URL.value_from(get_default_settings())",
            "        ```",
            "        \"\"\"",
            "        value = settings.value_of(self, bypass_callback=bypass_callback)",
            "",
            "        if not bypass_callback and self.deprecated and self.deprecated_when(value):",
            "            # Check if this setting is deprecated and someone is accessing the value",
            "            # via the old name",
            "            warnings.warn(self.deprecated_message, DeprecationWarning, stacklevel=3)",
            "",
            "            # If the the value is empty, return the new setting's value for compat",
            "            if value is None and self.deprecated_renamed_to is not None:",
            "                return self.deprecated_renamed_to.value_from(settings)",
            "",
            "        if not bypass_callback and self.deprecated_renamed_from is not None:",
            "            # Check if this setting is a rename of a deprecated setting and the",
            "            # deprecated setting is set and should be used for compatibility",
            "            deprecated_value = self.deprecated_renamed_from.value_from(",
            "                settings, bypass_callback=True",
            "            )",
            "            if deprecated_value is not None:",
            "                warnings.warn(",
            "                    (",
            "                        f\"{self.deprecated_renamed_from.deprecated_message} Because\"",
            "                        f\" {self.deprecated_renamed_from.name!r} is set it will be used\"",
            "                        f\" instead of {self.name!r} for backwards compatibility.\"",
            "                    ),",
            "                    DeprecationWarning,",
            "                    stacklevel=3,",
            "                )",
            "            return deprecated_value or value",
            "",
            "        return value",
            "",
            "    @property",
            "    def name(self):",
            "        if self._name:",
            "            return self._name",
            "",
            "        # Lookup the name on first access",
            "        for name, val in tuple(globals().items()):",
            "            if val == self:",
            "                self._name = name",
            "                return name",
            "",
            "        raise ValueError(\"Setting not found in `prefect.settings` module.\")",
            "",
            "    @name.setter",
            "    def name(self, value: str):",
            "        self._name = value",
            "",
            "    @property",
            "    def deprecated_message(self):",
            "        return generate_deprecation_message(",
            "            name=f\"Setting {self.name!r}\",",
            "            start_date=self.deprecated_start_date,",
            "            end_date=self.deprecated_end_date,",
            "            help=self.deprecated_help,",
            "            when=self.deprecated_when_message,",
            "        )",
            "",
            "    def __repr__(self) -> str:",
            "        return f\"<{self.name}: {self.type.__name__}>\"",
            "",
            "    def __bool__(self) -> bool:",
            "        \"\"\"",
            "        Returns a truthy check of the current value.",
            "        \"\"\"",
            "        return bool(self.value())",
            "",
            "    def __eq__(self, __o: object) -> bool:",
            "        return __o.__eq__(self.value())",
            "",
            "    def __hash__(self) -> int:",
            "        return hash((type(self), self.name))",
            "",
            "",
            "# Callbacks and validators",
            "",
            "",
            "def get_extra_loggers(_: \"Settings\", value: str) -> List[str]:",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_LOGGING_EXTRA_LOGGERS`that parses the CSV string into a",
            "    list and trims whitespace from logger names.",
            "    \"\"\"",
            "    return [name.strip() for name in value.split(\",\")] if value else []",
            "",
            "",
            "def expanduser_in_path(_, value: Path) -> Path:",
            "    return value.expanduser()",
            "",
            "",
            "def debug_mode_log_level(settings, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_LOGGING_LEVEL` that overrides the log level to DEBUG",
            "    when debug mode is enabled.",
            "    \"\"\"",
            "    if PREFECT_DEBUG_MODE.value_from(settings):",
            "        return \"DEBUG\"",
            "    else:",
            "        return value",
            "",
            "",
            "def only_return_value_in_test_mode(settings, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_TEST_SETTING` that only allows access during test mode",
            "    \"\"\"",
            "    if PREFECT_TEST_MODE.value_from(settings):",
            "        return value",
            "    else:",
            "        return None",
            "",
            "",
            "def default_ui_api_url(settings, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_UI_API_URL` that sets the default value to",
            "    relative path '/api', otherwise it constructs an API URL from the API settings.",
            "    \"\"\"",
            "    if value is None:",
            "        # Set a default value",
            "        value = \"/api\"",
            "",
            "    return template_with_settings(",
            "        PREFECT_SERVER_API_HOST, PREFECT_SERVER_API_PORT, PREFECT_API_URL",
            "    )(settings, value)",
            "",
            "",
            "def status_codes_as_integers_in_range(_, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_CLIENT_RETRY_EXTRA_CODES` that ensures status codes",
            "    are integers in the range 100-599.",
            "    \"\"\"",
            "    if value == \"\":",
            "        return set()",
            "",
            "    values = {v.strip() for v in value.split(\",\")}",
            "",
            "    if any(not v.isdigit() or int(v) < 100 or int(v) > 599 for v in values):",
            "        raise ValueError(",
            "            \"PREFECT_CLIENT_RETRY_EXTRA_CODES must be a comma separated list of \"",
            "            \"integers between 100 and 599.\"",
            "        )",
            "",
            "    values = {int(v) for v in values}",
            "    return values",
            "",
            "",
            "def template_with_settings(*upstream_settings: Setting) -> Callable[[\"Settings\", T], T]:",
            "    \"\"\"",
            "    Returns a `value_callback` that will template the given settings into the runtime",
            "    value for the setting.",
            "    \"\"\"",
            "",
            "    def templater(settings, value):",
            "        if value is None:",
            "            return value  # Do not attempt to template a null string",
            "",
            "        original_type = type(value)",
            "        template_values = {",
            "            setting.name: setting.value_from(settings) for setting in upstream_settings",
            "        }",
            "        template = string.Template(str(value))",
            "        return original_type(template.substitute(template_values))",
            "",
            "    return templater",
            "",
            "",
            "def max_log_size_smaller_than_batch_size(values):",
            "    \"\"\"",
            "    Validator for settings asserting the batch size and match log size are compatible",
            "    \"\"\"",
            "    if (",
            "        values[\"PREFECT_LOGGING_TO_API_BATCH_SIZE\"]",
            "        < values[\"PREFECT_LOGGING_TO_API_MAX_LOG_SIZE\"]",
            "    ):",
            "        raise ValueError(",
            "            \"`PREFECT_LOGGING_TO_API_MAX_LOG_SIZE` cannot be larger than\"",
            "            \" `PREFECT_LOGGING_TO_API_BATCH_SIZE`\"",
            "        )",
            "    return values",
            "",
            "",
            "def warn_on_database_password_value_without_usage(values):",
            "    \"\"\"",
            "    Validator for settings warning if the database password is set but not used.",
            "    \"\"\"",
            "    value = values[\"PREFECT_API_DATABASE_PASSWORD\"]",
            "    if (",
            "        value",
            "        and not value.startswith(OBFUSCATED_PREFIX)",
            "        and (",
            "            \"PREFECT_API_DATABASE_PASSWORD\"",
            "            not in values[\"PREFECT_API_DATABASE_CONNECTION_URL\"]",
            "        )",
            "    ):",
            "        warnings.warn(",
            "            \"PREFECT_API_DATABASE_PASSWORD is set but not included in the \"",
            "            \"PREFECT_API_DATABASE_CONNECTION_URL. \"",
            "            \"The provided password will be ignored.\"",
            "        )",
            "    return values",
            "",
            "",
            "def check_for_deprecated_cloud_url(settings, value):",
            "    deprecated_value = PREFECT_CLOUD_URL.value_from(settings, bypass_callback=True)",
            "    if deprecated_value is not None:",
            "        warnings.warn(",
            "            (",
            "                \"`PREFECT_CLOUD_URL` is set and will be used instead of\"",
            "                \" `PREFECT_CLOUD_API_URL` for backwards compatibility.\"",
            "                \" `PREFECT_CLOUD_URL` is deprecated, set `PREFECT_CLOUD_API_URL`\"",
            "                \" instead.\"",
            "            ),",
            "            DeprecationWarning,",
            "        )",
            "    return deprecated_value or value",
            "",
            "",
            "def warn_on_misconfigured_api_url(values):",
            "    \"\"\"",
            "    Validator for settings warning if the API URL is misconfigured.",
            "    \"\"\"",
            "    api_url = values[\"PREFECT_API_URL\"]",
            "    if api_url is not None:",
            "        misconfigured_mappings = {",
            "            \"app.prefect.cloud\": (",
            "                \"`PREFECT_API_URL` points to `app.prefect.cloud`. Did you\"",
            "                \" mean `api.prefect.cloud`?\"",
            "            ),",
            "            \"account/\": (",
            "                \"`PREFECT_API_URL` uses `/account/` but should use `/accounts/`.\"",
            "            ),",
            "            \"workspace/\": (",
            "                \"`PREFECT_API_URL` uses `/workspace/` but should use `/workspaces/`.\"",
            "            ),",
            "        }",
            "        warnings_list = []",
            "",
            "        for misconfig, warning in misconfigured_mappings.items():",
            "            if misconfig in api_url:",
            "                warnings_list.append(warning)",
            "",
            "        parsed_url = urlparse(api_url)",
            "        if parsed_url.path and not parsed_url.path.startswith(\"/api\"):",
            "            warnings_list.append(",
            "                \"`PREFECT_API_URL` should have `/api` after the base URL.\"",
            "            )",
            "",
            "        if warnings_list:",
            "            example = 'e.g. PREFECT_API_URL=\"https://api.prefect.cloud/api/accounts/[ACCOUNT-ID]/workspaces/[WORKSPACE-ID]\"'",
            "            warnings_list.append(example)",
            "",
            "            warnings.warn(\"\\n\".join(warnings_list), stacklevel=2)",
            "",
            "    return values",
            "",
            "",
            "def default_database_connection_url(settings, value):",
            "    templater = template_with_settings(PREFECT_HOME, PREFECT_API_DATABASE_PASSWORD)",
            "",
            "    # If the user has provided a value, use it",
            "    if value is not None:",
            "        return templater(settings, value)",
            "",
            "    # Otherwise, the default is a database in a local file",
            "    home = PREFECT_HOME.value_from(settings)",
            "",
            "    old_default = home / \"orion.db\"",
            "    new_default = home / \"prefect.db\"",
            "",
            "    # If the old one exists and the new one does not, continue using the old one",
            "    if not new_default.exists():",
            "        if old_default.exists():",
            "            return \"sqlite+aiosqlite:///\" + str(old_default)",
            "",
            "    # Otherwise, return the new default",
            "    return \"sqlite+aiosqlite:///\" + str(new_default)",
            "",
            "",
            "def default_ui_url(settings, value):",
            "    if value is not None:",
            "        return value",
            "",
            "    # Otherwise, infer a value from the API URL",
            "    ui_url = api_url = PREFECT_API_URL.value_from(settings)",
            "",
            "    if not api_url:",
            "        return None",
            "",
            "    cloud_url = PREFECT_CLOUD_API_URL.value_from(settings)",
            "    cloud_ui_url = PREFECT_CLOUD_UI_URL.value_from(settings)",
            "    if api_url.startswith(cloud_url):",
            "        ui_url = ui_url.replace(cloud_url, cloud_ui_url)",
            "",
            "    if ui_url.endswith(\"/api\"):",
            "        # Handles open-source APIs",
            "        ui_url = ui_url[:-4]",
            "",
            "    # Handles Cloud APIs with content after `/api`",
            "    ui_url = ui_url.replace(\"/api/\", \"/\")",
            "",
            "    # Update routing",
            "    ui_url = ui_url.replace(\"/accounts/\", \"/account/\")",
            "    ui_url = ui_url.replace(\"/workspaces/\", \"/workspace/\")",
            "",
            "    return ui_url",
            "",
            "",
            "def default_cloud_ui_url(settings, value):",
            "    if value is not None:",
            "        return value",
            "",
            "    # Otherwise, infer a value from the API URL",
            "    ui_url = api_url = PREFECT_CLOUD_API_URL.value_from(settings)",
            "",
            "    if api_url.startswith(\"https://api.prefect.cloud\"):",
            "        ui_url = ui_url.replace(",
            "            \"https://api.prefect.cloud\", \"https://app.prefect.cloud\", 1",
            "        )",
            "",
            "    if ui_url.endswith(\"/api\"):",
            "        ui_url = ui_url[:-4]",
            "",
            "    return ui_url",
            "",
            "",
            "# Setting definitions",
            "",
            "PREFECT_HOME = Setting(",
            "    Path,",
            "    default=Path(\"~\") / \".prefect\",",
            "    value_callback=expanduser_in_path,",
            ")",
            "\"\"\"Prefect's home directory. Defaults to `~/.prefect`. This",
            "directory may be created automatically when required.",
            "\"\"\"",
            "",
            "PREFECT_EXTRA_ENTRYPOINTS = Setting(",
            "    str,",
            "    default=\"\",",
            ")",
            "\"\"\"",
            "Modules for Prefect to import when Prefect is imported.",
            "",
            "Values should be separated by commas, e.g. `my_module,my_other_module`.",
            "Objects within modules may be specified by a ':' partition, e.g. `my_module:my_object`.",
            "If a callable object is provided, it will be called with no arguments on import.",
            "\"\"\"",
            "",
            "PREFECT_DEBUG_MODE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, places the API in debug mode. This may modify",
            "behavior to facilitate debugging, including extra logs and other verbose",
            "assistance. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_CLI_COLORS = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"If `True`, use colors in CLI output. If `False`,",
            "output will not include colors codes. Defaults to `True`.",
            "\"\"\"",
            "",
            "PREFECT_CLI_PROMPT = Setting(",
            "    Optional[bool],",
            "    default=None,",
            ")",
            "\"\"\"If `True`, use interactive prompts in CLI commands. If `False`, no interactive",
            "prompts will be used. If `None`, the value will be dynamically determined based on",
            "the presence of an interactive-enabled terminal.",
            "\"\"\"",
            "",
            "PREFECT_CLI_WRAP_LINES = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"If `True`, wrap text by inserting new lines in long lines",
            "in CLI output. If `False`, output will not be wrapped. Defaults to `True`.",
            "\"\"\"",
            "",
            "PREFECT_TEST_MODE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, places the API in test mode. This may modify",
            "behavior to facilitate testing. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_UNIT_TEST_MODE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "This variable only exists to facilitate unit testing. If `True`,",
            "code is executing in a unit test context. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_TEST_SETTING = Setting(",
            "    Any,",
            "    default=None,",
            "    value_callback=only_return_value_in_test_mode,",
            ")",
            "\"\"\"",
            "This variable only exists to facilitate testing of settings.",
            "If accessed when `PREFECT_TEST_MODE` is not set, `None` is returned.",
            "\"\"\"",
            "",
            "PREFECT_API_TLS_INSECURE_SKIP_VERIFY = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, disables SSL checking to allow insecure requests.",
            "This is recommended only during development, e.g. when using self-signed certificates.",
            "\"\"\"",
            "",
            "PREFECT_API_URL = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"",
            "If provided, the URL of a hosted Prefect API. Defaults to `None`.",
            "",
            "When using Prefect Cloud, this will include an account and workspace.",
            "\"\"\"",
            "",
            "PREFECT_SILENCE_API_URL_MISCONFIGURATION = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, disable the warning when a user accidentally misconfigure its `PREFECT_API_URL`",
            "Sometimes when a user manually set `PREFECT_API_URL` to a custom url,reverse-proxy for example,",
            "we would like to silence this warning so we will set it to `FALSE`.",
            "\"\"\"",
            "",
            "PREFECT_API_KEY = Setting(",
            "    str,",
            "    default=None,",
            "    is_secret=True,",
            ")",
            "\"\"\"API key used to authenticate with a the Prefect API. Defaults to `None`.\"\"\"",
            "",
            "PREFECT_API_ENABLE_HTTP2 = Setting(bool, default=True)",
            "\"\"\"",
            "If true, enable support for HTTP/2 for communicating with an API.",
            "",
            "If the API does not support HTTP/2, this will have no effect and connections will be",
            "made via HTTP/1.1.",
            "\"\"\"",
            "",
            "",
            "PREFECT_CLIENT_MAX_RETRIES = Setting(int, default=5)",
            "\"\"\"",
            "The maximum number of retries to perform on failed HTTP requests.",
            "",
            "Defaults to 5.",
            "Set to 0 to disable retries.",
            "",
            "See `PREFECT_CLIENT_RETRY_EXTRA_CODES` for details on which HTTP status codes are",
            "retried.",
            "\"\"\"",
            "",
            "PREFECT_CLIENT_RETRY_JITTER_FACTOR = Setting(float, default=0.2)",
            "\"\"\"",
            "A value greater than or equal to zero to control the amount of jitter added to retried",
            "client requests. Higher values introduce larger amounts of jitter.",
            "",
            "Set to 0 to disable jitter. See `clamped_poisson_interval` for details on the how jitter",
            "can affect retry lengths.",
            "\"\"\"",
            "",
            "",
            "PREFECT_CLIENT_RETRY_EXTRA_CODES = Setting(",
            "    str, default=\"\", value_callback=status_codes_as_integers_in_range",
            ")",
            "\"\"\"",
            "A comma-separated list of extra HTTP status codes to retry on. Defaults to an empty string.",
            "429, 502 and 503 are always retried. Please note that not all routes are idempotent and retrying",
            "may result in unexpected behavior.",
            "\"\"\"",
            "",
            "PREFECT_CLOUD_API_URL = Setting(",
            "    str,",
            "    default=\"https://api.prefect.cloud/api\",",
            "    value_callback=check_for_deprecated_cloud_url,",
            ")",
            "\"\"\"API URL for Prefect Cloud. Used for authentication.\"\"\"",
            "",
            "",
            "PREFECT_CLOUD_URL = Setting(",
            "    str,",
            "    default=None,",
            "    deprecated=True,",
            "    deprecated_start_date=\"Dec 2022\",",
            "    deprecated_help=\"Use `PREFECT_CLOUD_API_URL` instead.\",",
            ")",
            "\"\"\"",
            "DEPRECATED: Use `PREFECT_CLOUD_API_URL` instead.",
            "\"\"\"",
            "",
            "PREFECT_UI_URL = Setting(",
            "    Optional[str],",
            "    default=None,",
            "    value_callback=default_ui_url,",
            ")",
            "\"\"\"",
            "The URL for the UI. By default, this is inferred from the PREFECT_API_URL.",
            "",
            "When using Prefect Cloud, this will include the account and workspace.",
            "When using an ephemeral server, this will be `None`.",
            "\"\"\"",
            "",
            "",
            "PREFECT_CLOUD_UI_URL = Setting(",
            "    str,",
            "    default=None,",
            "    value_callback=default_cloud_ui_url,",
            ")",
            "\"\"\"",
            "The URL for the Cloud UI. By default, this is inferred from the PREFECT_CLOUD_API_URL.",
            "",
            "Note: PREFECT_UI_URL will be workspace specific and will be usable in the open source too.",
            "      In contrast, this value is only valid for Cloud and will not include the workspace.",
            "\"\"\"",
            "",
            "PREFECT_API_REQUEST_TIMEOUT = Setting(",
            "    float,",
            "    default=60.0,",
            ")",
            "\"\"\"The default timeout for requests to the API\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN = Setting(bool, default=True)",
            "\"\"\"",
            "If enabled, warn on usage of experimental features.",
            "\"\"\"",
            "",
            "PREFECT_PROFILES_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"profiles.toml\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"The path to a profiles configuration files.\"\"\"",
            "",
            "PREFECT_RESULTS_DEFAULT_SERIALIZER = Setting(",
            "    str,",
            "    default=\"pickle\",",
            ")",
            "\"\"\"The default serializer to use when not otherwise specified.\"\"\"",
            "",
            "",
            "PREFECT_RESULTS_PERSIST_BY_DEFAULT = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "The default setting for persisting results when not otherwise specified. If enabled,",
            "flow and task results will be persisted unless they opt out.",
            "\"\"\"",
            "",
            "PREFECT_TASKS_REFRESH_CACHE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "If `True`, enables a refresh of cached results: re-executing the",
            "task will refresh the cached results. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_TASK_DEFAULT_RETRIES = Setting(int, default=0)",
            "\"\"\"",
            "This value sets the default number of retries for all tasks.",
            "This value does not overwrite individually set retries values on tasks",
            "\"\"\"",
            "",
            "PREFECT_FLOW_DEFAULT_RETRIES = Setting(int, default=0)",
            "\"\"\"",
            "This value sets the default number of retries for all flows.",
            "This value does not overwrite individually set retries values on a flow",
            "\"\"\"",
            "",
            "PREFECT_FLOW_DEFAULT_RETRY_DELAY_SECONDS = Setting(Union[int, float], default=0)",
            "\"\"\"",
            "This value sets the retry delay seconds for all flows.",
            "This value does not overwrite individually set retry delay seconds",
            "\"\"\"",
            "",
            "PREFECT_TASK_DEFAULT_RETRY_DELAY_SECONDS = Setting(",
            "    Union[float, int, List[float]], default=0",
            ")",
            "\"\"\"",
            "This value sets the default retry delay seconds for all tasks.",
            "This value does not overwrite individually set retry delay seconds",
            "\"\"\"",
            "",
            "PREFECT_TASK_RUN_TAG_CONCURRENCY_SLOT_WAIT_SECONDS = Setting(int, default=30)",
            "\"\"\"",
            "The number of seconds to wait before retrying when a task run",
            "cannot secure a concurrency slot from the server.",
            "\"\"\"",
            "",
            "PREFECT_LOCAL_STORAGE_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"storage\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"The path to a block storage directory to store things in.\"\"\"",
            "",
            "PREFECT_MEMO_STORE_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"memo_store.toml\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"The path to the memo store file.\"\"\"",
            "",
            "PREFECT_MEMOIZE_BLOCK_AUTO_REGISTRATION = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "Controls whether or not block auto-registration on start",
            "up should be memoized. Setting to False may result in slower server start",
            "up times.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_LEVEL = Setting(",
            "    str,",
            "    default=\"INFO\",",
            "    value_callback=debug_mode_log_level,",
            ")",
            "\"\"\"",
            "The default logging level for Prefect loggers. Defaults to",
            "\"INFO\" during normal operation. Is forced to \"DEBUG\" during debug mode.",
            "\"\"\"",
            "",
            "",
            "PREFECT_LOGGING_INTERNAL_LEVEL = Setting(",
            "    str,",
            "    default=\"ERROR\",",
            "    value_callback=debug_mode_log_level,",
            ")",
            "\"\"\"",
            "The default logging level for Prefect's internal machinery loggers. Defaults to",
            "\"ERROR\" during normal operation. Is forced to \"DEBUG\" during debug mode.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_SERVER_LEVEL = Setting(",
            "    str,",
            "    default=\"WARNING\",",
            ")",
            "\"\"\"The default logging level for the Prefect API server.\"\"\"",
            "",
            "PREFECT_LOGGING_SETTINGS_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"logging.yml\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"",
            "The path to a custom YAML logging configuration file. If",
            "no file is found, the default `logging.yml` is used.",
            "Defaults to a logging.yml in the Prefect home directory.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_EXTRA_LOGGERS = Setting(",
            "    str,",
            "    default=\"\",",
            "    value_callback=get_extra_loggers,",
            ")",
            "\"\"\"",
            "Additional loggers to attach to Prefect logging at runtime.",
            "Values should be comma separated. The handlers attached to the 'prefect' logger",
            "will be added to these loggers. Additionally, if the level is not set, it will",
            "be set to the same level as the 'prefect' logger.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_LOG_PRINTS = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "If set, `print` statements in flows and tasks will be redirected to the Prefect logger",
            "for the given run. This setting can be overridden by individual tasks and flows.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "Toggles sending logs to the API.",
            "If `False`, logs sent to the API log handler will not be sent to the API.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_BATCH_INTERVAL = Setting(float, default=2.0)",
            "\"\"\"The number of seconds between batched writes of logs to the API.\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_BATCH_SIZE = Setting(",
            "    int,",
            "    default=4_000_000,",
            ")",
            "\"\"\"The maximum size in bytes for a batch of logs.\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_MAX_LOG_SIZE = Setting(",
            "    int,",
            "    default=1_000_000,",
            ")",
            "\"\"\"The maximum size in bytes for a single log.\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW = Setting(",
            "    Literal[\"warn\", \"error\", \"ignore\"],",
            "    default=\"warn\",",
            ")",
            "\"\"\"",
            "Controls the behavior when loggers attempt to send logs to the API handler from outside",
            "of a flow.",
            "",
            "All logs sent to the API must be associated with a flow run. The API log handler can",
            "only be used outside of a flow by manually providing a flow run identifier. Logs",
            "that are not associated with a flow run will not be sent to the API. This setting can",
            "be used to determine if a warning or error is displayed when the identifier is missing.",
            "",
            "The following options are available:",
            "",
            "- \"warn\": Log a warning message.",
            "- \"error\": Raise an error.",
            "- \"ignore\": Do not log a warning message or raise an error.",
            "\"\"\"",
            "",
            "PREFECT_SQLALCHEMY_POOL_SIZE = Setting(",
            "    int,",
            "    default=None,",
            ")",
            "\"\"\"",
            "Controls connection pool size when using a PostgreSQL database with the Prefect API. If not set, the default SQLAlchemy pool size will be used.",
            "\"\"\"",
            "",
            "PREFECT_SQLALCHEMY_MAX_OVERFLOW = Setting(",
            "    int,",
            "    default=None,",
            ")",
            "\"\"\"",
            "Controls maximum overflow of the connection pool when using a PostgreSQL database with the Prefect API. If not set, the default SQLAlchemy maximum overflow value will be used.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_COLORS = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether to style console logs with color.\"\"\"",
            "",
            "PREFECT_LOGGING_MARKUP = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "Whether to interpret strings wrapped in square brackets as a style.",
            "This allows styles to be conveniently added to log messages, e.g.",
            "`[red]This is a red message.[/red]`. However, the downside is,",
            "if enabled, strings that contain square brackets may be inaccurately",
            "interpreted and lead to incomplete output, e.g.",
            "`DROP TABLE [dbo].[SomeTable];\"` outputs `DROP TABLE .[SomeTable];`.",
            "\"\"\"",
            "",
            "PREFECT_TASK_INTROSPECTION_WARN_THRESHOLD = Setting(",
            "    float,",
            "    default=10.0,",
            ")",
            "\"\"\"",
            "Threshold time in seconds for logging a warning if task parameter introspection",
            "exceeds this duration. Parameter introspection can be a significant performance hit",
            "when the parameter is a large collection object, e.g. a large dictionary or DataFrame,",
            "and each element needs to be inspected. See `prefect.utilities.annotations.quote`",
            "for more details.",
            "Defaults to `10.0`.",
            "Set to `0` to disable logging the warning.",
            "\"\"\"",
            "",
            "PREFECT_AGENT_QUERY_INTERVAL = Setting(",
            "    float,",
            "    default=15,",
            ")",
            "\"\"\"",
            "The agent loop interval, in seconds. Agents will check for new runs this often.",
            "Defaults to `15`.",
            "\"\"\"",
            "",
            "PREFECT_AGENT_PREFETCH_SECONDS = Setting(",
            "    int,",
            "    default=15,",
            ")",
            "\"\"\"",
            "Agents will look for scheduled runs this many seconds in",
            "the future and attempt to run them. This accounts for any additional",
            "infrastructure spin-up time or latency in preparing a flow run. Note",
            "flow runs will not start before their scheduled time, even if they are",
            "prefetched. Defaults to `15`.",
            "\"\"\"",
            "",
            "PREFECT_ASYNC_FETCH_STATE_RESULT = Setting(bool, default=False)",
            "\"\"\"",
            "Determines whether `State.result()` fetches results automatically or not.",
            "In Prefect 2.6.0, the `State.result()` method was updated to be async",
            "to facilitate automatic retrieval of results from storage which means when",
            "writing async code you must `await` the call. For backwards compatibility,",
            "the result is not retrieved by default for async users. You may opt into this",
            "per call by passing  `fetch=True` or toggle this setting to change the behavior",
            "globally.",
            "This setting does not affect users writing synchronous tasks and flows.",
            "This setting does not affect retrieval of results when using `Future.result()`.",
            "\"\"\"",
            "",
            "",
            "PREFECT_API_BLOCKS_REGISTER_ON_START = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "If set, any block types that have been imported will be registered with the",
            "backend on application startup. If not set, block types must be manually",
            "registered.",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_PASSWORD = Setting(",
            "    str,",
            "    default=None,",
            "    is_secret=True,",
            ")",
            "\"\"\"",
            "Password to template into the `PREFECT_API_DATABASE_CONNECTION_URL`.",
            "This is useful if the password must be provided separately from the connection URL.",
            "To use this setting, you must include it in your connection URL.",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_CONNECTION_URL = Setting(",
            "    str,",
            "    default=None,",
            "    value_callback=default_database_connection_url,",
            "    is_secret=True,",
            ")",
            "\"\"\"",
            "A database connection URL in a SQLAlchemy-compatible",
            "format. Prefect currently supports SQLite and Postgres. Note that all",
            "Prefect database engines must use an async driver - for SQLite, use",
            "`sqlite+aiosqlite` and for Postgres use `postgresql+asyncpg`.",
            "",
            "SQLite in-memory databases can be used by providing the url",
            "`sqlite+aiosqlite:///file::memory:?cache=shared&uri=true&check_same_thread=false`,",
            "which will allow the database to be accessed by multiple threads. Note",
            "that in-memory databases can not be accessed from multiple processes and",
            "should only be used for simple tests.",
            "",
            "Defaults to a sqlite database stored in the Prefect home directory.",
            "",
            "If you need to provide password via a different environment variable, you use",
            "the `PREFECT_API_DATABASE_PASSWORD` setting. For example:",
            "",
            "```",
            "PREFECT_API_DATABASE_PASSWORD='mypassword'",
            "PREFECT_API_DATABASE_CONNECTION_URL='postgresql+asyncpg://postgres:${PREFECT_API_DATABASE_PASSWORD}@localhost/prefect'",
            "```",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_ECHO = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, SQLAlchemy will log all SQL issued to the database. Defaults to `False`.\"\"\"",
            "",
            "PREFECT_API_DATABASE_MIGRATE_ON_START = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"If `True`, the database will be upgraded on application creation. If `False`, the database will need to be upgraded manually.\"\"\"",
            "",
            "PREFECT_API_DATABASE_TIMEOUT = Setting(",
            "    Optional[float],",
            "    default=10.0,",
            ")",
            "\"\"\"",
            "A statement timeout, in seconds, applied to all database interactions made by the API.",
            "Defaults to 10 seconds.",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_CONNECTION_TIMEOUT = Setting(",
            "    Optional[float],",
            "    default=5,",
            ")",
            "\"\"\"A connection timeout, in seconds, applied to database",
            "connections. Defaults to `5`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=60,",
            ")",
            "\"\"\"The scheduler loop interval, in seconds. This determines",
            "how often the scheduler will attempt to schedule new flow runs, but has no",
            "impact on how quickly either flow runs or task runs are actually executed.",
            "Defaults to `60`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE = Setting(",
            "    int,",
            "    default=100,",
            ")",
            "\"\"\"The number of deployments the scheduler will attempt to",
            "schedule in a single batch. If there are more deployments than the batch",
            "size, the scheduler immediately attempts to schedule the next batch; it",
            "does not sleep for `scheduler_loop_seconds` until it has visited every",
            "deployment once. Defaults to `100`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS = Setting(",
            "    int,",
            "    default=100,",
            ")",
            "\"\"\"The scheduler will attempt to schedule up to this many",
            "auto-scheduled runs in the future. Note that runs may have fewer than",
            "this many scheduled runs, depending on the value of",
            "`scheduler_max_scheduled_time`.  Defaults to `100`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS = Setting(",
            "    int,",
            "    default=3,",
            ")",
            "\"\"\"The scheduler will attempt to schedule at least this many",
            "auto-scheduled runs in the future. Note that runs may have more than",
            "this many scheduled runs, depending on the value of",
            "`scheduler_min_scheduled_time`.  Defaults to `3`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME = Setting(",
            "    timedelta,",
            "    default=timedelta(days=100),",
            ")",
            "\"\"\"The scheduler will create new runs up to this far in the",
            "future. Note that this setting will take precedence over",
            "`scheduler_max_runs`: if a flow runs once a month and",
            "`scheduler_max_scheduled_time` is three months, then only three runs will be",
            "scheduled. Defaults to 100 days (`8640000` seconds).",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME = Setting(",
            "    timedelta,",
            "    default=timedelta(hours=1),",
            ")",
            "\"\"\"The scheduler will create new runs at least this far in the",
            "future. Note that this setting will take precedence over `scheduler_min_runs`:",
            "if a flow runs every hour and `scheduler_min_scheduled_time` is three hours,",
            "then three runs will be scheduled even if `scheduler_min_runs` is 1. Defaults to",
            "1 hour (`3600` seconds).",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE = Setting(",
            "    int,",
            "    default=500,",
            ")",
            "\"\"\"The number of flow runs the scheduler will attempt to insert",
            "in one batch across all deployments. If the number of flow runs to",
            "schedule exceeds this amount, the runs will be inserted in batches of this size.",
            "Defaults to `500`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=5,",
            ")",
            "\"\"\"The late runs service will look for runs to mark as late",
            "this often. Defaults to `5`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS = Setting(",
            "    timedelta,",
            "    default=timedelta(seconds=5),",
            ")",
            "\"\"\"The late runs service will mark runs as late after they",
            "have exceeded their scheduled start time by this many seconds. Defaults",
            "to `5` seconds.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=5,",
            ")",
            "\"\"\"The pause expiration service will look for runs to mark as failed",
            "this often. Defaults to `5`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=20,",
            ")",
            "\"\"\"The cancellation cleanup service will look non-terminal tasks and subflows",
            "this often. Defaults to `20`.",
            "\"\"\"",
            "",
            "PREFECT_API_DEFAULT_LIMIT = Setting(",
            "    int,",
            "    default=200,",
            ")",
            "\"\"\"The default limit applied to queries that can return",
            "multiple objects, such as `POST /flow_runs/filter`.",
            "\"\"\"",
            "",
            "PREFECT_SERVER_API_HOST = Setting(",
            "    str,",
            "    default=\"127.0.0.1\",",
            ")",
            "\"\"\"The API's host address (defaults to `127.0.0.1`).\"\"\"",
            "",
            "PREFECT_SERVER_API_PORT = Setting(",
            "    int,",
            "    default=4200,",
            ")",
            "\"\"\"The API's port address (defaults to `4200`).\"\"\"",
            "",
            "PREFECT_SERVER_API_KEEPALIVE_TIMEOUT = Setting(",
            "    int,",
            "    default=5,",
            ")",
            "\"\"\"",
            "The API's keep alive timeout (defaults to `5`).",
            "Refer to https://www.uvicorn.org/settings/#timeouts for details.",
            "",
            "When the API is hosted behind a load balancer, you may want to set this to a value",
            "greater than the load balancer's idle timeout.",
            "",
            "Note this setting only applies when calling `prefect server start`; if hosting the",
            "API with another tool you will need to configure this there instead.",
            "\"\"\"",
            "",
            "PREFECT_SERVER_CSRF_PROTECTION_ENABLED = Setting(bool, default=False)",
            "\"\"\"Whether or not to protect the API from CSRF attacks. Experimental and",
            "currently defaults to `False`.\"\"\"",
            "",
            "PREFECT_SERVER_CSRF_TOKEN_EXPIRATION = Setting(timedelta, default=timedelta(hours=1))",
            "\"\"\"How long a CSRF token is valid for. Defaults to 1 hour.\"\"\"",
            "",
            "PREFECT_UI_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to serve the Prefect UI.\"\"\"",
            "",
            "PREFECT_UI_API_URL = Setting(",
            "    str,",
            "    default=None,",
            "    value_callback=default_ui_api_url,",
            ")",
            "\"\"\"The connection url for communication from the UI to the API.",
            "Defaults to `PREFECT_API_URL` if set. Otherwise, the default URL is generated from",
            "`PREFECT_SERVER_API_HOST` and `PREFECT_SERVER_API_PORT`. If providing a custom value,",
            "the aforementioned settings may be templated into the given string.",
            "\"\"\"",
            "",
            "PREFECT_SERVER_ANALYTICS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "When enabled, Prefect sends anonymous data (e.g. count of flow runs, package version)",
            "on server startup to help us improve our product.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the scheduling service in the server application.",
            "If disabled, you will need to run this service separately to schedule runs for deployments.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_LATE_RUNS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the late runs service in the server application.",
            "If disabled, you will need to run this service separately to have runs past their",
            "scheduled start time marked as late.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the flow run notifications service in the server application.",
            "If disabled, you will need to run this service separately to send flow run notifications.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the paused flow run expiration service in the server",
            "application. If disabled, paused flows that have timed out will remain in a Paused state",
            "until a resume attempt.",
            "\"\"\"",
            "",
            "PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH = Setting(int, default=2000)",
            "\"\"\"",
            "The maximum number of characters allowed for a task run cache key.",
            "This setting cannot be changed client-side, it must be set on the server.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the cancellation cleanup service in the server",
            "application. If disabled, task runs and subflow runs belonging to cancelled flows may",
            "remain in non-terminal states.",
            "\"\"\"",
            "",
            "PREFECT_API_MAX_FLOW_RUN_GRAPH_NODES = Setting(int, default=10000)",
            "\"\"\"",
            "The maximum size of a flow run graph on the v2 API",
            "\"\"\"",
            "",
            "PREFECT_API_MAX_FLOW_RUN_GRAPH_ARTIFACTS = Setting(int, default=10000)",
            "\"\"\"",
            "The maximum number of artifacts to show on a flow run graph on the v2 API",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ARTIFACTS_ON_FLOW_RUN_GRAPH = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable artifacts on the flow run graph.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_STATES_ON_FLOW_RUN_GRAPH = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable flow run states on the flow run graph.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_EVENTS_CLIENT = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect work pools.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_EVENTS_CLIENT = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect work pools are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORK_POOLS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect work pools.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_WORK_POOLS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect work pools are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORKERS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect workers.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_WORKERS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect workers are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_VISUALIZE = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect visualize is used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ENHANCED_CANCELLATION = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental enhanced flow run cancellation.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ENHANCED_DEPLOYMENT_PARAMETERS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable enhanced deployment parameters.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_ENHANCED_CANCELLATION = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental enhanced flow run cancellation is used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_DEPLOYMENT_STATUS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable deployment status in the UI",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_DEPLOYMENT_STATUS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when deployment status is used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_FLOW_RUN_INPUT = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable flow run input.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INPUT = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable flow run input.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_PROCESS_LIMIT = Setting(int, default=5)",
            "\"\"\"",
            "Maximum number of processes a runner will execute in parallel.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_POLL_FREQUENCY = Setting(int, default=10)",
            "\"\"\"",
            "Number of seconds a runner should wait between queries for scheduled work.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_MISSED_POLLS_TOLERANCE = Setting(int, default=2)",
            "\"\"\"",
            "Number of missed polls before a runner is considered unhealthy by its webserver.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_HOST = Setting(str, default=\"localhost\")",
            "\"\"\"",
            "The host address the runner's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_PORT = Setting(int, default=8080)",
            "\"\"\"",
            "The port the runner's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_LOG_LEVEL = Setting(str, default=\"error\")",
            "\"\"\"",
            "The log level of the runner's webserver.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_ENABLE = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable the runner's webserver.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_HEARTBEAT_SECONDS = Setting(float, default=30)",
            "\"\"\"",
            "Number of seconds a worker should wait between sending a heartbeat.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_QUERY_SECONDS = Setting(float, default=10)",
            "\"\"\"",
            "Number of seconds a worker should wait between queries for scheduled flow runs.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_PREFETCH_SECONDS = Setting(float, default=10)",
            "\"\"\"",
            "The number of seconds into the future a worker should query for scheduled flow runs.",
            "Can be used to compensate for infrastructure start up time for a worker.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_WEBSERVER_HOST = Setting(",
            "    str,",
            "    default=\"0.0.0.0\",",
            ")",
            "\"\"\"",
            "The host address the worker's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_WEBSERVER_PORT = Setting(",
            "    int,",
            "    default=8080,",
            ")",
            "\"\"\"",
            "The port the worker's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_DEFAULT_STORAGE_BLOCK = Setting(",
            "    str,",
            "    default=\"local-file-system/prefect-task-scheduling\",",
            ")",
            "\"\"\"The `block-type/block-document` slug of a block to use as the default storage",
            "for autonomous tasks.\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_DELETE_FAILED_SUBMISSIONS = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "Whether or not to delete failed task submissions from the database.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_MAX_SCHEDULED_QUEUE_SIZE = Setting(",
            "    int,",
            "    default=1000,",
            ")",
            "\"\"\"",
            "The maximum number of scheduled tasks to queue for submission.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_MAX_RETRY_QUEUE_SIZE = Setting(",
            "    int,",
            "    default=100,",
            ")",
            "\"\"\"",
            "The maximum number of retries to queue for submission.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_PENDING_TASK_TIMEOUT = Setting(",
            "    timedelta,",
            "    default=timedelta(seconds=30),",
            ")",
            "\"\"\"",
            "How long before a PENDING task are made available to another task server.  In practice,",
            "a task server should move a task from PENDING to RUNNING very quickly, so runs stuck in",
            "PENDING for a while is a sign that the task server may have crashed.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_FLOW_RUN_INFRA_OVERRIDES = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable infrastructure overrides made on flow runs.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to warn infrastructure when experimental flow runs overrides are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_EXTRA_RUNNER_ENDPOINTS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable experimental worker webserver endpoints.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ARTIFACTS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect artifacts.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_ARTIFACTS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect artifacts are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORKSPACE_DASHBOARD = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable the experimental workspace dashboard.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_WORKSPACE_DASHBOARD = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when the experimental workspace dashboard is enabled.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_TASK_SCHEDULING = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable experimental task scheduling.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORK_QUEUE_STATUS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental work queue status in-place of work queue health.",
            "\"\"\"",
            "",
            "",
            "# Defaults -----------------------------------------------------------------------------",
            "",
            "PREFECT_DEFAULT_RESULT_STORAGE_BLOCK = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"The `block-type/block-document` slug of a block to use as the default result storage.\"\"\"",
            "",
            "PREFECT_DEFAULT_WORK_POOL_NAME = Setting(str, default=None)",
            "\"\"\"",
            "The default work pool to deploy to.",
            "\"\"\"",
            "",
            "PREFECT_DEFAULT_DOCKER_BUILD_NAMESPACE = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"",
            "The default Docker namespace to use when building images.",
            "",
            "Can be either an organization/username or a registry URL with an organization/username.",
            "\"\"\"",
            "",
            "PREFECT_UI_SERVE_BASE = Setting(",
            "    str,",
            "    default=\"/\",",
            ")",
            "\"\"\"",
            "The base URL path to serve the Prefect UI from.",
            "",
            "Defaults to the root path.",
            "\"\"\"",
            "",
            "PREFECT_UI_STATIC_DIRECTORY = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"",
            "The directory to serve static files from. This should be used when running into permissions issues",
            "when attempting to serve the UI from the default directory (for example when running in a Docker container)",
            "\"\"\"",
            "",
            "",
            "# Deprecated settings ------------------------------------------------------------------",
            "",
            "",
            "# Collect all defined settings ---------------------------------------------------------",
            "",
            "SETTING_VARIABLES = {",
            "    name: val for name, val in tuple(globals().items()) if isinstance(val, Setting)",
            "}",
            "",
            "# Populate names in settings objects from assignments above",
            "# Uses `__` to avoid setting these as global variables which can lead to sneaky bugs",
            "",
            "for __name, __setting in SETTING_VARIABLES.items():",
            "    __setting._name = __name",
            "",
            "# Dynamically create a pydantic model that includes all of our settings",
            "",
            "SettingsFieldsMixin = create_model(",
            "    \"SettingsFieldsMixin\",",
            "    # Inheriting from `BaseSettings` provides environment variable loading",
            "    __base__=BaseSettings,",
            "    **{",
            "        setting.name: (setting.type, setting.field)",
            "        for setting in SETTING_VARIABLES.values()",
            "    },",
            ")",
            "",
            "",
            "# Defining a class after this that inherits the dynamic class rather than setting",
            "# __base__ to the following class ensures that mkdocstrings properly generates",
            "# reference documentation. It does not support module-level variables, even if they are",
            "# an object which has __doc__ set.",
            "",
            "",
            "@add_cloudpickle_reduction",
            "class Settings(SettingsFieldsMixin):",
            "    \"\"\"",
            "    Contains validated Prefect settings.",
            "",
            "    Settings should be accessed using the relevant `Setting` object. For example:",
            "    ```python",
            "    from prefect.settings import PREFECT_HOME",
            "    PREFECT_HOME.value()",
            "    ```",
            "",
            "    Accessing a setting attribute directly will ignore any `value_callback` mutations.",
            "    This is not recommended:",
            "    ```python",
            "    from prefect.settings import Settings",
            "    Settings().PREFECT_PROFILES_PATH  # PosixPath('${PREFECT_HOME}/profiles.toml')",
            "    ```",
            "    \"\"\"",
            "",
            "    def value_of(self, setting: Setting[T], bypass_callback: bool = False) -> T:",
            "        \"\"\"",
            "        Retrieve a setting's value.",
            "        \"\"\"",
            "        value = getattr(self, setting.name)",
            "        if setting.value_callback and not bypass_callback:",
            "            value = setting.value_callback(self, value)",
            "        return value",
            "",
            "    @validator(PREFECT_LOGGING_LEVEL.name, PREFECT_LOGGING_SERVER_LEVEL.name)",
            "    def check_valid_log_level(cls, value):",
            "        if isinstance(value, str):",
            "            value = value.upper()",
            "        logging._checkLevel(value)",
            "        return value",
            "",
            "    @root_validator",
            "    def post_root_validators(cls, values):",
            "        \"\"\"",
            "        Add root validation functions for settings here.",
            "        \"\"\"",
            "        # TODO: We could probably register these dynamically but this is the simpler",
            "        #       approach for now. We can explore more interesting validation features",
            "        #       in the future.",
            "        values = max_log_size_smaller_than_batch_size(values)",
            "        values = warn_on_database_password_value_without_usage(values)",
            "        if not values[\"PREFECT_SILENCE_API_URL_MISCONFIGURATION\"]:",
            "            values = warn_on_misconfigured_api_url(values)",
            "        return values",
            "",
            "    def copy_with_update(",
            "        self,",
            "        updates: Mapping[Setting, Any] = None,",
            "        set_defaults: Mapping[Setting, Any] = None,",
            "        restore_defaults: Iterable[Setting] = None,",
            "    ) -> \"Settings\":",
            "        \"\"\"",
            "        Create a new `Settings` object with validation.",
            "",
            "        Arguments:",
            "            updates: A mapping of settings to new values. Existing values for the",
            "                given settings will be overridden.",
            "            set_defaults: A mapping of settings to new default values. Existing values for",
            "                the given settings will only be overridden if they were not set.",
            "            restore_defaults: An iterable of settings to restore to their default values.",
            "",
            "        Returns:",
            "            A new `Settings` object.",
            "        \"\"\"",
            "        updates = updates or {}",
            "        set_defaults = set_defaults or {}",
            "        restore_defaults = restore_defaults or set()",
            "        restore_defaults_names = {setting.name for setting in restore_defaults}",
            "",
            "        return self.__class__(",
            "            **{",
            "                **{setting.name: value for setting, value in set_defaults.items()},",
            "                **self.dict(exclude_unset=True, exclude=restore_defaults_names),",
            "                **{setting.name: value for setting, value in updates.items()},",
            "            }",
            "        )",
            "",
            "    def with_obfuscated_secrets(self):",
            "        \"\"\"",
            "        Returns a copy of this settings object with secret setting values obfuscated.",
            "        \"\"\"",
            "        settings = self.copy(",
            "            update={",
            "                setting.name: obfuscate(self.value_of(setting))",
            "                for setting in SETTING_VARIABLES.values()",
            "                if setting.is_secret",
            "                # Exclude deprecated settings with null values to avoid warnings",
            "                and not (setting.deprecated and self.value_of(setting) is None)",
            "            }",
            "        )",
            "        # Ensure that settings that have not been marked as \"set\" before are still so",
            "        # after we have updated their value above",
            "        settings.__fields_set__.intersection_update(self.__fields_set__)",
            "        return settings",
            "",
            "    def hash_key(self) -> str:",
            "        \"\"\"",
            "        Return a hash key for the settings object.  This is needed since some",
            "        settings may be unhashable.  An example is lists.",
            "        \"\"\"",
            "        env_variables = self.to_environment_variables()",
            "        return str(hash(tuple((key, value) for key, value in env_variables.items())))",
            "",
            "    def to_environment_variables(",
            "        self, include: Iterable[Setting] = None, exclude_unset: bool = False",
            "    ) -> Dict[str, str]:",
            "        \"\"\"",
            "        Convert the settings object to environment variables.",
            "",
            "        Note that setting values will not be run through their `value_callback` allowing",
            "        dynamic resolution to occur when loaded from the returned environment.",
            "",
            "        Args:",
            "            include_keys: An iterable of settings to include in the return value.",
            "                If not set, all settings are used.",
            "            exclude_unset: Only include settings that have been set (i.e. the value is",
            "                not from the default). If set, unset keys will be dropped even if they",
            "                are set in `include_keys`.",
            "",
            "        Returns:",
            "            A dictionary of settings with values cast to strings",
            "        \"\"\"",
            "        include = set(include or SETTING_VARIABLES.values())",
            "",
            "        if exclude_unset:",
            "            set_keys = {",
            "                # Collect all of the \"set\" keys and cast to `Setting` objects",
            "                SETTING_VARIABLES[key]",
            "                for key in self.dict(exclude_unset=True)",
            "            }",
            "            include.intersection_update(set_keys)",
            "",
            "        # Validate the types of items in `include` to prevent exclusion bugs",
            "        for key in include:",
            "            if not isinstance(key, Setting):",
            "                raise TypeError(",
            "                    \"Invalid type {type(key).__name__!r} for key in `include`.\"",
            "                )",
            "",
            "        env = {",
            "            # Use `getattr` instead of `value_of` to avoid value callback resolution",
            "            key: getattr(self, key)",
            "            for key, setting in SETTING_VARIABLES.items()",
            "            if setting in include",
            "        }",
            "",
            "        # Cast to strings and drop null values",
            "        return {key: str(value) for key, value in env.items() if value is not None}",
            "",
            "    class Config:",
            "        frozen = True",
            "",
            "",
            "# Functions to instantiate `Settings` instances",
            "",
            "_DEFAULTS_CACHE: Settings = None",
            "_FROM_ENV_CACHE: Dict[int, Settings] = {}",
            "",
            "",
            "def get_current_settings() -> Settings:",
            "    \"\"\"",
            "    Returns a settings object populated with values from the current settings context",
            "    or, if no settings context is active, the environment.",
            "    \"\"\"",
            "    from prefect.context import SettingsContext",
            "",
            "    settings_context = SettingsContext.get()",
            "    if settings_context is not None:",
            "        return settings_context.settings",
            "",
            "    return get_settings_from_env()",
            "",
            "",
            "def get_settings_from_env() -> Settings:",
            "    \"\"\"",
            "    Returns a settings object populated with default values and overrides from",
            "    environment variables, ignoring any values in profiles.",
            "",
            "    Calls with the same environment return a cached object instead of reconstructing",
            "    to avoid validation overhead.",
            "    \"\"\"",
            "    # Since os.environ is a Dict[str, str] we can safely hash it by contents, but we",
            "    # must be careful to avoid hashing a generator instead of a tuple",
            "    cache_key = hash(tuple((key, value) for key, value in os.environ.items()))",
            "",
            "    if cache_key not in _FROM_ENV_CACHE:",
            "        _FROM_ENV_CACHE[cache_key] = Settings()",
            "",
            "    return _FROM_ENV_CACHE[cache_key]",
            "",
            "",
            "def get_default_settings() -> Settings:",
            "    \"\"\"",
            "    Returns a settings object populated with default values, ignoring any overrides",
            "    from environment variables or profiles.",
            "",
            "    This is cached since the defaults should not change during the lifetime of the",
            "    module.",
            "    \"\"\"",
            "    global _DEFAULTS_CACHE",
            "",
            "    if not _DEFAULTS_CACHE:",
            "        old = os.environ",
            "        try:",
            "            os.environ = {}",
            "            settings = get_settings_from_env()",
            "        finally:",
            "            os.environ = old",
            "",
            "        _DEFAULTS_CACHE = settings",
            "",
            "    return _DEFAULTS_CACHE",
            "",
            "",
            "@contextmanager",
            "def temporary_settings(",
            "    updates: Mapping[Setting, Any] = None,",
            "    set_defaults: Mapping[Setting, Any] = None,",
            "    restore_defaults: Iterable[Setting] = None,",
            ") -> Settings:",
            "    \"\"\"",
            "    Temporarily override the current settings by entering a new profile.",
            "",
            "    See `Settings.copy_with_update` for details on different argument behavior.",
            "",
            "    Examples:",
            "        >>> from prefect.settings import PREFECT_API_URL",
            "        >>>",
            "        >>> with temporary_settings(updates={PREFECT_API_URL: \"foo\"}):",
            "        >>>    assert PREFECT_API_URL.value() == \"foo\"",
            "        >>>",
            "        >>>    with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"}):",
            "        >>>         assert PREFECT_API_URL.value() == \"foo\"",
            "        >>>",
            "        >>>    with temporary_settings(restore_defaults={PREFECT_API_URL}):",
            "        >>>         assert PREFECT_API_URL.value() is None",
            "        >>>",
            "        >>>         with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"})",
            "        >>>             assert PREFECT_API_URL.value() == \"bar\"",
            "        >>> assert PREFECT_API_URL.value() is None",
            "    \"\"\"",
            "    import prefect.context",
            "",
            "    context = prefect.context.get_settings_context()",
            "",
            "    new_settings = context.settings.copy_with_update(",
            "        updates=updates, set_defaults=set_defaults, restore_defaults=restore_defaults",
            "    )",
            "",
            "    with prefect.context.SettingsContext(",
            "        profile=context.profile, settings=new_settings",
            "    ):",
            "        yield new_settings",
            "",
            "",
            "class Profile(BaseModel):",
            "    \"\"\"",
            "    A user profile containing settings.",
            "    \"\"\"",
            "",
            "    name: str",
            "    settings: Dict[Setting, Any] = Field(default_factory=dict)",
            "    source: Optional[Path]",
            "",
            "    @validator(\"settings\", pre=True)",
            "    def map_names_to_settings(cls, value):",
            "        if value is None:",
            "            return value",
            "",
            "        # Cast string setting names to variables",
            "        validated = {}",
            "        for setting, val in value.items():",
            "            if isinstance(setting, str) and setting in SETTING_VARIABLES:",
            "                validated[SETTING_VARIABLES[setting]] = val",
            "            elif isinstance(setting, Setting):",
            "                validated[setting] = val",
            "            else:",
            "                raise ValueError(f\"Unknown setting {setting!r}.\")",
            "",
            "        return validated",
            "",
            "    def validate_settings(self) -> None:",
            "        \"\"\"",
            "        Validate the settings contained in this profile.",
            "",
            "        Raises:",
            "            pydantic.ValidationError: When settings do not have valid values.",
            "        \"\"\"",
            "        # Create a new `Settings` instance with the settings from this profile relying",
            "        # on Pydantic validation to raise an error.",
            "        # We do not return the `Settings` object because this is not the recommended",
            "        # path for constructing settings with a profile. See `use_profile` instead.",
            "        Settings(**{setting.name: value for setting, value in self.settings.items()})",
            "",
            "    def convert_deprecated_renamed_settings(self) -> List[Tuple[Setting, Setting]]:",
            "        \"\"\"",
            "        Update settings in place to replace deprecated settings with new settings when",
            "        renamed.",
            "",
            "        Returns a list of tuples with the old and new setting.",
            "        \"\"\"",
            "        changed = []",
            "        for setting in tuple(self.settings):",
            "            if (",
            "                setting.deprecated",
            "                and setting.deprecated_renamed_to",
            "                and setting.deprecated_renamed_to not in self.settings",
            "            ):",
            "                self.settings[setting.deprecated_renamed_to] = self.settings.pop(",
            "                    setting",
            "                )",
            "                changed.append((setting, setting.deprecated_renamed_to))",
            "        return changed",
            "",
            "    class Config:",
            "        arbitrary_types_allowed = True",
            "",
            "",
            "class ProfilesCollection:",
            "    \"\"\" \"",
            "    A utility class for working with a collection of profiles.",
            "",
            "    Profiles in the collection must have unique names.",
            "",
            "    The collection may store the name of the active profile.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self, profiles: Iterable[Profile], active: Optional[str] = None",
            "    ) -> None:",
            "        self.profiles_by_name = {profile.name: profile for profile in profiles}",
            "        self.active_name = active",
            "",
            "    @property",
            "    def names(self) -> Set[str]:",
            "        \"\"\"",
            "        Return a set of profile names in this collection.",
            "        \"\"\"",
            "        return set(self.profiles_by_name.keys())",
            "",
            "    @property",
            "    def active_profile(self) -> Optional[Profile]:",
            "        \"\"\"",
            "        Retrieve the active profile in this collection.",
            "        \"\"\"",
            "        if self.active_name is None:",
            "            return None",
            "        return self[self.active_name]",
            "",
            "    def set_active(self, name: Optional[str], check: bool = True):",
            "        \"\"\"",
            "        Set the active profile name in the collection.",
            "",
            "        A null value may be passed to indicate that this collection does not determine",
            "        the active profile.",
            "        \"\"\"",
            "        if check and name is not None and name not in self.names:",
            "            raise ValueError(f\"Unknown profile name {name!r}.\")",
            "        self.active_name = name",
            "",
            "    def update_profile(",
            "        self, name: str, settings: Mapping[Union[Dict, str], Any], source: Path = None",
            "    ) -> Profile:",
            "        \"\"\"",
            "        Add a profile to the collection or update the existing on if the name is already",
            "        present in this collection.",
            "",
            "        If updating an existing profile, the settings will be merged. Settings can",
            "        be dropped from the existing profile by setting them to `None` in the new",
            "        profile.",
            "",
            "        Returns the new profile object.",
            "        \"\"\"",
            "        existing = self.profiles_by_name.get(name)",
            "",
            "        # Convert the input to a `Profile` to cast settings to the correct type",
            "        profile = Profile(name=name, settings=settings, source=source)",
            "",
            "        if existing:",
            "            new_settings = {**existing.settings, **profile.settings}",
            "",
            "            # Drop null keys to restore to default",
            "            for key, value in tuple(new_settings.items()):",
            "                if value is None:",
            "                    new_settings.pop(key)",
            "",
            "            new_profile = Profile(",
            "                name=profile.name,",
            "                settings=new_settings,",
            "                source=source or profile.source,",
            "            )",
            "        else:",
            "            new_profile = profile",
            "",
            "        self.profiles_by_name[new_profile.name] = new_profile",
            "",
            "        return new_profile",
            "",
            "    def add_profile(self, profile: Profile) -> None:",
            "        \"\"\"",
            "        Add a profile to the collection.",
            "",
            "        If the profile name already exists, an exception will be raised.",
            "        \"\"\"",
            "        if profile.name in self.profiles_by_name:",
            "            raise ValueError(",
            "                f\"Profile name {profile.name!r} already exists in collection.\"",
            "            )",
            "",
            "        self.profiles_by_name[profile.name] = profile",
            "",
            "    def remove_profile(self, name: str) -> None:",
            "        \"\"\"",
            "        Remove a profile from the collection.",
            "        \"\"\"",
            "        self.profiles_by_name.pop(name)",
            "",
            "    def without_profile_source(self, path: Optional[Path]) -> \"ProfilesCollection\":",
            "        \"\"\"",
            "        Remove profiles that were loaded from a given path.",
            "",
            "        Returns a new collection.",
            "        \"\"\"",
            "        return ProfilesCollection(",
            "            [",
            "                profile",
            "                for profile in self.profiles_by_name.values()",
            "                if profile.source != path",
            "            ],",
            "            active=self.active_name,",
            "        )",
            "",
            "    def to_dict(self):",
            "        \"\"\"",
            "        Convert to a dictionary suitable for writing to disk.",
            "        \"\"\"",
            "        return {",
            "            \"active\": self.active_name,",
            "            \"profiles\": {",
            "                profile.name: {",
            "                    setting.name: value for setting, value in profile.settings.items()",
            "                }",
            "                for profile in self.profiles_by_name.values()",
            "            },",
            "        }",
            "",
            "    def __getitem__(self, name: str) -> Profile:",
            "        return self.profiles_by_name[name]",
            "",
            "    def __iter__(self):",
            "        return self.profiles_by_name.__iter__()",
            "",
            "    def items(self):",
            "        return self.profiles_by_name.items()",
            "",
            "    def __eq__(self, __o: object) -> bool:",
            "        if not isinstance(__o, ProfilesCollection):",
            "            return False",
            "",
            "        return (",
            "            self.profiles_by_name == __o.profiles_by_name",
            "            and self.active_name == __o.active_name",
            "        )",
            "",
            "    def __repr__(self) -> str:",
            "        return (",
            "            f\"ProfilesCollection(profiles={list(self.profiles_by_name.values())!r},\"",
            "            f\" active={self.active_name!r})>\"",
            "        )",
            "",
            "",
            "def _handle_removed_flags(profile_name: str, settings: dict) -> dict:",
            "    to_remove = [name for name in settings if name in REMOVED_EXPERIMENTAL_FLAGS]",
            "",
            "    for name in to_remove:",
            "        warnings.warn(",
            "            (",
            "                f\"Experimental flag {name!r} has been removed, please \"",
            "                f\"update your {profile_name!r} profile.\"",
            "            ),",
            "            UserWarning,",
            "            stacklevel=3,",
            "        )",
            "",
            "        settings.pop(name)",
            "",
            "    return settings",
            "",
            "",
            "def _read_profiles_from(path: Path) -> ProfilesCollection:",
            "    \"\"\"",
            "    Read profiles from a path into a new `ProfilesCollection`.",
            "",
            "    Profiles are expected to be written in TOML with the following schema:",
            "        ```",
            "        active = <name: Optional[str]>",
            "",
            "        [profiles.<name: str>]",
            "        <SETTING: str> = <value: Any>",
            "        ```",
            "    \"\"\"",
            "    contents = toml.loads(path.read_text())",
            "    active_profile = contents.get(\"active\")",
            "    raw_profiles = contents.get(\"profiles\", {})",
            "",
            "    profiles = []",
            "    for name, settings in raw_profiles.items():",
            "        settings = _handle_removed_flags(name, settings)",
            "        profiles.append(Profile(name=name, settings=settings, source=path))",
            "",
            "    return ProfilesCollection(profiles, active=active_profile)",
            "",
            "",
            "def _write_profiles_to(path: Path, profiles: ProfilesCollection) -> None:",
            "    \"\"\"",
            "    Write profiles in the given collection to a path as TOML.",
            "",
            "    Any existing data not present in the given `profiles` will be deleted.",
            "    \"\"\"",
            "    if not path.exists():",
            "        path.touch(mode=0o600)",
            "    return path.write_text(toml.dumps(profiles.to_dict()))",
            "",
            "",
            "def load_profiles() -> ProfilesCollection:",
            "    \"\"\"",
            "    Load all profiles from the default and current profile paths.",
            "    \"\"\"",
            "    profiles = _read_profiles_from(DEFAULT_PROFILES_PATH)",
            "",
            "    user_profiles_path = PREFECT_PROFILES_PATH.value()",
            "    if user_profiles_path.exists():",
            "        user_profiles = _read_profiles_from(user_profiles_path)",
            "",
            "        # Merge all of the user profiles with the defaults",
            "        for name in user_profiles:",
            "            profiles.update_profile(",
            "                name,",
            "                settings=user_profiles[name].settings,",
            "                source=user_profiles[name].source,",
            "            )",
            "",
            "        if user_profiles.active_name:",
            "            profiles.set_active(user_profiles.active_name, check=False)",
            "",
            "    return profiles",
            "",
            "",
            "def load_current_profile():",
            "    \"\"\"",
            "    Load the current profile from the default and current profile paths.",
            "",
            "    This will _not_ include settings from the current settings context. Only settings",
            "    that have been persisted to the profiles file will be saved.",
            "    \"\"\"",
            "    from prefect.context import SettingsContext",
            "",
            "    profiles = load_profiles()",
            "    context = SettingsContext.get()",
            "",
            "    if context:",
            "        profiles.set_active(context.profile.name)",
            "",
            "    return profiles.active_profile",
            "",
            "",
            "def save_profiles(profiles: ProfilesCollection) -> None:",
            "    \"\"\"",
            "    Writes all non-default profiles to the current profiles path.",
            "    \"\"\"",
            "    profiles_path = PREFECT_PROFILES_PATH.value()",
            "    profiles = profiles.without_profile_source(DEFAULT_PROFILES_PATH)",
            "    return _write_profiles_to(profiles_path, profiles)",
            "",
            "",
            "def load_profile(name: str) -> Profile:",
            "    \"\"\"",
            "    Load a single profile by name.",
            "    \"\"\"",
            "    profiles = load_profiles()",
            "    try:",
            "        return profiles[name]",
            "    except KeyError:",
            "        raise ValueError(f\"Profile {name!r} not found.\")",
            "",
            "",
            "def update_current_profile(settings: Dict[Union[str, Setting], Any]) -> Profile:",
            "    \"\"\"",
            "    Update the persisted data for the profile currently in-use.",
            "",
            "    If the profile does not exist in the profiles file, it will be created.",
            "",
            "    Given settings will be merged with the existing settings as described in",
            "    `ProfilesCollection.update_profile`.",
            "",
            "    Returns:",
            "        The new profile.",
            "    \"\"\"",
            "    import prefect.context",
            "",
            "    current_profile = prefect.context.get_settings_context().profile",
            "",
            "    if not current_profile:",
            "        raise MissingProfileError(\"No profile is currently in use.\")",
            "",
            "    profiles = load_profiles()",
            "",
            "    # Ensure the current profile's settings are present",
            "    profiles.update_profile(current_profile.name, current_profile.settings)",
            "    # Then merge the new settings in",
            "    new_profile = profiles.update_profile(current_profile.name, settings)",
            "",
            "    # Validate before saving",
            "    new_profile.validate_settings()",
            "",
            "    save_profiles(profiles)",
            "",
            "    return profiles[current_profile.name]"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Prefect settings management.",
            "",
            "Each setting is defined as a `Setting` type. The name of each setting is stylized in all",
            "caps, matching the environment variable that can be used to change the setting.",
            "",
            "All settings defined in this file are used to generate a dynamic Pydantic settings class",
            "called `Settings`. When instantiated, this class will load settings from environment",
            "variables and pull default values from the setting definitions.",
            "",
            "The current instance of `Settings` being used by the application is stored in a",
            "`SettingsContext` model which allows each instance of the `Settings` class to be",
            "accessed in an async-safe manner.",
            "",
            "Aside from environment variables, we allow settings to be changed during the runtime of",
            "the process using profiles. Profiles contain setting overrides that the user may",
            "persist without setting environment variables. Profiles are also used internally for",
            "managing settings during task run execution where differing settings may be used",
            "concurrently in the same process and during testing where we need to override settings",
            "to ensure their value is respected as intended.",
            "",
            "The `SettingsContext` is set when the `prefect` module is imported. This context is",
            "referred to as the \"root\" settings context for clarity. Generally, this is the only",
            "settings context that will be used. When this context is entered, we will instantiate",
            "a `Settings` object, loading settings from environment variables and defaults, then we",
            "will load the active profile and use it to override settings. See  `enter_root_settings_context`",
            "for details on determining the active profile.",
            "",
            "Another `SettingsContext` may be entered at any time to change the settings being",
            "used by the code within the context. Generally, users should not use this. Settings",
            "management should be left to Prefect application internals.",
            "",
            "Generally, settings should be accessed with `SETTING_VARIABLE.value()` which will",
            "pull the current `Settings` instance from the current `SettingsContext` and retrieve",
            "the value of the relevant setting.",
            "",
            "Accessing a setting's value will also call the `Setting.value_callback` which allows",
            "settings to be dynamically modified on retrieval. This allows us to make settings",
            "dependent on the value of other settings or perform other dynamic effects.",
            "",
            "\"\"\"",
            "",
            "import logging",
            "import os",
            "import string",
            "import warnings",
            "from contextlib import contextmanager",
            "from datetime import timedelta",
            "from pathlib import Path",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    Generic,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    TypeVar,",
            "    Union,",
            ")",
            "from urllib.parse import urlparse",
            "",
            "import toml",
            "",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "",
            "if HAS_PYDANTIC_V2:",
            "    from pydantic.v1 import (",
            "        BaseModel,",
            "        BaseSettings,",
            "        Field,",
            "        create_model,",
            "        fields,",
            "        root_validator,",
            "        validator,",
            "    )",
            "else:",
            "    from pydantic import (",
            "        BaseModel,",
            "        BaseSettings,",
            "        Field,",
            "        create_model,",
            "        fields,",
            "        root_validator,",
            "        validator,",
            "    )",
            "",
            "from typing_extensions import Literal",
            "",
            "from prefect._internal.compatibility.deprecated import generate_deprecation_message",
            "from prefect._internal.pydantic import HAS_PYDANTIC_V2",
            "from prefect.exceptions import MissingProfileError",
            "from prefect.utilities.names import OBFUSCATED_PREFIX, obfuscate",
            "from prefect.utilities.pydantic import add_cloudpickle_reduction",
            "",
            "T = TypeVar(\"T\")",
            "",
            "",
            "DEFAULT_PROFILES_PATH = Path(__file__).parent.joinpath(\"profiles.toml\")",
            "",
            "REMOVED_EXPERIMENTAL_FLAGS = {\"PREFECT_EXPERIMENTAL_ENABLE_ENHANCED_SCHEDULING_UI\"}",
            "",
            "",
            "class Setting(Generic[T]):",
            "    \"\"\"",
            "    Setting definition type.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        type: Type[T],",
            "        *,",
            "        deprecated: bool = False,",
            "        deprecated_start_date: Optional[str] = None,",
            "        deprecated_end_date: Optional[str] = None,",
            "        deprecated_help: str = \"\",",
            "        deprecated_when_message: str = \"\",",
            "        deprecated_when: Optional[Callable[[Any], bool]] = None,",
            "        deprecated_renamed_to: Optional[\"Setting[T]\"] = None,",
            "        value_callback: Optional[Callable[[\"Settings\", T], T]] = None,",
            "        is_secret: bool = False,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        self.field: fields.FieldInfo = Field(**kwargs)",
            "        self.type = type",
            "        self.value_callback = value_callback",
            "        self._name = None",
            "        self.is_secret = is_secret",
            "        self.deprecated = deprecated",
            "        self.deprecated_start_date = deprecated_start_date",
            "        self.deprecated_end_date = deprecated_end_date",
            "        self.deprecated_help = deprecated_help",
            "        self.deprecated_when = deprecated_when or (lambda _: True)",
            "        self.deprecated_when_message = deprecated_when_message",
            "        self.deprecated_renamed_to = deprecated_renamed_to",
            "        self.deprecated_renamed_from = None",
            "        self.__doc__ = self.field.description",
            "",
            "        # Validate the deprecation settings, will throw an error at setting definition",
            "        # time if the developer has not configured it correctly",
            "        if deprecated:",
            "            generate_deprecation_message(",
            "                name=\"...\",  # setting names not populated until after init",
            "                start_date=self.deprecated_start_date,",
            "                end_date=self.deprecated_end_date,",
            "                help=self.deprecated_help,",
            "                when=self.deprecated_when_message,",
            "            )",
            "",
            "        if deprecated_renamed_to is not None:",
            "            # Track the deprecation both ways",
            "            deprecated_renamed_to.deprecated_renamed_from = self",
            "",
            "    def value(self, bypass_callback: bool = False) -> T:",
            "        \"\"\"",
            "        Get the current value of a setting.",
            "",
            "        Example:",
            "        ```python",
            "        from prefect.settings import PREFECT_API_URL",
            "        PREFECT_API_URL.value()",
            "        ```",
            "        \"\"\"",
            "        return self.value_from(get_current_settings(), bypass_callback=bypass_callback)",
            "",
            "    def value_from(self, settings: \"Settings\", bypass_callback: bool = False) -> T:",
            "        \"\"\"",
            "        Get the value of a setting from a settings object",
            "",
            "        Example:",
            "        ```python",
            "        from prefect.settings import get_default_settings",
            "        PREFECT_API_URL.value_from(get_default_settings())",
            "        ```",
            "        \"\"\"",
            "        value = settings.value_of(self, bypass_callback=bypass_callback)",
            "",
            "        if not bypass_callback and self.deprecated and self.deprecated_when(value):",
            "            # Check if this setting is deprecated and someone is accessing the value",
            "            # via the old name",
            "            warnings.warn(self.deprecated_message, DeprecationWarning, stacklevel=3)",
            "",
            "            # If the the value is empty, return the new setting's value for compat",
            "            if value is None and self.deprecated_renamed_to is not None:",
            "                return self.deprecated_renamed_to.value_from(settings)",
            "",
            "        if not bypass_callback and self.deprecated_renamed_from is not None:",
            "            # Check if this setting is a rename of a deprecated setting and the",
            "            # deprecated setting is set and should be used for compatibility",
            "            deprecated_value = self.deprecated_renamed_from.value_from(",
            "                settings, bypass_callback=True",
            "            )",
            "            if deprecated_value is not None:",
            "                warnings.warn(",
            "                    (",
            "                        f\"{self.deprecated_renamed_from.deprecated_message} Because\"",
            "                        f\" {self.deprecated_renamed_from.name!r} is set it will be used\"",
            "                        f\" instead of {self.name!r} for backwards compatibility.\"",
            "                    ),",
            "                    DeprecationWarning,",
            "                    stacklevel=3,",
            "                )",
            "            return deprecated_value or value",
            "",
            "        return value",
            "",
            "    @property",
            "    def name(self):",
            "        if self._name:",
            "            return self._name",
            "",
            "        # Lookup the name on first access",
            "        for name, val in tuple(globals().items()):",
            "            if val == self:",
            "                self._name = name",
            "                return name",
            "",
            "        raise ValueError(\"Setting not found in `prefect.settings` module.\")",
            "",
            "    @name.setter",
            "    def name(self, value: str):",
            "        self._name = value",
            "",
            "    @property",
            "    def deprecated_message(self):",
            "        return generate_deprecation_message(",
            "            name=f\"Setting {self.name!r}\",",
            "            start_date=self.deprecated_start_date,",
            "            end_date=self.deprecated_end_date,",
            "            help=self.deprecated_help,",
            "            when=self.deprecated_when_message,",
            "        )",
            "",
            "    def __repr__(self) -> str:",
            "        return f\"<{self.name}: {self.type.__name__}>\"",
            "",
            "    def __bool__(self) -> bool:",
            "        \"\"\"",
            "        Returns a truthy check of the current value.",
            "        \"\"\"",
            "        return bool(self.value())",
            "",
            "    def __eq__(self, __o: object) -> bool:",
            "        return __o.__eq__(self.value())",
            "",
            "    def __hash__(self) -> int:",
            "        return hash((type(self), self.name))",
            "",
            "",
            "# Callbacks and validators",
            "",
            "",
            "def get_extra_loggers(_: \"Settings\", value: str) -> List[str]:",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_LOGGING_EXTRA_LOGGERS`that parses the CSV string into a",
            "    list and trims whitespace from logger names.",
            "    \"\"\"",
            "    return [name.strip() for name in value.split(\",\")] if value else []",
            "",
            "",
            "def expanduser_in_path(_, value: Path) -> Path:",
            "    return value.expanduser()",
            "",
            "",
            "def debug_mode_log_level(settings, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_LOGGING_LEVEL` that overrides the log level to DEBUG",
            "    when debug mode is enabled.",
            "    \"\"\"",
            "    if PREFECT_DEBUG_MODE.value_from(settings):",
            "        return \"DEBUG\"",
            "    else:",
            "        return value",
            "",
            "",
            "def only_return_value_in_test_mode(settings, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_TEST_SETTING` that only allows access during test mode",
            "    \"\"\"",
            "    if PREFECT_TEST_MODE.value_from(settings):",
            "        return value",
            "    else:",
            "        return None",
            "",
            "",
            "def default_ui_api_url(settings, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_UI_API_URL` that sets the default value to",
            "    relative path '/api', otherwise it constructs an API URL from the API settings.",
            "    \"\"\"",
            "    if value is None:",
            "        # Set a default value",
            "        value = \"/api\"",
            "",
            "    return template_with_settings(",
            "        PREFECT_SERVER_API_HOST, PREFECT_SERVER_API_PORT, PREFECT_API_URL",
            "    )(settings, value)",
            "",
            "",
            "def status_codes_as_integers_in_range(_, value):",
            "    \"\"\"",
            "    `value_callback` for `PREFECT_CLIENT_RETRY_EXTRA_CODES` that ensures status codes",
            "    are integers in the range 100-599.",
            "    \"\"\"",
            "    if value == \"\":",
            "        return set()",
            "",
            "    values = {v.strip() for v in value.split(\",\")}",
            "",
            "    if any(not v.isdigit() or int(v) < 100 or int(v) > 599 for v in values):",
            "        raise ValueError(",
            "            \"PREFECT_CLIENT_RETRY_EXTRA_CODES must be a comma separated list of \"",
            "            \"integers between 100 and 599.\"",
            "        )",
            "",
            "    values = {int(v) for v in values}",
            "    return values",
            "",
            "",
            "def template_with_settings(*upstream_settings: Setting) -> Callable[[\"Settings\", T], T]:",
            "    \"\"\"",
            "    Returns a `value_callback` that will template the given settings into the runtime",
            "    value for the setting.",
            "    \"\"\"",
            "",
            "    def templater(settings, value):",
            "        if value is None:",
            "            return value  # Do not attempt to template a null string",
            "",
            "        original_type = type(value)",
            "        template_values = {",
            "            setting.name: setting.value_from(settings) for setting in upstream_settings",
            "        }",
            "        template = string.Template(str(value))",
            "        return original_type(template.substitute(template_values))",
            "",
            "    return templater",
            "",
            "",
            "def max_log_size_smaller_than_batch_size(values):",
            "    \"\"\"",
            "    Validator for settings asserting the batch size and match log size are compatible",
            "    \"\"\"",
            "    if (",
            "        values[\"PREFECT_LOGGING_TO_API_BATCH_SIZE\"]",
            "        < values[\"PREFECT_LOGGING_TO_API_MAX_LOG_SIZE\"]",
            "    ):",
            "        raise ValueError(",
            "            \"`PREFECT_LOGGING_TO_API_MAX_LOG_SIZE` cannot be larger than\"",
            "            \" `PREFECT_LOGGING_TO_API_BATCH_SIZE`\"",
            "        )",
            "    return values",
            "",
            "",
            "def warn_on_database_password_value_without_usage(values):",
            "    \"\"\"",
            "    Validator for settings warning if the database password is set but not used.",
            "    \"\"\"",
            "    value = values[\"PREFECT_API_DATABASE_PASSWORD\"]",
            "    if (",
            "        value",
            "        and not value.startswith(OBFUSCATED_PREFIX)",
            "        and (",
            "            \"PREFECT_API_DATABASE_PASSWORD\"",
            "            not in values[\"PREFECT_API_DATABASE_CONNECTION_URL\"]",
            "        )",
            "    ):",
            "        warnings.warn(",
            "            \"PREFECT_API_DATABASE_PASSWORD is set but not included in the \"",
            "            \"PREFECT_API_DATABASE_CONNECTION_URL. \"",
            "            \"The provided password will be ignored.\"",
            "        )",
            "    return values",
            "",
            "",
            "def check_for_deprecated_cloud_url(settings, value):",
            "    deprecated_value = PREFECT_CLOUD_URL.value_from(settings, bypass_callback=True)",
            "    if deprecated_value is not None:",
            "        warnings.warn(",
            "            (",
            "                \"`PREFECT_CLOUD_URL` is set and will be used instead of\"",
            "                \" `PREFECT_CLOUD_API_URL` for backwards compatibility.\"",
            "                \" `PREFECT_CLOUD_URL` is deprecated, set `PREFECT_CLOUD_API_URL`\"",
            "                \" instead.\"",
            "            ),",
            "            DeprecationWarning,",
            "        )",
            "    return deprecated_value or value",
            "",
            "",
            "def warn_on_misconfigured_api_url(values):",
            "    \"\"\"",
            "    Validator for settings warning if the API URL is misconfigured.",
            "    \"\"\"",
            "    api_url = values[\"PREFECT_API_URL\"]",
            "    if api_url is not None:",
            "        misconfigured_mappings = {",
            "            \"app.prefect.cloud\": (",
            "                \"`PREFECT_API_URL` points to `app.prefect.cloud`. Did you\"",
            "                \" mean `api.prefect.cloud`?\"",
            "            ),",
            "            \"account/\": (",
            "                \"`PREFECT_API_URL` uses `/account/` but should use `/accounts/`.\"",
            "            ),",
            "            \"workspace/\": (",
            "                \"`PREFECT_API_URL` uses `/workspace/` but should use `/workspaces/`.\"",
            "            ),",
            "        }",
            "        warnings_list = []",
            "",
            "        for misconfig, warning in misconfigured_mappings.items():",
            "            if misconfig in api_url:",
            "                warnings_list.append(warning)",
            "",
            "        parsed_url = urlparse(api_url)",
            "        if parsed_url.path and not parsed_url.path.startswith(\"/api\"):",
            "            warnings_list.append(",
            "                \"`PREFECT_API_URL` should have `/api` after the base URL.\"",
            "            )",
            "",
            "        if warnings_list:",
            "            example = 'e.g. PREFECT_API_URL=\"https://api.prefect.cloud/api/accounts/[ACCOUNT-ID]/workspaces/[WORKSPACE-ID]\"'",
            "            warnings_list.append(example)",
            "",
            "            warnings.warn(\"\\n\".join(warnings_list), stacklevel=2)",
            "",
            "    return values",
            "",
            "",
            "def default_database_connection_url(settings, value):",
            "    templater = template_with_settings(PREFECT_HOME, PREFECT_API_DATABASE_PASSWORD)",
            "",
            "    # If the user has provided a value, use it",
            "    if value is not None:",
            "        return templater(settings, value)",
            "",
            "    # Otherwise, the default is a database in a local file",
            "    home = PREFECT_HOME.value_from(settings)",
            "",
            "    old_default = home / \"orion.db\"",
            "    new_default = home / \"prefect.db\"",
            "",
            "    # If the old one exists and the new one does not, continue using the old one",
            "    if not new_default.exists():",
            "        if old_default.exists():",
            "            return \"sqlite+aiosqlite:///\" + str(old_default)",
            "",
            "    # Otherwise, return the new default",
            "    return \"sqlite+aiosqlite:///\" + str(new_default)",
            "",
            "",
            "def default_ui_url(settings, value):",
            "    if value is not None:",
            "        return value",
            "",
            "    # Otherwise, infer a value from the API URL",
            "    ui_url = api_url = PREFECT_API_URL.value_from(settings)",
            "",
            "    if not api_url:",
            "        return None",
            "",
            "    cloud_url = PREFECT_CLOUD_API_URL.value_from(settings)",
            "    cloud_ui_url = PREFECT_CLOUD_UI_URL.value_from(settings)",
            "    if api_url.startswith(cloud_url):",
            "        ui_url = ui_url.replace(cloud_url, cloud_ui_url)",
            "",
            "    if ui_url.endswith(\"/api\"):",
            "        # Handles open-source APIs",
            "        ui_url = ui_url[:-4]",
            "",
            "    # Handles Cloud APIs with content after `/api`",
            "    ui_url = ui_url.replace(\"/api/\", \"/\")",
            "",
            "    # Update routing",
            "    ui_url = ui_url.replace(\"/accounts/\", \"/account/\")",
            "    ui_url = ui_url.replace(\"/workspaces/\", \"/workspace/\")",
            "",
            "    return ui_url",
            "",
            "",
            "def default_cloud_ui_url(settings, value):",
            "    if value is not None:",
            "        return value",
            "",
            "    # Otherwise, infer a value from the API URL",
            "    ui_url = api_url = PREFECT_CLOUD_API_URL.value_from(settings)",
            "",
            "    if api_url.startswith(\"https://api.prefect.cloud\"):",
            "        ui_url = ui_url.replace(",
            "            \"https://api.prefect.cloud\", \"https://app.prefect.cloud\", 1",
            "        )",
            "",
            "    if ui_url.endswith(\"/api\"):",
            "        ui_url = ui_url[:-4]",
            "",
            "    return ui_url",
            "",
            "",
            "# Setting definitions",
            "",
            "PREFECT_HOME = Setting(",
            "    Path,",
            "    default=Path(\"~\") / \".prefect\",",
            "    value_callback=expanduser_in_path,",
            ")",
            "\"\"\"Prefect's home directory. Defaults to `~/.prefect`. This",
            "directory may be created automatically when required.",
            "\"\"\"",
            "",
            "PREFECT_EXTRA_ENTRYPOINTS = Setting(",
            "    str,",
            "    default=\"\",",
            ")",
            "\"\"\"",
            "Modules for Prefect to import when Prefect is imported.",
            "",
            "Values should be separated by commas, e.g. `my_module,my_other_module`.",
            "Objects within modules may be specified by a ':' partition, e.g. `my_module:my_object`.",
            "If a callable object is provided, it will be called with no arguments on import.",
            "\"\"\"",
            "",
            "PREFECT_DEBUG_MODE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, places the API in debug mode. This may modify",
            "behavior to facilitate debugging, including extra logs and other verbose",
            "assistance. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_CLI_COLORS = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"If `True`, use colors in CLI output. If `False`,",
            "output will not include colors codes. Defaults to `True`.",
            "\"\"\"",
            "",
            "PREFECT_CLI_PROMPT = Setting(",
            "    Optional[bool],",
            "    default=None,",
            ")",
            "\"\"\"If `True`, use interactive prompts in CLI commands. If `False`, no interactive",
            "prompts will be used. If `None`, the value will be dynamically determined based on",
            "the presence of an interactive-enabled terminal.",
            "\"\"\"",
            "",
            "PREFECT_CLI_WRAP_LINES = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"If `True`, wrap text by inserting new lines in long lines",
            "in CLI output. If `False`, output will not be wrapped. Defaults to `True`.",
            "\"\"\"",
            "",
            "PREFECT_TEST_MODE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, places the API in test mode. This may modify",
            "behavior to facilitate testing. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_UNIT_TEST_MODE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "This variable only exists to facilitate unit testing. If `True`,",
            "code is executing in a unit test context. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_TEST_SETTING = Setting(",
            "    Any,",
            "    default=None,",
            "    value_callback=only_return_value_in_test_mode,",
            ")",
            "\"\"\"",
            "This variable only exists to facilitate testing of settings.",
            "If accessed when `PREFECT_TEST_MODE` is not set, `None` is returned.",
            "\"\"\"",
            "",
            "PREFECT_API_TLS_INSECURE_SKIP_VERIFY = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, disables SSL checking to allow insecure requests.",
            "This is recommended only during development, e.g. when using self-signed certificates.",
            "\"\"\"",
            "",
            "PREFECT_API_URL = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"",
            "If provided, the URL of a hosted Prefect API. Defaults to `None`.",
            "",
            "When using Prefect Cloud, this will include an account and workspace.",
            "\"\"\"",
            "",
            "PREFECT_SILENCE_API_URL_MISCONFIGURATION = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, disable the warning when a user accidentally misconfigure its `PREFECT_API_URL`",
            "Sometimes when a user manually set `PREFECT_API_URL` to a custom url,reverse-proxy for example,",
            "we would like to silence this warning so we will set it to `FALSE`.",
            "\"\"\"",
            "",
            "PREFECT_API_KEY = Setting(",
            "    str,",
            "    default=None,",
            "    is_secret=True,",
            ")",
            "\"\"\"API key used to authenticate with a the Prefect API. Defaults to `None`.\"\"\"",
            "",
            "PREFECT_API_ENABLE_HTTP2 = Setting(bool, default=True)",
            "\"\"\"",
            "If true, enable support for HTTP/2 for communicating with an API.",
            "",
            "If the API does not support HTTP/2, this will have no effect and connections will be",
            "made via HTTP/1.1.",
            "\"\"\"",
            "",
            "",
            "PREFECT_CLIENT_MAX_RETRIES = Setting(int, default=5)",
            "\"\"\"",
            "The maximum number of retries to perform on failed HTTP requests.",
            "",
            "Defaults to 5.",
            "Set to 0 to disable retries.",
            "",
            "See `PREFECT_CLIENT_RETRY_EXTRA_CODES` for details on which HTTP status codes are",
            "retried.",
            "\"\"\"",
            "",
            "PREFECT_CLIENT_RETRY_JITTER_FACTOR = Setting(float, default=0.2)",
            "\"\"\"",
            "A value greater than or equal to zero to control the amount of jitter added to retried",
            "client requests. Higher values introduce larger amounts of jitter.",
            "",
            "Set to 0 to disable jitter. See `clamped_poisson_interval` for details on the how jitter",
            "can affect retry lengths.",
            "\"\"\"",
            "",
            "",
            "PREFECT_CLIENT_RETRY_EXTRA_CODES = Setting(",
            "    str, default=\"\", value_callback=status_codes_as_integers_in_range",
            ")",
            "\"\"\"",
            "A comma-separated list of extra HTTP status codes to retry on. Defaults to an empty string.",
            "429, 502 and 503 are always retried. Please note that not all routes are idempotent and retrying",
            "may result in unexpected behavior.",
            "\"\"\"",
            "",
            "PREFECT_CLIENT_CSRF_SUPPORT_ENABLED = Setting(bool, default=True)",
            "\"\"\"",
            "Determines if CSRF token handling is active in the Prefect client for API",
            "requests.",
            "",
            "When enabled (`True`), the client automatically manages CSRF tokens by",
            "retrieving, storing, and including them in applicable state-changing requests",
            "(POST, PUT, PATCH, DELETE) to the API.",
            "",
            "Disabling this setting (`False`) means the client will not handle CSRF tokens,",
            "which might be suitable for environments where CSRF protection is disabled.",
            "",
            "Defaults to `True`, ensuring CSRF protection is enabled by default.",
            "\"\"\"",
            "",
            "PREFECT_CLOUD_API_URL = Setting(",
            "    str,",
            "    default=\"https://api.prefect.cloud/api\",",
            "    value_callback=check_for_deprecated_cloud_url,",
            ")",
            "\"\"\"API URL for Prefect Cloud. Used for authentication.\"\"\"",
            "",
            "",
            "PREFECT_CLOUD_URL = Setting(",
            "    str,",
            "    default=None,",
            "    deprecated=True,",
            "    deprecated_start_date=\"Dec 2022\",",
            "    deprecated_help=\"Use `PREFECT_CLOUD_API_URL` instead.\",",
            ")",
            "\"\"\"",
            "DEPRECATED: Use `PREFECT_CLOUD_API_URL` instead.",
            "\"\"\"",
            "",
            "PREFECT_UI_URL = Setting(",
            "    Optional[str],",
            "    default=None,",
            "    value_callback=default_ui_url,",
            ")",
            "\"\"\"",
            "The URL for the UI. By default, this is inferred from the PREFECT_API_URL.",
            "",
            "When using Prefect Cloud, this will include the account and workspace.",
            "When using an ephemeral server, this will be `None`.",
            "\"\"\"",
            "",
            "",
            "PREFECT_CLOUD_UI_URL = Setting(",
            "    str,",
            "    default=None,",
            "    value_callback=default_cloud_ui_url,",
            ")",
            "\"\"\"",
            "The URL for the Cloud UI. By default, this is inferred from the PREFECT_CLOUD_API_URL.",
            "",
            "Note: PREFECT_UI_URL will be workspace specific and will be usable in the open source too.",
            "      In contrast, this value is only valid for Cloud and will not include the workspace.",
            "\"\"\"",
            "",
            "PREFECT_API_REQUEST_TIMEOUT = Setting(",
            "    float,",
            "    default=60.0,",
            ")",
            "\"\"\"The default timeout for requests to the API\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN = Setting(bool, default=True)",
            "\"\"\"",
            "If enabled, warn on usage of experimental features.",
            "\"\"\"",
            "",
            "PREFECT_PROFILES_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"profiles.toml\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"The path to a profiles configuration files.\"\"\"",
            "",
            "PREFECT_RESULTS_DEFAULT_SERIALIZER = Setting(",
            "    str,",
            "    default=\"pickle\",",
            ")",
            "\"\"\"The default serializer to use when not otherwise specified.\"\"\"",
            "",
            "",
            "PREFECT_RESULTS_PERSIST_BY_DEFAULT = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "The default setting for persisting results when not otherwise specified. If enabled,",
            "flow and task results will be persisted unless they opt out.",
            "\"\"\"",
            "",
            "PREFECT_TASKS_REFRESH_CACHE = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "If `True`, enables a refresh of cached results: re-executing the",
            "task will refresh the cached results. Defaults to `False`.",
            "\"\"\"",
            "",
            "PREFECT_TASK_DEFAULT_RETRIES = Setting(int, default=0)",
            "\"\"\"",
            "This value sets the default number of retries for all tasks.",
            "This value does not overwrite individually set retries values on tasks",
            "\"\"\"",
            "",
            "PREFECT_FLOW_DEFAULT_RETRIES = Setting(int, default=0)",
            "\"\"\"",
            "This value sets the default number of retries for all flows.",
            "This value does not overwrite individually set retries values on a flow",
            "\"\"\"",
            "",
            "PREFECT_FLOW_DEFAULT_RETRY_DELAY_SECONDS = Setting(Union[int, float], default=0)",
            "\"\"\"",
            "This value sets the retry delay seconds for all flows.",
            "This value does not overwrite individually set retry delay seconds",
            "\"\"\"",
            "",
            "PREFECT_TASK_DEFAULT_RETRY_DELAY_SECONDS = Setting(",
            "    Union[float, int, List[float]], default=0",
            ")",
            "\"\"\"",
            "This value sets the default retry delay seconds for all tasks.",
            "This value does not overwrite individually set retry delay seconds",
            "\"\"\"",
            "",
            "PREFECT_TASK_RUN_TAG_CONCURRENCY_SLOT_WAIT_SECONDS = Setting(int, default=30)",
            "\"\"\"",
            "The number of seconds to wait before retrying when a task run",
            "cannot secure a concurrency slot from the server.",
            "\"\"\"",
            "",
            "PREFECT_LOCAL_STORAGE_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"storage\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"The path to a block storage directory to store things in.\"\"\"",
            "",
            "PREFECT_MEMO_STORE_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"memo_store.toml\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"The path to the memo store file.\"\"\"",
            "",
            "PREFECT_MEMOIZE_BLOCK_AUTO_REGISTRATION = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "Controls whether or not block auto-registration on start",
            "up should be memoized. Setting to False may result in slower server start",
            "up times.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_LEVEL = Setting(",
            "    str,",
            "    default=\"INFO\",",
            "    value_callback=debug_mode_log_level,",
            ")",
            "\"\"\"",
            "The default logging level for Prefect loggers. Defaults to",
            "\"INFO\" during normal operation. Is forced to \"DEBUG\" during debug mode.",
            "\"\"\"",
            "",
            "",
            "PREFECT_LOGGING_INTERNAL_LEVEL = Setting(",
            "    str,",
            "    default=\"ERROR\",",
            "    value_callback=debug_mode_log_level,",
            ")",
            "\"\"\"",
            "The default logging level for Prefect's internal machinery loggers. Defaults to",
            "\"ERROR\" during normal operation. Is forced to \"DEBUG\" during debug mode.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_SERVER_LEVEL = Setting(",
            "    str,",
            "    default=\"WARNING\",",
            ")",
            "\"\"\"The default logging level for the Prefect API server.\"\"\"",
            "",
            "PREFECT_LOGGING_SETTINGS_PATH = Setting(",
            "    Path,",
            "    default=Path(\"${PREFECT_HOME}\") / \"logging.yml\",",
            "    value_callback=template_with_settings(PREFECT_HOME),",
            ")",
            "\"\"\"",
            "The path to a custom YAML logging configuration file. If",
            "no file is found, the default `logging.yml` is used.",
            "Defaults to a logging.yml in the Prefect home directory.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_EXTRA_LOGGERS = Setting(",
            "    str,",
            "    default=\"\",",
            "    value_callback=get_extra_loggers,",
            ")",
            "\"\"\"",
            "Additional loggers to attach to Prefect logging at runtime.",
            "Values should be comma separated. The handlers attached to the 'prefect' logger",
            "will be added to these loggers. Additionally, if the level is not set, it will",
            "be set to the same level as the 'prefect' logger.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_LOG_PRINTS = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "If set, `print` statements in flows and tasks will be redirected to the Prefect logger",
            "for the given run. This setting can be overridden by individual tasks and flows.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "Toggles sending logs to the API.",
            "If `False`, logs sent to the API log handler will not be sent to the API.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_BATCH_INTERVAL = Setting(float, default=2.0)",
            "\"\"\"The number of seconds between batched writes of logs to the API.\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_BATCH_SIZE = Setting(",
            "    int,",
            "    default=4_000_000,",
            ")",
            "\"\"\"The maximum size in bytes for a batch of logs.\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_MAX_LOG_SIZE = Setting(",
            "    int,",
            "    default=1_000_000,",
            ")",
            "\"\"\"The maximum size in bytes for a single log.\"\"\"",
            "",
            "PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW = Setting(",
            "    Literal[\"warn\", \"error\", \"ignore\"],",
            "    default=\"warn\",",
            ")",
            "\"\"\"",
            "Controls the behavior when loggers attempt to send logs to the API handler from outside",
            "of a flow.",
            "",
            "All logs sent to the API must be associated with a flow run. The API log handler can",
            "only be used outside of a flow by manually providing a flow run identifier. Logs",
            "that are not associated with a flow run will not be sent to the API. This setting can",
            "be used to determine if a warning or error is displayed when the identifier is missing.",
            "",
            "The following options are available:",
            "",
            "- \"warn\": Log a warning message.",
            "- \"error\": Raise an error.",
            "- \"ignore\": Do not log a warning message or raise an error.",
            "\"\"\"",
            "",
            "PREFECT_SQLALCHEMY_POOL_SIZE = Setting(",
            "    int,",
            "    default=None,",
            ")",
            "\"\"\"",
            "Controls connection pool size when using a PostgreSQL database with the Prefect API. If not set, the default SQLAlchemy pool size will be used.",
            "\"\"\"",
            "",
            "PREFECT_SQLALCHEMY_MAX_OVERFLOW = Setting(",
            "    int,",
            "    default=None,",
            ")",
            "\"\"\"",
            "Controls maximum overflow of the connection pool when using a PostgreSQL database with the Prefect API. If not set, the default SQLAlchemy maximum overflow value will be used.",
            "\"\"\"",
            "",
            "PREFECT_LOGGING_COLORS = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether to style console logs with color.\"\"\"",
            "",
            "PREFECT_LOGGING_MARKUP = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"",
            "Whether to interpret strings wrapped in square brackets as a style.",
            "This allows styles to be conveniently added to log messages, e.g.",
            "`[red]This is a red message.[/red]`. However, the downside is,",
            "if enabled, strings that contain square brackets may be inaccurately",
            "interpreted and lead to incomplete output, e.g.",
            "`DROP TABLE [dbo].[SomeTable];\"` outputs `DROP TABLE .[SomeTable];`.",
            "\"\"\"",
            "",
            "PREFECT_TASK_INTROSPECTION_WARN_THRESHOLD = Setting(",
            "    float,",
            "    default=10.0,",
            ")",
            "\"\"\"",
            "Threshold time in seconds for logging a warning if task parameter introspection",
            "exceeds this duration. Parameter introspection can be a significant performance hit",
            "when the parameter is a large collection object, e.g. a large dictionary or DataFrame,",
            "and each element needs to be inspected. See `prefect.utilities.annotations.quote`",
            "for more details.",
            "Defaults to `10.0`.",
            "Set to `0` to disable logging the warning.",
            "\"\"\"",
            "",
            "PREFECT_AGENT_QUERY_INTERVAL = Setting(",
            "    float,",
            "    default=15,",
            ")",
            "\"\"\"",
            "The agent loop interval, in seconds. Agents will check for new runs this often.",
            "Defaults to `15`.",
            "\"\"\"",
            "",
            "PREFECT_AGENT_PREFETCH_SECONDS = Setting(",
            "    int,",
            "    default=15,",
            ")",
            "\"\"\"",
            "Agents will look for scheduled runs this many seconds in",
            "the future and attempt to run them. This accounts for any additional",
            "infrastructure spin-up time or latency in preparing a flow run. Note",
            "flow runs will not start before their scheduled time, even if they are",
            "prefetched. Defaults to `15`.",
            "\"\"\"",
            "",
            "PREFECT_ASYNC_FETCH_STATE_RESULT = Setting(bool, default=False)",
            "\"\"\"",
            "Determines whether `State.result()` fetches results automatically or not.",
            "In Prefect 2.6.0, the `State.result()` method was updated to be async",
            "to facilitate automatic retrieval of results from storage which means when",
            "writing async code you must `await` the call. For backwards compatibility,",
            "the result is not retrieved by default for async users. You may opt into this",
            "per call by passing  `fetch=True` or toggle this setting to change the behavior",
            "globally.",
            "This setting does not affect users writing synchronous tasks and flows.",
            "This setting does not affect retrieval of results when using `Future.result()`.",
            "\"\"\"",
            "",
            "",
            "PREFECT_API_BLOCKS_REGISTER_ON_START = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "If set, any block types that have been imported will be registered with the",
            "backend on application startup. If not set, block types must be manually",
            "registered.",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_PASSWORD = Setting(",
            "    str,",
            "    default=None,",
            "    is_secret=True,",
            ")",
            "\"\"\"",
            "Password to template into the `PREFECT_API_DATABASE_CONNECTION_URL`.",
            "This is useful if the password must be provided separately from the connection URL.",
            "To use this setting, you must include it in your connection URL.",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_CONNECTION_URL = Setting(",
            "    str,",
            "    default=None,",
            "    value_callback=default_database_connection_url,",
            "    is_secret=True,",
            ")",
            "\"\"\"",
            "A database connection URL in a SQLAlchemy-compatible",
            "format. Prefect currently supports SQLite and Postgres. Note that all",
            "Prefect database engines must use an async driver - for SQLite, use",
            "`sqlite+aiosqlite` and for Postgres use `postgresql+asyncpg`.",
            "",
            "SQLite in-memory databases can be used by providing the url",
            "`sqlite+aiosqlite:///file::memory:?cache=shared&uri=true&check_same_thread=false`,",
            "which will allow the database to be accessed by multiple threads. Note",
            "that in-memory databases can not be accessed from multiple processes and",
            "should only be used for simple tests.",
            "",
            "Defaults to a sqlite database stored in the Prefect home directory.",
            "",
            "If you need to provide password via a different environment variable, you use",
            "the `PREFECT_API_DATABASE_PASSWORD` setting. For example:",
            "",
            "```",
            "PREFECT_API_DATABASE_PASSWORD='mypassword'",
            "PREFECT_API_DATABASE_CONNECTION_URL='postgresql+asyncpg://postgres:${PREFECT_API_DATABASE_PASSWORD}@localhost/prefect'",
            "```",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_ECHO = Setting(",
            "    bool,",
            "    default=False,",
            ")",
            "\"\"\"If `True`, SQLAlchemy will log all SQL issued to the database. Defaults to `False`.\"\"\"",
            "",
            "PREFECT_API_DATABASE_MIGRATE_ON_START = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"If `True`, the database will be upgraded on application creation. If `False`, the database will need to be upgraded manually.\"\"\"",
            "",
            "PREFECT_API_DATABASE_TIMEOUT = Setting(",
            "    Optional[float],",
            "    default=10.0,",
            ")",
            "\"\"\"",
            "A statement timeout, in seconds, applied to all database interactions made by the API.",
            "Defaults to 10 seconds.",
            "\"\"\"",
            "",
            "PREFECT_API_DATABASE_CONNECTION_TIMEOUT = Setting(",
            "    Optional[float],",
            "    default=5,",
            ")",
            "\"\"\"A connection timeout, in seconds, applied to database",
            "connections. Defaults to `5`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=60,",
            ")",
            "\"\"\"The scheduler loop interval, in seconds. This determines",
            "how often the scheduler will attempt to schedule new flow runs, but has no",
            "impact on how quickly either flow runs or task runs are actually executed.",
            "Defaults to `60`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE = Setting(",
            "    int,",
            "    default=100,",
            ")",
            "\"\"\"The number of deployments the scheduler will attempt to",
            "schedule in a single batch. If there are more deployments than the batch",
            "size, the scheduler immediately attempts to schedule the next batch; it",
            "does not sleep for `scheduler_loop_seconds` until it has visited every",
            "deployment once. Defaults to `100`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS = Setting(",
            "    int,",
            "    default=100,",
            ")",
            "\"\"\"The scheduler will attempt to schedule up to this many",
            "auto-scheduled runs in the future. Note that runs may have fewer than",
            "this many scheduled runs, depending on the value of",
            "`scheduler_max_scheduled_time`.  Defaults to `100`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS = Setting(",
            "    int,",
            "    default=3,",
            ")",
            "\"\"\"The scheduler will attempt to schedule at least this many",
            "auto-scheduled runs in the future. Note that runs may have more than",
            "this many scheduled runs, depending on the value of",
            "`scheduler_min_scheduled_time`.  Defaults to `3`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME = Setting(",
            "    timedelta,",
            "    default=timedelta(days=100),",
            ")",
            "\"\"\"The scheduler will create new runs up to this far in the",
            "future. Note that this setting will take precedence over",
            "`scheduler_max_runs`: if a flow runs once a month and",
            "`scheduler_max_scheduled_time` is three months, then only three runs will be",
            "scheduled. Defaults to 100 days (`8640000` seconds).",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME = Setting(",
            "    timedelta,",
            "    default=timedelta(hours=1),",
            ")",
            "\"\"\"The scheduler will create new runs at least this far in the",
            "future. Note that this setting will take precedence over `scheduler_min_runs`:",
            "if a flow runs every hour and `scheduler_min_scheduled_time` is three hours,",
            "then three runs will be scheduled even if `scheduler_min_runs` is 1. Defaults to",
            "1 hour (`3600` seconds).",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE = Setting(",
            "    int,",
            "    default=500,",
            ")",
            "\"\"\"The number of flow runs the scheduler will attempt to insert",
            "in one batch across all deployments. If the number of flow runs to",
            "schedule exceeds this amount, the runs will be inserted in batches of this size.",
            "Defaults to `500`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=5,",
            ")",
            "\"\"\"The late runs service will look for runs to mark as late",
            "this often. Defaults to `5`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS = Setting(",
            "    timedelta,",
            "    default=timedelta(seconds=5),",
            ")",
            "\"\"\"The late runs service will mark runs as late after they",
            "have exceeded their scheduled start time by this many seconds. Defaults",
            "to `5` seconds.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=5,",
            ")",
            "\"\"\"The pause expiration service will look for runs to mark as failed",
            "this often. Defaults to `5`.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS = Setting(",
            "    float,",
            "    default=20,",
            ")",
            "\"\"\"The cancellation cleanup service will look non-terminal tasks and subflows",
            "this often. Defaults to `20`.",
            "\"\"\"",
            "",
            "PREFECT_API_DEFAULT_LIMIT = Setting(",
            "    int,",
            "    default=200,",
            ")",
            "\"\"\"The default limit applied to queries that can return",
            "multiple objects, such as `POST /flow_runs/filter`.",
            "\"\"\"",
            "",
            "PREFECT_SERVER_API_HOST = Setting(",
            "    str,",
            "    default=\"127.0.0.1\",",
            ")",
            "\"\"\"The API's host address (defaults to `127.0.0.1`).\"\"\"",
            "",
            "PREFECT_SERVER_API_PORT = Setting(",
            "    int,",
            "    default=4200,",
            ")",
            "\"\"\"The API's port address (defaults to `4200`).\"\"\"",
            "",
            "PREFECT_SERVER_API_KEEPALIVE_TIMEOUT = Setting(",
            "    int,",
            "    default=5,",
            ")",
            "\"\"\"",
            "The API's keep alive timeout (defaults to `5`).",
            "Refer to https://www.uvicorn.org/settings/#timeouts for details.",
            "",
            "When the API is hosted behind a load balancer, you may want to set this to a value",
            "greater than the load balancer's idle timeout.",
            "",
            "Note this setting only applies when calling `prefect server start`; if hosting the",
            "API with another tool you will need to configure this there instead.",
            "\"\"\"",
            "",
            "PREFECT_SERVER_CSRF_PROTECTION_ENABLED = Setting(bool, default=False)",
            "\"\"\"",
            "Controls the activation of CSRF protection for the Prefect server API.",
            "",
            "When enabled (`True`), the server enforces CSRF validation checks on incoming",
            "state-changing requests (POST, PUT, PATCH, DELETE), requiring a valid CSRF",
            "token to be included in the request headers or body. This adds a layer of",
            "security by preventing unauthorized or malicious sites from making requests on",
            "behalf of authenticated users.",
            "",
            "It is recommended to enable this setting in production environments where the",
            "API is exposed to web clients to safeguard against CSRF attacks.",
            "",
            "Note: Enabling this setting requires corresponding support in the client for",
            "CSRF token management. See PREFECT_CLIENT_CSRF_SUPPORT_ENABLED for more.",
            "\"\"\"",
            "",
            "PREFECT_SERVER_CSRF_TOKEN_EXPIRATION = Setting(timedelta, default=timedelta(hours=1))",
            "\"\"\"",
            "Specifies the duration for which a CSRF token remains valid after being issued",
            "by the server.",
            "",
            "The default expiration time is set to 1 hour, which offers a reasonable",
            "compromise. Adjust this setting based on your specific security requirements",
            "and usage patterns.",
            "\"\"\"",
            "",
            "PREFECT_UI_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to serve the Prefect UI.\"\"\"",
            "",
            "PREFECT_UI_API_URL = Setting(",
            "    str,",
            "    default=None,",
            "    value_callback=default_ui_api_url,",
            ")",
            "\"\"\"The connection url for communication from the UI to the API.",
            "Defaults to `PREFECT_API_URL` if set. Otherwise, the default URL is generated from",
            "`PREFECT_SERVER_API_HOST` and `PREFECT_SERVER_API_PORT`. If providing a custom value,",
            "the aforementioned settings may be templated into the given string.",
            "\"\"\"",
            "",
            "PREFECT_SERVER_ANALYTICS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "When enabled, Prefect sends anonymous data (e.g. count of flow runs, package version)",
            "on server startup to help us improve our product.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_SCHEDULER_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the scheduling service in the server application.",
            "If disabled, you will need to run this service separately to schedule runs for deployments.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_LATE_RUNS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the late runs service in the server application.",
            "If disabled, you will need to run this service separately to have runs past their",
            "scheduled start time marked as late.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the flow run notifications service in the server application.",
            "If disabled, you will need to run this service separately to send flow run notifications.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the paused flow run expiration service in the server",
            "application. If disabled, paused flows that have timed out will remain in a Paused state",
            "until a resume attempt.",
            "\"\"\"",
            "",
            "PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH = Setting(int, default=2000)",
            "\"\"\"",
            "The maximum number of characters allowed for a task run cache key.",
            "This setting cannot be changed client-side, it must be set on the server.",
            "\"\"\"",
            "",
            "PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"Whether or not to start the cancellation cleanup service in the server",
            "application. If disabled, task runs and subflow runs belonging to cancelled flows may",
            "remain in non-terminal states.",
            "\"\"\"",
            "",
            "PREFECT_API_MAX_FLOW_RUN_GRAPH_NODES = Setting(int, default=10000)",
            "\"\"\"",
            "The maximum size of a flow run graph on the v2 API",
            "\"\"\"",
            "",
            "PREFECT_API_MAX_FLOW_RUN_GRAPH_ARTIFACTS = Setting(int, default=10000)",
            "\"\"\"",
            "The maximum number of artifacts to show on a flow run graph on the v2 API",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ARTIFACTS_ON_FLOW_RUN_GRAPH = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable artifacts on the flow run graph.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_STATES_ON_FLOW_RUN_GRAPH = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable flow run states on the flow run graph.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_EVENTS_CLIENT = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect work pools.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_EVENTS_CLIENT = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect work pools are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORK_POOLS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect work pools.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_WORK_POOLS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect work pools are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORKERS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect workers.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_WORKERS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect workers are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_VISUALIZE = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect visualize is used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ENHANCED_CANCELLATION = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental enhanced flow run cancellation.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ENHANCED_DEPLOYMENT_PARAMETERS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable enhanced deployment parameters.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_ENHANCED_CANCELLATION = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental enhanced flow run cancellation is used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_DEPLOYMENT_STATUS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable deployment status in the UI",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_DEPLOYMENT_STATUS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when deployment status is used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_FLOW_RUN_INPUT = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable flow run input.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INPUT = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable flow run input.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_PROCESS_LIMIT = Setting(int, default=5)",
            "\"\"\"",
            "Maximum number of processes a runner will execute in parallel.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_POLL_FREQUENCY = Setting(int, default=10)",
            "\"\"\"",
            "Number of seconds a runner should wait between queries for scheduled work.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_MISSED_POLLS_TOLERANCE = Setting(int, default=2)",
            "\"\"\"",
            "Number of missed polls before a runner is considered unhealthy by its webserver.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_HOST = Setting(str, default=\"localhost\")",
            "\"\"\"",
            "The host address the runner's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_PORT = Setting(int, default=8080)",
            "\"\"\"",
            "The port the runner's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_LOG_LEVEL = Setting(str, default=\"error\")",
            "\"\"\"",
            "The log level of the runner's webserver.",
            "\"\"\"",
            "",
            "PREFECT_RUNNER_SERVER_ENABLE = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable the runner's webserver.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_HEARTBEAT_SECONDS = Setting(float, default=30)",
            "\"\"\"",
            "Number of seconds a worker should wait between sending a heartbeat.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_QUERY_SECONDS = Setting(float, default=10)",
            "\"\"\"",
            "Number of seconds a worker should wait between queries for scheduled flow runs.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_PREFETCH_SECONDS = Setting(float, default=10)",
            "\"\"\"",
            "The number of seconds into the future a worker should query for scheduled flow runs.",
            "Can be used to compensate for infrastructure start up time for a worker.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_WEBSERVER_HOST = Setting(",
            "    str,",
            "    default=\"0.0.0.0\",",
            ")",
            "\"\"\"",
            "The host address the worker's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_WORKER_WEBSERVER_PORT = Setting(",
            "    int,",
            "    default=8080,",
            ")",
            "\"\"\"",
            "The port the worker's webserver should bind to.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_DEFAULT_STORAGE_BLOCK = Setting(",
            "    str,",
            "    default=\"local-file-system/prefect-task-scheduling\",",
            ")",
            "\"\"\"The `block-type/block-document` slug of a block to use as the default storage",
            "for autonomous tasks.\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_DELETE_FAILED_SUBMISSIONS = Setting(",
            "    bool,",
            "    default=True,",
            ")",
            "\"\"\"",
            "Whether or not to delete failed task submissions from the database.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_MAX_SCHEDULED_QUEUE_SIZE = Setting(",
            "    int,",
            "    default=1000,",
            ")",
            "\"\"\"",
            "The maximum number of scheduled tasks to queue for submission.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_MAX_RETRY_QUEUE_SIZE = Setting(",
            "    int,",
            "    default=100,",
            ")",
            "\"\"\"",
            "The maximum number of retries to queue for submission.",
            "\"\"\"",
            "",
            "PREFECT_TASK_SCHEDULING_PENDING_TASK_TIMEOUT = Setting(",
            "    timedelta,",
            "    default=timedelta(seconds=30),",
            ")",
            "\"\"\"",
            "How long before a PENDING task are made available to another task server.  In practice,",
            "a task server should move a task from PENDING to RUNNING very quickly, so runs stuck in",
            "PENDING for a while is a sign that the task server may have crashed.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_FLOW_RUN_INFRA_OVERRIDES = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable infrastructure overrides made on flow runs.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_FLOW_RUN_INFRA_OVERRIDES = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to warn infrastructure when experimental flow runs overrides are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_EXTRA_RUNNER_ENDPOINTS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable experimental worker webserver endpoints.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_ARTIFACTS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental Prefect artifacts.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_ARTIFACTS = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when experimental Prefect artifacts are used.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORKSPACE_DASHBOARD = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable the experimental workspace dashboard.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_WARN_WORKSPACE_DASHBOARD = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to warn when the experimental workspace dashboard is enabled.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_TASK_SCHEDULING = Setting(bool, default=False)",
            "\"\"\"",
            "Whether or not to enable experimental task scheduling.",
            "\"\"\"",
            "",
            "PREFECT_EXPERIMENTAL_ENABLE_WORK_QUEUE_STATUS = Setting(bool, default=True)",
            "\"\"\"",
            "Whether or not to enable experimental work queue status in-place of work queue health.",
            "\"\"\"",
            "",
            "",
            "# Defaults -----------------------------------------------------------------------------",
            "",
            "PREFECT_DEFAULT_RESULT_STORAGE_BLOCK = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"The `block-type/block-document` slug of a block to use as the default result storage.\"\"\"",
            "",
            "PREFECT_DEFAULT_WORK_POOL_NAME = Setting(str, default=None)",
            "\"\"\"",
            "The default work pool to deploy to.",
            "\"\"\"",
            "",
            "PREFECT_DEFAULT_DOCKER_BUILD_NAMESPACE = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"",
            "The default Docker namespace to use when building images.",
            "",
            "Can be either an organization/username or a registry URL with an organization/username.",
            "\"\"\"",
            "",
            "PREFECT_UI_SERVE_BASE = Setting(",
            "    str,",
            "    default=\"/\",",
            ")",
            "\"\"\"",
            "The base URL path to serve the Prefect UI from.",
            "",
            "Defaults to the root path.",
            "\"\"\"",
            "",
            "PREFECT_UI_STATIC_DIRECTORY = Setting(",
            "    str,",
            "    default=None,",
            ")",
            "\"\"\"",
            "The directory to serve static files from. This should be used when running into permissions issues",
            "when attempting to serve the UI from the default directory (for example when running in a Docker container)",
            "\"\"\"",
            "",
            "",
            "# Deprecated settings ------------------------------------------------------------------",
            "",
            "",
            "# Collect all defined settings ---------------------------------------------------------",
            "",
            "SETTING_VARIABLES = {",
            "    name: val for name, val in tuple(globals().items()) if isinstance(val, Setting)",
            "}",
            "",
            "# Populate names in settings objects from assignments above",
            "# Uses `__` to avoid setting these as global variables which can lead to sneaky bugs",
            "",
            "for __name, __setting in SETTING_VARIABLES.items():",
            "    __setting._name = __name",
            "",
            "# Dynamically create a pydantic model that includes all of our settings",
            "",
            "SettingsFieldsMixin = create_model(",
            "    \"SettingsFieldsMixin\",",
            "    # Inheriting from `BaseSettings` provides environment variable loading",
            "    __base__=BaseSettings,",
            "    **{",
            "        setting.name: (setting.type, setting.field)",
            "        for setting in SETTING_VARIABLES.values()",
            "    },",
            ")",
            "",
            "",
            "# Defining a class after this that inherits the dynamic class rather than setting",
            "# __base__ to the following class ensures that mkdocstrings properly generates",
            "# reference documentation. It does not support module-level variables, even if they are",
            "# an object which has __doc__ set.",
            "",
            "",
            "@add_cloudpickle_reduction",
            "class Settings(SettingsFieldsMixin):",
            "    \"\"\"",
            "    Contains validated Prefect settings.",
            "",
            "    Settings should be accessed using the relevant `Setting` object. For example:",
            "    ```python",
            "    from prefect.settings import PREFECT_HOME",
            "    PREFECT_HOME.value()",
            "    ```",
            "",
            "    Accessing a setting attribute directly will ignore any `value_callback` mutations.",
            "    This is not recommended:",
            "    ```python",
            "    from prefect.settings import Settings",
            "    Settings().PREFECT_PROFILES_PATH  # PosixPath('${PREFECT_HOME}/profiles.toml')",
            "    ```",
            "    \"\"\"",
            "",
            "    def value_of(self, setting: Setting[T], bypass_callback: bool = False) -> T:",
            "        \"\"\"",
            "        Retrieve a setting's value.",
            "        \"\"\"",
            "        value = getattr(self, setting.name)",
            "        if setting.value_callback and not bypass_callback:",
            "            value = setting.value_callback(self, value)",
            "        return value",
            "",
            "    @validator(PREFECT_LOGGING_LEVEL.name, PREFECT_LOGGING_SERVER_LEVEL.name)",
            "    def check_valid_log_level(cls, value):",
            "        if isinstance(value, str):",
            "            value = value.upper()",
            "        logging._checkLevel(value)",
            "        return value",
            "",
            "    @root_validator",
            "    def post_root_validators(cls, values):",
            "        \"\"\"",
            "        Add root validation functions for settings here.",
            "        \"\"\"",
            "        # TODO: We could probably register these dynamically but this is the simpler",
            "        #       approach for now. We can explore more interesting validation features",
            "        #       in the future.",
            "        values = max_log_size_smaller_than_batch_size(values)",
            "        values = warn_on_database_password_value_without_usage(values)",
            "        if not values[\"PREFECT_SILENCE_API_URL_MISCONFIGURATION\"]:",
            "            values = warn_on_misconfigured_api_url(values)",
            "        return values",
            "",
            "    def copy_with_update(",
            "        self,",
            "        updates: Mapping[Setting, Any] = None,",
            "        set_defaults: Mapping[Setting, Any] = None,",
            "        restore_defaults: Iterable[Setting] = None,",
            "    ) -> \"Settings\":",
            "        \"\"\"",
            "        Create a new `Settings` object with validation.",
            "",
            "        Arguments:",
            "            updates: A mapping of settings to new values. Existing values for the",
            "                given settings will be overridden.",
            "            set_defaults: A mapping of settings to new default values. Existing values for",
            "                the given settings will only be overridden if they were not set.",
            "            restore_defaults: An iterable of settings to restore to their default values.",
            "",
            "        Returns:",
            "            A new `Settings` object.",
            "        \"\"\"",
            "        updates = updates or {}",
            "        set_defaults = set_defaults or {}",
            "        restore_defaults = restore_defaults or set()",
            "        restore_defaults_names = {setting.name for setting in restore_defaults}",
            "",
            "        return self.__class__(",
            "            **{",
            "                **{setting.name: value for setting, value in set_defaults.items()},",
            "                **self.dict(exclude_unset=True, exclude=restore_defaults_names),",
            "                **{setting.name: value for setting, value in updates.items()},",
            "            }",
            "        )",
            "",
            "    def with_obfuscated_secrets(self):",
            "        \"\"\"",
            "        Returns a copy of this settings object with secret setting values obfuscated.",
            "        \"\"\"",
            "        settings = self.copy(",
            "            update={",
            "                setting.name: obfuscate(self.value_of(setting))",
            "                for setting in SETTING_VARIABLES.values()",
            "                if setting.is_secret",
            "                # Exclude deprecated settings with null values to avoid warnings",
            "                and not (setting.deprecated and self.value_of(setting) is None)",
            "            }",
            "        )",
            "        # Ensure that settings that have not been marked as \"set\" before are still so",
            "        # after we have updated their value above",
            "        settings.__fields_set__.intersection_update(self.__fields_set__)",
            "        return settings",
            "",
            "    def hash_key(self) -> str:",
            "        \"\"\"",
            "        Return a hash key for the settings object.  This is needed since some",
            "        settings may be unhashable.  An example is lists.",
            "        \"\"\"",
            "        env_variables = self.to_environment_variables()",
            "        return str(hash(tuple((key, value) for key, value in env_variables.items())))",
            "",
            "    def to_environment_variables(",
            "        self, include: Iterable[Setting] = None, exclude_unset: bool = False",
            "    ) -> Dict[str, str]:",
            "        \"\"\"",
            "        Convert the settings object to environment variables.",
            "",
            "        Note that setting values will not be run through their `value_callback` allowing",
            "        dynamic resolution to occur when loaded from the returned environment.",
            "",
            "        Args:",
            "            include_keys: An iterable of settings to include in the return value.",
            "                If not set, all settings are used.",
            "            exclude_unset: Only include settings that have been set (i.e. the value is",
            "                not from the default). If set, unset keys will be dropped even if they",
            "                are set in `include_keys`.",
            "",
            "        Returns:",
            "            A dictionary of settings with values cast to strings",
            "        \"\"\"",
            "        include = set(include or SETTING_VARIABLES.values())",
            "",
            "        if exclude_unset:",
            "            set_keys = {",
            "                # Collect all of the \"set\" keys and cast to `Setting` objects",
            "                SETTING_VARIABLES[key]",
            "                for key in self.dict(exclude_unset=True)",
            "            }",
            "            include.intersection_update(set_keys)",
            "",
            "        # Validate the types of items in `include` to prevent exclusion bugs",
            "        for key in include:",
            "            if not isinstance(key, Setting):",
            "                raise TypeError(",
            "                    \"Invalid type {type(key).__name__!r} for key in `include`.\"",
            "                )",
            "",
            "        env = {",
            "            # Use `getattr` instead of `value_of` to avoid value callback resolution",
            "            key: getattr(self, key)",
            "            for key, setting in SETTING_VARIABLES.items()",
            "            if setting in include",
            "        }",
            "",
            "        # Cast to strings and drop null values",
            "        return {key: str(value) for key, value in env.items() if value is not None}",
            "",
            "    class Config:",
            "        frozen = True",
            "",
            "",
            "# Functions to instantiate `Settings` instances",
            "",
            "_DEFAULTS_CACHE: Settings = None",
            "_FROM_ENV_CACHE: Dict[int, Settings] = {}",
            "",
            "",
            "def get_current_settings() -> Settings:",
            "    \"\"\"",
            "    Returns a settings object populated with values from the current settings context",
            "    or, if no settings context is active, the environment.",
            "    \"\"\"",
            "    from prefect.context import SettingsContext",
            "",
            "    settings_context = SettingsContext.get()",
            "    if settings_context is not None:",
            "        return settings_context.settings",
            "",
            "    return get_settings_from_env()",
            "",
            "",
            "def get_settings_from_env() -> Settings:",
            "    \"\"\"",
            "    Returns a settings object populated with default values and overrides from",
            "    environment variables, ignoring any values in profiles.",
            "",
            "    Calls with the same environment return a cached object instead of reconstructing",
            "    to avoid validation overhead.",
            "    \"\"\"",
            "    # Since os.environ is a Dict[str, str] we can safely hash it by contents, but we",
            "    # must be careful to avoid hashing a generator instead of a tuple",
            "    cache_key = hash(tuple((key, value) for key, value in os.environ.items()))",
            "",
            "    if cache_key not in _FROM_ENV_CACHE:",
            "        _FROM_ENV_CACHE[cache_key] = Settings()",
            "",
            "    return _FROM_ENV_CACHE[cache_key]",
            "",
            "",
            "def get_default_settings() -> Settings:",
            "    \"\"\"",
            "    Returns a settings object populated with default values, ignoring any overrides",
            "    from environment variables or profiles.",
            "",
            "    This is cached since the defaults should not change during the lifetime of the",
            "    module.",
            "    \"\"\"",
            "    global _DEFAULTS_CACHE",
            "",
            "    if not _DEFAULTS_CACHE:",
            "        old = os.environ",
            "        try:",
            "            os.environ = {}",
            "            settings = get_settings_from_env()",
            "        finally:",
            "            os.environ = old",
            "",
            "        _DEFAULTS_CACHE = settings",
            "",
            "    return _DEFAULTS_CACHE",
            "",
            "",
            "@contextmanager",
            "def temporary_settings(",
            "    updates: Mapping[Setting, Any] = None,",
            "    set_defaults: Mapping[Setting, Any] = None,",
            "    restore_defaults: Iterable[Setting] = None,",
            ") -> Settings:",
            "    \"\"\"",
            "    Temporarily override the current settings by entering a new profile.",
            "",
            "    See `Settings.copy_with_update` for details on different argument behavior.",
            "",
            "    Examples:",
            "        >>> from prefect.settings import PREFECT_API_URL",
            "        >>>",
            "        >>> with temporary_settings(updates={PREFECT_API_URL: \"foo\"}):",
            "        >>>    assert PREFECT_API_URL.value() == \"foo\"",
            "        >>>",
            "        >>>    with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"}):",
            "        >>>         assert PREFECT_API_URL.value() == \"foo\"",
            "        >>>",
            "        >>>    with temporary_settings(restore_defaults={PREFECT_API_URL}):",
            "        >>>         assert PREFECT_API_URL.value() is None",
            "        >>>",
            "        >>>         with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"})",
            "        >>>             assert PREFECT_API_URL.value() == \"bar\"",
            "        >>> assert PREFECT_API_URL.value() is None",
            "    \"\"\"",
            "    import prefect.context",
            "",
            "    context = prefect.context.get_settings_context()",
            "",
            "    new_settings = context.settings.copy_with_update(",
            "        updates=updates, set_defaults=set_defaults, restore_defaults=restore_defaults",
            "    )",
            "",
            "    with prefect.context.SettingsContext(",
            "        profile=context.profile, settings=new_settings",
            "    ):",
            "        yield new_settings",
            "",
            "",
            "class Profile(BaseModel):",
            "    \"\"\"",
            "    A user profile containing settings.",
            "    \"\"\"",
            "",
            "    name: str",
            "    settings: Dict[Setting, Any] = Field(default_factory=dict)",
            "    source: Optional[Path]",
            "",
            "    @validator(\"settings\", pre=True)",
            "    def map_names_to_settings(cls, value):",
            "        if value is None:",
            "            return value",
            "",
            "        # Cast string setting names to variables",
            "        validated = {}",
            "        for setting, val in value.items():",
            "            if isinstance(setting, str) and setting in SETTING_VARIABLES:",
            "                validated[SETTING_VARIABLES[setting]] = val",
            "            elif isinstance(setting, Setting):",
            "                validated[setting] = val",
            "            else:",
            "                raise ValueError(f\"Unknown setting {setting!r}.\")",
            "",
            "        return validated",
            "",
            "    def validate_settings(self) -> None:",
            "        \"\"\"",
            "        Validate the settings contained in this profile.",
            "",
            "        Raises:",
            "            pydantic.ValidationError: When settings do not have valid values.",
            "        \"\"\"",
            "        # Create a new `Settings` instance with the settings from this profile relying",
            "        # on Pydantic validation to raise an error.",
            "        # We do not return the `Settings` object because this is not the recommended",
            "        # path for constructing settings with a profile. See `use_profile` instead.",
            "        Settings(**{setting.name: value for setting, value in self.settings.items()})",
            "",
            "    def convert_deprecated_renamed_settings(self) -> List[Tuple[Setting, Setting]]:",
            "        \"\"\"",
            "        Update settings in place to replace deprecated settings with new settings when",
            "        renamed.",
            "",
            "        Returns a list of tuples with the old and new setting.",
            "        \"\"\"",
            "        changed = []",
            "        for setting in tuple(self.settings):",
            "            if (",
            "                setting.deprecated",
            "                and setting.deprecated_renamed_to",
            "                and setting.deprecated_renamed_to not in self.settings",
            "            ):",
            "                self.settings[setting.deprecated_renamed_to] = self.settings.pop(",
            "                    setting",
            "                )",
            "                changed.append((setting, setting.deprecated_renamed_to))",
            "        return changed",
            "",
            "    class Config:",
            "        arbitrary_types_allowed = True",
            "",
            "",
            "class ProfilesCollection:",
            "    \"\"\" \"",
            "    A utility class for working with a collection of profiles.",
            "",
            "    Profiles in the collection must have unique names.",
            "",
            "    The collection may store the name of the active profile.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self, profiles: Iterable[Profile], active: Optional[str] = None",
            "    ) -> None:",
            "        self.profiles_by_name = {profile.name: profile for profile in profiles}",
            "        self.active_name = active",
            "",
            "    @property",
            "    def names(self) -> Set[str]:",
            "        \"\"\"",
            "        Return a set of profile names in this collection.",
            "        \"\"\"",
            "        return set(self.profiles_by_name.keys())",
            "",
            "    @property",
            "    def active_profile(self) -> Optional[Profile]:",
            "        \"\"\"",
            "        Retrieve the active profile in this collection.",
            "        \"\"\"",
            "        if self.active_name is None:",
            "            return None",
            "        return self[self.active_name]",
            "",
            "    def set_active(self, name: Optional[str], check: bool = True):",
            "        \"\"\"",
            "        Set the active profile name in the collection.",
            "",
            "        A null value may be passed to indicate that this collection does not determine",
            "        the active profile.",
            "        \"\"\"",
            "        if check and name is not None and name not in self.names:",
            "            raise ValueError(f\"Unknown profile name {name!r}.\")",
            "        self.active_name = name",
            "",
            "    def update_profile(",
            "        self, name: str, settings: Mapping[Union[Dict, str], Any], source: Path = None",
            "    ) -> Profile:",
            "        \"\"\"",
            "        Add a profile to the collection or update the existing on if the name is already",
            "        present in this collection.",
            "",
            "        If updating an existing profile, the settings will be merged. Settings can",
            "        be dropped from the existing profile by setting them to `None` in the new",
            "        profile.",
            "",
            "        Returns the new profile object.",
            "        \"\"\"",
            "        existing = self.profiles_by_name.get(name)",
            "",
            "        # Convert the input to a `Profile` to cast settings to the correct type",
            "        profile = Profile(name=name, settings=settings, source=source)",
            "",
            "        if existing:",
            "            new_settings = {**existing.settings, **profile.settings}",
            "",
            "            # Drop null keys to restore to default",
            "            for key, value in tuple(new_settings.items()):",
            "                if value is None:",
            "                    new_settings.pop(key)",
            "",
            "            new_profile = Profile(",
            "                name=profile.name,",
            "                settings=new_settings,",
            "                source=source or profile.source,",
            "            )",
            "        else:",
            "            new_profile = profile",
            "",
            "        self.profiles_by_name[new_profile.name] = new_profile",
            "",
            "        return new_profile",
            "",
            "    def add_profile(self, profile: Profile) -> None:",
            "        \"\"\"",
            "        Add a profile to the collection.",
            "",
            "        If the profile name already exists, an exception will be raised.",
            "        \"\"\"",
            "        if profile.name in self.profiles_by_name:",
            "            raise ValueError(",
            "                f\"Profile name {profile.name!r} already exists in collection.\"",
            "            )",
            "",
            "        self.profiles_by_name[profile.name] = profile",
            "",
            "    def remove_profile(self, name: str) -> None:",
            "        \"\"\"",
            "        Remove a profile from the collection.",
            "        \"\"\"",
            "        self.profiles_by_name.pop(name)",
            "",
            "    def without_profile_source(self, path: Optional[Path]) -> \"ProfilesCollection\":",
            "        \"\"\"",
            "        Remove profiles that were loaded from a given path.",
            "",
            "        Returns a new collection.",
            "        \"\"\"",
            "        return ProfilesCollection(",
            "            [",
            "                profile",
            "                for profile in self.profiles_by_name.values()",
            "                if profile.source != path",
            "            ],",
            "            active=self.active_name,",
            "        )",
            "",
            "    def to_dict(self):",
            "        \"\"\"",
            "        Convert to a dictionary suitable for writing to disk.",
            "        \"\"\"",
            "        return {",
            "            \"active\": self.active_name,",
            "            \"profiles\": {",
            "                profile.name: {",
            "                    setting.name: value for setting, value in profile.settings.items()",
            "                }",
            "                for profile in self.profiles_by_name.values()",
            "            },",
            "        }",
            "",
            "    def __getitem__(self, name: str) -> Profile:",
            "        return self.profiles_by_name[name]",
            "",
            "    def __iter__(self):",
            "        return self.profiles_by_name.__iter__()",
            "",
            "    def items(self):",
            "        return self.profiles_by_name.items()",
            "",
            "    def __eq__(self, __o: object) -> bool:",
            "        if not isinstance(__o, ProfilesCollection):",
            "            return False",
            "",
            "        return (",
            "            self.profiles_by_name == __o.profiles_by_name",
            "            and self.active_name == __o.active_name",
            "        )",
            "",
            "    def __repr__(self) -> str:",
            "        return (",
            "            f\"ProfilesCollection(profiles={list(self.profiles_by_name.values())!r},\"",
            "            f\" active={self.active_name!r})>\"",
            "        )",
            "",
            "",
            "def _handle_removed_flags(profile_name: str, settings: dict) -> dict:",
            "    to_remove = [name for name in settings if name in REMOVED_EXPERIMENTAL_FLAGS]",
            "",
            "    for name in to_remove:",
            "        warnings.warn(",
            "            (",
            "                f\"Experimental flag {name!r} has been removed, please \"",
            "                f\"update your {profile_name!r} profile.\"",
            "            ),",
            "            UserWarning,",
            "            stacklevel=3,",
            "        )",
            "",
            "        settings.pop(name)",
            "",
            "    return settings",
            "",
            "",
            "def _read_profiles_from(path: Path) -> ProfilesCollection:",
            "    \"\"\"",
            "    Read profiles from a path into a new `ProfilesCollection`.",
            "",
            "    Profiles are expected to be written in TOML with the following schema:",
            "        ```",
            "        active = <name: Optional[str]>",
            "",
            "        [profiles.<name: str>]",
            "        <SETTING: str> = <value: Any>",
            "        ```",
            "    \"\"\"",
            "    contents = toml.loads(path.read_text())",
            "    active_profile = contents.get(\"active\")",
            "    raw_profiles = contents.get(\"profiles\", {})",
            "",
            "    profiles = []",
            "    for name, settings in raw_profiles.items():",
            "        settings = _handle_removed_flags(name, settings)",
            "        profiles.append(Profile(name=name, settings=settings, source=path))",
            "",
            "    return ProfilesCollection(profiles, active=active_profile)",
            "",
            "",
            "def _write_profiles_to(path: Path, profiles: ProfilesCollection) -> None:",
            "    \"\"\"",
            "    Write profiles in the given collection to a path as TOML.",
            "",
            "    Any existing data not present in the given `profiles` will be deleted.",
            "    \"\"\"",
            "    if not path.exists():",
            "        path.touch(mode=0o600)",
            "    return path.write_text(toml.dumps(profiles.to_dict()))",
            "",
            "",
            "def load_profiles() -> ProfilesCollection:",
            "    \"\"\"",
            "    Load all profiles from the default and current profile paths.",
            "    \"\"\"",
            "    profiles = _read_profiles_from(DEFAULT_PROFILES_PATH)",
            "",
            "    user_profiles_path = PREFECT_PROFILES_PATH.value()",
            "    if user_profiles_path.exists():",
            "        user_profiles = _read_profiles_from(user_profiles_path)",
            "",
            "        # Merge all of the user profiles with the defaults",
            "        for name in user_profiles:",
            "            profiles.update_profile(",
            "                name,",
            "                settings=user_profiles[name].settings,",
            "                source=user_profiles[name].source,",
            "            )",
            "",
            "        if user_profiles.active_name:",
            "            profiles.set_active(user_profiles.active_name, check=False)",
            "",
            "    return profiles",
            "",
            "",
            "def load_current_profile():",
            "    \"\"\"",
            "    Load the current profile from the default and current profile paths.",
            "",
            "    This will _not_ include settings from the current settings context. Only settings",
            "    that have been persisted to the profiles file will be saved.",
            "    \"\"\"",
            "    from prefect.context import SettingsContext",
            "",
            "    profiles = load_profiles()",
            "    context = SettingsContext.get()",
            "",
            "    if context:",
            "        profiles.set_active(context.profile.name)",
            "",
            "    return profiles.active_profile",
            "",
            "",
            "def save_profiles(profiles: ProfilesCollection) -> None:",
            "    \"\"\"",
            "    Writes all non-default profiles to the current profiles path.",
            "    \"\"\"",
            "    profiles_path = PREFECT_PROFILES_PATH.value()",
            "    profiles = profiles.without_profile_source(DEFAULT_PROFILES_PATH)",
            "    return _write_profiles_to(profiles_path, profiles)",
            "",
            "",
            "def load_profile(name: str) -> Profile:",
            "    \"\"\"",
            "    Load a single profile by name.",
            "    \"\"\"",
            "    profiles = load_profiles()",
            "    try:",
            "        return profiles[name]",
            "    except KeyError:",
            "        raise ValueError(f\"Profile {name!r} not found.\")",
            "",
            "",
            "def update_current_profile(settings: Dict[Union[str, Setting], Any]) -> Profile:",
            "    \"\"\"",
            "    Update the persisted data for the profile currently in-use.",
            "",
            "    If the profile does not exist in the profiles file, it will be created.",
            "",
            "    Given settings will be merged with the existing settings as described in",
            "    `ProfilesCollection.update_profile`.",
            "",
            "    Returns:",
            "        The new profile.",
            "    \"\"\"",
            "    import prefect.context",
            "",
            "    current_profile = prefect.context.get_settings_context().profile",
            "",
            "    if not current_profile:",
            "        raise MissingProfileError(\"No profile is currently in use.\")",
            "",
            "    profiles = load_profiles()",
            "",
            "    # Ensure the current profile's settings are present",
            "    profiles.update_profile(current_profile.name, current_profile.settings)",
            "    # Then merge the new settings in",
            "    new_profile = profiles.update_profile(current_profile.name, settings)",
            "",
            "    # Validate before saving",
            "    new_profile.validate_settings()",
            "",
            "    save_profiles(profiles)",
            "",
            "    return profiles[current_profile.name]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1211": [],
            "1212": [],
            "1215": []
        },
        "addLocation": []
    }
}