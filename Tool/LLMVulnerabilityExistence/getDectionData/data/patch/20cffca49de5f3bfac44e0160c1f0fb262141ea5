{
    "pulpcore/app/models/task.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from django.core.serializers.json import DjangoJSONEncoder"
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from django.db import connection, models"
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from django.utils import timezone"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from django_lifecycle import hook, AFTER_CREATE"
            },
            "4": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from pulpcore.app.models import ("
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": "     AutoAddObjPermsMixin,"
            },
            "7": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "         \"\"\""
            },
            "8": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 165,
                "PatchRowcode": "         return current_task.get()"
            },
            "9": {
                "beforePatchRowNumber": 165,
                "afterPatchRowNumber": 166,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+    @hook(AFTER_CREATE)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+    def add_role_dispatcher(self):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+        \"\"\"Set the \"core.task_user_dispatcher\" role for the current user after creation.\"\"\""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+        self.add_roles_for_object_creator(\"core.task_user_dispatcher\")"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+"
            },
            "15": {
                "beforePatchRowNumber": 166,
                "afterPatchRowNumber": 172,
                "PatchRowcode": "     def set_running(self):"
            },
            "16": {
                "beforePatchRowNumber": 167,
                "afterPatchRowNumber": 173,
                "PatchRowcode": "         \"\"\""
            },
            "17": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 174,
                "PatchRowcode": "         Set this Task to the running state, save it, and log output in warning cases."
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Django models related to the Tasking system",
            "\"\"\"",
            "",
            "import logging",
            "import traceback",
            "from contextlib import suppress",
            "from datetime import timedelta",
            "from gettext import gettext as _",
            "",
            "from django.conf import settings",
            "from django.contrib.postgres.fields import ArrayField, HStoreField",
            "from django.core.serializers.json import DjangoJSONEncoder",
            "from django.db import connection, models",
            "from django.utils import timezone",
            "",
            "from pulpcore.app.models import (",
            "    AutoAddObjPermsMixin,",
            "    BaseModel,",
            "    GenericRelationModel,",
            ")",
            "from pulpcore.app.models.status import BaseAppStatus",
            "from pulpcore.app.models.fields import EncryptedJSONField",
            "from pulpcore.constants import TASK_CHOICES, TASK_INCOMPLETE_STATES, TASK_STATES",
            "from pulpcore.exceptions import AdvisoryLockError, exception_to_dict",
            "from pulpcore.app.util import get_domain_pk, current_task",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "",
            "class Worker(BaseAppStatus):",
            "    \"\"\"",
            "    Represents a worker",
            "    \"\"\"",
            "",
            "    APP_TTL = timedelta(seconds=settings.WORKER_TTL)",
            "",
            "    @property",
            "    def current_task(self):",
            "        \"\"\"",
            "        The task this worker is currently executing, if any.",
            "",
            "        Returns:",
            "            Task: The currently executing task",
            "        \"\"\"",
            "        return self.tasks.filter(state=\"running\").first()",
            "",
            "",
            "def _uuid_to_advisory_lock(value):",
            "    return ((value >> 64) ^ value) & 0x7FFFFFFFFFFFFFFF",
            "",
            "",
            "class ProfileArtifact(BaseModel):",
            "    \"\"\"",
            "    A model encapsulating profiled artifact data",
            "    \"\"\"",
            "",
            "    artifact = models.ForeignKey(\"Artifact\", on_delete=models.CASCADE)",
            "    task = models.ForeignKey(\"Task\", on_delete=models.CASCADE)",
            "    name = models.TextField()",
            "",
            "    class Meta:",
            "        unique_together = (\"task\", \"name\")",
            "",
            "",
            "class TaskManager(models.Manager):",
            "    def get_queryset(self):",
            "        # Always make encrypted args deferred.",
            "        # This will prevent a lot of issues when the fernet key is lost.",
            "        # Only task workers need to be able to read these args anyway.",
            "        return super().get_queryset().defer(\"enc_args\", \"enc_kwargs\")",
            "",
            "",
            "class Task(BaseModel, AutoAddObjPermsMixin):",
            "    \"\"\"",
            "    Represents a task",
            "",
            "    Fields:",
            "",
            "        state (models.TextField): The state of the task",
            "        name (models.TextField): The name of the task",
            "        logging_cid (models.TextField): The logging CID associated with the task",
            "        started_at (models.DateTimeField): The time the task started executing",
            "        finished_at (models.DateTimeField): The time the task finished executing",
            "        error (models.JSONField): Fatal errors generated by the task",
            "        args (models.JSONField): The JSON serialized arguments for the task",
            "        kwargs (models.JSONField): The JSON serialized keyword arguments for",
            "            the task",
            "        reserved_resources_record (django.contrib.postgres.fields.ArrayField): The reserved",
            "            resources required for the task.",
            "",
            "    Relations:",
            "",
            "        parent (models.ForeignKey): Task that spawned this task (if any)",
            "        worker (models.ForeignKey): The worker that this task is in",
            "        pulp_domain (models.ForeignKey): The domain the Task is a part of",
            "    \"\"\"",
            "",
            "    objects = TaskManager()",
            "",
            "    state = models.TextField(choices=TASK_CHOICES)",
            "    name = models.TextField()",
            "    logging_cid = models.TextField(db_index=True)",
            "",
            "    unblocked_at = models.DateTimeField(null=True)",
            "    started_at = models.DateTimeField(null=True)",
            "    finished_at = models.DateTimeField(null=True)",
            "",
            "    error = models.JSONField(null=True)",
            "",
            "    enc_args = EncryptedJSONField(null=True, encoder=DjangoJSONEncoder)",
            "    enc_kwargs = EncryptedJSONField(null=True, encoder=DjangoJSONEncoder)",
            "",
            "    worker = models.ForeignKey(\"Worker\", null=True, related_name=\"tasks\", on_delete=models.SET_NULL)",
            "",
            "    parent_task = models.ForeignKey(",
            "        \"Task\", null=True, related_name=\"child_tasks\", on_delete=models.SET_NULL",
            "    )",
            "    task_group = models.ForeignKey(",
            "        \"TaskGroup\", null=True, related_name=\"tasks\", on_delete=models.SET_NULL",
            "    )",
            "    reserved_resources_record = ArrayField(models.TextField(), null=True)",
            "    pulp_domain = models.ForeignKey(\"Domain\", default=get_domain_pk, on_delete=models.CASCADE)",
            "    versions = HStoreField(default=dict)",
            "",
            "    profile_artifacts = models.ManyToManyField(\"Artifact\", through=ProfileArtifact)",
            "",
            "    def __str__(self):",
            "        return \"Task: {name} [{state}]\".format(name=self.name, state=self.state)",
            "",
            "    def __enter__(self):",
            "        self.lock = _uuid_to_advisory_lock(self.pk.int)",
            "        with connection.cursor() as cursor:",
            "            cursor.execute(\"SELECT pg_try_advisory_lock(%s)\", [self.lock])",
            "            acquired = cursor.fetchone()[0]",
            "        if not acquired:",
            "            raise AdvisoryLockError(\"Could not acquire lock.\")",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_value, traceback):",
            "        with connection.cursor() as cursor:",
            "            cursor.execute(\"SELECT pg_advisory_unlock(%s)\", [self.lock])",
            "            released = cursor.fetchone()[0]",
            "        if not released:",
            "            raise RuntimeError(\"Lock not held.\")",
            "",
            "    @staticmethod",
            "    def current_id():",
            "        \"\"\"",
            "        Returns:",
            "            uuid.UUID: The current task id.",
            "        \"\"\"",
            "        try:",
            "            return current_task.get().pk",
            "        except AttributeError:",
            "            return None",
            "",
            "    @staticmethod",
            "    def current():",
            "        \"\"\"",
            "        Returns:",
            "            pulpcore.app.models.Task: The current task.",
            "        \"\"\"",
            "        return current_task.get()",
            "",
            "    def set_running(self):",
            "        \"\"\"",
            "        Set this Task to the running state, save it, and log output in warning cases.",
            "",
            "        This updates the :attr:`started_at` and sets the :attr:`state` to :attr:`RUNNING`.",
            "        \"\"\"",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.WAITING).update(",
            "            state=TASK_STATES.RUNNING, started_at=timezone.now()",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(",
            "                _(\"Task set_running() occurred but Task {} is not WAITING\").format(self.pk)",
            "            )",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_completed(self):",
            "        \"\"\"",
            "        Set this Task to the completed state, save it, and log output in warning cases.",
            "",
            "        This updates the :attr:`finished_at` and sets the :attr:`state` to :attr:`COMPLETED`.",
            "        \"\"\"",
            "        # Only set the state to finished if it's running. This is important for when the task has",
            "        # been canceled, so we don't move the task from canceled to finished.",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.RUNNING).update(",
            "            state=TASK_STATES.COMPLETED, finished_at=timezone.now()",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(",
            "                _(\"Task set_completed() occurred but Task {} is not RUNNING.\").format(self.pk)",
            "            )",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_failed(self, exc, tb):",
            "        \"\"\"",
            "        Set this Task to the failed state and save it.",
            "",
            "        This updates the :attr:`finished_at` attribute, sets the :attr:`state` to",
            "        :attr:`FAILED`, and sets the :attr:`error` attribute.",
            "",
            "        Args:",
            "            exc (Exception): The exception raised by the task.",
            "            tb (traceback): Traceback instance for the current exception.",
            "        \"\"\"",
            "        tb_str = \"\".join(traceback.format_tb(tb))",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.RUNNING).update(",
            "            state=TASK_STATES.FAILED,",
            "            finished_at=timezone.now(),",
            "            error=exception_to_dict(exc, tb_str),",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(_(\"Attempt to set a not running task to failed.\"))",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_canceling(self):",
            "        \"\"\"",
            "        Set this task to canceling from either waiting, running or canceling.",
            "",
            "        This is the only valid transition without holding the task lock.",
            "        \"\"\"",
            "        rows = Task.objects.filter(pk=self.pk, state__in=TASK_INCOMPLETE_STATES).update(",
            "            state=TASK_STATES.CANCELING,",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(_(\"Attempt to cancel a finished task.\"))",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_canceled(self, final_state=TASK_STATES.CANCELED, reason=None):",
            "        \"\"\"",
            "        Set this task to canceled or failed from canceling.",
            "        \"\"\"",
            "        # Make sure this function was called with a proper final state",
            "        assert final_state in [TASK_STATES.CANCELED, TASK_STATES.FAILED]",
            "        task_data = {}",
            "        if reason:",
            "            task_data[\"error\"] = {\"reason\": reason}",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.CANCELING).update(",
            "            state=final_state,",
            "            finished_at=timezone.now(),",
            "            **task_data,",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(_(\"Attempt to mark a task canceled that is not in canceling state.\"))",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def unblock(self):",
            "        # This should be safe to be called without holding the lock.",
            "        Task.objects.filter(pk=self.pk).update(unblocked_at=timezone.now())",
            "        with suppress(AttributeError):",
            "            del self.unblocked_at",
            "",
            "    # Example taken from here:",
            "    # https://docs.djangoproject.com/en/3.2/ref/models/instances/#refreshing-objects-from-database",
            "    def refresh_from_db(self, using=None, fields=None, **kwargs):",
            "        # fields contains the name of the deferred field to be",
            "        # loaded.",
            "        if fields is not None:",
            "            fields = set(fields)",
            "            deferred_fields = {",
            "                field for field in self.get_deferred_fields() if not field.startswith(\"enc_\")",
            "            }",
            "            # If any state related deferred field is going to be loaded",
            "            if fields.intersection(deferred_fields):",
            "                # then load all of them",
            "                fields = fields.union(deferred_fields)",
            "        super().refresh_from_db(using, fields, **kwargs)",
            "",
            "    class Meta:",
            "        indexes = [",
            "            models.Index(fields=[\"pulp_created\"]),",
            "            models.Index(fields=[\"unblocked_at\"]),",
            "            models.Index(fields=[\"state\"]),",
            "            models.Index(fields=[\"state\", \"pulp_created\"]),",
            "        ]",
            "        permissions = [",
            "            (\"manage_roles_task\", \"Can manage role assignments on task\"),",
            "            (\"view_task_profile_artifacts\", \"Can view profile data for task\"),",
            "        ]",
            "",
            "",
            "class TaskGroup(BaseModel):",
            "    description = models.TextField()",
            "    all_tasks_dispatched = models.BooleanField(default=False)",
            "    pulp_domain = models.ForeignKey(\"Domain\", default=get_domain_pk, on_delete=models.CASCADE)",
            "",
            "    @staticmethod",
            "    def current():",
            "        \"\"\"",
            "        Returns:",
            "            pulpcore.app.models.TaskGroup: The task group the current task is being executed and",
            "            belongs to.",
            "        \"\"\"",
            "        try:",
            "            task_group = Task.current().task_group",
            "        except AttributeError:",
            "            task_group = None",
            "        return task_group",
            "",
            "    def finish(self):",
            "        \"\"\"",
            "        Finalize the task group.",
            "",
            "        Set 'all_tasks_dispatched' to True so that API users can know that there are no",
            "        tasks in the group yet to be created.",
            "        \"\"\"",
            "        self.all_tasks_dispatched = True",
            "        self.save()",
            "",
            "",
            "class CreatedResource(GenericRelationModel):",
            "    \"\"\"",
            "    Resources created by the task.",
            "",
            "    Relations:",
            "        task (models.ForeignKey): The task that created the resource.",
            "    \"\"\"",
            "",
            "    task = models.ForeignKey(",
            "        Task, related_name=\"created_resources\", default=Task.current, on_delete=models.CASCADE",
            "    )",
            "",
            "",
            "class TaskSchedule(BaseModel):",
            "    name = models.TextField(unique=True, null=False)",
            "    next_dispatch = models.DateTimeField(default=timezone.now, null=True)",
            "    dispatch_interval = models.DurationField(null=True)",
            "    task_name = models.TextField()",
            "    last_task = models.ForeignKey(Task, null=True, on_delete=models.SET_NULL)",
            "",
            "    class Meta:",
            "        permissions = [",
            "            (\"manage_roles_taskschedule\", \"Can manage role assignments on task schedules\"),",
            "        ]"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Django models related to the Tasking system",
            "\"\"\"",
            "",
            "import logging",
            "import traceback",
            "from contextlib import suppress",
            "from datetime import timedelta",
            "from gettext import gettext as _",
            "",
            "from django.conf import settings",
            "from django.contrib.postgres.fields import ArrayField, HStoreField",
            "from django.core.serializers.json import DjangoJSONEncoder",
            "from django.db import connection, models",
            "from django.utils import timezone",
            "from django_lifecycle import hook, AFTER_CREATE",
            "",
            "from pulpcore.app.models import (",
            "    AutoAddObjPermsMixin,",
            "    BaseModel,",
            "    GenericRelationModel,",
            ")",
            "from pulpcore.app.models.status import BaseAppStatus",
            "from pulpcore.app.models.fields import EncryptedJSONField",
            "from pulpcore.constants import TASK_CHOICES, TASK_INCOMPLETE_STATES, TASK_STATES",
            "from pulpcore.exceptions import AdvisoryLockError, exception_to_dict",
            "from pulpcore.app.util import get_domain_pk, current_task",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "",
            "class Worker(BaseAppStatus):",
            "    \"\"\"",
            "    Represents a worker",
            "    \"\"\"",
            "",
            "    APP_TTL = timedelta(seconds=settings.WORKER_TTL)",
            "",
            "    @property",
            "    def current_task(self):",
            "        \"\"\"",
            "        The task this worker is currently executing, if any.",
            "",
            "        Returns:",
            "            Task: The currently executing task",
            "        \"\"\"",
            "        return self.tasks.filter(state=\"running\").first()",
            "",
            "",
            "def _uuid_to_advisory_lock(value):",
            "    return ((value >> 64) ^ value) & 0x7FFFFFFFFFFFFFFF",
            "",
            "",
            "class ProfileArtifact(BaseModel):",
            "    \"\"\"",
            "    A model encapsulating profiled artifact data",
            "    \"\"\"",
            "",
            "    artifact = models.ForeignKey(\"Artifact\", on_delete=models.CASCADE)",
            "    task = models.ForeignKey(\"Task\", on_delete=models.CASCADE)",
            "    name = models.TextField()",
            "",
            "    class Meta:",
            "        unique_together = (\"task\", \"name\")",
            "",
            "",
            "class TaskManager(models.Manager):",
            "    def get_queryset(self):",
            "        # Always make encrypted args deferred.",
            "        # This will prevent a lot of issues when the fernet key is lost.",
            "        # Only task workers need to be able to read these args anyway.",
            "        return super().get_queryset().defer(\"enc_args\", \"enc_kwargs\")",
            "",
            "",
            "class Task(BaseModel, AutoAddObjPermsMixin):",
            "    \"\"\"",
            "    Represents a task",
            "",
            "    Fields:",
            "",
            "        state (models.TextField): The state of the task",
            "        name (models.TextField): The name of the task",
            "        logging_cid (models.TextField): The logging CID associated with the task",
            "        started_at (models.DateTimeField): The time the task started executing",
            "        finished_at (models.DateTimeField): The time the task finished executing",
            "        error (models.JSONField): Fatal errors generated by the task",
            "        args (models.JSONField): The JSON serialized arguments for the task",
            "        kwargs (models.JSONField): The JSON serialized keyword arguments for",
            "            the task",
            "        reserved_resources_record (django.contrib.postgres.fields.ArrayField): The reserved",
            "            resources required for the task.",
            "",
            "    Relations:",
            "",
            "        parent (models.ForeignKey): Task that spawned this task (if any)",
            "        worker (models.ForeignKey): The worker that this task is in",
            "        pulp_domain (models.ForeignKey): The domain the Task is a part of",
            "    \"\"\"",
            "",
            "    objects = TaskManager()",
            "",
            "    state = models.TextField(choices=TASK_CHOICES)",
            "    name = models.TextField()",
            "    logging_cid = models.TextField(db_index=True)",
            "",
            "    unblocked_at = models.DateTimeField(null=True)",
            "    started_at = models.DateTimeField(null=True)",
            "    finished_at = models.DateTimeField(null=True)",
            "",
            "    error = models.JSONField(null=True)",
            "",
            "    enc_args = EncryptedJSONField(null=True, encoder=DjangoJSONEncoder)",
            "    enc_kwargs = EncryptedJSONField(null=True, encoder=DjangoJSONEncoder)",
            "",
            "    worker = models.ForeignKey(\"Worker\", null=True, related_name=\"tasks\", on_delete=models.SET_NULL)",
            "",
            "    parent_task = models.ForeignKey(",
            "        \"Task\", null=True, related_name=\"child_tasks\", on_delete=models.SET_NULL",
            "    )",
            "    task_group = models.ForeignKey(",
            "        \"TaskGroup\", null=True, related_name=\"tasks\", on_delete=models.SET_NULL",
            "    )",
            "    reserved_resources_record = ArrayField(models.TextField(), null=True)",
            "    pulp_domain = models.ForeignKey(\"Domain\", default=get_domain_pk, on_delete=models.CASCADE)",
            "    versions = HStoreField(default=dict)",
            "",
            "    profile_artifacts = models.ManyToManyField(\"Artifact\", through=ProfileArtifact)",
            "",
            "    def __str__(self):",
            "        return \"Task: {name} [{state}]\".format(name=self.name, state=self.state)",
            "",
            "    def __enter__(self):",
            "        self.lock = _uuid_to_advisory_lock(self.pk.int)",
            "        with connection.cursor() as cursor:",
            "            cursor.execute(\"SELECT pg_try_advisory_lock(%s)\", [self.lock])",
            "            acquired = cursor.fetchone()[0]",
            "        if not acquired:",
            "            raise AdvisoryLockError(\"Could not acquire lock.\")",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_value, traceback):",
            "        with connection.cursor() as cursor:",
            "            cursor.execute(\"SELECT pg_advisory_unlock(%s)\", [self.lock])",
            "            released = cursor.fetchone()[0]",
            "        if not released:",
            "            raise RuntimeError(\"Lock not held.\")",
            "",
            "    @staticmethod",
            "    def current_id():",
            "        \"\"\"",
            "        Returns:",
            "            uuid.UUID: The current task id.",
            "        \"\"\"",
            "        try:",
            "            return current_task.get().pk",
            "        except AttributeError:",
            "            return None",
            "",
            "    @staticmethod",
            "    def current():",
            "        \"\"\"",
            "        Returns:",
            "            pulpcore.app.models.Task: The current task.",
            "        \"\"\"",
            "        return current_task.get()",
            "",
            "    @hook(AFTER_CREATE)",
            "    def add_role_dispatcher(self):",
            "        \"\"\"Set the \"core.task_user_dispatcher\" role for the current user after creation.\"\"\"",
            "        self.add_roles_for_object_creator(\"core.task_user_dispatcher\")",
            "",
            "    def set_running(self):",
            "        \"\"\"",
            "        Set this Task to the running state, save it, and log output in warning cases.",
            "",
            "        This updates the :attr:`started_at` and sets the :attr:`state` to :attr:`RUNNING`.",
            "        \"\"\"",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.WAITING).update(",
            "            state=TASK_STATES.RUNNING, started_at=timezone.now()",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(",
            "                _(\"Task set_running() occurred but Task {} is not WAITING\").format(self.pk)",
            "            )",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_completed(self):",
            "        \"\"\"",
            "        Set this Task to the completed state, save it, and log output in warning cases.",
            "",
            "        This updates the :attr:`finished_at` and sets the :attr:`state` to :attr:`COMPLETED`.",
            "        \"\"\"",
            "        # Only set the state to finished if it's running. This is important for when the task has",
            "        # been canceled, so we don't move the task from canceled to finished.",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.RUNNING).update(",
            "            state=TASK_STATES.COMPLETED, finished_at=timezone.now()",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(",
            "                _(\"Task set_completed() occurred but Task {} is not RUNNING.\").format(self.pk)",
            "            )",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_failed(self, exc, tb):",
            "        \"\"\"",
            "        Set this Task to the failed state and save it.",
            "",
            "        This updates the :attr:`finished_at` attribute, sets the :attr:`state` to",
            "        :attr:`FAILED`, and sets the :attr:`error` attribute.",
            "",
            "        Args:",
            "            exc (Exception): The exception raised by the task.",
            "            tb (traceback): Traceback instance for the current exception.",
            "        \"\"\"",
            "        tb_str = \"\".join(traceback.format_tb(tb))",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.RUNNING).update(",
            "            state=TASK_STATES.FAILED,",
            "            finished_at=timezone.now(),",
            "            error=exception_to_dict(exc, tb_str),",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(_(\"Attempt to set a not running task to failed.\"))",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_canceling(self):",
            "        \"\"\"",
            "        Set this task to canceling from either waiting, running or canceling.",
            "",
            "        This is the only valid transition without holding the task lock.",
            "        \"\"\"",
            "        rows = Task.objects.filter(pk=self.pk, state__in=TASK_INCOMPLETE_STATES).update(",
            "            state=TASK_STATES.CANCELING,",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(_(\"Attempt to cancel a finished task.\"))",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def set_canceled(self, final_state=TASK_STATES.CANCELED, reason=None):",
            "        \"\"\"",
            "        Set this task to canceled or failed from canceling.",
            "        \"\"\"",
            "        # Make sure this function was called with a proper final state",
            "        assert final_state in [TASK_STATES.CANCELED, TASK_STATES.FAILED]",
            "        task_data = {}",
            "        if reason:",
            "            task_data[\"error\"] = {\"reason\": reason}",
            "        rows = Task.objects.filter(pk=self.pk, state=TASK_STATES.CANCELING).update(",
            "            state=final_state,",
            "            finished_at=timezone.now(),",
            "            **task_data,",
            "        )",
            "        if rows != 1:",
            "            raise RuntimeError(_(\"Attempt to mark a task canceled that is not in canceling state.\"))",
            "        with suppress(AttributeError):",
            "            del self.state",
            "        with suppress(AttributeError):",
            "            del self.started_at",
            "        with suppress(AttributeError):",
            "            del self.finished_at",
            "        with suppress(AttributeError):",
            "            del self.error",
            "",
            "    def unblock(self):",
            "        # This should be safe to be called without holding the lock.",
            "        Task.objects.filter(pk=self.pk).update(unblocked_at=timezone.now())",
            "        with suppress(AttributeError):",
            "            del self.unblocked_at",
            "",
            "    # Example taken from here:",
            "    # https://docs.djangoproject.com/en/3.2/ref/models/instances/#refreshing-objects-from-database",
            "    def refresh_from_db(self, using=None, fields=None, **kwargs):",
            "        # fields contains the name of the deferred field to be",
            "        # loaded.",
            "        if fields is not None:",
            "            fields = set(fields)",
            "            deferred_fields = {",
            "                field for field in self.get_deferred_fields() if not field.startswith(\"enc_\")",
            "            }",
            "            # If any state related deferred field is going to be loaded",
            "            if fields.intersection(deferred_fields):",
            "                # then load all of them",
            "                fields = fields.union(deferred_fields)",
            "        super().refresh_from_db(using, fields, **kwargs)",
            "",
            "    class Meta:",
            "        indexes = [",
            "            models.Index(fields=[\"pulp_created\"]),",
            "            models.Index(fields=[\"unblocked_at\"]),",
            "            models.Index(fields=[\"state\"]),",
            "            models.Index(fields=[\"state\", \"pulp_created\"]),",
            "        ]",
            "        permissions = [",
            "            (\"manage_roles_task\", \"Can manage role assignments on task\"),",
            "            (\"view_task_profile_artifacts\", \"Can view profile data for task\"),",
            "        ]",
            "",
            "",
            "class TaskGroup(BaseModel):",
            "    description = models.TextField()",
            "    all_tasks_dispatched = models.BooleanField(default=False)",
            "    pulp_domain = models.ForeignKey(\"Domain\", default=get_domain_pk, on_delete=models.CASCADE)",
            "",
            "    @staticmethod",
            "    def current():",
            "        \"\"\"",
            "        Returns:",
            "            pulpcore.app.models.TaskGroup: The task group the current task is being executed and",
            "            belongs to.",
            "        \"\"\"",
            "        try:",
            "            task_group = Task.current().task_group",
            "        except AttributeError:",
            "            task_group = None",
            "        return task_group",
            "",
            "    def finish(self):",
            "        \"\"\"",
            "        Finalize the task group.",
            "",
            "        Set 'all_tasks_dispatched' to True so that API users can know that there are no",
            "        tasks in the group yet to be created.",
            "        \"\"\"",
            "        self.all_tasks_dispatched = True",
            "        self.save()",
            "",
            "",
            "class CreatedResource(GenericRelationModel):",
            "    \"\"\"",
            "    Resources created by the task.",
            "",
            "    Relations:",
            "        task (models.ForeignKey): The task that created the resource.",
            "    \"\"\"",
            "",
            "    task = models.ForeignKey(",
            "        Task, related_name=\"created_resources\", default=Task.current, on_delete=models.CASCADE",
            "    )",
            "",
            "",
            "class TaskSchedule(BaseModel):",
            "    name = models.TextField(unique=True, null=False)",
            "    next_dispatch = models.DateTimeField(default=timezone.now, null=True)",
            "    dispatch_interval = models.DurationField(null=True)",
            "    task_name = models.TextField()",
            "    last_task = models.ForeignKey(Task, null=True, on_delete=models.SET_NULL)",
            "",
            "    class Meta:",
            "        permissions = [",
            "            (\"manage_roles_taskschedule\", \"Can manage role assignments on task schedules\"),",
            "        ]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "web.pgadmin.tools.import_export.create_import_export_job",
            "pulpcore.app.models.task.Task.__enter__",
            "pulpcore.app.models.task.Task.self"
        ]
    },
    "pulpcore/app/viewsets/task.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 145,
                "PatchRowcode": "             ],"
            },
            "1": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 146,
                "PatchRowcode": "         },"
            },
            "2": {
                "beforePatchRowNumber": 147,
                "afterPatchRowNumber": 147,
                "PatchRowcode": "         \"core.task_viewer\": [\"core.view_task\"],"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 148,
                "PatchRowcode": "+        # This is a special role to designate the user who dispatched the task, it is assigned to"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+        # the user on the object-level at task creation. It is not meant to be edited or manually"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+        # added/removed from users."
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+        \"core.task_user_dispatcher\": [\"core.add_task\"],"
            },
            "7": {
                "beforePatchRowNumber": 148,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "     }"
            },
            "8": {
                "beforePatchRowNumber": 149,
                "afterPatchRowNumber": 153,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "     def get_serializer(self, *args, **kwargs):"
            },
            "10": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "                 ),"
            },
            "11": {
                "beforePatchRowNumber": 194,
                "afterPatchRowNumber": 198,
                "PatchRowcode": "                 Prefetch(\"child_tasks\", queryset=Task.objects.only(\"pk\")),"
            },
            "12": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": 199,
                "PatchRowcode": "             )"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+        if self.action in (\"add_role\", \"remove_role\"):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+            if \"core.task_user_dispatcher\" == self.request.data.get(\"role\"):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+                raise ValidationError("
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+                    _(\"core.task_user_dispatcher can not be added/removed from a task.\")"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+                )"
            },
            "18": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "         return qs"
            },
            "19": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 206,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "     @extend_schema("
            }
        },
        "frontPatchFile": [
            "from gettext import gettext as _",
            "",
            "from django.db.models import Prefetch",
            "from django_filters.rest_framework import filters",
            "from drf_spectacular.utils import extend_schema, inline_serializer",
            "from rest_framework import mixins, status",
            "from rest_framework.decorators import action",
            "from rest_framework.response import Response",
            "from rest_framework.serializers import DictField, URLField, ValidationError",
            "",
            "from pulpcore.filters import BaseFilterSet",
            "from pulpcore.app.models import (",
            "    ProfileArtifact,",
            "    Task,",
            "    TaskGroup,",
            "    TaskSchedule,",
            "    Worker,",
            "    CreatedResource,",
            "    RepositoryVersion,",
            ")",
            "from pulpcore.app.models.role import UserRole",
            "from pulpcore.app.response import OperationPostponedResponse",
            "from pulpcore.app.serializers import (",
            "    AsyncOperationResponseSerializer,",
            "    MinimalTaskSerializer,",
            "    PurgeSerializer,",
            "    TaskCancelSerializer,",
            "    TaskGroupSerializer,",
            "    TaskScheduleSerializer,",
            "    TaskSerializer,",
            "    WorkerSerializer,",
            ")",
            "from pulpcore.app.tasks import purge",
            "from pulpcore.app.util import get_domain, get_artifact_url",
            "from pulpcore.app.viewsets import NamedModelViewSet, RolesMixin",
            "from pulpcore.app.viewsets.base import DATETIME_FILTER_OPTIONS, NAME_FILTER_OPTIONS",
            "from pulpcore.app.viewsets.custom_filters import (",
            "    ReservedResourcesFilter,",
            "    ReservedResourcesInFilter,",
            "    CreatedResourcesFilter,",
            ")",
            "from pulpcore.constants import TASK_INCOMPLETE_STATES, TASK_STATES",
            "from pulpcore.tasking.tasks import dispatch, cancel_task",
            "",
            "",
            "class TaskFilter(BaseFilterSet):",
            "    created_resources = CreatedResourcesFilter()",
            "    # Non model field filters",
            "    reserved_resources = ReservedResourcesFilter(exclusive=True, shared=True)",
            "    reserved_resources__in = ReservedResourcesInFilter(exclusive=True, shared=True)",
            "    exclusive_resources = ReservedResourcesFilter(exclusive=True, shared=False)",
            "    exclusive_resources__in = ReservedResourcesInFilter(exclusive=True, shared=False)",
            "    shared_resources = ReservedResourcesFilter(exclusive=False, shared=True)",
            "    shared_resources__in = ReservedResourcesInFilter(exclusive=False, shared=True)",
            "",
            "    class Meta:",
            "        model = Task",
            "        fields = {",
            "            \"state\": [\"exact\", \"in\", \"ne\"],",
            "            \"worker\": [\"exact\", \"in\", \"isnull\"],",
            "            \"name\": [\"exact\", \"contains\", \"in\", \"ne\"],",
            "            \"logging_cid\": [\"exact\", \"contains\"],",
            "            \"started_at\": DATETIME_FILTER_OPTIONS,",
            "            \"finished_at\": DATETIME_FILTER_OPTIONS,",
            "            \"parent_task\": [\"exact\"],",
            "            \"child_tasks\": [\"exact\"],",
            "            \"task_group\": [\"exact\"],",
            "            \"created_resources\": [\"exact\"],",
            "        }",
            "",
            "",
            "class TaskViewSet(",
            "    NamedModelViewSet,",
            "    mixins.RetrieveModelMixin,",
            "    mixins.ListModelMixin,",
            "    mixins.DestroyModelMixin,",
            "    RolesMixin,",
            "):",
            "    queryset = Task.objects.all()",
            "    endpoint_name = \"tasks\"",
            "    filterset_class = TaskFilter",
            "    serializer_class = TaskSerializer",
            "    minimal_serializer_class = MinimalTaskSerializer",
            "    ordering = \"-pulp_created\"",
            "    queryset_filtering_required_permission = \"core.view_task\"",
            "",
            "    DEFAULT_ACCESS_POLICY = {",
            "        \"statements\": [",
            "            {\"action\": [\"list\"], \"principal\": \"authenticated\", \"effect\": \"allow\"},",
            "            {",
            "                \"action\": [\"retrieve\", \"my_permissions\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.view_task\",",
            "            },",
            "            {",
            "                \"action\": [\"profile_artifacts\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.view_task_profile_artifacts\",",
            "            },",
            "            {",
            "                \"action\": [\"destroy\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.delete_task\",",
            "            },",
            "            {",
            "                \"action\": [\"update\", \"partial_update\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.change_task\",",
            "            },",
            "            # 'purge' is filtered by current-user and core.delete_task permissions at the queryset",
            "            # level, and needs no extra protections here",
            "            {",
            "                \"action\": [\"purge\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "            },",
            "            {",
            "                \"action\": [\"list_roles\", \"add_role\", \"remove_role\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.manage_roles_task\",",
            "            },",
            "        ],",
            "        \"creation_hooks\": [",
            "            {",
            "                \"function\": \"add_roles_for_object_creator\",",
            "                \"parameters\": {\"roles\": \"core.task_owner\"},",
            "            }",
            "        ],",
            "        \"queryset_scoping\": {\"function\": \"scope_queryset\"},",
            "    }",
            "    LOCKED_ROLES = {",
            "        \"core.task_owner\": {",
            "            \"description\": \"Allow all actions on a task.\",",
            "            \"permissions\": [",
            "                \"core.view_task\",",
            "                \"core.view_task_profile_artifacts\",",
            "                \"core.change_task\",",
            "                \"core.delete_task\",",
            "                \"core.manage_roles_task\",",
            "            ],",
            "        },",
            "        \"core.task_viewer\": [\"core.view_task\"],",
            "    }",
            "",
            "    def get_serializer(self, *args, **kwargs):",
            "        \"\"\"Set repo_ver_mapping in serializer context for optimized serialization.\"\"\"",
            "        many = kwargs.get(\"many\", False)",
            "        serializer = super().get_serializer(*args, **kwargs)",
            "        # Perform optimization for list & retrieve actions and only during **full** serialization",
            "        # \"swagger_fake_view\" is set when drf_spectacular is building the OpenAPI spec",
            "        if (",
            "            self.action in (\"list\", \"retrieve\")",
            "            and self.get_serializer_class() == TaskSerializer",
            "            and not getattr(self, \"swagger_fake_view\", False)",
            "        ):",
            "            task_list = args[0] if many else [args[0]]",
            "            created_resources = [cr for task in task_list for cr in task.created_resources.all()]",
            "            # Optimization for RepositoryVersion created_resources",
            "            repo_ver_pks = [",
            "                cr.object_id",
            "                for cr in created_resources",
            "                if issubclass(cr.content_type.model_class(), RepositoryVersion)",
            "            ]",
            "            repo_vers = (",
            "                RepositoryVersion.objects.select_related(\"repository\")",
            "                .filter(pk__in=repo_ver_pks, complete=True)",
            "                .only(\"pk\", \"number\", \"repository__pulp_type\")",
            "            )",
            "            serializer.context[\"repo_ver_mapping\"] = {rv.pk: rv for rv in repo_vers}",
            "            # Assume, all tasks and related resources are of the same domain.",
            "            serializer.context[\"pulp_domain\"] = get_domain()",
            "            # Optimization for User created_by",
            "            task_pks = [t.pk for t in task_list]",
            "            user_roles = UserRole.objects.filter(",
            "                role__name=\"core.task_owner\", object_id__in=task_pks",
            "            ).order_by(\"-pulp_created\")",
            "            serializer.context[\"task_user_mapping\"] = {u.object_id: u.user_id for u in user_roles}",
            "        return serializer",
            "",
            "    def get_queryset(self):",
            "        qs = super().get_queryset()",
            "        if self.action in (\"list\", \"retrieve\"):",
            "            qs = qs.prefetch_related(",
            "                \"progress_reports\",",
            "                Prefetch(",
            "                    \"created_resources\",",
            "                    queryset=CreatedResource.objects.select_related(\"content_type\"),",
            "                ),",
            "                Prefetch(\"child_tasks\", queryset=Task.objects.only(\"pk\")),",
            "            )",
            "        return qs",
            "",
            "    @extend_schema(",
            "        description=\"This operation cancels a task.\",",
            "        summary=\"Cancel a task\",",
            "        operation_id=\"tasks_cancel\",",
            "        responses={200: TaskSerializer, 409: TaskSerializer},",
            "    )",
            "    def partial_update(self, request, pk=None, partial=True):",
            "        task = self.get_object()",
            "        if \"state\" not in request.data:",
            "            raise ValidationError(_(\"'state' must be provided with the request.\"))",
            "        if request.data[\"state\"] != \"canceled\":",
            "            raise ValidationError(_(\"The only acceptable value for 'state' is 'canceled'.\"))",
            "        task = cancel_task(task.pk)",
            "        # Check whether task is actually canceled",
            "        http_status = (",
            "            None",
            "            if task.state in [TASK_STATES.CANCELING, TASK_STATES.CANCELED]",
            "            else status.HTTP_409_CONFLICT",
            "        )",
            "        serializer = self.serializer_class(task, context={\"request\": request})",
            "        return Response(serializer.data, status=http_status)",
            "",
            "    def destroy(self, request, pk=None):",
            "        task = self.get_object()",
            "        if task.state in TASK_INCOMPLETE_STATES:",
            "            return Response(status=status.HTTP_409_CONFLICT)",
            "        return super().destroy(request, pk)",
            "",
            "    def get_serializer_class(self):",
            "        if self.action == \"partial_update\":",
            "            return TaskCancelSerializer",
            "        return super().get_serializer_class()",
            "",
            "    @extend_schema(",
            "        description=(",
            "            \"Trigger an asynchronous task that deletes completed tasks that finished prior\"",
            "            \" to a specified timestamp.\"",
            "        ),",
            "        summary=\"Purge Completed Tasks\",",
            "        operation_id=\"tasks_purge\",",
            "        request=PurgeSerializer,",
            "        responses={202: AsyncOperationResponseSerializer},",
            "    )",
            "    @action(detail=False, methods=[\"post\"])",
            "    def purge(self, request):",
            "        \"\"\"",
            "        Purge task-records for tasks in 'final' states.",
            "        \"\"\"",
            "        serializer = PurgeSerializer(data=request.data)",
            "        serializer.is_valid(raise_exception=True)",
            "        task = dispatch(",
            "            purge, args=[serializer.data[\"finished_before\"], list(serializer.data[\"states\"])]",
            "        )",
            "        return OperationPostponedResponse(task, request)",
            "",
            "    @extend_schema(",
            "        summary=_(\"Fetch downloadable links for profile artifacts\"),",
            "        responses=inline_serializer(",
            "            \"ProfileArtifactResponse\",",
            "            fields={\"urls\": DictField(child=URLField())},",
            "        ),",
            "    )",
            "    @action(detail=True)",
            "    def profile_artifacts(self, request, pk):",
            "        \"\"\"",
            "        Return pre-signed URLs used for downloading raw profile artifacts.",
            "        \"\"\"",
            "        task = self.get_object()",
            "        data = {}",
            "",
            "        for pa in ProfileArtifact.objects.select_related(\"artifact\").filter(task=task):",
            "            data[pa.name] = get_artifact_url(pa.artifact)",
            "",
            "        return Response({\"urls\": data})",
            "",
            "",
            "class TaskGroupViewSet(NamedModelViewSet, mixins.RetrieveModelMixin, mixins.ListModelMixin):",
            "    queryset = TaskGroup.objects.all()",
            "    endpoint_name = \"task-groups\"",
            "    serializer_class = TaskGroupSerializer",
            "    ordering = \"-pulp_created\"",
            "",
            "    DEFAULT_ACCESS_POLICY = {",
            "        \"statements\": [",
            "            {\"action\": [\"list\", \"retrieve\"], \"principal\": \"authenticated\", \"effect\": \"allow\"},",
            "        ],",
            "        \"queryset_scoping\": {\"function\": \"scope_queryset\"},",
            "    }",
            "",
            "    def scope_queryset(self, qs):",
            "        \"\"\"Filter based on having view permission on the parent task of the group.\"\"\"",
            "        if not self.request.user.is_superuser:",
            "            task_viewset = TaskViewSet()",
            "            setattr(task_viewset, \"request\", self.request)",
            "            setattr(task_viewset, \"action\", \"group\")  # Set this to avoid extra prefetch queries",
            "            tasks = (",
            "                task_viewset.get_queryset()",
            "                .filter(parent_task__isnull=True, task_group__isnull=False)",
            "                .values_list(\"pk\", flat=True)",
            "            )",
            "            qs = qs.filter(tasks__in=tasks)",
            "        return qs",
            "",
            "",
            "class WorkerFilter(BaseFilterSet):",
            "    online = filters.BooleanFilter(method=\"filter_online\")",
            "    missing = filters.BooleanFilter(method=\"filter_missing\")",
            "",
            "    class Meta:",
            "        model = Worker",
            "        fields = {",
            "            \"name\": NAME_FILTER_OPTIONS,",
            "            \"last_heartbeat\": DATETIME_FILTER_OPTIONS,",
            "        }",
            "",
            "    def filter_online(self, queryset, name, value):",
            "        online_workers = Worker.objects.online()",
            "",
            "        if value:",
            "            return queryset.filter(pk__in=online_workers)",
            "        else:",
            "            return queryset.exclude(pk__in=online_workers)",
            "",
            "    def filter_missing(self, queryset, name, value):",
            "        missing_workers = Worker.objects.missing()",
            "",
            "        if value:",
            "            return queryset.filter(pk__in=missing_workers)",
            "        else:",
            "            return queryset.exclude(pk__in=missing_workers)",
            "",
            "",
            "class WorkerViewSet(NamedModelViewSet, mixins.RetrieveModelMixin, mixins.ListModelMixin):",
            "    queryset = Worker.objects.all()",
            "    serializer_class = WorkerSerializer",
            "    endpoint_name = \"workers\"",
            "    http_method_names = [\"get\", \"options\"]",
            "    lookup_value_regex = \"[^/]+\"",
            "    filterset_class = WorkerFilter",
            "",
            "",
            "class TaskScheduleViewSet(",
            "    NamedModelViewSet,",
            "    mixins.RetrieveModelMixin,",
            "    mixins.ListModelMixin,",
            "    RolesMixin,",
            "):",
            "    \"\"\"",
            "    ViewSet to monitor task schedules.",
            "    \"\"\"",
            "",
            "    queryset = TaskSchedule.objects.all()",
            "    endpoint_name = \"task-schedules\"",
            "    filterset_fields = {",
            "        \"name\": [\"exact\", \"contains\"],",
            "        \"task_name\": [\"exact\", \"contains\"],",
            "    }",
            "    serializer_class = TaskScheduleSerializer",
            "    ordering = \"-pulp_created\"",
            "    queryset_filtering_required_permission = \"core.view_taskschedule\"",
            "",
            "    DEFAULT_ACCESS_POLICY = {",
            "        \"statements\": [",
            "            {\"action\": [\"list\"], \"principal\": \"authenticated\", \"effect\": \"allow\"},",
            "            {",
            "                \"action\": [\"retrieve\", \"my_permissions\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.view_taskschedule\",",
            "            },",
            "            {",
            "                \"action\": [\"list_roles\", \"add_role\", \"remove_role\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.manage_roles_taskschedule\",",
            "            },",
            "        ],",
            "        \"creation_hooks\": [",
            "            {",
            "                \"function\": \"add_roles_for_object_creator\",",
            "                \"parameters\": {\"roles\": \"core.taskschedule_owner\"},",
            "            }",
            "        ],",
            "        \"queryset_scoping\": {\"function\": \"scope_queryset\"},",
            "    }",
            "    LOCKED_ROLES = {",
            "        \"core.taskschedule_owner\": {",
            "            \"description\": \"Allow all actions on a taskschedule.\",",
            "            \"permissions\": [",
            "                \"core.view_taskschedule\",",
            "                \"core.manage_roles_taskschedule\",",
            "            ],",
            "        },",
            "        \"core.taskschedule_viewer\": [\"core.view_taskschedule\"],",
            "    }"
        ],
        "afterPatchFile": [
            "from gettext import gettext as _",
            "",
            "from django.db.models import Prefetch",
            "from django_filters.rest_framework import filters",
            "from drf_spectacular.utils import extend_schema, inline_serializer",
            "from rest_framework import mixins, status",
            "from rest_framework.decorators import action",
            "from rest_framework.response import Response",
            "from rest_framework.serializers import DictField, URLField, ValidationError",
            "",
            "from pulpcore.filters import BaseFilterSet",
            "from pulpcore.app.models import (",
            "    ProfileArtifact,",
            "    Task,",
            "    TaskGroup,",
            "    TaskSchedule,",
            "    Worker,",
            "    CreatedResource,",
            "    RepositoryVersion,",
            ")",
            "from pulpcore.app.models.role import UserRole",
            "from pulpcore.app.response import OperationPostponedResponse",
            "from pulpcore.app.serializers import (",
            "    AsyncOperationResponseSerializer,",
            "    MinimalTaskSerializer,",
            "    PurgeSerializer,",
            "    TaskCancelSerializer,",
            "    TaskGroupSerializer,",
            "    TaskScheduleSerializer,",
            "    TaskSerializer,",
            "    WorkerSerializer,",
            ")",
            "from pulpcore.app.tasks import purge",
            "from pulpcore.app.util import get_domain, get_artifact_url",
            "from pulpcore.app.viewsets import NamedModelViewSet, RolesMixin",
            "from pulpcore.app.viewsets.base import DATETIME_FILTER_OPTIONS, NAME_FILTER_OPTIONS",
            "from pulpcore.app.viewsets.custom_filters import (",
            "    ReservedResourcesFilter,",
            "    ReservedResourcesInFilter,",
            "    CreatedResourcesFilter,",
            ")",
            "from pulpcore.constants import TASK_INCOMPLETE_STATES, TASK_STATES",
            "from pulpcore.tasking.tasks import dispatch, cancel_task",
            "",
            "",
            "class TaskFilter(BaseFilterSet):",
            "    created_resources = CreatedResourcesFilter()",
            "    # Non model field filters",
            "    reserved_resources = ReservedResourcesFilter(exclusive=True, shared=True)",
            "    reserved_resources__in = ReservedResourcesInFilter(exclusive=True, shared=True)",
            "    exclusive_resources = ReservedResourcesFilter(exclusive=True, shared=False)",
            "    exclusive_resources__in = ReservedResourcesInFilter(exclusive=True, shared=False)",
            "    shared_resources = ReservedResourcesFilter(exclusive=False, shared=True)",
            "    shared_resources__in = ReservedResourcesInFilter(exclusive=False, shared=True)",
            "",
            "    class Meta:",
            "        model = Task",
            "        fields = {",
            "            \"state\": [\"exact\", \"in\", \"ne\"],",
            "            \"worker\": [\"exact\", \"in\", \"isnull\"],",
            "            \"name\": [\"exact\", \"contains\", \"in\", \"ne\"],",
            "            \"logging_cid\": [\"exact\", \"contains\"],",
            "            \"started_at\": DATETIME_FILTER_OPTIONS,",
            "            \"finished_at\": DATETIME_FILTER_OPTIONS,",
            "            \"parent_task\": [\"exact\"],",
            "            \"child_tasks\": [\"exact\"],",
            "            \"task_group\": [\"exact\"],",
            "            \"created_resources\": [\"exact\"],",
            "        }",
            "",
            "",
            "class TaskViewSet(",
            "    NamedModelViewSet,",
            "    mixins.RetrieveModelMixin,",
            "    mixins.ListModelMixin,",
            "    mixins.DestroyModelMixin,",
            "    RolesMixin,",
            "):",
            "    queryset = Task.objects.all()",
            "    endpoint_name = \"tasks\"",
            "    filterset_class = TaskFilter",
            "    serializer_class = TaskSerializer",
            "    minimal_serializer_class = MinimalTaskSerializer",
            "    ordering = \"-pulp_created\"",
            "    queryset_filtering_required_permission = \"core.view_task\"",
            "",
            "    DEFAULT_ACCESS_POLICY = {",
            "        \"statements\": [",
            "            {\"action\": [\"list\"], \"principal\": \"authenticated\", \"effect\": \"allow\"},",
            "            {",
            "                \"action\": [\"retrieve\", \"my_permissions\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.view_task\",",
            "            },",
            "            {",
            "                \"action\": [\"profile_artifacts\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.view_task_profile_artifacts\",",
            "            },",
            "            {",
            "                \"action\": [\"destroy\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.delete_task\",",
            "            },",
            "            {",
            "                \"action\": [\"update\", \"partial_update\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.change_task\",",
            "            },",
            "            # 'purge' is filtered by current-user and core.delete_task permissions at the queryset",
            "            # level, and needs no extra protections here",
            "            {",
            "                \"action\": [\"purge\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "            },",
            "            {",
            "                \"action\": [\"list_roles\", \"add_role\", \"remove_role\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.manage_roles_task\",",
            "            },",
            "        ],",
            "        \"creation_hooks\": [",
            "            {",
            "                \"function\": \"add_roles_for_object_creator\",",
            "                \"parameters\": {\"roles\": \"core.task_owner\"},",
            "            }",
            "        ],",
            "        \"queryset_scoping\": {\"function\": \"scope_queryset\"},",
            "    }",
            "    LOCKED_ROLES = {",
            "        \"core.task_owner\": {",
            "            \"description\": \"Allow all actions on a task.\",",
            "            \"permissions\": [",
            "                \"core.view_task\",",
            "                \"core.view_task_profile_artifacts\",",
            "                \"core.change_task\",",
            "                \"core.delete_task\",",
            "                \"core.manage_roles_task\",",
            "            ],",
            "        },",
            "        \"core.task_viewer\": [\"core.view_task\"],",
            "        # This is a special role to designate the user who dispatched the task, it is assigned to",
            "        # the user on the object-level at task creation. It is not meant to be edited or manually",
            "        # added/removed from users.",
            "        \"core.task_user_dispatcher\": [\"core.add_task\"],",
            "    }",
            "",
            "    def get_serializer(self, *args, **kwargs):",
            "        \"\"\"Set repo_ver_mapping in serializer context for optimized serialization.\"\"\"",
            "        many = kwargs.get(\"many\", False)",
            "        serializer = super().get_serializer(*args, **kwargs)",
            "        # Perform optimization for list & retrieve actions and only during **full** serialization",
            "        # \"swagger_fake_view\" is set when drf_spectacular is building the OpenAPI spec",
            "        if (",
            "            self.action in (\"list\", \"retrieve\")",
            "            and self.get_serializer_class() == TaskSerializer",
            "            and not getattr(self, \"swagger_fake_view\", False)",
            "        ):",
            "            task_list = args[0] if many else [args[0]]",
            "            created_resources = [cr for task in task_list for cr in task.created_resources.all()]",
            "            # Optimization for RepositoryVersion created_resources",
            "            repo_ver_pks = [",
            "                cr.object_id",
            "                for cr in created_resources",
            "                if issubclass(cr.content_type.model_class(), RepositoryVersion)",
            "            ]",
            "            repo_vers = (",
            "                RepositoryVersion.objects.select_related(\"repository\")",
            "                .filter(pk__in=repo_ver_pks, complete=True)",
            "                .only(\"pk\", \"number\", \"repository__pulp_type\")",
            "            )",
            "            serializer.context[\"repo_ver_mapping\"] = {rv.pk: rv for rv in repo_vers}",
            "            # Assume, all tasks and related resources are of the same domain.",
            "            serializer.context[\"pulp_domain\"] = get_domain()",
            "            # Optimization for User created_by",
            "            task_pks = [t.pk for t in task_list]",
            "            user_roles = UserRole.objects.filter(",
            "                role__name=\"core.task_owner\", object_id__in=task_pks",
            "            ).order_by(\"-pulp_created\")",
            "            serializer.context[\"task_user_mapping\"] = {u.object_id: u.user_id for u in user_roles}",
            "        return serializer",
            "",
            "    def get_queryset(self):",
            "        qs = super().get_queryset()",
            "        if self.action in (\"list\", \"retrieve\"):",
            "            qs = qs.prefetch_related(",
            "                \"progress_reports\",",
            "                Prefetch(",
            "                    \"created_resources\",",
            "                    queryset=CreatedResource.objects.select_related(\"content_type\"),",
            "                ),",
            "                Prefetch(\"child_tasks\", queryset=Task.objects.only(\"pk\")),",
            "            )",
            "        if self.action in (\"add_role\", \"remove_role\"):",
            "            if \"core.task_user_dispatcher\" == self.request.data.get(\"role\"):",
            "                raise ValidationError(",
            "                    _(\"core.task_user_dispatcher can not be added/removed from a task.\")",
            "                )",
            "        return qs",
            "",
            "    @extend_schema(",
            "        description=\"This operation cancels a task.\",",
            "        summary=\"Cancel a task\",",
            "        operation_id=\"tasks_cancel\",",
            "        responses={200: TaskSerializer, 409: TaskSerializer},",
            "    )",
            "    def partial_update(self, request, pk=None, partial=True):",
            "        task = self.get_object()",
            "        if \"state\" not in request.data:",
            "            raise ValidationError(_(\"'state' must be provided with the request.\"))",
            "        if request.data[\"state\"] != \"canceled\":",
            "            raise ValidationError(_(\"The only acceptable value for 'state' is 'canceled'.\"))",
            "        task = cancel_task(task.pk)",
            "        # Check whether task is actually canceled",
            "        http_status = (",
            "            None",
            "            if task.state in [TASK_STATES.CANCELING, TASK_STATES.CANCELED]",
            "            else status.HTTP_409_CONFLICT",
            "        )",
            "        serializer = self.serializer_class(task, context={\"request\": request})",
            "        return Response(serializer.data, status=http_status)",
            "",
            "    def destroy(self, request, pk=None):",
            "        task = self.get_object()",
            "        if task.state in TASK_INCOMPLETE_STATES:",
            "            return Response(status=status.HTTP_409_CONFLICT)",
            "        return super().destroy(request, pk)",
            "",
            "    def get_serializer_class(self):",
            "        if self.action == \"partial_update\":",
            "            return TaskCancelSerializer",
            "        return super().get_serializer_class()",
            "",
            "    @extend_schema(",
            "        description=(",
            "            \"Trigger an asynchronous task that deletes completed tasks that finished prior\"",
            "            \" to a specified timestamp.\"",
            "        ),",
            "        summary=\"Purge Completed Tasks\",",
            "        operation_id=\"tasks_purge\",",
            "        request=PurgeSerializer,",
            "        responses={202: AsyncOperationResponseSerializer},",
            "    )",
            "    @action(detail=False, methods=[\"post\"])",
            "    def purge(self, request):",
            "        \"\"\"",
            "        Purge task-records for tasks in 'final' states.",
            "        \"\"\"",
            "        serializer = PurgeSerializer(data=request.data)",
            "        serializer.is_valid(raise_exception=True)",
            "        task = dispatch(",
            "            purge, args=[serializer.data[\"finished_before\"], list(serializer.data[\"states\"])]",
            "        )",
            "        return OperationPostponedResponse(task, request)",
            "",
            "    @extend_schema(",
            "        summary=_(\"Fetch downloadable links for profile artifacts\"),",
            "        responses=inline_serializer(",
            "            \"ProfileArtifactResponse\",",
            "            fields={\"urls\": DictField(child=URLField())},",
            "        ),",
            "    )",
            "    @action(detail=True)",
            "    def profile_artifacts(self, request, pk):",
            "        \"\"\"",
            "        Return pre-signed URLs used for downloading raw profile artifacts.",
            "        \"\"\"",
            "        task = self.get_object()",
            "        data = {}",
            "",
            "        for pa in ProfileArtifact.objects.select_related(\"artifact\").filter(task=task):",
            "            data[pa.name] = get_artifact_url(pa.artifact)",
            "",
            "        return Response({\"urls\": data})",
            "",
            "",
            "class TaskGroupViewSet(NamedModelViewSet, mixins.RetrieveModelMixin, mixins.ListModelMixin):",
            "    queryset = TaskGroup.objects.all()",
            "    endpoint_name = \"task-groups\"",
            "    serializer_class = TaskGroupSerializer",
            "    ordering = \"-pulp_created\"",
            "",
            "    DEFAULT_ACCESS_POLICY = {",
            "        \"statements\": [",
            "            {\"action\": [\"list\", \"retrieve\"], \"principal\": \"authenticated\", \"effect\": \"allow\"},",
            "        ],",
            "        \"queryset_scoping\": {\"function\": \"scope_queryset\"},",
            "    }",
            "",
            "    def scope_queryset(self, qs):",
            "        \"\"\"Filter based on having view permission on the parent task of the group.\"\"\"",
            "        if not self.request.user.is_superuser:",
            "            task_viewset = TaskViewSet()",
            "            setattr(task_viewset, \"request\", self.request)",
            "            setattr(task_viewset, \"action\", \"group\")  # Set this to avoid extra prefetch queries",
            "            tasks = (",
            "                task_viewset.get_queryset()",
            "                .filter(parent_task__isnull=True, task_group__isnull=False)",
            "                .values_list(\"pk\", flat=True)",
            "            )",
            "            qs = qs.filter(tasks__in=tasks)",
            "        return qs",
            "",
            "",
            "class WorkerFilter(BaseFilterSet):",
            "    online = filters.BooleanFilter(method=\"filter_online\")",
            "    missing = filters.BooleanFilter(method=\"filter_missing\")",
            "",
            "    class Meta:",
            "        model = Worker",
            "        fields = {",
            "            \"name\": NAME_FILTER_OPTIONS,",
            "            \"last_heartbeat\": DATETIME_FILTER_OPTIONS,",
            "        }",
            "",
            "    def filter_online(self, queryset, name, value):",
            "        online_workers = Worker.objects.online()",
            "",
            "        if value:",
            "            return queryset.filter(pk__in=online_workers)",
            "        else:",
            "            return queryset.exclude(pk__in=online_workers)",
            "",
            "    def filter_missing(self, queryset, name, value):",
            "        missing_workers = Worker.objects.missing()",
            "",
            "        if value:",
            "            return queryset.filter(pk__in=missing_workers)",
            "        else:",
            "            return queryset.exclude(pk__in=missing_workers)",
            "",
            "",
            "class WorkerViewSet(NamedModelViewSet, mixins.RetrieveModelMixin, mixins.ListModelMixin):",
            "    queryset = Worker.objects.all()",
            "    serializer_class = WorkerSerializer",
            "    endpoint_name = \"workers\"",
            "    http_method_names = [\"get\", \"options\"]",
            "    lookup_value_regex = \"[^/]+\"",
            "    filterset_class = WorkerFilter",
            "",
            "",
            "class TaskScheduleViewSet(",
            "    NamedModelViewSet,",
            "    mixins.RetrieveModelMixin,",
            "    mixins.ListModelMixin,",
            "    RolesMixin,",
            "):",
            "    \"\"\"",
            "    ViewSet to monitor task schedules.",
            "    \"\"\"",
            "",
            "    queryset = TaskSchedule.objects.all()",
            "    endpoint_name = \"task-schedules\"",
            "    filterset_fields = {",
            "        \"name\": [\"exact\", \"contains\"],",
            "        \"task_name\": [\"exact\", \"contains\"],",
            "    }",
            "    serializer_class = TaskScheduleSerializer",
            "    ordering = \"-pulp_created\"",
            "    queryset_filtering_required_permission = \"core.view_taskschedule\"",
            "",
            "    DEFAULT_ACCESS_POLICY = {",
            "        \"statements\": [",
            "            {\"action\": [\"list\"], \"principal\": \"authenticated\", \"effect\": \"allow\"},",
            "            {",
            "                \"action\": [\"retrieve\", \"my_permissions\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.view_taskschedule\",",
            "            },",
            "            {",
            "                \"action\": [\"list_roles\", \"add_role\", \"remove_role\"],",
            "                \"principal\": \"authenticated\",",
            "                \"effect\": \"allow\",",
            "                \"condition\": \"has_model_or_domain_or_obj_perms:core.manage_roles_taskschedule\",",
            "            },",
            "        ],",
            "        \"creation_hooks\": [",
            "            {",
            "                \"function\": \"add_roles_for_object_creator\",",
            "                \"parameters\": {\"roles\": \"core.taskschedule_owner\"},",
            "            }",
            "        ],",
            "        \"queryset_scoping\": {\"function\": \"scope_queryset\"},",
            "    }",
            "    LOCKED_ROLES = {",
            "        \"core.taskschedule_owner\": {",
            "            \"description\": \"Allow all actions on a taskschedule.\",",
            "            \"permissions\": [",
            "                \"core.view_taskschedule\",",
            "                \"core.manage_roles_taskschedule\",",
            "            ],",
            "        },",
            "        \"core.taskschedule_viewer\": [\"core.view_taskschedule\"],",
            "    }"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "pulpcore.app.viewsets.task.TaskGroupViewSet.scope_queryset.task_viewset",
            "pulpcore.app.viewsets.task.TaskViewSet.LOCKED_ROLES",
            "pulpcore.app.viewsets.task.TaskGroupViewSet.scope_queryset",
            "web.pgadmin.tools.import_export.create_import_export_job",
            "pulpcore.app.viewsets.task.TaskViewSet.DEFAULT_ACCESS_POLICY",
            "pulpcore.app.viewsets.task.TaskViewSet.self"
        ]
    },
    "pulpcore/tasking/_util.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "     connection.connection = None"
            },
            "1": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     # enc_args and enc_kwargs are deferred by default but we actually want them"
            },
            "2": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     task = Task.objects.defer(None).select_related(\"pulp_domain\").get(pk=task_pk)"
            },
            "3": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    user = get_users_with_perms(task, with_group_users=False).first()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+    # These queries were specifically constructed and ordered this way to ensure we have the highest"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+    # chance of getting the user who dispatched the task since we don't have a user relation on the"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+    # task model. The second query acts as a fallback to provide ZDU support. Future changes will"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+    # require to keep these around till a breaking change release is planned (3.70 the earliest)."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+    user = ("
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+        get_users_with_perms("
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+            task,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+            only_with_perms_in=[\"core.add_task\"],"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+            with_group_users=False,"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+            include_model_permissions=False,"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+            include_domain_permissions=False,"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+        ).first()"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+        or get_users_with_perms("
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+            task,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+            only_with_perms_in=[\"core.manage_roles_task\"],"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+            with_group_users=False,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+            include_model_permissions=False,"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+            include_domain_permissions=False,"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+        ).first()"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+    )"
            },
            "24": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "     # Isolate from the parent asyncio."
            },
            "25": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "     asyncio.set_event_loop(asyncio.new_event_loop())"
            },
            "26": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "     # Set current contexts"
            }
        },
        "frontPatchFile": [
            "import asyncio",
            "import importlib",
            "import logging",
            "import os",
            "import resource",
            "import signal",
            "import sys",
            "import threading",
            "import time",
            "import tempfile",
            "from gettext import gettext as _",
            "",
            "from contextlib import suppress",
            "",
            "from django.conf import settings",
            "from django.db import connection, transaction, IntegrityError",
            "from django.db.models import Q",
            "from django.utils import timezone",
            "from django_guid import set_guid",
            "from django_guid.utils import generate_guid",
            "from pulpcore.app.models import Artifact, Content, Task, TaskSchedule, ProfileArtifact",
            "from pulpcore.app.role_util import get_users_with_perms",
            "from pulpcore.app.util import (",
            "    set_current_user,",
            "    set_domain,",
            "    configure_analytics,",
            "    configure_cleanup,",
            ")",
            "from pulpcore.constants import TASK_FINAL_STATES, TASK_STATES",
            "from pulpcore.tasking.tasks import dispatch, execute_task",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "",
            "def startup_hook():",
            "    configure_analytics()",
            "    configure_cleanup()",
            "",
            "",
            "def delete_incomplete_resources(task):",
            "    \"\"\"",
            "    Delete all incomplete created-resources on a canceled task.",
            "",
            "    Args:",
            "        task (Task): A task.",
            "    \"\"\"",
            "    if task.state != TASK_STATES.CANCELING:",
            "        raise RuntimeError(_(\"Task must be canceling.\"))",
            "    for model in (r.content_object for r in task.created_resources.all()):",
            "        if isinstance(model, (Artifact, Content)):",
            "            continue",
            "        try:",
            "            if model.complete:",
            "                continue",
            "        except AttributeError:",
            "            continue",
            "        try:",
            "            with transaction.atomic():",
            "                model.delete()",
            "        except Exception as error:",
            "            _logger.error(_(\"Delete created resource, failed: {}\").format(str(error)))",
            "",
            "",
            "def write_memory_usage(stop_event, path):",
            "    with open(path, \"w\") as file:",
            "        file.write(\"# Seconds\\tMemory in MB\\n\")",
            "        seconds = 0",
            "        while not stop_event.is_set():",
            "            current_mb_in_use = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024",
            "            file.write(f\"{seconds}\\t{current_mb_in_use:.2f}\\n\")",
            "            file.flush()",
            "            time.sleep(5)",
            "            seconds += 5",
            "",
            "",
            "def child_signal_handler(sig, frame):",
            "    _logger.debug(\"Signal %s recieved by %s.\", sig, os.getpid())",
            "    # Reset signal handlers to default",
            "    # If you kill the process a second time it's not graceful anymore.",
            "    signal.signal(signal.SIGINT, signal.SIG_DFL)",
            "    signal.signal(signal.SIGTERM, signal.SIG_DFL)",
            "    signal.signal(signal.SIGHUP, signal.SIG_DFL)",
            "    signal.signal(signal.SIGUSR1, signal.SIG_DFL)",
            "",
            "    if sig == signal.SIGUSR1:",
            "        sys.exit()",
            "",
            "",
            "def perform_task(task_pk, task_working_dir_rel_path):",
            "    \"\"\"Setup the environment to handle a task and execute it.",
            "    This must be called as a subprocess, while the parent holds the advisory lock of the task.\"\"\"",
            "    signal.signal(signal.SIGINT, child_signal_handler)",
            "    signal.signal(signal.SIGTERM, child_signal_handler)",
            "    signal.signal(signal.SIGHUP, child_signal_handler)",
            "    signal.signal(signal.SIGUSR1, child_signal_handler)",
            "    # All processes need to create their own postgres connection",
            "    connection.connection = None",
            "    # enc_args and enc_kwargs are deferred by default but we actually want them",
            "    task = Task.objects.defer(None).select_related(\"pulp_domain\").get(pk=task_pk)",
            "    user = get_users_with_perms(task, with_group_users=False).first()",
            "    # Isolate from the parent asyncio.",
            "    asyncio.set_event_loop(asyncio.new_event_loop())",
            "    # Set current contexts",
            "    set_guid(task.logging_cid)",
            "    set_current_user(user)",
            "    set_domain(task.pulp_domain)",
            "    os.chdir(task_working_dir_rel_path)",
            "",
            "    if settings.TASK_DIAGNOSTICS:",
            "        _execute_task_and_profile(task)",
            "    else:",
            "        execute_task(task)",
            "",
            "",
            "def _execute_task_and_profile(task):",
            "    with tempfile.TemporaryDirectory(dir=settings.WORKING_DIRECTORY) as temp_dir:",
            "        pyinstrument_func = _pyinstrument_diagnostic_decorator(temp_dir, execute_task)",
            "        memory_func = _memory_diagnostic_decorator(temp_dir, pyinstrument_func)",
            "",
            "        memory_func(task)",
            "",
            "",
            "def _memory_diagnostic_decorator(temp_dir, func):",
            "    def __memory_diagnostic_decorator(task):",
            "        mem_diagnostics_file_path = os.path.join(temp_dir, \"memory.datum\")",
            "        # It would be better to have this recording happen in the parent process instead of here",
            "        # https://github.com/pulp/pulpcore/issues/2337",
            "        stop_event = threading.Event()",
            "        mem_diagnostics_thread = threading.Thread(",
            "            target=write_memory_usage, args=(stop_event, mem_diagnostics_file_path), daemon=True",
            "        )",
            "        mem_diagnostics_thread.start()",
            "",
            "        func(task)",
            "",
            "        stop_event.set()",
            "        artifact = Artifact.init_and_validate(mem_diagnostics_file_path)",
            "        with suppress(IntegrityError):",
            "            artifact.save()",
            "",
            "        ProfileArtifact.objects.get_or_create(artifact=artifact, name=\"memory_profile\", task=task)",
            "",
            "        _logger.info(\"Created memory diagnostic data.\")",
            "",
            "    return __memory_diagnostic_decorator",
            "",
            "",
            "def _pyinstrument_diagnostic_decorator(temp_dir, func):",
            "    def __pyinstrument_diagnostic_decorator(task):",
            "        if importlib.util.find_spec(\"pyinstrument\") is not None:",
            "            from pyinstrument import Profiler",
            "",
            "            with Profiler() as profiler:",
            "                func(task)",
            "",
            "            profile_file_path = os.path.join(temp_dir, \"pyinstrument.html\")",
            "            with open(profile_file_path, \"w+\") as f:",
            "                f.write(profiler.output_html())",
            "                f.flush()",
            "",
            "            artifact = Artifact.init_and_validate(str(profile_file_path))",
            "            with suppress(IntegrityError):",
            "                artifact.save()",
            "",
            "            ProfileArtifact.objects.get_or_create(",
            "                artifact=artifact, name=\"pyinstrument_data\", task=task",
            "            )",
            "",
            "            _logger.info(\"Created pyinstrument profile data.\")",
            "        else:",
            "            func(task)",
            "",
            "    return __pyinstrument_diagnostic_decorator",
            "",
            "",
            "def dispatch_scheduled_tasks():",
            "    # Warning, dispatch_scheduled_tasks is not race condition free!",
            "    now = timezone.now()",
            "    # Dispatch all tasks old enough and not still running",
            "    for task_schedule in TaskSchedule.objects.filter(next_dispatch__lte=now).filter(",
            "        Q(last_task=None) | Q(last_task__state__in=TASK_FINAL_STATES)",
            "    ):",
            "        try:",
            "            if task_schedule.dispatch_interval is None:",
            "                # This was a timed one shot task schedule",
            "                task_schedule.next_dispatch = None",
            "            else:",
            "                # This is a recurring task schedule",
            "                while task_schedule.next_dispatch < now:",
            "                    # Do not schedule in the past",
            "                    task_schedule.next_dispatch += task_schedule.dispatch_interval",
            "            set_guid(generate_guid())",
            "            with transaction.atomic():",
            "                task_schedule.last_task = dispatch(",
            "                    task_schedule.task_name,",
            "                )",
            "                task_schedule.save(update_fields=[\"next_dispatch\", \"last_task\"])",
            "",
            "            _logger.info(",
            "                \"Dispatched scheduled task {task_name} as task id {task_id}\".format(",
            "                    task_name=task_schedule.task_name, task_id=task_schedule.last_task.pk",
            "                )",
            "            )",
            "        except Exception as e:",
            "            _logger.warning(",
            "                \"Dispatching scheduled task {task_name} failed. {error}\".format(",
            "                    task_name=task_schedule.task_name, error=str(e)",
            "                )",
            "            )"
        ],
        "afterPatchFile": [
            "import asyncio",
            "import importlib",
            "import logging",
            "import os",
            "import resource",
            "import signal",
            "import sys",
            "import threading",
            "import time",
            "import tempfile",
            "from gettext import gettext as _",
            "",
            "from contextlib import suppress",
            "",
            "from django.conf import settings",
            "from django.db import connection, transaction, IntegrityError",
            "from django.db.models import Q",
            "from django.utils import timezone",
            "from django_guid import set_guid",
            "from django_guid.utils import generate_guid",
            "from pulpcore.app.models import Artifact, Content, Task, TaskSchedule, ProfileArtifact",
            "from pulpcore.app.role_util import get_users_with_perms",
            "from pulpcore.app.util import (",
            "    set_current_user,",
            "    set_domain,",
            "    configure_analytics,",
            "    configure_cleanup,",
            ")",
            "from pulpcore.constants import TASK_FINAL_STATES, TASK_STATES",
            "from pulpcore.tasking.tasks import dispatch, execute_task",
            "",
            "_logger = logging.getLogger(__name__)",
            "",
            "",
            "def startup_hook():",
            "    configure_analytics()",
            "    configure_cleanup()",
            "",
            "",
            "def delete_incomplete_resources(task):",
            "    \"\"\"",
            "    Delete all incomplete created-resources on a canceled task.",
            "",
            "    Args:",
            "        task (Task): A task.",
            "    \"\"\"",
            "    if task.state != TASK_STATES.CANCELING:",
            "        raise RuntimeError(_(\"Task must be canceling.\"))",
            "    for model in (r.content_object for r in task.created_resources.all()):",
            "        if isinstance(model, (Artifact, Content)):",
            "            continue",
            "        try:",
            "            if model.complete:",
            "                continue",
            "        except AttributeError:",
            "            continue",
            "        try:",
            "            with transaction.atomic():",
            "                model.delete()",
            "        except Exception as error:",
            "            _logger.error(_(\"Delete created resource, failed: {}\").format(str(error)))",
            "",
            "",
            "def write_memory_usage(stop_event, path):",
            "    with open(path, \"w\") as file:",
            "        file.write(\"# Seconds\\tMemory in MB\\n\")",
            "        seconds = 0",
            "        while not stop_event.is_set():",
            "            current_mb_in_use = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024",
            "            file.write(f\"{seconds}\\t{current_mb_in_use:.2f}\\n\")",
            "            file.flush()",
            "            time.sleep(5)",
            "            seconds += 5",
            "",
            "",
            "def child_signal_handler(sig, frame):",
            "    _logger.debug(\"Signal %s recieved by %s.\", sig, os.getpid())",
            "    # Reset signal handlers to default",
            "    # If you kill the process a second time it's not graceful anymore.",
            "    signal.signal(signal.SIGINT, signal.SIG_DFL)",
            "    signal.signal(signal.SIGTERM, signal.SIG_DFL)",
            "    signal.signal(signal.SIGHUP, signal.SIG_DFL)",
            "    signal.signal(signal.SIGUSR1, signal.SIG_DFL)",
            "",
            "    if sig == signal.SIGUSR1:",
            "        sys.exit()",
            "",
            "",
            "def perform_task(task_pk, task_working_dir_rel_path):",
            "    \"\"\"Setup the environment to handle a task and execute it.",
            "    This must be called as a subprocess, while the parent holds the advisory lock of the task.\"\"\"",
            "    signal.signal(signal.SIGINT, child_signal_handler)",
            "    signal.signal(signal.SIGTERM, child_signal_handler)",
            "    signal.signal(signal.SIGHUP, child_signal_handler)",
            "    signal.signal(signal.SIGUSR1, child_signal_handler)",
            "    # All processes need to create their own postgres connection",
            "    connection.connection = None",
            "    # enc_args and enc_kwargs are deferred by default but we actually want them",
            "    task = Task.objects.defer(None).select_related(\"pulp_domain\").get(pk=task_pk)",
            "    # These queries were specifically constructed and ordered this way to ensure we have the highest",
            "    # chance of getting the user who dispatched the task since we don't have a user relation on the",
            "    # task model. The second query acts as a fallback to provide ZDU support. Future changes will",
            "    # require to keep these around till a breaking change release is planned (3.70 the earliest).",
            "    user = (",
            "        get_users_with_perms(",
            "            task,",
            "            only_with_perms_in=[\"core.add_task\"],",
            "            with_group_users=False,",
            "            include_model_permissions=False,",
            "            include_domain_permissions=False,",
            "        ).first()",
            "        or get_users_with_perms(",
            "            task,",
            "            only_with_perms_in=[\"core.manage_roles_task\"],",
            "            with_group_users=False,",
            "            include_model_permissions=False,",
            "            include_domain_permissions=False,",
            "        ).first()",
            "    )",
            "    # Isolate from the parent asyncio.",
            "    asyncio.set_event_loop(asyncio.new_event_loop())",
            "    # Set current contexts",
            "    set_guid(task.logging_cid)",
            "    set_current_user(user)",
            "    set_domain(task.pulp_domain)",
            "    os.chdir(task_working_dir_rel_path)",
            "",
            "    if settings.TASK_DIAGNOSTICS:",
            "        _execute_task_and_profile(task)",
            "    else:",
            "        execute_task(task)",
            "",
            "",
            "def _execute_task_and_profile(task):",
            "    with tempfile.TemporaryDirectory(dir=settings.WORKING_DIRECTORY) as temp_dir:",
            "        pyinstrument_func = _pyinstrument_diagnostic_decorator(temp_dir, execute_task)",
            "        memory_func = _memory_diagnostic_decorator(temp_dir, pyinstrument_func)",
            "",
            "        memory_func(task)",
            "",
            "",
            "def _memory_diagnostic_decorator(temp_dir, func):",
            "    def __memory_diagnostic_decorator(task):",
            "        mem_diagnostics_file_path = os.path.join(temp_dir, \"memory.datum\")",
            "        # It would be better to have this recording happen in the parent process instead of here",
            "        # https://github.com/pulp/pulpcore/issues/2337",
            "        stop_event = threading.Event()",
            "        mem_diagnostics_thread = threading.Thread(",
            "            target=write_memory_usage, args=(stop_event, mem_diagnostics_file_path), daemon=True",
            "        )",
            "        mem_diagnostics_thread.start()",
            "",
            "        func(task)",
            "",
            "        stop_event.set()",
            "        artifact = Artifact.init_and_validate(mem_diagnostics_file_path)",
            "        with suppress(IntegrityError):",
            "            artifact.save()",
            "",
            "        ProfileArtifact.objects.get_or_create(artifact=artifact, name=\"memory_profile\", task=task)",
            "",
            "        _logger.info(\"Created memory diagnostic data.\")",
            "",
            "    return __memory_diagnostic_decorator",
            "",
            "",
            "def _pyinstrument_diagnostic_decorator(temp_dir, func):",
            "    def __pyinstrument_diagnostic_decorator(task):",
            "        if importlib.util.find_spec(\"pyinstrument\") is not None:",
            "            from pyinstrument import Profiler",
            "",
            "            with Profiler() as profiler:",
            "                func(task)",
            "",
            "            profile_file_path = os.path.join(temp_dir, \"pyinstrument.html\")",
            "            with open(profile_file_path, \"w+\") as f:",
            "                f.write(profiler.output_html())",
            "                f.flush()",
            "",
            "            artifact = Artifact.init_and_validate(str(profile_file_path))",
            "            with suppress(IntegrityError):",
            "                artifact.save()",
            "",
            "            ProfileArtifact.objects.get_or_create(",
            "                artifact=artifact, name=\"pyinstrument_data\", task=task",
            "            )",
            "",
            "            _logger.info(\"Created pyinstrument profile data.\")",
            "        else:",
            "            func(task)",
            "",
            "    return __pyinstrument_diagnostic_decorator",
            "",
            "",
            "def dispatch_scheduled_tasks():",
            "    # Warning, dispatch_scheduled_tasks is not race condition free!",
            "    now = timezone.now()",
            "    # Dispatch all tasks old enough and not still running",
            "    for task_schedule in TaskSchedule.objects.filter(next_dispatch__lte=now).filter(",
            "        Q(last_task=None) | Q(last_task__state__in=TASK_FINAL_STATES)",
            "    ):",
            "        try:",
            "            if task_schedule.dispatch_interval is None:",
            "                # This was a timed one shot task schedule",
            "                task_schedule.next_dispatch = None",
            "            else:",
            "                # This is a recurring task schedule",
            "                while task_schedule.next_dispatch < now:",
            "                    # Do not schedule in the past",
            "                    task_schedule.next_dispatch += task_schedule.dispatch_interval",
            "            set_guid(generate_guid())",
            "            with transaction.atomic():",
            "                task_schedule.last_task = dispatch(",
            "                    task_schedule.task_name,",
            "                )",
            "                task_schedule.save(update_fields=[\"next_dispatch\", \"last_task\"])",
            "",
            "            _logger.info(",
            "                \"Dispatched scheduled task {task_name} as task id {task_id}\".format(",
            "                    task_name=task_schedule.task_name, task_id=task_schedule.last_task.pk",
            "                )",
            "            )",
            "        except Exception as e:",
            "            _logger.warning(",
            "                \"Dispatching scheduled task {task_name} failed. {error}\".format(",
            "                    task_name=task_schedule.task_name, error=str(e)",
            "                )",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "100": [
                "perform_task"
            ]
        },
        "addLocation": []
    },
    "pulpcore/tests/functional/api/test_tasking.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 379,
                "afterPatchRowNumber": 379,
                "PatchRowcode": "     ]"
            },
            "1": {
                "beforePatchRowNumber": 380,
                "afterPatchRowNumber": 380,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 381,
                "afterPatchRowNumber": 381,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 382,
                "PatchRowcode": "+@pytest.mark.parallel"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 383,
                "PatchRowcode": "+def test_correct_task_ownership("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 384,
                "PatchRowcode": "+    dispatch_task, pulpcore_bindings, gen_user, file_repository_factory"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 385,
                "PatchRowcode": "+):"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 386,
                "PatchRowcode": "+    \"\"\"Test that tasks get the correct ownership when dispatched.\"\"\""
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 387,
                "PatchRowcode": "+    alice = gen_user(model_roles=[\"core.task_viewer\"])"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 388,
                "PatchRowcode": "+    bob = gen_user(model_roles=[\"file.filerepository_creator\"])"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 389,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 390,
                "PatchRowcode": "+    with alice:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 391,
                "PatchRowcode": "+        atask_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 392,
                "PatchRowcode": "+    aroles = pulpcore_bindings.UsersRolesApi.list(alice.user.pulp_href)"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 393,
                "PatchRowcode": "+    assert aroles.count == 3"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 394,
                "PatchRowcode": "+    roles = {r.role: r.content_object for r in aroles.results}"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 395,
                "PatchRowcode": "+    correct_roles = {"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 396,
                "PatchRowcode": "+        \"core.task_owner\": atask_href,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 397,
                "PatchRowcode": "+        \"core.task_user_dispatcher\": atask_href,"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 398,
                "PatchRowcode": "+        \"core.task_viewer\": None,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 399,
                "PatchRowcode": "+    }"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 400,
                "PatchRowcode": "+    assert roles == correct_roles"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 401,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 402,
                "PatchRowcode": "+    with bob:"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 403,
                "PatchRowcode": "+        btask_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 404,
                "PatchRowcode": "+        repo = file_repository_factory()"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 405,
                "PatchRowcode": "+    aroles = pulpcore_bindings.UsersRolesApi.list(alice.user.pulp_href)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 406,
                "PatchRowcode": "+    assert aroles.count == 3"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 407,
                "PatchRowcode": "+    broles = pulpcore_bindings.UsersRolesApi.list(bob.user.pulp_href)"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 408,
                "PatchRowcode": "+    assert broles.count == 4"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 409,
                "PatchRowcode": "+    roles = {r.role: r.content_object for r in broles.results}"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 410,
                "PatchRowcode": "+    correct_roles = {"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 411,
                "PatchRowcode": "+        \"core.task_owner\": btask_href,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 412,
                "PatchRowcode": "+        \"core.task_user_dispatcher\": btask_href,"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 413,
                "PatchRowcode": "+        \"file.filerepository_owner\": repo.pulp_href,"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 414,
                "PatchRowcode": "+        \"file.filerepository_creator\": None,"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 415,
                "PatchRowcode": "+    }"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 416,
                "PatchRowcode": "+    assert roles == correct_roles"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 417,
                "PatchRowcode": "+"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 418,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": 382,
                "afterPatchRowNumber": 419,
                "PatchRowcode": " @pytest.fixture"
            },
            "41": {
                "beforePatchRowNumber": 383,
                "afterPatchRowNumber": 420,
                "PatchRowcode": " def task_group(dispatch_task_group, monitor_task_group):"
            },
            "42": {
                "beforePatchRowNumber": 384,
                "afterPatchRowNumber": 421,
                "PatchRowcode": "     \"\"\"Fixture containing a finished Task Group.\"\"\""
            }
        },
        "frontPatchFile": [
            "\"\"\"Tests related to the tasking system.\"\"\"",
            "",
            "import os",
            "import json",
            "import pytest",
            "import time",
            "",
            "from aiohttp import BasicAuth",
            "from urllib.parse import urljoin",
            "from uuid import uuid4",
            "",
            "from pulpcore.client.pulpcore import ApiException",
            "",
            "from pulpcore.tests.functional.utils import download_file",
            "",
            "",
            "@pytest.fixture(scope=\"module\")",
            "def task(dispatch_task, monitor_task):",
            "    \"\"\"Fixture containing a finished Task.\"\"\"",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    return monitor_task(task_href)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieving_task_profile_artifacts(gen_user, pulpcore_bindings, task):",
            "    with gen_user(model_roles=[\"core.task_viewer\"]), pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.profile_artifacts(task.pulp_href)",
            "    assert ctx.value.status == 403",
            "",
            "    with gen_user(model_roles=[\"core.task_owner\"]):",
            "        assert pulpcore_bindings.TasksApi.profile_artifacts(task.pulp_href).urls is not None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_multi_resource_locking(dispatch_task, monitor_task):",
            "    task_href1 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\",",
            "        args=(1,),",
            "        exclusive_resources=[\"AAAA\"],",
            "        shared_resources=[\"BBBB\"],",
            "    )",
            "    task_href2 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), shared_resources=[\"AAAA\"]",
            "    )",
            "    task_href3 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), shared_resources=[\"AAAA\"]",
            "    )",
            "    task_href4 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), exclusive_resources=[\"AAAA\"]",
            "    )",
            "    task_href5 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), exclusive_resources=[\"BBBB\"]",
            "    )",
            "",
            "    task1 = monitor_task(task_href1)",
            "    task2 = monitor_task(task_href2)",
            "    task3 = monitor_task(task_href3)",
            "    task4 = monitor_task(task_href4)",
            "    task5 = monitor_task(task_href5)",
            "",
            "    assert task1.finished_at < task2.started_at",
            "    assert task1.finished_at < task3.started_at",
            "    assert task2.finished_at < task4.started_at",
            "    assert task3.finished_at < task4.started_at",
            "    assert task1.finished_at < task5.started_at",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_delete_cancel_waiting_task(dispatch_task, pulpcore_bindings):",
            "    # Queue one task after a long running one",
            "    resource = str(uuid4())",
            "    blocking_task_href = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(600,), exclusive_resources=[resource]",
            "    )",
            "    task_href = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(0,), exclusive_resources=[resource]",
            "    )",
            "",
            "    task = pulpcore_bindings.TasksApi.read(task_href)",
            "    assert task.state == \"waiting\"",
            "",
            "    # Try to delete first",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.delete(task_href)",
            "    assert ctx.value.status == 409",
            "",
            "    # Now cancel the task",
            "    task = pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "    # cancel the blocking task",
            "    pulpcore_bindings.TasksApi.tasks_cancel(blocking_task_href, {\"state\": \"canceled\"})",
            "",
            "    if task.state == \"canceling\":",
            "        assert task.started_at is None",
            "        assert task.finished_at is None",
            "",
            "        for i in range(10):",
            "            if task.state != \"canceling\":",
            "                break",
            "            time.sleep(1)",
            "            task = pulpcore_bindings.TasksApi.read(task_href)",
            "",
            "    assert task.state == \"canceled\"",
            "    assert task.started_at is None",
            "    assert task.finished_at is not None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_delete_cancel_running_task(dispatch_task, pulpcore_bindings):",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(600,))",
            "",
            "    for i in range(10):",
            "        task = pulpcore_bindings.TasksApi.read(task_href)",
            "        if task.state == \"running\":",
            "            break",
            "        time.sleep(1)",
            "",
            "    assert task.state == \"running\"",
            "",
            "    # Try to delete first",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.delete(task_href)",
            "    assert ctx.value.status == 409",
            "",
            "    # Now cancel the task",
            "    task = pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "",
            "    if task.state == \"canceling\":",
            "        assert task.started_at is not None",
            "        assert task.finished_at is None",
            "",
            "        for i in range(10):",
            "            if task.state != \"canceling\":",
            "                break",
            "            time.sleep(1)",
            "            task = pulpcore_bindings.TasksApi.read(task_href)",
            "",
            "    assert task.state == \"canceled\"",
            "    assert task.started_at is not None",
            "    assert task.finished_at is not None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_cancel_delete_finished_task(pulpcore_bindings, dispatch_task, monitor_task):",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    monitor_task(task_href)",
            "",
            "    # Try to cancel first",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "    assert ctx.value.status == 409",
            "",
            "    # Now delete the task",
            "    pulpcore_bindings.TasksApi.delete(task_href)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_cancel_nonexistent_task(pulp_api_v3_path, pulpcore_bindings):",
            "    task_href = f\"{pulp_api_v3_path}tasks/{uuid4()}/\"",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "    assert ctx.value.status == 404",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_with_limited_fields(task, bindings_cfg):",
            "    \"\"\"Verify for specific fields retrieve in the payload.\"\"\"",
            "    expected_fields = set((\"pulp_href\", \"state\", \"worker\"))",
            "",
            "    auth = BasicAuth(login=bindings_cfg.username, password=bindings_cfg.password)",
            "    full_href = urljoin(bindings_cfg.host, task.pulp_href)",
            "",
            "    response = download_file(f\"{full_href}?fields={','.join(expected_fields)}\", auth=auth)",
            "    parsed_response = json.loads(response.body)",
            "",
            "    returned_fields = set(parsed_response.keys())",
            "",
            "    assert expected_fields == returned_fields",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_without_specific_fields(task, bindings_cfg):",
            "    \"\"\"Verify if some fields are excluded from the response.\"\"\"",
            "    unexpected_fields = set((\"state\", \"worker\"))",
            "",
            "    auth = BasicAuth(login=bindings_cfg.username, password=bindings_cfg.password)",
            "    full_href = urljoin(bindings_cfg.host, task.pulp_href)",
            "",
            "    response = download_file(f\"{full_href}?exclude_fields={','.join(unexpected_fields)}\", auth=auth)",
            "    parsed_response = json.loads(response.body)",
            "",
            "    returned_fields = set(parsed_response.keys())",
            "",
            "    assert unexpected_fields.isdisjoint(returned_fields)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_with_minimal_fields(task, bindings_cfg):",
            "    \"\"\"Verify if some fields doesn't show when retrieving the minimal payload.\"\"\"",
            "    unexpected_fields = set((\"progress_reports\", \"parent_task\", \"error\"))",
            "",
            "    auth = BasicAuth(login=bindings_cfg.username, password=bindings_cfg.password)",
            "    full_href = urljoin(bindings_cfg.host, task.pulp_href)",
            "",
            "    response = download_file(f\"{full_href}?minimal=true\", auth=auth)",
            "    parsed_response = json.loads(response.body)",
            "",
            "    returned_fields = set(parsed_response.keys())",
            "",
            "    assert unexpected_fields.isdisjoint(returned_fields)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_invalid_worker(pulpcore_bindings):",
            "    \"\"\"Expects to raise an exception when using invalid worker value as filter.\"\"\"",
            "",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.list(worker=str(uuid4()))",
            "",
            "    assert ctx.value.status == 400",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_valid_worker(task, pulpcore_bindings):",
            "    \"\"\"Expects to retrieve a task using a valid worker URI as filter.\"\"\"",
            "",
            "    response = pulpcore_bindings.TasksApi.list(worker=task.worker)",
            "",
            "    assert response.results and response.count",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_invalid_date(pulpcore_bindings):",
            "    \"\"\"Expects to raise an exception when using invalid dates as filters\"\"\"",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.list(finished_at=str(uuid4()), started_at=str(uuid4()))",
            "",
            "    assert ctx.value.status == 400",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_valid_date(task, pulpcore_bindings):",
            "    \"\"\"Expects to retrieve a task using a valid date.\"\"\"",
            "",
            "    response = pulpcore_bindings.TasksApi.list(started_at=task.started_at)",
            "",
            "    assert response.results and response.count",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_search_task_by_name(task, pulpcore_bindings):",
            "    task_name = task.name",
            "    search_results = pulpcore_bindings.TasksApi.list(name=task.name).results",
            "",
            "    assert search_results",
            "    assert all([task.name == task_name for task in search_results])",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_search_task_using_an_invalid_name(pulpcore_bindings):",
            "    \"\"\"Expect to return an empty results array when searching using an invalid",
            "    task name.",
            "    \"\"\"",
            "",
            "    search_results = pulpcore_bindings.TasksApi.list(name=str(uuid4()))",
            "",
            "    assert not search_results.results and not search_results.count",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_filter_tasks_using_worker__in_filter(pulpcore_bindings, dispatch_task, monitor_task):",
            "    task1_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    task2_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "",
            "    task1 = monitor_task(task1_href)",
            "    task2 = monitor_task(task2_href)",
            "",
            "    search_results = pulpcore_bindings.TasksApi.list(worker__in=(task1.worker, task2.worker))",
            "",
            "    tasks_hrefs = [task.pulp_href for task in search_results.results]",
            "",
            "    assert task1_href in tasks_hrefs",
            "    assert task2_href in tasks_hrefs",
            "",
            "",
            "def test_cancel_gooey_task(pulpcore_bindings, dispatch_task, monitor_task):",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.gooey_task\", args=(60,))",
            "    for i in range(10):",
            "        task = pulpcore_bindings.TasksApi.read(task_href)",
            "        if task.state == \"running\":",
            "            break",
            "        time.sleep(1)",
            "",
            "    task = pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "",
            "    if task.state == \"canceling\":",
            "        for i in range(30):",
            "            if task.state != \"canceling\":",
            "                break",
            "            time.sleep(1)",
            "            task = pulpcore_bindings.TasksApi.read(task_href)",
            "",
            "    assert task.state == \"canceled\"",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_task_created_by(dispatch_task, monitor_task, gen_user, anonymous_user):",
            "    # Test admin dispatch, user_id == 1 / admin is always first user",
            "    task = monitor_task(dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,)))",
            "    assert task.created_by.endswith(\"/1/\")",
            "",
            "    # Test w/ new user, user_id != 1",
            "    user = gen_user()",
            "    with user:",
            "        task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    user_task = monitor_task(task_href)",
            "    assert task.created_by != user_task.created_by",
            "    assert user_task.created_by == user.user.pulp_href",
            "",
            "    # Test w/ anon (Pulp itself, i.e. analytics)",
            "    with anonymous_user:",
            "        task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    anon_task = monitor_task(task_href)",
            "    assert anon_task.created_by is None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_task_version_prevent_pickup(dispatch_task, pulpcore_bindings):",
            "    task1 = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,), versions={\"core\": \"4.0.0\"})",
            "    task2 = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,), versions={\"catdog\": \"0.0.0\"})",
            "",
            "    time.sleep(5)",
            "    for task_href in [task1, task2]:",
            "        task = pulpcore_bindings.TasksApi.read(task_href)",
            "        assert task.state == \"waiting\"",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "",
            "",
            "def test_emmiting_unblocked_task_telemetry(",
            "    dispatch_task, pulpcore_bindings, pulp_settings, received_otel_metrics",
            "):",
            "    if os.getenv(\"PULP_OTEL_ENABLED\").lower() != \"true\":",
            "        pytest.skip(\"Need PULP_OTEL_ENABLED to run this test.\")",
            "",
            "    # Checking online workers ready to get a task",
            "    workers_online = pulpcore_bindings.WorkersApi.list(online=\"true\").count",
            "",
            "    # We need to generate long running tasks to block the workers from executing other tasks",
            "    resident_task_hrefs = [",
            "        dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(30,))",
            "        for worker in range(workers_online)",
            "    ]",
            "",
            "    # Then we dispatch a quick unblockable task just to keep it waiting in the queue",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "",
            "    task = pulpcore_bindings.TasksApi.read(task_href)",
            "    assert task.state == \"waiting\"",
            "",
            "    # And trigger the metrics",
            "    assert received_otel_metrics(",
            "        {",
            "            \"name\": \"tasks_unblocked_queue\",",
            "            \"description\": \"Number of unblocked tasks waiting in the queue.\",",
            "            \"unit\": \"tasks\",",
            "        }",
            "    )",
            "",
            "    assert received_otel_metrics(",
            "        {",
            "            \"name\": \"tasks_longest_unblocked_time\",",
            "            \"description\": \"The age of the longest waiting task.\",",
            "            \"unit\": \"seconds\",",
            "        }",
            "    )",
            "",
            "    [",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "        for task_href in resident_task_hrefs",
            "    ]",
            "",
            "",
            "@pytest.fixture",
            "def task_group(dispatch_task_group, monitor_task_group):",
            "    \"\"\"Fixture containing a finished Task Group.\"\"\"",
            "    kwargs = {\"inbetween\": 0, \"intervals\": [0]}",
            "    tgroup_href = dispatch_task_group(\"pulpcore.app.tasks.test.dummy_group_task\", kwargs=kwargs)",
            "    return monitor_task_group(tgroup_href)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_scope_task_groups(pulpcore_bindings, task_group, gen_user):",
            "    \"\"\"Test that task groups can be queryset scoped by permission on Tasks.\"\"\"",
            "    for task in task_group.tasks:",
            "        if task.name == \"pulpcore.app.tasks.test.dummy_group_task\":",
            "            break",
            "",
            "    response = pulpcore_bindings.TaskGroupsApi.list()",
            "    assert response.count > 0",
            "",
            "    with gen_user():",
            "        response = pulpcore_bindings.TaskGroupsApi.list()",
            "        assert response.count == 0",
            "",
            "    with gen_user(model_roles=[\"core.task_viewer\"]):",
            "        response = pulpcore_bindings.TaskGroupsApi.list()",
            "        assert response.count > 0",
            "",
            "    with gen_user(object_roles=[(\"core.task_owner\", task.pulp_href)]):",
            "        response = pulpcore_bindings.TaskGroupsApi.list()",
            "        assert response.count == 1"
        ],
        "afterPatchFile": [
            "\"\"\"Tests related to the tasking system.\"\"\"",
            "",
            "import os",
            "import json",
            "import pytest",
            "import time",
            "",
            "from aiohttp import BasicAuth",
            "from urllib.parse import urljoin",
            "from uuid import uuid4",
            "",
            "from pulpcore.client.pulpcore import ApiException",
            "",
            "from pulpcore.tests.functional.utils import download_file",
            "",
            "",
            "@pytest.fixture(scope=\"module\")",
            "def task(dispatch_task, monitor_task):",
            "    \"\"\"Fixture containing a finished Task.\"\"\"",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    return monitor_task(task_href)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieving_task_profile_artifacts(gen_user, pulpcore_bindings, task):",
            "    with gen_user(model_roles=[\"core.task_viewer\"]), pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.profile_artifacts(task.pulp_href)",
            "    assert ctx.value.status == 403",
            "",
            "    with gen_user(model_roles=[\"core.task_owner\"]):",
            "        assert pulpcore_bindings.TasksApi.profile_artifacts(task.pulp_href).urls is not None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_multi_resource_locking(dispatch_task, monitor_task):",
            "    task_href1 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\",",
            "        args=(1,),",
            "        exclusive_resources=[\"AAAA\"],",
            "        shared_resources=[\"BBBB\"],",
            "    )",
            "    task_href2 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), shared_resources=[\"AAAA\"]",
            "    )",
            "    task_href3 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), shared_resources=[\"AAAA\"]",
            "    )",
            "    task_href4 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), exclusive_resources=[\"AAAA\"]",
            "    )",
            "    task_href5 = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(1,), exclusive_resources=[\"BBBB\"]",
            "    )",
            "",
            "    task1 = monitor_task(task_href1)",
            "    task2 = monitor_task(task_href2)",
            "    task3 = monitor_task(task_href3)",
            "    task4 = monitor_task(task_href4)",
            "    task5 = monitor_task(task_href5)",
            "",
            "    assert task1.finished_at < task2.started_at",
            "    assert task1.finished_at < task3.started_at",
            "    assert task2.finished_at < task4.started_at",
            "    assert task3.finished_at < task4.started_at",
            "    assert task1.finished_at < task5.started_at",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_delete_cancel_waiting_task(dispatch_task, pulpcore_bindings):",
            "    # Queue one task after a long running one",
            "    resource = str(uuid4())",
            "    blocking_task_href = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(600,), exclusive_resources=[resource]",
            "    )",
            "    task_href = dispatch_task(",
            "        \"pulpcore.app.tasks.test.sleep\", args=(0,), exclusive_resources=[resource]",
            "    )",
            "",
            "    task = pulpcore_bindings.TasksApi.read(task_href)",
            "    assert task.state == \"waiting\"",
            "",
            "    # Try to delete first",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.delete(task_href)",
            "    assert ctx.value.status == 409",
            "",
            "    # Now cancel the task",
            "    task = pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "    # cancel the blocking task",
            "    pulpcore_bindings.TasksApi.tasks_cancel(blocking_task_href, {\"state\": \"canceled\"})",
            "",
            "    if task.state == \"canceling\":",
            "        assert task.started_at is None",
            "        assert task.finished_at is None",
            "",
            "        for i in range(10):",
            "            if task.state != \"canceling\":",
            "                break",
            "            time.sleep(1)",
            "            task = pulpcore_bindings.TasksApi.read(task_href)",
            "",
            "    assert task.state == \"canceled\"",
            "    assert task.started_at is None",
            "    assert task.finished_at is not None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_delete_cancel_running_task(dispatch_task, pulpcore_bindings):",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(600,))",
            "",
            "    for i in range(10):",
            "        task = pulpcore_bindings.TasksApi.read(task_href)",
            "        if task.state == \"running\":",
            "            break",
            "        time.sleep(1)",
            "",
            "    assert task.state == \"running\"",
            "",
            "    # Try to delete first",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.delete(task_href)",
            "    assert ctx.value.status == 409",
            "",
            "    # Now cancel the task",
            "    task = pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "",
            "    if task.state == \"canceling\":",
            "        assert task.started_at is not None",
            "        assert task.finished_at is None",
            "",
            "        for i in range(10):",
            "            if task.state != \"canceling\":",
            "                break",
            "            time.sleep(1)",
            "            task = pulpcore_bindings.TasksApi.read(task_href)",
            "",
            "    assert task.state == \"canceled\"",
            "    assert task.started_at is not None",
            "    assert task.finished_at is not None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_cancel_delete_finished_task(pulpcore_bindings, dispatch_task, monitor_task):",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    monitor_task(task_href)",
            "",
            "    # Try to cancel first",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "    assert ctx.value.status == 409",
            "",
            "    # Now delete the task",
            "    pulpcore_bindings.TasksApi.delete(task_href)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_cancel_nonexistent_task(pulp_api_v3_path, pulpcore_bindings):",
            "    task_href = f\"{pulp_api_v3_path}tasks/{uuid4()}/\"",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "    assert ctx.value.status == 404",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_with_limited_fields(task, bindings_cfg):",
            "    \"\"\"Verify for specific fields retrieve in the payload.\"\"\"",
            "    expected_fields = set((\"pulp_href\", \"state\", \"worker\"))",
            "",
            "    auth = BasicAuth(login=bindings_cfg.username, password=bindings_cfg.password)",
            "    full_href = urljoin(bindings_cfg.host, task.pulp_href)",
            "",
            "    response = download_file(f\"{full_href}?fields={','.join(expected_fields)}\", auth=auth)",
            "    parsed_response = json.loads(response.body)",
            "",
            "    returned_fields = set(parsed_response.keys())",
            "",
            "    assert expected_fields == returned_fields",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_without_specific_fields(task, bindings_cfg):",
            "    \"\"\"Verify if some fields are excluded from the response.\"\"\"",
            "    unexpected_fields = set((\"state\", \"worker\"))",
            "",
            "    auth = BasicAuth(login=bindings_cfg.username, password=bindings_cfg.password)",
            "    full_href = urljoin(bindings_cfg.host, task.pulp_href)",
            "",
            "    response = download_file(f\"{full_href}?exclude_fields={','.join(unexpected_fields)}\", auth=auth)",
            "    parsed_response = json.loads(response.body)",
            "",
            "    returned_fields = set(parsed_response.keys())",
            "",
            "    assert unexpected_fields.isdisjoint(returned_fields)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_with_minimal_fields(task, bindings_cfg):",
            "    \"\"\"Verify if some fields doesn't show when retrieving the minimal payload.\"\"\"",
            "    unexpected_fields = set((\"progress_reports\", \"parent_task\", \"error\"))",
            "",
            "    auth = BasicAuth(login=bindings_cfg.username, password=bindings_cfg.password)",
            "    full_href = urljoin(bindings_cfg.host, task.pulp_href)",
            "",
            "    response = download_file(f\"{full_href}?minimal=true\", auth=auth)",
            "    parsed_response = json.loads(response.body)",
            "",
            "    returned_fields = set(parsed_response.keys())",
            "",
            "    assert unexpected_fields.isdisjoint(returned_fields)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_invalid_worker(pulpcore_bindings):",
            "    \"\"\"Expects to raise an exception when using invalid worker value as filter.\"\"\"",
            "",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.list(worker=str(uuid4()))",
            "",
            "    assert ctx.value.status == 400",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_valid_worker(task, pulpcore_bindings):",
            "    \"\"\"Expects to retrieve a task using a valid worker URI as filter.\"\"\"",
            "",
            "    response = pulpcore_bindings.TasksApi.list(worker=task.worker)",
            "",
            "    assert response.results and response.count",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_invalid_date(pulpcore_bindings):",
            "    \"\"\"Expects to raise an exception when using invalid dates as filters\"\"\"",
            "    with pytest.raises(ApiException) as ctx:",
            "        pulpcore_bindings.TasksApi.list(finished_at=str(uuid4()), started_at=str(uuid4()))",
            "",
            "    assert ctx.value.status == 400",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_retrieve_task_using_valid_date(task, pulpcore_bindings):",
            "    \"\"\"Expects to retrieve a task using a valid date.\"\"\"",
            "",
            "    response = pulpcore_bindings.TasksApi.list(started_at=task.started_at)",
            "",
            "    assert response.results and response.count",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_search_task_by_name(task, pulpcore_bindings):",
            "    task_name = task.name",
            "    search_results = pulpcore_bindings.TasksApi.list(name=task.name).results",
            "",
            "    assert search_results",
            "    assert all([task.name == task_name for task in search_results])",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_search_task_using_an_invalid_name(pulpcore_bindings):",
            "    \"\"\"Expect to return an empty results array when searching using an invalid",
            "    task name.",
            "    \"\"\"",
            "",
            "    search_results = pulpcore_bindings.TasksApi.list(name=str(uuid4()))",
            "",
            "    assert not search_results.results and not search_results.count",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_filter_tasks_using_worker__in_filter(pulpcore_bindings, dispatch_task, monitor_task):",
            "    task1_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    task2_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "",
            "    task1 = monitor_task(task1_href)",
            "    task2 = monitor_task(task2_href)",
            "",
            "    search_results = pulpcore_bindings.TasksApi.list(worker__in=(task1.worker, task2.worker))",
            "",
            "    tasks_hrefs = [task.pulp_href for task in search_results.results]",
            "",
            "    assert task1_href in tasks_hrefs",
            "    assert task2_href in tasks_hrefs",
            "",
            "",
            "def test_cancel_gooey_task(pulpcore_bindings, dispatch_task, monitor_task):",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.gooey_task\", args=(60,))",
            "    for i in range(10):",
            "        task = pulpcore_bindings.TasksApi.read(task_href)",
            "        if task.state == \"running\":",
            "            break",
            "        time.sleep(1)",
            "",
            "    task = pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "",
            "    if task.state == \"canceling\":",
            "        for i in range(30):",
            "            if task.state != \"canceling\":",
            "                break",
            "            time.sleep(1)",
            "            task = pulpcore_bindings.TasksApi.read(task_href)",
            "",
            "    assert task.state == \"canceled\"",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_task_created_by(dispatch_task, monitor_task, gen_user, anonymous_user):",
            "    # Test admin dispatch, user_id == 1 / admin is always first user",
            "    task = monitor_task(dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,)))",
            "    assert task.created_by.endswith(\"/1/\")",
            "",
            "    # Test w/ new user, user_id != 1",
            "    user = gen_user()",
            "    with user:",
            "        task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    user_task = monitor_task(task_href)",
            "    assert task.created_by != user_task.created_by",
            "    assert user_task.created_by == user.user.pulp_href",
            "",
            "    # Test w/ anon (Pulp itself, i.e. analytics)",
            "    with anonymous_user:",
            "        task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    anon_task = monitor_task(task_href)",
            "    assert anon_task.created_by is None",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_task_version_prevent_pickup(dispatch_task, pulpcore_bindings):",
            "    task1 = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,), versions={\"core\": \"4.0.0\"})",
            "    task2 = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,), versions={\"catdog\": \"0.0.0\"})",
            "",
            "    time.sleep(5)",
            "    for task_href in [task1, task2]:",
            "        task = pulpcore_bindings.TasksApi.read(task_href)",
            "        assert task.state == \"waiting\"",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "",
            "",
            "def test_emmiting_unblocked_task_telemetry(",
            "    dispatch_task, pulpcore_bindings, pulp_settings, received_otel_metrics",
            "):",
            "    if os.getenv(\"PULP_OTEL_ENABLED\").lower() != \"true\":",
            "        pytest.skip(\"Need PULP_OTEL_ENABLED to run this test.\")",
            "",
            "    # Checking online workers ready to get a task",
            "    workers_online = pulpcore_bindings.WorkersApi.list(online=\"true\").count",
            "",
            "    # We need to generate long running tasks to block the workers from executing other tasks",
            "    resident_task_hrefs = [",
            "        dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(30,))",
            "        for worker in range(workers_online)",
            "    ]",
            "",
            "    # Then we dispatch a quick unblockable task just to keep it waiting in the queue",
            "    task_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "",
            "    task = pulpcore_bindings.TasksApi.read(task_href)",
            "    assert task.state == \"waiting\"",
            "",
            "    # And trigger the metrics",
            "    assert received_otel_metrics(",
            "        {",
            "            \"name\": \"tasks_unblocked_queue\",",
            "            \"description\": \"Number of unblocked tasks waiting in the queue.\",",
            "            \"unit\": \"tasks\",",
            "        }",
            "    )",
            "",
            "    assert received_otel_metrics(",
            "        {",
            "            \"name\": \"tasks_longest_unblocked_time\",",
            "            \"description\": \"The age of the longest waiting task.\",",
            "            \"unit\": \"seconds\",",
            "        }",
            "    )",
            "",
            "    [",
            "        pulpcore_bindings.TasksApi.tasks_cancel(task_href, {\"state\": \"canceled\"})",
            "        for task_href in resident_task_hrefs",
            "    ]",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_correct_task_ownership(",
            "    dispatch_task, pulpcore_bindings, gen_user, file_repository_factory",
            "):",
            "    \"\"\"Test that tasks get the correct ownership when dispatched.\"\"\"",
            "    alice = gen_user(model_roles=[\"core.task_viewer\"])",
            "    bob = gen_user(model_roles=[\"file.filerepository_creator\"])",
            "",
            "    with alice:",
            "        atask_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "    aroles = pulpcore_bindings.UsersRolesApi.list(alice.user.pulp_href)",
            "    assert aroles.count == 3",
            "    roles = {r.role: r.content_object for r in aroles.results}",
            "    correct_roles = {",
            "        \"core.task_owner\": atask_href,",
            "        \"core.task_user_dispatcher\": atask_href,",
            "        \"core.task_viewer\": None,",
            "    }",
            "    assert roles == correct_roles",
            "",
            "    with bob:",
            "        btask_href = dispatch_task(\"pulpcore.app.tasks.test.sleep\", args=(0,))",
            "        repo = file_repository_factory()",
            "    aroles = pulpcore_bindings.UsersRolesApi.list(alice.user.pulp_href)",
            "    assert aroles.count == 3",
            "    broles = pulpcore_bindings.UsersRolesApi.list(bob.user.pulp_href)",
            "    assert broles.count == 4",
            "    roles = {r.role: r.content_object for r in broles.results}",
            "    correct_roles = {",
            "        \"core.task_owner\": btask_href,",
            "        \"core.task_user_dispatcher\": btask_href,",
            "        \"file.filerepository_owner\": repo.pulp_href,",
            "        \"file.filerepository_creator\": None,",
            "    }",
            "    assert roles == correct_roles",
            "",
            "",
            "@pytest.fixture",
            "def task_group(dispatch_task_group, monitor_task_group):",
            "    \"\"\"Fixture containing a finished Task Group.\"\"\"",
            "    kwargs = {\"inbetween\": 0, \"intervals\": [0]}",
            "    tgroup_href = dispatch_task_group(\"pulpcore.app.tasks.test.dummy_group_task\", kwargs=kwargs)",
            "    return monitor_task_group(tgroup_href)",
            "",
            "",
            "@pytest.mark.parallel",
            "def test_scope_task_groups(pulpcore_bindings, task_group, gen_user):",
            "    \"\"\"Test that task groups can be queryset scoped by permission on Tasks.\"\"\"",
            "    for task in task_group.tasks:",
            "        if task.name == \"pulpcore.app.tasks.test.dummy_group_task\":",
            "            break",
            "",
            "    response = pulpcore_bindings.TaskGroupsApi.list()",
            "    assert response.count > 0",
            "",
            "    with gen_user():",
            "        response = pulpcore_bindings.TaskGroupsApi.list()",
            "        assert response.count == 0",
            "",
            "    with gen_user(model_roles=[\"core.task_viewer\"]):",
            "        response = pulpcore_bindings.TaskGroupsApi.list()",
            "        assert response.count > 0",
            "",
            "    with gen_user(object_roles=[(\"core.task_owner\", task.pulp_href)]):",
            "        response = pulpcore_bindings.TaskGroupsApi.list()",
            "        assert response.count == 1"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "web.pgadmin.tools.import_export.create_import_export_job"
        ]
    }
}