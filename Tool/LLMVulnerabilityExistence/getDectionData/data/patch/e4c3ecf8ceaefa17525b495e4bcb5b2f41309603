{
    "airflow/api_connexion/endpoints/connection_endpoint.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from http import HTTPStatus"
            },
            "1": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from connexion import NoContent"
            },
            "3": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from flask import request"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+from flask import Response, request"
            },
            "5": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from marshmallow import ValidationError"
            },
            "6": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from sqlalchemy import func, select"
            },
            "7": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from sqlalchemy.orm import Session"
            },
            "8": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "     connection_test_schema,"
            },
            "9": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " )"
            },
            "10": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " from airflow.api_connexion.types import APIResponse, UpdateMask"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+from airflow.configuration import conf"
            },
            "12": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " from airflow.models import Connection"
            },
            "13": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " from airflow.secrets.environment_variables import CONN_ENV_PREFIX"
            },
            "14": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " from airflow.security import permissions"
            },
            "15": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "     env var, as some hook classes tries to find out the conn from their __init__ method & errors out"
            },
            "16": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 182,
                "PatchRowcode": "     if not found. It also deletes the conn id env variable after the test."
            },
            "17": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 183,
                "PatchRowcode": "     \"\"\""
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+        return Response("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+            \"Testing connections is disabled in Airflow configuration. Contact your deployment admin to \""
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 187,
                "PatchRowcode": "+            \"enable it.\","
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+            403,"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 189,
                "PatchRowcode": "+        )"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 190,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "     body = request.json"
            },
            "26": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 192,
                "PatchRowcode": "     transient_conn_id = get_random_string()"
            },
            "27": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 193,
                "PatchRowcode": "     conn_env_var = f\"{CONN_ENV_PREFIX}{transient_conn_id.upper()}\""
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import os",
            "from http import HTTPStatus",
            "",
            "from connexion import NoContent",
            "from flask import request",
            "from marshmallow import ValidationError",
            "from sqlalchemy import func, select",
            "from sqlalchemy.orm import Session",
            "",
            "from airflow.api_connexion import security",
            "from airflow.api_connexion.endpoints.update_mask import extract_update_mask_data",
            "from airflow.api_connexion.exceptions import AlreadyExists, BadRequest, NotFound",
            "from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
            "from airflow.api_connexion.schemas.connection_schema import (",
            "    ConnectionCollection,",
            "    connection_collection_schema,",
            "    connection_schema,",
            "    connection_test_schema,",
            ")",
            "from airflow.api_connexion.types import APIResponse, UpdateMask",
            "from airflow.models import Connection",
            "from airflow.secrets.environment_variables import CONN_ENV_PREFIX",
            "from airflow.security import permissions",
            "from airflow.utils import helpers",
            "from airflow.utils.log.action_logger import action_event_from_permission",
            "from airflow.utils.session import NEW_SESSION, provide_session",
            "from airflow.utils.strings import get_random_string",
            "from airflow.www.decorators import action_logging",
            "",
            "RESOURCE_EVENT_PREFIX = \"connection\"",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_DELETE, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "@action_logging(",
            "    event=action_event_from_permission(",
            "        prefix=RESOURCE_EVENT_PREFIX,",
            "        permission=permissions.ACTION_CAN_DELETE,",
            "    ),",
            ")",
            "def delete_connection(*, connection_id: str, session: Session = NEW_SESSION) -> APIResponse:",
            "    \"\"\"Delete a connection entry.\"\"\"",
            "    connection = session.scalar(select(Connection).filter_by(conn_id=connection_id))",
            "    if connection is None:",
            "        raise NotFound(",
            "            \"Connection not found\",",
            "            detail=f\"The Connection with connection_id: `{connection_id}` was not found\",",
            "        )",
            "    session.delete(connection)",
            "    return NoContent, HTTPStatus.NO_CONTENT",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "def get_connection(*, connection_id: str, session: Session = NEW_SESSION) -> APIResponse:",
            "    \"\"\"Get a connection entry.\"\"\"",
            "    connection = session.scalar(select(Connection).where(Connection.conn_id == connection_id))",
            "    if connection is None:",
            "        raise NotFound(",
            "            \"Connection not found\",",
            "            detail=f\"The Connection with connection_id: `{connection_id}` was not found\",",
            "        )",
            "    return connection_schema.dump(connection)",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONNECTION)])",
            "@format_parameters({\"limit\": check_limit})",
            "@provide_session",
            "def get_connections(",
            "    *,",
            "    limit: int,",
            "    offset: int = 0,",
            "    order_by: str = \"id\",",
            "    session: Session = NEW_SESSION,",
            ") -> APIResponse:",
            "    \"\"\"Get all connection entries.\"\"\"",
            "    to_replace = {\"connection_id\": \"conn_id\"}",
            "    allowed_filter_attrs = [\"connection_id\", \"conn_type\", \"description\", \"host\", \"port\", \"id\"]",
            "",
            "    total_entries = session.execute(select(func.count(Connection.id))).scalar_one()",
            "    query = select(Connection)",
            "    query = apply_sorting(query, order_by, to_replace, allowed_filter_attrs)",
            "    connections = session.scalars(query.offset(offset).limit(limit)).all()",
            "    return connection_collection_schema.dump(",
            "        ConnectionCollection(connections=connections, total_entries=total_entries)",
            "    )",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "@action_logging(",
            "    event=action_event_from_permission(",
            "        prefix=RESOURCE_EVENT_PREFIX,",
            "        permission=permissions.ACTION_CAN_EDIT,",
            "    ),",
            ")",
            "def patch_connection(",
            "    *,",
            "    connection_id: str,",
            "    update_mask: UpdateMask = None,",
            "    session: Session = NEW_SESSION,",
            ") -> APIResponse:",
            "    \"\"\"Update a connection entry.\"\"\"",
            "    try:",
            "        data = connection_schema.load(request.json, partial=True)",
            "    except ValidationError as err:",
            "        # If validation get to here, it is extra field validation.",
            "        raise BadRequest(detail=str(err.messages))",
            "    non_update_fields = [\"connection_id\", \"conn_id\"]",
            "    connection = session.scalar(select(Connection).filter_by(conn_id=connection_id).limit(1))",
            "    if connection is None:",
            "        raise NotFound(",
            "            \"Connection not found\",",
            "            detail=f\"The Connection with connection_id: `{connection_id}` was not found\",",
            "        )",
            "    if data.get(\"conn_id\") and connection.conn_id != data[\"conn_id\"]:",
            "        raise BadRequest(detail=\"The connection_id cannot be updated.\")",
            "    if update_mask:",
            "        data = extract_update_mask_data(update_mask, non_update_fields, data)",
            "    for key in data:",
            "        setattr(connection, key, data[key])",
            "    session.add(connection)",
            "    session.commit()",
            "    return connection_schema.dump(connection)",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "@action_logging(",
            "    event=action_event_from_permission(",
            "        prefix=RESOURCE_EVENT_PREFIX,",
            "        permission=permissions.ACTION_CAN_CREATE,",
            "    ),",
            ")",
            "def post_connection(*, session: Session = NEW_SESSION) -> APIResponse:",
            "    \"\"\"Create connection entry.\"\"\"",
            "    body = request.json",
            "    try:",
            "        data = connection_schema.load(body)",
            "    except ValidationError as err:",
            "        raise BadRequest(detail=str(err.messages))",
            "    conn_id = data[\"conn_id\"]",
            "    try:",
            "        helpers.validate_key(conn_id, max_length=200)",
            "    except Exception as e:",
            "        raise BadRequest(detail=str(e))",
            "    connection = session.scalar(select(Connection).filter_by(conn_id=conn_id).limit(1))",
            "    if not connection:",
            "        connection = Connection(**data)",
            "        session.add(connection)",
            "        session.commit()",
            "        return connection_schema.dump(connection)",
            "    raise AlreadyExists(detail=f\"Connection already exist. ID: {conn_id}\")",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_CONNECTION)])",
            "def test_connection() -> APIResponse:",
            "    \"\"\"",
            "    Test an API connection.",
            "",
            "    This method first creates an in-memory transient conn_id & exports that to an",
            "    env var, as some hook classes tries to find out the conn from their __init__ method & errors out",
            "    if not found. It also deletes the conn id env variable after the test.",
            "    \"\"\"",
            "    body = request.json",
            "    transient_conn_id = get_random_string()",
            "    conn_env_var = f\"{CONN_ENV_PREFIX}{transient_conn_id.upper()}\"",
            "    try:",
            "        data = connection_schema.load(body)",
            "        data[\"conn_id\"] = transient_conn_id",
            "        conn = Connection(**data)",
            "        os.environ[conn_env_var] = conn.get_uri()",
            "        status, message = conn.test_connection()",
            "        return connection_test_schema.dump({\"status\": status, \"message\": message})",
            "    except ValidationError as err:",
            "        raise BadRequest(detail=str(err.messages))",
            "    finally:",
            "        if conn_env_var in os.environ:",
            "            del os.environ[conn_env_var]"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import os",
            "from http import HTTPStatus",
            "",
            "from connexion import NoContent",
            "from flask import Response, request",
            "from marshmallow import ValidationError",
            "from sqlalchemy import func, select",
            "from sqlalchemy.orm import Session",
            "",
            "from airflow.api_connexion import security",
            "from airflow.api_connexion.endpoints.update_mask import extract_update_mask_data",
            "from airflow.api_connexion.exceptions import AlreadyExists, BadRequest, NotFound",
            "from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
            "from airflow.api_connexion.schemas.connection_schema import (",
            "    ConnectionCollection,",
            "    connection_collection_schema,",
            "    connection_schema,",
            "    connection_test_schema,",
            ")",
            "from airflow.api_connexion.types import APIResponse, UpdateMask",
            "from airflow.configuration import conf",
            "from airflow.models import Connection",
            "from airflow.secrets.environment_variables import CONN_ENV_PREFIX",
            "from airflow.security import permissions",
            "from airflow.utils import helpers",
            "from airflow.utils.log.action_logger import action_event_from_permission",
            "from airflow.utils.session import NEW_SESSION, provide_session",
            "from airflow.utils.strings import get_random_string",
            "from airflow.www.decorators import action_logging",
            "",
            "RESOURCE_EVENT_PREFIX = \"connection\"",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_DELETE, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "@action_logging(",
            "    event=action_event_from_permission(",
            "        prefix=RESOURCE_EVENT_PREFIX,",
            "        permission=permissions.ACTION_CAN_DELETE,",
            "    ),",
            ")",
            "def delete_connection(*, connection_id: str, session: Session = NEW_SESSION) -> APIResponse:",
            "    \"\"\"Delete a connection entry.\"\"\"",
            "    connection = session.scalar(select(Connection).filter_by(conn_id=connection_id))",
            "    if connection is None:",
            "        raise NotFound(",
            "            \"Connection not found\",",
            "            detail=f\"The Connection with connection_id: `{connection_id}` was not found\",",
            "        )",
            "    session.delete(connection)",
            "    return NoContent, HTTPStatus.NO_CONTENT",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "def get_connection(*, connection_id: str, session: Session = NEW_SESSION) -> APIResponse:",
            "    \"\"\"Get a connection entry.\"\"\"",
            "    connection = session.scalar(select(Connection).where(Connection.conn_id == connection_id))",
            "    if connection is None:",
            "        raise NotFound(",
            "            \"Connection not found\",",
            "            detail=f\"The Connection with connection_id: `{connection_id}` was not found\",",
            "        )",
            "    return connection_schema.dump(connection)",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONNECTION)])",
            "@format_parameters({\"limit\": check_limit})",
            "@provide_session",
            "def get_connections(",
            "    *,",
            "    limit: int,",
            "    offset: int = 0,",
            "    order_by: str = \"id\",",
            "    session: Session = NEW_SESSION,",
            ") -> APIResponse:",
            "    \"\"\"Get all connection entries.\"\"\"",
            "    to_replace = {\"connection_id\": \"conn_id\"}",
            "    allowed_filter_attrs = [\"connection_id\", \"conn_type\", \"description\", \"host\", \"port\", \"id\"]",
            "",
            "    total_entries = session.execute(select(func.count(Connection.id))).scalar_one()",
            "    query = select(Connection)",
            "    query = apply_sorting(query, order_by, to_replace, allowed_filter_attrs)",
            "    connections = session.scalars(query.offset(offset).limit(limit)).all()",
            "    return connection_collection_schema.dump(",
            "        ConnectionCollection(connections=connections, total_entries=total_entries)",
            "    )",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "@action_logging(",
            "    event=action_event_from_permission(",
            "        prefix=RESOURCE_EVENT_PREFIX,",
            "        permission=permissions.ACTION_CAN_EDIT,",
            "    ),",
            ")",
            "def patch_connection(",
            "    *,",
            "    connection_id: str,",
            "    update_mask: UpdateMask = None,",
            "    session: Session = NEW_SESSION,",
            ") -> APIResponse:",
            "    \"\"\"Update a connection entry.\"\"\"",
            "    try:",
            "        data = connection_schema.load(request.json, partial=True)",
            "    except ValidationError as err:",
            "        # If validation get to here, it is extra field validation.",
            "        raise BadRequest(detail=str(err.messages))",
            "    non_update_fields = [\"connection_id\", \"conn_id\"]",
            "    connection = session.scalar(select(Connection).filter_by(conn_id=connection_id).limit(1))",
            "    if connection is None:",
            "        raise NotFound(",
            "            \"Connection not found\",",
            "            detail=f\"The Connection with connection_id: `{connection_id}` was not found\",",
            "        )",
            "    if data.get(\"conn_id\") and connection.conn_id != data[\"conn_id\"]:",
            "        raise BadRequest(detail=\"The connection_id cannot be updated.\")",
            "    if update_mask:",
            "        data = extract_update_mask_data(update_mask, non_update_fields, data)",
            "    for key in data:",
            "        setattr(connection, key, data[key])",
            "    session.add(connection)",
            "    session.commit()",
            "    return connection_schema.dump(connection)",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_CONNECTION)])",
            "@provide_session",
            "@action_logging(",
            "    event=action_event_from_permission(",
            "        prefix=RESOURCE_EVENT_PREFIX,",
            "        permission=permissions.ACTION_CAN_CREATE,",
            "    ),",
            ")",
            "def post_connection(*, session: Session = NEW_SESSION) -> APIResponse:",
            "    \"\"\"Create connection entry.\"\"\"",
            "    body = request.json",
            "    try:",
            "        data = connection_schema.load(body)",
            "    except ValidationError as err:",
            "        raise BadRequest(detail=str(err.messages))",
            "    conn_id = data[\"conn_id\"]",
            "    try:",
            "        helpers.validate_key(conn_id, max_length=200)",
            "    except Exception as e:",
            "        raise BadRequest(detail=str(e))",
            "    connection = session.scalar(select(Connection).filter_by(conn_id=conn_id).limit(1))",
            "    if not connection:",
            "        connection = Connection(**data)",
            "        session.add(connection)",
            "        session.commit()",
            "        return connection_schema.dump(connection)",
            "    raise AlreadyExists(detail=f\"Connection already exist. ID: {conn_id}\")",
            "",
            "",
            "@security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_CONNECTION)])",
            "def test_connection() -> APIResponse:",
            "    \"\"\"",
            "    Test an API connection.",
            "",
            "    This method first creates an in-memory transient conn_id & exports that to an",
            "    env var, as some hook classes tries to find out the conn from their __init__ method & errors out",
            "    if not found. It also deletes the conn id env variable after the test.",
            "    \"\"\"",
            "    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":",
            "        return Response(",
            "            \"Testing connections is disabled in Airflow configuration. Contact your deployment admin to \"",
            "            \"enable it.\",",
            "            403,",
            "        )",
            "",
            "    body = request.json",
            "    transient_conn_id = get_random_string()",
            "    conn_env_var = f\"{CONN_ENV_PREFIX}{transient_conn_id.upper()}\"",
            "    try:",
            "        data = connection_schema.load(body)",
            "        data[\"conn_id\"] = transient_conn_id",
            "        conn = Connection(**data)",
            "        os.environ[conn_env_var] = conn.get_uri()",
            "        status, message = conn.test_connection()",
            "        return connection_test_schema.dump({\"status\": status, \"message\": message})",
            "    except ValidationError as err:",
            "        raise BadRequest(detail=str(err.messages))",
            "    finally:",
            "        if conn_env_var in os.environ:",
            "            del os.environ[conn_env_var]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "23": []
        },
        "addLocation": []
    },
    "airflow/cli/commands/connection_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from airflow.cli.simple_table import AirflowConsole"
            },
            "2": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from airflow.compat.functools import cache"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+from airflow.configuration import conf"
            },
            "4": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from airflow.exceptions import AirflowNotFoundException"
            },
            "5": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " from airflow.hooks.base import BaseHook"
            },
            "6": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from airflow.models import Connection"
            },
            "7": {
                "beforePatchRowNumber": 343,
                "afterPatchRowNumber": 344,
                "PatchRowcode": " def connections_test(args) -> None:"
            },
            "8": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": 345,
                "PatchRowcode": "     \"\"\"Test an Airflow connection.\"\"\""
            },
            "9": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "     console = AirflowConsole()"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 347,
                "PatchRowcode": "+    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 348,
                "PatchRowcode": "+        console.print("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 349,
                "PatchRowcode": "+            \"[bold yellow]\\nTesting connections is disabled in Airflow configuration. \""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 350,
                "PatchRowcode": "+            \"Contact your deployment admin to enable it.\\n\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 351,
                "PatchRowcode": "+        )"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 352,
                "PatchRowcode": "+        raise SystemExit(1)"
            },
            "16": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 353,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": 354,
                "PatchRowcode": "     print(f\"Retrieving connection: {args.conn_id!r}\")"
            },
            "18": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": 355,
                "PatchRowcode": "     try:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Connection sub-commands.\"\"\"",
            "from __future__ import annotations",
            "",
            "import io",
            "import json",
            "import os",
            "import sys",
            "import warnings",
            "from pathlib import Path",
            "from typing import Any",
            "from urllib.parse import urlsplit, urlunsplit",
            "",
            "from sqlalchemy.orm import exc",
            "",
            "from airflow.cli.simple_table import AirflowConsole",
            "from airflow.compat.functools import cache",
            "from airflow.exceptions import AirflowNotFoundException",
            "from airflow.hooks.base import BaseHook",
            "from airflow.models import Connection",
            "from airflow.providers_manager import ProvidersManager",
            "from airflow.secrets.local_filesystem import load_connections_dict",
            "from airflow.utils import cli as cli_utils, helpers, yaml",
            "from airflow.utils.cli import suppress_logs_and_warning",
            "from airflow.utils.session import create_session",
            "",
            "",
            "def _connection_mapper(conn: Connection) -> dict[str, Any]:",
            "    return {",
            "        \"id\": conn.id,",
            "        \"conn_id\": conn.conn_id,",
            "        \"conn_type\": conn.conn_type,",
            "        \"description\": conn.description,",
            "        \"host\": conn.host,",
            "        \"schema\": conn.schema,",
            "        \"login\": conn.login,",
            "        \"password\": conn.password,",
            "        \"port\": conn.port,",
            "        \"is_encrypted\": conn.is_encrypted,",
            "        \"is_extra_encrypted\": conn.is_encrypted,",
            "        \"extra_dejson\": conn.extra_dejson,",
            "        \"get_uri\": conn.get_uri(),",
            "    }",
            "",
            "",
            "@suppress_logs_and_warning",
            "def connections_get(args):",
            "    \"\"\"Get a connection.\"\"\"",
            "    try:",
            "        conn = BaseHook.get_connection(args.conn_id)",
            "    except AirflowNotFoundException:",
            "        raise SystemExit(\"Connection not found.\")",
            "    AirflowConsole().print_as(",
            "        data=[conn],",
            "        output=args.output,",
            "        mapper=_connection_mapper,",
            "    )",
            "",
            "",
            "@suppress_logs_and_warning",
            "def connections_list(args):",
            "    \"\"\"Lists all connections at the command line.\"\"\"",
            "    with create_session() as session:",
            "        query = session.query(Connection)",
            "        if args.conn_id:",
            "            query = query.filter(Connection.conn_id == args.conn_id)",
            "        conns = query.all()",
            "",
            "        AirflowConsole().print_as(",
            "            data=conns,",
            "            output=args.output,",
            "            mapper=_connection_mapper,",
            "        )",
            "",
            "",
            "def _connection_to_dict(conn: Connection) -> dict:",
            "    return dict(",
            "        conn_type=conn.conn_type,",
            "        description=conn.description,",
            "        login=conn.login,",
            "        password=conn.password,",
            "        host=conn.host,",
            "        port=conn.port,",
            "        schema=conn.schema,",
            "        extra=conn.extra,",
            "    )",
            "",
            "",
            "def _format_connections(conns: list[Connection], file_format: str, serialization_format: str) -> str:",
            "    if serialization_format == \"json\":",
            "        serializer_func = lambda x: json.dumps(_connection_to_dict(x))",
            "    elif serialization_format == \"uri\":",
            "        serializer_func = Connection.get_uri",
            "    else:",
            "        raise SystemExit(f\"Received unexpected value for `--serialization-format`: {serialization_format!r}\")",
            "    if file_format == \".env\":",
            "        connections_env = \"\"",
            "        for conn in conns:",
            "            connections_env += f\"{conn.conn_id}={serializer_func(conn)}\\n\"",
            "        return connections_env",
            "",
            "    connections_dict = {}",
            "    for conn in conns:",
            "        connections_dict[conn.conn_id] = _connection_to_dict(conn)",
            "",
            "    if file_format == \".yaml\":",
            "        return yaml.dump(connections_dict)",
            "",
            "    if file_format == \".json\":",
            "        return json.dumps(connections_dict, indent=2)",
            "",
            "    return json.dumps(connections_dict)",
            "",
            "",
            "def _is_stdout(fileio: io.TextIOWrapper) -> bool:",
            "    return fileio.name == \"<stdout>\"",
            "",
            "",
            "def _valid_uri(uri: str) -> bool:",
            "    \"\"\"Check if a URI is valid, by checking if scheme (conn_type) provided.\"\"\"",
            "    return urlsplit(uri).scheme != \"\"",
            "",
            "",
            "@cache",
            "def _get_connection_types() -> list[str]:",
            "    \"\"\"Returns connection types available.\"\"\"",
            "    _connection_types = [\"fs\", \"mesos_framework-id\", \"email\", \"generic\"]",
            "    providers_manager = ProvidersManager()",
            "    for connection_type, provider_info in providers_manager.hooks.items():",
            "        if provider_info:",
            "            _connection_types.append(connection_type)",
            "    return _connection_types",
            "",
            "",
            "def connections_export(args):",
            "    \"\"\"Exports all connections to a file.\"\"\"",
            "    file_formats = [\".yaml\", \".json\", \".env\"]",
            "    if args.format:",
            "        warnings.warn(\"Option `--format` is deprecated.  Use `--file-format` instead.\", DeprecationWarning)",
            "    if args.format and args.file_format:",
            "        raise SystemExit(\"Option `--format` is deprecated.  Use `--file-format` instead.\")",
            "    default_format = \".json\"",
            "    provided_file_format = None",
            "    if args.format or args.file_format:",
            "        provided_file_format = f\".{(args.format or args.file_format).lower()}\"",
            "",
            "    file_is_stdout = _is_stdout(args.file)",
            "    if file_is_stdout:",
            "        filetype = provided_file_format or default_format",
            "    elif provided_file_format:",
            "        filetype = provided_file_format",
            "    else:",
            "        filetype = Path(args.file.name).suffix",
            "        filetype = filetype.lower()",
            "        if filetype not in file_formats:",
            "            raise SystemExit(",
            "                f\"Unsupported file format. The file must have the extension {', '.join(file_formats)}.\"",
            "            )",
            "",
            "    if args.serialization_format and not filetype == \".env\":",
            "        raise SystemExit(\"Option `--serialization-format` may only be used with file type `env`.\")",
            "",
            "    with create_session() as session:",
            "        connections = session.query(Connection).order_by(Connection.conn_id).all()",
            "",
            "    msg = _format_connections(",
            "        conns=connections,",
            "        file_format=filetype,",
            "        serialization_format=args.serialization_format or \"uri\",",
            "    )",
            "",
            "    with args.file as f:",
            "        f.write(msg)",
            "",
            "    if file_is_stdout:",
            "        print(\"\\nConnections successfully exported.\", file=sys.stderr)",
            "    else:",
            "        print(f\"Connections successfully exported to {args.file.name}.\")",
            "",
            "",
            "alternative_conn_specs = [\"conn_type\", \"conn_host\", \"conn_login\", \"conn_password\", \"conn_schema\", \"conn_port\"]",
            "",
            "",
            "@cli_utils.action_cli",
            "def connections_add(args):",
            "    \"\"\"Adds new connection.\"\"\"",
            "    has_uri = bool(args.conn_uri)",
            "    has_json = bool(args.conn_json)",
            "    has_type = bool(args.conn_type)",
            "",
            "    # Validate connection-id",
            "    try:",
            "        helpers.validate_key(args.conn_id, max_length=200)",
            "    except Exception as e:",
            "        raise SystemExit(f\"Could not create connection. {e}\")",
            "",
            "    if not has_type and not (has_json or has_uri):",
            "        raise SystemExit(\"Must supply either conn-uri or conn-json if not supplying conn-type\")",
            "",
            "    if has_json and has_uri:",
            "        raise SystemExit(\"Cannot supply both conn-uri and conn-json\")",
            "",
            "    if has_type and args.conn_type not in _get_connection_types():",
            "        warnings.warn(f\"The type provided to --conn-type is invalid: {args.conn_type}\")",
            "        warnings.warn(",
            "            f\"Supported --conn-types are:{_get_connection_types()}.\"",
            "            \"Hence overriding the conn-type with generic\"",
            "        )",
            "        args.conn_type = \"generic\"",
            "",
            "    if has_uri or has_json:",
            "        invalid_args = []",
            "        if has_uri and not _valid_uri(args.conn_uri):",
            "            raise SystemExit(f\"The URI provided to --conn-uri is invalid: {args.conn_uri}\")",
            "",
            "        for arg in alternative_conn_specs:",
            "            if getattr(args, arg) is not None:",
            "                invalid_args.append(arg)",
            "",
            "        if has_json and args.conn_extra:",
            "            invalid_args.append(\"--conn-extra\")",
            "",
            "        if invalid_args:",
            "            raise SystemExit(",
            "                \"The following args are not compatible with \"",
            "                f\"the --conn-{'uri' if has_uri else 'json'} flag: {invalid_args!r}\"",
            "            )",
            "",
            "    if args.conn_uri:",
            "        new_conn = Connection(conn_id=args.conn_id, description=args.conn_description, uri=args.conn_uri)",
            "        if args.conn_extra is not None:",
            "            new_conn.set_extra(args.conn_extra)",
            "    elif args.conn_json:",
            "        new_conn = Connection.from_json(conn_id=args.conn_id, value=args.conn_json)",
            "        if not new_conn.conn_type:",
            "            raise SystemExit(\"conn-json is invalid; must supply conn-type\")",
            "    else:",
            "        new_conn = Connection(",
            "            conn_id=args.conn_id,",
            "            conn_type=args.conn_type,",
            "            description=args.conn_description,",
            "            host=args.conn_host,",
            "            login=args.conn_login,",
            "            password=args.conn_password,",
            "            schema=args.conn_schema,",
            "            port=args.conn_port,",
            "        )",
            "        if args.conn_extra is not None:",
            "            new_conn.set_extra(args.conn_extra)",
            "",
            "    with create_session() as session:",
            "        if not session.query(Connection).filter(Connection.conn_id == new_conn.conn_id).first():",
            "            session.add(new_conn)",
            "            msg = \"Successfully added `conn_id`={conn_id} : {uri}\"",
            "            msg = msg.format(",
            "                conn_id=new_conn.conn_id,",
            "                uri=args.conn_uri",
            "                or urlunsplit(",
            "                    (",
            "                        new_conn.conn_type,",
            "                        f\"{new_conn.login or ''}:{'******' if new_conn.password else ''}\"",
            "                        f\"@{new_conn.host or ''}:{new_conn.port or ''}\",",
            "                        new_conn.schema or \"\",",
            "                        \"\",",
            "                        \"\",",
            "                    )",
            "                ),",
            "            )",
            "            print(msg)",
            "        else:",
            "            msg = f\"A connection with `conn_id`={new_conn.conn_id} already exists.\"",
            "            raise SystemExit(msg)",
            "",
            "",
            "@cli_utils.action_cli",
            "def connections_delete(args):",
            "    \"\"\"Deletes connection from DB.\"\"\"",
            "    with create_session() as session:",
            "        try:",
            "            to_delete = session.query(Connection).filter(Connection.conn_id == args.conn_id).one()",
            "        except exc.NoResultFound:",
            "            raise SystemExit(f\"Did not find a connection with `conn_id`={args.conn_id}\")",
            "        except exc.MultipleResultsFound:",
            "            raise SystemExit(f\"Found more than one connection with `conn_id`={args.conn_id}\")",
            "        else:",
            "            session.delete(to_delete)",
            "            print(f\"Successfully deleted connection with `conn_id`={to_delete.conn_id}\")",
            "",
            "",
            "@cli_utils.action_cli(check_db=False)",
            "def connections_import(args):",
            "    \"\"\"Imports connections from a file.\"\"\"",
            "    if os.path.exists(args.file):",
            "        _import_helper(args.file, args.overwrite)",
            "    else:",
            "        raise SystemExit(\"Missing connections file.\")",
            "",
            "",
            "def _import_helper(file_path: str, overwrite: bool) -> None:",
            "    \"\"\"Load connections from a file and save them to the DB.",
            "",
            "    :param overwrite: Whether to skip or overwrite on collision.",
            "    \"\"\"",
            "    connections_dict = load_connections_dict(file_path)",
            "    with create_session() as session:",
            "        for conn_id, conn in connections_dict.items():",
            "            try:",
            "                helpers.validate_key(conn_id, max_length=200)",
            "            except Exception as e:",
            "                print(f\"Could not import connection. {e}\")",
            "                continue",
            "",
            "            existing_conn_id = session.query(Connection.id).filter(Connection.conn_id == conn_id).scalar()",
            "            if existing_conn_id is not None:",
            "                if not overwrite:",
            "                    print(f\"Could not import connection {conn_id}: connection already exists.\")",
            "                    continue",
            "",
            "                # The conn_ids match, but the PK of the new entry must also be the same as the old",
            "                conn.id = existing_conn_id",
            "",
            "            session.merge(conn)",
            "            session.commit()",
            "            print(f\"Imported connection {conn_id}\")",
            "",
            "",
            "@suppress_logs_and_warning",
            "def connections_test(args) -> None:",
            "    \"\"\"Test an Airflow connection.\"\"\"",
            "    console = AirflowConsole()",
            "",
            "    print(f\"Retrieving connection: {args.conn_id!r}\")",
            "    try:",
            "        conn = BaseHook.get_connection(args.conn_id)",
            "    except AirflowNotFoundException:",
            "        console.print(\"[bold yellow]\\nConnection not found.\\n\")",
            "        raise SystemExit(1)",
            "",
            "    print(\"\\nTesting...\")",
            "    status, message = conn.test_connection()",
            "    if status is True:",
            "        console.print(\"[bold green]\\nConnection success!\\n\")",
            "    else:",
            "        console.print(f\"[bold][red]\\nConnection failed![/bold]\\n{message}\\n\")"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"Connection sub-commands.\"\"\"",
            "from __future__ import annotations",
            "",
            "import io",
            "import json",
            "import os",
            "import sys",
            "import warnings",
            "from pathlib import Path",
            "from typing import Any",
            "from urllib.parse import urlsplit, urlunsplit",
            "",
            "from sqlalchemy.orm import exc",
            "",
            "from airflow.cli.simple_table import AirflowConsole",
            "from airflow.compat.functools import cache",
            "from airflow.configuration import conf",
            "from airflow.exceptions import AirflowNotFoundException",
            "from airflow.hooks.base import BaseHook",
            "from airflow.models import Connection",
            "from airflow.providers_manager import ProvidersManager",
            "from airflow.secrets.local_filesystem import load_connections_dict",
            "from airflow.utils import cli as cli_utils, helpers, yaml",
            "from airflow.utils.cli import suppress_logs_and_warning",
            "from airflow.utils.session import create_session",
            "",
            "",
            "def _connection_mapper(conn: Connection) -> dict[str, Any]:",
            "    return {",
            "        \"id\": conn.id,",
            "        \"conn_id\": conn.conn_id,",
            "        \"conn_type\": conn.conn_type,",
            "        \"description\": conn.description,",
            "        \"host\": conn.host,",
            "        \"schema\": conn.schema,",
            "        \"login\": conn.login,",
            "        \"password\": conn.password,",
            "        \"port\": conn.port,",
            "        \"is_encrypted\": conn.is_encrypted,",
            "        \"is_extra_encrypted\": conn.is_encrypted,",
            "        \"extra_dejson\": conn.extra_dejson,",
            "        \"get_uri\": conn.get_uri(),",
            "    }",
            "",
            "",
            "@suppress_logs_and_warning",
            "def connections_get(args):",
            "    \"\"\"Get a connection.\"\"\"",
            "    try:",
            "        conn = BaseHook.get_connection(args.conn_id)",
            "    except AirflowNotFoundException:",
            "        raise SystemExit(\"Connection not found.\")",
            "    AirflowConsole().print_as(",
            "        data=[conn],",
            "        output=args.output,",
            "        mapper=_connection_mapper,",
            "    )",
            "",
            "",
            "@suppress_logs_and_warning",
            "def connections_list(args):",
            "    \"\"\"Lists all connections at the command line.\"\"\"",
            "    with create_session() as session:",
            "        query = session.query(Connection)",
            "        if args.conn_id:",
            "            query = query.filter(Connection.conn_id == args.conn_id)",
            "        conns = query.all()",
            "",
            "        AirflowConsole().print_as(",
            "            data=conns,",
            "            output=args.output,",
            "            mapper=_connection_mapper,",
            "        )",
            "",
            "",
            "def _connection_to_dict(conn: Connection) -> dict:",
            "    return dict(",
            "        conn_type=conn.conn_type,",
            "        description=conn.description,",
            "        login=conn.login,",
            "        password=conn.password,",
            "        host=conn.host,",
            "        port=conn.port,",
            "        schema=conn.schema,",
            "        extra=conn.extra,",
            "    )",
            "",
            "",
            "def _format_connections(conns: list[Connection], file_format: str, serialization_format: str) -> str:",
            "    if serialization_format == \"json\":",
            "        serializer_func = lambda x: json.dumps(_connection_to_dict(x))",
            "    elif serialization_format == \"uri\":",
            "        serializer_func = Connection.get_uri",
            "    else:",
            "        raise SystemExit(f\"Received unexpected value for `--serialization-format`: {serialization_format!r}\")",
            "    if file_format == \".env\":",
            "        connections_env = \"\"",
            "        for conn in conns:",
            "            connections_env += f\"{conn.conn_id}={serializer_func(conn)}\\n\"",
            "        return connections_env",
            "",
            "    connections_dict = {}",
            "    for conn in conns:",
            "        connections_dict[conn.conn_id] = _connection_to_dict(conn)",
            "",
            "    if file_format == \".yaml\":",
            "        return yaml.dump(connections_dict)",
            "",
            "    if file_format == \".json\":",
            "        return json.dumps(connections_dict, indent=2)",
            "",
            "    return json.dumps(connections_dict)",
            "",
            "",
            "def _is_stdout(fileio: io.TextIOWrapper) -> bool:",
            "    return fileio.name == \"<stdout>\"",
            "",
            "",
            "def _valid_uri(uri: str) -> bool:",
            "    \"\"\"Check if a URI is valid, by checking if scheme (conn_type) provided.\"\"\"",
            "    return urlsplit(uri).scheme != \"\"",
            "",
            "",
            "@cache",
            "def _get_connection_types() -> list[str]:",
            "    \"\"\"Returns connection types available.\"\"\"",
            "    _connection_types = [\"fs\", \"mesos_framework-id\", \"email\", \"generic\"]",
            "    providers_manager = ProvidersManager()",
            "    for connection_type, provider_info in providers_manager.hooks.items():",
            "        if provider_info:",
            "            _connection_types.append(connection_type)",
            "    return _connection_types",
            "",
            "",
            "def connections_export(args):",
            "    \"\"\"Exports all connections to a file.\"\"\"",
            "    file_formats = [\".yaml\", \".json\", \".env\"]",
            "    if args.format:",
            "        warnings.warn(\"Option `--format` is deprecated.  Use `--file-format` instead.\", DeprecationWarning)",
            "    if args.format and args.file_format:",
            "        raise SystemExit(\"Option `--format` is deprecated.  Use `--file-format` instead.\")",
            "    default_format = \".json\"",
            "    provided_file_format = None",
            "    if args.format or args.file_format:",
            "        provided_file_format = f\".{(args.format or args.file_format).lower()}\"",
            "",
            "    file_is_stdout = _is_stdout(args.file)",
            "    if file_is_stdout:",
            "        filetype = provided_file_format or default_format",
            "    elif provided_file_format:",
            "        filetype = provided_file_format",
            "    else:",
            "        filetype = Path(args.file.name).suffix",
            "        filetype = filetype.lower()",
            "        if filetype not in file_formats:",
            "            raise SystemExit(",
            "                f\"Unsupported file format. The file must have the extension {', '.join(file_formats)}.\"",
            "            )",
            "",
            "    if args.serialization_format and not filetype == \".env\":",
            "        raise SystemExit(\"Option `--serialization-format` may only be used with file type `env`.\")",
            "",
            "    with create_session() as session:",
            "        connections = session.query(Connection).order_by(Connection.conn_id).all()",
            "",
            "    msg = _format_connections(",
            "        conns=connections,",
            "        file_format=filetype,",
            "        serialization_format=args.serialization_format or \"uri\",",
            "    )",
            "",
            "    with args.file as f:",
            "        f.write(msg)",
            "",
            "    if file_is_stdout:",
            "        print(\"\\nConnections successfully exported.\", file=sys.stderr)",
            "    else:",
            "        print(f\"Connections successfully exported to {args.file.name}.\")",
            "",
            "",
            "alternative_conn_specs = [\"conn_type\", \"conn_host\", \"conn_login\", \"conn_password\", \"conn_schema\", \"conn_port\"]",
            "",
            "",
            "@cli_utils.action_cli",
            "def connections_add(args):",
            "    \"\"\"Adds new connection.\"\"\"",
            "    has_uri = bool(args.conn_uri)",
            "    has_json = bool(args.conn_json)",
            "    has_type = bool(args.conn_type)",
            "",
            "    # Validate connection-id",
            "    try:",
            "        helpers.validate_key(args.conn_id, max_length=200)",
            "    except Exception as e:",
            "        raise SystemExit(f\"Could not create connection. {e}\")",
            "",
            "    if not has_type and not (has_json or has_uri):",
            "        raise SystemExit(\"Must supply either conn-uri or conn-json if not supplying conn-type\")",
            "",
            "    if has_json and has_uri:",
            "        raise SystemExit(\"Cannot supply both conn-uri and conn-json\")",
            "",
            "    if has_type and args.conn_type not in _get_connection_types():",
            "        warnings.warn(f\"The type provided to --conn-type is invalid: {args.conn_type}\")",
            "        warnings.warn(",
            "            f\"Supported --conn-types are:{_get_connection_types()}.\"",
            "            \"Hence overriding the conn-type with generic\"",
            "        )",
            "        args.conn_type = \"generic\"",
            "",
            "    if has_uri or has_json:",
            "        invalid_args = []",
            "        if has_uri and not _valid_uri(args.conn_uri):",
            "            raise SystemExit(f\"The URI provided to --conn-uri is invalid: {args.conn_uri}\")",
            "",
            "        for arg in alternative_conn_specs:",
            "            if getattr(args, arg) is not None:",
            "                invalid_args.append(arg)",
            "",
            "        if has_json and args.conn_extra:",
            "            invalid_args.append(\"--conn-extra\")",
            "",
            "        if invalid_args:",
            "            raise SystemExit(",
            "                \"The following args are not compatible with \"",
            "                f\"the --conn-{'uri' if has_uri else 'json'} flag: {invalid_args!r}\"",
            "            )",
            "",
            "    if args.conn_uri:",
            "        new_conn = Connection(conn_id=args.conn_id, description=args.conn_description, uri=args.conn_uri)",
            "        if args.conn_extra is not None:",
            "            new_conn.set_extra(args.conn_extra)",
            "    elif args.conn_json:",
            "        new_conn = Connection.from_json(conn_id=args.conn_id, value=args.conn_json)",
            "        if not new_conn.conn_type:",
            "            raise SystemExit(\"conn-json is invalid; must supply conn-type\")",
            "    else:",
            "        new_conn = Connection(",
            "            conn_id=args.conn_id,",
            "            conn_type=args.conn_type,",
            "            description=args.conn_description,",
            "            host=args.conn_host,",
            "            login=args.conn_login,",
            "            password=args.conn_password,",
            "            schema=args.conn_schema,",
            "            port=args.conn_port,",
            "        )",
            "        if args.conn_extra is not None:",
            "            new_conn.set_extra(args.conn_extra)",
            "",
            "    with create_session() as session:",
            "        if not session.query(Connection).filter(Connection.conn_id == new_conn.conn_id).first():",
            "            session.add(new_conn)",
            "            msg = \"Successfully added `conn_id`={conn_id} : {uri}\"",
            "            msg = msg.format(",
            "                conn_id=new_conn.conn_id,",
            "                uri=args.conn_uri",
            "                or urlunsplit(",
            "                    (",
            "                        new_conn.conn_type,",
            "                        f\"{new_conn.login or ''}:{'******' if new_conn.password else ''}\"",
            "                        f\"@{new_conn.host or ''}:{new_conn.port or ''}\",",
            "                        new_conn.schema or \"\",",
            "                        \"\",",
            "                        \"\",",
            "                    )",
            "                ),",
            "            )",
            "            print(msg)",
            "        else:",
            "            msg = f\"A connection with `conn_id`={new_conn.conn_id} already exists.\"",
            "            raise SystemExit(msg)",
            "",
            "",
            "@cli_utils.action_cli",
            "def connections_delete(args):",
            "    \"\"\"Deletes connection from DB.\"\"\"",
            "    with create_session() as session:",
            "        try:",
            "            to_delete = session.query(Connection).filter(Connection.conn_id == args.conn_id).one()",
            "        except exc.NoResultFound:",
            "            raise SystemExit(f\"Did not find a connection with `conn_id`={args.conn_id}\")",
            "        except exc.MultipleResultsFound:",
            "            raise SystemExit(f\"Found more than one connection with `conn_id`={args.conn_id}\")",
            "        else:",
            "            session.delete(to_delete)",
            "            print(f\"Successfully deleted connection with `conn_id`={to_delete.conn_id}\")",
            "",
            "",
            "@cli_utils.action_cli(check_db=False)",
            "def connections_import(args):",
            "    \"\"\"Imports connections from a file.\"\"\"",
            "    if os.path.exists(args.file):",
            "        _import_helper(args.file, args.overwrite)",
            "    else:",
            "        raise SystemExit(\"Missing connections file.\")",
            "",
            "",
            "def _import_helper(file_path: str, overwrite: bool) -> None:",
            "    \"\"\"Load connections from a file and save them to the DB.",
            "",
            "    :param overwrite: Whether to skip or overwrite on collision.",
            "    \"\"\"",
            "    connections_dict = load_connections_dict(file_path)",
            "    with create_session() as session:",
            "        for conn_id, conn in connections_dict.items():",
            "            try:",
            "                helpers.validate_key(conn_id, max_length=200)",
            "            except Exception as e:",
            "                print(f\"Could not import connection. {e}\")",
            "                continue",
            "",
            "            existing_conn_id = session.query(Connection.id).filter(Connection.conn_id == conn_id).scalar()",
            "            if existing_conn_id is not None:",
            "                if not overwrite:",
            "                    print(f\"Could not import connection {conn_id}: connection already exists.\")",
            "                    continue",
            "",
            "                # The conn_ids match, but the PK of the new entry must also be the same as the old",
            "                conn.id = existing_conn_id",
            "",
            "            session.merge(conn)",
            "            session.commit()",
            "            print(f\"Imported connection {conn_id}\")",
            "",
            "",
            "@suppress_logs_and_warning",
            "def connections_test(args) -> None:",
            "    \"\"\"Test an Airflow connection.\"\"\"",
            "    console = AirflowConsole()",
            "    if conf.get(\"core\", \"test_connection\", fallback=\"Disabled\").lower().strip() != \"enabled\":",
            "        console.print(",
            "            \"[bold yellow]\\nTesting connections is disabled in Airflow configuration. \"",
            "            \"Contact your deployment admin to enable it.\\n\"",
            "        )",
            "        raise SystemExit(1)",
            "",
            "    print(f\"Retrieving connection: {args.conn_id!r}\")",
            "    try:",
            "        conn = BaseHook.get_connection(args.conn_id)",
            "    except AirflowNotFoundException:",
            "        console.print(\"[bold yellow]\\nConnection not found.\\n\")",
            "        raise SystemExit(1)",
            "",
            "    print(\"\\nTesting...\")",
            "    status, message = conn.test_connection()",
            "    if status is True:",
            "        console.print(\"[bold green]\\nConnection success!\\n\")",
            "    else:",
            "        console.print(f\"[bold][red]\\nConnection failed![/bold]\\n{message}\\n\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "urllib3.util.retry"
        ]
    },
    "airflow/www/extensions/init_jinja_globals.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "             \"git_version\": git_version,"
            },
            "1": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "             \"k8s_or_k8scelery_executor\": IS_K8S_OR_K8SCELERY_EXECUTOR,"
            },
            "2": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "             \"rest_api_enabled\": False,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+            \"config_test_connection\": conf.get(\"core\", \"test_connection\", fallback=\"Disabled\"),"
            },
            "4": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "         }"
            },
            "5": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "         backends = conf.get(\"api\", \"auth_backends\")"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import logging",
            "",
            "import pendulum",
            "",
            "import airflow",
            "from airflow.configuration import conf",
            "from airflow.settings import IS_K8S_OR_K8SCELERY_EXECUTOR, STATE_COLORS",
            "from airflow.utils.net import get_hostname",
            "from airflow.utils.platform import get_airflow_git_version",
            "",
            "",
            "def init_jinja_globals(app):",
            "    \"\"\"Add extra globals variable to Jinja context.\"\"\"",
            "    server_timezone = conf.get(\"core\", \"default_timezone\")",
            "    if server_timezone == \"system\":",
            "        server_timezone = pendulum.local_timezone().name",
            "    elif server_timezone == \"utc\":",
            "        server_timezone = \"UTC\"",
            "",
            "    default_ui_timezone = conf.get(\"webserver\", \"default_ui_timezone\")",
            "    if default_ui_timezone == \"system\":",
            "        default_ui_timezone = pendulum.local_timezone().name",
            "    elif default_ui_timezone == \"utc\":",
            "        default_ui_timezone = \"UTC\"",
            "    if not default_ui_timezone:",
            "        default_ui_timezone = server_timezone",
            "",
            "    expose_hostname = conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")",
            "    hostname = get_hostname() if expose_hostname else \"redact\"",
            "",
            "    try:",
            "        airflow_version = airflow.__version__",
            "    except Exception as e:",
            "        airflow_version = None",
            "        logging.error(e)",
            "",
            "    git_version = get_airflow_git_version()",
            "",
            "    def prepare_jinja_globals():",
            "        extra_globals = {",
            "            \"server_timezone\": server_timezone,",
            "            \"default_ui_timezone\": default_ui_timezone,",
            "            \"hostname\": hostname,",
            "            \"navbar_color\": conf.get(\"webserver\", \"NAVBAR_COLOR\"),",
            "            \"log_fetch_delay_sec\": conf.getint(\"webserver\", \"log_fetch_delay_sec\", fallback=2),",
            "            \"log_auto_tailing_offset\": conf.getint(\"webserver\", \"log_auto_tailing_offset\", fallback=30),",
            "            \"log_animation_speed\": conf.getint(\"webserver\", \"log_animation_speed\", fallback=1000),",
            "            \"state_color_mapping\": STATE_COLORS,",
            "            \"airflow_version\": airflow_version,",
            "            \"git_version\": git_version,",
            "            \"k8s_or_k8scelery_executor\": IS_K8S_OR_K8SCELERY_EXECUTOR,",
            "            \"rest_api_enabled\": False,",
            "        }",
            "",
            "        backends = conf.get(\"api\", \"auth_backends\")",
            "        if len(backends) > 0 and backends[0] != \"airflow.api.auth.backend.deny_all\":",
            "            extra_globals[\"rest_api_enabled\"] = True",
            "",
            "        if \"analytics_tool\" in conf.getsection(\"webserver\"):",
            "            extra_globals.update(",
            "                {",
            "                    \"analytics_tool\": conf.get(\"webserver\", \"ANALYTICS_TOOL\"),",
            "                    \"analytics_id\": conf.get(\"webserver\", \"ANALYTICS_ID\"),",
            "                }",
            "            )",
            "",
            "        return extra_globals",
            "",
            "    app.context_processor(prepare_jinja_globals)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import logging",
            "",
            "import pendulum",
            "",
            "import airflow",
            "from airflow.configuration import conf",
            "from airflow.settings import IS_K8S_OR_K8SCELERY_EXECUTOR, STATE_COLORS",
            "from airflow.utils.net import get_hostname",
            "from airflow.utils.platform import get_airflow_git_version",
            "",
            "",
            "def init_jinja_globals(app):",
            "    \"\"\"Add extra globals variable to Jinja context.\"\"\"",
            "    server_timezone = conf.get(\"core\", \"default_timezone\")",
            "    if server_timezone == \"system\":",
            "        server_timezone = pendulum.local_timezone().name",
            "    elif server_timezone == \"utc\":",
            "        server_timezone = \"UTC\"",
            "",
            "    default_ui_timezone = conf.get(\"webserver\", \"default_ui_timezone\")",
            "    if default_ui_timezone == \"system\":",
            "        default_ui_timezone = pendulum.local_timezone().name",
            "    elif default_ui_timezone == \"utc\":",
            "        default_ui_timezone = \"UTC\"",
            "    if not default_ui_timezone:",
            "        default_ui_timezone = server_timezone",
            "",
            "    expose_hostname = conf.getboolean(\"webserver\", \"EXPOSE_HOSTNAME\")",
            "    hostname = get_hostname() if expose_hostname else \"redact\"",
            "",
            "    try:",
            "        airflow_version = airflow.__version__",
            "    except Exception as e:",
            "        airflow_version = None",
            "        logging.error(e)",
            "",
            "    git_version = get_airflow_git_version()",
            "",
            "    def prepare_jinja_globals():",
            "        extra_globals = {",
            "            \"server_timezone\": server_timezone,",
            "            \"default_ui_timezone\": default_ui_timezone,",
            "            \"hostname\": hostname,",
            "            \"navbar_color\": conf.get(\"webserver\", \"NAVBAR_COLOR\"),",
            "            \"log_fetch_delay_sec\": conf.getint(\"webserver\", \"log_fetch_delay_sec\", fallback=2),",
            "            \"log_auto_tailing_offset\": conf.getint(\"webserver\", \"log_auto_tailing_offset\", fallback=30),",
            "            \"log_animation_speed\": conf.getint(\"webserver\", \"log_animation_speed\", fallback=1000),",
            "            \"state_color_mapping\": STATE_COLORS,",
            "            \"airflow_version\": airflow_version,",
            "            \"git_version\": git_version,",
            "            \"k8s_or_k8scelery_executor\": IS_K8S_OR_K8SCELERY_EXECUTOR,",
            "            \"rest_api_enabled\": False,",
            "            \"config_test_connection\": conf.get(\"core\", \"test_connection\", fallback=\"Disabled\"),",
            "        }",
            "",
            "        backends = conf.get(\"api\", \"auth_backends\")",
            "        if len(backends) > 0 and backends[0] != \"airflow.api.auth.backend.deny_all\":",
            "            extra_globals[\"rest_api_enabled\"] = True",
            "",
            "        if \"analytics_tool\" in conf.getsection(\"webserver\"):",
            "            extra_globals.update(",
            "                {",
            "                    \"analytics_tool\": conf.get(\"webserver\", \"ANALYTICS_TOOL\"),",
            "                    \"analytics_id\": conf.get(\"webserver\", \"ANALYTICS_ID\"),",
            "                }",
            "            )",
            "",
            "        return extra_globals",
            "",
            "    app.context_processor(prepare_jinja_globals)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    }
}