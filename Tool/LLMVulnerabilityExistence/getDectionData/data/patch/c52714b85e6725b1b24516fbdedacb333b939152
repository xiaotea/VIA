{
    "src/check_jsonschema/cachedownloader.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " from __future__ import annotations"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import contextlib"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 4,
                "PatchRowcode": "+import hashlib"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import io"
            },
            "5": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " import os"
            },
            "6": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " import platform"
            },
            "7": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "     return cache_dir"
            },
            "8": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def _resolve_cache_dir(dirname: str = \"downloads\") -> str | None:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+def _resolve_cache_dir(dirname: str) -> str | None:"
            },
            "12": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "     cache_dir = _base_cache_dir()"
            },
            "13": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     if cache_dir:"
            },
            "14": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "         cache_dir = os.path.join(cache_dir, \"check_jsonschema\", dirname)"
            },
            "15": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "     return local_mtime >= remote_mtime"
            },
            "16": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 97,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 98,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+def url_to_cache_filename(ref_url: str) -> str:"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+    \"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+    Given a schema URL, convert it to a filename for caching in a cache dir."
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+    Rules are as follows:"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+    - the base filename is an sha256 hash of the URL"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+    - if the filename ends in an extension (.json, .yaml, etc) that extension"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+      is appended to the hash"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+    Preserving file extensions preserves the extension-based logic used for parsing, and"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+    it also helps a local editor (browsing the cache) identify filetypes."
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+    \"\"\""
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+    filename = hashlib.sha256(ref_url.encode()).hexdigest()"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+        _, _, extension = last_part.rpartition(\".\")"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+        filename = f\"{filename}.{extension}\""
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+    return filename"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+"
            },
            "37": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 118,
                "PatchRowcode": " class FailedDownloadError(Exception):"
            },
            "38": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "     pass"
            },
            "39": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 120,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 121,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 122,
                "PatchRowcode": " class CacheDownloader:"
            },
            "42": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __init__("
            },
            "43": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self, cache_dir: str | None = None, disable_cache: bool = False"
            },
            "44": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> None:"
            },
            "45": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if cache_dir is None:"
            },
            "46": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._cache_dir = _resolve_cache_dir()"
            },
            "47": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:"
            },
            "48": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._cache_dir = _resolve_cache_dir(cache_dir)"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+    def __init__(self, cache_dir: str, *, disable_cache: bool = False) -> None:"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        self._cache_dir = _resolve_cache_dir(cache_dir)"
            },
            "51": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "         self._disable_cache = disable_cache"
            },
            "52": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 126,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 127,
                "PatchRowcode": "     def _download("
            },
            "54": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": 175,
                "PatchRowcode": "         validation_callback: t.Callable[[bytes], t.Any] | None = None,"
            },
            "55": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": 176,
                "PatchRowcode": "     ) -> BoundCacheDownloader:"
            },
            "56": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 177,
                "PatchRowcode": "         return BoundCacheDownloader("
            },
            "57": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            file_url, filename, self, validation_callback=validation_callback"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+            file_url, self, filename=filename, validation_callback=validation_callback"
            },
            "59": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 179,
                "PatchRowcode": "         )"
            },
            "60": {
                "beforePatchRowNumber": 165,
                "afterPatchRowNumber": 180,
                "PatchRowcode": " "
            },
            "61": {
                "beforePatchRowNumber": 166,
                "afterPatchRowNumber": 181,
                "PatchRowcode": " "
            },
            "62": {
                "beforePatchRowNumber": 167,
                "afterPatchRowNumber": 182,
                "PatchRowcode": " class BoundCacheDownloader:"
            },
            "63": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 183,
                "PatchRowcode": "     def __init__("
            },
            "64": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": 184,
                "PatchRowcode": "         self,"
            },
            "65": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "         file_url: str,"
            },
            "66": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        filename: str | None,"
            },
            "67": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "         downloader: CacheDownloader,"
            },
            "68": {
                "beforePatchRowNumber": 173,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "         *,"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 188,
                "PatchRowcode": "+        filename: str | None = None,"
            },
            "70": {
                "beforePatchRowNumber": 174,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "         validation_callback: t.Callable[[bytes], t.Any] | None = None,"
            },
            "71": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "     ) -> None:"
            },
            "72": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "         self._file_url = file_url"
            },
            "73": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._filename = filename or file_url.split(\"/\")[-1]"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+        self._filename = filename or url_to_cache_filename(file_url)"
            },
            "75": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 193,
                "PatchRowcode": "         self._downloader = downloader"
            },
            "76": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 194,
                "PatchRowcode": "         self._validation_callback = validation_callback"
            },
            "77": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 195,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextlib",
            "import io",
            "import os",
            "import platform",
            "import shutil",
            "import tempfile",
            "import time",
            "import typing as t",
            "",
            "import requests",
            "",
            "_LASTMOD_FMT = \"%a, %d %b %Y %H:%M:%S %Z\"",
            "",
            "",
            "def _base_cache_dir() -> str | None:",
            "    sysname = platform.system()",
            "",
            "    # on windows, try to get the appdata env var",
            "    # this *could* result in cache_dir=None, which is fine, just skip caching in",
            "    # that case",
            "    if sysname == \"Windows\":",
            "        cache_dir = os.getenv(\"LOCALAPPDATA\", os.getenv(\"APPDATA\"))",
            "    # macOS -> app support dir",
            "    elif sysname == \"Darwin\":",
            "        cache_dir = os.path.expanduser(\"~/Library/Caches\")",
            "    # default for unknown platforms, namely linux behavior",
            "    # use XDG env var and default to ~/.cache/",
            "    else:",
            "        cache_dir = os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))",
            "",
            "    return cache_dir",
            "",
            "",
            "def _resolve_cache_dir(dirname: str = \"downloads\") -> str | None:",
            "    cache_dir = _base_cache_dir()",
            "    if cache_dir:",
            "        cache_dir = os.path.join(cache_dir, \"check_jsonschema\", dirname)",
            "    return cache_dir",
            "",
            "",
            "def _lastmod_from_response(response: requests.Response) -> float:",
            "    try:",
            "        return time.mktime(",
            "            time.strptime(response.headers[\"last-modified\"], _LASTMOD_FMT)",
            "        )",
            "    # OverflowError: time outside of platform-specific bounds",
            "    # ValueError: malformed/unparseable",
            "    # LookupError: no such header",
            "    except (OverflowError, ValueError, LookupError):",
            "        return 0.0",
            "",
            "",
            "def _get_request(",
            "    file_url: str, *, response_ok: t.Callable[[requests.Response], bool]",
            ") -> requests.Response:",
            "    num_retries = 2",
            "    r: requests.Response | None = None",
            "    for _attempt in range(num_retries + 1):",
            "        try:",
            "            r = requests.get(file_url, stream=True)",
            "        except requests.RequestException as e:",
            "            if _attempt == num_retries:",
            "                raise FailedDownloadError(\"encountered error during download\") from e",
            "            continue",
            "        if r.ok and response_ok(r):",
            "            return r",
            "    assert r is not None",
            "    raise FailedDownloadError(",
            "        f\"got response with status={r.status_code}, retries exhausted\"",
            "    )",
            "",
            "",
            "def _atomic_write(dest: str, content: bytes) -> None:",
            "    # download to a temp file and then move to the dest",
            "    # this makes the download safe if run in parallel (parallel runs",
            "    # won't create a new empty file for writing and cause failures)",
            "    fp = tempfile.NamedTemporaryFile(mode=\"wb\", delete=False)",
            "    fp.write(content)",
            "    fp.close()",
            "    shutil.copy(fp.name, dest)",
            "    os.remove(fp.name)",
            "",
            "",
            "def _cache_hit(cachefile: str, response: requests.Response) -> bool:",
            "    # no file? miss",
            "    if not os.path.exists(cachefile):",
            "        return False",
            "",
            "    # compare mtime on any cached file against the remote last-modified time",
            "    # it is considered a hit if the local file is at least as new as the remote file",
            "    local_mtime = os.path.getmtime(cachefile)",
            "    remote_mtime = _lastmod_from_response(response)",
            "    return local_mtime >= remote_mtime",
            "",
            "",
            "class FailedDownloadError(Exception):",
            "    pass",
            "",
            "",
            "class CacheDownloader:",
            "    def __init__(",
            "        self, cache_dir: str | None = None, disable_cache: bool = False",
            "    ) -> None:",
            "        if cache_dir is None:",
            "            self._cache_dir = _resolve_cache_dir()",
            "        else:",
            "            self._cache_dir = _resolve_cache_dir(cache_dir)",
            "        self._disable_cache = disable_cache",
            "",
            "    def _download(",
            "        self,",
            "        file_url: str,",
            "        filename: str,",
            "        response_ok: t.Callable[[requests.Response], bool],",
            "    ) -> str:",
            "        assert self._cache_dir is not None",
            "        os.makedirs(self._cache_dir, exist_ok=True)",
            "        dest = os.path.join(self._cache_dir, filename)",
            "",
            "        def check_response_for_download(r: requests.Response) -> bool:",
            "            # if the response indicates a cache hit, treat it as valid",
            "            # this ensures that we short-circuit any further evaluation immediately on",
            "            # a hit",
            "            if _cache_hit(dest, r):",
            "                return True",
            "            # we now know it's not a hit, so validate the content (forces download)",
            "            return response_ok(r)",
            "",
            "        response = _get_request(file_url, response_ok=check_response_for_download)",
            "        # check to see if we have a file which matches the connection",
            "        # only download if we do not (cache miss, vs hit)",
            "        if not _cache_hit(dest, response):",
            "            _atomic_write(dest, response.content)",
            "",
            "        return dest",
            "",
            "    @contextlib.contextmanager",
            "    def open(",
            "        self,",
            "        file_url: str,",
            "        filename: str,",
            "        validate_response: t.Callable[[requests.Response], bool],",
            "    ) -> t.Iterator[t.IO[bytes]]:",
            "        if (not self._cache_dir) or self._disable_cache:",
            "            yield io.BytesIO(",
            "                _get_request(file_url, response_ok=validate_response).content",
            "            )",
            "        else:",
            "            with open(",
            "                self._download(file_url, filename, response_ok=validate_response), \"rb\"",
            "            ) as fp:",
            "                yield fp",
            "",
            "    def bind(",
            "        self,",
            "        file_url: str,",
            "        filename: str | None = None,",
            "        validation_callback: t.Callable[[bytes], t.Any] | None = None,",
            "    ) -> BoundCacheDownloader:",
            "        return BoundCacheDownloader(",
            "            file_url, filename, self, validation_callback=validation_callback",
            "        )",
            "",
            "",
            "class BoundCacheDownloader:",
            "    def __init__(",
            "        self,",
            "        file_url: str,",
            "        filename: str | None,",
            "        downloader: CacheDownloader,",
            "        *,",
            "        validation_callback: t.Callable[[bytes], t.Any] | None = None,",
            "    ) -> None:",
            "        self._file_url = file_url",
            "        self._filename = filename or file_url.split(\"/\")[-1]",
            "        self._downloader = downloader",
            "        self._validation_callback = validation_callback",
            "",
            "    @contextlib.contextmanager",
            "    def open(self) -> t.Iterator[t.IO[bytes]]:",
            "        with self._downloader.open(",
            "            self._file_url,",
            "            self._filename,",
            "            validate_response=self._validate_response,",
            "        ) as fp:",
            "            yield fp",
            "",
            "    def _validate_response(self, response: requests.Response) -> bool:",
            "        if not self._validation_callback:",
            "            return True",
            "",
            "        try:",
            "            self._validation_callback(response.content)",
            "            return True",
            "        except ValueError:",
            "            return False"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextlib",
            "import hashlib",
            "import io",
            "import os",
            "import platform",
            "import shutil",
            "import tempfile",
            "import time",
            "import typing as t",
            "",
            "import requests",
            "",
            "_LASTMOD_FMT = \"%a, %d %b %Y %H:%M:%S %Z\"",
            "",
            "",
            "def _base_cache_dir() -> str | None:",
            "    sysname = platform.system()",
            "",
            "    # on windows, try to get the appdata env var",
            "    # this *could* result in cache_dir=None, which is fine, just skip caching in",
            "    # that case",
            "    if sysname == \"Windows\":",
            "        cache_dir = os.getenv(\"LOCALAPPDATA\", os.getenv(\"APPDATA\"))",
            "    # macOS -> app support dir",
            "    elif sysname == \"Darwin\":",
            "        cache_dir = os.path.expanduser(\"~/Library/Caches\")",
            "    # default for unknown platforms, namely linux behavior",
            "    # use XDG env var and default to ~/.cache/",
            "    else:",
            "        cache_dir = os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))",
            "",
            "    return cache_dir",
            "",
            "",
            "def _resolve_cache_dir(dirname: str) -> str | None:",
            "    cache_dir = _base_cache_dir()",
            "    if cache_dir:",
            "        cache_dir = os.path.join(cache_dir, \"check_jsonschema\", dirname)",
            "    return cache_dir",
            "",
            "",
            "def _lastmod_from_response(response: requests.Response) -> float:",
            "    try:",
            "        return time.mktime(",
            "            time.strptime(response.headers[\"last-modified\"], _LASTMOD_FMT)",
            "        )",
            "    # OverflowError: time outside of platform-specific bounds",
            "    # ValueError: malformed/unparseable",
            "    # LookupError: no such header",
            "    except (OverflowError, ValueError, LookupError):",
            "        return 0.0",
            "",
            "",
            "def _get_request(",
            "    file_url: str, *, response_ok: t.Callable[[requests.Response], bool]",
            ") -> requests.Response:",
            "    num_retries = 2",
            "    r: requests.Response | None = None",
            "    for _attempt in range(num_retries + 1):",
            "        try:",
            "            r = requests.get(file_url, stream=True)",
            "        except requests.RequestException as e:",
            "            if _attempt == num_retries:",
            "                raise FailedDownloadError(\"encountered error during download\") from e",
            "            continue",
            "        if r.ok and response_ok(r):",
            "            return r",
            "    assert r is not None",
            "    raise FailedDownloadError(",
            "        f\"got response with status={r.status_code}, retries exhausted\"",
            "    )",
            "",
            "",
            "def _atomic_write(dest: str, content: bytes) -> None:",
            "    # download to a temp file and then move to the dest",
            "    # this makes the download safe if run in parallel (parallel runs",
            "    # won't create a new empty file for writing and cause failures)",
            "    fp = tempfile.NamedTemporaryFile(mode=\"wb\", delete=False)",
            "    fp.write(content)",
            "    fp.close()",
            "    shutil.copy(fp.name, dest)",
            "    os.remove(fp.name)",
            "",
            "",
            "def _cache_hit(cachefile: str, response: requests.Response) -> bool:",
            "    # no file? miss",
            "    if not os.path.exists(cachefile):",
            "        return False",
            "",
            "    # compare mtime on any cached file against the remote last-modified time",
            "    # it is considered a hit if the local file is at least as new as the remote file",
            "    local_mtime = os.path.getmtime(cachefile)",
            "    remote_mtime = _lastmod_from_response(response)",
            "    return local_mtime >= remote_mtime",
            "",
            "",
            "def url_to_cache_filename(ref_url: str) -> str:",
            "    \"\"\"",
            "    Given a schema URL, convert it to a filename for caching in a cache dir.",
            "",
            "    Rules are as follows:",
            "    - the base filename is an sha256 hash of the URL",
            "    - if the filename ends in an extension (.json, .yaml, etc) that extension",
            "      is appended to the hash",
            "",
            "    Preserving file extensions preserves the extension-based logic used for parsing, and",
            "    it also helps a local editor (browsing the cache) identify filetypes.",
            "    \"\"\"",
            "    filename = hashlib.sha256(ref_url.encode()).hexdigest()",
            "    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):",
            "        _, _, extension = last_part.rpartition(\".\")",
            "        filename = f\"{filename}.{extension}\"",
            "    return filename",
            "",
            "",
            "class FailedDownloadError(Exception):",
            "    pass",
            "",
            "",
            "class CacheDownloader:",
            "    def __init__(self, cache_dir: str, *, disable_cache: bool = False) -> None:",
            "        self._cache_dir = _resolve_cache_dir(cache_dir)",
            "        self._disable_cache = disable_cache",
            "",
            "    def _download(",
            "        self,",
            "        file_url: str,",
            "        filename: str,",
            "        response_ok: t.Callable[[requests.Response], bool],",
            "    ) -> str:",
            "        assert self._cache_dir is not None",
            "        os.makedirs(self._cache_dir, exist_ok=True)",
            "        dest = os.path.join(self._cache_dir, filename)",
            "",
            "        def check_response_for_download(r: requests.Response) -> bool:",
            "            # if the response indicates a cache hit, treat it as valid",
            "            # this ensures that we short-circuit any further evaluation immediately on",
            "            # a hit",
            "            if _cache_hit(dest, r):",
            "                return True",
            "            # we now know it's not a hit, so validate the content (forces download)",
            "            return response_ok(r)",
            "",
            "        response = _get_request(file_url, response_ok=check_response_for_download)",
            "        # check to see if we have a file which matches the connection",
            "        # only download if we do not (cache miss, vs hit)",
            "        if not _cache_hit(dest, response):",
            "            _atomic_write(dest, response.content)",
            "",
            "        return dest",
            "",
            "    @contextlib.contextmanager",
            "    def open(",
            "        self,",
            "        file_url: str,",
            "        filename: str,",
            "        validate_response: t.Callable[[requests.Response], bool],",
            "    ) -> t.Iterator[t.IO[bytes]]:",
            "        if (not self._cache_dir) or self._disable_cache:",
            "            yield io.BytesIO(",
            "                _get_request(file_url, response_ok=validate_response).content",
            "            )",
            "        else:",
            "            with open(",
            "                self._download(file_url, filename, response_ok=validate_response), \"rb\"",
            "            ) as fp:",
            "                yield fp",
            "",
            "    def bind(",
            "        self,",
            "        file_url: str,",
            "        filename: str | None = None,",
            "        validation_callback: t.Callable[[bytes], t.Any] | None = None,",
            "    ) -> BoundCacheDownloader:",
            "        return BoundCacheDownloader(",
            "            file_url, self, filename=filename, validation_callback=validation_callback",
            "        )",
            "",
            "",
            "class BoundCacheDownloader:",
            "    def __init__(",
            "        self,",
            "        file_url: str,",
            "        downloader: CacheDownloader,",
            "        *,",
            "        filename: str | None = None,",
            "        validation_callback: t.Callable[[bytes], t.Any] | None = None,",
            "    ) -> None:",
            "        self._file_url = file_url",
            "        self._filename = filename or url_to_cache_filename(file_url)",
            "        self._downloader = downloader",
            "        self._validation_callback = validation_callback",
            "",
            "    @contextlib.contextmanager",
            "    def open(self) -> t.Iterator[t.IO[bytes]]:",
            "        with self._downloader.open(",
            "            self._file_url,",
            "            self._filename,",
            "            validate_response=self._validate_response,",
            "        ) as fp:",
            "            yield fp",
            "",
            "    def _validate_response(self, response: requests.Response) -> bool:",
            "        if not self._validation_callback:",
            "            return True",
            "",
            "        try:",
            "            self._validation_callback(response.content)",
            "            return True",
            "        except ValueError:",
            "            return False"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "36": [
                "_resolve_cache_dir"
            ],
            "103": [
                "CacheDownloader",
                "__init__"
            ],
            "104": [
                "CacheDownloader",
                "__init__"
            ],
            "105": [
                "CacheDownloader",
                "__init__"
            ],
            "106": [
                "CacheDownloader",
                "__init__"
            ],
            "107": [
                "CacheDownloader",
                "__init__"
            ],
            "108": [
                "CacheDownloader",
                "__init__"
            ],
            "109": [
                "CacheDownloader",
                "__init__"
            ],
            "163": [
                "CacheDownloader",
                "bind"
            ],
            "171": [
                "BoundCacheDownloader",
                "__init__"
            ],
            "177": [
                "BoundCacheDownloader",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "src/check_jsonschema/cli/main_command.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "     help=\"Disable schema caching. Always download remote schemas.\","
            },
            "1": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 131,
                "PatchRowcode": " )"
            },
            "2": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": 132,
                "PatchRowcode": " @click.option("
            },
            "3": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"--cache-filename\","
            },
            "4": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    help=("
            },
            "5": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"The name to use for caching a remote schema. \""
            },
            "6": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"Defaults to the last slash-delimited part of the URI.\""
            },
            "7": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ),"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+    \"--cache-filename\", help=\"Deprecated. This option no longer has any effect.\""
            },
            "9": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 134,
                "PatchRowcode": " )"
            },
            "10": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 135,
                "PatchRowcode": " @click.option("
            },
            "11": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "     \"--disable-formats\","
            },
            "12": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "     args.disable_cache = no_cache"
            },
            "13": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "     args.default_filetype = default_filetype"
            },
            "14": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 269,
                "PatchRowcode": "     args.fill_defaults = fill_defaults"
            },
            "15": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if cache_filename is not None:"
            },
            "16": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        args.cache_filename = cache_filename"
            },
            "17": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 270,
                "PatchRowcode": "     if data_transform is not None:"
            },
            "18": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": 271,
                "PatchRowcode": "         args.data_transform = TRANSFORM_LIBRARY[data_transform]"
            },
            "19": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": 272,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": 294,
                "PatchRowcode": "         assert args.schema_path is not None"
            },
            "21": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": 295,
                "PatchRowcode": "         return SchemaLoader("
            },
            "22": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": 296,
                "PatchRowcode": "             args.schema_path,"
            },
            "23": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            cache_filename=args.cache_filename,"
            },
            "24": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 297,
                "PatchRowcode": "             disable_cache=args.disable_cache,"
            },
            "25": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 298,
                "PatchRowcode": "             base_uri=args.base_uri,"
            },
            "26": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": 299,
                "PatchRowcode": "             validator_class=args.validator_class,"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import os",
            "import sys",
            "import textwrap",
            "import typing as t",
            "",
            "import click",
            "import jsonschema",
            "",
            "from ..catalog import CUSTOM_SCHEMA_NAMES, SCHEMA_CATALOG",
            "from ..checker import SchemaChecker",
            "from ..formats import KNOWN_FORMATS, RegexVariantName",
            "from ..instance_loader import InstanceLoader",
            "from ..parsers import SUPPORTED_FILE_FORMATS",
            "from ..reporter import REPORTER_BY_NAME, Reporter",
            "from ..schema_loader import (",
            "    BuiltinSchemaLoader,",
            "    MetaSchemaLoader,",
            "    SchemaLoader,",
            "    SchemaLoaderBase,",
            ")",
            "from ..transforms import TRANSFORM_LIBRARY",
            "from .param_types import CommaDelimitedList, LazyBinaryReadFile, ValidatorClassName",
            "from .parse_result import ParseResult, SchemaLoadingMode",
            "",
            "if sys.version_info >= (3, 8):",
            "    from typing import Literal",
            "else:",
            "    from typing_extensions import Literal",
            "",
            "BUILTIN_SCHEMA_NAMES = [f\"vendor.{k}\" for k in SCHEMA_CATALOG.keys()] + [",
            "    f\"custom.{k}\" for k in CUSTOM_SCHEMA_NAMES",
            "]",
            "BUILTIN_SCHEMA_CHOICES = (",
            "    BUILTIN_SCHEMA_NAMES + list(SCHEMA_CATALOG.keys()) + CUSTOM_SCHEMA_NAMES",
            ")",
            "",
            "",
            "def set_color_mode(ctx: click.Context, param: str, value: str) -> None:",
            "    if \"NO_COLOR\" in os.environ:",
            "        ctx.color = False",
            "    else:",
            "        ctx.color = {",
            "            \"auto\": None,",
            "            \"always\": True,",
            "            \"never\": False,",
            "        }[value]",
            "",
            "",
            "def pretty_helptext_list(values: list[str] | tuple[str, ...]) -> str:",
            "    return textwrap.indent(",
            "        \"\\n\".join(",
            "            textwrap.wrap(",
            "                \", \".join(values),",
            "                width=75,",
            "                break_long_words=False,",
            "                break_on_hyphens=False,",
            "            ),",
            "        ),",
            "        \"    \",",
            "    )",
            "",
            "",
            "@click.command(",
            "    \"check-jsonschema\",",
            "    help=\"\"\"\\",
            "Check JSON and YAML files against a JSON Schema.",
            "",
            "The schema is specified either with '--schemafile' or with '--builtin-schema'.",
            "",
            "'check-jsonschema' supports format checks with appropriate libraries installed,",
            "including the following formats by default:",
            "    date, date-time, email, ipv4, ipv6, regex, uuid",
            "",
            "\\b",
            "For the \"regex\" format, there are multiple modes which can be specified with",
            "'--format-regex':",
            "    default  |  check that the string is a valid ECMAScript regex",
            "    python   |  check that the string is a valid python regex",
            "",
            "\\b",
            "The '--builtin-schema' flag supports the following schema names:",
            "\"\"\"",
            "    + pretty_helptext_list(BUILTIN_SCHEMA_NAMES)",
            "    + \"\"\"\\",
            "",
            "\\b",
            "The '--disable-formats' flag supports the following formats:",
            "\"\"\"",
            "    + pretty_helptext_list(KNOWN_FORMATS),",
            ")",
            "@click.help_option(\"-h\", \"--help\")",
            "@click.version_option()",
            "@click.option(",
            "    \"--schemafile\",",
            "    help=(",
            "        \"The path to a file containing the JSON Schema to use or an \"",
            "        \"HTTP(S) URI for the schema. If a remote file is used, \"",
            "        \"it will be downloaded and cached locally based on mtime. \"",
            "        \"Use '-' for stdin.\"",
            "    ),",
            "    metavar=\"[PATH|URI]\",",
            ")",
            "@click.option(",
            "    \"--base-uri\",",
            "    help=(",
            "        \"Override the base URI for the schema. The default behavior is to \"",
            "        \"follow the behavior specified by the JSON Schema spec, which is to \"",
            "        \"prefer an explicit '$id' and failover to the retrieval URI.\"",
            "    ),",
            ")",
            "@click.option(",
            "    \"--builtin-schema\",",
            "    help=\"The name of an internal schema to use for '--schemafile'\",",
            "    type=click.Choice(BUILTIN_SCHEMA_CHOICES, case_sensitive=False),",
            "    metavar=\"BUILTIN_SCHEMA_NAME\",",
            ")",
            "@click.option(",
            "    \"--check-metaschema\",",
            "    is_flag=True,",
            "    help=(",
            "        \"Instead of validating the instances against a schema, treat each file as a \"",
            "        \"schema and validate them under their matching metaschemas.\"",
            "    ),",
            ")",
            "@click.option(",
            "    \"--no-cache\",",
            "    is_flag=True,",
            "    help=\"Disable schema caching. Always download remote schemas.\",",
            ")",
            "@click.option(",
            "    \"--cache-filename\",",
            "    help=(",
            "        \"The name to use for caching a remote schema. \"",
            "        \"Defaults to the last slash-delimited part of the URI.\"",
            "    ),",
            ")",
            "@click.option(",
            "    \"--disable-formats\",",
            "    multiple=True,",
            "    help=(",
            "        \"Disable specific format checks in the schema. \"",
            "        \"Pass '*' to disable all format checks.\"",
            "    ),",
            "    type=CommaDelimitedList(choices=(\"*\", *KNOWN_FORMATS)),",
            "    metavar=\"{*|FORMAT,FORMAT,...}\",",
            ")",
            "@click.option(",
            "    \"--format-regex\",",
            "    help=(",
            "        \"Set the mode of format validation for regexes. \"",
            "        \"If `--disable-formats regex` is used, this option has no effect.\"",
            "    ),",
            "    default=RegexVariantName.default.value,",
            "    type=click.Choice([x.value for x in RegexVariantName], case_sensitive=False),",
            ")",
            "@click.option(",
            "    \"--default-filetype\",",
            "    help=\"A default filetype to assume when a file's type is not detected\",",
            "    default=\"json\",",
            "    show_default=True,",
            "    type=click.Choice(SUPPORTED_FILE_FORMATS, case_sensitive=True),",
            ")",
            "@click.option(",
            "    \"--traceback-mode\",",
            "    help=(",
            "        \"Set the mode of presentation for error traces. \"",
            "        \"Defaults to shortened tracebacks.\"",
            "    ),",
            "    type=click.Choice((\"full\", \"short\")),",
            "    default=\"short\",",
            ")",
            "@click.option(",
            "    \"--data-transform\",",
            "    help=(",
            "        \"Select a builtin transform which should be applied to instancefiles before \"",
            "        \"they are checked.\"",
            "    ),",
            "    type=click.Choice(tuple(TRANSFORM_LIBRARY.keys())),",
            ")",
            "@click.option(",
            "    \"--fill-defaults\",",
            "    help=(",
            "        \"Autofill 'default' values prior to validation. \"",
            "        \"This may conflict with certain third-party validators used with \"",
            "        \"'--validator-class'\"",
            "    ),",
            "    is_flag=True,",
            ")",
            "@click.option(",
            "    \"--validator-class\",",
            "    help=(",
            "        \"The fully qualified name of a python validator to use in place of \"",
            "        \"the 'jsonschema' library validators, in the form of '<package>:<class>'. \"",
            "        \"The validator must be importable in the same environment where \"",
            "        \"'check-jsonschema' is run.\"",
            "    ),",
            "    type=ValidatorClassName(),",
            ")",
            "@click.option(",
            "    \"-o\",",
            "    \"--output-format\",",
            "    help=\"Which output format to use.\",",
            "    type=click.Choice(tuple(REPORTER_BY_NAME.keys()), case_sensitive=False),",
            "    default=\"text\",",
            ")",
            "@click.option(",
            "    \"--color\",",
            "    help=\"Force or disable colorized output. Defaults to autodetection.\",",
            "    default=\"auto\",",
            "    type=click.Choice((\"auto\", \"always\", \"never\")),",
            "    callback=set_color_mode,",
            "    expose_value=False,",
            ")",
            "@click.option(",
            "    \"-v\",",
            "    \"--verbose\",",
            "    help=(",
            "        \"Increase output verbosity. On validation errors, this may be especially \"",
            "        \"useful when oneOf or anyOf is used in the schema.\"",
            "    ),",
            "    count=True,",
            ")",
            "@click.option(",
            "    \"-q\",",
            "    \"--quiet\",",
            "    help=\"Reduce output verbosity\",",
            "    count=True,",
            ")",
            "@click.argument(",
            "    \"instancefiles\", required=True, nargs=-1, type=LazyBinaryReadFile(\"rb\", lazy=True)",
            ")",
            "def main(",
            "    *,",
            "    schemafile: str | None,",
            "    builtin_schema: str | None,",
            "    base_uri: str | None,",
            "    check_metaschema: bool,",
            "    no_cache: bool,",
            "    cache_filename: str | None,",
            "    disable_formats: tuple[list[str], ...],",
            "    format_regex: Literal[\"python\", \"default\"],",
            "    default_filetype: Literal[\"json\", \"yaml\", \"toml\", \"json5\"],",
            "    traceback_mode: Literal[\"full\", \"short\"],",
            "    data_transform: Literal[\"azure-pipelines\", \"gitlab-ci\"] | None,",
            "    fill_defaults: bool,",
            "    validator_class: type[jsonschema.protocols.Validator] | None,",
            "    output_format: Literal[\"text\", \"json\"],",
            "    verbose: int,",
            "    quiet: int,",
            "    instancefiles: tuple[t.IO[bytes], ...],",
            ") -> None:",
            "    args = ParseResult()",
            "",
            "    args.set_schema(schemafile, builtin_schema, check_metaschema)",
            "    args.set_validator(validator_class)",
            "",
            "    args.base_uri = base_uri",
            "    args.instancefiles = instancefiles",
            "",
            "    normalized_disable_formats: tuple[str, ...] = tuple(",
            "        f for sublist in disable_formats for f in sublist",
            "    )",
            "    if \"*\" in normalized_disable_formats:",
            "        args.disable_all_formats = True",
            "    else:",
            "        args.disable_formats = normalized_disable_formats",
            "",
            "    args.format_regex = RegexVariantName(format_regex)",
            "    args.disable_cache = no_cache",
            "    args.default_filetype = default_filetype",
            "    args.fill_defaults = fill_defaults",
            "    if cache_filename is not None:",
            "        args.cache_filename = cache_filename",
            "    if data_transform is not None:",
            "        args.data_transform = TRANSFORM_LIBRARY[data_transform]",
            "",
            "    # verbosity behavior:",
            "    # - default is 1",
            "    # - count '-v'",
            "    # - subtract count of '-q'",
            "    args.verbosity = 1 + verbose - quiet",
            "    args.traceback_mode = traceback_mode",
            "    args.output_format = output_format",
            "",
            "    execute(args)",
            "",
            "",
            "# separate parsing from execution for simpler mocking for unit tests",
            "",
            "",
            "def build_schema_loader(args: ParseResult) -> SchemaLoaderBase:",
            "    if args.schema_mode == SchemaLoadingMode.metaschema:",
            "        return MetaSchemaLoader(base_uri=args.base_uri)",
            "    elif args.schema_mode == SchemaLoadingMode.builtin:",
            "        assert args.schema_path is not None",
            "        return BuiltinSchemaLoader(args.schema_path, base_uri=args.base_uri)",
            "    elif args.schema_mode == SchemaLoadingMode.filepath:",
            "        assert args.schema_path is not None",
            "        return SchemaLoader(",
            "            args.schema_path,",
            "            cache_filename=args.cache_filename,",
            "            disable_cache=args.disable_cache,",
            "            base_uri=args.base_uri,",
            "            validator_class=args.validator_class,",
            "        )",
            "    else:",
            "        raise NotImplementedError(\"no valid schema option provided\")",
            "",
            "",
            "def build_instance_loader(args: ParseResult) -> InstanceLoader:",
            "    return InstanceLoader(",
            "        args.instancefiles,",
            "        default_filetype=args.default_filetype,",
            "        data_transform=args.data_transform,",
            "    )",
            "",
            "",
            "def build_reporter(args: ParseResult) -> Reporter:",
            "    cls = REPORTER_BY_NAME[args.output_format]",
            "    return cls(verbosity=args.verbosity)",
            "",
            "",
            "def build_checker(args: ParseResult) -> SchemaChecker:",
            "    schema_loader = build_schema_loader(args)",
            "    instance_loader = build_instance_loader(args)",
            "    reporter = build_reporter(args)",
            "    return SchemaChecker(",
            "        schema_loader,",
            "        instance_loader,",
            "        reporter,",
            "        format_opts=args.format_opts,",
            "        traceback_mode=args.traceback_mode,",
            "        fill_defaults=args.fill_defaults,",
            "    )",
            "",
            "",
            "def execute(args: ParseResult) -> None:",
            "    checker = build_checker(args)",
            "    ret = checker.run()",
            "    click.get_current_context().exit(ret)"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import os",
            "import sys",
            "import textwrap",
            "import typing as t",
            "",
            "import click",
            "import jsonschema",
            "",
            "from ..catalog import CUSTOM_SCHEMA_NAMES, SCHEMA_CATALOG",
            "from ..checker import SchemaChecker",
            "from ..formats import KNOWN_FORMATS, RegexVariantName",
            "from ..instance_loader import InstanceLoader",
            "from ..parsers import SUPPORTED_FILE_FORMATS",
            "from ..reporter import REPORTER_BY_NAME, Reporter",
            "from ..schema_loader import (",
            "    BuiltinSchemaLoader,",
            "    MetaSchemaLoader,",
            "    SchemaLoader,",
            "    SchemaLoaderBase,",
            ")",
            "from ..transforms import TRANSFORM_LIBRARY",
            "from .param_types import CommaDelimitedList, LazyBinaryReadFile, ValidatorClassName",
            "from .parse_result import ParseResult, SchemaLoadingMode",
            "",
            "if sys.version_info >= (3, 8):",
            "    from typing import Literal",
            "else:",
            "    from typing_extensions import Literal",
            "",
            "BUILTIN_SCHEMA_NAMES = [f\"vendor.{k}\" for k in SCHEMA_CATALOG.keys()] + [",
            "    f\"custom.{k}\" for k in CUSTOM_SCHEMA_NAMES",
            "]",
            "BUILTIN_SCHEMA_CHOICES = (",
            "    BUILTIN_SCHEMA_NAMES + list(SCHEMA_CATALOG.keys()) + CUSTOM_SCHEMA_NAMES",
            ")",
            "",
            "",
            "def set_color_mode(ctx: click.Context, param: str, value: str) -> None:",
            "    if \"NO_COLOR\" in os.environ:",
            "        ctx.color = False",
            "    else:",
            "        ctx.color = {",
            "            \"auto\": None,",
            "            \"always\": True,",
            "            \"never\": False,",
            "        }[value]",
            "",
            "",
            "def pretty_helptext_list(values: list[str] | tuple[str, ...]) -> str:",
            "    return textwrap.indent(",
            "        \"\\n\".join(",
            "            textwrap.wrap(",
            "                \", \".join(values),",
            "                width=75,",
            "                break_long_words=False,",
            "                break_on_hyphens=False,",
            "            ),",
            "        ),",
            "        \"    \",",
            "    )",
            "",
            "",
            "@click.command(",
            "    \"check-jsonschema\",",
            "    help=\"\"\"\\",
            "Check JSON and YAML files against a JSON Schema.",
            "",
            "The schema is specified either with '--schemafile' or with '--builtin-schema'.",
            "",
            "'check-jsonschema' supports format checks with appropriate libraries installed,",
            "including the following formats by default:",
            "    date, date-time, email, ipv4, ipv6, regex, uuid",
            "",
            "\\b",
            "For the \"regex\" format, there are multiple modes which can be specified with",
            "'--format-regex':",
            "    default  |  check that the string is a valid ECMAScript regex",
            "    python   |  check that the string is a valid python regex",
            "",
            "\\b",
            "The '--builtin-schema' flag supports the following schema names:",
            "\"\"\"",
            "    + pretty_helptext_list(BUILTIN_SCHEMA_NAMES)",
            "    + \"\"\"\\",
            "",
            "\\b",
            "The '--disable-formats' flag supports the following formats:",
            "\"\"\"",
            "    + pretty_helptext_list(KNOWN_FORMATS),",
            ")",
            "@click.help_option(\"-h\", \"--help\")",
            "@click.version_option()",
            "@click.option(",
            "    \"--schemafile\",",
            "    help=(",
            "        \"The path to a file containing the JSON Schema to use or an \"",
            "        \"HTTP(S) URI for the schema. If a remote file is used, \"",
            "        \"it will be downloaded and cached locally based on mtime. \"",
            "        \"Use '-' for stdin.\"",
            "    ),",
            "    metavar=\"[PATH|URI]\",",
            ")",
            "@click.option(",
            "    \"--base-uri\",",
            "    help=(",
            "        \"Override the base URI for the schema. The default behavior is to \"",
            "        \"follow the behavior specified by the JSON Schema spec, which is to \"",
            "        \"prefer an explicit '$id' and failover to the retrieval URI.\"",
            "    ),",
            ")",
            "@click.option(",
            "    \"--builtin-schema\",",
            "    help=\"The name of an internal schema to use for '--schemafile'\",",
            "    type=click.Choice(BUILTIN_SCHEMA_CHOICES, case_sensitive=False),",
            "    metavar=\"BUILTIN_SCHEMA_NAME\",",
            ")",
            "@click.option(",
            "    \"--check-metaschema\",",
            "    is_flag=True,",
            "    help=(",
            "        \"Instead of validating the instances against a schema, treat each file as a \"",
            "        \"schema and validate them under their matching metaschemas.\"",
            "    ),",
            ")",
            "@click.option(",
            "    \"--no-cache\",",
            "    is_flag=True,",
            "    help=\"Disable schema caching. Always download remote schemas.\",",
            ")",
            "@click.option(",
            "    \"--cache-filename\", help=\"Deprecated. This option no longer has any effect.\"",
            ")",
            "@click.option(",
            "    \"--disable-formats\",",
            "    multiple=True,",
            "    help=(",
            "        \"Disable specific format checks in the schema. \"",
            "        \"Pass '*' to disable all format checks.\"",
            "    ),",
            "    type=CommaDelimitedList(choices=(\"*\", *KNOWN_FORMATS)),",
            "    metavar=\"{*|FORMAT,FORMAT,...}\",",
            ")",
            "@click.option(",
            "    \"--format-regex\",",
            "    help=(",
            "        \"Set the mode of format validation for regexes. \"",
            "        \"If `--disable-formats regex` is used, this option has no effect.\"",
            "    ),",
            "    default=RegexVariantName.default.value,",
            "    type=click.Choice([x.value for x in RegexVariantName], case_sensitive=False),",
            ")",
            "@click.option(",
            "    \"--default-filetype\",",
            "    help=\"A default filetype to assume when a file's type is not detected\",",
            "    default=\"json\",",
            "    show_default=True,",
            "    type=click.Choice(SUPPORTED_FILE_FORMATS, case_sensitive=True),",
            ")",
            "@click.option(",
            "    \"--traceback-mode\",",
            "    help=(",
            "        \"Set the mode of presentation for error traces. \"",
            "        \"Defaults to shortened tracebacks.\"",
            "    ),",
            "    type=click.Choice((\"full\", \"short\")),",
            "    default=\"short\",",
            ")",
            "@click.option(",
            "    \"--data-transform\",",
            "    help=(",
            "        \"Select a builtin transform which should be applied to instancefiles before \"",
            "        \"they are checked.\"",
            "    ),",
            "    type=click.Choice(tuple(TRANSFORM_LIBRARY.keys())),",
            ")",
            "@click.option(",
            "    \"--fill-defaults\",",
            "    help=(",
            "        \"Autofill 'default' values prior to validation. \"",
            "        \"This may conflict with certain third-party validators used with \"",
            "        \"'--validator-class'\"",
            "    ),",
            "    is_flag=True,",
            ")",
            "@click.option(",
            "    \"--validator-class\",",
            "    help=(",
            "        \"The fully qualified name of a python validator to use in place of \"",
            "        \"the 'jsonschema' library validators, in the form of '<package>:<class>'. \"",
            "        \"The validator must be importable in the same environment where \"",
            "        \"'check-jsonschema' is run.\"",
            "    ),",
            "    type=ValidatorClassName(),",
            ")",
            "@click.option(",
            "    \"-o\",",
            "    \"--output-format\",",
            "    help=\"Which output format to use.\",",
            "    type=click.Choice(tuple(REPORTER_BY_NAME.keys()), case_sensitive=False),",
            "    default=\"text\",",
            ")",
            "@click.option(",
            "    \"--color\",",
            "    help=\"Force or disable colorized output. Defaults to autodetection.\",",
            "    default=\"auto\",",
            "    type=click.Choice((\"auto\", \"always\", \"never\")),",
            "    callback=set_color_mode,",
            "    expose_value=False,",
            ")",
            "@click.option(",
            "    \"-v\",",
            "    \"--verbose\",",
            "    help=(",
            "        \"Increase output verbosity. On validation errors, this may be especially \"",
            "        \"useful when oneOf or anyOf is used in the schema.\"",
            "    ),",
            "    count=True,",
            ")",
            "@click.option(",
            "    \"-q\",",
            "    \"--quiet\",",
            "    help=\"Reduce output verbosity\",",
            "    count=True,",
            ")",
            "@click.argument(",
            "    \"instancefiles\", required=True, nargs=-1, type=LazyBinaryReadFile(\"rb\", lazy=True)",
            ")",
            "def main(",
            "    *,",
            "    schemafile: str | None,",
            "    builtin_schema: str | None,",
            "    base_uri: str | None,",
            "    check_metaschema: bool,",
            "    no_cache: bool,",
            "    cache_filename: str | None,",
            "    disable_formats: tuple[list[str], ...],",
            "    format_regex: Literal[\"python\", \"default\"],",
            "    default_filetype: Literal[\"json\", \"yaml\", \"toml\", \"json5\"],",
            "    traceback_mode: Literal[\"full\", \"short\"],",
            "    data_transform: Literal[\"azure-pipelines\", \"gitlab-ci\"] | None,",
            "    fill_defaults: bool,",
            "    validator_class: type[jsonschema.protocols.Validator] | None,",
            "    output_format: Literal[\"text\", \"json\"],",
            "    verbose: int,",
            "    quiet: int,",
            "    instancefiles: tuple[t.IO[bytes], ...],",
            ") -> None:",
            "    args = ParseResult()",
            "",
            "    args.set_schema(schemafile, builtin_schema, check_metaschema)",
            "    args.set_validator(validator_class)",
            "",
            "    args.base_uri = base_uri",
            "    args.instancefiles = instancefiles",
            "",
            "    normalized_disable_formats: tuple[str, ...] = tuple(",
            "        f for sublist in disable_formats for f in sublist",
            "    )",
            "    if \"*\" in normalized_disable_formats:",
            "        args.disable_all_formats = True",
            "    else:",
            "        args.disable_formats = normalized_disable_formats",
            "",
            "    args.format_regex = RegexVariantName(format_regex)",
            "    args.disable_cache = no_cache",
            "    args.default_filetype = default_filetype",
            "    args.fill_defaults = fill_defaults",
            "    if data_transform is not None:",
            "        args.data_transform = TRANSFORM_LIBRARY[data_transform]",
            "",
            "    # verbosity behavior:",
            "    # - default is 1",
            "    # - count '-v'",
            "    # - subtract count of '-q'",
            "    args.verbosity = 1 + verbose - quiet",
            "    args.traceback_mode = traceback_mode",
            "    args.output_format = output_format",
            "",
            "    execute(args)",
            "",
            "",
            "# separate parsing from execution for simpler mocking for unit tests",
            "",
            "",
            "def build_schema_loader(args: ParseResult) -> SchemaLoaderBase:",
            "    if args.schema_mode == SchemaLoadingMode.metaschema:",
            "        return MetaSchemaLoader(base_uri=args.base_uri)",
            "    elif args.schema_mode == SchemaLoadingMode.builtin:",
            "        assert args.schema_path is not None",
            "        return BuiltinSchemaLoader(args.schema_path, base_uri=args.base_uri)",
            "    elif args.schema_mode == SchemaLoadingMode.filepath:",
            "        assert args.schema_path is not None",
            "        return SchemaLoader(",
            "            args.schema_path,",
            "            disable_cache=args.disable_cache,",
            "            base_uri=args.base_uri,",
            "            validator_class=args.validator_class,",
            "        )",
            "    else:",
            "        raise NotImplementedError(\"no valid schema option provided\")",
            "",
            "",
            "def build_instance_loader(args: ParseResult) -> InstanceLoader:",
            "    return InstanceLoader(",
            "        args.instancefiles,",
            "        default_filetype=args.default_filetype,",
            "        data_transform=args.data_transform,",
            "    )",
            "",
            "",
            "def build_reporter(args: ParseResult) -> Reporter:",
            "    cls = REPORTER_BY_NAME[args.output_format]",
            "    return cls(verbosity=args.verbosity)",
            "",
            "",
            "def build_checker(args: ParseResult) -> SchemaChecker:",
            "    schema_loader = build_schema_loader(args)",
            "    instance_loader = build_instance_loader(args)",
            "    reporter = build_reporter(args)",
            "    return SchemaChecker(",
            "        schema_loader,",
            "        instance_loader,",
            "        reporter,",
            "        format_opts=args.format_opts,",
            "        traceback_mode=args.traceback_mode,",
            "        fill_defaults=args.fill_defaults,",
            "    )",
            "",
            "",
            "def execute(args: ParseResult) -> None:",
            "    checker = build_checker(args)",
            "    ret = checker.run()",
            "    click.get_current_context().exit(ret)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "133": [],
            "134": [],
            "135": [],
            "136": [],
            "137": [],
            "274": [
                "main"
            ],
            "275": [
                "main"
            ],
            "303": [
                "build_schema_loader"
            ]
        },
        "addLocation": []
    },
    "src/check_jsonschema/schema_loader/main.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "         self,"
            },
            "1": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "         schemafile: str,"
            },
            "2": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "         *,"
            },
            "3": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cache_filename: str | None = None,"
            },
            "4": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "         base_uri: str | None = None,"
            },
            "5": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "         validator_class: type[jsonschema.protocols.Validator] | None = None,"
            },
            "6": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "         disable_cache: bool = True,"
            },
            "7": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "     ) -> None:"
            },
            "8": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "         # record input parameters (these are not to be modified)"
            },
            "9": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "         self.schemafile = schemafile"
            },
            "10": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.cache_filename = cache_filename"
            },
            "11": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "         self.disable_cache = disable_cache"
            },
            "12": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "         self.base_uri = base_uri"
            },
            "13": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         self.validator_class = validator_class"
            },
            "14": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "             return LocalSchemaReader(self.schemafile)"
            },
            "15": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 104,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "         if self.url_info.scheme in (\"http\", \"https\"):"
            },
            "17": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return HttpSchemaReader("
            },
            "18": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.schemafile,"
            },
            "19": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.cache_filename,"
            },
            "20": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.disable_cache,"
            },
            "21": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+            return HttpSchemaReader(self.schemafile, self.disable_cache)"
            },
            "23": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "         else:"
            },
            "24": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "             raise UnsupportedUrlScheme("
            },
            "25": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 109,
                "PatchRowcode": "                 \"check-jsonschema only supports http, https, and local files. \""
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import functools",
            "import pathlib",
            "import typing as t",
            "import urllib.error",
            "import urllib.parse",
            "",
            "import jsonschema",
            "",
            "from ..builtin_schemas import get_builtin_schema",
            "from ..formats import FormatOptions, make_format_checker",
            "from ..parsers import ParserSet",
            "from ..utils import is_url_ish",
            "from .errors import UnsupportedUrlScheme",
            "from .readers import HttpSchemaReader, LocalSchemaReader, StdinSchemaReader",
            "from .resolver import make_reference_registry",
            "",
            "",
            "def _extend_with_default(",
            "    validator_class: type[jsonschema.protocols.Validator],",
            ") -> type[jsonschema.Validator]:",
            "    validate_properties = validator_class.VALIDATORS[\"properties\"]",
            "",
            "    def set_defaults_then_validate(",
            "        validator: jsonschema.Validator,",
            "        properties: dict[str, dict[str, t.Any]],",
            "        instance: dict[str, t.Any],",
            "        schema: dict[str, t.Any],",
            "    ) -> t.Iterator[jsonschema.ValidationError]:",
            "        for property_name, subschema in properties.items():",
            "            if \"default\" in subschema and property_name not in instance:",
            "                instance[property_name] = subschema[\"default\"]",
            "",
            "        yield from validate_properties(",
            "            validator,",
            "            properties,",
            "            instance,",
            "            schema,",
            "        )",
            "",
            "    return jsonschema.validators.extend(",
            "        validator_class,",
            "        {\"properties\": set_defaults_then_validate},",
            "    )",
            "",
            "",
            "class SchemaLoaderBase:",
            "    def get_validator(",
            "        self,",
            "        path: pathlib.Path | str,",
            "        instance_doc: dict[str, t.Any],",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        raise NotImplementedError",
            "",
            "",
            "class SchemaLoader(SchemaLoaderBase):",
            "    validator_class: type[jsonschema.protocols.Validator] | None = None",
            "    disable_cache: bool = True",
            "",
            "    def __init__(",
            "        self,",
            "        schemafile: str,",
            "        *,",
            "        cache_filename: str | None = None,",
            "        base_uri: str | None = None,",
            "        validator_class: type[jsonschema.protocols.Validator] | None = None,",
            "        disable_cache: bool = True,",
            "    ) -> None:",
            "        # record input parameters (these are not to be modified)",
            "        self.schemafile = schemafile",
            "        self.cache_filename = cache_filename",
            "        self.disable_cache = disable_cache",
            "        self.base_uri = base_uri",
            "        self.validator_class = validator_class",
            "",
            "        # if the schema location is a URL, which may include a file:// URL, parse it",
            "        self.url_info = None",
            "        if is_url_ish(self.schemafile):",
            "            self.url_info = urllib.parse.urlparse(self.schemafile)",
            "",
            "        # setup a parser collection",
            "        self._parsers = ParserSet()",
            "",
            "        # setup a schema reader lazily, when needed",
            "        self._reader: (",
            "            LocalSchemaReader | HttpSchemaReader | StdinSchemaReader | None",
            "        ) = None",
            "",
            "    @property",
            "    def reader(self) -> LocalSchemaReader | HttpSchemaReader | StdinSchemaReader:",
            "        if self._reader is None:",
            "            self._reader = self._get_schema_reader()",
            "        return self._reader",
            "",
            "    def _get_schema_reader(",
            "        self,",
            "    ) -> LocalSchemaReader | HttpSchemaReader | StdinSchemaReader:",
            "        if self.schemafile == \"-\":",
            "            return StdinSchemaReader()",
            "",
            "        if self.url_info is None or self.url_info.scheme in (\"file\", \"\"):",
            "            return LocalSchemaReader(self.schemafile)",
            "",
            "        if self.url_info.scheme in (\"http\", \"https\"):",
            "            return HttpSchemaReader(",
            "                self.schemafile,",
            "                self.cache_filename,",
            "                self.disable_cache,",
            "            )",
            "        else:",
            "            raise UnsupportedUrlScheme(",
            "                \"check-jsonschema only supports http, https, and local files. \"",
            "                f\"detected parsed URL had an unrecognized scheme: {self.url_info}\"",
            "            )",
            "",
            "    def get_schema_retrieval_uri(self) -> str | None:",
            "        return self.reader.get_retrieval_uri()",
            "",
            "    def get_schema(self) -> dict[str, t.Any]:",
            "        data = self.reader.read_schema()",
            "        if self.base_uri is not None:",
            "            data[\"$id\"] = self.base_uri",
            "        return data",
            "",
            "    def get_validator(",
            "        self,",
            "        path: pathlib.Path | str,",
            "        instance_doc: dict[str, t.Any],",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        return self._get_validator(format_opts, fill_defaults)",
            "",
            "    @functools.lru_cache",
            "    def _get_validator(",
            "        self,",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        retrieval_uri = self.get_schema_retrieval_uri()",
            "        schema = self.get_schema()",
            "",
            "        schema_dialect = schema.get(\"$schema\")",
            "        if schema_dialect is not None and not isinstance(schema_dialect, str):",
            "            schema_dialect = None",
            "",
            "        # format checker (which may be None)",
            "        format_checker = make_format_checker(format_opts, schema_dialect)",
            "",
            "        # reference resolution",
            "        # with support for YAML, TOML, and other formats from the parsers",
            "        reference_registry = make_reference_registry(",
            "            self._parsers, retrieval_uri, schema, self.disable_cache",
            "        )",
            "",
            "        if self.validator_class is None:",
            "            # get the correct validator class and check the schema under its metaschema",
            "            validator_cls = jsonschema.validators.validator_for(schema)",
            "            validator_cls.check_schema(schema)",
            "        else:",
            "            # for a user-provided validator class, don't check_schema",
            "            # on the grounds that it might *not* be valid but the user wants to use",
            "            # their custom validator anyway",
            "            #",
            "            # in fact, there's no real guarantee that a user-provided",
            "            # validator_class properly conforms to the jsonschema.Validator protocol",
            "            # we *hope* that it does, but we can't be fully sure",
            "            validator_cls = self.validator_class",
            "",
            "        # extend the validator class with default-filling behavior if appropriate",
            "        if fill_defaults:",
            "            validator_cls = _extend_with_default(validator_cls)",
            "",
            "        # now that we know it's safe to try to create the validator instance, do it",
            "        validator = validator_cls(",
            "            schema,",
            "            registry=reference_registry,",
            "            format_checker=format_checker,",
            "        )",
            "        return t.cast(jsonschema.protocols.Validator, validator)",
            "",
            "",
            "class BuiltinSchemaLoader(SchemaLoader):",
            "    def __init__(self, schema_name: str, *, base_uri: str | None = None) -> None:",
            "        self.schema_name = schema_name",
            "        self.base_uri = base_uri",
            "        self._parsers = ParserSet()",
            "",
            "    def get_schema_retrieval_uri(self) -> str | None:",
            "        return None",
            "",
            "    def get_schema(self) -> dict[str, t.Any]:",
            "        data = get_builtin_schema(self.schema_name)",
            "        if self.base_uri is not None:",
            "            data[\"$id\"] = self.base_uri",
            "        return data",
            "",
            "",
            "class MetaSchemaLoader(SchemaLoaderBase):",
            "    def __init__(self, *, base_uri: str | None = None) -> None:",
            "        if base_uri is not None:",
            "            raise NotImplementedError(",
            "                \"'--base-uri' was used with '--metaschema'. \"",
            "                \"This combination is not supported.\"",
            "            )",
            "",
            "    def get_validator(",
            "        self,",
            "        path: pathlib.Path | str,",
            "        instance_doc: dict[str, t.Any],",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        schema_validator = jsonschema.validators.validator_for(instance_doc)",
            "        meta_validator_class = jsonschema.validators.validator_for(",
            "            schema_validator.META_SCHEMA, default=schema_validator",
            "        )",
            "",
            "        # format checker (which may be None)",
            "        meta_schema_dialect = schema_validator.META_SCHEMA.get(\"$schema\")",
            "        format_checker = make_format_checker(format_opts, meta_schema_dialect)",
            "",
            "        meta_validator = meta_validator_class(",
            "            schema_validator.META_SCHEMA, format_checker=format_checker",
            "        )",
            "        return meta_validator"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import functools",
            "import pathlib",
            "import typing as t",
            "import urllib.error",
            "import urllib.parse",
            "",
            "import jsonschema",
            "",
            "from ..builtin_schemas import get_builtin_schema",
            "from ..formats import FormatOptions, make_format_checker",
            "from ..parsers import ParserSet",
            "from ..utils import is_url_ish",
            "from .errors import UnsupportedUrlScheme",
            "from .readers import HttpSchemaReader, LocalSchemaReader, StdinSchemaReader",
            "from .resolver import make_reference_registry",
            "",
            "",
            "def _extend_with_default(",
            "    validator_class: type[jsonschema.protocols.Validator],",
            ") -> type[jsonschema.Validator]:",
            "    validate_properties = validator_class.VALIDATORS[\"properties\"]",
            "",
            "    def set_defaults_then_validate(",
            "        validator: jsonschema.Validator,",
            "        properties: dict[str, dict[str, t.Any]],",
            "        instance: dict[str, t.Any],",
            "        schema: dict[str, t.Any],",
            "    ) -> t.Iterator[jsonschema.ValidationError]:",
            "        for property_name, subschema in properties.items():",
            "            if \"default\" in subschema and property_name not in instance:",
            "                instance[property_name] = subschema[\"default\"]",
            "",
            "        yield from validate_properties(",
            "            validator,",
            "            properties,",
            "            instance,",
            "            schema,",
            "        )",
            "",
            "    return jsonschema.validators.extend(",
            "        validator_class,",
            "        {\"properties\": set_defaults_then_validate},",
            "    )",
            "",
            "",
            "class SchemaLoaderBase:",
            "    def get_validator(",
            "        self,",
            "        path: pathlib.Path | str,",
            "        instance_doc: dict[str, t.Any],",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        raise NotImplementedError",
            "",
            "",
            "class SchemaLoader(SchemaLoaderBase):",
            "    validator_class: type[jsonschema.protocols.Validator] | None = None",
            "    disable_cache: bool = True",
            "",
            "    def __init__(",
            "        self,",
            "        schemafile: str,",
            "        *,",
            "        base_uri: str | None = None,",
            "        validator_class: type[jsonschema.protocols.Validator] | None = None,",
            "        disable_cache: bool = True,",
            "    ) -> None:",
            "        # record input parameters (these are not to be modified)",
            "        self.schemafile = schemafile",
            "        self.disable_cache = disable_cache",
            "        self.base_uri = base_uri",
            "        self.validator_class = validator_class",
            "",
            "        # if the schema location is a URL, which may include a file:// URL, parse it",
            "        self.url_info = None",
            "        if is_url_ish(self.schemafile):",
            "            self.url_info = urllib.parse.urlparse(self.schemafile)",
            "",
            "        # setup a parser collection",
            "        self._parsers = ParserSet()",
            "",
            "        # setup a schema reader lazily, when needed",
            "        self._reader: (",
            "            LocalSchemaReader | HttpSchemaReader | StdinSchemaReader | None",
            "        ) = None",
            "",
            "    @property",
            "    def reader(self) -> LocalSchemaReader | HttpSchemaReader | StdinSchemaReader:",
            "        if self._reader is None:",
            "            self._reader = self._get_schema_reader()",
            "        return self._reader",
            "",
            "    def _get_schema_reader(",
            "        self,",
            "    ) -> LocalSchemaReader | HttpSchemaReader | StdinSchemaReader:",
            "        if self.schemafile == \"-\":",
            "            return StdinSchemaReader()",
            "",
            "        if self.url_info is None or self.url_info.scheme in (\"file\", \"\"):",
            "            return LocalSchemaReader(self.schemafile)",
            "",
            "        if self.url_info.scheme in (\"http\", \"https\"):",
            "            return HttpSchemaReader(self.schemafile, self.disable_cache)",
            "        else:",
            "            raise UnsupportedUrlScheme(",
            "                \"check-jsonschema only supports http, https, and local files. \"",
            "                f\"detected parsed URL had an unrecognized scheme: {self.url_info}\"",
            "            )",
            "",
            "    def get_schema_retrieval_uri(self) -> str | None:",
            "        return self.reader.get_retrieval_uri()",
            "",
            "    def get_schema(self) -> dict[str, t.Any]:",
            "        data = self.reader.read_schema()",
            "        if self.base_uri is not None:",
            "            data[\"$id\"] = self.base_uri",
            "        return data",
            "",
            "    def get_validator(",
            "        self,",
            "        path: pathlib.Path | str,",
            "        instance_doc: dict[str, t.Any],",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        return self._get_validator(format_opts, fill_defaults)",
            "",
            "    @functools.lru_cache",
            "    def _get_validator(",
            "        self,",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        retrieval_uri = self.get_schema_retrieval_uri()",
            "        schema = self.get_schema()",
            "",
            "        schema_dialect = schema.get(\"$schema\")",
            "        if schema_dialect is not None and not isinstance(schema_dialect, str):",
            "            schema_dialect = None",
            "",
            "        # format checker (which may be None)",
            "        format_checker = make_format_checker(format_opts, schema_dialect)",
            "",
            "        # reference resolution",
            "        # with support for YAML, TOML, and other formats from the parsers",
            "        reference_registry = make_reference_registry(",
            "            self._parsers, retrieval_uri, schema, self.disable_cache",
            "        )",
            "",
            "        if self.validator_class is None:",
            "            # get the correct validator class and check the schema under its metaschema",
            "            validator_cls = jsonschema.validators.validator_for(schema)",
            "            validator_cls.check_schema(schema)",
            "        else:",
            "            # for a user-provided validator class, don't check_schema",
            "            # on the grounds that it might *not* be valid but the user wants to use",
            "            # their custom validator anyway",
            "            #",
            "            # in fact, there's no real guarantee that a user-provided",
            "            # validator_class properly conforms to the jsonschema.Validator protocol",
            "            # we *hope* that it does, but we can't be fully sure",
            "            validator_cls = self.validator_class",
            "",
            "        # extend the validator class with default-filling behavior if appropriate",
            "        if fill_defaults:",
            "            validator_cls = _extend_with_default(validator_cls)",
            "",
            "        # now that we know it's safe to try to create the validator instance, do it",
            "        validator = validator_cls(",
            "            schema,",
            "            registry=reference_registry,",
            "            format_checker=format_checker,",
            "        )",
            "        return t.cast(jsonschema.protocols.Validator, validator)",
            "",
            "",
            "class BuiltinSchemaLoader(SchemaLoader):",
            "    def __init__(self, schema_name: str, *, base_uri: str | None = None) -> None:",
            "        self.schema_name = schema_name",
            "        self.base_uri = base_uri",
            "        self._parsers = ParserSet()",
            "",
            "    def get_schema_retrieval_uri(self) -> str | None:",
            "        return None",
            "",
            "    def get_schema(self) -> dict[str, t.Any]:",
            "        data = get_builtin_schema(self.schema_name)",
            "        if self.base_uri is not None:",
            "            data[\"$id\"] = self.base_uri",
            "        return data",
            "",
            "",
            "class MetaSchemaLoader(SchemaLoaderBase):",
            "    def __init__(self, *, base_uri: str | None = None) -> None:",
            "        if base_uri is not None:",
            "            raise NotImplementedError(",
            "                \"'--base-uri' was used with '--metaschema'. \"",
            "                \"This combination is not supported.\"",
            "            )",
            "",
            "    def get_validator(",
            "        self,",
            "        path: pathlib.Path | str,",
            "        instance_doc: dict[str, t.Any],",
            "        format_opts: FormatOptions,",
            "        fill_defaults: bool,",
            "    ) -> jsonschema.protocols.Validator:",
            "        schema_validator = jsonschema.validators.validator_for(instance_doc)",
            "        meta_validator_class = jsonschema.validators.validator_for(",
            "            schema_validator.META_SCHEMA, default=schema_validator",
            "        )",
            "",
            "        # format checker (which may be None)",
            "        meta_schema_dialect = schema_validator.META_SCHEMA.get(\"$schema\")",
            "        format_checker = make_format_checker(format_opts, meta_schema_dialect)",
            "",
            "        meta_validator = meta_validator_class(",
            "            schema_validator.META_SCHEMA, format_checker=format_checker",
            "        )",
            "        return meta_validator"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "67": [
                "SchemaLoader",
                "__init__"
            ],
            "74": [
                "SchemaLoader",
                "__init__"
            ],
            "108": [
                "SchemaLoader",
                "_get_schema_reader"
            ],
            "109": [
                "SchemaLoader",
                "_get_schema_reader"
            ],
            "110": [
                "SchemaLoader",
                "_get_schema_reader"
            ],
            "111": [
                "SchemaLoader",
                "_get_schema_reader"
            ],
            "112": [
                "SchemaLoader",
                "_get_schema_reader"
            ]
        },
        "addLocation": []
    },
    "src/check_jsonschema/schema_loader/readers.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "     def __init__("
            },
            "1": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "         self,"
            },
            "2": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         url: str,"
            },
            "3": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        cache_filename: str | None,"
            },
            "4": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "         disable_cache: bool,"
            },
            "5": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     ) -> None:"
            },
            "6": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "         self.url = url"
            },
            "7": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "         self.parsers = ParserSet()"
            },
            "8": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.downloader = CacheDownloader("
            },
            "9": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            disable_cache=disable_cache,"
            },
            "10": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ).bind(url, cache_filename, validation_callback=self._parse)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+        self.downloader = CacheDownloader(\"schemas\", disable_cache=disable_cache).bind("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+            url, validation_callback=self._parse"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+        )"
            },
            "14": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "         self._parsed_schema: dict | _UnsetType = _UNSET"
            },
            "15": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "     def _parse(self, schema_bytes: bytes) -> t.Any:"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import io",
            "import json",
            "import sys",
            "import typing as t",
            "",
            "import ruamel.yaml",
            "",
            "from ..cachedownloader import CacheDownloader",
            "from ..parsers import ParseError, ParserSet",
            "from ..utils import filename2path",
            "from .errors import SchemaParseError",
            "",
            "yaml = ruamel.yaml.YAML(typ=\"safe\")",
            "",
            "",
            "class _UnsetType:",
            "    pass",
            "",
            "",
            "_UNSET = _UnsetType()",
            "",
            "",
            "def _run_load_callback(schema_location: str, callback: t.Callable) -> dict:",
            "    try:",
            "        schema = callback()",
            "    # only local loads can raise the YAMLError, but catch for both cases for simplicity",
            "    except (ValueError, ruamel.yaml.error.YAMLError) as e:",
            "        raise SchemaParseError(schema_location) from e",
            "    if not isinstance(schema, dict):",
            "        raise SchemaParseError(schema_location)",
            "    return schema",
            "",
            "",
            "class LocalSchemaReader:",
            "    def __init__(self, filename: str) -> None:",
            "        self.path = filename2path(filename)",
            "        self.filename = str(self.path)",
            "        self.parsers = ParserSet()",
            "        self._parsed_schema: dict | _UnsetType = _UNSET",
            "",
            "    def get_retrieval_uri(self) -> str | None:",
            "        return self.path.as_uri()",
            "",
            "    def _read_impl(self) -> t.Any:",
            "        return self.parsers.parse_file(self.path, default_filetype=\"json\")",
            "",
            "    def read_schema(self) -> dict:",
            "        if self._parsed_schema is _UNSET:",
            "            self._parsed_schema = _run_load_callback(self.filename, self._read_impl)",
            "        return t.cast(dict, self._parsed_schema)",
            "",
            "",
            "class StdinSchemaReader:",
            "    def __init__(self) -> None:",
            "        self.parsers = ParserSet()",
            "        self._parsed_schema: dict | _UnsetType = _UNSET",
            "",
            "    def get_retrieval_uri(self) -> str | None:",
            "        return None",
            "",
            "    def read_schema(self) -> dict:",
            "        if self._parsed_schema is _UNSET:",
            "            try:",
            "                self._parsed_schema = json.load(sys.stdin)",
            "            except ValueError as e:",
            "                raise ParseError(\"Failed to parse JSON from stdin\") from e",
            "        return t.cast(dict, self._parsed_schema)",
            "",
            "",
            "class HttpSchemaReader:",
            "    def __init__(",
            "        self,",
            "        url: str,",
            "        cache_filename: str | None,",
            "        disable_cache: bool,",
            "    ) -> None:",
            "        self.url = url",
            "        self.parsers = ParserSet()",
            "        self.downloader = CacheDownloader(",
            "            disable_cache=disable_cache,",
            "        ).bind(url, cache_filename, validation_callback=self._parse)",
            "        self._parsed_schema: dict | _UnsetType = _UNSET",
            "",
            "    def _parse(self, schema_bytes: bytes) -> t.Any:",
            "        return self.parsers.parse_data_with_path(",
            "            io.BytesIO(schema_bytes), self.url, default_filetype=\"json\"",
            "        )",
            "",
            "    def get_retrieval_uri(self) -> str | None:",
            "        return self.url",
            "",
            "    def _read_impl(self) -> t.Any:",
            "        with self.downloader.open() as fp:",
            "            return self._parse(fp.read())",
            "",
            "    def read_schema(self) -> dict:",
            "        if self._parsed_schema is _UNSET:",
            "            self._parsed_schema = _run_load_callback(self.url, self._read_impl)",
            "        return t.cast(dict, self._parsed_schema)"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import io",
            "import json",
            "import sys",
            "import typing as t",
            "",
            "import ruamel.yaml",
            "",
            "from ..cachedownloader import CacheDownloader",
            "from ..parsers import ParseError, ParserSet",
            "from ..utils import filename2path",
            "from .errors import SchemaParseError",
            "",
            "yaml = ruamel.yaml.YAML(typ=\"safe\")",
            "",
            "",
            "class _UnsetType:",
            "    pass",
            "",
            "",
            "_UNSET = _UnsetType()",
            "",
            "",
            "def _run_load_callback(schema_location: str, callback: t.Callable) -> dict:",
            "    try:",
            "        schema = callback()",
            "    # only local loads can raise the YAMLError, but catch for both cases for simplicity",
            "    except (ValueError, ruamel.yaml.error.YAMLError) as e:",
            "        raise SchemaParseError(schema_location) from e",
            "    if not isinstance(schema, dict):",
            "        raise SchemaParseError(schema_location)",
            "    return schema",
            "",
            "",
            "class LocalSchemaReader:",
            "    def __init__(self, filename: str) -> None:",
            "        self.path = filename2path(filename)",
            "        self.filename = str(self.path)",
            "        self.parsers = ParserSet()",
            "        self._parsed_schema: dict | _UnsetType = _UNSET",
            "",
            "    def get_retrieval_uri(self) -> str | None:",
            "        return self.path.as_uri()",
            "",
            "    def _read_impl(self) -> t.Any:",
            "        return self.parsers.parse_file(self.path, default_filetype=\"json\")",
            "",
            "    def read_schema(self) -> dict:",
            "        if self._parsed_schema is _UNSET:",
            "            self._parsed_schema = _run_load_callback(self.filename, self._read_impl)",
            "        return t.cast(dict, self._parsed_schema)",
            "",
            "",
            "class StdinSchemaReader:",
            "    def __init__(self) -> None:",
            "        self.parsers = ParserSet()",
            "        self._parsed_schema: dict | _UnsetType = _UNSET",
            "",
            "    def get_retrieval_uri(self) -> str | None:",
            "        return None",
            "",
            "    def read_schema(self) -> dict:",
            "        if self._parsed_schema is _UNSET:",
            "            try:",
            "                self._parsed_schema = json.load(sys.stdin)",
            "            except ValueError as e:",
            "                raise ParseError(\"Failed to parse JSON from stdin\") from e",
            "        return t.cast(dict, self._parsed_schema)",
            "",
            "",
            "class HttpSchemaReader:",
            "    def __init__(",
            "        self,",
            "        url: str,",
            "        disable_cache: bool,",
            "    ) -> None:",
            "        self.url = url",
            "        self.parsers = ParserSet()",
            "        self.downloader = CacheDownloader(\"schemas\", disable_cache=disable_cache).bind(",
            "            url, validation_callback=self._parse",
            "        )",
            "        self._parsed_schema: dict | _UnsetType = _UNSET",
            "",
            "    def _parse(self, schema_bytes: bytes) -> t.Any:",
            "        return self.parsers.parse_data_with_path(",
            "            io.BytesIO(schema_bytes), self.url, default_filetype=\"json\"",
            "        )",
            "",
            "    def get_retrieval_uri(self) -> str | None:",
            "        return self.url",
            "",
            "    def _read_impl(self) -> t.Any:",
            "        with self.downloader.open() as fp:",
            "            return self._parse(fp.read())",
            "",
            "    def read_schema(self) -> dict:",
            "        if self._parsed_schema is _UNSET:",
            "            self._parsed_schema = _run_load_callback(self.url, self._read_impl)",
            "        return t.cast(dict, self._parsed_schema)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "76": [
                "HttpSchemaReader",
                "__init__"
            ],
            "81": [
                "HttpSchemaReader",
                "__init__"
            ],
            "82": [
                "HttpSchemaReader",
                "__init__"
            ],
            "83": [
                "HttpSchemaReader",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "src/check_jsonschema/schema_loader/resolver.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " from __future__ import annotations"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import hashlib"
            },
            "3": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import typing as t"
            },
            "4": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import urllib.parse"
            },
            "5": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from ..utils import filename2path"
            },
            "7": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def ref_url_to_cache_filename(ref_url: str) -> str:"
            },
            "10": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\""
            },
            "11": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Given a $ref URL, convert it to the filename in the refs/ cache dir."
            },
            "12": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Rules are as follows:"
            },
            "13": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    - the base filename is an md5 hash of the URL"
            },
            "14": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    - if the filename ends in an extension (.json, .yaml, etc) that extension"
            },
            "15": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-      is appended to the hash"
            },
            "16": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\""
            },
            "17": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    filename = hashlib.md5(ref_url.encode()).hexdigest()"
            },
            "18": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):"
            },
            "19": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        _, _, extension = last_part.rpartition(\".\")"
            },
            "20": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        filename = f\"{filename}.{extension}\""
            },
            "21": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return filename"
            },
            "22": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "23": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "24": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " def make_reference_registry("
            },
            "25": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 15,
                "PatchRowcode": "     parsers: ParserSet, retrieval_uri: str | None, schema: dict, disable_cache: bool"
            },
            "26": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " ) -> referencing.Registry:"
            },
            "27": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "         base_uri = retrieval_uri"
            },
            "28": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     cache = ResourceCache()"
            },
            "30": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    downloader = CacheDownloader(\"refs\", disable_cache)"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+    downloader = CacheDownloader(\"refs\", disable_cache=disable_cache)"
            },
            "32": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " "
            },
            "33": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "     def get_local_file(uri: str) -> t.Any:"
            },
            "34": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "         path = filename2path(uri)"
            },
            "35": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "                 parser_set.parse_data_with_path(content, full_uri, \"json\")"
            },
            "36": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " "
            },
            "37": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "             bound_downloader = downloader.bind("
            },
            "38": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                full_uri,"
            },
            "39": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                ref_url_to_cache_filename(full_uri),"
            },
            "40": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                validation_callback,"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+                full_uri, validation_callback=validation_callback"
            },
            "42": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "             )"
            },
            "43": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "             with bound_downloader.open() as fp:"
            },
            "44": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "                 data = fp.read()"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import hashlib",
            "import typing as t",
            "import urllib.parse",
            "",
            "import referencing",
            "from referencing.jsonschema import DRAFT202012, Schema",
            "",
            "from ..cachedownloader import CacheDownloader",
            "from ..parsers import ParserSet",
            "from ..utils import filename2path",
            "",
            "",
            "def ref_url_to_cache_filename(ref_url: str) -> str:",
            "    \"\"\"",
            "    Given a $ref URL, convert it to the filename in the refs/ cache dir.",
            "    Rules are as follows:",
            "    - the base filename is an md5 hash of the URL",
            "    - if the filename ends in an extension (.json, .yaml, etc) that extension",
            "      is appended to the hash",
            "    \"\"\"",
            "    filename = hashlib.md5(ref_url.encode()).hexdigest()",
            "    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):",
            "        _, _, extension = last_part.rpartition(\".\")",
            "        filename = f\"{filename}.{extension}\"",
            "    return filename",
            "",
            "",
            "def make_reference_registry(",
            "    parsers: ParserSet, retrieval_uri: str | None, schema: dict, disable_cache: bool",
            ") -> referencing.Registry:",
            "    id_attribute_: t.Any = schema.get(\"$id\")",
            "    if isinstance(id_attribute_, str):",
            "        id_attribute: str | None = id_attribute_",
            "    else:",
            "        id_attribute = None",
            "",
            "    schema_resource = referencing.Resource.from_contents(",
            "        schema, default_specification=DRAFT202012",
            "    )",
            "    # mypy does not recognize that Registry is an `attrs` class and has `retrieve` as an",
            "    # argument to its implicit initializer",
            "    registry: referencing.Registry = referencing.Registry(  # type: ignore[call-arg]",
            "        retrieve=create_retrieve_callable(",
            "            parsers, retrieval_uri, id_attribute, disable_cache",
            "        )",
            "    )",
            "",
            "    if retrieval_uri is not None:",
            "        registry = registry.with_resource(uri=retrieval_uri, resource=schema_resource)",
            "    if id_attribute is not None:",
            "        registry = registry.with_resource(uri=id_attribute, resource=schema_resource)",
            "",
            "    return registry",
            "",
            "",
            "def create_retrieve_callable(",
            "    parser_set: ParserSet,",
            "    retrieval_uri: str | None,",
            "    id_attribute: str | None,",
            "    disable_cache: bool,",
            ") -> t.Callable[[str], referencing.Resource[Schema]]:",
            "    base_uri = id_attribute",
            "    if base_uri is None:",
            "        base_uri = retrieval_uri",
            "",
            "    cache = ResourceCache()",
            "    downloader = CacheDownloader(\"refs\", disable_cache)",
            "",
            "    def get_local_file(uri: str) -> t.Any:",
            "        path = filename2path(uri)",
            "        return parser_set.parse_file(path, \"json\")",
            "",
            "    def retrieve_reference(uri: str) -> referencing.Resource[Schema]:",
            "        scheme = urllib.parse.urlsplit(uri).scheme",
            "        if scheme == \"\" and base_uri is not None:",
            "            full_uri = urllib.parse.urljoin(base_uri, uri)",
            "        else:",
            "            full_uri = uri",
            "",
            "        if full_uri in cache:",
            "            return cache[full_uri]",
            "",
            "        full_uri_scheme = urllib.parse.urlsplit(full_uri).scheme",
            "        if full_uri_scheme in (\"http\", \"https\"):",
            "",
            "            def validation_callback(content: bytes) -> None:",
            "                parser_set.parse_data_with_path(content, full_uri, \"json\")",
            "",
            "            bound_downloader = downloader.bind(",
            "                full_uri,",
            "                ref_url_to_cache_filename(full_uri),",
            "                validation_callback,",
            "            )",
            "            with bound_downloader.open() as fp:",
            "                data = fp.read()",
            "",
            "            parsed_object = parser_set.parse_data_with_path(data, full_uri, \"json\")",
            "        else:",
            "            parsed_object = get_local_file(full_uri)",
            "",
            "        cache[full_uri] = parsed_object",
            "        return cache[full_uri]",
            "",
            "    return retrieve_reference",
            "",
            "",
            "class ResourceCache:",
            "    def __init__(self) -> None:",
            "        self._cache: t.Dict[str, referencing.Resource[Schema]] = {}",
            "",
            "    def __setitem__(self, uri: str, data: t.Any) -> referencing.Resource[Schema]:",
            "        resource = referencing.Resource.from_contents(",
            "            data, default_specification=DRAFT202012",
            "        )",
            "        self._cache[uri] = resource",
            "        return resource",
            "",
            "    def __getitem__(self, uri: str) -> referencing.Resource[Schema]:",
            "        return self._cache[uri]",
            "",
            "    def __contains__(self, uri: str) -> bool:",
            "        return uri in self._cache"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import typing as t",
            "import urllib.parse",
            "",
            "import referencing",
            "from referencing.jsonschema import DRAFT202012, Schema",
            "",
            "from ..cachedownloader import CacheDownloader",
            "from ..parsers import ParserSet",
            "from ..utils import filename2path",
            "",
            "",
            "def make_reference_registry(",
            "    parsers: ParserSet, retrieval_uri: str | None, schema: dict, disable_cache: bool",
            ") -> referencing.Registry:",
            "    id_attribute_: t.Any = schema.get(\"$id\")",
            "    if isinstance(id_attribute_, str):",
            "        id_attribute: str | None = id_attribute_",
            "    else:",
            "        id_attribute = None",
            "",
            "    schema_resource = referencing.Resource.from_contents(",
            "        schema, default_specification=DRAFT202012",
            "    )",
            "    # mypy does not recognize that Registry is an `attrs` class and has `retrieve` as an",
            "    # argument to its implicit initializer",
            "    registry: referencing.Registry = referencing.Registry(  # type: ignore[call-arg]",
            "        retrieve=create_retrieve_callable(",
            "            parsers, retrieval_uri, id_attribute, disable_cache",
            "        )",
            "    )",
            "",
            "    if retrieval_uri is not None:",
            "        registry = registry.with_resource(uri=retrieval_uri, resource=schema_resource)",
            "    if id_attribute is not None:",
            "        registry = registry.with_resource(uri=id_attribute, resource=schema_resource)",
            "",
            "    return registry",
            "",
            "",
            "def create_retrieve_callable(",
            "    parser_set: ParserSet,",
            "    retrieval_uri: str | None,",
            "    id_attribute: str | None,",
            "    disable_cache: bool,",
            ") -> t.Callable[[str], referencing.Resource[Schema]]:",
            "    base_uri = id_attribute",
            "    if base_uri is None:",
            "        base_uri = retrieval_uri",
            "",
            "    cache = ResourceCache()",
            "    downloader = CacheDownloader(\"refs\", disable_cache=disable_cache)",
            "",
            "    def get_local_file(uri: str) -> t.Any:",
            "        path = filename2path(uri)",
            "        return parser_set.parse_file(path, \"json\")",
            "",
            "    def retrieve_reference(uri: str) -> referencing.Resource[Schema]:",
            "        scheme = urllib.parse.urlsplit(uri).scheme",
            "        if scheme == \"\" and base_uri is not None:",
            "            full_uri = urllib.parse.urljoin(base_uri, uri)",
            "        else:",
            "            full_uri = uri",
            "",
            "        if full_uri in cache:",
            "            return cache[full_uri]",
            "",
            "        full_uri_scheme = urllib.parse.urlsplit(full_uri).scheme",
            "        if full_uri_scheme in (\"http\", \"https\"):",
            "",
            "            def validation_callback(content: bytes) -> None:",
            "                parser_set.parse_data_with_path(content, full_uri, \"json\")",
            "",
            "            bound_downloader = downloader.bind(",
            "                full_uri, validation_callback=validation_callback",
            "            )",
            "            with bound_downloader.open() as fp:",
            "                data = fp.read()",
            "",
            "            parsed_object = parser_set.parse_data_with_path(data, full_uri, \"json\")",
            "        else:",
            "            parsed_object = get_local_file(full_uri)",
            "",
            "        cache[full_uri] = parsed_object",
            "        return cache[full_uri]",
            "",
            "    return retrieve_reference",
            "",
            "",
            "class ResourceCache:",
            "    def __init__(self) -> None:",
            "        self._cache: t.Dict[str, referencing.Resource[Schema]] = {}",
            "",
            "    def __setitem__(self, uri: str, data: t.Any) -> referencing.Resource[Schema]:",
            "        resource = referencing.Resource.from_contents(",
            "            data, default_specification=DRAFT202012",
            "        )",
            "        self._cache[uri] = resource",
            "        return resource",
            "",
            "    def __getitem__(self, uri: str) -> referencing.Resource[Schema]:",
            "        return self._cache[uri]",
            "",
            "    def __contains__(self, uri: str) -> bool:",
            "        return uri in self._cache"
        ],
        "action": [
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "3": [],
            "15": [
                "ref_url_to_cache_filename"
            ],
            "16": [
                "ref_url_to_cache_filename"
            ],
            "17": [
                "ref_url_to_cache_filename"
            ],
            "18": [
                "ref_url_to_cache_filename"
            ],
            "19": [
                "ref_url_to_cache_filename"
            ],
            "20": [
                "ref_url_to_cache_filename"
            ],
            "21": [
                "ref_url_to_cache_filename"
            ],
            "22": [
                "ref_url_to_cache_filename"
            ],
            "23": [
                "ref_url_to_cache_filename"
            ],
            "24": [
                "ref_url_to_cache_filename"
            ],
            "25": [
                "ref_url_to_cache_filename"
            ],
            "26": [
                "ref_url_to_cache_filename"
            ],
            "27": [
                "ref_url_to_cache_filename"
            ],
            "28": [],
            "29": [],
            "69": [
                "create_retrieve_callable"
            ],
            "92": [
                "create_retrieve_callable",
                "retrieve_reference"
            ],
            "93": [
                "create_retrieve_callable",
                "retrieve_reference"
            ],
            "94": [
                "create_retrieve_callable",
                "retrieve_reference"
            ]
        },
        "addLocation": []
    }
}