{
    "web/server/codechecker_server/api/config_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": "     Manages Thrift requests regarding configuration."
            },
            "1": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "     \"\"\""
            },
            "2": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __init__(self, auth_session, config_session):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+    def __init__(self, auth_session, config_session, session_manager):"
            },
            "5": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "         self.__auth_session = auth_session"
            },
            "6": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "         self.__session = config_session"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+        self.__session_manager = session_manager"
            },
            "8": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     def __require_supermission(self):"
            },
            "10": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "         \"\"\""
            },
            "11": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "         Checks if the current user isn't a SUPERUSER."
            },
            "12": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "         \"\"\""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+        # Anonymous access is only allowed if authentication is"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+        # turned off"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        if self.__session_manager.is_enabled and not self.__auth_session:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+            raise codechecker_api_shared.ttypes.RequestFailed("
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+                codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+                \"You are not authorized to execute this action.\")"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+"
            },
            "21": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "         if (not (self.__auth_session is None) and"
            },
            "22": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "                 not self.__auth_session.is_root):"
            },
            "23": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "             raise codechecker_api_shared.ttypes.RequestFailed("
            },
            "24": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "     def setNotificationBannerText(self, notification_b64):"
            },
            "25": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "         \"\"\""
            },
            "26": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "         Sets the notification banner remove_products_except."
            },
            "27": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Bevare: This method only works if the use is a SUPERUSER."
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        Beware: This method only works if the use is a SUPERUSER."
            },
            "29": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "         \"\"\""
            },
            "30": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 83,
                "PatchRowcode": " "
            },
            "31": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "         self.__require_supermission()"
            }
        },
        "frontPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Handle Thrift requests for configuration.",
            "\"\"\"",
            "",
            "",
            "import codechecker_api_shared",
            "",
            "from codechecker_common.logger import get_logger",
            "",
            "from codechecker_server.profiler import timeit",
            "",
            "from codechecker_web.shared import convert",
            "",
            "from ..database.config_db_model import Configuration",
            "from ..database.database import DBSession",
            "",
            "LOG = get_logger('server')",
            "",
            "",
            "# These names are inherited from Thrift stubs.",
            "# pylint: disable=invalid-name",
            "class ThriftConfigHandler:",
            "    \"\"\"",
            "    Manages Thrift requests regarding configuration.",
            "    \"\"\"",
            "",
            "    def __init__(self, auth_session, config_session):",
            "        self.__auth_session = auth_session",
            "        self.__session = config_session",
            "",
            "    def __require_supermission(self):",
            "        \"\"\"",
            "        Checks if the current user isn't a SUPERUSER.",
            "        \"\"\"",
            "        if (not (self.__auth_session is None) and",
            "                not self.__auth_session.is_root):",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                \"You are not authorized to modify the notification.\")",
            "",
            "        return True",
            "",
            "    @timeit",
            "    def getNotificationBannerText(self):",
            "        \"\"\"",
            "        Retrieves the notification banner text.",
            "        \"\"\"",
            "",
            "        notificationString = ''",
            "        with DBSession(self.__session) as session:",
            "            notificationQuery = session.query(Configuration) \\",
            "                .filter(",
            "                    Configuration.config_key == 'notification_banner_text') \\",
            "                .one_or_none()",
            "",
            "            if notificationQuery is not None:",
            "                notificationString = notificationQuery.config_value",
            "",
            "        return convert.to_b64(notificationString)",
            "",
            "    @timeit",
            "    def setNotificationBannerText(self, notification_b64):",
            "        \"\"\"",
            "        Sets the notification banner remove_products_except.",
            "        Bevare: This method only works if the use is a SUPERUSER.",
            "        \"\"\"",
            "",
            "        self.__require_supermission()",
            "",
            "        notification = convert.from_b64(notification_b64)",
            "",
            "        with DBSession(self.__session) as session:",
            "            notificationQuery = session.query(Configuration) \\",
            "                .filter(",
            "                    Configuration.config_key == 'notification_banner_text') \\",
            "                .one_or_none()",
            "",
            "            if notificationQuery is None:",
            "                conf = Configuration('notification_banner_text', notification)",
            "                session.add(conf)",
            "                session.flush()",
            "            else:",
            "                # update it",
            "                notificationQuery.config_value = notification",
            "",
            "            session.commit()",
            "            session.close()"
        ],
        "afterPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Handle Thrift requests for configuration.",
            "\"\"\"",
            "",
            "",
            "import codechecker_api_shared",
            "",
            "from codechecker_common.logger import get_logger",
            "",
            "from codechecker_server.profiler import timeit",
            "",
            "from codechecker_web.shared import convert",
            "",
            "from ..database.config_db_model import Configuration",
            "from ..database.database import DBSession",
            "",
            "LOG = get_logger('server')",
            "",
            "",
            "# These names are inherited from Thrift stubs.",
            "# pylint: disable=invalid-name",
            "class ThriftConfigHandler:",
            "    \"\"\"",
            "    Manages Thrift requests regarding configuration.",
            "    \"\"\"",
            "",
            "    def __init__(self, auth_session, config_session, session_manager):",
            "        self.__auth_session = auth_session",
            "        self.__session = config_session",
            "        self.__session_manager = session_manager",
            "",
            "    def __require_supermission(self):",
            "        \"\"\"",
            "        Checks if the current user isn't a SUPERUSER.",
            "        \"\"\"",
            "",
            "        # Anonymous access is only allowed if authentication is",
            "        # turned off",
            "        if self.__session_manager.is_enabled and not self.__auth_session:",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                \"You are not authorized to execute this action.\")",
            "",
            "        if (not (self.__auth_session is None) and",
            "                not self.__auth_session.is_root):",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                \"You are not authorized to modify the notification.\")",
            "",
            "        return True",
            "",
            "    @timeit",
            "    def getNotificationBannerText(self):",
            "        \"\"\"",
            "        Retrieves the notification banner text.",
            "        \"\"\"",
            "",
            "        notificationString = ''",
            "        with DBSession(self.__session) as session:",
            "            notificationQuery = session.query(Configuration) \\",
            "                .filter(",
            "                    Configuration.config_key == 'notification_banner_text') \\",
            "                .one_or_none()",
            "",
            "            if notificationQuery is not None:",
            "                notificationString = notificationQuery.config_value",
            "",
            "        return convert.to_b64(notificationString)",
            "",
            "    @timeit",
            "    def setNotificationBannerText(self, notification_b64):",
            "        \"\"\"",
            "        Sets the notification banner remove_products_except.",
            "        Beware: This method only works if the use is a SUPERUSER.",
            "        \"\"\"",
            "",
            "        self.__require_supermission()",
            "",
            "        notification = convert.from_b64(notification_b64)",
            "",
            "        with DBSession(self.__session) as session:",
            "            notificationQuery = session.query(Configuration) \\",
            "                .filter(",
            "                    Configuration.config_key == 'notification_banner_text') \\",
            "                .one_or_none()",
            "",
            "            if notificationQuery is None:",
            "                conf = Configuration('notification_banner_text', notification)",
            "                session.add(conf)",
            "                session.flush()",
            "            else:",
            "                # update it",
            "                notificationQuery.config_value = notification",
            "",
            "            session.commit()",
            "            session.close()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "34": [
                "ThriftConfigHandler",
                "__init__"
            ],
            "72": [
                "ThriftConfigHandler",
                "setNotificationBannerText"
            ]
        },
        "addLocation": []
    },
    "web/server/codechecker_server/api/product_server.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "                 args = dict(self.__permission_args)"
            },
            "1": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "                 args['config_db_session'] = session"
            },
            "2": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+            # Anonymous access is only allowed if authentication is"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+            # turned off"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+            if self.__server.manager.is_enabled and not self.__auth_session:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+                raise codechecker_api_shared.ttypes.RequestFailed("
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+                    \"You are not authorized to execute this action.\")"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+"
            },
            "10": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "             if not any(permissions.require_permission("
            },
            "11": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "                            perm, args, self.__auth_session)"
            },
            "12": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "                        for perm in required):"
            },
            "13": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 254,
                "PatchRowcode": "         Get the product configuration --- WITHOUT THE DB PASSWORD --- of the"
            },
            "14": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "         given product."
            },
            "15": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 256,
                "PatchRowcode": "         \"\"\""
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 257,
                "PatchRowcode": "+        self.__require_permission([permissions.PRODUCT_VIEW])"
            },
            "17": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 258,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 251,
                "afterPatchRowNumber": 259,
                "PatchRowcode": "         with DBSession(self.__session) as session:"
            },
            "19": {
                "beforePatchRowNumber": 252,
                "afterPatchRowNumber": 260,
                "PatchRowcode": "             product = session.query(Product).get(product_id)"
            }
        },
        "frontPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Handle Thrift requests for the product manager service.",
            "\"\"\"",
            "",
            "",
            "import os",
            "import random",
            "",
            "from sqlalchemy.sql.expression import and_",
            "",
            "import codechecker_api_shared",
            "from codechecker_api.ProductManagement_v6 import ttypes",
            "",
            "from codechecker_common.logger import get_logger",
            "",
            "from codechecker_server.profiler import timeit",
            "from codechecker_web.shared import convert",
            "",
            "from .. import permissions",
            "from ..database.config_db_model import IDENTIFIER, Product, ProductPermission",
            "from ..database.database import DBSession, SQLServer, conv, escape_like",
            "from ..routing import is_valid_product_endpoint",
            "",
            "from .thrift_enum_helper import confidentiality_enum, \\",
            "        confidentiality_str",
            "",
            "LOG = get_logger('server')",
            "",
            "",
            "# These names are inherited from Thrift stubs.",
            "# pylint: disable=invalid-name",
            "class ThriftProductHandler:",
            "    \"\"\"",
            "    Connect to database and handle thrift client requests.",
            "    \"\"\"",
            "",
            "    def __init__(self,",
            "                 server,",
            "                 auth_session,",
            "                 config_session,",
            "                 routed_product,",
            "                 package_version):",
            "",
            "        self.__server = server",
            "        self.__auth_session = auth_session",
            "        self.__package_version = package_version",
            "        self.__session = config_session",
            "        self.__product = routed_product",
            "",
            "        self.__permission_args = {",
            "            'productID': routed_product.id if routed_product else None",
            "        }",
            "",
            "    def __require_permission(self, required, args=None):",
            "        \"\"\"",
            "        Helper method to raise an UNAUTHORIZED exception if the user does not",
            "        have any of the given permissions.",
            "        \"\"\"",
            "",
            "        with DBSession(self.__session) as session:",
            "            if args is None:",
            "                args = dict(self.__permission_args)",
            "                args['config_db_session'] = session",
            "",
            "            if not any(permissions.require_permission(",
            "                           perm, args, self.__auth_session)",
            "                       for perm in required):",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                    \"You are not authorized to execute this action.\")",
            "",
            "            return True",
            "",
            "    def __administrating(self, args):",
            "        \"\"\" True if the current user can administrate the given product. \"\"\"",
            "        if permissions.require_permission(permissions.SUPERUSER, args,",
            "                                          self.__auth_session):",
            "            return True",
            "",
            "        if permissions.require_permission(permissions.PRODUCT_ADMIN, args,",
            "                                          self.__auth_session):",
            "            return True",
            "",
            "        return False",
            "",
            "    def __get_product(self, session, product):",
            "        \"\"\"",
            "        Retrieve the product connection object and create a Thrift Product",
            "        object for the given product record in the database.",
            "        \"\"\"",
            "",
            "        server_product = self.__server.get_product(product.endpoint)",
            "        if not server_product:",
            "            LOG.info(\"Product '%s' was found in the configuration \"",
            "                     \"database but no database connection was \"",
            "                     \"present. Mounting analysis run database...\",",
            "                     product.endpoint)",
            "            self.__server.add_product(product)",
            "            server_product = self.__server.get_product(product.endpoint)",
            "",
            "        descr = convert.to_b64(product.description) \\",
            "            if product.description else None",
            "",
            "        args = {'config_db_session': session,",
            "                'productID': product.id}",
            "        product_access = permissions.require_permission(",
            "            permissions.PRODUCT_VIEW, args, self.__auth_session)",
            "",
            "        admin_perm_name = permissions.PRODUCT_ADMIN.name",
            "        admins = session.query(ProductPermission). \\",
            "            filter(and_(ProductPermission.permission == admin_perm_name,",
            "                        ProductPermission.product_id == product.id)) \\",
            "            .all()",
            "",
            "        connected = server_product.db_status ==\\",
            "            codechecker_api_shared.ttypes.DBStatus.OK",
            "",
            "        latest_storage_date = str(product.latest_storage_date) \\",
            "            if product.latest_storage_date else None",
            "",
            "        if product.confidentiality is None:",
            "            confidentiality = ttypes.Confidentiality.CONFIDENTIAL",
            "        else:",
            "            confidentiality = \\",
            "                    confidentiality_enum(product.confidentiality)",
            "",
            "        report_limit = product.report_limit",
            "",
            "        return server_product, ttypes.Product(",
            "            id=product.id,",
            "            endpoint=product.endpoint,",
            "            displayedName_b64=convert.to_b64(product.display_name),",
            "            description_b64=descr,",
            "            runCount=product.num_of_runs,",
            "            latestStoreToProduct=latest_storage_date,",
            "            connected=connected,",
            "            accessible=product_access,",
            "            administrating=self.__administrating(args),",
            "            databaseStatus=server_product.db_status,",
            "            admins=[admin.name for admin in admins],",
            "            confidentiality=confidentiality,",
            "            reportLimit=report_limit)",
            "",
            "    @timeit",
            "    def getPackageVersion(self):",
            "        return self.__package_version",
            "",
            "    @timeit",
            "    def isAdministratorOfAnyProduct(self):",
            "        with DBSession(self.__session) as session:",
            "            prods = session.query(Product).all()",
            "",
            "            for prod in prods:",
            "                args = {'config_db_session': session,",
            "                        'productID': prod.id}",
            "                if permissions.require_permission(",
            "                        permissions.PRODUCT_ADMIN,",
            "                        args, self.__auth_session):",
            "                    return True",
            "",
            "            return False",
            "",
            "    @timeit",
            "    def getProducts(self, product_endpoint_filter, product_name_filter):",
            "        \"\"\"",
            "        Get the list of products configured on the server.",
            "        \"\"\"",
            "",
            "        result = []",
            "",
            "        with DBSession(self.__session) as session:",
            "            prods = session.query(Product)",
            "",
            "            num_all_products = prods.count()  # prods get filtered later.",
            "            if num_all_products < self.__server.num_products:",
            "                # It can happen that a product gets removed from the",
            "                # configuration database from a different server that uses the",
            "                # same configuration database. In this case, the product is",
            "                # no longer valid, yet the current server keeps a connection",
            "                # object up.",
            "                LOG.info(\"%d products were removed but server is still \"",
            "                         \"connected to them. Disconnecting these...\",",
            "                         self.__server.num_products - num_all_products)",
            "",
            "                all_products = session.query(Product).all()",
            "                self.__server.remove_products_except([prod.endpoint for prod",
            "                                                      in all_products])",
            "",
            "            if product_endpoint_filter:",
            "                prods = prods.filter(Product.endpoint.ilike(",
            "                    conv(escape_like(product_endpoint_filter, '\\\\')),",
            "                    escape='\\\\'))",
            "",
            "            if product_name_filter:",
            "                prods = prods.filter(Product.display_name.ilike(",
            "                    conv(escape_like(product_name_filter, '\\\\')),",
            "                    escape='\\\\'))",
            "",
            "            prods = prods.all()",
            "            for prod in prods:",
            "                _, ret = self.__get_product(session, prod)",
            "                result.append(ret)",
            "",
            "            return result",
            "",
            "    @timeit",
            "    def getCurrentProduct(self):",
            "        \"\"\"",
            "        Return information about the current product.",
            "",
            "        The request MUST be routed as /product-name/ProductService!",
            "        \"\"\"",
            "",
            "        if not self.__product:",
            "            msg = \"Requested current product from ProductService but the \" \\",
            "                  \"request came through the main endpoint.\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.IOERROR,",
            "                msg)",
            "",
            "        with DBSession(self.__session) as session:",
            "            prod = session.query(Product).get(self.__product.id)",
            "",
            "            if not prod:",
            "                msg = \"The product requested has been disconnected from the \" \\",
            "                      \"server.\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.IOERROR,",
            "                    msg)",
            "",
            "            _, ret = self.__get_product(session, prod)",
            "            LOG.debug(ret)",
            "            return ret",
            "",
            "    @timeit",
            "    def getProductConfiguration(self, product_id):",
            "        \"\"\"",
            "        Get the product configuration --- WITHOUT THE DB PASSWORD --- of the",
            "        given product.",
            "        \"\"\"",
            "",
            "        with DBSession(self.__session) as session:",
            "            product = session.query(Product).get(product_id)",
            "            if product is None:",
            "                msg = f\"Product with ID {product_id} does not exist!\"",
            "                LOG.error(msg)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            # Put together the database connection's descriptor.",
            "            args = SQLServer.connection_string_to_args(product.connection)",
            "            if args['postgresql']:",
            "                db_engine = 'postgresql'",
            "                db_host = args['dbaddress']",
            "                db_port = args['dbport']",
            "                db_user = args['dbusername']",
            "                db_name = args['dbname']",
            "            else:",
            "                db_engine = 'sqlite'",
            "                db_host = \"\"",
            "                db_port = 0",
            "                db_user = \"\"",
            "                db_name = args['sqlite']",
            "",
            "            dbc = ttypes.DatabaseConnection(",
            "                engine=db_engine,",
            "                host=db_host,",
            "                port=db_port,",
            "                username_b64=convert.to_b64(db_user),",
            "                # DO NOT TRANSPORT PASSWORD BACK TO THE USER!",
            "                database=db_name)",
            "",
            "            # Put together the product configuration.",
            "            descr = convert.to_b64(product.description) \\",
            "                if product.description else None",
            "",
            "            is_review_status_change_disabled = \\",
            "                product.is_review_status_change_disabled",
            "",
            "            if product.confidentiality is None:",
            "                confidentiality = ttypes.Confidentiality.CONFIDENTIAL",
            "            else:",
            "                confidentiality = \\",
            "                        confidentiality_enum(product.confidentiality)",
            "",
            "            prod = ttypes.ProductConfiguration(",
            "                id=product.id,",
            "                endpoint=product.endpoint,",
            "                displayedName_b64=convert.to_b64(product.display_name),",
            "                description_b64=descr,",
            "                connection=dbc,",
            "                runLimit=product.run_limit,",
            "                reportLimit=product.report_limit,",
            "                isReviewStatusChangeDisabled=is_review_status_change_disabled,",
            "                confidentiality=confidentiality)",
            "",
            "            return prod",
            "",
            "    @timeit",
            "    def addProduct(self, product):",
            "        \"\"\"",
            "        Add the given product to the products configured by the server.",
            "        \"\"\"",
            "        self.__require_permission([permissions.SUPERUSER])",
            "",
            "        session = None",
            "        LOG.info(\"User requested add product '%s'\", product.endpoint)",
            "",
            "        if not is_valid_product_endpoint(product.endpoint):",
            "            msg = \"The specified endpoint is invalid.\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        dbc = product.connection",
            "        if not dbc:",
            "            msg = \"Product cannot be added without a database configuration!\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        if self.__server.get_product(product.endpoint):",
            "            msg = \\",
            "                f\"A product endpoint '/{product.endpoint}' is already \" \\",
            "                \"configured!\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        # Some values come encoded as Base64, decode these.",
            "        displayed_name = convert.from_b64(product.displayedName_b64) \\",
            "            if product.displayedName_b64 else product.endpoint",
            "        description = convert.from_b64(product.description_b64) \\",
            "            if product.description_b64 else None",
            "",
            "        if dbc.engine == 'sqlite' and not os.path.isabs(dbc.database):",
            "            # Transform the database relative path to be under the",
            "            # server's config directory.",
            "            dbc.database = os.path.join(self.__server.config_directory,",
            "                                        dbc.database)",
            "",
            "        # Transform arguments into a database connection string.",
            "        if dbc.engine == 'postgresql':",
            "            dbuser = \"codechecker\"",
            "            dbpass = \"\"",
            "            if dbc.username_b64 and dbc.username_b64 != '':",
            "                dbuser = convert.from_b64(dbc.username_b64)",
            "            if dbc.password_b64 and dbc.password_b64 != '':",
            "                dbpass = convert.from_b64(dbc.password_b64)",
            "",
            "            conn_str_args = {'postgresql': True,",
            "                             'sqlite': False,",
            "                             'dbaddress': dbc.host,",
            "                             'dbport': dbc.port,",
            "                             'dbusername': dbuser,",
            "                             'dbpassword': dbpass,",
            "                             'dbname': dbc.database}",
            "        elif dbc.engine == 'sqlite':",
            "            conn_str_args = {'postgresql': False,",
            "                             'sqlite': dbc.database}",
            "        else:",
            "            msg = f\"Database engine '{dbc.engine}' unknown!\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        conn_str = SQLServer \\",
            "            .from_cmdline_args(conn_str_args, product.endpoint, IDENTIFIER,",
            "                               None, False, None) \\",
            "            .get_connection_string()",
            "",
            "        is_rws_change_disabled = product.isReviewStatusChangeDisabled",
            "",
            "        confidentiality = confidentiality_str(product.confidentiality)",
            "",
            "        # Create the product's entity in the database.",
            "        with DBSession(self.__session) as session:",
            "            orm_prod = Product(",
            "                endpoint=product.endpoint,",
            "                conn_str=conn_str,",
            "                name=displayed_name,",
            "                description=description,",
            "                run_limit=product.runLimit,",
            "                report_limit=product.reportLimit,",
            "                is_review_status_change_disabled=is_rws_change_disabled,",
            "                confidentiality=confidentiality)",
            "",
            "            LOG.debug(\"Attempting database connection to new product...\")",
            "",
            "            # Connect and create the database schema.",
            "            self.__server.add_product(orm_prod, init_db=True)",
            "            connection_wrapper = self.__server.get_product(product.endpoint)",
            "            if connection_wrapper.last_connection_failure:",
            "                msg = \\",
            "                    f\"The configured connection for '/{product.endpoint}' \" \\",
            "                    f\"failed: {connection_wrapper.last_connection_failure}\"",
            "                LOG.error(msg)",
            "",
            "                self.__server.remove_product(product.endpoint)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.IOERROR, msg)",
            "",
            "            LOG.debug(\"Product database successfully connected to.\")",
            "            session.add(orm_prod)",
            "            session.flush()",
            "",
            "            # Create the default permissions for the product",
            "            permissions.initialise_defaults('PRODUCT', {",
            "                'config_db_session': session,",
            "                'productID': orm_prod.id",
            "            })",
            "            session.commit()",
            "            LOG.debug(\"Product configuration added to database successfully.\")",
            "",
            "            # The orm_prod object above is not bound to the database as it",
            "            # was just created. We use the actual database-backed configuration",
            "            # entry to handle connections, so a \"reconnect\" is issued here.",
            "            self.__server.remove_product(product.endpoint)",
            "",
            "            orm_prod = session.query(Product) \\",
            "                .filter(Product.endpoint == product.endpoint).one()",
            "            self.__server.add_product(orm_prod)",
            "",
            "            LOG.debug(\"Product database connected and ready to serve.\")",
            "            return True",
            "",
            "    @timeit",
            "    def editProduct(self, product_id, new_config):",
            "        \"\"\"",
            "        Edit the given product's properties to the one specified by",
            "        new_configuration.",
            "        \"\"\"",
            "        with DBSession(self.__session) as session:",
            "            product = session.query(Product).get(product_id)",
            "            if product is None:",
            "                msg = f\"Product with ID {product_id} does not exist!\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            # Editing the metadata of the product, such as display name and",
            "            # description is available for product admins.",
            "",
            "            # Because this query doesn't come through a product endpoint,",
            "            # __init__ sets the value in the extra_args to None.",
            "            self.__permission_args['productID'] = product.id",
            "            self.__require_permission([permissions.PRODUCT_ADMIN])",
            "",
            "            LOG.info(\"User requested edit product '%s'\",",
            "                     product.endpoint)",
            "",
            "            dbc = new_config.connection",
            "            if not dbc:",
            "                msg = \"Product's database configuration cannot be removed!\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "            if new_config.endpoint != product.endpoint:",
            "                if not is_valid_product_endpoint(new_config.endpoint):",
            "                    msg = \"The endpoint to move the product to is invalid.\"",
            "                    LOG.error(msg)",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "                if self.__server.get_product(new_config.endpoint):",
            "                    msg = f\"A product endpoint '/{product.endpoint}' is \" \\",
            "                          f\"already configured!\"",
            "                    LOG.error(msg)",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "                LOG.info(\"User renamed product '%s' to '%s'\",",
            "                         product.endpoint, new_config.endpoint)",
            "",
            "            # Some values come encoded as Base64, decode these.",
            "            displayed_name = convert.from_b64(new_config.displayedName_b64) \\",
            "                if new_config.displayedName_b64 \\",
            "                else new_config.endpoint",
            "            description = convert.from_b64(new_config.description_b64) \\",
            "                if new_config.description_b64 else None",
            "",
            "            confidentiality = confidentiality_str(new_config.confidentiality)",
            "",
            "            if dbc.engine == 'sqlite' and not os.path.isabs(dbc.database):",
            "                # Transform the database relative path to be under the",
            "                # server's config directory.",
            "                dbc.database = os.path.join(self.__server.config_directory,",
            "                                            dbc.database)",
            "",
            "            # Transform arguments into a database connection string.",
            "            if dbc.engine == 'postgresql':",
            "                dbuser = \"codechecker\"",
            "                if dbc.username_b64 and dbc.username_b64 != '':",
            "                    dbuser = convert.from_b64(dbc.username_b64)",
            "",
            "                old_connection_args = SQLServer.connection_string_to_args(",
            "                        product.connection)",
            "                if dbc.password_b64 and dbc.password_b64 != '':",
            "                    dbpass = convert.from_b64(dbc.password_b64)",
            "                elif 'dbpassword' in old_connection_args:",
            "                    # The password was not changed. Retrieving from old",
            "                    # configuration -- if the old configuration contained such!",
            "                    dbpass = old_connection_args['dbpassword']",
            "                else:",
            "                    dbpass = None",
            "",
            "                conn_str_args = {'postgresql': True,",
            "                                 'sqlite': False,",
            "                                 'dbaddress': dbc.host,",
            "                                 'dbport': dbc.port,",
            "                                 'dbusername': dbuser,",
            "                                 'dbpassword': dbpass,",
            "                                 'dbname': dbc.database}",
            "            elif dbc.engine == 'sqlite':",
            "                conn_str_args = {'postgresql': False,",
            "                                 'sqlite': dbc.database}",
            "            else:",
            "                msg = f\"Database engine '{dbc.engine}' unknown!\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                    msg)",
            "",
            "            conn_str = SQLServer \\",
            "                .from_cmdline_args(conn_str_args, product.endpoint,",
            "                                   IDENTIFIER, None, False, None) \\",
            "                .get_connection_string()",
            "",
            "            # If endpoint or database arguments change, the product",
            "            # configuration has changed so severely, that it needs",
            "            # to be reconnected.",
            "            product_needs_reconnect = \\",
            "                product.endpoint != new_config.endpoint or \\",
            "                product.connection != conn_str",
            "            old_endpoint = product.endpoint",
            "",
            "            if product_needs_reconnect:",
            "                # Changing values that alter connection-specific data",
            "                # should only be available for superusers!",
            "                self.__require_permission([permissions.SUPERUSER])",
            "",
            "                # Test if the new database settings are correct or not.",
            "                dummy_endpoint = new_config.endpoint + '_' + ''.join(",
            "                    random.sample(new_config.endpoint,",
            "                                  min(len(new_config.endpoint), 5)))",
            "                dummy_prod = Product(",
            "                    endpoint=dummy_endpoint,",
            "                    conn_str=conn_str,",
            "                    name=displayed_name,",
            "                    description=description)",
            "",
            "                LOG.debug(\"Attempting database connection with new \"",
            "                          \"settings...\")",
            "",
            "                # Connect and create the database schema.",
            "                self.__server.add_product(dummy_prod)",
            "                LOG.debug(\"Product database successfully connected to.\")",
            "",
            "                connection_wrapper = self.__server.get_product(dummy_endpoint)",
            "                if connection_wrapper.last_connection_failure:",
            "                    msg = \\",
            "                        f\"The configured connection for \" \\",
            "                        f\"'/{new_config.endpoint}' failed: \" \\",
            "                        f\"{connection_wrapper.last_connection_failure}\"",
            "                    LOG.error(msg)",
            "",
            "                    self.__server.remove_product(dummy_endpoint)",
            "",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.IOERROR, msg)",
            "",
            "                # The orm_prod object above is not bound to the database as it",
            "                # was just created. We use the actual database-backed",
            "                # configuration entry to handle connections, so a \"reconnect\"",
            "                # is issued later.",
            "                self.__server.remove_product(dummy_endpoint)",
            "",
            "            # Update the settings in the database.",
            "            product.endpoint = new_config.endpoint",
            "            product.run_limit = new_config.runLimit",
            "            product.report_limit = new_config.reportLimit",
            "            product.is_review_status_change_disabled = \\",
            "                new_config.isReviewStatusChangeDisabled",
            "            product.connection = conn_str",
            "            product.display_name = displayed_name",
            "            product.description = description",
            "            product.confidentiality = confidentiality",
            "",
            "            session.commit()",
            "            LOG.info(\"Product configuration edited and saved successfully.\")",
            "",
            "            if product_needs_reconnect:",
            "                product = session.query(Product).get(product_id)",
            "                LOG.info(\"Product change requires database reconnection...\")",
            "",
            "                LOG.debug(\"Disconnecting...\")",
            "                try:",
            "                    # Because of the process pool it is possible that in the",
            "                    # local cache of the current process the product with the",
            "                    # old endpoint is not found and it will raise an exception.",
            "                    self.__server.remove_product(old_endpoint)",
            "                except ValueError:",
            "                    pass",
            "",
            "                LOG.debug(\"Connecting new settings...\")",
            "                self.__server.add_product(product)",
            "",
            "                LOG.info(\"Product reconnected successfully.\")",
            "",
            "            return True",
            "",
            "    @timeit",
            "    def removeProduct(self, product_id):",
            "        \"\"\"",
            "        Disconnect the product specified by the ID from the server.",
            "        \"\"\"",
            "        self.__require_permission([permissions.SUPERUSER])",
            "",
            "        with DBSession(self.__session) as session:",
            "            product = session.query(Product).get(product_id)",
            "            if product is None:",
            "                msg = f\"Product with ID {product_id} does not exist!\"",
            "                LOG.error(msg)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            LOG.info(\"User requested to remove product '%s'\", product.endpoint)",
            "            self.__server.remove_product(product.endpoint)",
            "",
            "            session.delete(product)",
            "            session.commit()",
            "            return True"
        ],
        "afterPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Handle Thrift requests for the product manager service.",
            "\"\"\"",
            "",
            "",
            "import os",
            "import random",
            "",
            "from sqlalchemy.sql.expression import and_",
            "",
            "import codechecker_api_shared",
            "from codechecker_api.ProductManagement_v6 import ttypes",
            "",
            "from codechecker_common.logger import get_logger",
            "",
            "from codechecker_server.profiler import timeit",
            "from codechecker_web.shared import convert",
            "",
            "from .. import permissions",
            "from ..database.config_db_model import IDENTIFIER, Product, ProductPermission",
            "from ..database.database import DBSession, SQLServer, conv, escape_like",
            "from ..routing import is_valid_product_endpoint",
            "",
            "from .thrift_enum_helper import confidentiality_enum, \\",
            "        confidentiality_str",
            "",
            "LOG = get_logger('server')",
            "",
            "",
            "# These names are inherited from Thrift stubs.",
            "# pylint: disable=invalid-name",
            "class ThriftProductHandler:",
            "    \"\"\"",
            "    Connect to database and handle thrift client requests.",
            "    \"\"\"",
            "",
            "    def __init__(self,",
            "                 server,",
            "                 auth_session,",
            "                 config_session,",
            "                 routed_product,",
            "                 package_version):",
            "",
            "        self.__server = server",
            "        self.__auth_session = auth_session",
            "        self.__package_version = package_version",
            "        self.__session = config_session",
            "        self.__product = routed_product",
            "",
            "        self.__permission_args = {",
            "            'productID': routed_product.id if routed_product else None",
            "        }",
            "",
            "    def __require_permission(self, required, args=None):",
            "        \"\"\"",
            "        Helper method to raise an UNAUTHORIZED exception if the user does not",
            "        have any of the given permissions.",
            "        \"\"\"",
            "",
            "        with DBSession(self.__session) as session:",
            "            if args is None:",
            "                args = dict(self.__permission_args)",
            "                args['config_db_session'] = session",
            "",
            "            # Anonymous access is only allowed if authentication is",
            "            # turned off",
            "            if self.__server.manager.is_enabled and not self.__auth_session:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                    \"You are not authorized to execute this action.\")",
            "",
            "            if not any(permissions.require_permission(",
            "                           perm, args, self.__auth_session)",
            "                       for perm in required):",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                    \"You are not authorized to execute this action.\")",
            "",
            "            return True",
            "",
            "    def __administrating(self, args):",
            "        \"\"\" True if the current user can administrate the given product. \"\"\"",
            "        if permissions.require_permission(permissions.SUPERUSER, args,",
            "                                          self.__auth_session):",
            "            return True",
            "",
            "        if permissions.require_permission(permissions.PRODUCT_ADMIN, args,",
            "                                          self.__auth_session):",
            "            return True",
            "",
            "        return False",
            "",
            "    def __get_product(self, session, product):",
            "        \"\"\"",
            "        Retrieve the product connection object and create a Thrift Product",
            "        object for the given product record in the database.",
            "        \"\"\"",
            "",
            "        server_product = self.__server.get_product(product.endpoint)",
            "        if not server_product:",
            "            LOG.info(\"Product '%s' was found in the configuration \"",
            "                     \"database but no database connection was \"",
            "                     \"present. Mounting analysis run database...\",",
            "                     product.endpoint)",
            "            self.__server.add_product(product)",
            "            server_product = self.__server.get_product(product.endpoint)",
            "",
            "        descr = convert.to_b64(product.description) \\",
            "            if product.description else None",
            "",
            "        args = {'config_db_session': session,",
            "                'productID': product.id}",
            "        product_access = permissions.require_permission(",
            "            permissions.PRODUCT_VIEW, args, self.__auth_session)",
            "",
            "        admin_perm_name = permissions.PRODUCT_ADMIN.name",
            "        admins = session.query(ProductPermission). \\",
            "            filter(and_(ProductPermission.permission == admin_perm_name,",
            "                        ProductPermission.product_id == product.id)) \\",
            "            .all()",
            "",
            "        connected = server_product.db_status ==\\",
            "            codechecker_api_shared.ttypes.DBStatus.OK",
            "",
            "        latest_storage_date = str(product.latest_storage_date) \\",
            "            if product.latest_storage_date else None",
            "",
            "        if product.confidentiality is None:",
            "            confidentiality = ttypes.Confidentiality.CONFIDENTIAL",
            "        else:",
            "            confidentiality = \\",
            "                    confidentiality_enum(product.confidentiality)",
            "",
            "        report_limit = product.report_limit",
            "",
            "        return server_product, ttypes.Product(",
            "            id=product.id,",
            "            endpoint=product.endpoint,",
            "            displayedName_b64=convert.to_b64(product.display_name),",
            "            description_b64=descr,",
            "            runCount=product.num_of_runs,",
            "            latestStoreToProduct=latest_storage_date,",
            "            connected=connected,",
            "            accessible=product_access,",
            "            administrating=self.__administrating(args),",
            "            databaseStatus=server_product.db_status,",
            "            admins=[admin.name for admin in admins],",
            "            confidentiality=confidentiality,",
            "            reportLimit=report_limit)",
            "",
            "    @timeit",
            "    def getPackageVersion(self):",
            "        return self.__package_version",
            "",
            "    @timeit",
            "    def isAdministratorOfAnyProduct(self):",
            "        with DBSession(self.__session) as session:",
            "            prods = session.query(Product).all()",
            "",
            "            for prod in prods:",
            "                args = {'config_db_session': session,",
            "                        'productID': prod.id}",
            "                if permissions.require_permission(",
            "                        permissions.PRODUCT_ADMIN,",
            "                        args, self.__auth_session):",
            "                    return True",
            "",
            "            return False",
            "",
            "    @timeit",
            "    def getProducts(self, product_endpoint_filter, product_name_filter):",
            "        \"\"\"",
            "        Get the list of products configured on the server.",
            "        \"\"\"",
            "",
            "        result = []",
            "",
            "        with DBSession(self.__session) as session:",
            "            prods = session.query(Product)",
            "",
            "            num_all_products = prods.count()  # prods get filtered later.",
            "            if num_all_products < self.__server.num_products:",
            "                # It can happen that a product gets removed from the",
            "                # configuration database from a different server that uses the",
            "                # same configuration database. In this case, the product is",
            "                # no longer valid, yet the current server keeps a connection",
            "                # object up.",
            "                LOG.info(\"%d products were removed but server is still \"",
            "                         \"connected to them. Disconnecting these...\",",
            "                         self.__server.num_products - num_all_products)",
            "",
            "                all_products = session.query(Product).all()",
            "                self.__server.remove_products_except([prod.endpoint for prod",
            "                                                      in all_products])",
            "",
            "            if product_endpoint_filter:",
            "                prods = prods.filter(Product.endpoint.ilike(",
            "                    conv(escape_like(product_endpoint_filter, '\\\\')),",
            "                    escape='\\\\'))",
            "",
            "            if product_name_filter:",
            "                prods = prods.filter(Product.display_name.ilike(",
            "                    conv(escape_like(product_name_filter, '\\\\')),",
            "                    escape='\\\\'))",
            "",
            "            prods = prods.all()",
            "            for prod in prods:",
            "                _, ret = self.__get_product(session, prod)",
            "                result.append(ret)",
            "",
            "            return result",
            "",
            "    @timeit",
            "    def getCurrentProduct(self):",
            "        \"\"\"",
            "        Return information about the current product.",
            "",
            "        The request MUST be routed as /product-name/ProductService!",
            "        \"\"\"",
            "",
            "        if not self.__product:",
            "            msg = \"Requested current product from ProductService but the \" \\",
            "                  \"request came through the main endpoint.\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.IOERROR,",
            "                msg)",
            "",
            "        with DBSession(self.__session) as session:",
            "            prod = session.query(Product).get(self.__product.id)",
            "",
            "            if not prod:",
            "                msg = \"The product requested has been disconnected from the \" \\",
            "                      \"server.\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.IOERROR,",
            "                    msg)",
            "",
            "            _, ret = self.__get_product(session, prod)",
            "            LOG.debug(ret)",
            "            return ret",
            "",
            "    @timeit",
            "    def getProductConfiguration(self, product_id):",
            "        \"\"\"",
            "        Get the product configuration --- WITHOUT THE DB PASSWORD --- of the",
            "        given product.",
            "        \"\"\"",
            "        self.__require_permission([permissions.PRODUCT_VIEW])",
            "",
            "        with DBSession(self.__session) as session:",
            "            product = session.query(Product).get(product_id)",
            "            if product is None:",
            "                msg = f\"Product with ID {product_id} does not exist!\"",
            "                LOG.error(msg)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            # Put together the database connection's descriptor.",
            "            args = SQLServer.connection_string_to_args(product.connection)",
            "            if args['postgresql']:",
            "                db_engine = 'postgresql'",
            "                db_host = args['dbaddress']",
            "                db_port = args['dbport']",
            "                db_user = args['dbusername']",
            "                db_name = args['dbname']",
            "            else:",
            "                db_engine = 'sqlite'",
            "                db_host = \"\"",
            "                db_port = 0",
            "                db_user = \"\"",
            "                db_name = args['sqlite']",
            "",
            "            dbc = ttypes.DatabaseConnection(",
            "                engine=db_engine,",
            "                host=db_host,",
            "                port=db_port,",
            "                username_b64=convert.to_b64(db_user),",
            "                # DO NOT TRANSPORT PASSWORD BACK TO THE USER!",
            "                database=db_name)",
            "",
            "            # Put together the product configuration.",
            "            descr = convert.to_b64(product.description) \\",
            "                if product.description else None",
            "",
            "            is_review_status_change_disabled = \\",
            "                product.is_review_status_change_disabled",
            "",
            "            if product.confidentiality is None:",
            "                confidentiality = ttypes.Confidentiality.CONFIDENTIAL",
            "            else:",
            "                confidentiality = \\",
            "                        confidentiality_enum(product.confidentiality)",
            "",
            "            prod = ttypes.ProductConfiguration(",
            "                id=product.id,",
            "                endpoint=product.endpoint,",
            "                displayedName_b64=convert.to_b64(product.display_name),",
            "                description_b64=descr,",
            "                connection=dbc,",
            "                runLimit=product.run_limit,",
            "                reportLimit=product.report_limit,",
            "                isReviewStatusChangeDisabled=is_review_status_change_disabled,",
            "                confidentiality=confidentiality)",
            "",
            "            return prod",
            "",
            "    @timeit",
            "    def addProduct(self, product):",
            "        \"\"\"",
            "        Add the given product to the products configured by the server.",
            "        \"\"\"",
            "        self.__require_permission([permissions.SUPERUSER])",
            "",
            "        session = None",
            "        LOG.info(\"User requested add product '%s'\", product.endpoint)",
            "",
            "        if not is_valid_product_endpoint(product.endpoint):",
            "            msg = \"The specified endpoint is invalid.\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        dbc = product.connection",
            "        if not dbc:",
            "            msg = \"Product cannot be added without a database configuration!\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        if self.__server.get_product(product.endpoint):",
            "            msg = \\",
            "                f\"A product endpoint '/{product.endpoint}' is already \" \\",
            "                \"configured!\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        # Some values come encoded as Base64, decode these.",
            "        displayed_name = convert.from_b64(product.displayedName_b64) \\",
            "            if product.displayedName_b64 else product.endpoint",
            "        description = convert.from_b64(product.description_b64) \\",
            "            if product.description_b64 else None",
            "",
            "        if dbc.engine == 'sqlite' and not os.path.isabs(dbc.database):",
            "            # Transform the database relative path to be under the",
            "            # server's config directory.",
            "            dbc.database = os.path.join(self.__server.config_directory,",
            "                                        dbc.database)",
            "",
            "        # Transform arguments into a database connection string.",
            "        if dbc.engine == 'postgresql':",
            "            dbuser = \"codechecker\"",
            "            dbpass = \"\"",
            "            if dbc.username_b64 and dbc.username_b64 != '':",
            "                dbuser = convert.from_b64(dbc.username_b64)",
            "            if dbc.password_b64 and dbc.password_b64 != '':",
            "                dbpass = convert.from_b64(dbc.password_b64)",
            "",
            "            conn_str_args = {'postgresql': True,",
            "                             'sqlite': False,",
            "                             'dbaddress': dbc.host,",
            "                             'dbport': dbc.port,",
            "                             'dbusername': dbuser,",
            "                             'dbpassword': dbpass,",
            "                             'dbname': dbc.database}",
            "        elif dbc.engine == 'sqlite':",
            "            conn_str_args = {'postgresql': False,",
            "                             'sqlite': dbc.database}",
            "        else:",
            "            msg = f\"Database engine '{dbc.engine}' unknown!\"",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                msg)",
            "",
            "        conn_str = SQLServer \\",
            "            .from_cmdline_args(conn_str_args, product.endpoint, IDENTIFIER,",
            "                               None, False, None) \\",
            "            .get_connection_string()",
            "",
            "        is_rws_change_disabled = product.isReviewStatusChangeDisabled",
            "",
            "        confidentiality = confidentiality_str(product.confidentiality)",
            "",
            "        # Create the product's entity in the database.",
            "        with DBSession(self.__session) as session:",
            "            orm_prod = Product(",
            "                endpoint=product.endpoint,",
            "                conn_str=conn_str,",
            "                name=displayed_name,",
            "                description=description,",
            "                run_limit=product.runLimit,",
            "                report_limit=product.reportLimit,",
            "                is_review_status_change_disabled=is_rws_change_disabled,",
            "                confidentiality=confidentiality)",
            "",
            "            LOG.debug(\"Attempting database connection to new product...\")",
            "",
            "            # Connect and create the database schema.",
            "            self.__server.add_product(orm_prod, init_db=True)",
            "            connection_wrapper = self.__server.get_product(product.endpoint)",
            "            if connection_wrapper.last_connection_failure:",
            "                msg = \\",
            "                    f\"The configured connection for '/{product.endpoint}' \" \\",
            "                    f\"failed: {connection_wrapper.last_connection_failure}\"",
            "                LOG.error(msg)",
            "",
            "                self.__server.remove_product(product.endpoint)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.IOERROR, msg)",
            "",
            "            LOG.debug(\"Product database successfully connected to.\")",
            "            session.add(orm_prod)",
            "            session.flush()",
            "",
            "            # Create the default permissions for the product",
            "            permissions.initialise_defaults('PRODUCT', {",
            "                'config_db_session': session,",
            "                'productID': orm_prod.id",
            "            })",
            "            session.commit()",
            "            LOG.debug(\"Product configuration added to database successfully.\")",
            "",
            "            # The orm_prod object above is not bound to the database as it",
            "            # was just created. We use the actual database-backed configuration",
            "            # entry to handle connections, so a \"reconnect\" is issued here.",
            "            self.__server.remove_product(product.endpoint)",
            "",
            "            orm_prod = session.query(Product) \\",
            "                .filter(Product.endpoint == product.endpoint).one()",
            "            self.__server.add_product(orm_prod)",
            "",
            "            LOG.debug(\"Product database connected and ready to serve.\")",
            "            return True",
            "",
            "    @timeit",
            "    def editProduct(self, product_id, new_config):",
            "        \"\"\"",
            "        Edit the given product's properties to the one specified by",
            "        new_configuration.",
            "        \"\"\"",
            "        with DBSession(self.__session) as session:",
            "            product = session.query(Product).get(product_id)",
            "            if product is None:",
            "                msg = f\"Product with ID {product_id} does not exist!\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            # Editing the metadata of the product, such as display name and",
            "            # description is available for product admins.",
            "",
            "            # Because this query doesn't come through a product endpoint,",
            "            # __init__ sets the value in the extra_args to None.",
            "            self.__permission_args['productID'] = product.id",
            "            self.__require_permission([permissions.PRODUCT_ADMIN])",
            "",
            "            LOG.info(\"User requested edit product '%s'\",",
            "                     product.endpoint)",
            "",
            "            dbc = new_config.connection",
            "            if not dbc:",
            "                msg = \"Product's database configuration cannot be removed!\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "            if new_config.endpoint != product.endpoint:",
            "                if not is_valid_product_endpoint(new_config.endpoint):",
            "                    msg = \"The endpoint to move the product to is invalid.\"",
            "                    LOG.error(msg)",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "                if self.__server.get_product(new_config.endpoint):",
            "                    msg = f\"A product endpoint '/{product.endpoint}' is \" \\",
            "                          f\"already configured!\"",
            "                    LOG.error(msg)",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "                LOG.info(\"User renamed product '%s' to '%s'\",",
            "                         product.endpoint, new_config.endpoint)",
            "",
            "            # Some values come encoded as Base64, decode these.",
            "            displayed_name = convert.from_b64(new_config.displayedName_b64) \\",
            "                if new_config.displayedName_b64 \\",
            "                else new_config.endpoint",
            "            description = convert.from_b64(new_config.description_b64) \\",
            "                if new_config.description_b64 else None",
            "",
            "            confidentiality = confidentiality_str(new_config.confidentiality)",
            "",
            "            if dbc.engine == 'sqlite' and not os.path.isabs(dbc.database):",
            "                # Transform the database relative path to be under the",
            "                # server's config directory.",
            "                dbc.database = os.path.join(self.__server.config_directory,",
            "                                            dbc.database)",
            "",
            "            # Transform arguments into a database connection string.",
            "            if dbc.engine == 'postgresql':",
            "                dbuser = \"codechecker\"",
            "                if dbc.username_b64 and dbc.username_b64 != '':",
            "                    dbuser = convert.from_b64(dbc.username_b64)",
            "",
            "                old_connection_args = SQLServer.connection_string_to_args(",
            "                        product.connection)",
            "                if dbc.password_b64 and dbc.password_b64 != '':",
            "                    dbpass = convert.from_b64(dbc.password_b64)",
            "                elif 'dbpassword' in old_connection_args:",
            "                    # The password was not changed. Retrieving from old",
            "                    # configuration -- if the old configuration contained such!",
            "                    dbpass = old_connection_args['dbpassword']",
            "                else:",
            "                    dbpass = None",
            "",
            "                conn_str_args = {'postgresql': True,",
            "                                 'sqlite': False,",
            "                                 'dbaddress': dbc.host,",
            "                                 'dbport': dbc.port,",
            "                                 'dbusername': dbuser,",
            "                                 'dbpassword': dbpass,",
            "                                 'dbname': dbc.database}",
            "            elif dbc.engine == 'sqlite':",
            "                conn_str_args = {'postgresql': False,",
            "                                 'sqlite': dbc.database}",
            "            else:",
            "                msg = f\"Database engine '{dbc.engine}' unknown!\"",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                    msg)",
            "",
            "            conn_str = SQLServer \\",
            "                .from_cmdline_args(conn_str_args, product.endpoint,",
            "                                   IDENTIFIER, None, False, None) \\",
            "                .get_connection_string()",
            "",
            "            # If endpoint or database arguments change, the product",
            "            # configuration has changed so severely, that it needs",
            "            # to be reconnected.",
            "            product_needs_reconnect = \\",
            "                product.endpoint != new_config.endpoint or \\",
            "                product.connection != conn_str",
            "            old_endpoint = product.endpoint",
            "",
            "            if product_needs_reconnect:",
            "                # Changing values that alter connection-specific data",
            "                # should only be available for superusers!",
            "                self.__require_permission([permissions.SUPERUSER])",
            "",
            "                # Test if the new database settings are correct or not.",
            "                dummy_endpoint = new_config.endpoint + '_' + ''.join(",
            "                    random.sample(new_config.endpoint,",
            "                                  min(len(new_config.endpoint), 5)))",
            "                dummy_prod = Product(",
            "                    endpoint=dummy_endpoint,",
            "                    conn_str=conn_str,",
            "                    name=displayed_name,",
            "                    description=description)",
            "",
            "                LOG.debug(\"Attempting database connection with new \"",
            "                          \"settings...\")",
            "",
            "                # Connect and create the database schema.",
            "                self.__server.add_product(dummy_prod)",
            "                LOG.debug(\"Product database successfully connected to.\")",
            "",
            "                connection_wrapper = self.__server.get_product(dummy_endpoint)",
            "                if connection_wrapper.last_connection_failure:",
            "                    msg = \\",
            "                        f\"The configured connection for \" \\",
            "                        f\"'/{new_config.endpoint}' failed: \" \\",
            "                        f\"{connection_wrapper.last_connection_failure}\"",
            "                    LOG.error(msg)",
            "",
            "                    self.__server.remove_product(dummy_endpoint)",
            "",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.IOERROR, msg)",
            "",
            "                # The orm_prod object above is not bound to the database as it",
            "                # was just created. We use the actual database-backed",
            "                # configuration entry to handle connections, so a \"reconnect\"",
            "                # is issued later.",
            "                self.__server.remove_product(dummy_endpoint)",
            "",
            "            # Update the settings in the database.",
            "            product.endpoint = new_config.endpoint",
            "            product.run_limit = new_config.runLimit",
            "            product.report_limit = new_config.reportLimit",
            "            product.is_review_status_change_disabled = \\",
            "                new_config.isReviewStatusChangeDisabled",
            "            product.connection = conn_str",
            "            product.display_name = displayed_name",
            "            product.description = description",
            "            product.confidentiality = confidentiality",
            "",
            "            session.commit()",
            "            LOG.info(\"Product configuration edited and saved successfully.\")",
            "",
            "            if product_needs_reconnect:",
            "                product = session.query(Product).get(product_id)",
            "                LOG.info(\"Product change requires database reconnection...\")",
            "",
            "                LOG.debug(\"Disconnecting...\")",
            "                try:",
            "                    # Because of the process pool it is possible that in the",
            "                    # local cache of the current process the product with the",
            "                    # old endpoint is not found and it will raise an exception.",
            "                    self.__server.remove_product(old_endpoint)",
            "                except ValueError:",
            "                    pass",
            "",
            "                LOG.debug(\"Connecting new settings...\")",
            "                self.__server.add_product(product)",
            "",
            "                LOG.info(\"Product reconnected successfully.\")",
            "",
            "            return True",
            "",
            "    @timeit",
            "    def removeProduct(self, product_id):",
            "        \"\"\"",
            "        Disconnect the product specified by the ID from the server.",
            "        \"\"\"",
            "        self.__require_permission([permissions.SUPERUSER])",
            "",
            "        with DBSession(self.__session) as session:",
            "            product = session.query(Product).get(product_id)",
            "            if product is None:",
            "                msg = f\"Product with ID {product_id} does not exist!\"",
            "                LOG.error(msg)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            LOG.info(\"User requested to remove product '%s'\", product.endpoint)",
            "            self.__server.remove_product(product.endpoint)",
            "",
            "            session.delete(product)",
            "            session.commit()",
            "            return True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "web.server.codechecker_server.api.product_server.ThriftProductHandler.__require_permission.required",
            "web.server.codechecker_server.api.product_server.ThriftProductHandler.editProduct",
            "web.server.codechecker_server.api.product_server.ThriftProductHandler.addProduct",
            "web.server.codechecker_server.api.product_server.ThriftProductHandler.getProductConfiguration",
            "copyparty.httpsrv",
            "web.server.codechecker_server.api.product_server.ThriftProductHandler.removeProduct"
        ]
    },
    "web/server/codechecker_server/api/report_server.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1447,
                "afterPatchRowNumber": 1447,
                "PatchRowcode": "             args = dict(self.__permission_args)"
            },
            "1": {
                "beforePatchRowNumber": 1448,
                "afterPatchRowNumber": 1448,
                "PatchRowcode": "             args['config_db_session'] = session"
            },
            "2": {
                "beforePatchRowNumber": 1449,
                "afterPatchRowNumber": 1449,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1450,
                "PatchRowcode": "+            # Anonymous access is only allowed if authentication is"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1451,
                "PatchRowcode": "+            # turned off"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1452,
                "PatchRowcode": "+            if self._manager.is_enabled and not self._auth_session:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1453,
                "PatchRowcode": "+                raise codechecker_api_shared.ttypes.RequestFailed("
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1454,
                "PatchRowcode": "+                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1455,
                "PatchRowcode": "+                    \"You are not authorized to execute this action.\")"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1456,
                "PatchRowcode": "+"
            },
            "10": {
                "beforePatchRowNumber": 1450,
                "afterPatchRowNumber": 1457,
                "PatchRowcode": "             if not any(permissions.require_permission("
            },
            "11": {
                "beforePatchRowNumber": 1451,
                "afterPatchRowNumber": 1458,
                "PatchRowcode": "                     perm, args, self._auth_session)"
            },
            "12": {
                "beforePatchRowNumber": 1452,
                "afterPatchRowNumber": 1459,
                "PatchRowcode": "                     for perm in required):"
            },
            "13": {
                "beforePatchRowNumber": 2320,
                "afterPatchRowNumber": 2327,
                "PatchRowcode": "         database transaction. This is needed because during storage a specific"
            },
            "14": {
                "beforePatchRowNumber": 2321,
                "afterPatchRowNumber": 2328,
                "PatchRowcode": "         session object has to be used."
            },
            "15": {
                "beforePatchRowNumber": 2322,
                "afterPatchRowNumber": 2329,
                "PatchRowcode": "         \"\"\""
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2330,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": 2323,
                "afterPatchRowNumber": 2331,
                "PatchRowcode": "         review_status = session.query(ReviewStatus).get(report_hash)"
            },
            "18": {
                "beforePatchRowNumber": 2324,
                "afterPatchRowNumber": 2332,
                "PatchRowcode": "         if review_status is None:"
            },
            "19": {
                "beforePatchRowNumber": 2325,
                "afterPatchRowNumber": 2333,
                "PatchRowcode": "             review_status = ReviewStatus()"
            },
            "20": {
                "beforePatchRowNumber": 2421,
                "afterPatchRowNumber": 2429,
                "PatchRowcode": "         \"\"\""
            },
            "21": {
                "beforePatchRowNumber": 2422,
                "afterPatchRowNumber": 2430,
                "PatchRowcode": "         Return True if review status change is disabled."
            },
            "22": {
                "beforePatchRowNumber": 2423,
                "afterPatchRowNumber": 2431,
                "PatchRowcode": "         \"\"\""
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2432,
                "PatchRowcode": "+        self.__require_view()"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2433,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": 2424,
                "afterPatchRowNumber": 2434,
                "PatchRowcode": "         with DBSession(self._config_database) as session:"
            },
            "26": {
                "beforePatchRowNumber": 2425,
                "afterPatchRowNumber": 2435,
                "PatchRowcode": "             product = session.query(Product).get(self._product.id)"
            },
            "27": {
                "beforePatchRowNumber": 2426,
                "afterPatchRowNumber": 2436,
                "PatchRowcode": "             return product.is_review_status_change_disabled"
            },
            "28": {
                "beforePatchRowNumber": 2746,
                "afterPatchRowNumber": 2756,
                "PatchRowcode": "         Parameters:"
            },
            "29": {
                "beforePatchRowNumber": 2747,
                "afterPatchRowNumber": 2757,
                "PatchRowcode": "          - checkerId"
            },
            "30": {
                "beforePatchRowNumber": 2748,
                "afterPatchRowNumber": 2758,
                "PatchRowcode": "         \"\"\""
            },
            "31": {
                "beforePatchRowNumber": 2749,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2759,
                "PatchRowcode": "+        self.__require_view()"
            },
            "33": {
                "beforePatchRowNumber": 2750,
                "afterPatchRowNumber": 2760,
                "PatchRowcode": "         return \"\""
            },
            "34": {
                "beforePatchRowNumber": 2751,
                "afterPatchRowNumber": 2761,
                "PatchRowcode": " "
            },
            "35": {
                "beforePatchRowNumber": 2752,
                "afterPatchRowNumber": 2762,
                "PatchRowcode": "     @exc_to_thrift_reqfail"
            },
            "36": {
                "beforePatchRowNumber": 2756,
                "afterPatchRowNumber": 2766,
                "PatchRowcode": "         checkers: List[ttypes.Checker]"
            },
            "37": {
                "beforePatchRowNumber": 2757,
                "afterPatchRowNumber": 2767,
                "PatchRowcode": "     ) -> List[List[str]]:"
            },
            "38": {
                "beforePatchRowNumber": 2758,
                "afterPatchRowNumber": 2768,
                "PatchRowcode": "         \"\"\" Return the list of labels to each checker. \"\"\""
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2769,
                "PatchRowcode": "+        self.__require_view()"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2770,
                "PatchRowcode": "+"
            },
            "41": {
                "beforePatchRowNumber": 2759,
                "afterPatchRowNumber": 2771,
                "PatchRowcode": "         labels = []"
            },
            "42": {
                "beforePatchRowNumber": 2760,
                "afterPatchRowNumber": 2772,
                "PatchRowcode": "         for checker in checkers:"
            },
            "43": {
                "beforePatchRowNumber": 2761,
                "afterPatchRowNumber": 2773,
                "PatchRowcode": "             analyzer_name = None if not checker.analyzerName \\"
            },
            "44": {
                "beforePatchRowNumber": 3569,
                "afterPatchRowNumber": 3581,
                "PatchRowcode": "         given run. If the run id list is empty the number of failed files will"
            },
            "45": {
                "beforePatchRowNumber": 3570,
                "afterPatchRowNumber": 3582,
                "PatchRowcode": "         be counted for all of the runs."
            },
            "46": {
                "beforePatchRowNumber": 3571,
                "afterPatchRowNumber": 3583,
                "PatchRowcode": "         \"\"\""
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3584,
                "PatchRowcode": "+        self.__require_view()"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3585,
                "PatchRowcode": "+"
            },
            "49": {
                "beforePatchRowNumber": 3572,
                "afterPatchRowNumber": 3586,
                "PatchRowcode": "         # Unfortunately we can't distinct the failed file paths by using SQL"
            },
            "50": {
                "beforePatchRowNumber": 3573,
                "afterPatchRowNumber": 3587,
                "PatchRowcode": "         # queries because the list of failed files for a run / analyzer are"
            },
            "51": {
                "beforePatchRowNumber": 3574,
                "afterPatchRowNumber": 3588,
                "PatchRowcode": "         # stored in one column in a compressed way. For this reason we need to"
            },
            "52": {
                "beforePatchRowNumber": 3611,
                "afterPatchRowNumber": 3625,
                "PatchRowcode": "     # -----------------------------------------------------------------------"
            },
            "53": {
                "beforePatchRowNumber": 3612,
                "afterPatchRowNumber": 3626,
                "PatchRowcode": "     @timeit"
            },
            "54": {
                "beforePatchRowNumber": 3613,
                "afterPatchRowNumber": 3627,
                "PatchRowcode": "     def getPackageVersion(self):"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3628,
                "PatchRowcode": "+        self.__require_view()"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3629,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": 3614,
                "afterPatchRowNumber": 3630,
                "PatchRowcode": "         return self.__package_version"
            },
            "58": {
                "beforePatchRowNumber": 3615,
                "afterPatchRowNumber": 3631,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": 3616,
                "afterPatchRowNumber": 3632,
                "PatchRowcode": "     # -----------------------------------------------------------------------"
            }
        },
        "frontPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Handle Thrift requests.",
            "\"\"\"",
            "",
            "import base64",
            "import html",
            "import json",
            "import os",
            "import re",
            "import shlex",
            "import stat",
            "import time",
            "import zlib",
            "",
            "from copy import deepcopy",
            "from collections import OrderedDict, defaultdict, namedtuple",
            "from datetime import datetime, timedelta",
            "from typing import Any, Dict, List, Optional, Set, Tuple",
            "",
            "import sqlalchemy",
            "from sqlalchemy.sql.expression import or_, and_, not_, func, \\",
            "    asc, desc, union_all, select, bindparam, literal_column, case, cast",
            "from sqlalchemy.orm import contains_eager",
            "",
            "import codechecker_api_shared",
            "from codechecker_api.codeCheckerDBAccess_v6 import constants, ttypes",
            "from codechecker_api.codeCheckerDBAccess_v6.ttypes import \\",
            "    AnalysisInfoFilter, AnalysisInfoChecker as API_AnalysisInfoChecker, \\",
            "    BlameData, BlameInfo, BugPathPos, \\",
            "    CheckerCount, CheckerStatusVerificationDetail, Commit, CommitAuthor, \\",
            "    CommentData, \\",
            "    DetectionStatus, DiffType, \\",
            "    Encoding, ExportData, \\",
            "    Order, \\",
            "    ReportData, ReportDetails, ReportStatus, ReviewData, ReviewStatusRule, \\",
            "    ReviewStatusRuleFilter, ReviewStatusRuleSortMode, \\",
            "    ReviewStatusRuleSortType, RunData, RunFilter, RunHistoryData, \\",
            "    RunReportCount, RunSortType, RunTagCount, \\",
            "    ReviewStatus as API_ReviewStatus, \\",
            "    SourceComponentData, SourceFileData, SortMode, SortType",
            "",
            "from codechecker_common import util",
            "from codechecker_common.logger import get_logger",
            "",
            "from codechecker_web.shared import webserver_context",
            "from codechecker_web.shared import convert",
            "",
            "from codechecker_server.profiler import timeit",
            "",
            "from .. import permissions",
            "from ..database import db_cleanup",
            "from ..database.config_db_model import Product",
            "from ..database.database import conv, DBSession, escape_like",
            "from ..database.run_db_model import \\",
            "    AnalysisInfo, AnalysisInfoChecker as DB_AnalysisInfoChecker, \\",
            "    AnalyzerStatistic, \\",
            "    BugPathEvent, BugReportPoint, \\",
            "    CleanupPlan, CleanupPlanReportHash, Checker, Comment, \\",
            "    ExtendedReportData, \\",
            "    File, FileContent, \\",
            "    Report, ReportAnnotations, ReportAnalysisInfo, ReviewStatus, \\",
            "    Run, RunHistory, RunHistoryAnalysisInfo, RunLock, \\",
            "    SourceComponent",
            "",
            "from .thrift_enum_helper import detection_status_enum, \\",
            "    detection_status_str, report_status_enum, \\",
            "    review_status_enum, review_status_str, report_extended_data_type_enum",
            "",
            "# These names are inherited from Thrift stubs.",
            "# pylint: disable=invalid-name",
            "",
            "LOG = get_logger('server')",
            "",
            "GEN_OTHER_COMPONENT_NAME = \"Other (auto-generated)\"",
            "",
            "SQLITE_MAX_VARIABLE_NUMBER = 999",
            "SQLITE_MAX_COMPOUND_SELECT = 500",
            "",
            "",
            "class CommentKindValue:",
            "    USER = 0",
            "    SYSTEM = 1",
            "",
            "",
            "def comment_kind_from_thrift_type(kind):",
            "    \"\"\" Convert the given comment kind from Thrift type to Python enum. \"\"\"",
            "    if kind == ttypes.CommentKind.USER:",
            "        return CommentKindValue.USER",
            "    elif kind == ttypes.CommentKind.SYSTEM:",
            "        return CommentKindValue.SYSTEM",
            "",
            "    assert False, f\"Unknown ttypes.CommentKind: {kind}\"",
            "",
            "",
            "def comment_kind_to_thrift_type(kind):",
            "    \"\"\" Convert the given comment kind from Python enum to Thrift type. \"\"\"",
            "    if kind == CommentKindValue.USER:",
            "        return ttypes.CommentKind.USER",
            "    elif kind == CommentKindValue.SYSTEM:",
            "        return ttypes.CommentKind.SYSTEM",
            "",
            "    assert False, f\"Unknown CommentKindValue: {kind}\"",
            "",
            "",
            "def verify_limit_range(limit):",
            "    \"\"\"Verify limit value for the queries.",
            "",
            "    Query limit should not be larger than the max allowed value.",
            "    Max is returned if the value is larger than max.",
            "    \"\"\"",
            "    max_query_limit = constants.MAX_QUERY_SIZE",
            "    if not limit:",
            "        return max_query_limit",
            "    if limit > max_query_limit:",
            "        LOG.warning('Query limit %d was larger than max query limit %d, '",
            "                    'setting limit to %d',",
            "                    limit,",
            "                    max_query_limit,",
            "                    max_query_limit)",
            "        limit = max_query_limit",
            "    return limit",
            "",
            "",
            "def slugify(text):",
            "    \"\"\"",
            "    Removes and replaces special characters in a given text.",
            "    \"\"\"",
            "    # Removes non-alpha characters.",
            "    norm_text = re.sub(r'[^\\w\\s\\-/]', '', text)",
            "",
            "    # Converts spaces and slashes to underscores.",
            "    norm_text = re.sub(r'([\\s]+|[/]+)', '_', norm_text)",
            "",
            "    return norm_text",
            "",
            "",
            "def exc_to_thrift_reqfail(function):",
            "    \"\"\"",
            "    Convert internal exceptions to RequestFailed exception",
            "    which can be sent back on the thrift connections.",
            "    \"\"\"",
            "    func_name = function.__name__",
            "",
            "    def wrapper(*args, **kwargs):",
            "        try:",
            "            res = function(*args, **kwargs)",
            "            return res",
            "",
            "        except sqlalchemy.exc.SQLAlchemyError as alchemy_ex:",
            "            # Convert SQLAlchemy exceptions.",
            "            msg = str(alchemy_ex)",
            "            import traceback",
            "            traceback.print_exc()",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "        except codechecker_api_shared.ttypes.RequestFailed as rf:",
            "            LOG.warning(\"%s:\\n%s\", func_name, rf.message)",
            "            raise",
            "        except Exception as ex:",
            "            import traceback",
            "            traceback.print_exc()",
            "            msg = str(ex)",
            "            LOG.warning(\"%s:\\n%s\", func_name, msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "    return wrapper",
            "",
            "",
            "def get_component_values(",
            "    session: DBSession,",
            "    component_name: str",
            ") -> Tuple[List[str], List[str]]:",
            "    \"\"\"",
            "    Get component values by component names and returns a tuple where the",
            "    first item contains a list path which should be skipped and the second",
            "    item contains a list of path which should be included.",
            "    E.g.:",
            "      +/a/b/x.cpp",
            "      +/a/b/y.cpp",
            "      -/a/b",
            "    On the above component value this function will return the following:",
            "      (['/a/b'], ['/a/b/x.cpp', '/a/b/y.cpp'])",
            "    \"\"\"",
            "    components = session.query(SourceComponent) \\",
            "        .filter(SourceComponent.name.like(component_name)) \\",
            "        .all()",
            "",
            "    skip = []",
            "    include = []",
            "",
            "    for component in components:",
            "        values = component.value.decode('utf-8').split('\\n')",
            "        for value in values:",
            "            value = value.strip()",
            "            if not value:",
            "                continue",
            "",
            "            v = value[1:]",
            "            if value[0] == '+':",
            "                include.append(v)",
            "            elif value[0] == '-':",
            "                skip.append(v)",
            "",
            "    return skip, include",
            "",
            "",
            "def process_report_filter(",
            "    session,",
            "    run_ids,",
            "    report_filter,",
            "    cmp_data=None,",
            "    keep_all_annotations=False",
            "):",
            "    \"\"\"",
            "    Process the new report filter.",
            "    \"\"\"",
            "    AND = []",
            "",
            "    cmp_filter_expr, join_tables = process_cmp_data_filter(",
            "        session, run_ids, report_filter, cmp_data)",
            "",
            "    if cmp_filter_expr is not None:",
            "        AND.append(cmp_filter_expr)",
            "",
            "    if report_filter is None:",
            "        return and_(*AND), join_tables",
            "",
            "    if report_filter.reportHash == []:",
            "        return and_(False), []",
            "",
            "    if report_filter.filepath:",
            "        if report_filter.fileMatchesAnyPoint:",
            "            AND.append(Report.id.in_(get_reports_by_files(",
            "                session,",
            "                report_filter.filepath)))",
            "        else:",
            "            OR = [File.filepath.ilike(conv(fp))",
            "                  for fp in report_filter.filepath]",
            "",
            "            AND.append(or_(*OR))",
            "            join_tables.append(File)",
            "",
            "    if report_filter.checkerMsg:",
            "        OR = [Report.checker_message.ilike(conv(cm))",
            "              for cm in report_filter.checkerMsg]",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.analyzerNames or report_filter.checkerName \\",
            "            or report_filter.severity:",
            "        if report_filter.analyzerNames:",
            "            OR = [Checker.analyzer_name.ilike(conv(an))",
            "                  for an in report_filter.analyzerNames]",
            "            AND.append(or_(*OR))",
            "",
            "        if report_filter.checkerName:",
            "            OR = [Checker.checker_name.ilike(conv(cn))",
            "                  for cn in report_filter.checkerName]",
            "            AND.append(or_(*OR))",
            "",
            "        if report_filter.severity:",
            "            AND.append(Checker.severity.in_(report_filter.severity))",
            "",
            "        join_tables.append(Checker)",
            "",
            "    if report_filter.runName:",
            "        OR = [Run.name.ilike(conv(rn))",
            "              for rn in report_filter.runName]",
            "        AND.append(or_(*OR))",
            "        join_tables.append(Run)",
            "",
            "    if report_filter.reportHash:",
            "        OR = []",
            "        no_joker = []",
            "",
            "        for rh in report_filter.reportHash:",
            "            if '*' in rh:",
            "                OR.append(Report.bug_id.ilike(conv(rh)))",
            "            else:",
            "                no_joker.append(rh)",
            "",
            "        if no_joker:",
            "            OR.append(Report.bug_id.in_(no_joker))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.cleanupPlanNames:",
            "        OR = []",
            "        for cleanup_plan_name in report_filter.cleanupPlanNames:",
            "            q = select([CleanupPlanReportHash.bug_hash]) \\",
            "                .where(",
            "                    CleanupPlanReportHash.cleanup_plan_id.in_(",
            "                        select([CleanupPlan.id])",
            "                        .where(CleanupPlan.name == cleanup_plan_name)",
            "                        .distinct()",
            "                    )) \\",
            "                .distinct()",
            "",
            "            OR.append(Report.bug_id.in_(q))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.reportStatus:",
            "        dst = list(map(detection_status_str,",
            "                       (DetectionStatus.NEW,",
            "                        DetectionStatus.UNRESOLVED,",
            "                        DetectionStatus.REOPENED)))",
            "        rst = list(map(review_status_str,",
            "                       (API_ReviewStatus.UNREVIEWED,",
            "                        API_ReviewStatus.CONFIRMED)))",
            "",
            "        OR = []",
            "        filter_query = and_(",
            "            Report.review_status.in_(rst),",
            "            Report.detection_status.in_(dst)",
            "        )",
            "        if ReportStatus.OUTSTANDING in report_filter.reportStatus:",
            "            OR.append(filter_query)",
            "",
            "        if ReportStatus.CLOSED in report_filter.reportStatus:",
            "            OR.append(not_(filter_query))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.detectionStatus:",
            "        dst = list(map(detection_status_str,",
            "                       report_filter.detectionStatus))",
            "        AND.append(Report.detection_status.in_(dst))",
            "",
            "    if report_filter.reviewStatus:",
            "        OR = [Report.review_status.in_(",
            "            list(map(review_status_str, report_filter.reviewStatus)))]",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.firstDetectionDate is not None:",
            "        date = datetime.fromtimestamp(report_filter.firstDetectionDate)",
            "        AND.append(Report.detected_at >= date)",
            "",
            "    if report_filter.fixDate is not None:",
            "        date = datetime.fromtimestamp(report_filter.fixDate)",
            "        AND.append(Report.detected_at < date)",
            "",
            "    if report_filter.date:",
            "        detected_at = report_filter.date.detected",
            "        if detected_at:",
            "            if detected_at.before:",
            "                detected_before = datetime.fromtimestamp(detected_at.before)",
            "                AND.append(Report.detected_at <= detected_before)",
            "",
            "            if detected_at.after:",
            "                detected_after = datetime.fromtimestamp(detected_at.after)",
            "                AND.append(Report.detected_at >= detected_after)",
            "",
            "        fixed_at = report_filter.date.fixed",
            "        if fixed_at:",
            "            if fixed_at.before:",
            "                fixed_before = datetime.fromtimestamp(fixed_at.before)",
            "                AND.append(Report.fixed_at <= fixed_before)",
            "",
            "            if fixed_at.after:",
            "                fixed_after = datetime.fromtimestamp(fixed_at.after)",
            "                AND.append(Report.fixed_at >= fixed_after)",
            "",
            "    if report_filter.runHistoryTag:",
            "        OR = []",
            "        for history_date in report_filter.runHistoryTag:",
            "            date = datetime.strptime(history_date,",
            "                                     '%Y-%m-%d %H:%M:%S.%f')",
            "            OR.append(and_(Report.detected_at <= date, or_(",
            "                Report.fixed_at.is_(None), Report.fixed_at >= date)))",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.componentNames:",
            "        if report_filter.componentMatchesAnyPoint:",
            "            AND.append(Report.id.in_(get_reports_by_components(",
            "                session,",
            "                report_filter.componentNames)))",
            "        else:",
            "            AND.append(process_source_component_filter(",
            "                session, report_filter.componentNames))",
            "            join_tables.append(File)",
            "",
            "    if report_filter.bugPathLength is not None:",
            "        min_path_length = report_filter.bugPathLength.min",
            "        if min_path_length is not None:",
            "            AND.append(Report.path_length >= min_path_length)",
            "",
            "        max_path_length = report_filter.bugPathLength.max",
            "        if max_path_length is not None:",
            "            AND.append(Report.path_length <= max_path_length)",
            "",
            "    if report_filter.annotations is not None:",
            "        annotations = defaultdict(list)",
            "        for annotation in report_filter.annotations:",
            "            annotations[annotation.first].append(annotation.second)",
            "",
            "        OR = []",
            "        for key, values in annotations.items():",
            "            if keep_all_annotations:",
            "                OR.append(or_(",
            "                    ReportAnnotations.key != key,",
            "                    *[ReportAnnotations.value.ilike(conv(v))",
            "                      for v in values]))",
            "            else:",
            "                OR.append(and_(",
            "                    ReportAnnotations.key == key,",
            "                    or_(*[ReportAnnotations.value.ilike(conv(v))",
            "                          for v in values])) if values else and_(",
            "                              ReportAnnotations.key == key))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    filter_expr = and_(*AND)",
            "    return filter_expr, join_tables",
            "",
            "",
            "def process_source_component_filter(session, component_names):",
            "    \"\"\" Process source component filter.",
            "",
            "    The virtual auto-generated Other component will be handled separately and",
            "    the query part will be added to the filter.",
            "    \"\"\"",
            "    OR = []",
            "",
            "    for component_name in component_names:",
            "        if component_name == GEN_OTHER_COMPONENT_NAME:",
            "            file_query = get_other_source_component_file_query(session)",
            "        else:",
            "            file_query = get_source_component_file_query(session,",
            "                                                         component_name)",
            "",
            "        if file_query is not None:",
            "            OR.append(file_query)",
            "",
            "    return or_(*OR)",
            "",
            "",
            "def filter_open_reports_in_tags(results, run_ids, tag_ids):",
            "    \"\"\"",
            "    Adding filters on \"results\" query which filter on open reports in",
            "    given runs and tags.",
            "    For further information see the documentation of",
            "    filter_open_reports_in_tags_old().",
            "    \"\"\"",
            "",
            "    if run_ids:",
            "        results = results.filter(Report.run_id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        results = results.outerjoin(",
            "            RunHistory, RunHistory.run_id == Report.run_id) \\",
            "            .filter(RunHistory.id.in_(tag_ids)) \\",
            "            .filter(get_open_reports_date_filter_query())",
            "",
            "    return results",
            "",
            "",
            "def filter_open_reports_in_tags_old(results, run_ids, tag_ids):",
            "    \"\"\"",
            "    Adding filters on \"results\" query which filter on open reports in",
            "    given runs and tags.",
            "",
            "    This function is almost the same as filter_open_reports_in_tags() except",
            "    that is uses get_open_reports_date_filter_query_old() for filtering open",
            "    reports on a given date. This function is duplicated, because we didn't",
            "    want to add an extra parameter for this function, but express the fact that",
            "    an old client (i.e. API version before 6.50) should be given a different",
            "    result set.",
            "    This function and its duplicate are used in getDiffResultHash() which",
            "    should behave differently when called by an old client. The reasons of this",
            "    different behavior is described a previous commit",
            "    (f6d0fedaf14b583df7bd26078a8a22b557be57c6) where another case of the issue",
            "    was fixed.",
            "    \"\"\"",
            "",
            "    if run_ids:",
            "        results = results.filter(Report.run_id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        results = results.outerjoin(",
            "            RunHistory, RunHistory.run_id == Report.run_id) \\",
            "            .filter(RunHistory.id.in_(tag_ids)) \\",
            "            .filter(get_open_reports_date_filter_query_old())",
            "",
            "    return results",
            "",
            "",
            "def get_include_skip_queries(",
            "    include: List[str],",
            "    skip: List[str]",
            "):",
            "    \"\"\" Get queries for include and skip values of a component.",
            "",
            "    To get the include and skip lists use the 'get_component_values' function.",
            "    \"\"\"",
            "    include_q = select([File.id]) \\",
            "        .where(or_(*[",
            "            File.filepath.like(conv(fp)) for fp in include])) \\",
            "        .distinct()",
            "",
            "    skip_q = select([File.id]) \\",
            "        .where(or_(*[",
            "            File.filepath.like(conv(fp)) for fp in skip])) \\",
            "        .distinct()",
            "",
            "    return include_q, skip_q",
            "",
            "",
            "def get_source_component_file_query(",
            "    session: DBSession,",
            "    component_name: str",
            "):",
            "    \"\"\" Get filter query for a single source component. \"\"\"",
            "    skip, include = get_component_values(session, component_name)",
            "",
            "    if skip and include:",
            "        include_q, skip_q = get_include_skip_queries(include, skip)",
            "        return File.id.in_(include_q.except_(skip_q))",
            "",
            "    if include:",
            "        return or_(*[File.filepath.like(conv(fp)) for fp in include])",
            "    elif skip:",
            "        return and_(*[not_(File.filepath.like(conv(fp))) for fp in skip])",
            "",
            "    return None",
            "",
            "",
            "def get_reports_by_bugpath_filter(session, file_filter_q) -> Set[int]:",
            "    \"\"\"",
            "    This function returns a set of report IDs that are related to any file",
            "    described by the query in the second parameter, either because their bug",
            "    path goes through these files, or there is any bug note, etc. in these",
            "    files.",
            "    \"\"\"",
            "    def first_col_values(query):",
            "        \"\"\"",
            "        This function executes a query and returns the set of first columns'",
            "        values.",
            "        \"\"\"",
            "        return set(map(lambda x: x[0], query.all()))",
            "",
            "    report_ids = set()",
            "",
            "    q = session.query(Report.id) \\",
            "        .join(File, File.id == Report.file_id) \\",
            "        .filter(file_filter_q)",
            "",
            "    report_ids.update(first_col_values(q))",
            "",
            "    q = session.query(BugPathEvent.report_id) \\",
            "        .join(File, File.id == BugPathEvent.file_id) \\",
            "        .filter(file_filter_q)",
            "",
            "    report_ids.update(first_col_values(q))",
            "",
            "    q = session.query(ExtendedReportData.report_id) \\",
            "        .join(File, File.id == ExtendedReportData.file_id) \\",
            "        .filter(file_filter_q)",
            "",
            "    report_ids.update(first_col_values(q))",
            "",
            "    return report_ids",
            "",
            "",
            "def get_reports_by_components(session, component_names: List[str]) -> Set[int]:",
            "    \"\"\"",
            "    This function returns a set of report IDs that are related to any component",
            "    in the second parameter, either because their bug path goes through these",
            "    components, or there is any bug note, etc. in these components.",
            "    \"\"\"",
            "    source_component_filter = \\",
            "        process_source_component_filter(session, component_names)",
            "    return get_reports_by_bugpath_filter(session, source_component_filter)",
            "",
            "",
            "def get_reports_by_files(session, files: List[str]) -> Set[int]:",
            "    \"\"\"",
            "    This function returns a set of report IDs that are related to any file in",
            "    the second parameter, either because their bug path goes through these",
            "    files, or there is any bug note, etc. in these files.",
            "    \"\"\"",
            "    file_filter = or_(*[File.filepath.ilike(conv(fp)) for fp in files])",
            "    return get_reports_by_bugpath_filter(session, file_filter)",
            "",
            "",
            "def get_other_source_component_file_query(session):",
            "    \"\"\" Get filter query for the auto-generated Others component.",
            "    If there are no user defined source components in the database this",
            "    function will return with None.",
            "",
            "    The returned query will look like this:",
            "        (Files NOT IN Component_1) AND (Files NOT IN Component_2) ... AND",
            "        (Files NOT IN Component_N)",
            "    \"\"\"",
            "    component_names = session.query(SourceComponent.name).all()",
            "",
            "    # If there are no user defined source components we don't have to filter.",
            "    if not component_names:",
            "        return None",
            "",
            "    def get_query(component_name: str):",
            "        \"\"\" Get file filter query for auto generated Other component. \"\"\"",
            "        skip, include = get_component_values(session, component_name)",
            "",
            "        if skip and include:",
            "            include_q, skip_q = get_include_skip_queries(include, skip)",
            "            return File.id.notin_(include_q.except_(skip_q))",
            "        elif include:",
            "            return and_(*[File.filepath.notlike(conv(fp)) for fp in include])",
            "        elif skip:",
            "            return or_(*[File.filepath.like(conv(fp)) for fp in skip])",
            "",
            "        return None",
            "",
            "    queries = [get_query(n) for (n, ) in component_names]",
            "    return and_(*queries)",
            "",
            "",
            "def get_open_reports_date_filter_query(tbl=Report, date=RunHistory.time):",
            "    \"\"\" Get open reports date filter. \"\"\"",
            "    return and_(tbl.detected_at <= date,",
            "                or_(tbl.fixed_at.is_(None),",
            "                    tbl.fixed_at > date))",
            "",
            "",
            "def get_open_reports_date_filter_query_old(tbl=Report, date=RunHistory.time):",
            "    \"\"\" Get open reports date filter.",
            "",
            "    This function is a dupliation of get_open_reports_date_filter_query().",
            "    For the reson of duplication see the documentation of",
            "    filter_open_reports_in_tags_old().",
            "    \"\"\"",
            "    return tbl.detected_at <= date",
            "",
            "",
            "def get_diff_bug_id_query(session, run_ids, tag_ids, open_reports_date):",
            "    \"\"\" Get bug id query for diff. \"\"\"",
            "    q = session.query(Report.bug_id.distinct())",
            "",
            "    if run_ids:",
            "        q = q.filter(Report.run_id.in_(run_ids))",
            "        if not tag_ids and not open_reports_date:",
            "            q = q.filter(Report.fixed_at.is_(None))",
            "",
            "    if tag_ids:",
            "        q = q.outerjoin(RunHistory,",
            "                        RunHistory.run_id == Report.run_id) \\",
            "            .filter(RunHistory.id.in_(tag_ids)) \\",
            "            .filter(get_open_reports_date_filter_query())",
            "",
            "    if open_reports_date:",
            "        date = datetime.fromtimestamp(open_reports_date)",
            "",
            "        q = q.filter(get_open_reports_date_filter_query(Report, date))",
            "",
            "    return q",
            "",
            "",
            "def get_diff_bug_id_filter(run_ids, tag_ids, open_reports_date):",
            "    \"\"\" Get bug id filter for diff. \"\"\"",
            "    AND = []",
            "",
            "    if run_ids:",
            "        AND.append(Report.run_id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        AND.append(RunHistory.id.in_(tag_ids))",
            "        AND.append(get_open_reports_date_filter_query())",
            "",
            "    if open_reports_date:",
            "        date = datetime.fromtimestamp(open_reports_date)",
            "        AND.append(get_open_reports_date_filter_query(Report, date))",
            "",
            "    return and_(*AND)",
            "",
            "",
            "def get_diff_run_id_query(session, run_ids, tag_ids):",
            "    \"\"\" Get run id query for diff. \"\"\"",
            "    q = session.query(Run.id.distinct())",
            "",
            "    if run_ids:",
            "        q = q.filter(Run.id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        q = q.outerjoin(RunHistory,",
            "                        RunHistory.run_id == Run.id) \\",
            "            .filter(RunHistory.id.in_(tag_ids))",
            "",
            "    return q",
            "",
            "",
            "def is_cmp_data_empty(cmp_data):",
            "    \"\"\" True if the parameter is None or no filter fields are set. \"\"\"",
            "    if not cmp_data:",
            "        return True",
            "",
            "    return not any([cmp_data.runIds,",
            "                    cmp_data.runTag,",
            "                    cmp_data.openReportsDate])",
            "",
            "",
            "def is_baseline_empty(report_filter):",
            "    \"\"\" True if the parameter is None or no baseline filter fields are set. \"\"\"",
            "    if not report_filter:",
            "        return True",
            "",
            "    return not any([report_filter.runTag,",
            "                    report_filter.openReportsDate])",
            "",
            "",
            "def process_cmp_data_filter(session, run_ids, report_filter, cmp_data):",
            "    \"\"\" Process compare data filter. \"\"\"",
            "    base_tag_ids = report_filter.runTag if report_filter else None",
            "    base_open_reports_date = report_filter.openReportsDate \\",
            "        if report_filter else None",
            "",
            "    if is_cmp_data_empty(cmp_data):",
            "        if not run_ids and is_baseline_empty(report_filter):",
            "            return None, []",
            "",
            "        diff_filter = get_diff_bug_id_filter(",
            "            run_ids, base_tag_ids, base_open_reports_date)",
            "        join_tables = []",
            "",
            "        if run_ids:",
            "            join_tables.append(Run)",
            "        if base_tag_ids:",
            "            join_tables.append(RunHistory)",
            "",
            "        return and_(diff_filter), join_tables",
            "",
            "    query_base = get_diff_bug_id_query(session, run_ids, base_tag_ids,",
            "                                       base_open_reports_date)",
            "    query_base_runs = get_diff_run_id_query(session, run_ids, base_tag_ids)",
            "",
            "    query_new = get_diff_bug_id_query(session, cmp_data.runIds,",
            "                                      cmp_data.runTag,",
            "                                      cmp_data.openReportsDate)",
            "    query_new_runs = get_diff_run_id_query(session, cmp_data.runIds,",
            "                                           cmp_data.runTag)",
            "",
            "    if cmp_data.diffType == DiffType.NEW:",
            "        return and_(Report.bug_id.in_(query_new.except_(query_base)),",
            "                    Report.run_id.in_(query_new_runs)), [Run]",
            "    elif cmp_data.diffType == DiffType.RESOLVED:",
            "        return and_(Report.bug_id.in_(query_base.except_(query_new)),",
            "                    Report.run_id.in_(query_base_runs)), [Run]",
            "    elif cmp_data.diffType == DiffType.UNRESOLVED:",
            "        return and_(Report.bug_id.in_(query_base.intersect(query_new)),",
            "                    Report.run_id.in_(query_new_runs)), [Run]",
            "    else:",
            "        raise codechecker_api_shared.ttypes.RequestFailed(",
            "            codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "            'Unsupported diff type: ' + str(cmp_data.diffType))",
            "",
            "",
            "def process_run_history_filter(query, run_ids, run_history_filter):",
            "    \"\"\"",
            "    Process run history filter.",
            "    \"\"\"",
            "    if run_ids:",
            "        query = query.filter(RunHistory.run_id.in_(run_ids))",
            "",
            "    if run_history_filter:",
            "        if run_history_filter.tagNames:",
            "            OR = [RunHistory.version_tag.ilike('{0}'.format(conv(",
            "                escape_like(name, '\\\\'))), escape='\\\\') for",
            "                name in run_history_filter.tagNames]",
            "",
            "            query = query.filter(or_(*OR))",
            "",
            "        if run_history_filter.tagIds:",
            "            query = query.filter(RunHistory.id.in_(run_history_filter.tagIds))",
            "",
            "        stored = run_history_filter.stored",
            "        if stored:",
            "            if stored.before:",
            "                stored_before = datetime.fromtimestamp(stored.before)",
            "                query = query.filter(RunHistory.time <= stored_before)",
            "",
            "            if stored.after:",
            "                stored_after = datetime.fromtimestamp(stored.after)",
            "                query = query.filter(RunHistory.time >= stored_after)",
            "",
            "    return query",
            "",
            "",
            "def process_run_filter(session, query, run_filter):",
            "    \"\"\"",
            "    Process run filter.",
            "    \"\"\"",
            "    if run_filter is None:",
            "        return query",
            "",
            "    if run_filter.ids:",
            "        query = query.filter(Run.id.in_(run_filter.ids))",
            "    if run_filter.names:",
            "        if run_filter.exactMatch:",
            "            query = query.filter(Run.name.in_(run_filter.names))",
            "        else:",
            "            OR = [Run.name.ilike('{0}'.format(conv(",
            "                escape_like(name, '\\\\'))), escape='\\\\') for",
            "                name in run_filter.names]",
            "            query = query.filter(or_(*OR))",
            "",
            "    if run_filter.beforeTime:",
            "        date = datetime.fromtimestamp(run_filter.beforeTime)",
            "        query = query.filter(Run.date < date)",
            "",
            "    if run_filter.afterTime:",
            "        date = datetime.fromtimestamp(run_filter.afterTime)",
            "        query = query.filter(Run.date > date)",
            "",
            "    if run_filter.beforeRun:",
            "        run = session.query(Run.date) \\",
            "            .filter(Run.name == run_filter.beforeRun) \\",
            "            .one_or_none()",
            "",
            "        if run:",
            "            query = query.filter(Run.date < run.date)",
            "",
            "    if run_filter.afterRun:",
            "        run = session.query(Run.date) \\",
            "            .filter(Run.name == run_filter.afterRun) \\",
            "            .one_or_none()",
            "",
            "        if run:",
            "            query = query.filter(Run.date > run.date)",
            "",
            "    return query",
            "",
            "",
            "def get_report_details(session, report_ids):",
            "    \"\"\"",
            "    Returns report details for the given report ids.",
            "    \"\"\"",
            "    details = {}",
            "",
            "    # Get bug path events.",
            "    bug_path_events = session.query(BugPathEvent, File.filepath) \\",
            "        .filter(BugPathEvent.report_id.in_(report_ids)) \\",
            "        .outerjoin(File,",
            "                   File.id == BugPathEvent.file_id) \\",
            "        .order_by(BugPathEvent.report_id, BugPathEvent.order)",
            "",
            "    bug_events_list = defaultdict(list)",
            "    for event, file_path in bug_path_events:",
            "        report_id = event.report_id",
            "        event = bugpathevent_db_to_api(event)",
            "        event.filePath = file_path",
            "        bug_events_list[report_id].append(event)",
            "",
            "    # Get bug report points.",
            "    bug_report_points = session.query(BugReportPoint, File.filepath) \\",
            "        .filter(BugReportPoint.report_id.in_(report_ids)) \\",
            "        .outerjoin(File,",
            "                   File.id == BugReportPoint.file_id) \\",
            "        .order_by(BugReportPoint.report_id, BugReportPoint.order)",
            "",
            "    bug_point_list = defaultdict(list)",
            "    for bug_point, file_path in bug_report_points:",
            "        report_id = bug_point.report_id",
            "        bug_point = bugreportpoint_db_to_api(bug_point)",
            "        bug_point.filePath = file_path",
            "        bug_point_list[report_id].append(bug_point)",
            "",
            "    # Get extended report data.",
            "    extended_data_list = defaultdict(list)",
            "    q = session.query(ExtendedReportData, File.filepath) \\",
            "        .filter(ExtendedReportData.report_id.in_(report_ids)) \\",
            "        .outerjoin(File,",
            "                   File.id == ExtendedReportData.file_id)",
            "",
            "    for data, file_path in q:",
            "        report_id = data.report_id",
            "        extended_data = extended_data_db_to_api(data)",
            "        extended_data.filePath = file_path",
            "        extended_data_list[report_id].append(extended_data)",
            "",
            "    # Get Comments for report data",
            "    comment_data_list = defaultdict(list)",
            "    comment_query = session.query(Comment, Report.id)\\",
            "        .filter(Report.id.in_(report_ids)) \\",
            "        .outerjoin(Report, Report.bug_id == Comment.bug_hash) \\",
            "        .order_by(Comment.created_at.desc())",
            "",
            "    for data, report_id in comment_query:",
            "        comment_data = comment_data_db_to_api(data)",
            "        comment_data_list[report_id].append(comment_data)",
            "",
            "    for report_id in report_ids:",
            "        details[report_id] = \\",
            "            ReportDetails(pathEvents=bug_events_list[report_id],",
            "                          executionPath=bug_point_list[report_id],",
            "                          extendedData=extended_data_list[report_id],",
            "                          comments=comment_data_list[report_id])",
            "",
            "    return details",
            "",
            "",
            "def bugpathevent_db_to_api(bpe):",
            "    return ttypes.BugPathEvent(",
            "        startLine=bpe.line_begin,",
            "        startCol=bpe.col_begin,",
            "        endLine=bpe.line_end,",
            "        endCol=bpe.col_end,",
            "        msg=bpe.msg,",
            "        fileId=bpe.file_id)",
            "",
            "",
            "def bugreportpoint_db_to_api(brp):",
            "    return BugPathPos(",
            "        startLine=brp.line_begin,",
            "        startCol=brp.col_begin,",
            "        endLine=brp.line_end,",
            "        endCol=brp.col_end,",
            "        fileId=brp.file_id)",
            "",
            "",
            "def extended_data_db_to_api(erd):",
            "    return ttypes.ExtendedReportData(",
            "        type=report_extended_data_type_enum(erd.type),",
            "        startLine=erd.line_begin,",
            "        startCol=erd.col_begin,",
            "        endLine=erd.line_end,",
            "        endCol=erd.col_end,",
            "        message=erd.message,",
            "        fileId=erd.file_id)",
            "",
            "",
            "def comment_data_db_to_api(comm):",
            "    \"\"\"",
            "    Returns a CommentData Object with all the relevant fields",
            "    \"\"\"",
            "    return ttypes.CommentData(",
            "        id=comm.id,",
            "        author=comm.author,",
            "        message=get_comment_msg(comm),",
            "        createdAt=str(comm.created_at),",
            "        kind=comm.kind",
            "    )",
            "",
            "",
            "def get_comment_msg(comment):",
            "    \"\"\"",
            "    Checks for the comment kind. If the comment is",
            "    identified as a system comment, it is formatted accordindly.",
            "    \"\"\"",
            "    context = webserver_context.get_context()",
            "    message = comment.message.decode('utf-8')",
            "    sys_comment = comment_kind_from_thrift_type(ttypes.CommentKind.SYSTEM)",
            "",
            "    if comment.kind == sys_comment:",
            "        try:",
            "            elements = shlex.split(message)",
            "        except ValueError:",
            "            # In earlier CodeChecker we saved system comments",
            "            # without escaping special characters such as",
            "            # quotes. This is kept only for backward",
            "            # compatibility reason.",
            "            message = message \\",
            "                .replace(\"'\", \"\\\\'\") \\",
            "                .replace('\"', '\\\\\"')",
            "",
            "            elements = shlex.split(message)",
            "",
            "        system_comment = context.system_comment_map.get(elements[0])",
            "        if system_comment:",
            "            for idx, value in enumerate(elements[1:]):",
            "                system_comment = system_comment.replace(",
            "                    '{' + str(idx) + '}', html.escape(value))",
            "            return system_comment",
            "",
            "    return html.escape(message)",
            "",
            "",
            "def create_review_data(",
            "    review_status: str,",
            "    message: Optional[str],",
            "    author,",
            "    date,",
            "    is_in_source: bool",
            "):",
            "    return ReviewData(",
            "        status=review_status_enum(review_status),",
            "        comment=None if message is None else message.decode('utf-8'),",
            "        author=author,",
            "        date=None if date is None else str(date),",
            "        isInSource=is_in_source)",
            "",
            "",
            "def apply_report_filter(q, filter_expression,",
            "                        join_tables: List[Any],",
            "                        already_joined_tables: Optional[List[Any]] = None):",
            "    \"\"\"",
            "    Applies the given filter expression and joins the Checker, File, Run, and",
            "    RunHistory tables if necessary based on join_tables parameter. If a table",
            "    is already joined by the main query and this is indicated, that will not",
            "    be joined by this function to prevent a \"duplicate alias\" error.",
            "    \"\"\"",
            "    def needs_join(tbl):",
            "        return tbl in join_tables and (already_joined_tables is None or",
            "                                       tbl not in already_joined_tables)",
            "",
            "    if needs_join(Checker):",
            "        q = q.join(Checker, Report.checker_id == Checker.id)",
            "    if needs_join(File):",
            "        q = q.outerjoin(File, Report.file_id == File.id)",
            "    if needs_join(Run):",
            "        q = q.outerjoin(Run, Run.id == Report.run_id)",
            "    if needs_join(RunHistory):",
            "        q = q.outerjoin(RunHistory, RunHistory.run_id == Report.run_id)",
            "",
            "    return q.filter(filter_expression)",
            "",
            "",
            "def get_sort_map(sort_types, is_unique=False):",
            "    # Get a list of sort_types which will be a nested ORDER BY.",
            "    sort_type_map = {",
            "        SortType.FILENAME: [(File.filepath, 'filepath'),",
            "                            (Report.line, 'line')],",
            "        SortType.BUG_PATH_LENGTH: [(Report.path_length, 'bug_path_length')],",
            "        SortType.CHECKER_NAME: [(Checker.checker_name, 'checker_name')],",
            "        SortType.SEVERITY: [(Checker.severity, 'severity')],",
            "        SortType.REVIEW_STATUS: [(Report.review_status, 'rw_status')],",
            "        SortType.DETECTION_STATUS: [(Report.detection_status, 'dt_status')],",
            "        SortType.TIMESTAMP: [('annotation_timestamp', 'annotation_timestamp')],",
            "        SortType.TESTCASE: [('annotation_testcase', 'annotation_testcase')]}",
            "",
            "    if is_unique:",
            "        sort_type_map[SortType.FILENAME] = [(File.filename, 'filename')]",
            "        sort_type_map[SortType.DETECTION_STATUS] = []",
            "",
            "    # Mapping the SQLAlchemy functions.",
            "    order_type_map = {Order.ASC: asc, Order.DESC: desc}",
            "",
            "    if sort_types is None:",
            "        sort_types = [SortMode(SortType.SEVERITY, Order.DESC)]",
            "",
            "    return sort_types, sort_type_map, order_type_map",
            "",
            "",
            "def sort_results_query(query, sort_types, sort_type_map, order_type_map,",
            "                       order_by_label=False):",
            "    \"\"\"",
            "    Helper method for __queryDiffResults and queryResults to apply sorting.",
            "    \"\"\"",
            "    for sort in sort_types:",
            "        sorttypes = sort_type_map.get(sort.type)",
            "        for sorttype in sorttypes:",
            "            order_type = order_type_map.get(sort.ord)",
            "            sort_col = sorttype[1] if order_by_label else sorttype[0]",
            "            query = query.order_by(order_type(sort_col))",
            "",
            "    return query",
            "",
            "",
            "def filter_unresolved_reports(q):",
            "    \"\"\"",
            "    Filter reports which are unresolved.",
            "",
            "    Note: review status of these reports are not in skip_review_statuses",
            "    and detection statuses are not in skip_detection_statuses.",
            "    \"\"\"",
            "    skip_review_statuses = ['false_positive', 'intentional']",
            "    skip_detection_statuses = ['resolved', 'off', 'unavailable']",
            "",
            "    return q.filter(Report.detection_status.notin_(skip_detection_statuses)) \\",
            "            .filter(Report.review_status.notin_(skip_review_statuses))",
            "",
            "",
            "def check_remove_runs_lock(session, run_ids):",
            "    \"\"\"",
            "    Check if there is an existing lock on the given runs, which has not",
            "    expired yet. If so, the run cannot be deleted, as someone is assumed to",
            "    be storing into it.",
            "    \"\"\"",
            "    locks_expired_at = datetime.now() - timedelta(",
            "        seconds=db_cleanup.RUN_LOCK_TIMEOUT_IN_DATABASE)",
            "",
            "    run_locks = session.query(RunLock.name) \\",
            "        .filter(RunLock.locked_at >= locks_expired_at)",
            "",
            "    if run_ids:",
            "        run_locks = run_locks.filter(Run.id.in_(run_ids))",
            "",
            "    run_locks = run_locks \\",
            "        .outerjoin(Run,",
            "                   Run.name == RunLock.name) \\",
            "        .all()",
            "",
            "    if run_locks:",
            "        raise codechecker_api_shared.ttypes.RequestFailed(",
            "            codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "            \"Can not remove results because the following runs \"",
            "            f\"are locked: {', '.join([r[0] for r in run_locks])}\")",
            "",
            "",
            "def sort_run_data_query(query, sort_mode):",
            "    \"\"\"",
            "    Sort run data query by the given sort type.",
            "    \"\"\"",
            "    # Sort by run date by default.",
            "    if not sort_mode:",
            "        return query.order_by(desc(Run.date))",
            "",
            "    order_type_map = {Order.ASC: asc, Order.DESC: desc}",
            "    order_type = order_type_map.get(sort_mode.ord)",
            "    if sort_mode.type == RunSortType.NAME:",
            "        query = query.order_by(order_type(Run.name))",
            "    elif sort_mode.type == RunSortType.UNRESOLVED_REPORTS:",
            "        query = query.order_by(order_type('report_count'))",
            "    elif sort_mode.type == RunSortType.DATE:",
            "        query = query.order_by(order_type(Run.date))",
            "    elif sort_mode.type == RunSortType.DURATION:",
            "        query = query.order_by(order_type(Run.duration))",
            "    elif sort_mode.type == RunSortType.CC_VERSION:",
            "        query = query.order_by(order_type(RunHistory.cc_version))",
            "",
            "    return query",
            "",
            "",
            "def get_failed_files_query(session, run_ids, query_fields,",
            "                           extra_sub_query_fields=None):",
            "    \"\"\"",
            "    General function to get query to fetch the list of failed files and to get",
            "    the number of failed files.",
            "    \"\"\"",
            "    sub_query_fields = [func.max(RunHistory.id).label('history_id')]",
            "    if extra_sub_query_fields:",
            "        sub_query_fields.extend(extra_sub_query_fields)",
            "",
            "    sub_q = session.query(*sub_query_fields)",
            "",
            "    if run_ids:",
            "        sub_q = sub_q.filter(RunHistory.run_id.in_(run_ids))",
            "",
            "    sub_q = sub_q \\",
            "        .group_by(RunHistory.run_id) \\",
            "        .subquery()",
            "",
            "    query = session \\",
            "        .query(*query_fields) \\",
            "        .outerjoin(sub_q,",
            "                   AnalyzerStatistic.run_history_id == sub_q.c.history_id) \\",
            "        .filter(AnalyzerStatistic.run_history_id == sub_q.c.history_id)",
            "",
            "    return query, sub_q",
            "",
            "",
            "def get_analysis_statistics_query(session, run_ids, run_history_ids=None):",
            "    \"\"\" Get analyzer statistics query. \"\"\"",
            "    query = session.query(AnalyzerStatistic, Run.id)",
            "",
            "    if run_ids:",
            "        # Subquery to get analyzer statistics only for these run history id's.",
            "        history_ids_subq = session.query(",
            "                func.max(AnalyzerStatistic.run_history_id)) \\",
            "            .filter(RunHistory.run_id.in_(run_ids)) \\",
            "            .outerjoin(",
            "                RunHistory,",
            "                RunHistory.id == AnalyzerStatistic.run_history_id) \\",
            "            .group_by(RunHistory.run_id) \\",
            "            .subquery()",
            "",
            "        query = query.filter(",
            "            AnalyzerStatistic.run_history_id.in_(history_ids_subq))",
            "    elif run_history_ids:",
            "        query = query.filter(RunHistory.id.in_(run_history_ids))",
            "",
            "    return query \\",
            "        .outerjoin(RunHistory,",
            "                   RunHistory.id == AnalyzerStatistic.run_history_id) \\",
            "        .outerjoin(Run,",
            "                   Run.id == RunHistory.run_id)",
            "",
            "",
            "def get_commit_url(",
            "    remote_url: Optional[str],",
            "    git_commit_urls: List",
            ") -> Optional[str]:",
            "    \"\"\" Get commit url for the given remote url. \"\"\"",
            "    if not remote_url:",
            "        return None",
            "",
            "    for git_commit_url in git_commit_urls:",
            "        m = git_commit_url[\"regex\"].match(remote_url)",
            "        if m:",
            "            url = git_commit_url[\"url\"]",
            "            for key, value in m.groupdict().items():",
            "                if value is not None:",
            "                    url = url.replace(f\"${key}\", value)",
            "",
            "            return url",
            "",
            "    return None",
            "",
            "",
            "def get_cleanup_plan(session, cleanup_plan_id: int) -> CleanupPlan:",
            "    \"\"\"",
            "    Check if the given cleanup id exists in the database and returns",
            "    the cleanup. Otherwise it will raise an exception.",
            "    \"\"\"",
            "    cleanup_plan = session.query(CleanupPlan).get(cleanup_plan_id)",
            "",
            "    if not cleanup_plan:",
            "        raise codechecker_api_shared.ttypes.RequestFailed(",
            "            codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "            f\"Cleanup plan '{cleanup_plan_id}' was not found in the database.\")",
            "",
            "    return cleanup_plan",
            "",
            "",
            "def get_cleanup_plan_report_hashes(",
            "    session,",
            "    cleanup_plan_ids: List[int]",
            ") -> Dict[int, List[str]]:",
            "    \"\"\" Get report hashes for the given cleanup plan ids. \"\"\"",
            "    cleanup_plan_hashes = defaultdict(list)",
            "",
            "    q = session \\",
            "        .query(",
            "            CleanupPlanReportHash.cleanup_plan_id,",
            "            CleanupPlanReportHash.bug_hash) \\",
            "        .filter(CleanupPlanReportHash.cleanup_plan_id.in_(",
            "            cleanup_plan_ids))",
            "",
            "    for cleanup_plan_id, report_hash in q:",
            "        cleanup_plan_hashes[cleanup_plan_id].append(report_hash)",
            "",
            "    return cleanup_plan_hashes",
            "",
            "",
            "def sort_review_statuses_query(",
            "    query,",
            "    sort_mode: ReviewStatusRuleSortMode,",
            "    report_count_label",
            "):",
            "    \"\"\"",
            "    Sort review status rule query by the given sort mode.",
            "    \"\"\"",
            "    # Sort by rule date by default.",
            "    if not sort_mode:",
            "        return query.order_by(desc(ReviewStatus.date))",
            "",
            "    order_type_map = {Order.ASC: asc, Order.DESC: desc}",
            "    order_type = order_type_map.get(sort_mode.ord)",
            "    if sort_mode.type == ReviewStatusRuleSortType.REPORT_HASH:",
            "        query = query.order_by(order_type(ReviewStatus.bug_hash))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.STATUS:",
            "        query = query.order_by(order_type(ReviewStatus.status))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.AUTHOR:",
            "        query = query.order_by(order_type(ReviewStatus.author))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.DATE:",
            "        query = query.order_by(order_type(ReviewStatus.date))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.ASSOCIATED_REPORTS_COUNT:",
            "        query = query.order_by(order_type(report_count_label))",
            "",
            "    return query",
            "",
            "",
            "def process_rs_rule_filter(",
            "    query,",
            "    rule_filter: Optional[ReviewStatusRuleFilter] = None",
            "):",
            "    \"\"\" Process review status rule filter. \"\"\"",
            "    if rule_filter:",
            "        if rule_filter.reportHashes is not None:",
            "            OR = [ReviewStatus.bug_hash.ilike(conv(report_hash))",
            "                  for report_hash in rule_filter.reportHashes]",
            "            query = query.filter(or_(*OR))",
            "",
            "        if rule_filter.reviewStatuses is not None:",
            "            query = query.filter(",
            "                ReviewStatus.status.in_(",
            "                    map(review_status_str, rule_filter.reviewStatuses)))",
            "",
            "        if rule_filter.authors is not None:",
            "            OR = [ReviewStatus.author.ilike(conv(author))",
            "                  for author in rule_filter.authors]",
            "            query = query.filter(or_(*OR))",
            "",
            "    return query",
            "",
            "",
            "def get_rs_rule_query(",
            "    session: DBSession,",
            "    rule_filter: Optional[ReviewStatusRuleFilter] = None,",
            "    sort_mode: Optional[ReviewStatusRuleSortMode] = None",
            "):",
            "    \"\"\" Returns query to get review status rules. \"\"\"",
            "    report_count = func.count(Report.id).label('report_count')",
            "    q = session \\",
            "        .query(ReviewStatus, report_count) \\",
            "        .join(Report,",
            "              Report.bug_id == ReviewStatus.bug_hash,",
            "              isouter=True)",
            "    q = process_rs_rule_filter(q, rule_filter)",
            "",
            "    if sort_mode:",
            "        q = sort_review_statuses_query(q, sort_mode, report_count)",
            "",
            "    q = q.group_by(ReviewStatus.bug_hash)",
            "",
            "    # Filter review status rules by aggregate columns.",
            "    if rule_filter and rule_filter.noAssociatedReports:",
            "        q = q.having(report_count == 0)",
            "",
            "    return q",
            "",
            "",
            "def get_run_id_expression(session, report_filter):",
            "    \"\"\"",
            "    Get run id or concatenated run id list by the unique mode and the DB type",
            "    \"\"\"",
            "    if report_filter.isUnique:",
            "        if session.bind.dialect.name == \"postgresql\":",
            "            return func.string_agg(",
            "                cast(Run.id, sqlalchemy.String).distinct(),",
            "                ','",
            "            ).label(\"run_id\")",
            "        return func.group_concat(Run.id.distinct()).label(\"run_id\")",
            "    return Run.id.label(\"run_id\")",
            "",
            "",
            "def get_is_enabled_case(subquery):",
            "    \"\"\"",
            "    Creating a case statement to decide the report",
            "    is enabled or not based on the detection status",
            "    \"\"\"",
            "    detection_status_filters = subquery.c.detection_status.in_(list(",
            "        map(detection_status_str,",
            "            (DetectionStatus.OFF, DetectionStatus.UNAVAILABLE))",
            "    ))",
            "",
            "    return case(",
            "        [(detection_status_filters, False)],",
            "        else_=True",
            "    )",
            "",
            "",
            "def get_is_opened_case(subquery):",
            "    \"\"\"",
            "    Creating a case statement to decide the report is opened or not",
            "    based on the detection status and the review status",
            "    \"\"\"",
            "    detection_statuses = (",
            "        DetectionStatus.NEW,",
            "        DetectionStatus.UNRESOLVED,",
            "        DetectionStatus.REOPENED",
            "    )",
            "    review_statuses = (",
            "        API_ReviewStatus.UNREVIEWED,",
            "        API_ReviewStatus.CONFIRMED",
            "    )",
            "    detection_and_review_status_filters = [",
            "        subquery.c.detection_status.in_(list(map(",
            "            detection_status_str, detection_statuses))),",
            "        subquery.c.review_status.in_(list(map(",
            "            review_status_str, review_statuses)))",
            "    ]",
            "    return case(",
            "        [(and_(*detection_and_review_status_filters), True)],",
            "        else_=False",
            "    )",
            "",
            "",
            "class ThriftRequestHandler:",
            "    \"\"\"",
            "    Connect to database and handle thrift client requests.",
            "    \"\"\"",
            "",
            "    def __init__(self,",
            "                 manager,",
            "                 Session,",
            "                 product,",
            "                 auth_session,",
            "                 config_database,",
            "                 package_version,",
            "                 client_version,",
            "                 context):",
            "",
            "        if not product:",
            "            raise ValueError(\"Cannot initialize request handler without \"",
            "                             \"a product to serve.\")",
            "",
            "        self._manager = manager",
            "        self._product = product",
            "        self._auth_session = auth_session",
            "        self._config_database = config_database",
            "        self.__package_version = package_version",
            "        self.__client_version = client_version",
            "        self._Session = Session",
            "        self._context = context",
            "        self.__permission_args = {",
            "            'productID': product.id",
            "        }",
            "",
            "    def _get_username(self):",
            "        \"\"\"",
            "        Returns the actually logged in user name.",
            "        \"\"\"",
            "        return self._auth_session.user if self._auth_session else \"Anonymous\"",
            "",
            "    def _set_run_data_for_curr_product(",
            "        self,",
            "        inc_num_of_runs: Optional[int],",
            "        latest_storage_date: Optional[datetime] = None",
            "    ):",
            "        \"\"\"",
            "        Increment the number of runs related to the current product with the",
            "        given value and set the latest storage date.",
            "        \"\"\"",
            "        values = {}",
            "",
            "        if inc_num_of_runs is not None:",
            "            values[\"num_of_runs\"] = Product.num_of_runs + inc_num_of_runs",
            "            # FIXME: This log is likely overkill.",
            "            LOG.info(\"Run counter in the config database was %s by %i.\",",
            "                     'increased' if inc_num_of_runs >= 0 else 'decreased',",
            "                     abs(inc_num_of_runs))",
            "",
            "        if latest_storage_date is not None:",
            "            values[\"latest_storage_date\"] = latest_storage_date",
            "",
            "        with DBSession(self._config_database) as session:",
            "            session.query(Product) \\",
            "                .filter(Product.id == self._product.id) \\",
            "                .update(values)",
            "",
            "            session.commit()",
            "",
            "    def __require_permission(self, required):",
            "        \"\"\"",
            "        Helper method to raise an UNAUTHORIZED exception if the user does not",
            "        have any of the given permissions.",
            "        \"\"\"",
            "",
            "        with DBSession(self._config_database) as session:",
            "            args = dict(self.__permission_args)",
            "            args['config_db_session'] = session",
            "",
            "            if not any(permissions.require_permission(",
            "                    perm, args, self._auth_session)",
            "                    for perm in required):",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                    \"You are not authorized to execute this action.\")",
            "",
            "            return True",
            "",
            "    def __require_admin(self):",
            "        self.__require_permission([permissions.PRODUCT_ADMIN])",
            "",
            "    def __require_access(self):",
            "        self.__require_permission([permissions.PRODUCT_ACCESS])",
            "",
            "    def __require_store(self):",
            "        self.__require_permission([permissions.PRODUCT_STORE])",
            "",
            "    def __require_view(self):",
            "        self.__require_permission([permissions.PRODUCT_VIEW])",
            "",
            "    def __add_comment(self, bug_id, message, kind=CommentKindValue.USER,",
            "                      date=None):",
            "        \"\"\" Creates a new comment object. \"\"\"",
            "        user = self._get_username()",
            "        return Comment(bug_id,",
            "                       user,",
            "                       message.encode('utf-8'),",
            "                       kind,",
            "                       date or datetime.now())",
            "",
            "    @timeit",
            "    def getRunData(self, run_filter, limit, offset, sort_mode):",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            # Count the reports subquery.",
            "            stmt = session.query(Report.run_id,",
            "                                 func.count(Report.bug_id)",
            "                                 .label('report_count'))",
            "",
            "            stmt = filter_unresolved_reports(stmt) \\",
            "                .group_by(Report.run_id).subquery()",
            "",
            "            tag_q = session.query(RunHistory.run_id,",
            "                                  func.max(RunHistory.id).label(",
            "                                      'run_history_id'),",
            "                                  func.max(RunHistory.time).label(",
            "                                      'run_history_time')) \\",
            "                .group_by(RunHistory.run_id) \\",
            "                .subquery()",
            "",
            "            q = session.query(Run.id,",
            "                              Run.date,",
            "                              Run.name,",
            "                              Run.duration,",
            "                              RunHistory.version_tag,",
            "                              RunHistory.cc_version,",
            "                              RunHistory.description,",
            "                              stmt.c.report_count)",
            "",
            "            q = process_run_filter(session, q, run_filter)",
            "",
            "            q = q.outerjoin(stmt, Run.id == stmt.c.run_id) \\",
            "                .outerjoin(tag_q, Run.id == tag_q.c.run_id) \\",
            "                .outerjoin(RunHistory,",
            "                           RunHistory.id == tag_q.c.run_history_id) \\",
            "                .group_by(Run.id,",
            "                          RunHistory.version_tag,",
            "                          RunHistory.cc_version,",
            "                          RunHistory.description,",
            "                          stmt.c.report_count)",
            "",
            "            q = sort_run_data_query(q, sort_mode)",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            # Get the runs.",
            "            run_data = q.all()",
            "",
            "            # Set run ids filter by using the previous results.",
            "            if not run_filter:",
            "                run_filter = RunFilter()",
            "",
            "            run_filter.ids = [r[0] for r in run_data]",
            "",
            "            # Get report count for each detection statuses.",
            "            status_q = session.query(Report.run_id,",
            "                                     Report.detection_status,",
            "                                     func.count(Report.bug_id)) \\",
            "                .filter(Report.run_id.in_(run_filter.ids)) \\",
            "                .group_by(Report.run_id, Report.detection_status)",
            "",
            "            status_sum = defaultdict(defaultdict)",
            "            for run_id, status, count in status_q:",
            "                status_sum[run_id][detection_status_enum(status)] = count",
            "",
            "            # Get analyzer statistics.",
            "            analyzer_statistics = defaultdict(defaultdict)",
            "",
            "            stat_q = get_analysis_statistics_query(session, run_filter.ids)",
            "            for analyzer_stat, run_id in stat_q:",
            "                analyzer_statistics[run_id][analyzer_stat.analyzer_type] = \\",
            "                    ttypes.AnalyzerStatistics(",
            "                        failed=analyzer_stat.failed,",
            "                        successful=analyzer_stat.successful)",
            "",
            "            results = []",
            "",
            "            for run_id, run_date, run_name, duration, tag, cc_version, \\",
            "                description, report_count \\",
            "                    in run_data:",
            "",
            "                if report_count is None:",
            "                    report_count = 0",
            "",
            "                analyzer_stats = analyzer_statistics[run_id]",
            "                results.append(RunData(runId=run_id,",
            "                                       runDate=str(run_date),",
            "                                       name=run_name,",
            "                                       duration=duration,",
            "                                       resultCount=report_count,",
            "                                       detectionStatusCount=status_sum[run_id],",
            "                                       versionTag=tag,",
            "                                       codeCheckerVersion=cc_version,",
            "                                       analyzerStatistics=analyzer_stats,",
            "                                       description=description))",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunCount(self, run_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            query = session.query(Run.id)",
            "            query = process_run_filter(session, query, run_filter)",
            "",
            "        return query.count()",
            "",
            "    # DEPRECATED: use getAnalysisInfo API function instead of this function.",
            "    def getCheckCommand(self, run_history_id, run_id):",
            "        \"\"\" Get analyzer command based on the given filter. \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = None",
            "        offset = 0",
            "        analysis_info_filter = AnalysisInfoFilter(",
            "            runId=run_id,",
            "            runHistoryId=run_history_id)",
            "",
            "        analysis_info = self.getAnalysisInfo(",
            "            analysis_info_filter, limit, offset)",
            "",
            "        return \"; \".join([html.escape(i.analyzerCommand)",
            "                          for i in analysis_info])",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalysisInfo(self, analysis_info_filter, limit, offset):",
            "        \"\"\" Get analysis information based on the given filter. \"\"\"",
            "        self.__require_view()",
            "",
            "        res: List[ttypes.AnalysisInfo] = []",
            "        if not analysis_info_filter:",
            "            return res",
            "",
            "        analysis_info_query = None",
            "        with DBSession(self._Session) as session:",
            "            run_id = analysis_info_filter.runId",
            "            run_history_ids = None",
            "            if run_id is not None:",
            "                run_history_ids = session \\",
            "                    .query(RunHistory.id) \\",
            "                    .filter(RunHistory.run_id == run_id) \\",
            "                    .order_by(RunHistory.time.desc()) \\",
            "                    .limit(1)",
            "",
            "            if run_history_ids is None:",
            "                run_history_ids = [analysis_info_filter.runHistoryId]",
            "",
            "            if run_history_ids is not None:",
            "                rh_a_tbl = RunHistoryAnalysisInfo",
            "                analysis_info_query = session.query(AnalysisInfo) \\",
            "                    .outerjoin(",
            "                        rh_a_tbl,",
            "                        rh_a_tbl.c.analysis_info_id == AnalysisInfo.id) \\",
            "                    .filter(rh_a_tbl.c.run_history_id.in_(run_history_ids))",
            "",
            "            report_id = analysis_info_filter.reportId",
            "            if report_id is not None:",
            "                r_a_tbl = ReportAnalysisInfo",
            "                analysis_info_query = session.query(AnalysisInfo) \\",
            "                    .outerjoin(",
            "                        r_a_tbl,",
            "                        r_a_tbl.c.analysis_info_id == AnalysisInfo.id) \\",
            "                    .filter(r_a_tbl.c.report_id == report_id)",
            "",
            "            if analysis_info_query:",
            "                if limit:",
            "                    analysis_info_query = analysis_info_query \\",
            "                        .limit(limit).offset(offset)",
            "",
            "                for cmd in analysis_info_query:",
            "                    command = zlib.decompress(cmd.analyzer_command) \\",
            "                        .decode(\"utf-8\")",
            "",
            "                    checkers_q = session \\",
            "                        .query(Checker.analyzer_name,",
            "                               Checker.checker_name,",
            "                               DB_AnalysisInfoChecker.enabled) \\",
            "                        .join(Checker, DB_AnalysisInfoChecker.checker_id ==",
            "                              Checker.id) \\",
            "                        .filter(DB_AnalysisInfoChecker.",
            "                                analysis_info_id == cmd.id)",
            "",
            "                    checkers: Dict[str, Dict[str, API_AnalysisInfoChecker]] = \\",
            "                        defaultdict(dict)",
            "                    for chk in checkers_q.all():",
            "                        analyzer, checker, enabled = chk",
            "                        checkers[analyzer][checker] = API_AnalysisInfoChecker(",
            "                            enabled=enabled)",
            "",
            "                    res.append(ttypes.AnalysisInfo(",
            "                        analyzerCommand=html.escape(command),",
            "                        checkers=checkers))",
            "",
            "        return res",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunHistory(self, run_ids, limit, offset, run_history_filter):",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            res = session.query(RunHistory)",
            "",
            "            res = process_run_history_filter(res, run_ids, run_history_filter)",
            "",
            "            res = res.order_by(RunHistory.time.desc())",
            "",
            "            if limit:",
            "                res = res.limit(limit).offset(offset)",
            "",
            "            results = []",
            "            for history in res:",
            "                analyzer_statistics = {}",
            "                for analyzer_stat in history.analyzer_statistics:",
            "                    analyzer_statistics[analyzer_stat.analyzer_type] = \\",
            "                        ttypes.AnalyzerStatistics(",
            "                            failed=analyzer_stat.failed,",
            "                            successful=analyzer_stat.successful)",
            "",
            "                results.append(RunHistoryData(",
            "                    id=history.id,",
            "                    runId=history.run.id,",
            "                    runName=history.run.name,",
            "                    versionTag=history.version_tag,",
            "                    user=history.user,",
            "                    time=str(history.time),",
            "                    codeCheckerVersion=history.cc_version,",
            "                    analyzerStatistics=analyzer_statistics,",
            "                    description=history.description))",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunHistoryCount(self, run_ids, run_history_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            query = session.query(RunHistory.id)",
            "            query = process_run_history_filter(query,",
            "                                               run_ids,",
            "                                               run_history_filter)",
            "",
            "        return query.count()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReport(self, reportId):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            result = session \\",
            "                .query(Report, File) \\",
            "                .filter(Report.id == reportId) \\",
            "                .join(Checker, Report.checker_id == Checker.id) \\",
            "                .options(contains_eager(Report.checker)) \\",
            "                .outerjoin(File, Report.file_id == File.id) \\",
            "                .limit(1).one_or_none()",
            "",
            "            if not result:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    \"Report \" + str(reportId) + \" not found!\")",
            "",
            "            report, source_file = result",
            "            return ReportData(",
            "                runId=report.run_id,",
            "                bugHash=report.bug_id,",
            "                checkedFile=source_file.filepath,",
            "                checkerMsg=report.checker_message,",
            "                reportId=report.id,",
            "                fileId=source_file.id,",
            "                line=report.line,",
            "                column=report.column,",
            "                analyzerName=report.checker.analyzer_name,",
            "                checkerId=report.checker.checker_name,",
            "                severity=report.checker.severity,",
            "                reviewData=create_review_data(",
            "                    report.review_status,",
            "                    report.review_status_message,",
            "                    report.review_status_author,",
            "                    report.review_status_date,",
            "                    report.review_status_is_in_source),",
            "                detectionStatus=detection_status_enum(report.detection_status),",
            "                detectedAt=str(report.detected_at),",
            "                fixedAt=str(report.fixed_at) if report.fixed_at else None)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getDiffResultsHash(self, run_ids, report_hashes, diff_type,",
            "                           skip_detection_statuses, tag_ids):",
            "        self.__require_view()",
            "",
            "        # FIXME: This getDiffResultsHash() function is returning a set of",
            "        # reports based on what are they compared to in a \"CodeChecker cmd",
            "        # diff\" command. Earlier this function didn't consider false positive",
            "        # and intentional reports as closed reports. The client's behavior also",
            "        # changed from CodeChecker 6.20.0 and this behavior is adapted to the",
            "        # new server behavior. The problem is that the old client works",
            "        # correcly only with the old server. For this reason we are branching",
            "        # based on the client's version. We are having access to the Thrift",
            "        # API version here. The behavior change happend in Thrift API version",
            "        # 6.50.",
            "        client_version = tuple(map(int, self.__client_version.split('.')))",
            "",
            "        if not skip_detection_statuses:",
            "            skip_detection_statuses = [ttypes.DetectionStatus.RESOLVED,",
            "                                       ttypes.DetectionStatus.OFF,",
            "                                       ttypes.DetectionStatus.UNAVAILABLE]",
            "",
            "        # Convert statuses to string.",
            "        skip_statuses_str = [detection_status_str(status)",
            "                             for status in skip_detection_statuses]",
            "",
            "        with DBSession(self._Session) as session:",
            "            if diff_type == DiffType.NEW:",
            "                # In postgresql we can select multiple rows filled with",
            "                # constants by using `unnest` function. In sqlite we have to",
            "                # use multiple UNION ALL.",
            "",
            "                if not report_hashes:",
            "                    return []",
            "",
            "                base_hashes = session.query(Report.bug_id.label('bug_id')) \\",
            "                    .outerjoin(File, Report.file_id == File.id)",
            "",
            "                if client_version >= (6, 50):",
            "                    base_hashes = base_hashes.filter(",
            "                        Report.detection_status.notin_(skip_statuses_str),",
            "                        Report.fixed_at.is_(None))",
            "                    base_hashes = filter_open_reports_in_tags(",
            "                        base_hashes, run_ids, tag_ids)",
            "                else:",
            "                    base_hashes = base_hashes.filter(",
            "                        Report.detection_status.notin_(skip_statuses_str))",
            "                    base_hashes = filter_open_reports_in_tags_old(",
            "                        base_hashes, run_ids, tag_ids)",
            "",
            "                if self._product.driver_name == 'postgresql':",
            "                    new_hashes = select([func.unnest(report_hashes)",
            "                                         .label('bug_id')]) \\",
            "                        .except_(base_hashes).alias('new_bugs')",
            "                    return [res[0] for res in session.query(new_hashes)]",
            "                else:",
            "                    # The maximum number of compound select in sqlite is 500",
            "                    # by default. We increased SQLITE_MAX_COMPOUND_SELECT",
            "                    # limit but when the number of compound select was larger",
            "                    # than 8435 sqlite threw a `Segmentation fault` error.",
            "                    # For this reason we create queries with chunks.",
            "                    new_hashes = []",
            "                    for chunk in util.chunks(",
            "                            iter(report_hashes), SQLITE_MAX_COMPOUND_SELECT):",
            "                        new_hashes_query = union_all(*[",
            "                            select([bindparam('bug_id' + str(i), h)",
            "                                    .label('bug_id')])",
            "                            for i, h in enumerate(chunk)])",
            "                        q = select([new_hashes_query]).except_(base_hashes)",
            "                        new_hashes.extend([res[0] for res in session.query(q)])",
            "",
            "                    return new_hashes",
            "            elif diff_type == DiffType.RESOLVED:",
            "                results = session.query(Report.bug_id)",
            "",
            "                if client_version >= (6, 50):",
            "                    results = results.filter(or_(",
            "                        Report.bug_id.notin_(report_hashes),",
            "                        Report.fixed_at.isnot(None)))",
            "                    results = filter_open_reports_in_tags(",
            "                        results, run_ids, tag_ids)",
            "                else:",
            "                    results = results.filter(",
            "                        Report.bug_id.notin_(report_hashes))",
            "                    results = filter_open_reports_in_tags_old(",
            "                        results, run_ids, tag_ids)",
            "",
            "                return [res[0] for res in results]",
            "            elif diff_type == DiffType.UNRESOLVED:",
            "                results = session.query(Report.bug_id) \\",
            "                    .filter(Report.bug_id.in_(report_hashes))",
            "",
            "                if client_version >= (6, 50):",
            "                    results = results \\",
            "                        .filter(Report.detection_status.notin_(",
            "                            skip_statuses_str)) \\",
            "                        .filter(Report.fixed_at.is_(None))",
            "                    results = filter_open_reports_in_tags(",
            "                        results, run_ids, tag_ids)",
            "                else:",
            "                    results = results.filter(",
            "                        Report.detection_status.notin_(skip_statuses_str))",
            "                    results = filter_open_reports_in_tags_old(",
            "                        results, run_ids, tag_ids)",
            "",
            "                return [res[0] for res in results]",
            "            else:",
            "                return []",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunResults(self, run_ids, limit, offset, sort_types,",
            "                      report_filter, cmp_data, get_details):",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        with DBSession(self._Session) as session:",
            "            results = []",
            "",
            "            # Extending \"reports\" table with report annotation columns.",
            "            #",
            "            # Suppose that we have these tables in the database:",
            "            #",
            "            # reports",
            "            # =================",
            "            # id, checker_id, ...",
            "            # -------------------",
            "            # 1,  123456,     ...",
            "            # 2,  999999,     ...",
            "            #",
            "            # report_annotations",
            "            # =======================",
            "            # report_id, key,  value",
            "            # -----------------------",
            "            # 1,         key1, value1",
            "            # 1,         key2, value2",
            "            # 2,         key1, value3",
            "            #",
            "            # The resulting table should look like this:",
            "            #",
            "            # reports extended",
            "            # ===================================================",
            "            # id, checker_id, ..., annotation_key1, annotation_key2",
            "            # ---------------------------------------------------",
            "            # 1,  123456,     ..., value1,          value2",
            "            # 2,  999999,     ..., value3,          NULL",
            "            #",
            "            # The SQL query which results this table is similar to this:",
            "            #",
            "            # SELECT",
            "            #   <every column in \"reports\" table>,",
            "            #   MAX(CASE WHEN report_annotations.key == <col1> THEN",
            "            #       report_annotations.value END) AS annotation_<col1>",
            "            #   MAX(CASE WHEN report_annotations.key == <col2> THEN",
            "            #       report_annotations.value END) AS annotation_<col2>",
            "            # FROM",
            "            #   reports",
            "            #   LEFT OUTER JOIN report_annotations ON",
            "            #     report_annotations.report_id = reports.id",
            "            # GROUP BY",
            "            #   reports.id;",
            "            #",
            "            # <col1>, <col2>... are the distinct keys in table",
            "            # \"report_annotations\". These are collected in a previous query.",
            "            #",
            "            # Since the \"join\" operation makes a Cartesian product of the two",
            "            # tables, the resulting table contains as many rows for a report as",
            "            # many annotations belong to it. These have to be joined by report",
            "            # ID and this is the reason of the aggregating MAX() functions.",
            "            #",
            "            # TODO: The creation of this extended table should be produced by",
            "            # a helper function and it could be used as a sub-query in every",
            "            # other query which originally works on \"reports\" table.",
            "",
            "            annotation_keys = list(map(",
            "                lambda x: x[0],",
            "                session.query(ReportAnnotations.key).distinct().all()))",
            "",
            "            annotation_cols = OrderedDict()",
            "            for col in annotation_keys:",
            "                annotation_cols[col] = func.max(sqlalchemy.case([(",
            "                    ReportAnnotations.key == col,",
            "                    ReportAnnotations.value)])).label(f\"annotation_{col}\")",
            "",
            "            if report_filter.isUnique:",
            "                # A report annotation filter cannot be set in WHERE clause if",
            "                # we use annotation parameters in aggregate functions to",
            "                # create a pivot table. Instead of filtering report",
            "                # annotations in WHERE clause, we should use HAVING clause",
            "                # only for filtering aggregate functions.",
            "                # TODO: Fixing report annotation filter in every report server",
            "                # endpoint function.",
            "                annotations_backup = report_filter.annotations",
            "                report_filter.annotations = None",
            "                filter_expression, join_tables = process_report_filter(",
            "                    session, run_ids, report_filter, cmp_data)",
            "",
            "                sort_types, sort_type_map, order_type_map = \\",
            "                    get_sort_map(sort_types, True)",
            "",
            "                # TODO: Create a helper function for common section of unique",
            "                # and non unique modes.",
            "                sub_query = session.query(Report,",
            "                                          File.filename,",
            "                                          Checker.analyzer_name,",
            "                                          Checker.checker_name,",
            "                                          Checker.severity,",
            "                                          func.row_number().over(",
            "                                            partition_by=Report.bug_id,",
            "                                            order_by=desc(Report.id)",
            "                                          ).label(\"row_num\"),",
            "                                          *annotation_cols.values()) \\",
            "                                   .join(Checker,",
            "                                         Report.checker_id == Checker.id) \\",
            "                                   .options(contains_eager(Report.checker)) \\",
            "                                   .outerjoin(File,",
            "                                              Report.file_id == File.id) \\",
            "                                   .outerjoin(ReportAnnotations,",
            "                                              Report.id ==",
            "                                              ReportAnnotations.report_id)",
            "",
            "                sub_query = apply_report_filter(sub_query,",
            "                                                filter_expression,",
            "                                                join_tables,",
            "                                                [File, Checker])",
            "",
            "                sub_query = sub_query.group_by(Report.id, File.id, Checker.id)",
            "",
            "                if annotations_backup:",
            "                    annotations = defaultdict(list)",
            "                    for annotation in annotations_backup:",
            "                        annotations[annotation.first].append(annotation.second)",
            "",
            "                    OR = []",
            "                    for key, values in annotations.items():",
            "                        OR.extend([annotation_cols[key].ilike(conv(v))",
            "                                   for v in values])",
            "                    sub_query = sub_query.having(or_(*OR))",
            "",
            "                sub_query = sort_results_query(sub_query,",
            "                                               sort_types,",
            "                                               sort_type_map,",
            "                                               order_type_map)",
            "",
            "                sub_query = sub_query.subquery().alias()",
            "",
            "                q = session.query(sub_query) \\",
            "                           .filter(sub_query.c.row_num == 1) \\",
            "                           .limit(limit).offset(offset)",
            "",
            "                QueryResult = namedtuple('QueryResult', sub_query.c.keys())",
            "                query_result = [QueryResult(*row) for row in q.all()]",
            "",
            "                # Get report details if it is required.",
            "                report_details = {}",
            "                if get_details:",
            "                    report_ids = [r.id for r in query_result]",
            "                    report_details = get_report_details(session, report_ids)",
            "",
            "                for row in query_result:",
            "                    annotations = {",
            "                        k: v for k, v in zip(",
            "                            annotation_keys,",
            "                            [getattr(row, 'annotation_testcase', None),",
            "                             getattr(row, 'annotation_timestamp', None)]",
            "                            ) if v is not None}",
            "",
            "                    review_data = create_review_data(",
            "                        row.review_status,",
            "                        row.review_status_message,",
            "                        row.review_status_author,",
            "                        row.review_status_date,",
            "                        row.review_status_is_in_source)",
            "",
            "                    results.append(",
            "                        ReportData(runId=row.run_id,",
            "                                   bugHash=row.bug_id,",
            "                                   checkedFile=row.filename,",
            "                                   checkerMsg=row.checker_message,",
            "                                   reportId=row.id,",
            "                                   fileId=row.file_id,",
            "                                   line=row.line,",
            "                                   column=row.column,",
            "                                   analyzerName=row.analyzer_name,",
            "                                   checkerId=row.checker_name,",
            "                                   severity=row.severity,",
            "                                   reviewData=review_data,",
            "                                   detectionStatus=detection_status_enum(",
            "                                    row.detection_status),",
            "                                   detectedAt=str(row.detected_at),",
            "                                   fixedAt=str(row.fixed_at),",
            "                                   bugPathLength=row.path_length,",
            "                                   details=report_details.get(row.id),",
            "                                   annotations=annotations))",
            "            else:  # not is_unique",
            "                filter_expression, join_tables = process_report_filter(",
            "                    session, run_ids, report_filter, cmp_data,",
            "                    keep_all_annotations=True)",
            "",
            "                sort_types, sort_type_map, order_type_map = \\",
            "                    get_sort_map(sort_types)",
            "",
            "                q = session.query(Report,",
            "                                  File.filepath,",
            "                                  *annotation_cols.values()) \\",
            "                    .join(Checker,",
            "                          Report.checker_id == Checker.id) \\",
            "                    .options(contains_eager(Report.checker)) \\",
            "                    .outerjoin(File,",
            "                               Report.file_id == File.id) \\",
            "                    .outerjoin(",
            "                        ReportAnnotations,",
            "                        Report.id == ReportAnnotations.report_id)",
            "",
            "                # Grouping by \"reports.id\" is described at the beginning of",
            "                # this function. Grouping by \"files.id\" is necessary, because",
            "                # \"files\" table is joined for gathering file names belonging to",
            "                # the given report. According to SQL syntax if there is a group",
            "                # by report IDs then files should also be either grouped or an",
            "                # aggregate function must be applied on them. The same applies",
            "                # to the \"checkers\" table.",
            "                q = q.group_by(Report.id, File.id, Checker.id)",
            "",
            "                # The \"Checker\" entity is eagerly loaded for each \"Report\" as",
            "                # there is a guaranteed FOREIGN KEY ... NOT NULL relationship",
            "                # to a valid entity. Because of this, letting \"join_tables\"",
            "                # add \"Checker\" here is actually ill-formed, as it would",
            "                # result in queries that ambiguously refer to the same table.",
            "                q = apply_report_filter(q, filter_expression, join_tables,",
            "                                        [File, Checker])",
            "                q = sort_results_query(q,",
            "                                       sort_types,",
            "                                       sort_type_map,",
            "                                       order_type_map)",
            "",
            "                # Most queries are using paging of reports due their great",
            "                # number. This is implemented by LIMIT and OFFSET in the SQL",
            "                # queries. However, if there is no ordering in the query, then",
            "                # the reports in different pages may overlap. This ordering",
            "                # prevents it.",
            "                q = q.order_by(Report.id)",
            "",
            "                if report_filter.annotations is not None:",
            "                    annotations = defaultdict(list)",
            "                    for annotation in report_filter.annotations:",
            "                        annotations[annotation.first].append(annotation.second)",
            "",
            "                    OR = []",
            "                    for key, values in annotations.items():",
            "                        OR.extend([annotation_cols[key].ilike(conv(v))",
            "                                   for v in values])",
            "                    q = q.having(or_(*OR))",
            "",
            "                q = q.limit(limit).offset(offset)",
            "",
            "                query_result = q.all()",
            "",
            "                # Get report details if it is required.",
            "                report_details = {}",
            "                if get_details:",
            "                    report_ids = [r[0].id for r in query_result]",
            "                    report_details = get_report_details(session, report_ids)",
            "",
            "                for row in query_result:",
            "                    report, filepath = row[0], row[1]",
            "                    annotations = {",
            "                        k: v for k, v in zip(annotation_keys, row[2:])",
            "                        if v is not None}",
            "",
            "                    review_data = create_review_data(",
            "                        report.review_status,",
            "                        report.review_status_message,",
            "                        report.review_status_author,",
            "                        report.review_status_date,",
            "                        report.review_status_is_in_source)",
            "",
            "                    results.append(",
            "                        ReportData(runId=report.run_id,",
            "                                   bugHash=report.bug_id,",
            "                                   checkedFile=filepath,",
            "                                   checkerMsg=report.checker_message,",
            "                                   reportId=report.id,",
            "                                   fileId=report.file_id,",
            "                                   line=report.line,",
            "                                   column=report.column,",
            "                                   analyzerName=report.checker.analyzer_name,",
            "                                   checkerId=report.checker.checker_name,",
            "                                   severity=report.checker.severity,",
            "                                   reviewData=review_data,",
            "                                   detectionStatus=detection_status_enum(",
            "                                       report.detection_status),",
            "                                   detectedAt=str(report.detected_at),",
            "                                   fixedAt=str(report.fixed_at) if",
            "                                   report.fixed_at else None,",
            "                                   bugPathLength=report.path_length,",
            "                                   details=report_details.get(report.id),",
            "                                   annotations=annotations))",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReportAnnotations(self, run_ids, report_filter, cmp_data):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "                extended_table = extended_table.add_columns(",
            "                    ReportAnnotations.key.label('annotations_key'),",
            "                    ReportAnnotations.value.label('annotations_value')",
            "                ).group_by(ReportAnnotations.key, ReportAnnotations.value)",
            "",
            "                extended_table = extended_table.subquery()",
            "",
            "                result = session.query(extended_table.c.annotations_value) \\",
            "                    .distinct() \\",
            "                    .filter(",
            "                        *(extended_table.c.annotations_key == annotation.first",
            "                          for annotation in report_filter.annotations)) \\",
            "                    .all()",
            "            else:",
            "                extended_table = extended_table.subquery()",
            "",
            "                result = session.query(ReportAnnotations.value) \\",
            "                    .distinct() \\",
            "                    .join(",
            "                        extended_table,",
            "                        ReportAnnotations.report_id == extended_table.c.id) \\",
            "                    .all()",
            "",
            "        return list(map(lambda x: x[0], result))",
            "",
            "    @timeit",
            "    def getRunReportCounts(self, run_ids, report_filter, limit, offset):",
            "        \"\"\"",
            "          Count the results separately for multiple runs.",
            "          If an empty run id list is provided the report",
            "          counts will be calculated for all of the available runs.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = []",
            "",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter)",
            "",
            "            reports_subq = session.query(Report.bug_id, Report.run_id)",
            "            reports_subq = apply_report_filter(",
            "                reports_subq, filter_expression, join_tables)",
            "",
            "            if report_filter.annotations is not None:",
            "                reports_subq = reports_subq.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                reports_subq = reports_subq.group_by(Report.id)",
            "",
            "            reports_subq = reports_subq.subquery()",
            "",
            "            if report_filter is not None and report_filter.isUnique:",
            "                count_col = func.count(reports_subq.c.bug_id.distinct())",
            "            else:",
            "                count_col = func.count(literal_column('*'))",
            "",
            "            q = session.query(Run.id, func.max(Run.name), count_col) \\",
            "                .select_from(reports_subq) \\",
            "                .join(Run, Run.id == reports_subq.c.run_id) \\",
            "                .group_by(Run.id) \\",
            "                .order_by(Run.name)",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            for run_id, run_name, count in q:",
            "                report_count = RunReportCount(runId=run_id,",
            "                                              name=run_name,",
            "                                              reportCount=count)",
            "                results.append(report_count)",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunResultCount(self, run_ids, report_filter, cmp_data):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(Report.bug_id.distinct())",
            "            else:",
            "                q = session.query(Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                q = q.outerjoin(ReportAnnotations,",
            "                                ReportAnnotations.report_id == Report.id)",
            "                q = q.group_by(Report.id)",
            "",
            "            q = apply_report_filter(q, filter_expression, join_tables)",
            "",
            "            report_count = q.count()",
            "            if report_count is None:",
            "                report_count = 0",
            "",
            "            return report_count",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReportDetails(self, reportId):",
            "        \"\"\"",
            "        Parameters:",
            "         - reportId",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            return get_report_details(session, [reportId])[reportId]",
            "",
            "    def _setReviewStatus(self, session, report_hash, status,",
            "                         message, date=None):",
            "        \"\"\"",
            "        This function sets the review status of all the reports of a",
            "        given hash. This is the implementation of addReviewStatusRule(),",
            "        but it is also extended with a session parameter which represents a",
            "        database transaction. This is needed because during storage a specific",
            "        session object has to be used.",
            "        \"\"\"",
            "        review_status = session.query(ReviewStatus).get(report_hash)",
            "        if review_status is None:",
            "            review_status = ReviewStatus()",
            "            review_status.bug_hash = report_hash",
            "",
            "        old_status = review_status.status or \\",
            "            review_status_str(ttypes.ReviewStatus.UNREVIEWED)",
            "        old_msg = review_status.message or None",
            "",
            "        new_status = review_status_str(status)",
            "        new_user = self._get_username()",
            "        new_message = message.encode('utf8') if message else b''",
            "",
            "        # Review status is a shared table among runs. When multiple runs",
            "        # are stored in parallel, there may be a race condition in updating",
            "        # review status fields. The most common reason of deadlocks is",
            "        # changing only the date to current date. This condition checks if",
            "        # something else is also changed other than dates.",
            "        # We assume that report status in source code comments belong to",
            "        # the first user who stored the reports. If another user stores the",
            "        # same project with same report status then we don't change it.",
            "        if (old_status, old_msg) == (new_status, new_message):",
            "            return None",
            "",
            "        review_status.status = new_status",
            "        review_status.author = new_user",
            "        review_status.message = new_message",
            "        review_status.date = date or datetime.now()",
            "        session.add(review_status)",
            "",
            "        # Create a system comment if the review status or the message",
            "        # is changed.",
            "        old_review_status = old_status.capitalize()",
            "        new_review_status = review_status.status.capitalize()",
            "        if message:",
            "            system_comment_msg = \\",
            "                f'rev_st_changed_msg {old_review_status} ' \\",
            "                f'{new_review_status} {shlex.quote(message)}'",
            "        else:",
            "            system_comment_msg = \\",
            "                f'rev_st_changed {old_review_status} {new_review_status}'",
            "",
            "        system_comment = self.__add_comment(review_status.bug_hash,",
            "                                            system_comment_msg,",
            "                                            CommentKindValue.SYSTEM,",
            "                                            review_status.date)",
            "        session.add(system_comment)",
            "",
            "        # False positive and intentional reports are considered closed, so",
            "        # their \"fix date\" is set. The reports are reopened when they become",
            "        # unreviewed or confirmed again. Don't change \"fix date\" for closed",
            "        # report which remain closed.",
            "        if review_status.status in [\"false_positive\", \"intentional\"]:",
            "            session \\",
            "                .query(Report) \\",
            "                .filter(Report.bug_id == report_hash) \\",
            "                .filter(Report.detection_status.in_([",
            "                    \"unresolved\", \"new\", \"reopened\"])) \\",
            "                .filter(Report.review_status.notin_(",
            "                    [\"false_positive\", \"intentional\"])) \\",
            "                .filter(Report.review_status_is_in_source.is_(False)) \\",
            "                .update(",
            "                    {\"fixed_at\": review_status.date},",
            "                    synchronize_session=False)",
            "        else:",
            "            reports = session \\",
            "                .query(Report) \\",
            "                .filter(",
            "                    Report.bug_id == report_hash,",
            "                    Report.detection_status.in_([",
            "                        \"unresolved\", \"new\", \"reopened\"]),",
            "                    Report.review_status.in_([",
            "                        \"false_positive\", \"intentional\"]))",
            "",
            "            session \\",
            "                .query(Report) \\",
            "                .filter(Report.id.in_(",
            "                    map(lambda report: report.id, reports))) \\",
            "                .filter(Report.review_status_is_in_source.is_(False)) \\",
            "                .update({\"fixed_at\": None}, synchronize_session=False)",
            "",
            "        session \\",
            "            .query(Report) \\",
            "            .filter(Report.review_status_is_in_source.is_(False)) \\",
            "            .filter(Report.bug_id == report_hash) \\",
            "            .update({",
            "                'review_status': review_status.status,",
            "                'review_status_author': review_status.author,",
            "                'review_status_message': review_status.message,",
            "                'review_status_date': review_status.date})",
            "",
            "        session.flush()",
            "",
            "        return None",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def isReviewStatusChangeDisabled(self):",
            "        \"\"\"",
            "        Return True if review status change is disabled.",
            "        \"\"\"",
            "        with DBSession(self._config_database) as session:",
            "            product = session.query(Product).get(self._product.id)",
            "            return product.is_review_status_change_disabled",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def changeReviewStatus(self, report_id, status, message):",
            "        \"\"\"",
            "        Change the review status of a report by report id.",
            "        \"\"\"",
            "        self.__require_permission([permissions.PRODUCT_ACCESS,",
            "                                   permissions.PRODUCT_STORE])",
            "",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            if report:",
            "                # False positive and intentional reports are considered closed,",
            "                # so their \"fix date\" is set. The reports are reopened when",
            "                # they become unreviewed or confirmed again.",
            "                # Don't change \"fix date\" for closed",
            "                # report which remain closed.",
            "                if status in [\"false_positive\", \"intentional\"]:",
            "                    if report.detection_status in [",
            "                            \"unresolved\", \"new\", \"reopened\"]\\",
            "                        and report.review_status not in [",
            "                            \"false_positive\", \"intentional\"]:",
            "                        session.query(Report).filter(",
            "                            Report.id == report_id).update(",
            "                                {\"fixed_at\": datetime.now()})",
            "                elif report.detection_status in [",
            "                    \"unresolved\", \"new\", \"reopened\"]\\",
            "                    and report.review_status in [",
            "                        \"false_positive\", \"intentional\"]:",
            "                    session.query(Report).filter(",
            "                        Report.id == report_id).update({\"fixed_at\": None})",
            "",
            "                session.query(Report).filter(Report.id == report_id).update({",
            "                        'review_status': review_status_str(status),",
            "                        'review_status_author': self._get_username(),",
            "                        'review_status_message': bytes(message, 'utf-8'),",
            "                        'review_status_date': datetime.now()",
            "                        })",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    \"No report found in the database.\")",
            "            session.commit()",
            "",
            "            LOG.info(\"Review status of report '%s' was changed to '%s' by %s.\",",
            "                     report_id, review_status_str(status),",
            "                     self._get_username())",
            "",
            "        return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReviewStatusRules(self, rule_filter, sort_mode, limit, offset):",
            "        self.__require_view()",
            "        if not sort_mode:",
            "            sort_mode = ReviewStatusRuleSortMode(",
            "                type=ReviewStatusRuleSortType.DATE,",
            "                ord=Order.DESC)",
            "",
            "        result = []",
            "",
            "        # To avoid modifiying the collection due to chunking",
            "        rule_filter_copy = deepcopy(rule_filter)",
            "",
            "        def getRules(reportHashes=None):",
            "            if rule_filter and reportHashes:",
            "                rule_filter_copy.reportHashes = reportHashes",
            "            q = get_rs_rule_query(session, rule_filter_copy, sort_mode)",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            for review_status, associated_report_count in q:",
            "                result.append(ReviewStatusRule(",
            "                    reportHash=review_status.bug_hash,",
            "                    reviewData=create_review_data(",
            "                        review_status.status,",
            "                        review_status.message,",
            "                        review_status.author,",
            "                        review_status.date,",
            "                        False),",
            "                    associatedReportCount=associated_report_count))",
            "            return result",
            "",
            "        with DBSession(self._Session) as session:",
            "            if rule_filter and rule_filter.reportHashes:",
            "                # Diffing with a large ammount of report hashes passed in the",
            "                # filter (60K) caused a hanging report diffing.",
            "                # The probable cause of this is the ILIKE matching in the",
            "                # underlying logic. Chunking the request solves this issue.",
            "                for hash_chunk in util.chunks(",
            "                        rule_filter.reportHashes,",
            "                        SQLITE_MAX_VARIABLE_NUMBER):",
            "                    getRules(hash_chunk)",
            "                return result",
            "            else:",
            "                return getRules()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReviewStatusRulesCount(self, rule_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            q = get_rs_rule_query(session, rule_filter)",
            "            return q.count()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeReviewStatusRules(self, rule_filter):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            q = get_rs_rule_query(session, rule_filter)",
            "            for review_status, _ in q:",
            "                session.delete(review_status)",
            "",
            "                # Reports become unreviewed when the corresponding review",
            "                # status rule is removed and the report doesn't have a review",
            "                # status as source code comment.",
            "                session \\",
            "                    .query(Report) \\",
            "                    .filter(Report.bug_id == review_status.bug_hash) \\",
            "                    .filter(Report.review_status_is_in_source.is_(False)) \\",
            "                    .update({",
            "                        'review_status': 'unreviewed',",
            "                        'review_status_author': None,",
            "                        'review_status_message': None,",
            "                        'review_status_date': None,",
            "                        'fixed_at': None})",
            "",
            "            session.commit()",
            "",
            "            LOG.info(\"Review status rules were removed based on filter '%s' by\"",
            "                     \"'%s'.\", rule_filter, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addReviewStatusRule(self, report_hash, review_status, message):",
            "        self.__require_permission([permissions.PRODUCT_ACCESS,",
            "                                   permissions.PRODUCT_STORE])",
            "",
            "        if self.isReviewStatusChangeDisabled():",
            "            msg = \"Review status change is disabled!\"",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "        with DBSession(self._Session) as session:",
            "            self._setReviewStatus(",
            "                session, report_hash, review_status, message)",
            "            session.commit()",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getComments(self, report_id):",
            "        \"\"\"",
            "            Return the list of comments for the given bug.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            if report:",
            "                result = []",
            "",
            "                comments = session.query(Comment) \\",
            "                    .filter(Comment.bug_hash == report.bug_id) \\",
            "                    .order_by(Comment.created_at.desc()) \\",
            "                    .all()",
            "",
            "                for comment in comments:",
            "                    message = get_comment_msg(comment)",
            "                    result.append(CommentData(",
            "                        comment.id,",
            "                        comment.author,",
            "                        message,",
            "                        str(comment.created_at),",
            "                        comment_kind_to_thrift_type(comment.kind)))",
            "",
            "                return result",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f'Report id {report_id} was not found in the database.')",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCommentCount(self, report_id):",
            "        \"\"\"",
            "            Return the number of comments for the given bug.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            commentCount = 0",
            "            if report:",
            "                commentCount = session.query(Comment) \\",
            "                    .filter(Comment.bug_hash == report.bug_id) \\",
            "                    .count()",
            "",
            "            return commentCount",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addComment(self, report_id, comment_data):",
            "        \"\"\" Add new comment for the given bug. \"\"\"",
            "        self.__require_access()",
            "",
            "        if not comment_data.message or not comment_data.message.strip():",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                'The comment message can not be empty!')",
            "",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            if report:",
            "                comment = self.__add_comment(report.bug_id,",
            "                                             comment_data.message)",
            "                session.add(comment)",
            "                session.commit()",
            "",
            "                return True",
            "            else:",
            "                msg = f'Report id {report_id} was not found in the database.'",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def updateComment(self, comment_id, content):",
            "        \"\"\"",
            "            Update the given comment message with new content. We allow",
            "            comments to be updated by it's original author only, except for",
            "            Anyonymous comments that can be updated by anybody.",
            "        \"\"\"",
            "        self.__require_access()",
            "",
            "        if not content.strip():",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                'The comment message can not be empty!')",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            user = self._get_username()",
            "",
            "            comment = session.query(Comment).get(comment_id)",
            "            if comment:",
            "                if comment.author not in ('Anonymous', user):",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                        'Unathorized comment modification!')",
            "",
            "                # Create system comment if the message is changed.",
            "                message = comment.message.decode('utf-8')",
            "                if message != content:",
            "                    system_comment_msg = \\",
            "                        f'comment_changed {shlex.quote(message)} ' \\",
            "                        f'{shlex.quote(content)}'",
            "",
            "                    system_comment = \\",
            "                        self.__add_comment(comment.bug_hash,",
            "                                           system_comment_msg,",
            "                                           CommentKindValue.SYSTEM)",
            "                    session.add(system_comment)",
            "",
            "                comment.message = content.encode('utf-8')",
            "                session.add(comment)",
            "",
            "                session.commit()",
            "                return True",
            "            else:",
            "                msg = f'Comment id {comment_id} was not found in the database.'",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeComment(self, comment_id):",
            "        \"\"\"",
            "            Remove the comment. We allow comments to be removed by it's",
            "            original author only, except for Anyonymous comments that can be",
            "            updated by anybody.",
            "        \"\"\"",
            "        self.__require_access()",
            "",
            "        user = self._get_username()",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            comment = session.query(Comment).get(comment_id)",
            "            if comment:",
            "                if comment.author not in ('Anonymous', user):",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                        'Unathorized comment modification!')",
            "                session.delete(comment)",
            "                session.commit()",
            "",
            "                LOG.info(\"Comment '%s...' was removed from bug hash '%s' by \"",
            "                         \"'%s'.\", comment.message[:10], comment.bug_hash,",
            "                         self._get_username())",
            "",
            "                return True",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f'Comment id {comment_id} was not found in the database.')",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerDoc(self, _):",
            "        \"\"\"",
            "        Parameters:",
            "         - checkerId",
            "        \"\"\"",
            "",
            "        return \"\"",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerLabels(",
            "        self,",
            "        checkers: List[ttypes.Checker]",
            "    ) -> List[List[str]]:",
            "        \"\"\" Return the list of labels to each checker. \"\"\"",
            "        labels = []",
            "        for checker in checkers:",
            "            analyzer_name = None if not checker.analyzerName \\",
            "                else str(checker.analyzerName)",
            "            analyzer_name = analyzer_name \\",
            "                if (analyzer_name and analyzer_name.lower() != \"unknown\") \\",
            "                else None",
            "",
            "            labels.append(list(map(",
            "                lambda x: f'{x[0]}:{x[1]}',",
            "                self._context.checker_labels.labels_of_checker(",
            "                    checker.checkerId, analyzer_name))))",
            "",
            "        return labels",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getSourceFileData(self, fileId, fileContent, encoding):",
            "        \"\"\"",
            "        Parameters:",
            "         - fileId",
            "         - fileContent",
            "         - enum Encoding",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            sourcefile = session.query(File).get(fileId)",
            "",
            "            if sourcefile is None:",
            "                return SourceFileData()",
            "",
            "            content_hash = sourcefile.content_hash",
            "            cont = session \\",
            "                .query(FileContent.content, FileContent.blame_info) \\",
            "                .filter(FileContent.content_hash == content_hash) \\",
            "                .one_or_none()",
            "",
            "            source_file_data = SourceFileData(",
            "                fileId=sourcefile.id,",
            "                filePath=sourcefile.filepath,",
            "                hasBlameInfo=bool(cont.blame_info),",
            "                remoteUrl=get_commit_url(sourcefile.remote_url,",
            "                                         self._context.git_commit_urls),",
            "                trackingBranch=sourcefile.tracking_branch)",
            "",
            "            if fileContent:",
            "                source = zlib.decompress(cont.content)",
            "",
            "                if encoding == Encoding.BASE64:",
            "                    source = base64.b64encode(source)",
            "",
            "                source_file_data.fileContent = source.decode(",
            "                    'utf-8', errors='ignore')",
            "",
            "            return source_file_data",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getBlameInfo(self, fileId):",
            "        \"\"\" Get blame information for the given file. \"\"\"",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            sourcefile = session.query(File).get(fileId)",
            "",
            "            if sourcefile is None:",
            "                return BlameInfo()",
            "",
            "            cont = session \\",
            "                .query(FileContent.blame_info) \\",
            "                .filter(FileContent.content_hash == sourcefile.content_hash) \\",
            "                .one_or_none()",
            "",
            "            if not cont or not cont.blame_info:",
            "                return BlameInfo()",
            "",
            "            try:",
            "                blame_info = json.loads(",
            "                    zlib.decompress(cont.blame_info).decode(",
            "                        'utf-8', errors='ignore'))",
            "",
            "                commits = {",
            "                    commitHash: Commit(",
            "                        author=CommitAuthor(",
            "                            name=commit[\"author\"][\"name\"],",
            "                            email=commit[\"author\"][\"email\"]),",
            "                        summary=commit[\"summary\"],",
            "                        message=html.escape(commit[\"message\"]),",
            "                        committedDateTime=commit[\"committed_datetime\"],",
            "                    )",
            "                    for commitHash, commit in blame_info[\"commits\"].items()",
            "                }",
            "",
            "                blame_data = [BlameData(",
            "                    startLine=b[\"from\"],",
            "                    endLine=b[\"to\"],",
            "                    commitHash=b[\"commit\"]) for b in blame_info[\"blame\"]]",
            "",
            "                return BlameInfo(",
            "                    commits=commits,",
            "                    blame=blame_data)",
            "            except Exception:",
            "                # pylint: disable=raise-missing-from",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    \"Failed to get blame information for file id: \" + fileId)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getLinesInSourceFileContents(self, lines_in_files_requested, encoding):",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            res = defaultdict(lambda: defaultdict(str))",
            "",
            "            # This will contain all the lines for the given fileId",
            "            contents_to_file_id = defaultdict(list)",
            "            # The goal of the chunking is not for achieving better performace",
            "            # but to be compatible with SQLITE dbms with larger report counts,",
            "            # with larger report data.",
            "            for chunk in util.chunks(",
            "                    lines_in_files_requested, SQLITE_MAX_VARIABLE_NUMBER):",
            "                contents = session.query(FileContent.content, File.id) \\",
            "                        .join(",
            "                            File,",
            "                            FileContent.content_hash == File.content_hash) \\",
            "                        .filter(File.id.in_(",
            "                                [line.fileId if line.fileId is not None",
            "                                    else LOG.warning(",
            "                                        \"File content requested \"",
            "                                        f\"without fileId {line.fileId}\")",
            "                                    for line in chunk])) \\",
            "                        .all()",
            "                for content in contents:",
            "                    lines = zlib.decompress(",
            "                        content.content).decode('utf-8', 'ignore').split('\\n')",
            "                    contents_to_file_id[content.id] = lines",
            "",
            "            for files in lines_in_files_requested:",
            "                for line in files.lines:",
            "                    lines = contents_to_file_id[files.fileId]",
            "                    content = '' if len(lines) < line else lines[line - 1]",
            "                    if encoding == Encoding.BASE64:",
            "                        content = convert.to_b64(content)",
            "                    res[files.fileId][line] = content",
            "",
            "            return res",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                         offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = []",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session \\",
            "                .query(Report.bug_id,",
            "                       Checker.checker_name,",
            "                       Checker.severity) \\",
            "                .join(Checker,",
            "                      Report.checker_id == Checker.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables, [Checker])",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    func.max(extended_table.c.checker_name)",
            "                        .label(\"checker_name\"),",
            "                    func.max(extended_table.c.severity).label(\"severity\"),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(",
            "                    extended_table.c.checker_name,",
            "                    extended_table.c.severity,",
            "                    func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                unique_checker_q = session.query(q.c.checker_name,",
            "                                                 func.max(q.c.severity),",
            "                                                 func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.checker_name) \\",
            "                    .order_by(q.c.checker_name)",
            "            else:",
            "                unique_checker_q = q.group_by(extended_table.c.checker_name,",
            "                                              extended_table.c.severity) \\",
            "                    .order_by(extended_table.c.checker_name)",
            "",
            "            if limit:",
            "                unique_checker_q = unique_checker_q.limit(limit).offset(offset)",
            "",
            "            for name, severity, count in unique_checker_q:",
            "                checker_count = CheckerCount(name=name,",
            "                                             severity=severity,",
            "                                             count=count)",
            "                results.append(checker_count)",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerStatusVerificationDetails(self, run_ids, report_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            max_run_histories = session.query(",
            "                RunHistory.run_id,",
            "                func.max(RunHistory.id).label('max_run_history_id'),",
            "            ) \\",
            "                .filter(RunHistory.run_id.in_(run_ids) if run_ids else True) \\",
            "                .group_by(RunHistory.run_id)",
            "",
            "            run_id_expression = get_run_id_expression(session, report_filter)",
            "",
            "            subquery = (",
            "                session.query(",
            "                    run_id_expression,",
            "                    Checker.id.label(\"checker_id\"),",
            "                    Checker.checker_name,",
            "                    Checker.analyzer_name,",
            "                    Checker.severity,",
            "                    Report.bug_id,",
            "                    Report.detection_status,",
            "                    Report.review_status,",
            "                )",
            "                .join(RunHistory)",
            "                .join(AnalysisInfo, RunHistory.analysis_info)",
            "                .join(DB_AnalysisInfoChecker, (",
            "                    (AnalysisInfo.id ==",
            "                     DB_AnalysisInfoChecker.analysis_info_id)",
            "                    & (DB_AnalysisInfoChecker.enabled.is_(True))))",
            "                .join(Checker,",
            "                      DB_AnalysisInfoChecker.checker_id == Checker.id)",
            "                .outerjoin(Report, ((Checker.id == Report.checker_id)",
            "                                    & (Run.id == Report.run_id)))",
            "                .filter(RunHistory.id == max_run_histories.subquery()",
            "                        .c.max_run_history_id)",
            "            )",
            "",
            "            if report_filter.isUnique:",
            "                subquery = subquery.group_by(",
            "                    Checker.id,",
            "                    Checker.checker_name,",
            "                    Checker.analyzer_name,",
            "                    Checker.severity,",
            "                    Report.bug_id,",
            "                    Report.detection_status,",
            "                    Report.review_status",
            "                )",
            "",
            "            subquery = subquery.subquery()",
            "",
            "            is_enabled_case = get_is_enabled_case(subquery)",
            "            is_opened_case = get_is_opened_case(subquery)",
            "",
            "            query = (",
            "                session.query(",
            "                    subquery.c.checker_id,",
            "                    subquery.c.checker_name,",
            "                    subquery.c.analyzer_name,",
            "                    subquery.c.severity,",
            "                    subquery.c.run_id,",
            "                    is_enabled_case.label(\"isEnabled\"),",
            "                    is_opened_case.label(\"isOpened\"),",
            "                    func.count(subquery.c.bug_id)",
            "                )",
            "                .group_by(",
            "                    subquery.c.checker_id,",
            "                    subquery.c.checker_name,",
            "                    subquery.c.analyzer_name,",
            "                    subquery.c.severity,",
            "                    subquery.c.run_id,",
            "                    is_enabled_case,",
            "                    is_opened_case",
            "                )",
            "            )",
            "",
            "            checker_stats = {}",
            "            all_run_id = [runId for runId, _ in max_run_histories.all()]",
            "            for checker_id, \\",
            "                checker_name, \\",
            "                analyzer_name, \\",
            "                severity, \\",
            "                run_id_list, \\",
            "                is_enabled, \\",
            "                is_opened, \\",
            "                cnt \\",
            "                    in query.all():",
            "",
            "                checker_stat = checker_stats.get(",
            "                    checker_id,",
            "                    CheckerStatusVerificationDetail(",
            "                        checkerName=checker_name,",
            "                        analyzerName=analyzer_name,",
            "                        enabled=[],",
            "                        disabled=all_run_id.copy(),",
            "                        severity=severity,",
            "                        closed=0,",
            "                        outstanding=0",
            "                    ))",
            "",
            "                if is_enabled:",
            "                    for r in (run_id_list.split(\",\")",
            "                              if isinstance(run_id_list, str)",
            "                              else [run_id_list]):",
            "                        run_id = int(r)",
            "                        if run_id not in checker_stat.enabled:",
            "                            checker_stat.enabled.append(run_id)",
            "                        if run_id in checker_stat.disabled:",
            "                            checker_stat.disabled.remove(run_id)",
            "",
            "                if is_enabled and is_opened:",
            "                    checker_stat.outstanding += cnt",
            "                else:",
            "                    checker_stat.closed += cnt",
            "",
            "                checker_stats[checker_id] = checker_stat",
            "",
            "            return checker_stats",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalyzerNameCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                              offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session \\",
            "                .query(Checker.analyzer_name,",
            "                       Report.bug_id) \\",
            "                .join(Checker,",
            "                      Report.checker_id == Checker.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables, [Checker])",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(func.max(",
            "                    extended_table.c.analyzer_name).label('analyzer_name'),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(extended_table.c.analyzer_name,",
            "                                  func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                analyzer_name_q = session.query(q.c.analyzer_name,",
            "                                                func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.analyzer_name) \\",
            "                    .order_by(q.c.analyzer_name)",
            "            else:",
            "                analyzer_name_q = q.group_by(extended_table.c.analyzer_name) \\",
            "                    .order_by(extended_table.c.analyzer_name)",
            "",
            "            if limit:",
            "                analyzer_name_q = analyzer_name_q.limit(limit).offset(offset)",
            "",
            "            for name, count in analyzer_name_q:",
            "                results[name] = count",
            "",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getSeverityCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session \\",
            "                .query(Report.bug_id,",
            "                       Checker.severity) \\",
            "                .join(Checker,",
            "                      Report.checker_id == Checker.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables, [Checker])",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    func.max(extended_table.c.severity).label(\"severity\"),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(extended_table.c.severity,",
            "                                  func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                severities = session.query(q.c.severity,",
            "                                           func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.severity)",
            "            else:",
            "                severities = q.group_by(extended_table.c.severity)",
            "",
            "            results = dict(severities)",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerMsgCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                            offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.checker_message,",
            "                Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    func.max(extended_table.c.checker_message).label(",
            "                        'checker_message'),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(extended_table.c.checker_message,",
            "                                  func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                checker_messages = session.query(q.c.checker_message,",
            "                                                 func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.checker_message) \\",
            "                    .order_by(q.c.checker_message)",
            "            else:",
            "                checker_messages = q \\",
            "                    .group_by(extended_table.c.checker_message) \\",
            "                    .order_by(extended_table.c.checker_message)",
            "",
            "            if limit:",
            "                checker_messages = checker_messages.limit(limit).offset(offset)",
            "",
            "            results = dict(checker_messages.all())",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReportStatusCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.review_status,",
            "                Report.detection_status,",
            "                Report.bug_id",
            "            )",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id",
            "                )",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            is_outstanding_case = get_is_opened_case(extended_table)",
            "            case_label = \"isOutstanding\"",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    is_outstanding_case.label(case_label),",
            "                    func.count(extended_table.c.bug_id.distinct())) \\",
            "                    .group_by(is_outstanding_case)",
            "            else:",
            "                q = session.query(",
            "                    is_outstanding_case.label(case_label),",
            "                    func.count(extended_table.c.bug_id)) \\",
            "                    .group_by(is_outstanding_case)",
            "",
            "            results = {",
            "                report_status_enum(",
            "                    \"outstanding\" if isOutstanding",
            "                    else \"closed\"",
            "                ): count for isOutstanding, count in q",
            "            }",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReviewStatusCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.review_status,",
            "                Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    extended_table.c.review_status,",
            "                    func.count(extended_table.c.bug_id.distinct()))",
            "            else:",
            "                q = session.query(",
            "                    extended_table.c.review_status,",
            "                    func.count(extended_table.c.bug_id))",
            "",
            "            q = q \\",
            "                .select_from(extended_table) \\",
            "                .group_by(extended_table.c.review_status)",
            "",
            "        return {review_status_enum(rev_status): count",
            "                for rev_status, count in q}",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getFileCounts(self, run_ids, report_filter, cmp_data, limit, offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.file_id,",
            "                Report.bug_id,",
            "                Report.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            count_col = extended_table.c.bug_id.distinct() if \\",
            "                report_filter.isUnique else extended_table.c.bug_id",
            "",
            "            stmt = session.query(",
            "                    File.filepath,",
            "                    func.count(count_col).label('report_num')) \\",
            "                .join(",
            "                    extended_table, File.id == extended_table.c.file_id) \\",
            "                .group_by(File.filepath) \\",
            "                .order_by(desc('report_num'))",
            "",
            "            if limit:",
            "                stmt = stmt.limit(limit).offset(offset)",
            "",
            "            for fp, count in stmt:",
            "                results[fp] = count",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunHistoryTagCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                               offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = []",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            tag_run_ids = session.query(RunHistory.run_id.distinct())",
            "",
            "            if run_ids:",
            "                tag_run_ids = tag_run_ids.filter(",
            "                    RunHistory.run_id.in_(run_ids))",
            "",
            "            tag_run_ids = tag_run_ids.subquery()",
            "",
            "            report_cnt_q = session.query(Report.run_id,",
            "                                         Report.bug_id,",
            "                                         Report.detected_at,",
            "                                         Report.fixed_at)",
            "",
            "            if report_filter.annotations is not None:",
            "                report_cnt_q = report_cnt_q.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                report_cnt_q = report_cnt_q.group_by(Report.id)",
            "",
            "            report_cnt_q = apply_report_filter(",
            "                report_cnt_q, filter_expression, join_tables)",
            "            report_cnt_q = report_cnt_q.filter(",
            "                Report.run_id.in_(tag_run_ids)).subquery()",
            "",
            "            count_expr = func.count(",
            "                report_cnt_q.c.bug_id if not report_filter.isUnique",
            "                else report_cnt_q.c.bug_id.distinct())",
            "",
            "            count_q = session.query(RunHistory.id.label('run_history_id'),",
            "                                    count_expr.label('report_count')) \\",
            "                .outerjoin(report_cnt_q,",
            "                           report_cnt_q.c.run_id == RunHistory.run_id) \\",
            "                .filter(get_open_reports_date_filter_query(report_cnt_q.c)) \\",
            "                .group_by(RunHistory.id) \\",
            "                .subquery()",
            "",
            "            tag_q = session.query(RunHistory.run_id.label('run_id'),",
            "                                  RunHistory.id.label('run_history_id'))",
            "",
            "            if run_ids:",
            "                tag_q = tag_q.filter(RunHistory.run_id.in_(run_ids))",
            "",
            "            if report_filter and report_filter.runTag is not None:",
            "                tag_q = tag_q.filter(RunHistory.id.in_(report_filter.runTag))",
            "",
            "            tag_q = tag_q.subquery()",
            "",
            "            q = session.query(tag_q.c.run_history_id,",
            "                              func.max(Run.id),",
            "                              func.max(Run.name).label('run_name'),",
            "                              func.max(RunHistory.id),",
            "                              func.max(RunHistory.time),",
            "                              func.max(RunHistory.version_tag),",
            "                              func.max(count_q.c.report_count)) \\",
            "                .outerjoin(RunHistory,",
            "                           RunHistory.id == tag_q.c.run_history_id) \\",
            "                .outerjoin(Run, Run.id == tag_q.c.run_id) \\",
            "                .outerjoin(count_q,",
            "                           count_q.c.run_history_id == RunHistory.id) \\",
            "                .group_by(tag_q.c.run_history_id, RunHistory.time) \\",
            "                .order_by(RunHistory.time.desc())",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            for _, run_id, run_name, tag_id, version_time, tag, count in q:",
            "                results.append(RunTagCount(id=tag_id,",
            "                                           time=str(version_time),",
            "                                           name=tag,",
            "                                           runName=run_name,",
            "                                           runId=run_id,",
            "                                           count=count if count else 0))",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getDetectionStatusCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.detection_status,",
            "                Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    extended_table.c.detection_status,",
            "                    func.count(extended_table.c.bug_id.distinct()))",
            "            else:",
            "                q = session.query(",
            "                    extended_table.c.detection_status,",
            "                    func.count(literal_column('*')))",
            "",
            "            q = q \\",
            "                .select_from(extended_table) \\",
            "                .group_by(extended_table.c.detection_status)",
            "",
            "            results = {",
            "                detection_status_enum(k): v for k,",
            "                v in q}",
            "",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getFailedFilesCount(self, run_ids):",
            "        \"\"\"",
            "        Count the number of uniqued failed files in the latest storage of each",
            "        given run. If the run id list is empty the number of failed files will",
            "        be counted for all of the runs.",
            "        \"\"\"",
            "        # Unfortunately we can't distinct the failed file paths by using SQL",
            "        # queries because the list of failed files for a run / analyzer are",
            "        # stored in one column in a compressed way. For this reason we need to",
            "        # decompress the value in the Python code before uniqueing.",
            "        return len(self.getFailedFiles(run_ids).keys())",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getFailedFiles(self, run_ids):",
            "        \"\"\"",
            "        Get files which failed to analyze in the latest storage of the given",
            "        runs. For each files it will return a list where each element contains",
            "        information in which run the failure happened.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        res = defaultdict(list)",
            "        with DBSession(self._Session) as session:",
            "            query, sub_q = get_failed_files_query(",
            "                session, run_ids, [AnalyzerStatistic.failed_files, Run.name],",
            "                [RunHistory.run_id])",
            "",
            "            query = query \\",
            "                .outerjoin(Run, Run.id == sub_q.c.run_id) \\",
            "                .filter(AnalyzerStatistic.failed_files.isnot(None))",
            "",
            "            for failed_files, run_name in query.all():",
            "                failed_files = zlib.decompress(failed_files).decode('utf-8')",
            "",
            "                for failed_file in failed_files.split('\\n'):",
            "                    already_exists = \\",
            "                        any(i.runName == run_name for i in res[failed_file])",
            "",
            "                    if not already_exists:",
            "                        res[failed_file].append(",
            "                            ttypes.AnalysisFailureInfo(runName=run_name))",
            "",
            "        return res",
            "",
            "    # -----------------------------------------------------------------------",
            "    @timeit",
            "    def getPackageVersion(self):",
            "        return self.__package_version",
            "",
            "    # -----------------------------------------------------------------------",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeRunResults(self, run_ids):",
            "        self.__require_store()",
            "",
            "        failed = False",
            "        for run_id in run_ids:",
            "            try:",
            "                self.removeRun(run_id, None)",
            "            except Exception as ex:",
            "                LOG.error(\"Failed to remove run: %s\", run_id)",
            "                LOG.error(ex)",
            "                failed = True",
            "        return not failed",
            "",
            "    def _removeReports(self, session, report_ids,",
            "                       chunk_size=SQLITE_MAX_VARIABLE_NUMBER):",
            "        \"\"\"",
            "        Removing reports in chunks.",
            "        \"\"\"",
            "        for r_ids in util.chunks(iter(report_ids), chunk_size):",
            "            session.query(Report) \\",
            "                .filter(Report.id.in_(r_ids)) \\",
            "                .delete(synchronize_session=False)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeRunReports(self, run_ids, report_filter, cmp_data):",
            "        self.__require_store()",
            "",
            "        if not run_ids:",
            "            run_ids = []",
            "",
            "        if cmp_data and cmp_data.runIds:",
            "            run_ids.extend(cmp_data.runIds)",
            "",
            "        with DBSession(self._Session) as session:",
            "            check_remove_runs_lock(session, run_ids)",
            "",
            "            try:",
            "                filter_expression, join_tables = process_report_filter(",
            "                    session, run_ids, report_filter, cmp_data)",
            "",
            "                q = session.query(Report.id)",
            "",
            "                if report_filter.annotations is not None:",
            "                    q = q.outerjoin(ReportAnnotations,",
            "                                    ReportAnnotations.report_id == Report.id)",
            "                    q = q.group_by(Report.id)",
            "",
            "                q = apply_report_filter(q, filter_expression, join_tables)",
            "",
            "                reports_to_delete = [r[0] for r in q]",
            "                if reports_to_delete:",
            "                    self._removeReports(session, reports_to_delete)",
            "",
            "                session.commit()",
            "                session.close()",
            "",
            "                LOG.info(\"The following reports were removed by '%s': %s\",",
            "                         self._get_username(), reports_to_delete)",
            "            except Exception as ex:",
            "                session.rollback()",
            "                LOG.error(\"Database cleanup failed.\")",
            "                LOG.error(ex)",
            "                return False",
            "",
            "        # Remove unused comments and unused analysis info from the database.",
            "        # Originally db_cleanup.remove_unused_data() was used here which",
            "        # removes unused file entries too. However, removing files at the same",
            "        # time with a concurrently ongoing storage may result in a foreign key",
            "        # constraint error. An alternative solution can be adding the last",
            "        # access timestamp to file entries to delay their removal (and avoid",
            "        # removing frequently accessed files). The same comment applies to",
            "        # removeRun() function.",
            "        db_cleanup.remove_unused_comments(self._product)",
            "        db_cleanup.remove_unused_analysis_info(self._product)",
            "",
            "        return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeRun(self, run_id, run_filter):",
            "        self.__require_store()",
            "",
            "        # Remove the whole run.",
            "        with DBSession(self._Session) as session:",
            "            check_remove_runs_lock(session, [run_id])",
            "",
            "            if not run_filter:",
            "                run_filter = RunFilter(ids=[run_id])",
            "",
            "            q = process_run_filter(session, session.query(Run), run_filter)",
            "",
            "            # q.delete(synchronize_session=False) could also be used here,",
            "            # however, a run deletion tends to be a slow operation due to",
            "            # cascades and such. Deleting runs in separate transactions don't",
            "            # exceed a potential statement timeout threshold in a DBMS.",
            "            runs = []",
            "            deleted_run_cnt = 0",
            "",
            "            for run in q.all():",
            "                try:",
            "                    runs.append(run.name)",
            "                    session.delete(run)",
            "                    session.commit()",
            "                    deleted_run_cnt += 1",
            "                except Exception as e:",
            "                    # TODO: Display alert on the GUI if there's an exception",
            "                    # TODO: Catch SQLAlchemyError instead of generic",
            "                    #  exception once it is confirmed that the exception is",
            "                    #  due to a large run deletion timeout based on server",
            "                    #  log warnings",
            "                    # This exception is handled silently because it is",
            "                    # expected to never occur, but there have been some rare",
            "                    # cases where it occurred due to underlying reasons.",
            "                    # Handling it silently ensures that the Number of runs",
            "                    # counter is not affected by the exception.",
            "                    LOG.warning(f\"Suppressed an exception while \"",
            "                                f\"deleting run {run.name}. Error: {e}\")",
            "",
            "            session.close()",
            "",
            "            LOG.info(\"Runs '%s' were removed by '%s'.\", \"', '\".join(runs),",
            "                     self._get_username())",
            "",
            "        # Decrement the number of runs but do not update the latest storage",
            "        # date.",
            "        self._set_run_data_for_curr_product(-1 * deleted_run_cnt)",
            "",
            "        # Remove unused comments and unused analysis info from the database.",
            "        # Originally db_cleanup.remove_unused_data() was used here which",
            "        # removes unused file entries tool. However removing files at the same",
            "        # time with a storage concurrently results foreign key constraint",
            "        # error. An alternative solution can be adding a timestamp to file",
            "        # entries to delay their removal. The same comment applies to",
            "        # removeRunReports() function.",
            "        db_cleanup.remove_unused_comments(self._product)",
            "        db_cleanup.remove_unused_analysis_info(self._product)",
            "",
            "        return bool(runs)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def updateRunData(self, run_id, new_run_name):",
            "        self.__require_store()",
            "",
            "        if not new_run_name:",
            "            msg = 'No new run name was given to update the run.'",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "        with DBSession(self._Session) as session:",
            "            check_new_run_name = session.query(Run) \\",
            "                .filter(Run.name == new_run_name) \\",
            "                .all()",
            "            if check_new_run_name:",
            "                msg = \"New run name '\" + new_run_name + \"' already exists.\"",
            "                LOG.error(msg)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            run_data = session.query(Run).get(run_id)",
            "            if run_data:",
            "                old_run_name = run_data.name",
            "                run_data.name = new_run_name",
            "                session.add(run_data)",
            "                session.commit()",
            "",
            "                LOG.info(\"Run name '%s' (%d) was changed to %s by '%s'.\",",
            "                         old_run_name, run_id, new_run_name,",
            "                         self._get_username())",
            "",
            "                return True",
            "            else:",
            "                msg = f'Run id {run_id} was not found in the database.'",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "        return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    def getSuppressFile(self):",
            "        \"\"\"",
            "        DEPRECATED the server is not started with a suppress file anymore.",
            "        Returning empty string.",
            "        \"\"\"",
            "        self.__require_access()",
            "        return ''",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addSourceComponent(self, name, value, description):",
            "        \"\"\"",
            "        Adds a new source if it does not exist or updates an old one.",
            "        \"\"\"",
            "        self.__require_admin()",
            "        with DBSession(self._Session) as session:",
            "            component = session.query(SourceComponent).get(name)",
            "            user = self._auth_session.user if self._auth_session else None",
            "",
            "            if component:",
            "                component.value = value.encode('utf-8')",
            "                component.description = description",
            "                component.user = user",
            "            else:",
            "                component = SourceComponent(name,",
            "                                            value.encode('utf-8'),",
            "                                            description,",
            "                                            user)",
            "",
            "            session.add(component)",
            "            session.commit()",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getSourceComponents(self, component_filter):",
            "        \"\"\"",
            "        Returns the available source components.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            q = session.query(SourceComponent)",
            "",
            "            if component_filter:",
            "                sql_component_filter = [SourceComponent.name.ilike(conv(cf))",
            "                                        for cf in component_filter]",
            "                q = q.filter(*sql_component_filter)",
            "",
            "            q = q.order_by(SourceComponent.name)",
            "",
            "            components = [SourceComponentData(c.name, c.value.decode('utf-8'),",
            "                                              c.description) for c in q]",
            "",
            "            # If no filter is set or the auto generated component name can",
            "            # be found in the filter list we will return with this",
            "            # component too.",
            "            if not component_filter or \\",
            "                    GEN_OTHER_COMPONENT_NAME in component_filter:",
            "                component_other = \\",
            "                    SourceComponentData(GEN_OTHER_COMPONENT_NAME, None,",
            "                                        \"Special auto-generated source \"",
            "                                        \"component which contains files that \"",
            "                                        \"are uncovered by the rest of the \"",
            "                                        \"components.\")",
            "",
            "                components.append(component_other)",
            "",
            "            return components",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeSourceComponent(self, name):",
            "        \"\"\"",
            "        Removes a source component.",
            "        \"\"\"",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            component = session.query(SourceComponent).get(name)",
            "            if component:",
            "                session.delete(component)",
            "                session.commit()",
            "                LOG.info(\"Source component '%s' has been removed by '%s'\",",
            "                         name, self._get_username())",
            "                return True",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f'Source component {name} was not found in the database.')",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getMissingContentHashes(self, file_hashes):",
            "        self.__require_store()",
            "",
            "        if not file_hashes:",
            "            return []",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            q = session.query(FileContent) \\",
            "                .options(sqlalchemy.orm.load_only('content_hash')) \\",
            "                .filter(FileContent.content_hash.in_(file_hashes))",
            "",
            "            return list(set(file_hashes) -",
            "                        set(fc.content_hash for fc in q))",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getMissingContentHashesForBlameInfo(self, file_hashes):",
            "        self.__require_store()",
            "",
            "        if not file_hashes:",
            "            return []",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            q = session.query(FileContent) \\",
            "                .options(sqlalchemy.orm.load_only('content_hash')) \\",
            "                .filter(FileContent.content_hash.in_(file_hashes)) \\",
            "                .filter(FileContent.blame_info.isnot(None))",
            "",
            "            return list(set(file_hashes) -",
            "                        set(fc.content_hash for fc in q))",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def massStoreRun(self, name, tag, version, b64zip, force,",
            "                     trim_path_prefixes, description):",
            "        self.__require_store()",
            "",
            "        from codechecker_server.api.mass_store_run import MassStoreRun",
            "        m = MassStoreRun(self, name, tag, version, b64zip, force,",
            "                         trim_path_prefixes, description)",
            "        return m.store()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def allowsStoringAnalysisStatistics(self):",
            "        self.__require_store()",
            "",
            "        return bool(self._manager.get_analysis_statistics_dir())",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalysisStatisticsLimits(self):",
            "        self.__require_store()",
            "",
            "        cfg = {}",
            "",
            "        # Get the limit of failure zip size.",
            "        failure_zip_size = self._manager.get_failure_zip_size()",
            "        if failure_zip_size:",
            "            cfg[ttypes.StoreLimitKind.FAILURE_ZIP_SIZE] = failure_zip_size",
            "",
            "        # Get the limit of compilation database size.",
            "        compilation_database_size = \\",
            "            self._manager.get_compilation_database_size()",
            "        if compilation_database_size:",
            "            cfg[ttypes.StoreLimitKind.COMPILATION_DATABASE_SIZE] = \\",
            "                compilation_database_size",
            "",
            "        return cfg",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def storeAnalysisStatistics(self, run_name, b64zip):",
            "        self.__require_store()",
            "",
            "        report_dir_store = self._manager.get_analysis_statistics_dir()",
            "        if report_dir_store:",
            "            try:",
            "                product_dir = os.path.join(report_dir_store,",
            "                                           self._product.endpoint)",
            "                # Create report store directory.",
            "                if not os.path.exists(product_dir):",
            "                    os.makedirs(product_dir, mode=stat.S_IRWXU | stat.S_IRGRP)",
            "",
            "                # Removes and replaces special characters in the run name.",
            "                run_name = slugify(run_name)",
            "                run_zip_file = os.path.join(product_dir, run_name + '.zip')",
            "                with open(run_zip_file, 'wb') as run_zip:",
            "                    run_zip.write(zlib.decompress(",
            "                        base64.b64decode(b64zip.encode('utf-8'))))",
            "",
            "                # Change permission, so only current user and group have access",
            "                # to this file.",
            "                os.chmod(",
            "                    run_zip_file, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP)",
            "",
            "                return True",
            "            except Exception as ex:",
            "                LOG.error(str(ex))",
            "                return False",
            "",
            "        return False",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalysisStatistics(self, run_id, run_history_id):",
            "        self.__require_view()",
            "",
            "        analyzer_statistics = {}",
            "",
            "        with DBSession(self._Session) as session:",
            "            run_ids = None if run_id is None else [run_id]",
            "            run_history_ids = None if run_history_id is None \\",
            "                else [run_history_id]",
            "",
            "            query = get_analysis_statistics_query(",
            "                session, run_ids, run_history_ids)",
            "",
            "            for anal_stat, _ in query:",
            "                failed_files = zlib.decompress(anal_stat.failed_files).decode(",
            "                    'utf-8').split('\\n') if anal_stat.failed_files else []",
            "                analyzer_version = zlib.decompress(",
            "                    anal_stat.version).decode('utf-8') \\",
            "                    if anal_stat.version else None",
            "",
            "                analyzer_statistics[anal_stat.analyzer_type] = \\",
            "                    ttypes.AnalyzerStatistics(version=analyzer_version,",
            "                                              failed=anal_stat.failed,",
            "                                              failedFilePaths=failed_files,",
            "                                              successful=anal_stat.successful)",
            "        return analyzer_statistics",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def exportData(self, run_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            # Logic for getting comments",
            "            comment_data_list = defaultdict(list)",
            "            comment_query = session.query(Comment, Report.bug_id) \\",
            "                .outerjoin(Report, Report.bug_id == Comment.bug_hash) \\",
            "                .order_by(Comment.created_at.desc(), Comment.id.desc())",
            "",
            "            if run_filter:",
            "                comment_query = process_run_filter(session, comment_query,",
            "                                                   run_filter) \\",
            "                    .outerjoin(Run, Report.run_id == Run.id)",
            "",
            "            for data, report_id in comment_query:",
            "                comment_data = ttypes.CommentData(",
            "                    id=data.id,",
            "                    author=data.author,",
            "                    message=html.unescape(data.message.decode('utf-8')),",
            "                    createdAt=str(data.created_at),",
            "                    kind=data.kind)",
            "                comment_data_list[report_id].append(comment_data)",
            "",
            "            # Logic for getting review status",
            "            review_data_list = {}",
            "            review_query = session.query(Report) \\",
            "                .filter(Report.review_status != \"unreviewed\") \\",
            "                .order_by(Report.review_status_date)",
            "",
            "            if run_filter:",
            "                review_query = process_run_filter(session, review_query,",
            "                                                  run_filter) \\",
            "                    .outerjoin(Run, Report.run_id == Run.id)",
            "",
            "            for report in review_query:",
            "                review_data = create_review_data(",
            "                    report.review_status,",
            "                    report.review_status_message,",
            "                    report.review_status_author,",
            "                    report.review_status_date,",
            "                    report.review_status_is_in_source)",
            "                review_data_list[report.bug_id] = review_data",
            "",
            "        return ExportData(comments=comment_data_list,",
            "                          reviewData=review_data_list)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def importData(self, exportData):",
            "        self.__require_admin()",
            "        with DBSession(self._Session) as session:",
            "",
            "            # Logic for importing comments",
            "            comment_bug_ids = list(exportData.comments.keys())",
            "            comment_query = session.query(Comment) \\",
            "                .filter(Comment.bug_hash.in_(comment_bug_ids)) \\",
            "                .order_by(Comment.created_at.desc())",
            "            comments_in_db = defaultdict(list)",
            "            for comment in comment_query:",
            "                comments_in_db[comment.bug_hash].append(comment)",
            "            for bug_hash, comments in exportData.comments.items():",
            "                db_comments = comments_in_db[bug_hash]",
            "                for comment in comments:",
            "                    date = datetime.strptime(comment.createdAt,",
            "                                             '%Y-%m-%d %H:%M:%S.%f')",
            "                    message = comment.message.encode('utf-8') \\",
            "                        if comment.message else b''",
            "                    # See if the comment is already in the database.",
            "                    if any(c.created_at == date and",
            "                           c.kind == comment.kind and",
            "                           c.message == message for c in db_comments):",
            "                        continue",
            "                    c = Comment(bug_hash, comment.author, message,",
            "                                comment.kind, date)",
            "                    session.add(c)",
            "",
            "            # Logic for importing review status",
            "            review_bug_ids = list(exportData.reviewData.keys())",
            "            review_query = session.query(ReviewStatus) \\",
            "                .filter(ReviewStatus.bug_hash.in_(review_bug_ids)) \\",
            "                .order_by(ReviewStatus.date.desc())",
            "            db_review_data = {}",
            "            for review_status in review_query:",
            "                db_review_data[review_status.bug_hash] = review_status",
            "            for bug_hash, imported_review in exportData.reviewData.items():",
            "                db_status = db_review_data.get(bug_hash)",
            "                # The status is up-to-date.",
            "                if db_status and str(db_status.date) == imported_review.date:",
            "                    continue",
            "                date = datetime.strptime(imported_review.date,",
            "                                         '%Y-%m-%d %H:%M:%S.%f')",
            "                self._setReviewStatus(session,",
            "                                      bug_hash,",
            "                                      imported_review.status,",
            "                                      imported_review.comment,",
            "                                      date)",
            "",
            "            session.commit()",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addCleanupPlan(self, name, description, dueDate):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = session.query(CleanupPlan) \\",
            "                .filter(CleanupPlan.name == name) \\",
            "                .one_or_none()",
            "",
            "            if cleanup_plan:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f\"Cleanup plan '{name}' already exists.\")",
            "",
            "            cleanup_plan = CleanupPlan(name)",
            "            cleanup_plan.description = description",
            "            cleanup_plan.due_date = \\",
            "                datetime.fromtimestamp(dueDate) if dueDate else None",
            "",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"New cleanup plan '%s' has been created by '%s'\",",
            "                     name, self._get_username())",
            "",
            "            return cleanup_plan.id",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def updateCleanupPlan(self, cleanup_plan_id, name, description, dueDate):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "            cleanup_plan.name = name",
            "            cleanup_plan.description = description",
            "            cleanup_plan.due_date = \\",
            "                datetime.fromtimestamp(dueDate) if dueDate else None",
            "",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"Cleanup plan '%d' has been updated by '%s'\",",
            "                     cleanup_plan_id, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCleanupPlans(self, cleanup_plan_filter):",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            q = session \\",
            "                .query(CleanupPlan) \\",
            "                .order_by(CleanupPlan.name)",
            "",
            "            if cleanup_plan_filter:",
            "                if cleanup_plan_filter.ids:",
            "                    q = q.filter(CleanupPlan.id.in_(",
            "                        cleanup_plan_filter.ids))",
            "",
            "                if cleanup_plan_filter.names:",
            "                    q = q.filter(CleanupPlan.name.in_(",
            "                        cleanup_plan_filter.names))",
            "",
            "                if cleanup_plan_filter.isOpen is not None:",
            "                    if cleanup_plan_filter.isOpen:",
            "                        q = q.filter(CleanupPlan.closed_at.is_(None))",
            "                    else:",
            "                        q = q.filter(CleanupPlan.closed_at.isnot(None))",
            "",
            "            cleanup_plans = q.all()",
            "",
            "            cleanup_plan_hashes = get_cleanup_plan_report_hashes(",
            "                session, [c.id for c in cleanup_plans])",
            "",
            "            return [ttypes.CleanupPlan(",
            "                id=cp.id,",
            "                name=cp.name,",
            "                description=cp.description,",
            "                dueDate=int(time.mktime(",
            "                    cp.due_date.timetuple())) if cp.due_date else None,",
            "                closedAt=int(time.mktime(",
            "                    cp.closed_at.timetuple())) if cp.closed_at else None,",
            "                reportHashes=cleanup_plan_hashes[cp.id]) for cp in q]",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeCleanupPlan(self, cleanup_plan_id):",
            "        self.__require_admin()",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "            name = cleanup_plan.name",
            "",
            "            session.delete(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"Cleanup plan '%s' has been removed by '%s'\",",
            "                     name, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def closeCleanupPlan(self, cleanup_plan_id):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            cleanup_plan.closed_at = datetime.now()",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"Cleanup plan '%s' has been closed by '%s'\",",
            "                     cleanup_plan.name, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def reopenCleanupPlan(self, cleanup_plan_id):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            cleanup_plan.closed_at = None",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "            LOG.info(\"Cleanup plan '%s' has been reopened by '%s'\",",
            "                     cleanup_plan.name, self._get_username())",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def setCleanupPlan(self, cleanup_plan_id, reportHashes):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            q = session \\",
            "                .query(CleanupPlanReportHash.bug_hash) \\",
            "                .filter(",
            "                    CleanupPlanReportHash.cleanup_plan_id == cleanup_plan.id) \\",
            "                .filter(CleanupPlanReportHash.bug_hash.in_(reportHashes))",
            "            new_report_hashes = set(reportHashes) - set(b[0] for b in q)",
            "",
            "            for report_hash in new_report_hashes:",
            "                session.add(CleanupPlanReportHash(",
            "                    cleanup_plan_id=cleanup_plan.id, bug_hash=report_hash))",
            "",
            "            session.commit()",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def unsetCleanupPlan(self, cleanup_plan_id, reportHashes):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            session \\",
            "                .query(CleanupPlanReportHash) \\",
            "                .filter(",
            "                    CleanupPlanReportHash.cleanup_plan_id == cleanup_plan.id) \\",
            "                .filter(CleanupPlanReportHash.bug_hash.in_(reportHashes)) \\",
            "                .delete(synchronize_session=False)",
            "",
            "            session.commit()",
            "            session.close()",
            "",
            "            return True"
        ],
        "afterPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Handle Thrift requests.",
            "\"\"\"",
            "",
            "import base64",
            "import html",
            "import json",
            "import os",
            "import re",
            "import shlex",
            "import stat",
            "import time",
            "import zlib",
            "",
            "from copy import deepcopy",
            "from collections import OrderedDict, defaultdict, namedtuple",
            "from datetime import datetime, timedelta",
            "from typing import Any, Dict, List, Optional, Set, Tuple",
            "",
            "import sqlalchemy",
            "from sqlalchemy.sql.expression import or_, and_, not_, func, \\",
            "    asc, desc, union_all, select, bindparam, literal_column, case, cast",
            "from sqlalchemy.orm import contains_eager",
            "",
            "import codechecker_api_shared",
            "from codechecker_api.codeCheckerDBAccess_v6 import constants, ttypes",
            "from codechecker_api.codeCheckerDBAccess_v6.ttypes import \\",
            "    AnalysisInfoFilter, AnalysisInfoChecker as API_AnalysisInfoChecker, \\",
            "    BlameData, BlameInfo, BugPathPos, \\",
            "    CheckerCount, CheckerStatusVerificationDetail, Commit, CommitAuthor, \\",
            "    CommentData, \\",
            "    DetectionStatus, DiffType, \\",
            "    Encoding, ExportData, \\",
            "    Order, \\",
            "    ReportData, ReportDetails, ReportStatus, ReviewData, ReviewStatusRule, \\",
            "    ReviewStatusRuleFilter, ReviewStatusRuleSortMode, \\",
            "    ReviewStatusRuleSortType, RunData, RunFilter, RunHistoryData, \\",
            "    RunReportCount, RunSortType, RunTagCount, \\",
            "    ReviewStatus as API_ReviewStatus, \\",
            "    SourceComponentData, SourceFileData, SortMode, SortType",
            "",
            "from codechecker_common import util",
            "from codechecker_common.logger import get_logger",
            "",
            "from codechecker_web.shared import webserver_context",
            "from codechecker_web.shared import convert",
            "",
            "from codechecker_server.profiler import timeit",
            "",
            "from .. import permissions",
            "from ..database import db_cleanup",
            "from ..database.config_db_model import Product",
            "from ..database.database import conv, DBSession, escape_like",
            "from ..database.run_db_model import \\",
            "    AnalysisInfo, AnalysisInfoChecker as DB_AnalysisInfoChecker, \\",
            "    AnalyzerStatistic, \\",
            "    BugPathEvent, BugReportPoint, \\",
            "    CleanupPlan, CleanupPlanReportHash, Checker, Comment, \\",
            "    ExtendedReportData, \\",
            "    File, FileContent, \\",
            "    Report, ReportAnnotations, ReportAnalysisInfo, ReviewStatus, \\",
            "    Run, RunHistory, RunHistoryAnalysisInfo, RunLock, \\",
            "    SourceComponent",
            "",
            "from .thrift_enum_helper import detection_status_enum, \\",
            "    detection_status_str, report_status_enum, \\",
            "    review_status_enum, review_status_str, report_extended_data_type_enum",
            "",
            "# These names are inherited from Thrift stubs.",
            "# pylint: disable=invalid-name",
            "",
            "LOG = get_logger('server')",
            "",
            "GEN_OTHER_COMPONENT_NAME = \"Other (auto-generated)\"",
            "",
            "SQLITE_MAX_VARIABLE_NUMBER = 999",
            "SQLITE_MAX_COMPOUND_SELECT = 500",
            "",
            "",
            "class CommentKindValue:",
            "    USER = 0",
            "    SYSTEM = 1",
            "",
            "",
            "def comment_kind_from_thrift_type(kind):",
            "    \"\"\" Convert the given comment kind from Thrift type to Python enum. \"\"\"",
            "    if kind == ttypes.CommentKind.USER:",
            "        return CommentKindValue.USER",
            "    elif kind == ttypes.CommentKind.SYSTEM:",
            "        return CommentKindValue.SYSTEM",
            "",
            "    assert False, f\"Unknown ttypes.CommentKind: {kind}\"",
            "",
            "",
            "def comment_kind_to_thrift_type(kind):",
            "    \"\"\" Convert the given comment kind from Python enum to Thrift type. \"\"\"",
            "    if kind == CommentKindValue.USER:",
            "        return ttypes.CommentKind.USER",
            "    elif kind == CommentKindValue.SYSTEM:",
            "        return ttypes.CommentKind.SYSTEM",
            "",
            "    assert False, f\"Unknown CommentKindValue: {kind}\"",
            "",
            "",
            "def verify_limit_range(limit):",
            "    \"\"\"Verify limit value for the queries.",
            "",
            "    Query limit should not be larger than the max allowed value.",
            "    Max is returned if the value is larger than max.",
            "    \"\"\"",
            "    max_query_limit = constants.MAX_QUERY_SIZE",
            "    if not limit:",
            "        return max_query_limit",
            "    if limit > max_query_limit:",
            "        LOG.warning('Query limit %d was larger than max query limit %d, '",
            "                    'setting limit to %d',",
            "                    limit,",
            "                    max_query_limit,",
            "                    max_query_limit)",
            "        limit = max_query_limit",
            "    return limit",
            "",
            "",
            "def slugify(text):",
            "    \"\"\"",
            "    Removes and replaces special characters in a given text.",
            "    \"\"\"",
            "    # Removes non-alpha characters.",
            "    norm_text = re.sub(r'[^\\w\\s\\-/]', '', text)",
            "",
            "    # Converts spaces and slashes to underscores.",
            "    norm_text = re.sub(r'([\\s]+|[/]+)', '_', norm_text)",
            "",
            "    return norm_text",
            "",
            "",
            "def exc_to_thrift_reqfail(function):",
            "    \"\"\"",
            "    Convert internal exceptions to RequestFailed exception",
            "    which can be sent back on the thrift connections.",
            "    \"\"\"",
            "    func_name = function.__name__",
            "",
            "    def wrapper(*args, **kwargs):",
            "        try:",
            "            res = function(*args, **kwargs)",
            "            return res",
            "",
            "        except sqlalchemy.exc.SQLAlchemyError as alchemy_ex:",
            "            # Convert SQLAlchemy exceptions.",
            "            msg = str(alchemy_ex)",
            "            import traceback",
            "            traceback.print_exc()",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "        except codechecker_api_shared.ttypes.RequestFailed as rf:",
            "            LOG.warning(\"%s:\\n%s\", func_name, rf.message)",
            "            raise",
            "        except Exception as ex:",
            "            import traceback",
            "            traceback.print_exc()",
            "            msg = str(ex)",
            "            LOG.warning(\"%s:\\n%s\", func_name, msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "    return wrapper",
            "",
            "",
            "def get_component_values(",
            "    session: DBSession,",
            "    component_name: str",
            ") -> Tuple[List[str], List[str]]:",
            "    \"\"\"",
            "    Get component values by component names and returns a tuple where the",
            "    first item contains a list path which should be skipped and the second",
            "    item contains a list of path which should be included.",
            "    E.g.:",
            "      +/a/b/x.cpp",
            "      +/a/b/y.cpp",
            "      -/a/b",
            "    On the above component value this function will return the following:",
            "      (['/a/b'], ['/a/b/x.cpp', '/a/b/y.cpp'])",
            "    \"\"\"",
            "    components = session.query(SourceComponent) \\",
            "        .filter(SourceComponent.name.like(component_name)) \\",
            "        .all()",
            "",
            "    skip = []",
            "    include = []",
            "",
            "    for component in components:",
            "        values = component.value.decode('utf-8').split('\\n')",
            "        for value in values:",
            "            value = value.strip()",
            "            if not value:",
            "                continue",
            "",
            "            v = value[1:]",
            "            if value[0] == '+':",
            "                include.append(v)",
            "            elif value[0] == '-':",
            "                skip.append(v)",
            "",
            "    return skip, include",
            "",
            "",
            "def process_report_filter(",
            "    session,",
            "    run_ids,",
            "    report_filter,",
            "    cmp_data=None,",
            "    keep_all_annotations=False",
            "):",
            "    \"\"\"",
            "    Process the new report filter.",
            "    \"\"\"",
            "    AND = []",
            "",
            "    cmp_filter_expr, join_tables = process_cmp_data_filter(",
            "        session, run_ids, report_filter, cmp_data)",
            "",
            "    if cmp_filter_expr is not None:",
            "        AND.append(cmp_filter_expr)",
            "",
            "    if report_filter is None:",
            "        return and_(*AND), join_tables",
            "",
            "    if report_filter.reportHash == []:",
            "        return and_(False), []",
            "",
            "    if report_filter.filepath:",
            "        if report_filter.fileMatchesAnyPoint:",
            "            AND.append(Report.id.in_(get_reports_by_files(",
            "                session,",
            "                report_filter.filepath)))",
            "        else:",
            "            OR = [File.filepath.ilike(conv(fp))",
            "                  for fp in report_filter.filepath]",
            "",
            "            AND.append(or_(*OR))",
            "            join_tables.append(File)",
            "",
            "    if report_filter.checkerMsg:",
            "        OR = [Report.checker_message.ilike(conv(cm))",
            "              for cm in report_filter.checkerMsg]",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.analyzerNames or report_filter.checkerName \\",
            "            or report_filter.severity:",
            "        if report_filter.analyzerNames:",
            "            OR = [Checker.analyzer_name.ilike(conv(an))",
            "                  for an in report_filter.analyzerNames]",
            "            AND.append(or_(*OR))",
            "",
            "        if report_filter.checkerName:",
            "            OR = [Checker.checker_name.ilike(conv(cn))",
            "                  for cn in report_filter.checkerName]",
            "            AND.append(or_(*OR))",
            "",
            "        if report_filter.severity:",
            "            AND.append(Checker.severity.in_(report_filter.severity))",
            "",
            "        join_tables.append(Checker)",
            "",
            "    if report_filter.runName:",
            "        OR = [Run.name.ilike(conv(rn))",
            "              for rn in report_filter.runName]",
            "        AND.append(or_(*OR))",
            "        join_tables.append(Run)",
            "",
            "    if report_filter.reportHash:",
            "        OR = []",
            "        no_joker = []",
            "",
            "        for rh in report_filter.reportHash:",
            "            if '*' in rh:",
            "                OR.append(Report.bug_id.ilike(conv(rh)))",
            "            else:",
            "                no_joker.append(rh)",
            "",
            "        if no_joker:",
            "            OR.append(Report.bug_id.in_(no_joker))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.cleanupPlanNames:",
            "        OR = []",
            "        for cleanup_plan_name in report_filter.cleanupPlanNames:",
            "            q = select([CleanupPlanReportHash.bug_hash]) \\",
            "                .where(",
            "                    CleanupPlanReportHash.cleanup_plan_id.in_(",
            "                        select([CleanupPlan.id])",
            "                        .where(CleanupPlan.name == cleanup_plan_name)",
            "                        .distinct()",
            "                    )) \\",
            "                .distinct()",
            "",
            "            OR.append(Report.bug_id.in_(q))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.reportStatus:",
            "        dst = list(map(detection_status_str,",
            "                       (DetectionStatus.NEW,",
            "                        DetectionStatus.UNRESOLVED,",
            "                        DetectionStatus.REOPENED)))",
            "        rst = list(map(review_status_str,",
            "                       (API_ReviewStatus.UNREVIEWED,",
            "                        API_ReviewStatus.CONFIRMED)))",
            "",
            "        OR = []",
            "        filter_query = and_(",
            "            Report.review_status.in_(rst),",
            "            Report.detection_status.in_(dst)",
            "        )",
            "        if ReportStatus.OUTSTANDING in report_filter.reportStatus:",
            "            OR.append(filter_query)",
            "",
            "        if ReportStatus.CLOSED in report_filter.reportStatus:",
            "            OR.append(not_(filter_query))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.detectionStatus:",
            "        dst = list(map(detection_status_str,",
            "                       report_filter.detectionStatus))",
            "        AND.append(Report.detection_status.in_(dst))",
            "",
            "    if report_filter.reviewStatus:",
            "        OR = [Report.review_status.in_(",
            "            list(map(review_status_str, report_filter.reviewStatus)))]",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.firstDetectionDate is not None:",
            "        date = datetime.fromtimestamp(report_filter.firstDetectionDate)",
            "        AND.append(Report.detected_at >= date)",
            "",
            "    if report_filter.fixDate is not None:",
            "        date = datetime.fromtimestamp(report_filter.fixDate)",
            "        AND.append(Report.detected_at < date)",
            "",
            "    if report_filter.date:",
            "        detected_at = report_filter.date.detected",
            "        if detected_at:",
            "            if detected_at.before:",
            "                detected_before = datetime.fromtimestamp(detected_at.before)",
            "                AND.append(Report.detected_at <= detected_before)",
            "",
            "            if detected_at.after:",
            "                detected_after = datetime.fromtimestamp(detected_at.after)",
            "                AND.append(Report.detected_at >= detected_after)",
            "",
            "        fixed_at = report_filter.date.fixed",
            "        if fixed_at:",
            "            if fixed_at.before:",
            "                fixed_before = datetime.fromtimestamp(fixed_at.before)",
            "                AND.append(Report.fixed_at <= fixed_before)",
            "",
            "            if fixed_at.after:",
            "                fixed_after = datetime.fromtimestamp(fixed_at.after)",
            "                AND.append(Report.fixed_at >= fixed_after)",
            "",
            "    if report_filter.runHistoryTag:",
            "        OR = []",
            "        for history_date in report_filter.runHistoryTag:",
            "            date = datetime.strptime(history_date,",
            "                                     '%Y-%m-%d %H:%M:%S.%f')",
            "            OR.append(and_(Report.detected_at <= date, or_(",
            "                Report.fixed_at.is_(None), Report.fixed_at >= date)))",
            "        AND.append(or_(*OR))",
            "",
            "    if report_filter.componentNames:",
            "        if report_filter.componentMatchesAnyPoint:",
            "            AND.append(Report.id.in_(get_reports_by_components(",
            "                session,",
            "                report_filter.componentNames)))",
            "        else:",
            "            AND.append(process_source_component_filter(",
            "                session, report_filter.componentNames))",
            "            join_tables.append(File)",
            "",
            "    if report_filter.bugPathLength is not None:",
            "        min_path_length = report_filter.bugPathLength.min",
            "        if min_path_length is not None:",
            "            AND.append(Report.path_length >= min_path_length)",
            "",
            "        max_path_length = report_filter.bugPathLength.max",
            "        if max_path_length is not None:",
            "            AND.append(Report.path_length <= max_path_length)",
            "",
            "    if report_filter.annotations is not None:",
            "        annotations = defaultdict(list)",
            "        for annotation in report_filter.annotations:",
            "            annotations[annotation.first].append(annotation.second)",
            "",
            "        OR = []",
            "        for key, values in annotations.items():",
            "            if keep_all_annotations:",
            "                OR.append(or_(",
            "                    ReportAnnotations.key != key,",
            "                    *[ReportAnnotations.value.ilike(conv(v))",
            "                      for v in values]))",
            "            else:",
            "                OR.append(and_(",
            "                    ReportAnnotations.key == key,",
            "                    or_(*[ReportAnnotations.value.ilike(conv(v))",
            "                          for v in values])) if values else and_(",
            "                              ReportAnnotations.key == key))",
            "",
            "        AND.append(or_(*OR))",
            "",
            "    filter_expr = and_(*AND)",
            "    return filter_expr, join_tables",
            "",
            "",
            "def process_source_component_filter(session, component_names):",
            "    \"\"\" Process source component filter.",
            "",
            "    The virtual auto-generated Other component will be handled separately and",
            "    the query part will be added to the filter.",
            "    \"\"\"",
            "    OR = []",
            "",
            "    for component_name in component_names:",
            "        if component_name == GEN_OTHER_COMPONENT_NAME:",
            "            file_query = get_other_source_component_file_query(session)",
            "        else:",
            "            file_query = get_source_component_file_query(session,",
            "                                                         component_name)",
            "",
            "        if file_query is not None:",
            "            OR.append(file_query)",
            "",
            "    return or_(*OR)",
            "",
            "",
            "def filter_open_reports_in_tags(results, run_ids, tag_ids):",
            "    \"\"\"",
            "    Adding filters on \"results\" query which filter on open reports in",
            "    given runs and tags.",
            "    For further information see the documentation of",
            "    filter_open_reports_in_tags_old().",
            "    \"\"\"",
            "",
            "    if run_ids:",
            "        results = results.filter(Report.run_id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        results = results.outerjoin(",
            "            RunHistory, RunHistory.run_id == Report.run_id) \\",
            "            .filter(RunHistory.id.in_(tag_ids)) \\",
            "            .filter(get_open_reports_date_filter_query())",
            "",
            "    return results",
            "",
            "",
            "def filter_open_reports_in_tags_old(results, run_ids, tag_ids):",
            "    \"\"\"",
            "    Adding filters on \"results\" query which filter on open reports in",
            "    given runs and tags.",
            "",
            "    This function is almost the same as filter_open_reports_in_tags() except",
            "    that is uses get_open_reports_date_filter_query_old() for filtering open",
            "    reports on a given date. This function is duplicated, because we didn't",
            "    want to add an extra parameter for this function, but express the fact that",
            "    an old client (i.e. API version before 6.50) should be given a different",
            "    result set.",
            "    This function and its duplicate are used in getDiffResultHash() which",
            "    should behave differently when called by an old client. The reasons of this",
            "    different behavior is described a previous commit",
            "    (f6d0fedaf14b583df7bd26078a8a22b557be57c6) where another case of the issue",
            "    was fixed.",
            "    \"\"\"",
            "",
            "    if run_ids:",
            "        results = results.filter(Report.run_id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        results = results.outerjoin(",
            "            RunHistory, RunHistory.run_id == Report.run_id) \\",
            "            .filter(RunHistory.id.in_(tag_ids)) \\",
            "            .filter(get_open_reports_date_filter_query_old())",
            "",
            "    return results",
            "",
            "",
            "def get_include_skip_queries(",
            "    include: List[str],",
            "    skip: List[str]",
            "):",
            "    \"\"\" Get queries for include and skip values of a component.",
            "",
            "    To get the include and skip lists use the 'get_component_values' function.",
            "    \"\"\"",
            "    include_q = select([File.id]) \\",
            "        .where(or_(*[",
            "            File.filepath.like(conv(fp)) for fp in include])) \\",
            "        .distinct()",
            "",
            "    skip_q = select([File.id]) \\",
            "        .where(or_(*[",
            "            File.filepath.like(conv(fp)) for fp in skip])) \\",
            "        .distinct()",
            "",
            "    return include_q, skip_q",
            "",
            "",
            "def get_source_component_file_query(",
            "    session: DBSession,",
            "    component_name: str",
            "):",
            "    \"\"\" Get filter query for a single source component. \"\"\"",
            "    skip, include = get_component_values(session, component_name)",
            "",
            "    if skip and include:",
            "        include_q, skip_q = get_include_skip_queries(include, skip)",
            "        return File.id.in_(include_q.except_(skip_q))",
            "",
            "    if include:",
            "        return or_(*[File.filepath.like(conv(fp)) for fp in include])",
            "    elif skip:",
            "        return and_(*[not_(File.filepath.like(conv(fp))) for fp in skip])",
            "",
            "    return None",
            "",
            "",
            "def get_reports_by_bugpath_filter(session, file_filter_q) -> Set[int]:",
            "    \"\"\"",
            "    This function returns a set of report IDs that are related to any file",
            "    described by the query in the second parameter, either because their bug",
            "    path goes through these files, or there is any bug note, etc. in these",
            "    files.",
            "    \"\"\"",
            "    def first_col_values(query):",
            "        \"\"\"",
            "        This function executes a query and returns the set of first columns'",
            "        values.",
            "        \"\"\"",
            "        return set(map(lambda x: x[0], query.all()))",
            "",
            "    report_ids = set()",
            "",
            "    q = session.query(Report.id) \\",
            "        .join(File, File.id == Report.file_id) \\",
            "        .filter(file_filter_q)",
            "",
            "    report_ids.update(first_col_values(q))",
            "",
            "    q = session.query(BugPathEvent.report_id) \\",
            "        .join(File, File.id == BugPathEvent.file_id) \\",
            "        .filter(file_filter_q)",
            "",
            "    report_ids.update(first_col_values(q))",
            "",
            "    q = session.query(ExtendedReportData.report_id) \\",
            "        .join(File, File.id == ExtendedReportData.file_id) \\",
            "        .filter(file_filter_q)",
            "",
            "    report_ids.update(first_col_values(q))",
            "",
            "    return report_ids",
            "",
            "",
            "def get_reports_by_components(session, component_names: List[str]) -> Set[int]:",
            "    \"\"\"",
            "    This function returns a set of report IDs that are related to any component",
            "    in the second parameter, either because their bug path goes through these",
            "    components, or there is any bug note, etc. in these components.",
            "    \"\"\"",
            "    source_component_filter = \\",
            "        process_source_component_filter(session, component_names)",
            "    return get_reports_by_bugpath_filter(session, source_component_filter)",
            "",
            "",
            "def get_reports_by_files(session, files: List[str]) -> Set[int]:",
            "    \"\"\"",
            "    This function returns a set of report IDs that are related to any file in",
            "    the second parameter, either because their bug path goes through these",
            "    files, or there is any bug note, etc. in these files.",
            "    \"\"\"",
            "    file_filter = or_(*[File.filepath.ilike(conv(fp)) for fp in files])",
            "    return get_reports_by_bugpath_filter(session, file_filter)",
            "",
            "",
            "def get_other_source_component_file_query(session):",
            "    \"\"\" Get filter query for the auto-generated Others component.",
            "    If there are no user defined source components in the database this",
            "    function will return with None.",
            "",
            "    The returned query will look like this:",
            "        (Files NOT IN Component_1) AND (Files NOT IN Component_2) ... AND",
            "        (Files NOT IN Component_N)",
            "    \"\"\"",
            "    component_names = session.query(SourceComponent.name).all()",
            "",
            "    # If there are no user defined source components we don't have to filter.",
            "    if not component_names:",
            "        return None",
            "",
            "    def get_query(component_name: str):",
            "        \"\"\" Get file filter query for auto generated Other component. \"\"\"",
            "        skip, include = get_component_values(session, component_name)",
            "",
            "        if skip and include:",
            "            include_q, skip_q = get_include_skip_queries(include, skip)",
            "            return File.id.notin_(include_q.except_(skip_q))",
            "        elif include:",
            "            return and_(*[File.filepath.notlike(conv(fp)) for fp in include])",
            "        elif skip:",
            "            return or_(*[File.filepath.like(conv(fp)) for fp in skip])",
            "",
            "        return None",
            "",
            "    queries = [get_query(n) for (n, ) in component_names]",
            "    return and_(*queries)",
            "",
            "",
            "def get_open_reports_date_filter_query(tbl=Report, date=RunHistory.time):",
            "    \"\"\" Get open reports date filter. \"\"\"",
            "    return and_(tbl.detected_at <= date,",
            "                or_(tbl.fixed_at.is_(None),",
            "                    tbl.fixed_at > date))",
            "",
            "",
            "def get_open_reports_date_filter_query_old(tbl=Report, date=RunHistory.time):",
            "    \"\"\" Get open reports date filter.",
            "",
            "    This function is a dupliation of get_open_reports_date_filter_query().",
            "    For the reson of duplication see the documentation of",
            "    filter_open_reports_in_tags_old().",
            "    \"\"\"",
            "    return tbl.detected_at <= date",
            "",
            "",
            "def get_diff_bug_id_query(session, run_ids, tag_ids, open_reports_date):",
            "    \"\"\" Get bug id query for diff. \"\"\"",
            "    q = session.query(Report.bug_id.distinct())",
            "",
            "    if run_ids:",
            "        q = q.filter(Report.run_id.in_(run_ids))",
            "        if not tag_ids and not open_reports_date:",
            "            q = q.filter(Report.fixed_at.is_(None))",
            "",
            "    if tag_ids:",
            "        q = q.outerjoin(RunHistory,",
            "                        RunHistory.run_id == Report.run_id) \\",
            "            .filter(RunHistory.id.in_(tag_ids)) \\",
            "            .filter(get_open_reports_date_filter_query())",
            "",
            "    if open_reports_date:",
            "        date = datetime.fromtimestamp(open_reports_date)",
            "",
            "        q = q.filter(get_open_reports_date_filter_query(Report, date))",
            "",
            "    return q",
            "",
            "",
            "def get_diff_bug_id_filter(run_ids, tag_ids, open_reports_date):",
            "    \"\"\" Get bug id filter for diff. \"\"\"",
            "    AND = []",
            "",
            "    if run_ids:",
            "        AND.append(Report.run_id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        AND.append(RunHistory.id.in_(tag_ids))",
            "        AND.append(get_open_reports_date_filter_query())",
            "",
            "    if open_reports_date:",
            "        date = datetime.fromtimestamp(open_reports_date)",
            "        AND.append(get_open_reports_date_filter_query(Report, date))",
            "",
            "    return and_(*AND)",
            "",
            "",
            "def get_diff_run_id_query(session, run_ids, tag_ids):",
            "    \"\"\" Get run id query for diff. \"\"\"",
            "    q = session.query(Run.id.distinct())",
            "",
            "    if run_ids:",
            "        q = q.filter(Run.id.in_(run_ids))",
            "",
            "    if tag_ids:",
            "        q = q.outerjoin(RunHistory,",
            "                        RunHistory.run_id == Run.id) \\",
            "            .filter(RunHistory.id.in_(tag_ids))",
            "",
            "    return q",
            "",
            "",
            "def is_cmp_data_empty(cmp_data):",
            "    \"\"\" True if the parameter is None or no filter fields are set. \"\"\"",
            "    if not cmp_data:",
            "        return True",
            "",
            "    return not any([cmp_data.runIds,",
            "                    cmp_data.runTag,",
            "                    cmp_data.openReportsDate])",
            "",
            "",
            "def is_baseline_empty(report_filter):",
            "    \"\"\" True if the parameter is None or no baseline filter fields are set. \"\"\"",
            "    if not report_filter:",
            "        return True",
            "",
            "    return not any([report_filter.runTag,",
            "                    report_filter.openReportsDate])",
            "",
            "",
            "def process_cmp_data_filter(session, run_ids, report_filter, cmp_data):",
            "    \"\"\" Process compare data filter. \"\"\"",
            "    base_tag_ids = report_filter.runTag if report_filter else None",
            "    base_open_reports_date = report_filter.openReportsDate \\",
            "        if report_filter else None",
            "",
            "    if is_cmp_data_empty(cmp_data):",
            "        if not run_ids and is_baseline_empty(report_filter):",
            "            return None, []",
            "",
            "        diff_filter = get_diff_bug_id_filter(",
            "            run_ids, base_tag_ids, base_open_reports_date)",
            "        join_tables = []",
            "",
            "        if run_ids:",
            "            join_tables.append(Run)",
            "        if base_tag_ids:",
            "            join_tables.append(RunHistory)",
            "",
            "        return and_(diff_filter), join_tables",
            "",
            "    query_base = get_diff_bug_id_query(session, run_ids, base_tag_ids,",
            "                                       base_open_reports_date)",
            "    query_base_runs = get_diff_run_id_query(session, run_ids, base_tag_ids)",
            "",
            "    query_new = get_diff_bug_id_query(session, cmp_data.runIds,",
            "                                      cmp_data.runTag,",
            "                                      cmp_data.openReportsDate)",
            "    query_new_runs = get_diff_run_id_query(session, cmp_data.runIds,",
            "                                           cmp_data.runTag)",
            "",
            "    if cmp_data.diffType == DiffType.NEW:",
            "        return and_(Report.bug_id.in_(query_new.except_(query_base)),",
            "                    Report.run_id.in_(query_new_runs)), [Run]",
            "    elif cmp_data.diffType == DiffType.RESOLVED:",
            "        return and_(Report.bug_id.in_(query_base.except_(query_new)),",
            "                    Report.run_id.in_(query_base_runs)), [Run]",
            "    elif cmp_data.diffType == DiffType.UNRESOLVED:",
            "        return and_(Report.bug_id.in_(query_base.intersect(query_new)),",
            "                    Report.run_id.in_(query_new_runs)), [Run]",
            "    else:",
            "        raise codechecker_api_shared.ttypes.RequestFailed(",
            "            codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "            'Unsupported diff type: ' + str(cmp_data.diffType))",
            "",
            "",
            "def process_run_history_filter(query, run_ids, run_history_filter):",
            "    \"\"\"",
            "    Process run history filter.",
            "    \"\"\"",
            "    if run_ids:",
            "        query = query.filter(RunHistory.run_id.in_(run_ids))",
            "",
            "    if run_history_filter:",
            "        if run_history_filter.tagNames:",
            "            OR = [RunHistory.version_tag.ilike('{0}'.format(conv(",
            "                escape_like(name, '\\\\'))), escape='\\\\') for",
            "                name in run_history_filter.tagNames]",
            "",
            "            query = query.filter(or_(*OR))",
            "",
            "        if run_history_filter.tagIds:",
            "            query = query.filter(RunHistory.id.in_(run_history_filter.tagIds))",
            "",
            "        stored = run_history_filter.stored",
            "        if stored:",
            "            if stored.before:",
            "                stored_before = datetime.fromtimestamp(stored.before)",
            "                query = query.filter(RunHistory.time <= stored_before)",
            "",
            "            if stored.after:",
            "                stored_after = datetime.fromtimestamp(stored.after)",
            "                query = query.filter(RunHistory.time >= stored_after)",
            "",
            "    return query",
            "",
            "",
            "def process_run_filter(session, query, run_filter):",
            "    \"\"\"",
            "    Process run filter.",
            "    \"\"\"",
            "    if run_filter is None:",
            "        return query",
            "",
            "    if run_filter.ids:",
            "        query = query.filter(Run.id.in_(run_filter.ids))",
            "    if run_filter.names:",
            "        if run_filter.exactMatch:",
            "            query = query.filter(Run.name.in_(run_filter.names))",
            "        else:",
            "            OR = [Run.name.ilike('{0}'.format(conv(",
            "                escape_like(name, '\\\\'))), escape='\\\\') for",
            "                name in run_filter.names]",
            "            query = query.filter(or_(*OR))",
            "",
            "    if run_filter.beforeTime:",
            "        date = datetime.fromtimestamp(run_filter.beforeTime)",
            "        query = query.filter(Run.date < date)",
            "",
            "    if run_filter.afterTime:",
            "        date = datetime.fromtimestamp(run_filter.afterTime)",
            "        query = query.filter(Run.date > date)",
            "",
            "    if run_filter.beforeRun:",
            "        run = session.query(Run.date) \\",
            "            .filter(Run.name == run_filter.beforeRun) \\",
            "            .one_or_none()",
            "",
            "        if run:",
            "            query = query.filter(Run.date < run.date)",
            "",
            "    if run_filter.afterRun:",
            "        run = session.query(Run.date) \\",
            "            .filter(Run.name == run_filter.afterRun) \\",
            "            .one_or_none()",
            "",
            "        if run:",
            "            query = query.filter(Run.date > run.date)",
            "",
            "    return query",
            "",
            "",
            "def get_report_details(session, report_ids):",
            "    \"\"\"",
            "    Returns report details for the given report ids.",
            "    \"\"\"",
            "    details = {}",
            "",
            "    # Get bug path events.",
            "    bug_path_events = session.query(BugPathEvent, File.filepath) \\",
            "        .filter(BugPathEvent.report_id.in_(report_ids)) \\",
            "        .outerjoin(File,",
            "                   File.id == BugPathEvent.file_id) \\",
            "        .order_by(BugPathEvent.report_id, BugPathEvent.order)",
            "",
            "    bug_events_list = defaultdict(list)",
            "    for event, file_path in bug_path_events:",
            "        report_id = event.report_id",
            "        event = bugpathevent_db_to_api(event)",
            "        event.filePath = file_path",
            "        bug_events_list[report_id].append(event)",
            "",
            "    # Get bug report points.",
            "    bug_report_points = session.query(BugReportPoint, File.filepath) \\",
            "        .filter(BugReportPoint.report_id.in_(report_ids)) \\",
            "        .outerjoin(File,",
            "                   File.id == BugReportPoint.file_id) \\",
            "        .order_by(BugReportPoint.report_id, BugReportPoint.order)",
            "",
            "    bug_point_list = defaultdict(list)",
            "    for bug_point, file_path in bug_report_points:",
            "        report_id = bug_point.report_id",
            "        bug_point = bugreportpoint_db_to_api(bug_point)",
            "        bug_point.filePath = file_path",
            "        bug_point_list[report_id].append(bug_point)",
            "",
            "    # Get extended report data.",
            "    extended_data_list = defaultdict(list)",
            "    q = session.query(ExtendedReportData, File.filepath) \\",
            "        .filter(ExtendedReportData.report_id.in_(report_ids)) \\",
            "        .outerjoin(File,",
            "                   File.id == ExtendedReportData.file_id)",
            "",
            "    for data, file_path in q:",
            "        report_id = data.report_id",
            "        extended_data = extended_data_db_to_api(data)",
            "        extended_data.filePath = file_path",
            "        extended_data_list[report_id].append(extended_data)",
            "",
            "    # Get Comments for report data",
            "    comment_data_list = defaultdict(list)",
            "    comment_query = session.query(Comment, Report.id)\\",
            "        .filter(Report.id.in_(report_ids)) \\",
            "        .outerjoin(Report, Report.bug_id == Comment.bug_hash) \\",
            "        .order_by(Comment.created_at.desc())",
            "",
            "    for data, report_id in comment_query:",
            "        comment_data = comment_data_db_to_api(data)",
            "        comment_data_list[report_id].append(comment_data)",
            "",
            "    for report_id in report_ids:",
            "        details[report_id] = \\",
            "            ReportDetails(pathEvents=bug_events_list[report_id],",
            "                          executionPath=bug_point_list[report_id],",
            "                          extendedData=extended_data_list[report_id],",
            "                          comments=comment_data_list[report_id])",
            "",
            "    return details",
            "",
            "",
            "def bugpathevent_db_to_api(bpe):",
            "    return ttypes.BugPathEvent(",
            "        startLine=bpe.line_begin,",
            "        startCol=bpe.col_begin,",
            "        endLine=bpe.line_end,",
            "        endCol=bpe.col_end,",
            "        msg=bpe.msg,",
            "        fileId=bpe.file_id)",
            "",
            "",
            "def bugreportpoint_db_to_api(brp):",
            "    return BugPathPos(",
            "        startLine=brp.line_begin,",
            "        startCol=brp.col_begin,",
            "        endLine=brp.line_end,",
            "        endCol=brp.col_end,",
            "        fileId=brp.file_id)",
            "",
            "",
            "def extended_data_db_to_api(erd):",
            "    return ttypes.ExtendedReportData(",
            "        type=report_extended_data_type_enum(erd.type),",
            "        startLine=erd.line_begin,",
            "        startCol=erd.col_begin,",
            "        endLine=erd.line_end,",
            "        endCol=erd.col_end,",
            "        message=erd.message,",
            "        fileId=erd.file_id)",
            "",
            "",
            "def comment_data_db_to_api(comm):",
            "    \"\"\"",
            "    Returns a CommentData Object with all the relevant fields",
            "    \"\"\"",
            "    return ttypes.CommentData(",
            "        id=comm.id,",
            "        author=comm.author,",
            "        message=get_comment_msg(comm),",
            "        createdAt=str(comm.created_at),",
            "        kind=comm.kind",
            "    )",
            "",
            "",
            "def get_comment_msg(comment):",
            "    \"\"\"",
            "    Checks for the comment kind. If the comment is",
            "    identified as a system comment, it is formatted accordindly.",
            "    \"\"\"",
            "    context = webserver_context.get_context()",
            "    message = comment.message.decode('utf-8')",
            "    sys_comment = comment_kind_from_thrift_type(ttypes.CommentKind.SYSTEM)",
            "",
            "    if comment.kind == sys_comment:",
            "        try:",
            "            elements = shlex.split(message)",
            "        except ValueError:",
            "            # In earlier CodeChecker we saved system comments",
            "            # without escaping special characters such as",
            "            # quotes. This is kept only for backward",
            "            # compatibility reason.",
            "            message = message \\",
            "                .replace(\"'\", \"\\\\'\") \\",
            "                .replace('\"', '\\\\\"')",
            "",
            "            elements = shlex.split(message)",
            "",
            "        system_comment = context.system_comment_map.get(elements[0])",
            "        if system_comment:",
            "            for idx, value in enumerate(elements[1:]):",
            "                system_comment = system_comment.replace(",
            "                    '{' + str(idx) + '}', html.escape(value))",
            "            return system_comment",
            "",
            "    return html.escape(message)",
            "",
            "",
            "def create_review_data(",
            "    review_status: str,",
            "    message: Optional[str],",
            "    author,",
            "    date,",
            "    is_in_source: bool",
            "):",
            "    return ReviewData(",
            "        status=review_status_enum(review_status),",
            "        comment=None if message is None else message.decode('utf-8'),",
            "        author=author,",
            "        date=None if date is None else str(date),",
            "        isInSource=is_in_source)",
            "",
            "",
            "def apply_report_filter(q, filter_expression,",
            "                        join_tables: List[Any],",
            "                        already_joined_tables: Optional[List[Any]] = None):",
            "    \"\"\"",
            "    Applies the given filter expression and joins the Checker, File, Run, and",
            "    RunHistory tables if necessary based on join_tables parameter. If a table",
            "    is already joined by the main query and this is indicated, that will not",
            "    be joined by this function to prevent a \"duplicate alias\" error.",
            "    \"\"\"",
            "    def needs_join(tbl):",
            "        return tbl in join_tables and (already_joined_tables is None or",
            "                                       tbl not in already_joined_tables)",
            "",
            "    if needs_join(Checker):",
            "        q = q.join(Checker, Report.checker_id == Checker.id)",
            "    if needs_join(File):",
            "        q = q.outerjoin(File, Report.file_id == File.id)",
            "    if needs_join(Run):",
            "        q = q.outerjoin(Run, Run.id == Report.run_id)",
            "    if needs_join(RunHistory):",
            "        q = q.outerjoin(RunHistory, RunHistory.run_id == Report.run_id)",
            "",
            "    return q.filter(filter_expression)",
            "",
            "",
            "def get_sort_map(sort_types, is_unique=False):",
            "    # Get a list of sort_types which will be a nested ORDER BY.",
            "    sort_type_map = {",
            "        SortType.FILENAME: [(File.filepath, 'filepath'),",
            "                            (Report.line, 'line')],",
            "        SortType.BUG_PATH_LENGTH: [(Report.path_length, 'bug_path_length')],",
            "        SortType.CHECKER_NAME: [(Checker.checker_name, 'checker_name')],",
            "        SortType.SEVERITY: [(Checker.severity, 'severity')],",
            "        SortType.REVIEW_STATUS: [(Report.review_status, 'rw_status')],",
            "        SortType.DETECTION_STATUS: [(Report.detection_status, 'dt_status')],",
            "        SortType.TIMESTAMP: [('annotation_timestamp', 'annotation_timestamp')],",
            "        SortType.TESTCASE: [('annotation_testcase', 'annotation_testcase')]}",
            "",
            "    if is_unique:",
            "        sort_type_map[SortType.FILENAME] = [(File.filename, 'filename')]",
            "        sort_type_map[SortType.DETECTION_STATUS] = []",
            "",
            "    # Mapping the SQLAlchemy functions.",
            "    order_type_map = {Order.ASC: asc, Order.DESC: desc}",
            "",
            "    if sort_types is None:",
            "        sort_types = [SortMode(SortType.SEVERITY, Order.DESC)]",
            "",
            "    return sort_types, sort_type_map, order_type_map",
            "",
            "",
            "def sort_results_query(query, sort_types, sort_type_map, order_type_map,",
            "                       order_by_label=False):",
            "    \"\"\"",
            "    Helper method for __queryDiffResults and queryResults to apply sorting.",
            "    \"\"\"",
            "    for sort in sort_types:",
            "        sorttypes = sort_type_map.get(sort.type)",
            "        for sorttype in sorttypes:",
            "            order_type = order_type_map.get(sort.ord)",
            "            sort_col = sorttype[1] if order_by_label else sorttype[0]",
            "            query = query.order_by(order_type(sort_col))",
            "",
            "    return query",
            "",
            "",
            "def filter_unresolved_reports(q):",
            "    \"\"\"",
            "    Filter reports which are unresolved.",
            "",
            "    Note: review status of these reports are not in skip_review_statuses",
            "    and detection statuses are not in skip_detection_statuses.",
            "    \"\"\"",
            "    skip_review_statuses = ['false_positive', 'intentional']",
            "    skip_detection_statuses = ['resolved', 'off', 'unavailable']",
            "",
            "    return q.filter(Report.detection_status.notin_(skip_detection_statuses)) \\",
            "            .filter(Report.review_status.notin_(skip_review_statuses))",
            "",
            "",
            "def check_remove_runs_lock(session, run_ids):",
            "    \"\"\"",
            "    Check if there is an existing lock on the given runs, which has not",
            "    expired yet. If so, the run cannot be deleted, as someone is assumed to",
            "    be storing into it.",
            "    \"\"\"",
            "    locks_expired_at = datetime.now() - timedelta(",
            "        seconds=db_cleanup.RUN_LOCK_TIMEOUT_IN_DATABASE)",
            "",
            "    run_locks = session.query(RunLock.name) \\",
            "        .filter(RunLock.locked_at >= locks_expired_at)",
            "",
            "    if run_ids:",
            "        run_locks = run_locks.filter(Run.id.in_(run_ids))",
            "",
            "    run_locks = run_locks \\",
            "        .outerjoin(Run,",
            "                   Run.name == RunLock.name) \\",
            "        .all()",
            "",
            "    if run_locks:",
            "        raise codechecker_api_shared.ttypes.RequestFailed(",
            "            codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "            \"Can not remove results because the following runs \"",
            "            f\"are locked: {', '.join([r[0] for r in run_locks])}\")",
            "",
            "",
            "def sort_run_data_query(query, sort_mode):",
            "    \"\"\"",
            "    Sort run data query by the given sort type.",
            "    \"\"\"",
            "    # Sort by run date by default.",
            "    if not sort_mode:",
            "        return query.order_by(desc(Run.date))",
            "",
            "    order_type_map = {Order.ASC: asc, Order.DESC: desc}",
            "    order_type = order_type_map.get(sort_mode.ord)",
            "    if sort_mode.type == RunSortType.NAME:",
            "        query = query.order_by(order_type(Run.name))",
            "    elif sort_mode.type == RunSortType.UNRESOLVED_REPORTS:",
            "        query = query.order_by(order_type('report_count'))",
            "    elif sort_mode.type == RunSortType.DATE:",
            "        query = query.order_by(order_type(Run.date))",
            "    elif sort_mode.type == RunSortType.DURATION:",
            "        query = query.order_by(order_type(Run.duration))",
            "    elif sort_mode.type == RunSortType.CC_VERSION:",
            "        query = query.order_by(order_type(RunHistory.cc_version))",
            "",
            "    return query",
            "",
            "",
            "def get_failed_files_query(session, run_ids, query_fields,",
            "                           extra_sub_query_fields=None):",
            "    \"\"\"",
            "    General function to get query to fetch the list of failed files and to get",
            "    the number of failed files.",
            "    \"\"\"",
            "    sub_query_fields = [func.max(RunHistory.id).label('history_id')]",
            "    if extra_sub_query_fields:",
            "        sub_query_fields.extend(extra_sub_query_fields)",
            "",
            "    sub_q = session.query(*sub_query_fields)",
            "",
            "    if run_ids:",
            "        sub_q = sub_q.filter(RunHistory.run_id.in_(run_ids))",
            "",
            "    sub_q = sub_q \\",
            "        .group_by(RunHistory.run_id) \\",
            "        .subquery()",
            "",
            "    query = session \\",
            "        .query(*query_fields) \\",
            "        .outerjoin(sub_q,",
            "                   AnalyzerStatistic.run_history_id == sub_q.c.history_id) \\",
            "        .filter(AnalyzerStatistic.run_history_id == sub_q.c.history_id)",
            "",
            "    return query, sub_q",
            "",
            "",
            "def get_analysis_statistics_query(session, run_ids, run_history_ids=None):",
            "    \"\"\" Get analyzer statistics query. \"\"\"",
            "    query = session.query(AnalyzerStatistic, Run.id)",
            "",
            "    if run_ids:",
            "        # Subquery to get analyzer statistics only for these run history id's.",
            "        history_ids_subq = session.query(",
            "                func.max(AnalyzerStatistic.run_history_id)) \\",
            "            .filter(RunHistory.run_id.in_(run_ids)) \\",
            "            .outerjoin(",
            "                RunHistory,",
            "                RunHistory.id == AnalyzerStatistic.run_history_id) \\",
            "            .group_by(RunHistory.run_id) \\",
            "            .subquery()",
            "",
            "        query = query.filter(",
            "            AnalyzerStatistic.run_history_id.in_(history_ids_subq))",
            "    elif run_history_ids:",
            "        query = query.filter(RunHistory.id.in_(run_history_ids))",
            "",
            "    return query \\",
            "        .outerjoin(RunHistory,",
            "                   RunHistory.id == AnalyzerStatistic.run_history_id) \\",
            "        .outerjoin(Run,",
            "                   Run.id == RunHistory.run_id)",
            "",
            "",
            "def get_commit_url(",
            "    remote_url: Optional[str],",
            "    git_commit_urls: List",
            ") -> Optional[str]:",
            "    \"\"\" Get commit url for the given remote url. \"\"\"",
            "    if not remote_url:",
            "        return None",
            "",
            "    for git_commit_url in git_commit_urls:",
            "        m = git_commit_url[\"regex\"].match(remote_url)",
            "        if m:",
            "            url = git_commit_url[\"url\"]",
            "            for key, value in m.groupdict().items():",
            "                if value is not None:",
            "                    url = url.replace(f\"${key}\", value)",
            "",
            "            return url",
            "",
            "    return None",
            "",
            "",
            "def get_cleanup_plan(session, cleanup_plan_id: int) -> CleanupPlan:",
            "    \"\"\"",
            "    Check if the given cleanup id exists in the database and returns",
            "    the cleanup. Otherwise it will raise an exception.",
            "    \"\"\"",
            "    cleanup_plan = session.query(CleanupPlan).get(cleanup_plan_id)",
            "",
            "    if not cleanup_plan:",
            "        raise codechecker_api_shared.ttypes.RequestFailed(",
            "            codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "            f\"Cleanup plan '{cleanup_plan_id}' was not found in the database.\")",
            "",
            "    return cleanup_plan",
            "",
            "",
            "def get_cleanup_plan_report_hashes(",
            "    session,",
            "    cleanup_plan_ids: List[int]",
            ") -> Dict[int, List[str]]:",
            "    \"\"\" Get report hashes for the given cleanup plan ids. \"\"\"",
            "    cleanup_plan_hashes = defaultdict(list)",
            "",
            "    q = session \\",
            "        .query(",
            "            CleanupPlanReportHash.cleanup_plan_id,",
            "            CleanupPlanReportHash.bug_hash) \\",
            "        .filter(CleanupPlanReportHash.cleanup_plan_id.in_(",
            "            cleanup_plan_ids))",
            "",
            "    for cleanup_plan_id, report_hash in q:",
            "        cleanup_plan_hashes[cleanup_plan_id].append(report_hash)",
            "",
            "    return cleanup_plan_hashes",
            "",
            "",
            "def sort_review_statuses_query(",
            "    query,",
            "    sort_mode: ReviewStatusRuleSortMode,",
            "    report_count_label",
            "):",
            "    \"\"\"",
            "    Sort review status rule query by the given sort mode.",
            "    \"\"\"",
            "    # Sort by rule date by default.",
            "    if not sort_mode:",
            "        return query.order_by(desc(ReviewStatus.date))",
            "",
            "    order_type_map = {Order.ASC: asc, Order.DESC: desc}",
            "    order_type = order_type_map.get(sort_mode.ord)",
            "    if sort_mode.type == ReviewStatusRuleSortType.REPORT_HASH:",
            "        query = query.order_by(order_type(ReviewStatus.bug_hash))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.STATUS:",
            "        query = query.order_by(order_type(ReviewStatus.status))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.AUTHOR:",
            "        query = query.order_by(order_type(ReviewStatus.author))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.DATE:",
            "        query = query.order_by(order_type(ReviewStatus.date))",
            "    elif sort_mode.type == ReviewStatusRuleSortType.ASSOCIATED_REPORTS_COUNT:",
            "        query = query.order_by(order_type(report_count_label))",
            "",
            "    return query",
            "",
            "",
            "def process_rs_rule_filter(",
            "    query,",
            "    rule_filter: Optional[ReviewStatusRuleFilter] = None",
            "):",
            "    \"\"\" Process review status rule filter. \"\"\"",
            "    if rule_filter:",
            "        if rule_filter.reportHashes is not None:",
            "            OR = [ReviewStatus.bug_hash.ilike(conv(report_hash))",
            "                  for report_hash in rule_filter.reportHashes]",
            "            query = query.filter(or_(*OR))",
            "",
            "        if rule_filter.reviewStatuses is not None:",
            "            query = query.filter(",
            "                ReviewStatus.status.in_(",
            "                    map(review_status_str, rule_filter.reviewStatuses)))",
            "",
            "        if rule_filter.authors is not None:",
            "            OR = [ReviewStatus.author.ilike(conv(author))",
            "                  for author in rule_filter.authors]",
            "            query = query.filter(or_(*OR))",
            "",
            "    return query",
            "",
            "",
            "def get_rs_rule_query(",
            "    session: DBSession,",
            "    rule_filter: Optional[ReviewStatusRuleFilter] = None,",
            "    sort_mode: Optional[ReviewStatusRuleSortMode] = None",
            "):",
            "    \"\"\" Returns query to get review status rules. \"\"\"",
            "    report_count = func.count(Report.id).label('report_count')",
            "    q = session \\",
            "        .query(ReviewStatus, report_count) \\",
            "        .join(Report,",
            "              Report.bug_id == ReviewStatus.bug_hash,",
            "              isouter=True)",
            "    q = process_rs_rule_filter(q, rule_filter)",
            "",
            "    if sort_mode:",
            "        q = sort_review_statuses_query(q, sort_mode, report_count)",
            "",
            "    q = q.group_by(ReviewStatus.bug_hash)",
            "",
            "    # Filter review status rules by aggregate columns.",
            "    if rule_filter and rule_filter.noAssociatedReports:",
            "        q = q.having(report_count == 0)",
            "",
            "    return q",
            "",
            "",
            "def get_run_id_expression(session, report_filter):",
            "    \"\"\"",
            "    Get run id or concatenated run id list by the unique mode and the DB type",
            "    \"\"\"",
            "    if report_filter.isUnique:",
            "        if session.bind.dialect.name == \"postgresql\":",
            "            return func.string_agg(",
            "                cast(Run.id, sqlalchemy.String).distinct(),",
            "                ','",
            "            ).label(\"run_id\")",
            "        return func.group_concat(Run.id.distinct()).label(\"run_id\")",
            "    return Run.id.label(\"run_id\")",
            "",
            "",
            "def get_is_enabled_case(subquery):",
            "    \"\"\"",
            "    Creating a case statement to decide the report",
            "    is enabled or not based on the detection status",
            "    \"\"\"",
            "    detection_status_filters = subquery.c.detection_status.in_(list(",
            "        map(detection_status_str,",
            "            (DetectionStatus.OFF, DetectionStatus.UNAVAILABLE))",
            "    ))",
            "",
            "    return case(",
            "        [(detection_status_filters, False)],",
            "        else_=True",
            "    )",
            "",
            "",
            "def get_is_opened_case(subquery):",
            "    \"\"\"",
            "    Creating a case statement to decide the report is opened or not",
            "    based on the detection status and the review status",
            "    \"\"\"",
            "    detection_statuses = (",
            "        DetectionStatus.NEW,",
            "        DetectionStatus.UNRESOLVED,",
            "        DetectionStatus.REOPENED",
            "    )",
            "    review_statuses = (",
            "        API_ReviewStatus.UNREVIEWED,",
            "        API_ReviewStatus.CONFIRMED",
            "    )",
            "    detection_and_review_status_filters = [",
            "        subquery.c.detection_status.in_(list(map(",
            "            detection_status_str, detection_statuses))),",
            "        subquery.c.review_status.in_(list(map(",
            "            review_status_str, review_statuses)))",
            "    ]",
            "    return case(",
            "        [(and_(*detection_and_review_status_filters), True)],",
            "        else_=False",
            "    )",
            "",
            "",
            "class ThriftRequestHandler:",
            "    \"\"\"",
            "    Connect to database and handle thrift client requests.",
            "    \"\"\"",
            "",
            "    def __init__(self,",
            "                 manager,",
            "                 Session,",
            "                 product,",
            "                 auth_session,",
            "                 config_database,",
            "                 package_version,",
            "                 client_version,",
            "                 context):",
            "",
            "        if not product:",
            "            raise ValueError(\"Cannot initialize request handler without \"",
            "                             \"a product to serve.\")",
            "",
            "        self._manager = manager",
            "        self._product = product",
            "        self._auth_session = auth_session",
            "        self._config_database = config_database",
            "        self.__package_version = package_version",
            "        self.__client_version = client_version",
            "        self._Session = Session",
            "        self._context = context",
            "        self.__permission_args = {",
            "            'productID': product.id",
            "        }",
            "",
            "    def _get_username(self):",
            "        \"\"\"",
            "        Returns the actually logged in user name.",
            "        \"\"\"",
            "        return self._auth_session.user if self._auth_session else \"Anonymous\"",
            "",
            "    def _set_run_data_for_curr_product(",
            "        self,",
            "        inc_num_of_runs: Optional[int],",
            "        latest_storage_date: Optional[datetime] = None",
            "    ):",
            "        \"\"\"",
            "        Increment the number of runs related to the current product with the",
            "        given value and set the latest storage date.",
            "        \"\"\"",
            "        values = {}",
            "",
            "        if inc_num_of_runs is not None:",
            "            values[\"num_of_runs\"] = Product.num_of_runs + inc_num_of_runs",
            "            # FIXME: This log is likely overkill.",
            "            LOG.info(\"Run counter in the config database was %s by %i.\",",
            "                     'increased' if inc_num_of_runs >= 0 else 'decreased',",
            "                     abs(inc_num_of_runs))",
            "",
            "        if latest_storage_date is not None:",
            "            values[\"latest_storage_date\"] = latest_storage_date",
            "",
            "        with DBSession(self._config_database) as session:",
            "            session.query(Product) \\",
            "                .filter(Product.id == self._product.id) \\",
            "                .update(values)",
            "",
            "            session.commit()",
            "",
            "    def __require_permission(self, required):",
            "        \"\"\"",
            "        Helper method to raise an UNAUTHORIZED exception if the user does not",
            "        have any of the given permissions.",
            "        \"\"\"",
            "",
            "        with DBSession(self._config_database) as session:",
            "            args = dict(self.__permission_args)",
            "            args['config_db_session'] = session",
            "",
            "            # Anonymous access is only allowed if authentication is",
            "            # turned off",
            "            if self._manager.is_enabled and not self._auth_session:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                    \"You are not authorized to execute this action.\")",
            "",
            "            if not any(permissions.require_permission(",
            "                    perm, args, self._auth_session)",
            "                    for perm in required):",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                    \"You are not authorized to execute this action.\")",
            "",
            "            return True",
            "",
            "    def __require_admin(self):",
            "        self.__require_permission([permissions.PRODUCT_ADMIN])",
            "",
            "    def __require_access(self):",
            "        self.__require_permission([permissions.PRODUCT_ACCESS])",
            "",
            "    def __require_store(self):",
            "        self.__require_permission([permissions.PRODUCT_STORE])",
            "",
            "    def __require_view(self):",
            "        self.__require_permission([permissions.PRODUCT_VIEW])",
            "",
            "    def __add_comment(self, bug_id, message, kind=CommentKindValue.USER,",
            "                      date=None):",
            "        \"\"\" Creates a new comment object. \"\"\"",
            "        user = self._get_username()",
            "        return Comment(bug_id,",
            "                       user,",
            "                       message.encode('utf-8'),",
            "                       kind,",
            "                       date or datetime.now())",
            "",
            "    @timeit",
            "    def getRunData(self, run_filter, limit, offset, sort_mode):",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            # Count the reports subquery.",
            "            stmt = session.query(Report.run_id,",
            "                                 func.count(Report.bug_id)",
            "                                 .label('report_count'))",
            "",
            "            stmt = filter_unresolved_reports(stmt) \\",
            "                .group_by(Report.run_id).subquery()",
            "",
            "            tag_q = session.query(RunHistory.run_id,",
            "                                  func.max(RunHistory.id).label(",
            "                                      'run_history_id'),",
            "                                  func.max(RunHistory.time).label(",
            "                                      'run_history_time')) \\",
            "                .group_by(RunHistory.run_id) \\",
            "                .subquery()",
            "",
            "            q = session.query(Run.id,",
            "                              Run.date,",
            "                              Run.name,",
            "                              Run.duration,",
            "                              RunHistory.version_tag,",
            "                              RunHistory.cc_version,",
            "                              RunHistory.description,",
            "                              stmt.c.report_count)",
            "",
            "            q = process_run_filter(session, q, run_filter)",
            "",
            "            q = q.outerjoin(stmt, Run.id == stmt.c.run_id) \\",
            "                .outerjoin(tag_q, Run.id == tag_q.c.run_id) \\",
            "                .outerjoin(RunHistory,",
            "                           RunHistory.id == tag_q.c.run_history_id) \\",
            "                .group_by(Run.id,",
            "                          RunHistory.version_tag,",
            "                          RunHistory.cc_version,",
            "                          RunHistory.description,",
            "                          stmt.c.report_count)",
            "",
            "            q = sort_run_data_query(q, sort_mode)",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            # Get the runs.",
            "            run_data = q.all()",
            "",
            "            # Set run ids filter by using the previous results.",
            "            if not run_filter:",
            "                run_filter = RunFilter()",
            "",
            "            run_filter.ids = [r[0] for r in run_data]",
            "",
            "            # Get report count for each detection statuses.",
            "            status_q = session.query(Report.run_id,",
            "                                     Report.detection_status,",
            "                                     func.count(Report.bug_id)) \\",
            "                .filter(Report.run_id.in_(run_filter.ids)) \\",
            "                .group_by(Report.run_id, Report.detection_status)",
            "",
            "            status_sum = defaultdict(defaultdict)",
            "            for run_id, status, count in status_q:",
            "                status_sum[run_id][detection_status_enum(status)] = count",
            "",
            "            # Get analyzer statistics.",
            "            analyzer_statistics = defaultdict(defaultdict)",
            "",
            "            stat_q = get_analysis_statistics_query(session, run_filter.ids)",
            "            for analyzer_stat, run_id in stat_q:",
            "                analyzer_statistics[run_id][analyzer_stat.analyzer_type] = \\",
            "                    ttypes.AnalyzerStatistics(",
            "                        failed=analyzer_stat.failed,",
            "                        successful=analyzer_stat.successful)",
            "",
            "            results = []",
            "",
            "            for run_id, run_date, run_name, duration, tag, cc_version, \\",
            "                description, report_count \\",
            "                    in run_data:",
            "",
            "                if report_count is None:",
            "                    report_count = 0",
            "",
            "                analyzer_stats = analyzer_statistics[run_id]",
            "                results.append(RunData(runId=run_id,",
            "                                       runDate=str(run_date),",
            "                                       name=run_name,",
            "                                       duration=duration,",
            "                                       resultCount=report_count,",
            "                                       detectionStatusCount=status_sum[run_id],",
            "                                       versionTag=tag,",
            "                                       codeCheckerVersion=cc_version,",
            "                                       analyzerStatistics=analyzer_stats,",
            "                                       description=description))",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunCount(self, run_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            query = session.query(Run.id)",
            "            query = process_run_filter(session, query, run_filter)",
            "",
            "        return query.count()",
            "",
            "    # DEPRECATED: use getAnalysisInfo API function instead of this function.",
            "    def getCheckCommand(self, run_history_id, run_id):",
            "        \"\"\" Get analyzer command based on the given filter. \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = None",
            "        offset = 0",
            "        analysis_info_filter = AnalysisInfoFilter(",
            "            runId=run_id,",
            "            runHistoryId=run_history_id)",
            "",
            "        analysis_info = self.getAnalysisInfo(",
            "            analysis_info_filter, limit, offset)",
            "",
            "        return \"; \".join([html.escape(i.analyzerCommand)",
            "                          for i in analysis_info])",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalysisInfo(self, analysis_info_filter, limit, offset):",
            "        \"\"\" Get analysis information based on the given filter. \"\"\"",
            "        self.__require_view()",
            "",
            "        res: List[ttypes.AnalysisInfo] = []",
            "        if not analysis_info_filter:",
            "            return res",
            "",
            "        analysis_info_query = None",
            "        with DBSession(self._Session) as session:",
            "            run_id = analysis_info_filter.runId",
            "            run_history_ids = None",
            "            if run_id is not None:",
            "                run_history_ids = session \\",
            "                    .query(RunHistory.id) \\",
            "                    .filter(RunHistory.run_id == run_id) \\",
            "                    .order_by(RunHistory.time.desc()) \\",
            "                    .limit(1)",
            "",
            "            if run_history_ids is None:",
            "                run_history_ids = [analysis_info_filter.runHistoryId]",
            "",
            "            if run_history_ids is not None:",
            "                rh_a_tbl = RunHistoryAnalysisInfo",
            "                analysis_info_query = session.query(AnalysisInfo) \\",
            "                    .outerjoin(",
            "                        rh_a_tbl,",
            "                        rh_a_tbl.c.analysis_info_id == AnalysisInfo.id) \\",
            "                    .filter(rh_a_tbl.c.run_history_id.in_(run_history_ids))",
            "",
            "            report_id = analysis_info_filter.reportId",
            "            if report_id is not None:",
            "                r_a_tbl = ReportAnalysisInfo",
            "                analysis_info_query = session.query(AnalysisInfo) \\",
            "                    .outerjoin(",
            "                        r_a_tbl,",
            "                        r_a_tbl.c.analysis_info_id == AnalysisInfo.id) \\",
            "                    .filter(r_a_tbl.c.report_id == report_id)",
            "",
            "            if analysis_info_query:",
            "                if limit:",
            "                    analysis_info_query = analysis_info_query \\",
            "                        .limit(limit).offset(offset)",
            "",
            "                for cmd in analysis_info_query:",
            "                    command = zlib.decompress(cmd.analyzer_command) \\",
            "                        .decode(\"utf-8\")",
            "",
            "                    checkers_q = session \\",
            "                        .query(Checker.analyzer_name,",
            "                               Checker.checker_name,",
            "                               DB_AnalysisInfoChecker.enabled) \\",
            "                        .join(Checker, DB_AnalysisInfoChecker.checker_id ==",
            "                              Checker.id) \\",
            "                        .filter(DB_AnalysisInfoChecker.",
            "                                analysis_info_id == cmd.id)",
            "",
            "                    checkers: Dict[str, Dict[str, API_AnalysisInfoChecker]] = \\",
            "                        defaultdict(dict)",
            "                    for chk in checkers_q.all():",
            "                        analyzer, checker, enabled = chk",
            "                        checkers[analyzer][checker] = API_AnalysisInfoChecker(",
            "                            enabled=enabled)",
            "",
            "                    res.append(ttypes.AnalysisInfo(",
            "                        analyzerCommand=html.escape(command),",
            "                        checkers=checkers))",
            "",
            "        return res",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunHistory(self, run_ids, limit, offset, run_history_filter):",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            res = session.query(RunHistory)",
            "",
            "            res = process_run_history_filter(res, run_ids, run_history_filter)",
            "",
            "            res = res.order_by(RunHistory.time.desc())",
            "",
            "            if limit:",
            "                res = res.limit(limit).offset(offset)",
            "",
            "            results = []",
            "            for history in res:",
            "                analyzer_statistics = {}",
            "                for analyzer_stat in history.analyzer_statistics:",
            "                    analyzer_statistics[analyzer_stat.analyzer_type] = \\",
            "                        ttypes.AnalyzerStatistics(",
            "                            failed=analyzer_stat.failed,",
            "                            successful=analyzer_stat.successful)",
            "",
            "                results.append(RunHistoryData(",
            "                    id=history.id,",
            "                    runId=history.run.id,",
            "                    runName=history.run.name,",
            "                    versionTag=history.version_tag,",
            "                    user=history.user,",
            "                    time=str(history.time),",
            "                    codeCheckerVersion=history.cc_version,",
            "                    analyzerStatistics=analyzer_statistics,",
            "                    description=history.description))",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunHistoryCount(self, run_ids, run_history_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            query = session.query(RunHistory.id)",
            "            query = process_run_history_filter(query,",
            "                                               run_ids,",
            "                                               run_history_filter)",
            "",
            "        return query.count()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReport(self, reportId):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            result = session \\",
            "                .query(Report, File) \\",
            "                .filter(Report.id == reportId) \\",
            "                .join(Checker, Report.checker_id == Checker.id) \\",
            "                .options(contains_eager(Report.checker)) \\",
            "                .outerjoin(File, Report.file_id == File.id) \\",
            "                .limit(1).one_or_none()",
            "",
            "            if not result:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    \"Report \" + str(reportId) + \" not found!\")",
            "",
            "            report, source_file = result",
            "            return ReportData(",
            "                runId=report.run_id,",
            "                bugHash=report.bug_id,",
            "                checkedFile=source_file.filepath,",
            "                checkerMsg=report.checker_message,",
            "                reportId=report.id,",
            "                fileId=source_file.id,",
            "                line=report.line,",
            "                column=report.column,",
            "                analyzerName=report.checker.analyzer_name,",
            "                checkerId=report.checker.checker_name,",
            "                severity=report.checker.severity,",
            "                reviewData=create_review_data(",
            "                    report.review_status,",
            "                    report.review_status_message,",
            "                    report.review_status_author,",
            "                    report.review_status_date,",
            "                    report.review_status_is_in_source),",
            "                detectionStatus=detection_status_enum(report.detection_status),",
            "                detectedAt=str(report.detected_at),",
            "                fixedAt=str(report.fixed_at) if report.fixed_at else None)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getDiffResultsHash(self, run_ids, report_hashes, diff_type,",
            "                           skip_detection_statuses, tag_ids):",
            "        self.__require_view()",
            "",
            "        # FIXME: This getDiffResultsHash() function is returning a set of",
            "        # reports based on what are they compared to in a \"CodeChecker cmd",
            "        # diff\" command. Earlier this function didn't consider false positive",
            "        # and intentional reports as closed reports. The client's behavior also",
            "        # changed from CodeChecker 6.20.0 and this behavior is adapted to the",
            "        # new server behavior. The problem is that the old client works",
            "        # correcly only with the old server. For this reason we are branching",
            "        # based on the client's version. We are having access to the Thrift",
            "        # API version here. The behavior change happend in Thrift API version",
            "        # 6.50.",
            "        client_version = tuple(map(int, self.__client_version.split('.')))",
            "",
            "        if not skip_detection_statuses:",
            "            skip_detection_statuses = [ttypes.DetectionStatus.RESOLVED,",
            "                                       ttypes.DetectionStatus.OFF,",
            "                                       ttypes.DetectionStatus.UNAVAILABLE]",
            "",
            "        # Convert statuses to string.",
            "        skip_statuses_str = [detection_status_str(status)",
            "                             for status in skip_detection_statuses]",
            "",
            "        with DBSession(self._Session) as session:",
            "            if diff_type == DiffType.NEW:",
            "                # In postgresql we can select multiple rows filled with",
            "                # constants by using `unnest` function. In sqlite we have to",
            "                # use multiple UNION ALL.",
            "",
            "                if not report_hashes:",
            "                    return []",
            "",
            "                base_hashes = session.query(Report.bug_id.label('bug_id')) \\",
            "                    .outerjoin(File, Report.file_id == File.id)",
            "",
            "                if client_version >= (6, 50):",
            "                    base_hashes = base_hashes.filter(",
            "                        Report.detection_status.notin_(skip_statuses_str),",
            "                        Report.fixed_at.is_(None))",
            "                    base_hashes = filter_open_reports_in_tags(",
            "                        base_hashes, run_ids, tag_ids)",
            "                else:",
            "                    base_hashes = base_hashes.filter(",
            "                        Report.detection_status.notin_(skip_statuses_str))",
            "                    base_hashes = filter_open_reports_in_tags_old(",
            "                        base_hashes, run_ids, tag_ids)",
            "",
            "                if self._product.driver_name == 'postgresql':",
            "                    new_hashes = select([func.unnest(report_hashes)",
            "                                         .label('bug_id')]) \\",
            "                        .except_(base_hashes).alias('new_bugs')",
            "                    return [res[0] for res in session.query(new_hashes)]",
            "                else:",
            "                    # The maximum number of compound select in sqlite is 500",
            "                    # by default. We increased SQLITE_MAX_COMPOUND_SELECT",
            "                    # limit but when the number of compound select was larger",
            "                    # than 8435 sqlite threw a `Segmentation fault` error.",
            "                    # For this reason we create queries with chunks.",
            "                    new_hashes = []",
            "                    for chunk in util.chunks(",
            "                            iter(report_hashes), SQLITE_MAX_COMPOUND_SELECT):",
            "                        new_hashes_query = union_all(*[",
            "                            select([bindparam('bug_id' + str(i), h)",
            "                                    .label('bug_id')])",
            "                            for i, h in enumerate(chunk)])",
            "                        q = select([new_hashes_query]).except_(base_hashes)",
            "                        new_hashes.extend([res[0] for res in session.query(q)])",
            "",
            "                    return new_hashes",
            "            elif diff_type == DiffType.RESOLVED:",
            "                results = session.query(Report.bug_id)",
            "",
            "                if client_version >= (6, 50):",
            "                    results = results.filter(or_(",
            "                        Report.bug_id.notin_(report_hashes),",
            "                        Report.fixed_at.isnot(None)))",
            "                    results = filter_open_reports_in_tags(",
            "                        results, run_ids, tag_ids)",
            "                else:",
            "                    results = results.filter(",
            "                        Report.bug_id.notin_(report_hashes))",
            "                    results = filter_open_reports_in_tags_old(",
            "                        results, run_ids, tag_ids)",
            "",
            "                return [res[0] for res in results]",
            "            elif diff_type == DiffType.UNRESOLVED:",
            "                results = session.query(Report.bug_id) \\",
            "                    .filter(Report.bug_id.in_(report_hashes))",
            "",
            "                if client_version >= (6, 50):",
            "                    results = results \\",
            "                        .filter(Report.detection_status.notin_(",
            "                            skip_statuses_str)) \\",
            "                        .filter(Report.fixed_at.is_(None))",
            "                    results = filter_open_reports_in_tags(",
            "                        results, run_ids, tag_ids)",
            "                else:",
            "                    results = results.filter(",
            "                        Report.detection_status.notin_(skip_statuses_str))",
            "                    results = filter_open_reports_in_tags_old(",
            "                        results, run_ids, tag_ids)",
            "",
            "                return [res[0] for res in results]",
            "            else:",
            "                return []",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunResults(self, run_ids, limit, offset, sort_types,",
            "                      report_filter, cmp_data, get_details):",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        with DBSession(self._Session) as session:",
            "            results = []",
            "",
            "            # Extending \"reports\" table with report annotation columns.",
            "            #",
            "            # Suppose that we have these tables in the database:",
            "            #",
            "            # reports",
            "            # =================",
            "            # id, checker_id, ...",
            "            # -------------------",
            "            # 1,  123456,     ...",
            "            # 2,  999999,     ...",
            "            #",
            "            # report_annotations",
            "            # =======================",
            "            # report_id, key,  value",
            "            # -----------------------",
            "            # 1,         key1, value1",
            "            # 1,         key2, value2",
            "            # 2,         key1, value3",
            "            #",
            "            # The resulting table should look like this:",
            "            #",
            "            # reports extended",
            "            # ===================================================",
            "            # id, checker_id, ..., annotation_key1, annotation_key2",
            "            # ---------------------------------------------------",
            "            # 1,  123456,     ..., value1,          value2",
            "            # 2,  999999,     ..., value3,          NULL",
            "            #",
            "            # The SQL query which results this table is similar to this:",
            "            #",
            "            # SELECT",
            "            #   <every column in \"reports\" table>,",
            "            #   MAX(CASE WHEN report_annotations.key == <col1> THEN",
            "            #       report_annotations.value END) AS annotation_<col1>",
            "            #   MAX(CASE WHEN report_annotations.key == <col2> THEN",
            "            #       report_annotations.value END) AS annotation_<col2>",
            "            # FROM",
            "            #   reports",
            "            #   LEFT OUTER JOIN report_annotations ON",
            "            #     report_annotations.report_id = reports.id",
            "            # GROUP BY",
            "            #   reports.id;",
            "            #",
            "            # <col1>, <col2>... are the distinct keys in table",
            "            # \"report_annotations\". These are collected in a previous query.",
            "            #",
            "            # Since the \"join\" operation makes a Cartesian product of the two",
            "            # tables, the resulting table contains as many rows for a report as",
            "            # many annotations belong to it. These have to be joined by report",
            "            # ID and this is the reason of the aggregating MAX() functions.",
            "            #",
            "            # TODO: The creation of this extended table should be produced by",
            "            # a helper function and it could be used as a sub-query in every",
            "            # other query which originally works on \"reports\" table.",
            "",
            "            annotation_keys = list(map(",
            "                lambda x: x[0],",
            "                session.query(ReportAnnotations.key).distinct().all()))",
            "",
            "            annotation_cols = OrderedDict()",
            "            for col in annotation_keys:",
            "                annotation_cols[col] = func.max(sqlalchemy.case([(",
            "                    ReportAnnotations.key == col,",
            "                    ReportAnnotations.value)])).label(f\"annotation_{col}\")",
            "",
            "            if report_filter.isUnique:",
            "                # A report annotation filter cannot be set in WHERE clause if",
            "                # we use annotation parameters in aggregate functions to",
            "                # create a pivot table. Instead of filtering report",
            "                # annotations in WHERE clause, we should use HAVING clause",
            "                # only for filtering aggregate functions.",
            "                # TODO: Fixing report annotation filter in every report server",
            "                # endpoint function.",
            "                annotations_backup = report_filter.annotations",
            "                report_filter.annotations = None",
            "                filter_expression, join_tables = process_report_filter(",
            "                    session, run_ids, report_filter, cmp_data)",
            "",
            "                sort_types, sort_type_map, order_type_map = \\",
            "                    get_sort_map(sort_types, True)",
            "",
            "                # TODO: Create a helper function for common section of unique",
            "                # and non unique modes.",
            "                sub_query = session.query(Report,",
            "                                          File.filename,",
            "                                          Checker.analyzer_name,",
            "                                          Checker.checker_name,",
            "                                          Checker.severity,",
            "                                          func.row_number().over(",
            "                                            partition_by=Report.bug_id,",
            "                                            order_by=desc(Report.id)",
            "                                          ).label(\"row_num\"),",
            "                                          *annotation_cols.values()) \\",
            "                                   .join(Checker,",
            "                                         Report.checker_id == Checker.id) \\",
            "                                   .options(contains_eager(Report.checker)) \\",
            "                                   .outerjoin(File,",
            "                                              Report.file_id == File.id) \\",
            "                                   .outerjoin(ReportAnnotations,",
            "                                              Report.id ==",
            "                                              ReportAnnotations.report_id)",
            "",
            "                sub_query = apply_report_filter(sub_query,",
            "                                                filter_expression,",
            "                                                join_tables,",
            "                                                [File, Checker])",
            "",
            "                sub_query = sub_query.group_by(Report.id, File.id, Checker.id)",
            "",
            "                if annotations_backup:",
            "                    annotations = defaultdict(list)",
            "                    for annotation in annotations_backup:",
            "                        annotations[annotation.first].append(annotation.second)",
            "",
            "                    OR = []",
            "                    for key, values in annotations.items():",
            "                        OR.extend([annotation_cols[key].ilike(conv(v))",
            "                                   for v in values])",
            "                    sub_query = sub_query.having(or_(*OR))",
            "",
            "                sub_query = sort_results_query(sub_query,",
            "                                               sort_types,",
            "                                               sort_type_map,",
            "                                               order_type_map)",
            "",
            "                sub_query = sub_query.subquery().alias()",
            "",
            "                q = session.query(sub_query) \\",
            "                           .filter(sub_query.c.row_num == 1) \\",
            "                           .limit(limit).offset(offset)",
            "",
            "                QueryResult = namedtuple('QueryResult', sub_query.c.keys())",
            "                query_result = [QueryResult(*row) for row in q.all()]",
            "",
            "                # Get report details if it is required.",
            "                report_details = {}",
            "                if get_details:",
            "                    report_ids = [r.id for r in query_result]",
            "                    report_details = get_report_details(session, report_ids)",
            "",
            "                for row in query_result:",
            "                    annotations = {",
            "                        k: v for k, v in zip(",
            "                            annotation_keys,",
            "                            [getattr(row, 'annotation_testcase', None),",
            "                             getattr(row, 'annotation_timestamp', None)]",
            "                            ) if v is not None}",
            "",
            "                    review_data = create_review_data(",
            "                        row.review_status,",
            "                        row.review_status_message,",
            "                        row.review_status_author,",
            "                        row.review_status_date,",
            "                        row.review_status_is_in_source)",
            "",
            "                    results.append(",
            "                        ReportData(runId=row.run_id,",
            "                                   bugHash=row.bug_id,",
            "                                   checkedFile=row.filename,",
            "                                   checkerMsg=row.checker_message,",
            "                                   reportId=row.id,",
            "                                   fileId=row.file_id,",
            "                                   line=row.line,",
            "                                   column=row.column,",
            "                                   analyzerName=row.analyzer_name,",
            "                                   checkerId=row.checker_name,",
            "                                   severity=row.severity,",
            "                                   reviewData=review_data,",
            "                                   detectionStatus=detection_status_enum(",
            "                                    row.detection_status),",
            "                                   detectedAt=str(row.detected_at),",
            "                                   fixedAt=str(row.fixed_at),",
            "                                   bugPathLength=row.path_length,",
            "                                   details=report_details.get(row.id),",
            "                                   annotations=annotations))",
            "            else:  # not is_unique",
            "                filter_expression, join_tables = process_report_filter(",
            "                    session, run_ids, report_filter, cmp_data,",
            "                    keep_all_annotations=True)",
            "",
            "                sort_types, sort_type_map, order_type_map = \\",
            "                    get_sort_map(sort_types)",
            "",
            "                q = session.query(Report,",
            "                                  File.filepath,",
            "                                  *annotation_cols.values()) \\",
            "                    .join(Checker,",
            "                          Report.checker_id == Checker.id) \\",
            "                    .options(contains_eager(Report.checker)) \\",
            "                    .outerjoin(File,",
            "                               Report.file_id == File.id) \\",
            "                    .outerjoin(",
            "                        ReportAnnotations,",
            "                        Report.id == ReportAnnotations.report_id)",
            "",
            "                # Grouping by \"reports.id\" is described at the beginning of",
            "                # this function. Grouping by \"files.id\" is necessary, because",
            "                # \"files\" table is joined for gathering file names belonging to",
            "                # the given report. According to SQL syntax if there is a group",
            "                # by report IDs then files should also be either grouped or an",
            "                # aggregate function must be applied on them. The same applies",
            "                # to the \"checkers\" table.",
            "                q = q.group_by(Report.id, File.id, Checker.id)",
            "",
            "                # The \"Checker\" entity is eagerly loaded for each \"Report\" as",
            "                # there is a guaranteed FOREIGN KEY ... NOT NULL relationship",
            "                # to a valid entity. Because of this, letting \"join_tables\"",
            "                # add \"Checker\" here is actually ill-formed, as it would",
            "                # result in queries that ambiguously refer to the same table.",
            "                q = apply_report_filter(q, filter_expression, join_tables,",
            "                                        [File, Checker])",
            "                q = sort_results_query(q,",
            "                                       sort_types,",
            "                                       sort_type_map,",
            "                                       order_type_map)",
            "",
            "                # Most queries are using paging of reports due their great",
            "                # number. This is implemented by LIMIT and OFFSET in the SQL",
            "                # queries. However, if there is no ordering in the query, then",
            "                # the reports in different pages may overlap. This ordering",
            "                # prevents it.",
            "                q = q.order_by(Report.id)",
            "",
            "                if report_filter.annotations is not None:",
            "                    annotations = defaultdict(list)",
            "                    for annotation in report_filter.annotations:",
            "                        annotations[annotation.first].append(annotation.second)",
            "",
            "                    OR = []",
            "                    for key, values in annotations.items():",
            "                        OR.extend([annotation_cols[key].ilike(conv(v))",
            "                                   for v in values])",
            "                    q = q.having(or_(*OR))",
            "",
            "                q = q.limit(limit).offset(offset)",
            "",
            "                query_result = q.all()",
            "",
            "                # Get report details if it is required.",
            "                report_details = {}",
            "                if get_details:",
            "                    report_ids = [r[0].id for r in query_result]",
            "                    report_details = get_report_details(session, report_ids)",
            "",
            "                for row in query_result:",
            "                    report, filepath = row[0], row[1]",
            "                    annotations = {",
            "                        k: v for k, v in zip(annotation_keys, row[2:])",
            "                        if v is not None}",
            "",
            "                    review_data = create_review_data(",
            "                        report.review_status,",
            "                        report.review_status_message,",
            "                        report.review_status_author,",
            "                        report.review_status_date,",
            "                        report.review_status_is_in_source)",
            "",
            "                    results.append(",
            "                        ReportData(runId=report.run_id,",
            "                                   bugHash=report.bug_id,",
            "                                   checkedFile=filepath,",
            "                                   checkerMsg=report.checker_message,",
            "                                   reportId=report.id,",
            "                                   fileId=report.file_id,",
            "                                   line=report.line,",
            "                                   column=report.column,",
            "                                   analyzerName=report.checker.analyzer_name,",
            "                                   checkerId=report.checker.checker_name,",
            "                                   severity=report.checker.severity,",
            "                                   reviewData=review_data,",
            "                                   detectionStatus=detection_status_enum(",
            "                                       report.detection_status),",
            "                                   detectedAt=str(report.detected_at),",
            "                                   fixedAt=str(report.fixed_at) if",
            "                                   report.fixed_at else None,",
            "                                   bugPathLength=report.path_length,",
            "                                   details=report_details.get(report.id),",
            "                                   annotations=annotations))",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReportAnnotations(self, run_ids, report_filter, cmp_data):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "                extended_table = extended_table.add_columns(",
            "                    ReportAnnotations.key.label('annotations_key'),",
            "                    ReportAnnotations.value.label('annotations_value')",
            "                ).group_by(ReportAnnotations.key, ReportAnnotations.value)",
            "",
            "                extended_table = extended_table.subquery()",
            "",
            "                result = session.query(extended_table.c.annotations_value) \\",
            "                    .distinct() \\",
            "                    .filter(",
            "                        *(extended_table.c.annotations_key == annotation.first",
            "                          for annotation in report_filter.annotations)) \\",
            "                    .all()",
            "            else:",
            "                extended_table = extended_table.subquery()",
            "",
            "                result = session.query(ReportAnnotations.value) \\",
            "                    .distinct() \\",
            "                    .join(",
            "                        extended_table,",
            "                        ReportAnnotations.report_id == extended_table.c.id) \\",
            "                    .all()",
            "",
            "        return list(map(lambda x: x[0], result))",
            "",
            "    @timeit",
            "    def getRunReportCounts(self, run_ids, report_filter, limit, offset):",
            "        \"\"\"",
            "          Count the results separately for multiple runs.",
            "          If an empty run id list is provided the report",
            "          counts will be calculated for all of the available runs.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = []",
            "",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter)",
            "",
            "            reports_subq = session.query(Report.bug_id, Report.run_id)",
            "            reports_subq = apply_report_filter(",
            "                reports_subq, filter_expression, join_tables)",
            "",
            "            if report_filter.annotations is not None:",
            "                reports_subq = reports_subq.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                reports_subq = reports_subq.group_by(Report.id)",
            "",
            "            reports_subq = reports_subq.subquery()",
            "",
            "            if report_filter is not None and report_filter.isUnique:",
            "                count_col = func.count(reports_subq.c.bug_id.distinct())",
            "            else:",
            "                count_col = func.count(literal_column('*'))",
            "",
            "            q = session.query(Run.id, func.max(Run.name), count_col) \\",
            "                .select_from(reports_subq) \\",
            "                .join(Run, Run.id == reports_subq.c.run_id) \\",
            "                .group_by(Run.id) \\",
            "                .order_by(Run.name)",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            for run_id, run_name, count in q:",
            "                report_count = RunReportCount(runId=run_id,",
            "                                              name=run_name,",
            "                                              reportCount=count)",
            "                results.append(report_count)",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunResultCount(self, run_ids, report_filter, cmp_data):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(Report.bug_id.distinct())",
            "            else:",
            "                q = session.query(Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                q = q.outerjoin(ReportAnnotations,",
            "                                ReportAnnotations.report_id == Report.id)",
            "                q = q.group_by(Report.id)",
            "",
            "            q = apply_report_filter(q, filter_expression, join_tables)",
            "",
            "            report_count = q.count()",
            "            if report_count is None:",
            "                report_count = 0",
            "",
            "            return report_count",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReportDetails(self, reportId):",
            "        \"\"\"",
            "        Parameters:",
            "         - reportId",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            return get_report_details(session, [reportId])[reportId]",
            "",
            "    def _setReviewStatus(self, session, report_hash, status,",
            "                         message, date=None):",
            "        \"\"\"",
            "        This function sets the review status of all the reports of a",
            "        given hash. This is the implementation of addReviewStatusRule(),",
            "        but it is also extended with a session parameter which represents a",
            "        database transaction. This is needed because during storage a specific",
            "        session object has to be used.",
            "        \"\"\"",
            "",
            "        review_status = session.query(ReviewStatus).get(report_hash)",
            "        if review_status is None:",
            "            review_status = ReviewStatus()",
            "            review_status.bug_hash = report_hash",
            "",
            "        old_status = review_status.status or \\",
            "            review_status_str(ttypes.ReviewStatus.UNREVIEWED)",
            "        old_msg = review_status.message or None",
            "",
            "        new_status = review_status_str(status)",
            "        new_user = self._get_username()",
            "        new_message = message.encode('utf8') if message else b''",
            "",
            "        # Review status is a shared table among runs. When multiple runs",
            "        # are stored in parallel, there may be a race condition in updating",
            "        # review status fields. The most common reason of deadlocks is",
            "        # changing only the date to current date. This condition checks if",
            "        # something else is also changed other than dates.",
            "        # We assume that report status in source code comments belong to",
            "        # the first user who stored the reports. If another user stores the",
            "        # same project with same report status then we don't change it.",
            "        if (old_status, old_msg) == (new_status, new_message):",
            "            return None",
            "",
            "        review_status.status = new_status",
            "        review_status.author = new_user",
            "        review_status.message = new_message",
            "        review_status.date = date or datetime.now()",
            "        session.add(review_status)",
            "",
            "        # Create a system comment if the review status or the message",
            "        # is changed.",
            "        old_review_status = old_status.capitalize()",
            "        new_review_status = review_status.status.capitalize()",
            "        if message:",
            "            system_comment_msg = \\",
            "                f'rev_st_changed_msg {old_review_status} ' \\",
            "                f'{new_review_status} {shlex.quote(message)}'",
            "        else:",
            "            system_comment_msg = \\",
            "                f'rev_st_changed {old_review_status} {new_review_status}'",
            "",
            "        system_comment = self.__add_comment(review_status.bug_hash,",
            "                                            system_comment_msg,",
            "                                            CommentKindValue.SYSTEM,",
            "                                            review_status.date)",
            "        session.add(system_comment)",
            "",
            "        # False positive and intentional reports are considered closed, so",
            "        # their \"fix date\" is set. The reports are reopened when they become",
            "        # unreviewed or confirmed again. Don't change \"fix date\" for closed",
            "        # report which remain closed.",
            "        if review_status.status in [\"false_positive\", \"intentional\"]:",
            "            session \\",
            "                .query(Report) \\",
            "                .filter(Report.bug_id == report_hash) \\",
            "                .filter(Report.detection_status.in_([",
            "                    \"unresolved\", \"new\", \"reopened\"])) \\",
            "                .filter(Report.review_status.notin_(",
            "                    [\"false_positive\", \"intentional\"])) \\",
            "                .filter(Report.review_status_is_in_source.is_(False)) \\",
            "                .update(",
            "                    {\"fixed_at\": review_status.date},",
            "                    synchronize_session=False)",
            "        else:",
            "            reports = session \\",
            "                .query(Report) \\",
            "                .filter(",
            "                    Report.bug_id == report_hash,",
            "                    Report.detection_status.in_([",
            "                        \"unresolved\", \"new\", \"reopened\"]),",
            "                    Report.review_status.in_([",
            "                        \"false_positive\", \"intentional\"]))",
            "",
            "            session \\",
            "                .query(Report) \\",
            "                .filter(Report.id.in_(",
            "                    map(lambda report: report.id, reports))) \\",
            "                .filter(Report.review_status_is_in_source.is_(False)) \\",
            "                .update({\"fixed_at\": None}, synchronize_session=False)",
            "",
            "        session \\",
            "            .query(Report) \\",
            "            .filter(Report.review_status_is_in_source.is_(False)) \\",
            "            .filter(Report.bug_id == report_hash) \\",
            "            .update({",
            "                'review_status': review_status.status,",
            "                'review_status_author': review_status.author,",
            "                'review_status_message': review_status.message,",
            "                'review_status_date': review_status.date})",
            "",
            "        session.flush()",
            "",
            "        return None",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def isReviewStatusChangeDisabled(self):",
            "        \"\"\"",
            "        Return True if review status change is disabled.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        with DBSession(self._config_database) as session:",
            "            product = session.query(Product).get(self._product.id)",
            "            return product.is_review_status_change_disabled",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def changeReviewStatus(self, report_id, status, message):",
            "        \"\"\"",
            "        Change the review status of a report by report id.",
            "        \"\"\"",
            "        self.__require_permission([permissions.PRODUCT_ACCESS,",
            "                                   permissions.PRODUCT_STORE])",
            "",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            if report:",
            "                # False positive and intentional reports are considered closed,",
            "                # so their \"fix date\" is set. The reports are reopened when",
            "                # they become unreviewed or confirmed again.",
            "                # Don't change \"fix date\" for closed",
            "                # report which remain closed.",
            "                if status in [\"false_positive\", \"intentional\"]:",
            "                    if report.detection_status in [",
            "                            \"unresolved\", \"new\", \"reopened\"]\\",
            "                        and report.review_status not in [",
            "                            \"false_positive\", \"intentional\"]:",
            "                        session.query(Report).filter(",
            "                            Report.id == report_id).update(",
            "                                {\"fixed_at\": datetime.now()})",
            "                elif report.detection_status in [",
            "                    \"unresolved\", \"new\", \"reopened\"]\\",
            "                    and report.review_status in [",
            "                        \"false_positive\", \"intentional\"]:",
            "                    session.query(Report).filter(",
            "                        Report.id == report_id).update({\"fixed_at\": None})",
            "",
            "                session.query(Report).filter(Report.id == report_id).update({",
            "                        'review_status': review_status_str(status),",
            "                        'review_status_author': self._get_username(),",
            "                        'review_status_message': bytes(message, 'utf-8'),",
            "                        'review_status_date': datetime.now()",
            "                        })",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    \"No report found in the database.\")",
            "            session.commit()",
            "",
            "            LOG.info(\"Review status of report '%s' was changed to '%s' by %s.\",",
            "                     report_id, review_status_str(status),",
            "                     self._get_username())",
            "",
            "        return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReviewStatusRules(self, rule_filter, sort_mode, limit, offset):",
            "        self.__require_view()",
            "        if not sort_mode:",
            "            sort_mode = ReviewStatusRuleSortMode(",
            "                type=ReviewStatusRuleSortType.DATE,",
            "                ord=Order.DESC)",
            "",
            "        result = []",
            "",
            "        # To avoid modifiying the collection due to chunking",
            "        rule_filter_copy = deepcopy(rule_filter)",
            "",
            "        def getRules(reportHashes=None):",
            "            if rule_filter and reportHashes:",
            "                rule_filter_copy.reportHashes = reportHashes",
            "            q = get_rs_rule_query(session, rule_filter_copy, sort_mode)",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            for review_status, associated_report_count in q:",
            "                result.append(ReviewStatusRule(",
            "                    reportHash=review_status.bug_hash,",
            "                    reviewData=create_review_data(",
            "                        review_status.status,",
            "                        review_status.message,",
            "                        review_status.author,",
            "                        review_status.date,",
            "                        False),",
            "                    associatedReportCount=associated_report_count))",
            "            return result",
            "",
            "        with DBSession(self._Session) as session:",
            "            if rule_filter and rule_filter.reportHashes:",
            "                # Diffing with a large ammount of report hashes passed in the",
            "                # filter (60K) caused a hanging report diffing.",
            "                # The probable cause of this is the ILIKE matching in the",
            "                # underlying logic. Chunking the request solves this issue.",
            "                for hash_chunk in util.chunks(",
            "                        rule_filter.reportHashes,",
            "                        SQLITE_MAX_VARIABLE_NUMBER):",
            "                    getRules(hash_chunk)",
            "                return result",
            "            else:",
            "                return getRules()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReviewStatusRulesCount(self, rule_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            q = get_rs_rule_query(session, rule_filter)",
            "            return q.count()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeReviewStatusRules(self, rule_filter):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            q = get_rs_rule_query(session, rule_filter)",
            "            for review_status, _ in q:",
            "                session.delete(review_status)",
            "",
            "                # Reports become unreviewed when the corresponding review",
            "                # status rule is removed and the report doesn't have a review",
            "                # status as source code comment.",
            "                session \\",
            "                    .query(Report) \\",
            "                    .filter(Report.bug_id == review_status.bug_hash) \\",
            "                    .filter(Report.review_status_is_in_source.is_(False)) \\",
            "                    .update({",
            "                        'review_status': 'unreviewed',",
            "                        'review_status_author': None,",
            "                        'review_status_message': None,",
            "                        'review_status_date': None,",
            "                        'fixed_at': None})",
            "",
            "            session.commit()",
            "",
            "            LOG.info(\"Review status rules were removed based on filter '%s' by\"",
            "                     \"'%s'.\", rule_filter, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addReviewStatusRule(self, report_hash, review_status, message):",
            "        self.__require_permission([permissions.PRODUCT_ACCESS,",
            "                                   permissions.PRODUCT_STORE])",
            "",
            "        if self.isReviewStatusChangeDisabled():",
            "            msg = \"Review status change is disabled!\"",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "        with DBSession(self._Session) as session:",
            "            self._setReviewStatus(",
            "                session, report_hash, review_status, message)",
            "            session.commit()",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getComments(self, report_id):",
            "        \"\"\"",
            "            Return the list of comments for the given bug.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            if report:",
            "                result = []",
            "",
            "                comments = session.query(Comment) \\",
            "                    .filter(Comment.bug_hash == report.bug_id) \\",
            "                    .order_by(Comment.created_at.desc()) \\",
            "                    .all()",
            "",
            "                for comment in comments:",
            "                    message = get_comment_msg(comment)",
            "                    result.append(CommentData(",
            "                        comment.id,",
            "                        comment.author,",
            "                        message,",
            "                        str(comment.created_at),",
            "                        comment_kind_to_thrift_type(comment.kind)))",
            "",
            "                return result",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f'Report id {report_id} was not found in the database.')",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCommentCount(self, report_id):",
            "        \"\"\"",
            "            Return the number of comments for the given bug.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            commentCount = 0",
            "            if report:",
            "                commentCount = session.query(Comment) \\",
            "                    .filter(Comment.bug_hash == report.bug_id) \\",
            "                    .count()",
            "",
            "            return commentCount",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addComment(self, report_id, comment_data):",
            "        \"\"\" Add new comment for the given bug. \"\"\"",
            "        self.__require_access()",
            "",
            "        if not comment_data.message or not comment_data.message.strip():",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                'The comment message can not be empty!')",
            "",
            "        with DBSession(self._Session) as session:",
            "            report = session.query(Report).get(report_id)",
            "            if report:",
            "                comment = self.__add_comment(report.bug_id,",
            "                                             comment_data.message)",
            "                session.add(comment)",
            "                session.commit()",
            "",
            "                return True",
            "            else:",
            "                msg = f'Report id {report_id} was not found in the database.'",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def updateComment(self, comment_id, content):",
            "        \"\"\"",
            "            Update the given comment message with new content. We allow",
            "            comments to be updated by it's original author only, except for",
            "            Anyonymous comments that can be updated by anybody.",
            "        \"\"\"",
            "        self.__require_access()",
            "",
            "        if not content.strip():",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL,",
            "                'The comment message can not be empty!')",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            user = self._get_username()",
            "",
            "            comment = session.query(Comment).get(comment_id)",
            "            if comment:",
            "                if comment.author not in ('Anonymous', user):",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                        'Unathorized comment modification!')",
            "",
            "                # Create system comment if the message is changed.",
            "                message = comment.message.decode('utf-8')",
            "                if message != content:",
            "                    system_comment_msg = \\",
            "                        f'comment_changed {shlex.quote(message)} ' \\",
            "                        f'{shlex.quote(content)}'",
            "",
            "                    system_comment = \\",
            "                        self.__add_comment(comment.bug_hash,",
            "                                           system_comment_msg,",
            "                                           CommentKindValue.SYSTEM)",
            "                    session.add(system_comment)",
            "",
            "                comment.message = content.encode('utf-8')",
            "                session.add(comment)",
            "",
            "                session.commit()",
            "                return True",
            "            else:",
            "                msg = f'Comment id {comment_id} was not found in the database.'",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeComment(self, comment_id):",
            "        \"\"\"",
            "            Remove the comment. We allow comments to be removed by it's",
            "            original author only, except for Anyonymous comments that can be",
            "            updated by anybody.",
            "        \"\"\"",
            "        self.__require_access()",
            "",
            "        user = self._get_username()",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            comment = session.query(Comment).get(comment_id)",
            "            if comment:",
            "                if comment.author not in ('Anonymous', user):",
            "                    raise codechecker_api_shared.ttypes.RequestFailed(",
            "                        codechecker_api_shared.ttypes.ErrorCode.UNAUTHORIZED,",
            "                        'Unathorized comment modification!')",
            "                session.delete(comment)",
            "                session.commit()",
            "",
            "                LOG.info(\"Comment '%s...' was removed from bug hash '%s' by \"",
            "                         \"'%s'.\", comment.message[:10], comment.bug_hash,",
            "                         self._get_username())",
            "",
            "                return True",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f'Comment id {comment_id} was not found in the database.')",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerDoc(self, _):",
            "        \"\"\"",
            "        Parameters:",
            "         - checkerId",
            "        \"\"\"",
            "        self.__require_view()",
            "        return \"\"",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerLabels(",
            "        self,",
            "        checkers: List[ttypes.Checker]",
            "    ) -> List[List[str]]:",
            "        \"\"\" Return the list of labels to each checker. \"\"\"",
            "        self.__require_view()",
            "",
            "        labels = []",
            "        for checker in checkers:",
            "            analyzer_name = None if not checker.analyzerName \\",
            "                else str(checker.analyzerName)",
            "            analyzer_name = analyzer_name \\",
            "                if (analyzer_name and analyzer_name.lower() != \"unknown\") \\",
            "                else None",
            "",
            "            labels.append(list(map(",
            "                lambda x: f'{x[0]}:{x[1]}',",
            "                self._context.checker_labels.labels_of_checker(",
            "                    checker.checkerId, analyzer_name))))",
            "",
            "        return labels",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getSourceFileData(self, fileId, fileContent, encoding):",
            "        \"\"\"",
            "        Parameters:",
            "         - fileId",
            "         - fileContent",
            "         - enum Encoding",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            sourcefile = session.query(File).get(fileId)",
            "",
            "            if sourcefile is None:",
            "                return SourceFileData()",
            "",
            "            content_hash = sourcefile.content_hash",
            "            cont = session \\",
            "                .query(FileContent.content, FileContent.blame_info) \\",
            "                .filter(FileContent.content_hash == content_hash) \\",
            "                .one_or_none()",
            "",
            "            source_file_data = SourceFileData(",
            "                fileId=sourcefile.id,",
            "                filePath=sourcefile.filepath,",
            "                hasBlameInfo=bool(cont.blame_info),",
            "                remoteUrl=get_commit_url(sourcefile.remote_url,",
            "                                         self._context.git_commit_urls),",
            "                trackingBranch=sourcefile.tracking_branch)",
            "",
            "            if fileContent:",
            "                source = zlib.decompress(cont.content)",
            "",
            "                if encoding == Encoding.BASE64:",
            "                    source = base64.b64encode(source)",
            "",
            "                source_file_data.fileContent = source.decode(",
            "                    'utf-8', errors='ignore')",
            "",
            "            return source_file_data",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getBlameInfo(self, fileId):",
            "        \"\"\" Get blame information for the given file. \"\"\"",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            sourcefile = session.query(File).get(fileId)",
            "",
            "            if sourcefile is None:",
            "                return BlameInfo()",
            "",
            "            cont = session \\",
            "                .query(FileContent.blame_info) \\",
            "                .filter(FileContent.content_hash == sourcefile.content_hash) \\",
            "                .one_or_none()",
            "",
            "            if not cont or not cont.blame_info:",
            "                return BlameInfo()",
            "",
            "            try:",
            "                blame_info = json.loads(",
            "                    zlib.decompress(cont.blame_info).decode(",
            "                        'utf-8', errors='ignore'))",
            "",
            "                commits = {",
            "                    commitHash: Commit(",
            "                        author=CommitAuthor(",
            "                            name=commit[\"author\"][\"name\"],",
            "                            email=commit[\"author\"][\"email\"]),",
            "                        summary=commit[\"summary\"],",
            "                        message=html.escape(commit[\"message\"]),",
            "                        committedDateTime=commit[\"committed_datetime\"],",
            "                    )",
            "                    for commitHash, commit in blame_info[\"commits\"].items()",
            "                }",
            "",
            "                blame_data = [BlameData(",
            "                    startLine=b[\"from\"],",
            "                    endLine=b[\"to\"],",
            "                    commitHash=b[\"commit\"]) for b in blame_info[\"blame\"]]",
            "",
            "                return BlameInfo(",
            "                    commits=commits,",
            "                    blame=blame_data)",
            "            except Exception:",
            "                # pylint: disable=raise-missing-from",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    \"Failed to get blame information for file id: \" + fileId)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getLinesInSourceFileContents(self, lines_in_files_requested, encoding):",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            res = defaultdict(lambda: defaultdict(str))",
            "",
            "            # This will contain all the lines for the given fileId",
            "            contents_to_file_id = defaultdict(list)",
            "            # The goal of the chunking is not for achieving better performace",
            "            # but to be compatible with SQLITE dbms with larger report counts,",
            "            # with larger report data.",
            "            for chunk in util.chunks(",
            "                    lines_in_files_requested, SQLITE_MAX_VARIABLE_NUMBER):",
            "                contents = session.query(FileContent.content, File.id) \\",
            "                        .join(",
            "                            File,",
            "                            FileContent.content_hash == File.content_hash) \\",
            "                        .filter(File.id.in_(",
            "                                [line.fileId if line.fileId is not None",
            "                                    else LOG.warning(",
            "                                        \"File content requested \"",
            "                                        f\"without fileId {line.fileId}\")",
            "                                    for line in chunk])) \\",
            "                        .all()",
            "                for content in contents:",
            "                    lines = zlib.decompress(",
            "                        content.content).decode('utf-8', 'ignore').split('\\n')",
            "                    contents_to_file_id[content.id] = lines",
            "",
            "            for files in lines_in_files_requested:",
            "                for line in files.lines:",
            "                    lines = contents_to_file_id[files.fileId]",
            "                    content = '' if len(lines) < line else lines[line - 1]",
            "                    if encoding == Encoding.BASE64:",
            "                        content = convert.to_b64(content)",
            "                    res[files.fileId][line] = content",
            "",
            "            return res",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                         offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = []",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session \\",
            "                .query(Report.bug_id,",
            "                       Checker.checker_name,",
            "                       Checker.severity) \\",
            "                .join(Checker,",
            "                      Report.checker_id == Checker.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables, [Checker])",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    func.max(extended_table.c.checker_name)",
            "                        .label(\"checker_name\"),",
            "                    func.max(extended_table.c.severity).label(\"severity\"),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(",
            "                    extended_table.c.checker_name,",
            "                    extended_table.c.severity,",
            "                    func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                unique_checker_q = session.query(q.c.checker_name,",
            "                                                 func.max(q.c.severity),",
            "                                                 func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.checker_name) \\",
            "                    .order_by(q.c.checker_name)",
            "            else:",
            "                unique_checker_q = q.group_by(extended_table.c.checker_name,",
            "                                              extended_table.c.severity) \\",
            "                    .order_by(extended_table.c.checker_name)",
            "",
            "            if limit:",
            "                unique_checker_q = unique_checker_q.limit(limit).offset(offset)",
            "",
            "            for name, severity, count in unique_checker_q:",
            "                checker_count = CheckerCount(name=name,",
            "                                             severity=severity,",
            "                                             count=count)",
            "                results.append(checker_count)",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerStatusVerificationDetails(self, run_ids, report_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "            max_run_histories = session.query(",
            "                RunHistory.run_id,",
            "                func.max(RunHistory.id).label('max_run_history_id'),",
            "            ) \\",
            "                .filter(RunHistory.run_id.in_(run_ids) if run_ids else True) \\",
            "                .group_by(RunHistory.run_id)",
            "",
            "            run_id_expression = get_run_id_expression(session, report_filter)",
            "",
            "            subquery = (",
            "                session.query(",
            "                    run_id_expression,",
            "                    Checker.id.label(\"checker_id\"),",
            "                    Checker.checker_name,",
            "                    Checker.analyzer_name,",
            "                    Checker.severity,",
            "                    Report.bug_id,",
            "                    Report.detection_status,",
            "                    Report.review_status,",
            "                )",
            "                .join(RunHistory)",
            "                .join(AnalysisInfo, RunHistory.analysis_info)",
            "                .join(DB_AnalysisInfoChecker, (",
            "                    (AnalysisInfo.id ==",
            "                     DB_AnalysisInfoChecker.analysis_info_id)",
            "                    & (DB_AnalysisInfoChecker.enabled.is_(True))))",
            "                .join(Checker,",
            "                      DB_AnalysisInfoChecker.checker_id == Checker.id)",
            "                .outerjoin(Report, ((Checker.id == Report.checker_id)",
            "                                    & (Run.id == Report.run_id)))",
            "                .filter(RunHistory.id == max_run_histories.subquery()",
            "                        .c.max_run_history_id)",
            "            )",
            "",
            "            if report_filter.isUnique:",
            "                subquery = subquery.group_by(",
            "                    Checker.id,",
            "                    Checker.checker_name,",
            "                    Checker.analyzer_name,",
            "                    Checker.severity,",
            "                    Report.bug_id,",
            "                    Report.detection_status,",
            "                    Report.review_status",
            "                )",
            "",
            "            subquery = subquery.subquery()",
            "",
            "            is_enabled_case = get_is_enabled_case(subquery)",
            "            is_opened_case = get_is_opened_case(subquery)",
            "",
            "            query = (",
            "                session.query(",
            "                    subquery.c.checker_id,",
            "                    subquery.c.checker_name,",
            "                    subquery.c.analyzer_name,",
            "                    subquery.c.severity,",
            "                    subquery.c.run_id,",
            "                    is_enabled_case.label(\"isEnabled\"),",
            "                    is_opened_case.label(\"isOpened\"),",
            "                    func.count(subquery.c.bug_id)",
            "                )",
            "                .group_by(",
            "                    subquery.c.checker_id,",
            "                    subquery.c.checker_name,",
            "                    subquery.c.analyzer_name,",
            "                    subquery.c.severity,",
            "                    subquery.c.run_id,",
            "                    is_enabled_case,",
            "                    is_opened_case",
            "                )",
            "            )",
            "",
            "            checker_stats = {}",
            "            all_run_id = [runId for runId, _ in max_run_histories.all()]",
            "            for checker_id, \\",
            "                checker_name, \\",
            "                analyzer_name, \\",
            "                severity, \\",
            "                run_id_list, \\",
            "                is_enabled, \\",
            "                is_opened, \\",
            "                cnt \\",
            "                    in query.all():",
            "",
            "                checker_stat = checker_stats.get(",
            "                    checker_id,",
            "                    CheckerStatusVerificationDetail(",
            "                        checkerName=checker_name,",
            "                        analyzerName=analyzer_name,",
            "                        enabled=[],",
            "                        disabled=all_run_id.copy(),",
            "                        severity=severity,",
            "                        closed=0,",
            "                        outstanding=0",
            "                    ))",
            "",
            "                if is_enabled:",
            "                    for r in (run_id_list.split(\",\")",
            "                              if isinstance(run_id_list, str)",
            "                              else [run_id_list]):",
            "                        run_id = int(r)",
            "                        if run_id not in checker_stat.enabled:",
            "                            checker_stat.enabled.append(run_id)",
            "                        if run_id in checker_stat.disabled:",
            "                            checker_stat.disabled.remove(run_id)",
            "",
            "                if is_enabled and is_opened:",
            "                    checker_stat.outstanding += cnt",
            "                else:",
            "                    checker_stat.closed += cnt",
            "",
            "                checker_stats[checker_id] = checker_stat",
            "",
            "            return checker_stats",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalyzerNameCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                              offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session \\",
            "                .query(Checker.analyzer_name,",
            "                       Report.bug_id) \\",
            "                .join(Checker,",
            "                      Report.checker_id == Checker.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables, [Checker])",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(func.max(",
            "                    extended_table.c.analyzer_name).label('analyzer_name'),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(extended_table.c.analyzer_name,",
            "                                  func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                analyzer_name_q = session.query(q.c.analyzer_name,",
            "                                                func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.analyzer_name) \\",
            "                    .order_by(q.c.analyzer_name)",
            "            else:",
            "                analyzer_name_q = q.group_by(extended_table.c.analyzer_name) \\",
            "                    .order_by(extended_table.c.analyzer_name)",
            "",
            "            if limit:",
            "                analyzer_name_q = analyzer_name_q.limit(limit).offset(offset)",
            "",
            "            for name, count in analyzer_name_q:",
            "                results[name] = count",
            "",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getSeverityCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session \\",
            "                .query(Report.bug_id,",
            "                       Checker.severity) \\",
            "                .join(Checker,",
            "                      Report.checker_id == Checker.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables, [Checker])",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    func.max(extended_table.c.severity).label(\"severity\"),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(extended_table.c.severity,",
            "                                  func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                severities = session.query(q.c.severity,",
            "                                           func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.severity)",
            "            else:",
            "                severities = q.group_by(extended_table.c.severity)",
            "",
            "            results = dict(severities)",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCheckerMsgCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                            offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.checker_message,",
            "                Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    func.max(extended_table.c.checker_message).label(",
            "                        'checker_message'),",
            "                    extended_table.c.bug_id)",
            "            else:",
            "                q = session.query(extended_table.c.checker_message,",
            "                                  func.count(literal_column('*')))",
            "",
            "            q = q.select_from(extended_table)",
            "",
            "            if report_filter.isUnique:",
            "                q = q.group_by(extended_table.c.bug_id).subquery()",
            "                checker_messages = session.query(q.c.checker_message,",
            "                                                 func.count(q.c.bug_id)) \\",
            "                    .group_by(q.c.checker_message) \\",
            "                    .order_by(q.c.checker_message)",
            "            else:",
            "                checker_messages = q \\",
            "                    .group_by(extended_table.c.checker_message) \\",
            "                    .order_by(extended_table.c.checker_message)",
            "",
            "            if limit:",
            "                checker_messages = checker_messages.limit(limit).offset(offset)",
            "",
            "            results = dict(checker_messages.all())",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReportStatusCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.review_status,",
            "                Report.detection_status,",
            "                Report.bug_id",
            "            )",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id",
            "                )",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            is_outstanding_case = get_is_opened_case(extended_table)",
            "            case_label = \"isOutstanding\"",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    is_outstanding_case.label(case_label),",
            "                    func.count(extended_table.c.bug_id.distinct())) \\",
            "                    .group_by(is_outstanding_case)",
            "            else:",
            "                q = session.query(",
            "                    is_outstanding_case.label(case_label),",
            "                    func.count(extended_table.c.bug_id)) \\",
            "                    .group_by(is_outstanding_case)",
            "",
            "            results = {",
            "                report_status_enum(",
            "                    \"outstanding\" if isOutstanding",
            "                    else \"closed\"",
            "                ): count for isOutstanding, count in q",
            "            }",
            "",
            "            return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getReviewStatusCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.review_status,",
            "                Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    extended_table.c.review_status,",
            "                    func.count(extended_table.c.bug_id.distinct()))",
            "            else:",
            "                q = session.query(",
            "                    extended_table.c.review_status,",
            "                    func.count(extended_table.c.bug_id))",
            "",
            "            q = q \\",
            "                .select_from(extended_table) \\",
            "                .group_by(extended_table.c.review_status)",
            "",
            "        return {review_status_enum(rev_status): count",
            "                for rev_status, count in q}",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getFileCounts(self, run_ids, report_filter, cmp_data, limit, offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.file_id,",
            "                Report.bug_id,",
            "                Report.id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            count_col = extended_table.c.bug_id.distinct() if \\",
            "                report_filter.isUnique else extended_table.c.bug_id",
            "",
            "            stmt = session.query(",
            "                    File.filepath,",
            "                    func.count(count_col).label('report_num')) \\",
            "                .join(",
            "                    extended_table, File.id == extended_table.c.file_id) \\",
            "                .group_by(File.filepath) \\",
            "                .order_by(desc('report_num'))",
            "",
            "            if limit:",
            "                stmt = stmt.limit(limit).offset(offset)",
            "",
            "            for fp, count in stmt:",
            "                results[fp] = count",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getRunHistoryTagCounts(self, run_ids, report_filter, cmp_data, limit,",
            "                               offset):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        limit = verify_limit_range(limit)",
            "",
            "        results = []",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            tag_run_ids = session.query(RunHistory.run_id.distinct())",
            "",
            "            if run_ids:",
            "                tag_run_ids = tag_run_ids.filter(",
            "                    RunHistory.run_id.in_(run_ids))",
            "",
            "            tag_run_ids = tag_run_ids.subquery()",
            "",
            "            report_cnt_q = session.query(Report.run_id,",
            "                                         Report.bug_id,",
            "                                         Report.detected_at,",
            "                                         Report.fixed_at)",
            "",
            "            if report_filter.annotations is not None:",
            "                report_cnt_q = report_cnt_q.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                report_cnt_q = report_cnt_q.group_by(Report.id)",
            "",
            "            report_cnt_q = apply_report_filter(",
            "                report_cnt_q, filter_expression, join_tables)",
            "            report_cnt_q = report_cnt_q.filter(",
            "                Report.run_id.in_(tag_run_ids)).subquery()",
            "",
            "            count_expr = func.count(",
            "                report_cnt_q.c.bug_id if not report_filter.isUnique",
            "                else report_cnt_q.c.bug_id.distinct())",
            "",
            "            count_q = session.query(RunHistory.id.label('run_history_id'),",
            "                                    count_expr.label('report_count')) \\",
            "                .outerjoin(report_cnt_q,",
            "                           report_cnt_q.c.run_id == RunHistory.run_id) \\",
            "                .filter(get_open_reports_date_filter_query(report_cnt_q.c)) \\",
            "                .group_by(RunHistory.id) \\",
            "                .subquery()",
            "",
            "            tag_q = session.query(RunHistory.run_id.label('run_id'),",
            "                                  RunHistory.id.label('run_history_id'))",
            "",
            "            if run_ids:",
            "                tag_q = tag_q.filter(RunHistory.run_id.in_(run_ids))",
            "",
            "            if report_filter and report_filter.runTag is not None:",
            "                tag_q = tag_q.filter(RunHistory.id.in_(report_filter.runTag))",
            "",
            "            tag_q = tag_q.subquery()",
            "",
            "            q = session.query(tag_q.c.run_history_id,",
            "                              func.max(Run.id),",
            "                              func.max(Run.name).label('run_name'),",
            "                              func.max(RunHistory.id),",
            "                              func.max(RunHistory.time),",
            "                              func.max(RunHistory.version_tag),",
            "                              func.max(count_q.c.report_count)) \\",
            "                .outerjoin(RunHistory,",
            "                           RunHistory.id == tag_q.c.run_history_id) \\",
            "                .outerjoin(Run, Run.id == tag_q.c.run_id) \\",
            "                .outerjoin(count_q,",
            "                           count_q.c.run_history_id == RunHistory.id) \\",
            "                .group_by(tag_q.c.run_history_id, RunHistory.time) \\",
            "                .order_by(RunHistory.time.desc())",
            "",
            "            if limit:",
            "                q = q.limit(limit).offset(offset)",
            "",
            "            for _, run_id, run_name, tag_id, version_time, tag, count in q:",
            "                results.append(RunTagCount(id=tag_id,",
            "                                           time=str(version_time),",
            "                                           name=tag,",
            "                                           runName=run_name,",
            "                                           runId=run_id,",
            "                                           count=count if count else 0))",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getDetectionStatusCounts(self, run_ids, report_filter, cmp_data):",
            "        \"\"\"",
            "          If the run id list is empty the metrics will be counted",
            "          for all of the runs and in compare mode all of the runs",
            "          will be used as a baseline excluding the runs in compare data.",
            "        \"\"\"",
            "        self.__require_view()",
            "        results = {}",
            "        with DBSession(self._Session) as session:",
            "            filter_expression, join_tables = process_report_filter(",
            "                session, run_ids, report_filter, cmp_data)",
            "",
            "            extended_table = session.query(",
            "                Report.detection_status,",
            "                Report.bug_id)",
            "",
            "            if report_filter.annotations is not None:",
            "                extended_table = extended_table.outerjoin(",
            "                    ReportAnnotations,",
            "                    ReportAnnotations.report_id == Report.id)",
            "                extended_table = extended_table.group_by(Report.id)",
            "",
            "            extended_table = apply_report_filter(",
            "                extended_table, filter_expression, join_tables)",
            "",
            "            extended_table = extended_table.subquery()",
            "",
            "            if report_filter.isUnique:",
            "                q = session.query(",
            "                    extended_table.c.detection_status,",
            "                    func.count(extended_table.c.bug_id.distinct()))",
            "            else:",
            "                q = session.query(",
            "                    extended_table.c.detection_status,",
            "                    func.count(literal_column('*')))",
            "",
            "            q = q \\",
            "                .select_from(extended_table) \\",
            "                .group_by(extended_table.c.detection_status)",
            "",
            "            results = {",
            "                detection_status_enum(k): v for k,",
            "                v in q}",
            "",
            "        return results",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getFailedFilesCount(self, run_ids):",
            "        \"\"\"",
            "        Count the number of uniqued failed files in the latest storage of each",
            "        given run. If the run id list is empty the number of failed files will",
            "        be counted for all of the runs.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        # Unfortunately we can't distinct the failed file paths by using SQL",
            "        # queries because the list of failed files for a run / analyzer are",
            "        # stored in one column in a compressed way. For this reason we need to",
            "        # decompress the value in the Python code before uniqueing.",
            "        return len(self.getFailedFiles(run_ids).keys())",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getFailedFiles(self, run_ids):",
            "        \"\"\"",
            "        Get files which failed to analyze in the latest storage of the given",
            "        runs. For each files it will return a list where each element contains",
            "        information in which run the failure happened.",
            "        \"\"\"",
            "        self.__require_view()",
            "",
            "        res = defaultdict(list)",
            "        with DBSession(self._Session) as session:",
            "            query, sub_q = get_failed_files_query(",
            "                session, run_ids, [AnalyzerStatistic.failed_files, Run.name],",
            "                [RunHistory.run_id])",
            "",
            "            query = query \\",
            "                .outerjoin(Run, Run.id == sub_q.c.run_id) \\",
            "                .filter(AnalyzerStatistic.failed_files.isnot(None))",
            "",
            "            for failed_files, run_name in query.all():",
            "                failed_files = zlib.decompress(failed_files).decode('utf-8')",
            "",
            "                for failed_file in failed_files.split('\\n'):",
            "                    already_exists = \\",
            "                        any(i.runName == run_name for i in res[failed_file])",
            "",
            "                    if not already_exists:",
            "                        res[failed_file].append(",
            "                            ttypes.AnalysisFailureInfo(runName=run_name))",
            "",
            "        return res",
            "",
            "    # -----------------------------------------------------------------------",
            "    @timeit",
            "    def getPackageVersion(self):",
            "        self.__require_view()",
            "",
            "        return self.__package_version",
            "",
            "    # -----------------------------------------------------------------------",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeRunResults(self, run_ids):",
            "        self.__require_store()",
            "",
            "        failed = False",
            "        for run_id in run_ids:",
            "            try:",
            "                self.removeRun(run_id, None)",
            "            except Exception as ex:",
            "                LOG.error(\"Failed to remove run: %s\", run_id)",
            "                LOG.error(ex)",
            "                failed = True",
            "        return not failed",
            "",
            "    def _removeReports(self, session, report_ids,",
            "                       chunk_size=SQLITE_MAX_VARIABLE_NUMBER):",
            "        \"\"\"",
            "        Removing reports in chunks.",
            "        \"\"\"",
            "        for r_ids in util.chunks(iter(report_ids), chunk_size):",
            "            session.query(Report) \\",
            "                .filter(Report.id.in_(r_ids)) \\",
            "                .delete(synchronize_session=False)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeRunReports(self, run_ids, report_filter, cmp_data):",
            "        self.__require_store()",
            "",
            "        if not run_ids:",
            "            run_ids = []",
            "",
            "        if cmp_data and cmp_data.runIds:",
            "            run_ids.extend(cmp_data.runIds)",
            "",
            "        with DBSession(self._Session) as session:",
            "            check_remove_runs_lock(session, run_ids)",
            "",
            "            try:",
            "                filter_expression, join_tables = process_report_filter(",
            "                    session, run_ids, report_filter, cmp_data)",
            "",
            "                q = session.query(Report.id)",
            "",
            "                if report_filter.annotations is not None:",
            "                    q = q.outerjoin(ReportAnnotations,",
            "                                    ReportAnnotations.report_id == Report.id)",
            "                    q = q.group_by(Report.id)",
            "",
            "                q = apply_report_filter(q, filter_expression, join_tables)",
            "",
            "                reports_to_delete = [r[0] for r in q]",
            "                if reports_to_delete:",
            "                    self._removeReports(session, reports_to_delete)",
            "",
            "                session.commit()",
            "                session.close()",
            "",
            "                LOG.info(\"The following reports were removed by '%s': %s\",",
            "                         self._get_username(), reports_to_delete)",
            "            except Exception as ex:",
            "                session.rollback()",
            "                LOG.error(\"Database cleanup failed.\")",
            "                LOG.error(ex)",
            "                return False",
            "",
            "        # Remove unused comments and unused analysis info from the database.",
            "        # Originally db_cleanup.remove_unused_data() was used here which",
            "        # removes unused file entries too. However, removing files at the same",
            "        # time with a concurrently ongoing storage may result in a foreign key",
            "        # constraint error. An alternative solution can be adding the last",
            "        # access timestamp to file entries to delay their removal (and avoid",
            "        # removing frequently accessed files). The same comment applies to",
            "        # removeRun() function.",
            "        db_cleanup.remove_unused_comments(self._product)",
            "        db_cleanup.remove_unused_analysis_info(self._product)",
            "",
            "        return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeRun(self, run_id, run_filter):",
            "        self.__require_store()",
            "",
            "        # Remove the whole run.",
            "        with DBSession(self._Session) as session:",
            "            check_remove_runs_lock(session, [run_id])",
            "",
            "            if not run_filter:",
            "                run_filter = RunFilter(ids=[run_id])",
            "",
            "            q = process_run_filter(session, session.query(Run), run_filter)",
            "",
            "            # q.delete(synchronize_session=False) could also be used here,",
            "            # however, a run deletion tends to be a slow operation due to",
            "            # cascades and such. Deleting runs in separate transactions don't",
            "            # exceed a potential statement timeout threshold in a DBMS.",
            "            runs = []",
            "            deleted_run_cnt = 0",
            "",
            "            for run in q.all():",
            "                try:",
            "                    runs.append(run.name)",
            "                    session.delete(run)",
            "                    session.commit()",
            "                    deleted_run_cnt += 1",
            "                except Exception as e:",
            "                    # TODO: Display alert on the GUI if there's an exception",
            "                    # TODO: Catch SQLAlchemyError instead of generic",
            "                    #  exception once it is confirmed that the exception is",
            "                    #  due to a large run deletion timeout based on server",
            "                    #  log warnings",
            "                    # This exception is handled silently because it is",
            "                    # expected to never occur, but there have been some rare",
            "                    # cases where it occurred due to underlying reasons.",
            "                    # Handling it silently ensures that the Number of runs",
            "                    # counter is not affected by the exception.",
            "                    LOG.warning(f\"Suppressed an exception while \"",
            "                                f\"deleting run {run.name}. Error: {e}\")",
            "",
            "            session.close()",
            "",
            "            LOG.info(\"Runs '%s' were removed by '%s'.\", \"', '\".join(runs),",
            "                     self._get_username())",
            "",
            "        # Decrement the number of runs but do not update the latest storage",
            "        # date.",
            "        self._set_run_data_for_curr_product(-1 * deleted_run_cnt)",
            "",
            "        # Remove unused comments and unused analysis info from the database.",
            "        # Originally db_cleanup.remove_unused_data() was used here which",
            "        # removes unused file entries tool. However removing files at the same",
            "        # time with a storage concurrently results foreign key constraint",
            "        # error. An alternative solution can be adding a timestamp to file",
            "        # entries to delay their removal. The same comment applies to",
            "        # removeRunReports() function.",
            "        db_cleanup.remove_unused_comments(self._product)",
            "        db_cleanup.remove_unused_analysis_info(self._product)",
            "",
            "        return bool(runs)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def updateRunData(self, run_id, new_run_name):",
            "        self.__require_store()",
            "",
            "        if not new_run_name:",
            "            msg = 'No new run name was given to update the run.'",
            "            LOG.error(msg)",
            "            raise codechecker_api_shared.ttypes.RequestFailed(",
            "                codechecker_api_shared.ttypes.ErrorCode.GENERAL, msg)",
            "",
            "        with DBSession(self._Session) as session:",
            "            check_new_run_name = session.query(Run) \\",
            "                .filter(Run.name == new_run_name) \\",
            "                .all()",
            "            if check_new_run_name:",
            "                msg = \"New run name '\" + new_run_name + \"' already exists.\"",
            "                LOG.error(msg)",
            "",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "            run_data = session.query(Run).get(run_id)",
            "            if run_data:",
            "                old_run_name = run_data.name",
            "                run_data.name = new_run_name",
            "                session.add(run_data)",
            "                session.commit()",
            "",
            "                LOG.info(\"Run name '%s' (%d) was changed to %s by '%s'.\",",
            "                         old_run_name, run_id, new_run_name,",
            "                         self._get_username())",
            "",
            "                return True",
            "            else:",
            "                msg = f'Run id {run_id} was not found in the database.'",
            "                LOG.error(msg)",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE, msg)",
            "",
            "        return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    def getSuppressFile(self):",
            "        \"\"\"",
            "        DEPRECATED the server is not started with a suppress file anymore.",
            "        Returning empty string.",
            "        \"\"\"",
            "        self.__require_access()",
            "        return ''",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addSourceComponent(self, name, value, description):",
            "        \"\"\"",
            "        Adds a new source if it does not exist or updates an old one.",
            "        \"\"\"",
            "        self.__require_admin()",
            "        with DBSession(self._Session) as session:",
            "            component = session.query(SourceComponent).get(name)",
            "            user = self._auth_session.user if self._auth_session else None",
            "",
            "            if component:",
            "                component.value = value.encode('utf-8')",
            "                component.description = description",
            "                component.user = user",
            "            else:",
            "                component = SourceComponent(name,",
            "                                            value.encode('utf-8'),",
            "                                            description,",
            "                                            user)",
            "",
            "            session.add(component)",
            "            session.commit()",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getSourceComponents(self, component_filter):",
            "        \"\"\"",
            "        Returns the available source components.",
            "        \"\"\"",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            q = session.query(SourceComponent)",
            "",
            "            if component_filter:",
            "                sql_component_filter = [SourceComponent.name.ilike(conv(cf))",
            "                                        for cf in component_filter]",
            "                q = q.filter(*sql_component_filter)",
            "",
            "            q = q.order_by(SourceComponent.name)",
            "",
            "            components = [SourceComponentData(c.name, c.value.decode('utf-8'),",
            "                                              c.description) for c in q]",
            "",
            "            # If no filter is set or the auto generated component name can",
            "            # be found in the filter list we will return with this",
            "            # component too.",
            "            if not component_filter or \\",
            "                    GEN_OTHER_COMPONENT_NAME in component_filter:",
            "                component_other = \\",
            "                    SourceComponentData(GEN_OTHER_COMPONENT_NAME, None,",
            "                                        \"Special auto-generated source \"",
            "                                        \"component which contains files that \"",
            "                                        \"are uncovered by the rest of the \"",
            "                                        \"components.\")",
            "",
            "                components.append(component_other)",
            "",
            "            return components",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeSourceComponent(self, name):",
            "        \"\"\"",
            "        Removes a source component.",
            "        \"\"\"",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            component = session.query(SourceComponent).get(name)",
            "            if component:",
            "                session.delete(component)",
            "                session.commit()",
            "                LOG.info(\"Source component '%s' has been removed by '%s'\",",
            "                         name, self._get_username())",
            "                return True",
            "            else:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f'Source component {name} was not found in the database.')",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getMissingContentHashes(self, file_hashes):",
            "        self.__require_store()",
            "",
            "        if not file_hashes:",
            "            return []",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            q = session.query(FileContent) \\",
            "                .options(sqlalchemy.orm.load_only('content_hash')) \\",
            "                .filter(FileContent.content_hash.in_(file_hashes))",
            "",
            "            return list(set(file_hashes) -",
            "                        set(fc.content_hash for fc in q))",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getMissingContentHashesForBlameInfo(self, file_hashes):",
            "        self.__require_store()",
            "",
            "        if not file_hashes:",
            "            return []",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            q = session.query(FileContent) \\",
            "                .options(sqlalchemy.orm.load_only('content_hash')) \\",
            "                .filter(FileContent.content_hash.in_(file_hashes)) \\",
            "                .filter(FileContent.blame_info.isnot(None))",
            "",
            "            return list(set(file_hashes) -",
            "                        set(fc.content_hash for fc in q))",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def massStoreRun(self, name, tag, version, b64zip, force,",
            "                     trim_path_prefixes, description):",
            "        self.__require_store()",
            "",
            "        from codechecker_server.api.mass_store_run import MassStoreRun",
            "        m = MassStoreRun(self, name, tag, version, b64zip, force,",
            "                         trim_path_prefixes, description)",
            "        return m.store()",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def allowsStoringAnalysisStatistics(self):",
            "        self.__require_store()",
            "",
            "        return bool(self._manager.get_analysis_statistics_dir())",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalysisStatisticsLimits(self):",
            "        self.__require_store()",
            "",
            "        cfg = {}",
            "",
            "        # Get the limit of failure zip size.",
            "        failure_zip_size = self._manager.get_failure_zip_size()",
            "        if failure_zip_size:",
            "            cfg[ttypes.StoreLimitKind.FAILURE_ZIP_SIZE] = failure_zip_size",
            "",
            "        # Get the limit of compilation database size.",
            "        compilation_database_size = \\",
            "            self._manager.get_compilation_database_size()",
            "        if compilation_database_size:",
            "            cfg[ttypes.StoreLimitKind.COMPILATION_DATABASE_SIZE] = \\",
            "                compilation_database_size",
            "",
            "        return cfg",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def storeAnalysisStatistics(self, run_name, b64zip):",
            "        self.__require_store()",
            "",
            "        report_dir_store = self._manager.get_analysis_statistics_dir()",
            "        if report_dir_store:",
            "            try:",
            "                product_dir = os.path.join(report_dir_store,",
            "                                           self._product.endpoint)",
            "                # Create report store directory.",
            "                if not os.path.exists(product_dir):",
            "                    os.makedirs(product_dir, mode=stat.S_IRWXU | stat.S_IRGRP)",
            "",
            "                # Removes and replaces special characters in the run name.",
            "                run_name = slugify(run_name)",
            "                run_zip_file = os.path.join(product_dir, run_name + '.zip')",
            "                with open(run_zip_file, 'wb') as run_zip:",
            "                    run_zip.write(zlib.decompress(",
            "                        base64.b64decode(b64zip.encode('utf-8'))))",
            "",
            "                # Change permission, so only current user and group have access",
            "                # to this file.",
            "                os.chmod(",
            "                    run_zip_file, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP)",
            "",
            "                return True",
            "            except Exception as ex:",
            "                LOG.error(str(ex))",
            "                return False",
            "",
            "        return False",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getAnalysisStatistics(self, run_id, run_history_id):",
            "        self.__require_view()",
            "",
            "        analyzer_statistics = {}",
            "",
            "        with DBSession(self._Session) as session:",
            "            run_ids = None if run_id is None else [run_id]",
            "            run_history_ids = None if run_history_id is None \\",
            "                else [run_history_id]",
            "",
            "            query = get_analysis_statistics_query(",
            "                session, run_ids, run_history_ids)",
            "",
            "            for anal_stat, _ in query:",
            "                failed_files = zlib.decompress(anal_stat.failed_files).decode(",
            "                    'utf-8').split('\\n') if anal_stat.failed_files else []",
            "                analyzer_version = zlib.decompress(",
            "                    anal_stat.version).decode('utf-8') \\",
            "                    if anal_stat.version else None",
            "",
            "                analyzer_statistics[anal_stat.analyzer_type] = \\",
            "                    ttypes.AnalyzerStatistics(version=analyzer_version,",
            "                                              failed=anal_stat.failed,",
            "                                              failedFilePaths=failed_files,",
            "                                              successful=anal_stat.successful)",
            "        return analyzer_statistics",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def exportData(self, run_filter):",
            "        self.__require_view()",
            "",
            "        with DBSession(self._Session) as session:",
            "",
            "            # Logic for getting comments",
            "            comment_data_list = defaultdict(list)",
            "            comment_query = session.query(Comment, Report.bug_id) \\",
            "                .outerjoin(Report, Report.bug_id == Comment.bug_hash) \\",
            "                .order_by(Comment.created_at.desc(), Comment.id.desc())",
            "",
            "            if run_filter:",
            "                comment_query = process_run_filter(session, comment_query,",
            "                                                   run_filter) \\",
            "                    .outerjoin(Run, Report.run_id == Run.id)",
            "",
            "            for data, report_id in comment_query:",
            "                comment_data = ttypes.CommentData(",
            "                    id=data.id,",
            "                    author=data.author,",
            "                    message=html.unescape(data.message.decode('utf-8')),",
            "                    createdAt=str(data.created_at),",
            "                    kind=data.kind)",
            "                comment_data_list[report_id].append(comment_data)",
            "",
            "            # Logic for getting review status",
            "            review_data_list = {}",
            "            review_query = session.query(Report) \\",
            "                .filter(Report.review_status != \"unreviewed\") \\",
            "                .order_by(Report.review_status_date)",
            "",
            "            if run_filter:",
            "                review_query = process_run_filter(session, review_query,",
            "                                                  run_filter) \\",
            "                    .outerjoin(Run, Report.run_id == Run.id)",
            "",
            "            for report in review_query:",
            "                review_data = create_review_data(",
            "                    report.review_status,",
            "                    report.review_status_message,",
            "                    report.review_status_author,",
            "                    report.review_status_date,",
            "                    report.review_status_is_in_source)",
            "                review_data_list[report.bug_id] = review_data",
            "",
            "        return ExportData(comments=comment_data_list,",
            "                          reviewData=review_data_list)",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def importData(self, exportData):",
            "        self.__require_admin()",
            "        with DBSession(self._Session) as session:",
            "",
            "            # Logic for importing comments",
            "            comment_bug_ids = list(exportData.comments.keys())",
            "            comment_query = session.query(Comment) \\",
            "                .filter(Comment.bug_hash.in_(comment_bug_ids)) \\",
            "                .order_by(Comment.created_at.desc())",
            "            comments_in_db = defaultdict(list)",
            "            for comment in comment_query:",
            "                comments_in_db[comment.bug_hash].append(comment)",
            "            for bug_hash, comments in exportData.comments.items():",
            "                db_comments = comments_in_db[bug_hash]",
            "                for comment in comments:",
            "                    date = datetime.strptime(comment.createdAt,",
            "                                             '%Y-%m-%d %H:%M:%S.%f')",
            "                    message = comment.message.encode('utf-8') \\",
            "                        if comment.message else b''",
            "                    # See if the comment is already in the database.",
            "                    if any(c.created_at == date and",
            "                           c.kind == comment.kind and",
            "                           c.message == message for c in db_comments):",
            "                        continue",
            "                    c = Comment(bug_hash, comment.author, message,",
            "                                comment.kind, date)",
            "                    session.add(c)",
            "",
            "            # Logic for importing review status",
            "            review_bug_ids = list(exportData.reviewData.keys())",
            "            review_query = session.query(ReviewStatus) \\",
            "                .filter(ReviewStatus.bug_hash.in_(review_bug_ids)) \\",
            "                .order_by(ReviewStatus.date.desc())",
            "            db_review_data = {}",
            "            for review_status in review_query:",
            "                db_review_data[review_status.bug_hash] = review_status",
            "            for bug_hash, imported_review in exportData.reviewData.items():",
            "                db_status = db_review_data.get(bug_hash)",
            "                # The status is up-to-date.",
            "                if db_status and str(db_status.date) == imported_review.date:",
            "                    continue",
            "                date = datetime.strptime(imported_review.date,",
            "                                         '%Y-%m-%d %H:%M:%S.%f')",
            "                self._setReviewStatus(session,",
            "                                      bug_hash,",
            "                                      imported_review.status,",
            "                                      imported_review.comment,",
            "                                      date)",
            "",
            "            session.commit()",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def addCleanupPlan(self, name, description, dueDate):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = session.query(CleanupPlan) \\",
            "                .filter(CleanupPlan.name == name) \\",
            "                .one_or_none()",
            "",
            "            if cleanup_plan:",
            "                raise codechecker_api_shared.ttypes.RequestFailed(",
            "                    codechecker_api_shared.ttypes.ErrorCode.DATABASE,",
            "                    f\"Cleanup plan '{name}' already exists.\")",
            "",
            "            cleanup_plan = CleanupPlan(name)",
            "            cleanup_plan.description = description",
            "            cleanup_plan.due_date = \\",
            "                datetime.fromtimestamp(dueDate) if dueDate else None",
            "",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"New cleanup plan '%s' has been created by '%s'\",",
            "                     name, self._get_username())",
            "",
            "            return cleanup_plan.id",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def updateCleanupPlan(self, cleanup_plan_id, name, description, dueDate):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "            cleanup_plan.name = name",
            "            cleanup_plan.description = description",
            "            cleanup_plan.due_date = \\",
            "                datetime.fromtimestamp(dueDate) if dueDate else None",
            "",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"Cleanup plan '%d' has been updated by '%s'\",",
            "                     cleanup_plan_id, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def getCleanupPlans(self, cleanup_plan_filter):",
            "        self.__require_view()",
            "        with DBSession(self._Session) as session:",
            "            q = session \\",
            "                .query(CleanupPlan) \\",
            "                .order_by(CleanupPlan.name)",
            "",
            "            if cleanup_plan_filter:",
            "                if cleanup_plan_filter.ids:",
            "                    q = q.filter(CleanupPlan.id.in_(",
            "                        cleanup_plan_filter.ids))",
            "",
            "                if cleanup_plan_filter.names:",
            "                    q = q.filter(CleanupPlan.name.in_(",
            "                        cleanup_plan_filter.names))",
            "",
            "                if cleanup_plan_filter.isOpen is not None:",
            "                    if cleanup_plan_filter.isOpen:",
            "                        q = q.filter(CleanupPlan.closed_at.is_(None))",
            "                    else:",
            "                        q = q.filter(CleanupPlan.closed_at.isnot(None))",
            "",
            "            cleanup_plans = q.all()",
            "",
            "            cleanup_plan_hashes = get_cleanup_plan_report_hashes(",
            "                session, [c.id for c in cleanup_plans])",
            "",
            "            return [ttypes.CleanupPlan(",
            "                id=cp.id,",
            "                name=cp.name,",
            "                description=cp.description,",
            "                dueDate=int(time.mktime(",
            "                    cp.due_date.timetuple())) if cp.due_date else None,",
            "                closedAt=int(time.mktime(",
            "                    cp.closed_at.timetuple())) if cp.closed_at else None,",
            "                reportHashes=cleanup_plan_hashes[cp.id]) for cp in q]",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def removeCleanupPlan(self, cleanup_plan_id):",
            "        self.__require_admin()",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "            name = cleanup_plan.name",
            "",
            "            session.delete(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"Cleanup plan '%s' has been removed by '%s'\",",
            "                     name, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def closeCleanupPlan(self, cleanup_plan_id):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            cleanup_plan.closed_at = datetime.now()",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "",
            "            LOG.info(\"Cleanup plan '%s' has been closed by '%s'\",",
            "                     cleanup_plan.name, self._get_username())",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def reopenCleanupPlan(self, cleanup_plan_id):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            cleanup_plan.closed_at = None",
            "            session.add(cleanup_plan)",
            "            session.commit()",
            "            LOG.info(\"Cleanup plan '%s' has been reopened by '%s'\",",
            "                     cleanup_plan.name, self._get_username())",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def setCleanupPlan(self, cleanup_plan_id, reportHashes):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            q = session \\",
            "                .query(CleanupPlanReportHash.bug_hash) \\",
            "                .filter(",
            "                    CleanupPlanReportHash.cleanup_plan_id == cleanup_plan.id) \\",
            "                .filter(CleanupPlanReportHash.bug_hash.in_(reportHashes))",
            "            new_report_hashes = set(reportHashes) - set(b[0] for b in q)",
            "",
            "            for report_hash in new_report_hashes:",
            "                session.add(CleanupPlanReportHash(",
            "                    cleanup_plan_id=cleanup_plan.id, bug_hash=report_hash))",
            "",
            "            session.commit()",
            "",
            "            return True",
            "",
            "    @exc_to_thrift_reqfail",
            "    @timeit",
            "    def unsetCleanupPlan(self, cleanup_plan_id, reportHashes):",
            "        self.__require_admin()",
            "",
            "        with DBSession(self._Session) as session:",
            "            cleanup_plan = get_cleanup_plan(session, cleanup_plan_id)",
            "",
            "            session \\",
            "                .query(CleanupPlanReportHash) \\",
            "                .filter(",
            "                    CleanupPlanReportHash.cleanup_plan_id == cleanup_plan.id) \\",
            "                .filter(CleanupPlanReportHash.bug_hash.in_(reportHashes)) \\",
            "                .delete(synchronize_session=False)",
            "",
            "            session.commit()",
            "            session.close()",
            "",
            "            return True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "2749": [
                "ThriftRequestHandler",
                "getCheckerDoc"
            ]
        },
        "addLocation": []
    },
    "web/server/codechecker_server/routing.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "     version = version.lstrip('v')"
            },
            "2": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "     version_parts = version.split('.')"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+    if len(version_parts) < 2:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+        return False"
            },
            "5": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 76,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     # We don't care if accidentally the version tag contains a revision number."
            },
            "7": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "     major, minor = int(version_parts[0]), int(version_parts[1])"
            },
            "8": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "     \"\"\""
            },
            "9": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 118,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "     # A standard POST request from an API client looks like:"
            },
            "11": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # http://localhost:8001/[product-name]/<API version>/<API service>"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+    # http://localhost:8001/[product-name/]<API version>/<API service>"
            },
            "13": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "     # where specifying the product name is optional."
            },
            "14": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 122,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 123,
                "PatchRowcode": "     split_path = urlparse(path).path.split('/', 3)"
            },
            "16": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 124,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "     endpoint_part = split_path[1]"
            },
            "18": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if is_valid_product_endpoint(split_path[1]):"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+    if is_valid_product_endpoint(split_path[1]) and len(split_path) == 4:"
            },
            "20": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 127,
                "PatchRowcode": "         version_tag = split_path[2].lstrip('v')"
            },
            "21": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        remainder = split_path[3]"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        if not is_supported_version(version_tag):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+            return None, None, None"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+        endpoint = split_path[3]"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+        return endpoint_part, version_tag, endpoint"
            },
            "26": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 132,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return endpoint_part, version_tag, remainder"
            },
            "28": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    elif split_path[1].startswith('v'):"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+    elif split_path[1].startswith('v') and len(split_path) == 3:"
            },
            "30": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "         # Request came through without a valid product URL endpoint to"
            },
            "31": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "         # possibly the main server."
            },
            "32": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "         version_tag = split_path[1].lstrip('v')"
            },
            "33": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        remainder = split_path[2]"
            },
            "34": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "35": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return None, version_tag, remainder"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+        if not is_supported_version(version_tag):"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+            return None, None, None"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+        endpoint = split_path[2]"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+        return None, version_tag, endpoint"
            },
            "40": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 141,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "     return None, None, None"
            },
            "42": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 143,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Defines the routing rules for the CodeChecker server.",
            "\"\"\"",
            "",
            "",
            "import re",
            "from urllib.parse import urlparse",
            "",
            "from codechecker_web.shared.version import SUPPORTED_VERSIONS",
            "",
            "# A list of top-level path elements under the webserver root",
            "# which should not be considered as a product route.",
            "NON_PRODUCT_ENDPOINTS = ['index.html',",
            "                         'images',",
            "                         'docs',",
            "                         'live',",
            "                         'ready']",
            "",
            "# A list of top-level path elements in requests (such as Thrift endpoints)",
            "# which should not be considered as a product route.",
            "NON_PRODUCT_ENDPOINTS += ['Authentication',",
            "                          'Products',",
            "                          'CodeCheckerService']",
            "",
            "",
            "# A list of top-level path elements under the webserver root which should",
            "# be protected by authentication requirement when accessing the server.",
            "PROTECTED_ENTRY_POINTS = ['',  # Empty string in a request is 'index.html'.",
            "                          'index.html']",
            "",
            "",
            "def is_valid_product_endpoint(uripart):",
            "    \"\"\"",
            "    Returns whether or not the given URI part is to be considered a valid",
            "    product name.",
            "    \"\"\"",
            "    # FIXME: Endpoint \"all\" should be disallowed, as commit",
            "    # fd59927013d5482ff10e80994511971770753d0c in Dec 2017 added the ability",
            "    # for \"CodeChecker server\" to specify \"--db-status all\" and",
            "    # \"--db-upgrade-schema all\" for the case where *every* product needs to",
            "    # be checked/upgraded, essentially blocking the ability to status-check",
            "    # or schema migrate the product at the endpoint literal \"all\".",
            "",
            "    # There are some forbidden keywords.",
            "    if uripart in NON_PRODUCT_ENDPOINTS:",
            "        return False",
            "",
            "    pattern = r'^[A-Za-z0-9_\\-]+$'",
            "    if not re.match(pattern, uripart):",
            "        return False",
            "",
            "    return True",
            "",
            "",
            "def is_supported_version(version):",
            "    \"\"\"",
            "    Returns whether or not the given version tag is supported by the current",
            "    build. A version is supported if its MAJOR version is supported, and if",
            "    its MINOR version is at most the highest minor version accepted by the",
            "    server.",
            "",
            "    If supported, returns the major and minor version as a tuple.",
            "    \"\"\"",
            "",
            "    version = version.lstrip('v')",
            "    version_parts = version.split('.')",
            "",
            "    # We don't care if accidentally the version tag contains a revision number.",
            "    major, minor = int(version_parts[0]), int(version_parts[1])",
            "    if major in SUPPORTED_VERSIONS and minor <= SUPPORTED_VERSIONS[major]:",
            "        return major, minor",
            "",
            "    return False",
            "",
            "",
            "# pylint: disable=invalid-name",
            "def split_client_GET_request(path):",
            "    \"\"\"",
            "    Split the given request URI to its parts relevant to the server.",
            "",
            "    Returns the product endpoint and the \"remainder\" of the request path",
            "    as a tuple of 2.",
            "    \"\"\"",
            "",
            "    # A standard GET request from a browser looks like:",
            "    # http://localhost:8001/[product-name]/#{request-parts}",
            "    # where the parts are, e.g.: run=[run_id]&report=[report_id]",
            "",
            "    parsed_path = urlparse(path).path",
            "    split_path = parsed_path.split('/', 2)",
            "",
            "    endpoint_part = split_path[1] if len(split_path) > 1 else None",
            "    if endpoint_part and is_valid_product_endpoint(endpoint_part):",
            "        remainder = split_path[2] if len(split_path) == 3 else ''",
            "        return endpoint_part, remainder",
            "    else:",
            "        # The request wasn't pointing to a valid product endpoint.",
            "        return None, parsed_path.lstrip('/')",
            "",
            "",
            "# pylint: disable=invalid-name",
            "def split_client_POST_request(path):",
            "    \"\"\"",
            "    Split the given request URI to its parts relevant to the server.",
            "",
            "    Returns the product endpoint, the API version and the API service endpoint",
            "    as a tuple of 3.",
            "    \"\"\"",
            "",
            "    # A standard POST request from an API client looks like:",
            "    # http://localhost:8001/[product-name]/<API version>/<API service>",
            "    # where specifying the product name is optional.",
            "",
            "    split_path = urlparse(path).path.split('/', 3)",
            "",
            "    endpoint_part = split_path[1]",
            "    if is_valid_product_endpoint(split_path[1]):",
            "        version_tag = split_path[2].lstrip('v')",
            "        remainder = split_path[3]",
            "",
            "        return endpoint_part, version_tag, remainder",
            "    elif split_path[1].startswith('v'):",
            "        # Request came through without a valid product URL endpoint to",
            "        # possibly the main server.",
            "        version_tag = split_path[1].lstrip('v')",
            "        remainder = split_path[2]",
            "",
            "        return None, version_tag, remainder",
            "",
            "    return None, None, None",
            "",
            "",
            "# pylint: disable=invalid-name",
            "def is_protected_GET_entrypoint(path):",
            "    \"\"\"",
            "    Returns if the given GET request's PATH enters the server through an",
            "    entry point which is considered protected by authentication requirements.",
            "    \"\"\"",
            "    return path in PROTECTED_ENTRY_POINTS"
        ],
        "afterPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Defines the routing rules for the CodeChecker server.",
            "\"\"\"",
            "",
            "",
            "import re",
            "from urllib.parse import urlparse",
            "",
            "from codechecker_web.shared.version import SUPPORTED_VERSIONS",
            "",
            "# A list of top-level path elements under the webserver root",
            "# which should not be considered as a product route.",
            "NON_PRODUCT_ENDPOINTS = ['index.html',",
            "                         'images',",
            "                         'docs',",
            "                         'live',",
            "                         'ready']",
            "",
            "# A list of top-level path elements in requests (such as Thrift endpoints)",
            "# which should not be considered as a product route.",
            "NON_PRODUCT_ENDPOINTS += ['Authentication',",
            "                          'Products',",
            "                          'CodeCheckerService']",
            "",
            "",
            "# A list of top-level path elements under the webserver root which should",
            "# be protected by authentication requirement when accessing the server.",
            "PROTECTED_ENTRY_POINTS = ['',  # Empty string in a request is 'index.html'.",
            "                          'index.html']",
            "",
            "",
            "def is_valid_product_endpoint(uripart):",
            "    \"\"\"",
            "    Returns whether or not the given URI part is to be considered a valid",
            "    product name.",
            "    \"\"\"",
            "    # FIXME: Endpoint \"all\" should be disallowed, as commit",
            "    # fd59927013d5482ff10e80994511971770753d0c in Dec 2017 added the ability",
            "    # for \"CodeChecker server\" to specify \"--db-status all\" and",
            "    # \"--db-upgrade-schema all\" for the case where *every* product needs to",
            "    # be checked/upgraded, essentially blocking the ability to status-check",
            "    # or schema migrate the product at the endpoint literal \"all\".",
            "",
            "    # There are some forbidden keywords.",
            "    if uripart in NON_PRODUCT_ENDPOINTS:",
            "        return False",
            "",
            "    pattern = r'^[A-Za-z0-9_\\-]+$'",
            "    if not re.match(pattern, uripart):",
            "        return False",
            "",
            "    return True",
            "",
            "",
            "def is_supported_version(version):",
            "    \"\"\"",
            "    Returns whether or not the given version tag is supported by the current",
            "    build. A version is supported if its MAJOR version is supported, and if",
            "    its MINOR version is at most the highest minor version accepted by the",
            "    server.",
            "",
            "    If supported, returns the major and minor version as a tuple.",
            "    \"\"\"",
            "",
            "    version = version.lstrip('v')",
            "    version_parts = version.split('.')",
            "    if len(version_parts) < 2:",
            "        return False",
            "",
            "    # We don't care if accidentally the version tag contains a revision number.",
            "    major, minor = int(version_parts[0]), int(version_parts[1])",
            "    if major in SUPPORTED_VERSIONS and minor <= SUPPORTED_VERSIONS[major]:",
            "        return major, minor",
            "",
            "    return False",
            "",
            "",
            "# pylint: disable=invalid-name",
            "def split_client_GET_request(path):",
            "    \"\"\"",
            "    Split the given request URI to its parts relevant to the server.",
            "",
            "    Returns the product endpoint and the \"remainder\" of the request path",
            "    as a tuple of 2.",
            "    \"\"\"",
            "",
            "    # A standard GET request from a browser looks like:",
            "    # http://localhost:8001/[product-name]/#{request-parts}",
            "    # where the parts are, e.g.: run=[run_id]&report=[report_id]",
            "",
            "    parsed_path = urlparse(path).path",
            "    split_path = parsed_path.split('/', 2)",
            "",
            "    endpoint_part = split_path[1] if len(split_path) > 1 else None",
            "    if endpoint_part and is_valid_product_endpoint(endpoint_part):",
            "        remainder = split_path[2] if len(split_path) == 3 else ''",
            "        return endpoint_part, remainder",
            "    else:",
            "        # The request wasn't pointing to a valid product endpoint.",
            "        return None, parsed_path.lstrip('/')",
            "",
            "",
            "# pylint: disable=invalid-name",
            "def split_client_POST_request(path):",
            "    \"\"\"",
            "    Split the given request URI to its parts relevant to the server.",
            "",
            "    Returns the product endpoint, the API version and the API service endpoint",
            "    as a tuple of 3.",
            "    \"\"\"",
            "",
            "    # A standard POST request from an API client looks like:",
            "    # http://localhost:8001/[product-name/]<API version>/<API service>",
            "    # where specifying the product name is optional.",
            "",
            "    split_path = urlparse(path).path.split('/', 3)",
            "",
            "    endpoint_part = split_path[1]",
            "    if is_valid_product_endpoint(split_path[1]) and len(split_path) == 4:",
            "        version_tag = split_path[2].lstrip('v')",
            "        if not is_supported_version(version_tag):",
            "            return None, None, None",
            "        endpoint = split_path[3]",
            "        return endpoint_part, version_tag, endpoint",
            "",
            "    elif split_path[1].startswith('v') and len(split_path) == 3:",
            "        # Request came through without a valid product URL endpoint to",
            "        # possibly the main server.",
            "        version_tag = split_path[1].lstrip('v')",
            "        if not is_supported_version(version_tag):",
            "            return None, None, None",
            "        endpoint = split_path[2]",
            "        return None, version_tag, endpoint",
            "",
            "    return None, None, None",
            "",
            "",
            "# pylint: disable=invalid-name",
            "def is_protected_GET_entrypoint(path):",
            "    \"\"\"",
            "    Returns if the given GET request's PATH enters the server through an",
            "    entry point which is considered protected by authentication requirements.",
            "    \"\"\"",
            "    return path in PROTECTED_ENTRY_POINTS"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "118": [
                "split_client_POST_request"
            ],
            "124": [
                "split_client_POST_request"
            ],
            "126": [
                "split_client_POST_request"
            ],
            "128": [
                "split_client_POST_request"
            ],
            "129": [
                "split_client_POST_request"
            ],
            "133": [
                "split_client_POST_request"
            ],
            "134": [
                "split_client_POST_request"
            ],
            "135": [
                "split_client_POST_request"
            ]
        },
        "addLocation": [
            "web.server.codechecker_server.routing.split_client_POST_request",
            "web.server.codechecker_server.api.report_server"
        ]
    },
    "web/server/codechecker_server/server.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 328,
                "afterPatchRowNumber": 328,
                "PatchRowcode": "         otrans = TTransport.TMemoryBuffer()"
            },
            "1": {
                "beforePatchRowNumber": 329,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "         oprot = output_protocol_factory.getProtocol(otrans)"
            },
            "2": {
                "beforePatchRowNumber": 330,
                "afterPatchRowNumber": 330,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 331,
                "PatchRowcode": "+        product_endpoint, api_ver, request_endpoint = \\"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+            routing.split_client_POST_request(self.path)"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 333,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 334,
                "PatchRowcode": "+        if product_endpoint is None and api_ver is None and \\"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 335,
                "PatchRowcode": "+                request_endpoint is None:"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 336,
                "PatchRowcode": "+            self.send_thrift_exception(\"Invalid request endpoint path.\", iprot,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 337,
                "PatchRowcode": "+                                       oprot, otrans)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 338,
                "PatchRowcode": "+            return"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 339,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 340,
                "PatchRowcode": "+        # Only Authentication, Configuration, ServerInof"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 341,
                "PatchRowcode": "+        # endpoints are allowed for Anonymous users"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+        # if authentication is required."
            },
            "15": {
                "beforePatchRowNumber": 331,
                "afterPatchRowNumber": 343,
                "PatchRowcode": "         if self.server.manager.is_enabled and \\"
            },
            "16": {
                "beforePatchRowNumber": 332,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                not self.path.endswith(('/Authentication',"
            },
            "17": {
                "beforePatchRowNumber": 333,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                        '/Configuration',"
            },
            "18": {
                "beforePatchRowNumber": 334,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                        '/ServerInfo')) and \\"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 344,
                "PatchRowcode": "+                request_endpoint not in \\"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 345,
                "PatchRowcode": "+                ['Authentication', 'Configuration', 'ServerInfo'] and \\"
            },
            "21": {
                "beforePatchRowNumber": 335,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "                 not self.auth_session:"
            },
            "22": {
                "beforePatchRowNumber": 336,
                "afterPatchRowNumber": 347,
                "PatchRowcode": "             # Bail out if the user is not authenticated..."
            },
            "23": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 348,
                "PatchRowcode": "             # This response has the possibility of melting down Thrift clients,"
            },
            "24": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": 358,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": 359,
                "PatchRowcode": "         # Authentication is handled, we may now respond to the user."
            },
            "26": {
                "beforePatchRowNumber": 349,
                "afterPatchRowNumber": 360,
                "PatchRowcode": "         try:"
            },
            "27": {
                "beforePatchRowNumber": 350,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            product_endpoint, api_ver, request_endpoint = \\"
            },
            "28": {
                "beforePatchRowNumber": 351,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                routing.split_client_POST_request(self.path)"
            },
            "29": {
                "beforePatchRowNumber": 352,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if product_endpoint is None and api_ver is None and \\"
            },
            "30": {
                "beforePatchRowNumber": 353,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    request_endpoint is None:"
            },
            "31": {
                "beforePatchRowNumber": 354,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                raise ValueError(\"Invalid request endpoint path.\")"
            },
            "32": {
                "beforePatchRowNumber": 355,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "33": {
                "beforePatchRowNumber": 356,
                "afterPatchRowNumber": 361,
                "PatchRowcode": "             product = None"
            },
            "34": {
                "beforePatchRowNumber": 357,
                "afterPatchRowNumber": 362,
                "PatchRowcode": "             if product_endpoint:"
            },
            "35": {
                "beforePatchRowNumber": 358,
                "afterPatchRowNumber": 363,
                "PatchRowcode": "                 # The current request came through a product route, and not"
            },
            "36": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 378,
                "PatchRowcode": "                     elif request_endpoint == 'Configuration':"
            },
            "37": {
                "beforePatchRowNumber": 374,
                "afterPatchRowNumber": 379,
                "PatchRowcode": "                         conf_handler = ConfigHandler_v6("
            },
            "38": {
                "beforePatchRowNumber": 375,
                "afterPatchRowNumber": 380,
                "PatchRowcode": "                             self.auth_session,"
            },
            "39": {
                "beforePatchRowNumber": 376,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            self.server.config_session)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 381,
                "PatchRowcode": "+                            self.server.config_session,"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 382,
                "PatchRowcode": "+                            self.server.manager)"
            },
            "42": {
                "beforePatchRowNumber": 377,
                "afterPatchRowNumber": 383,
                "PatchRowcode": "                         processor = ConfigAPI_v6.Processor(conf_handler)"
            },
            "43": {
                "beforePatchRowNumber": 378,
                "afterPatchRowNumber": 384,
                "PatchRowcode": "                     elif request_endpoint == 'ServerInfo':"
            },
            "44": {
                "beforePatchRowNumber": 379,
                "afterPatchRowNumber": 385,
                "PatchRowcode": "                         server_info_handler = ServerInfoHandler_v6(version)"
            }
        },
        "frontPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Main server starts a http server which handles Thrift client",
            "and browser requests.",
            "\"\"\"",
            "",
            "",
            "import atexit",
            "import datetime",
            "from functools import partial",
            "from http.server import HTTPServer, SimpleHTTPRequestHandler",
            "import os",
            "import posixpath",
            "import shutil",
            "import signal",
            "import socket",
            "import ssl",
            "import sys",
            "from typing import List, Optional, Tuple",
            "import urllib",
            "",
            "import multiprocess",
            "from sqlalchemy.orm import sessionmaker",
            "from sqlalchemy.sql.expression import func",
            "from thrift.protocol import TJSONProtocol",
            "from thrift.transport import TTransport",
            "from thrift.Thrift import TApplicationException",
            "from thrift.Thrift import TMessageType",
            "",
            "from codechecker_api_shared.ttypes import DBStatus",
            "from codechecker_api.Authentication_v6 import \\",
            "    codeCheckerAuthentication as AuthAPI_v6",
            "from codechecker_api.Configuration_v6 import \\",
            "    configurationService as ConfigAPI_v6",
            "from codechecker_api.codeCheckerDBAccess_v6 import \\",
            "    codeCheckerDBAccess as ReportAPI_v6",
            "from codechecker_api.ProductManagement_v6 import \\",
            "    codeCheckerProductService as ProductAPI_v6",
            "from codechecker_api.ServerInfo_v6 import \\",
            "    serverInfoService as ServerInfoAPI_v6",
            "",
            "from codechecker_common import util",
            "from codechecker_common.logger import get_logger",
            "from codechecker_common.compatibility.multiprocessing import \\",
            "    Pool, cpu_count",
            "",
            "from codechecker_web.shared import database_status",
            "from codechecker_web.shared.version import get_version_str",
            "",
            "from . import instance_manager, permissions, routing, session_manager",
            "from .api.authentication import ThriftAuthHandler as AuthHandler_v6",
            "from .api.config_handler import ThriftConfigHandler as ConfigHandler_v6",
            "from .api.product_server import ThriftProductHandler as ProductHandler_v6",
            "from .api.report_server import ThriftRequestHandler as ReportHandler_v6",
            "from .api.server_info_handler import \\",
            "    ThriftServerInfoHandler as ServerInfoHandler_v6",
            "from .database import database, db_cleanup",
            "from .database.config_db_model import Product as ORMProduct, \\",
            "    Configuration as ORMConfiguration",
            "from .database.database import DBSession",
            "from .database.run_db_model import IDENTIFIER as RUN_META, Run, RunLock",
            "",
            "LOG = get_logger('server')",
            "",
            "",
            "class RequestHandler(SimpleHTTPRequestHandler):",
            "    \"\"\"",
            "    Handle thrift and browser requests",
            "    Simply modified and extended version of SimpleHTTPRequestHandler",
            "    \"\"\"",
            "    auth_session = None",
            "",
            "    def __init__(self, request, client_address, server):",
            "        self.path = None",
            "        super().__init__(request, client_address, server)",
            "",
            "    def log_message(self, *args):",
            "        \"\"\" Silencing http server. \"\"\"",
            "        return",
            "",
            "    def send_thrift_exception(self, error_msg, iprot, oprot, otrans):",
            "        \"\"\"",
            "        Send an exception response to the client in a proper format which can",
            "        be parsed by the Thrift clients expecting JSON responses.",
            "        \"\"\"",
            "        ex = TApplicationException(TApplicationException.INTERNAL_ERROR,",
            "                                   error_msg)",
            "        fname, _, seqid = iprot.readMessageBegin()",
            "        oprot.writeMessageBegin(fname, TMessageType.EXCEPTION, seqid)",
            "        ex.write(oprot)",
            "        oprot.writeMessageEnd()",
            "        oprot.trans.flush()",
            "        result = otrans.getvalue()",
            "        self.send_response(200)",
            "        self.send_header(\"content-type\", \"application/x-thrift\")",
            "        self.send_header(\"Content-Length\", len(result))",
            "        self.end_headers()",
            "        self.wfile.write(result)",
            "",
            "    def __check_session_cookie(self):",
            "        \"\"\"",
            "        Check the CodeChecker privileged access cookie in the request headers.",
            "",
            "        :returns: A session_manager._Session object if a correct, valid session",
            "        cookie was found in the headers. None, otherwise.",
            "        \"\"\"",
            "",
            "        if not self.server.manager.is_enabled:",
            "            return None",
            "",
            "        session = None",
            "        # Check if the user has presented a privileged access cookie.",
            "        cookies = self.headers.get(\"Cookie\")",
            "        if cookies:",
            "            split = cookies.split(\"; \")",
            "            for cookie in split:",
            "                values = cookie.split(\"=\")",
            "                if len(values) == 2 and \\",
            "                        values[0] == session_manager.SESSION_COOKIE_NAME:",
            "                    session = self.server.manager.get_session(values[1])",
            "",
            "        if session and session.is_alive:",
            "            # If a valid session token was found and it can still be used,",
            "            # mark that the user's last access to the server was the",
            "            # request that resulted in the execution of this function.",
            "            session.revalidate()",
            "            return session",
            "        else:",
            "            # If the user's access cookie is no longer usable (invalid),",
            "            # present an error.",
            "            client_host, client_port, is_ipv6 = \\",
            "                RequestHandler._get_client_host_port(self.client_address)",
            "            LOG.debug(",
            "                \"%s:%s Invalid access, credentials not found - \"",
            "                \"session refused\",",
            "                client_host if not is_ipv6 else '[' + client_host + ']',",
            "                str(client_port))",
            "            return None",
            "",
            "    def __handle_readiness(self):",
            "        \"\"\" Handle readiness probe. \"\"\"",
            "        try:",
            "            cfg_sess = self.server.config_session()",
            "            cfg_sess.query(ORMConfiguration).count()",
            "",
            "            self.send_response(200)",
            "            self.end_headers()",
            "            self.wfile.write(b'CODECHECKER_SERVER_IS_READY')",
            "        except Exception:",
            "            self.send_response(500)",
            "            self.end_headers()",
            "            self.wfile.write(b'CODECHECKER_SERVER_IS_NOT_READY')",
            "        finally:",
            "            if cfg_sess:",
            "                cfg_sess.close()",
            "                cfg_sess.commit()",
            "",
            "    def __handle_liveness(self):",
            "        \"\"\" Handle liveness probe. \"\"\"",
            "        self.send_response(200)",
            "        self.end_headers()",
            "        self.wfile.write(b'CODECHECKER_SERVER_IS_LIVE')",
            "",
            "    def end_headers(self):",
            "        # Sending the authentication cookie",
            "        # in every response if any.",
            "        # This will update the the session cookie",
            "        # on the clients to the newest.",
            "        if self.auth_session:",
            "            token = self.auth_session.token",
            "            if token:",
            "                self.send_header(",
            "                    \"Set-Cookie\",",
            "                    f\"{session_manager.SESSION_COOKIE_NAME}={token}; Path=/\")",
            "",
            "            # Set the current user name in the header.",
            "            user_name = self.auth_session.user",
            "            if user_name:",
            "                self.send_header(\"X-User\", user_name)",
            "",
            "        SimpleHTTPRequestHandler.end_headers(self)",
            "",
            "    @staticmethod",
            "    def _get_client_host_port(address):",
            "        \"\"\"",
            "        Returns the host and port of the request's address, and whether it",
            "        was an IPv6 address.",
            "        \"\"\"",
            "        if len(address) == 2:",
            "            return address[0], address[1], False",
            "        if len(address) == 4:",
            "            return address[0], address[1], True",
            "",
            "        raise IndexError(\"Invalid address tuple given.\")",
            "",
            "    def do_GET(self):",
            "        \"\"\" Handles the SPA browser access (GET requests).",
            "",
            "        It will do the following steps:",
            "         - for requests for index.html ('/'), just respond with the file.",
            "         - if the requested path contains a product endpoint name",
            "           ('/prod/app.js', '/prod/runs'), remove the endpoint from the path.",
            "         - if the requested path is a valid file (e.g: 'app.js'), respond with",
            "           the file.",
            "         - otherwise (e.g: 'runs') respond with index.html.",
            "        \"\"\"",
            "        client_host, client_port, is_ipv6 = \\",
            "            RequestHandler._get_client_host_port(self.client_address)",
            "        self.auth_session = self.__check_session_cookie()",
            "",
            "        username = self.auth_session.user if self.auth_session else 'Anonymous'",
            "        LOG.debug(\"%s:%s -- [%s] GET %s\",",
            "                  client_host if not is_ipv6 else '[' + client_host + ']',",
            "                  client_port, username, self.path)",
            "",
            "        if self.path == '/':",
            "            self.path = 'index.html'",
            "            SimpleHTTPRequestHandler.do_GET(self)",
            "            return",
            "",
            "        if self.path == '/live':",
            "            self.__handle_liveness()",
            "            return",
            "",
            "        if self.path == '/ready':",
            "            self.__handle_readiness()",
            "            return",
            "",
            "        product_endpoint, _ = routing.split_client_GET_request(self.path)",
            "",
            "        # Check that path contains a product endpoint.",
            "        if product_endpoint is not None and product_endpoint != '':",
            "            self.path = self.path.replace(f\"{product_endpoint}/\", \"\", 1)",
            "",
            "        if self.path == '/':",
            "            self.path = \"index.html\"",
            "",
            "        # Check that the given path is a file.",
            "        if not os.path.exists(self.translate_path(self.path)):",
            "            self.path = 'index.html'",
            "",
            "        SimpleHTTPRequestHandler.do_GET(self)",
            "",
            "    def __check_prod_db(self, product_endpoint):",
            "        \"\"\"",
            "        Check the product database status.",
            "        Try to reconnect in some cases.",
            "",
            "        Returns if everything is ok with the database or throw an exception",
            "        with the error message if something is wrong with the database.",
            "        \"\"\"",
            "",
            "        product = self.server.get_product(product_endpoint)",
            "        if not product:",
            "            raise ValueError(",
            "                f\"The product with the given endpoint '{product_endpoint}' \"",
            "                \"does not exist!\")",
            "",
            "        if product.db_status == DBStatus.OK:",
            "            # No reconnect needed.",
            "            return product",
            "",
            "        # Try to reconnect in these cases.",
            "        # Do not try to reconnect if there is a schema mismatch.",
            "        # If the product is not connected, try reconnecting...",
            "        if product.db_status in [DBStatus.FAILED_TO_CONNECT,",
            "                                 DBStatus.MISSING,",
            "                                 DBStatus.SCHEMA_INIT_ERROR]:",
            "            LOG.error(\"Request's product '%s' is not connected! \"",
            "                      \"Attempting reconnect...\", product.endpoint)",
            "            product.connect()",
            "            if product.db_status != DBStatus.OK:",
            "                # If the reconnection fails send an error to the user.",
            "                LOG.debug(\"Product reconnection failed.\")",
            "                error_msg = f\"'{product.endpoint}' database connection failed!\"",
            "                LOG.error(error_msg)",
            "                raise ValueError(error_msg)",
            "        else:",
            "            # Send an error to the user.",
            "            db_stat = DBStatus._VALUES_TO_NAMES.get(product.db_status)",
            "            error_msg = f\"'{product.endpoint}' database connection \" \\",
            "                f\"failed. DB status: {str(db_stat)}\"",
            "            LOG.error(error_msg)",
            "            raise ValueError(error_msg)",
            "",
            "        return product",
            "",
            "    # pylint: disable=invalid-name",
            "    def do_POST(self):",
            "        \"\"\"",
            "        Handles POST queries, which are usually Thrift messages.",
            "        \"\"\"",
            "        protocol_factory = TJSONProtocol.TJSONProtocolFactory()",
            "        input_protocol_factory = protocol_factory",
            "        output_protocol_factory = protocol_factory",
            "",
            "        # Get Thrift API function name to print to the log output.",
            "        itrans = TTransport.TFileObjectTransport(self.rfile)",
            "        itrans = TTransport.TBufferedTransport(itrans,",
            "                                               int(self.headers[",
            "                                                   'Content-Length']))",
            "        iprot = input_protocol_factory.getProtocol(itrans)",
            "        fname, _, _ = iprot.readMessageBegin()",
            "",
            "        client_host, client_port, is_ipv6 = \\",
            "            RequestHandler._get_client_host_port(self.client_address)",
            "        self.auth_session = self.__check_session_cookie()",
            "        auth_user = \\",
            "            self.auth_session.user if self.auth_session else \"Anonymous\"",
            "        host_info = client_host if not is_ipv6 else '[' + client_host + ']'",
            "        api_info = f\"{host_info}:{client_port} -- [{auth_user}] POST \" \\",
            "                   f\"{self.path}@{fname}\"",
            "        LOG.info(api_info)",
            "",
            "        # Create new thrift handler.",
            "        version = self.server.version",
            "",
            "        cstringio_buf = itrans.cstringio_buf.getvalue()",
            "        itrans = TTransport.TMemoryBuffer(cstringio_buf)",
            "        iprot = input_protocol_factory.getProtocol(itrans)",
            "",
            "        otrans = TTransport.TMemoryBuffer()",
            "        oprot = output_protocol_factory.getProtocol(otrans)",
            "",
            "        if self.server.manager.is_enabled and \\",
            "                not self.path.endswith(('/Authentication',",
            "                                        '/Configuration',",
            "                                        '/ServerInfo')) and \\",
            "                not self.auth_session:",
            "            # Bail out if the user is not authenticated...",
            "            # This response has the possibility of melting down Thrift clients,",
            "            # but the user is expected to properly authenticate first.",
            "            LOG.debug(\"%s:%s Invalid access, credentials not found \"",
            "                      \"- session refused.\",",
            "                      client_host if not is_ipv6 else '[' + client_host + ']',",
            "                      str(client_port))",
            "",
            "            self.send_thrift_exception(\"Error code 401: Unauthorized!\", iprot,",
            "                                       oprot, otrans)",
            "            return",
            "",
            "        # Authentication is handled, we may now respond to the user.",
            "        try:",
            "            product_endpoint, api_ver, request_endpoint = \\",
            "                routing.split_client_POST_request(self.path)",
            "            if product_endpoint is None and api_ver is None and \\",
            "                    request_endpoint is None:",
            "                raise ValueError(\"Invalid request endpoint path.\")",
            "",
            "            product = None",
            "            if product_endpoint:",
            "                # The current request came through a product route, and not",
            "                # to the main endpoint.",
            "                product = self.__check_prod_db(product_endpoint)",
            "",
            "            version_supported = routing.is_supported_version(api_ver)",
            "            if version_supported:",
            "                major_version, _ = version_supported",
            "",
            "                if major_version == 6:",
            "                    if request_endpoint == 'Authentication':",
            "                        auth_handler = AuthHandler_v6(",
            "                            self.server.manager,",
            "                            self.auth_session,",
            "                            self.server.config_session)",
            "                        processor = AuthAPI_v6.Processor(auth_handler)",
            "                    elif request_endpoint == 'Configuration':",
            "                        conf_handler = ConfigHandler_v6(",
            "                            self.auth_session,",
            "                            self.server.config_session)",
            "                        processor = ConfigAPI_v6.Processor(conf_handler)",
            "                    elif request_endpoint == 'ServerInfo':",
            "                        server_info_handler = ServerInfoHandler_v6(version)",
            "                        processor = ServerInfoAPI_v6.Processor(",
            "                            server_info_handler)",
            "                    elif request_endpoint == 'Products':",
            "                        prod_handler = ProductHandler_v6(",
            "                            self.server,",
            "                            self.auth_session,",
            "                            self.server.config_session,",
            "                            product,",
            "                            version)",
            "                        processor = ProductAPI_v6.Processor(prod_handler)",
            "                    elif request_endpoint == 'CodeCheckerService':",
            "                        # This endpoint is a product's report_server.",
            "                        if not product:",
            "                            error_msg = \\",
            "                                \"Requested CodeCheckerService on a \" \\",
            "                                f\"nonexistent product: '{product_endpoint}'.\"",
            "                            LOG.error(error_msg)",
            "                            raise ValueError(error_msg)",
            "",
            "                        if product_endpoint:",
            "                            # The current request came through a",
            "                            # product route, and not to the main endpoint.",
            "                            product = self.__check_prod_db(product_endpoint)",
            "",
            "                        acc_handler = ReportHandler_v6(",
            "                            self.server.manager,",
            "                            product.session_factory,",
            "                            product,",
            "                            self.auth_session,",
            "                            self.server.config_session,",
            "                            version,",
            "                            api_ver,",
            "                            self.server.context)",
            "                        processor = ReportAPI_v6.Processor(acc_handler)",
            "                    else:",
            "                        LOG.debug(\"This API endpoint does not exist.\")",
            "                        error_msg = f\"No API endpoint named '{self.path}'.\"",
            "                        raise ValueError(error_msg)",
            "                else:",
            "                    raise ValueError(",
            "                        f\"API version {major_version} not supported\")",
            "",
            "            else:",
            "                error_msg = \\",
            "                    \"The API version you are using is not supported \" \\",
            "                    \"by this server (server API version: \" \\",
            "                    f\"{get_version_str()})!\"",
            "                self.send_thrift_exception(error_msg, iprot, oprot, otrans)",
            "                return",
            "",
            "            processor.process(iprot, oprot)",
            "            result = otrans.getvalue()",
            "",
            "            self.send_response(200)",
            "            self.send_header(\"content-type\", \"application/x-thrift\")",
            "            self.send_header(\"Content-Length\", len(result))",
            "            self.end_headers()",
            "            self.wfile.write(result)",
            "            return",
            "",
            "        except BrokenPipeError as ex:",
            "            LOG.warning(\"%s failed with BrokenPipeError: %s\",",
            "                        api_info, str(ex))",
            "            import traceback",
            "            traceback.print_exc()",
            "        except Exception as ex:",
            "            LOG.warning(\"%s failed with Exception: %s\", api_info, str(ex))",
            "            import traceback",
            "            traceback.print_exc()",
            "",
            "            cstringio_buf = itrans.cstringio_buf.getvalue()",
            "            if cstringio_buf:",
            "                itrans = TTransport.TMemoryBuffer(cstringio_buf)",
            "                iprot = input_protocol_factory.getProtocol(itrans)",
            "",
            "            self.send_thrift_exception(str(ex), iprot, oprot, otrans)",
            "",
            "    def list_directory(self, path):",
            "        \"\"\" Disable directory listing. \"\"\"",
            "        self.send_error(405, \"No permission to list directory\")",
            "",
            "    def translate_path(self, path):",
            "        \"\"\"",
            "        Modified version from SimpleHTTPRequestHandler.",
            "        Path is set to www_root.",
            "        \"\"\"",
            "        # Abandon query parameters.",
            "        path = path.split('?', 1)[0]",
            "        path = path.split('#', 1)[0]",
            "        path = posixpath.normpath(urllib.parse.unquote(path))",
            "        words = path.split('/')",
            "        words = [_f for _f in words if _f]",
            "        path = self.server.www_root",
            "        for word in words:",
            "            _, word = os.path.splitdrive(word)",
            "            _, word = os.path.split(word)",
            "            if word in (os.curdir, os.pardir):",
            "                continue",
            "            path = os.path.join(path, word)",
            "        return path",
            "",
            "",
            "class Product:",
            "    \"\"\"",
            "    Represents a product, which is a distinct storage of analysis reports in",
            "    a separate database (and database connection) with its own access control.",
            "    \"\"\"",
            "",
            "    # The amount of SECONDS that need to pass after the last unsuccessful",
            "    # connect() call so the next could be made.",
            "    CONNECT_RETRY_TIMEOUT = 300",
            "",
            "    def __init__(self, id_: int, endpoint: str, display_name: str,",
            "                 connection_string: str, context, check_env):",
            "        \"\"\"",
            "        Set up a new managed product object for the configuration given.",
            "        \"\"\"",
            "        self.__id = id_",
            "        self.__endpoint = endpoint",
            "        self.__display_name = display_name",
            "        self.__connection_string = connection_string",
            "        self.__driver_name = None",
            "        self.__context = context",
            "        self.__check_env = check_env",
            "        self.__engine = None",
            "        self.__session = None",
            "        self.__db_status = DBStatus.MISSING",
            "",
            "        self.__last_connect_attempt = None",
            "",
            "    @property",
            "    def id(self):",
            "        return self.__id",
            "",
            "    @property",
            "    def endpoint(self):",
            "        \"\"\"",
            "        Returns the accessible URL endpoint of the product.",
            "        \"\"\"",
            "        return self.__endpoint",
            "",
            "    @property",
            "    def name(self):",
            "        \"\"\"",
            "        Returns the display name of the product.",
            "        \"\"\"",
            "        return self.__display_name",
            "",
            "    @property",
            "    def session_factory(self):",
            "        \"\"\"",
            "        Returns the session maker on this product's database engine which",
            "        can be used to initiate transactional connections.",
            "        \"\"\"",
            "        return self.__session",
            "",
            "    @property",
            "    def driver_name(self):",
            "        \"\"\"",
            "        Returns the name of the sql driver (sqlite, postgres).",
            "        \"\"\"",
            "        return self.__driver_name",
            "",
            "    @property",
            "    def db_status(self):",
            "        \"\"\"",
            "        Returns the status of the database which belongs to this product.",
            "        Call connect to update it.",
            "        \"\"\"",
            "        return self.__db_status",
            "",
            "    @property",
            "    def last_connection_failure(self):",
            "        \"\"\"",
            "        Returns the reason behind the last executed connection attempt's",
            "        failure.",
            "        \"\"\"",
            "        return self.__last_connect_attempt[1] if self.__last_connect_attempt \\",
            "            else None",
            "",
            "    def connect(self, init_db=False):",
            "        \"\"\"",
            "        Initiates the actual connection to the database configured for the",
            "        product.",
            "",
            "        Each time the connect is called the db_status is updated.",
            "        \"\"\"",
            "        LOG.debug(\"Checking '%s' database.\", self.endpoint)",
            "",
            "        sql_server = database.SQLServer.from_connection_string(",
            "            self.__connection_string,",
            "            self.__endpoint,",
            "            RUN_META,",
            "            self.__context.run_migration_root,",
            "            interactive=False,",
            "            env=self.__check_env)",
            "",
            "        if isinstance(sql_server, database.PostgreSQLServer):",
            "            self.__driver_name = 'postgresql'",
            "        elif isinstance(sql_server, database.SQLiteDatabase):",
            "            self.__driver_name = 'sqlite'",
            "",
            "        try:",
            "            LOG.debug(\"Trying to connect to the database\")",
            "",
            "            # Create the SQLAlchemy engine.",
            "            self.__engine = sql_server.create_engine()",
            "            LOG.debug(self.__engine)",
            "",
            "            self.__session = sessionmaker(bind=self.__engine)",
            "",
            "            self.__engine.execute('SELECT 1')",
            "            self.__db_status = sql_server.check_schema()",
            "            self.__last_connect_attempt = None",
            "",
            "            if self.__db_status == DBStatus.SCHEMA_MISSING and init_db:",
            "                LOG.debug(\"Initializing new database schema.\")",
            "                self.__db_status = sql_server.connect(init_db)",
            "",
            "        except Exception as ex:",
            "            LOG.exception(\"The database for product '%s' cannot be\"",
            "                          \" connected to.\", self.endpoint)",
            "            self.__db_status = DBStatus.FAILED_TO_CONNECT",
            "            self.__last_connect_attempt = (datetime.datetime.now(), str(ex))",
            "",
            "    def get_details(self):",
            "        \"\"\"",
            "        Get details for a product from the database.",
            "",
            "        It may throw different error messages depending on the used SQL driver",
            "        adapter in case of connection error.",
            "        \"\"\"",
            "        with DBSession(self.session_factory) as run_db_session:",
            "            run_locks = run_db_session.query(RunLock.name) \\",
            "                .filter(RunLock.locked_at.isnot(None)) \\",
            "                .all()",
            "",
            "            runs_in_progress = set(run_lock[0] for run_lock in run_locks)",
            "",
            "            num_of_runs = run_db_session.query(Run).count()",
            "",
            "            latest_store_to_product = \"\"",
            "            if num_of_runs:",
            "                last_updated_run = run_db_session.query(Run) \\",
            "                    .order_by(Run.date.desc()) \\",
            "                    .limit(1) \\",
            "                    .one_or_none()",
            "",
            "                latest_store_to_product = last_updated_run.date",
            "",
            "        return num_of_runs, runs_in_progress, latest_store_to_product",
            "",
            "    def teardown(self):",
            "        \"\"\"",
            "        Disposes the database connection to the product's backend.",
            "        \"\"\"",
            "        if self.__db_status == DBStatus.FAILED_TO_CONNECT:",
            "            return",
            "",
            "        self.__engine.dispose()",
            "",
            "        self.__session = None",
            "        self.__engine = None",
            "",
            "    def cleanup_run_db(self):",
            "        \"\"\"",
            "        Cleanup the run database which belongs to this product.",
            "        \"\"\"",
            "        LOG.info(\"[%s] Garbage collection started...\", self.endpoint)",
            "",
            "        db_cleanup.remove_expired_data(self)",
            "        db_cleanup.remove_unused_data(self)",
            "        db_cleanup.update_contextual_data(self, self.__context)",
            "",
            "        LOG.info(\"[%s] Garbage collection finished.\", self.endpoint)",
            "        return True",
            "",
            "",
            "def _do_db_cleanup(context, check_env,",
            "                   id_: int, endpoint: str, display_name: str,",
            "                   connection_str: str) -> Tuple[Optional[bool], str]:",
            "    # This functions is a concurrent job handler!",
            "    try:",
            "        prod = Product(id_, endpoint, display_name, connection_str,",
            "                       context, check_env)",
            "        prod.connect(init_db=False)",
            "        if prod.db_status != DBStatus.OK:",
            "            status_str = database_status.db_status_msg.get(prod.db_status)",
            "            return None, \\",
            "                f\"Cleanup not attempted, database status is \\\"{status_str}\\\"\"",
            "",
            "        prod.cleanup_run_db()",
            "        prod.teardown()",
            "",
            "        # Result is hard-wired to True, because the db_cleanup routines",
            "        # swallow and log the potential errors but do not return them.",
            "        return True, \"\"",
            "    except Exception as e:",
            "        import traceback",
            "        traceback.print_exc()",
            "        return False, str(e)",
            "",
            "",
            "def _do_db_cleanups(config_database, context, check_env) \\",
            "        -> Tuple[bool, List[Tuple[str, str]]]:",
            "    \"\"\"",
            "    Performs on-demand start-up database cleanup on all the products present",
            "    in the ``config_database``.",
            "",
            "    Returns whether database clean-up succeeded for all products, and the",
            "    list of products for which it failed, along with the failure reason.",
            "    \"\"\"",
            "    def _get_products() -> List[Product]:",
            "        products = []",
            "        cfg_engine = config_database.create_engine()",
            "        cfg_session_factory = sessionmaker(bind=cfg_engine)",
            "        with DBSession(cfg_session_factory) as cfg_db:",
            "            for row in cfg_db.query(ORMProduct) \\",
            "                    .order_by(ORMProduct.endpoint.asc()) \\",
            "                    .all():",
            "                products.append((row.id, row.endpoint, row.display_name,",
            "                                 row.connection))",
            "        cfg_engine.dispose()",
            "        return products",
            "",
            "    products = _get_products()",
            "    if not products:",
            "        return True, []",
            "",
            "    thr_count = util.clamp(1, len(products), cpu_count())",
            "    overall_result, failures = True, []",
            "    with Pool(max_workers=thr_count) as executor:",
            "        LOG.info(\"Performing database cleanup using %d concurrent jobs...\",",
            "                 thr_count)",
            "        for product, result in \\",
            "                zip(products, executor.map(",
            "                    partial(_do_db_cleanup, context, check_env),",
            "                    *zip(*products))):",
            "            success, reason = result",
            "            if not success:",
            "                _, endpoint, _, _ = product",
            "                overall_result = False",
            "                failures.append((endpoint, reason))",
            "",
            "    return overall_result, failures",
            "",
            "",
            "class CCSimpleHttpServer(HTTPServer):",
            "    \"\"\"",
            "    Simple http server to handle requests from the clients.",
            "    \"\"\"",
            "",
            "    daemon_threads = False",
            "    address_family = socket.AF_INET  # IPv4",
            "",
            "    def __init__(self,",
            "                 server_address,",
            "                 RequestHandlerClass,",
            "                 config_directory,",
            "                 product_db_sql_server,",
            "                 pckg_data,",
            "                 context,",
            "                 check_env,",
            "                 manager):",
            "",
            "        LOG.debug(\"Initializing HTTP server...\")",
            "",
            "        self.config_directory = config_directory",
            "        self.www_root = pckg_data['www_root']",
            "        self.doc_root = pckg_data['doc_root']",
            "        self.version = pckg_data['version']",
            "        self.context = context",
            "        self.check_env = check_env",
            "        self.manager = manager",
            "        self.__products = {}",
            "",
            "        # Create a database engine for the configuration database.",
            "        LOG.debug(\"Creating database engine for CONFIG DATABASE...\")",
            "        self.__engine = product_db_sql_server.create_engine()",
            "        self.config_session = sessionmaker(bind=self.__engine)",
            "        self.manager.set_database_connection(self.config_session)",
            "",
            "        # Load the initial list of products and set up the server.",
            "        cfg_sess = self.config_session()",
            "        permissions.initialise_defaults('SYSTEM', {",
            "            'config_db_session': cfg_sess",
            "        })",
            "        products = cfg_sess.query(ORMProduct).all()",
            "        for product in products:",
            "            self.add_product(product)",
            "            permissions.initialise_defaults('PRODUCT', {",
            "                'config_db_session': cfg_sess,",
            "                'productID': product.id",
            "            })",
            "        cfg_sess.commit()",
            "        cfg_sess.close()",
            "",
            "        try:",
            "            HTTPServer.__init__(self, server_address,",
            "                                RequestHandlerClass,",
            "                                bind_and_activate=True)",
            "            ssl_key_file = os.path.join(config_directory, \"key.pem\")",
            "            ssl_cert_file = os.path.join(config_directory, \"cert.pem\")",
            "",
            "            self.configure_keepalive()",
            "",
            "            if os.path.isfile(ssl_key_file) and os.path.isfile(ssl_cert_file):",
            "                LOG.info(\"Initiating SSL. Server listening on secure socket.\")",
            "                LOG.debug(\"Using cert file: %s\", ssl_cert_file)",
            "                LOG.debug(\"Using key file: %s\", ssl_key_file)",
            "                ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)",
            "                ssl_context.load_cert_chain(certfile=ssl_cert_file,",
            "                                            keyfile=ssl_key_file)",
            "                # FIXME introduce with python 3.7",
            "                # ssl_context.minimum_version = ssl.TLSVersion.TLSv1_2",
            "",
            "                # TLS1 and TLS1.1 were deprecated in RFC8996",
            "                # https://datatracker.ietf.org/doc/html/rfc8996",
            "                ssl_context.options |= (ssl.OP_NO_TLSv1 | ssl.OP_NO_TLSv1_1)",
            "                self.socket = ssl_context.wrap_socket(self.socket,",
            "                                                      server_side=True)",
            "",
            "            else:",
            "                LOG.info(\"Searching for SSL key at %s, cert at %s, \"",
            "                         \"not found...\", ssl_key_file, ssl_cert_file)",
            "                LOG.info(\"Falling back to simple, insecure HTTP.\")",
            "",
            "        except Exception as e:",
            "            LOG.error(\"Couldn't start the server: %s\", e.__str__())",
            "            raise",
            "",
            "    def configure_keepalive(self):",
            "        \"\"\"",
            "        Enable keepalive on the socket and some TCP keepalive configuration",
            "        option based on the server configuration file.",
            "        \"\"\"",
            "        if not self.manager.is_keepalive_enabled():",
            "            return",
            "",
            "        keepalive_is_on = self.socket.getsockopt(socket.SOL_SOCKET,",
            "                                                 socket.SO_KEEPALIVE)",
            "        if keepalive_is_on != 0:",
            "            LOG.debug('Socket keepalive already on.')",
            "        else:",
            "            LOG.debug('Socket keepalive off, turning on.')",
            "",
            "        ret = self.socket.setsockopt(socket.SOL_SOCKET,",
            "                                     socket.SO_KEEPALIVE, 1)",
            "        if ret:",
            "            LOG.error('Failed to set socket keepalive: %s', ret)",
            "",
            "        idle = self.manager.get_keepalive_idle()",
            "        if idle:",
            "            ret = self.socket.setsockopt(socket.IPPROTO_TCP,",
            "                                         socket.TCP_KEEPIDLE, idle)",
            "            if ret:",
            "                LOG.error('Failed to set TCP keepalive idle: %s', ret)",
            "",
            "        interval = self.manager.get_keepalive_interval()",
            "        if interval:",
            "            ret = self.socket.setsockopt(socket.IPPROTO_TCP,",
            "                                         socket.TCP_KEEPINTVL, interval)",
            "            if ret:",
            "                LOG.error('Failed to set TCP keepalive interval: %s', ret)",
            "",
            "        max_probe = self.manager.get_keepalive_max_probe()",
            "        if max_probe:",
            "            ret = self.socket.setsockopt(socket.IPPROTO_TCP,",
            "                                         socket.TCP_KEEPCNT, max_probe)",
            "            if ret:",
            "                LOG.error('Failed to set TCP max keepalive probe: %s', ret)",
            "",
            "    def terminate(self):",
            "        \"\"\"",
            "        Terminating the server.",
            "        \"\"\"",
            "        try:",
            "            self.server_close()",
            "            self.__engine.dispose()",
            "        except Exception as ex:",
            "            LOG.error(\"Failed to shut down the WEB server!\")",
            "            LOG.error(str(ex))",
            "            sys.exit(1)",
            "",
            "    def add_product(self, orm_product, init_db=False):",
            "        \"\"\"",
            "        Adds a product to the list of product databases connected to",
            "        by the server.",
            "        Checks the database connection for the product databases.",
            "        \"\"\"",
            "        if orm_product.endpoint in self.__products:",
            "            LOG.debug(\"This product is already configured!\")",
            "            return",
            "",
            "        LOG.debug(\"Setting up product '%s'\", orm_product.endpoint)",
            "",
            "        prod = Product(orm_product.id,",
            "                       orm_product.endpoint,",
            "                       orm_product.display_name,",
            "                       orm_product.connection,",
            "                       self.context,",
            "                       self.check_env)",
            "",
            "        # Update the product database status.",
            "        prod.connect()",
            "        if prod.db_status == DBStatus.SCHEMA_MISSING and init_db:",
            "            LOG.debug(\"Schema was missing in the database. Initializing new\")",
            "            prod.connect(init_db=True)",
            "",
            "        # The \"num_of_runs\" column of the config database is shown on the",
            "        # product page of the web interface. This is intentionally redundant",
            "        # with a simple query that would count the number of runs in a product:",
            "        # measurements have proven that this caching significantly improves",
            "        # responsibility.",
            "        # This field is incremented whenever a run is added to a product, and",
            "        # decreased when run(s) are removed. However, if these numbers ever",
            "        # diverge, the product page and the bottom right of the run page would",
            "        # display different run counts. To help on this, the num_of_runs column",
            "        # is updated at every server startup.",
            "        # FIXME: Pylint emits a false positive here, and states that",
            "        # session_factory() is not callable, because it initializes to None.",
            "        # More on this:",
            "        # https://github.com/Ericsson/codechecker/pull/3733#issuecomment-1235304179",
            "        # https://github.com/PyCQA/pylint/issues/6005",
            "        orm_product.num_of_runs = \\",
            "            prod.session_factory().query(func.count(Run.id)).one_or_none()[0] \\",
            "            # pylint: disable=not-callable",
            "",
            "        self.__products[prod.endpoint] = prod",
            "",
            "    @property",
            "    def num_products(self):",
            "        \"\"\"",
            "        Returns the number of products currently mounted by the server.",
            "        \"\"\"",
            "        return len(self.__products)",
            "",
            "    def get_product(self, endpoint):",
            "        \"\"\"",
            "        Get the product connection object for the given endpoint, or None.",
            "        \"\"\"",
            "        if endpoint in self.__products:",
            "            return self.__products.get(endpoint)",
            "",
            "        LOG.debug(\"Product with the given endpoint '%s' does not exist in \"",
            "                  \"the local cache. Try to get it from the database.\",",
            "                  endpoint)",
            "",
            "        # If the product doesn't find in the cache, try to get it from the",
            "        # database.",
            "        try:",
            "            cfg_sess = self.config_session()",
            "            product = cfg_sess.query(ORMProduct) \\",
            "                .filter(ORMProduct.endpoint == endpoint) \\",
            "                .limit(1).one_or_none()",
            "",
            "            if not product:",
            "                return None",
            "",
            "            self.add_product(product)",
            "            permissions.initialise_defaults('PRODUCT', {",
            "                'config_db_session': cfg_sess,",
            "                'productID': product.id",
            "            })",
            "",
            "            return self.__products.get(endpoint, None)",
            "        finally:",
            "            if cfg_sess:",
            "                cfg_sess.close()",
            "                cfg_sess.commit()",
            "",
            "    def get_only_product(self):",
            "        \"\"\"",
            "        Returns the Product object for the only product connected to by the",
            "        server, or None, if there are 0 or >= 2 products managed.",
            "        \"\"\"",
            "        return list(self.__products.items())[0][1] if self.num_products == 1 \\",
            "            else None",
            "",
            "    def remove_product(self, endpoint):",
            "        product = self.get_product(endpoint)",
            "        if not product:",
            "            raise ValueError(",
            "                f\"The product with the given endpoint '{endpoint}' does \"",
            "                \"not exist!\")",
            "",
            "        LOG.info(\"Disconnecting product '%s'\", endpoint)",
            "        product.teardown()",
            "",
            "        del self.__products[endpoint]",
            "",
            "    def remove_products_except(self, endpoints_to_keep):",
            "        \"\"\"",
            "        Removes EVERY product connection from the server except those",
            "        endpoints specified in :endpoints_to_keep.",
            "        \"\"\"",
            "        for ep in list(self.__products):",
            "            if ep not in endpoints_to_keep:",
            "                self.remove_product(ep)",
            "",
            "",
            "class CCSimpleHttpServerIPv6(CCSimpleHttpServer):",
            "    \"\"\"",
            "    CodeChecker HTTP simple server that listens over an IPv6 socket.",
            "    \"\"\"",
            "",
            "    address_family = socket.AF_INET6",
            "",
            "",
            "def start_server(config_directory, package_data, port, config_sql_server,",
            "                 listen_address, force_auth, skip_db_cleanup: bool,",
            "                 context, check_env):",
            "    \"\"\"",
            "    Start http server to handle web client and thrift requests.",
            "    \"\"\"",
            "    LOG.debug(\"Starting CodeChecker server...\")",
            "",
            "    server_addr = (listen_address, port)",
            "",
            "    # The root user file is DEPRECATED AND IGNORED",
            "    root_file = os.path.join(config_directory, 'root.user')",
            "    if os.path.exists(root_file):",
            "        LOG.warning(\"The 'root.user' file:  %s\"",
            "                    \" is deprecated and ignored. If you want to\"",
            "                    \" setup an initial user with SUPER_USER permission,\"",
            "                    \" configure the super_user field in the server_config.json\"",
            "                    \" as described in the documentation.\"",
            "                    \" To get rid off this warning,\"",
            "                    \" simply delete the root.user file.\",",
            "                    root_file)",
            "    # Check whether configuration file exists, create an example if not.",
            "    server_cfg_file = os.path.join(config_directory, 'server_config.json')",
            "    if not os.path.exists(server_cfg_file):",
            "        # For backward compatibility reason if the session_config.json file",
            "        # exists we rename it to server_config.json.",
            "        session_cfg_file = os.path.join(config_directory,",
            "                                        'session_config.json')",
            "        example_cfg_file = os.path.join(os.environ['CC_DATA_FILES_DIR'],",
            "                                        'config', 'server_config.json')",
            "        if os.path.exists(session_cfg_file):",
            "            LOG.info(\"Renaming '%s' to '%s'. Please check the example \"",
            "                     \"configuration file ('%s') or the user guide for more \"",
            "                     \"information.\", session_cfg_file,",
            "                     server_cfg_file, example_cfg_file)",
            "            os.rename(session_cfg_file, server_cfg_file)",
            "        else:",
            "            LOG.info(\"CodeChecker server's example configuration file \"",
            "                     \"created at '%s'\", server_cfg_file)",
            "            shutil.copyfile(example_cfg_file, server_cfg_file)",
            "",
            "    try:",
            "        manager = session_manager.SessionManager(",
            "            server_cfg_file,",
            "            force_auth)",
            "    except IOError as ioerr:",
            "        LOG.debug(ioerr)",
            "        LOG.error(\"The server's configuration file \"",
            "                  \"is missing or can not be read!\")",
            "        sys.exit(1)",
            "    except ValueError as verr:",
            "        LOG.debug(verr)",
            "        LOG.error(\"The server's configuration file is invalid!\")",
            "        sys.exit(1)",
            "",
            "    if not skip_db_cleanup:",
            "        all_success, fails = _do_db_cleanups(config_sql_server,",
            "                                             context,",
            "                                             check_env)",
            "        if not all_success:",
            "            LOG.error(\"Failed to perform automatic cleanup on %d products! \"",
            "                      \"Earlier logs might contain additional detailed \"",
            "                      \"reasoning.\\n\\t* %s\", len(fails),",
            "                      \"\\n\\t* \".join(",
            "                          (f\"'{ep}' ({reason})\" for (ep, reason) in fails)",
            "                      ))",
            "    else:",
            "        LOG.debug(\"Skipping db_cleanup, as requested.\")",
            "",
            "    server_clazz = CCSimpleHttpServer",
            "    if ':' in server_addr[0]:",
            "        # IPv6 address specified for listening.",
            "        # FIXME: Python>=3.8 automatically handles IPv6 if ':' is in the bind",
            "        # address, see https://bugs.python.org/issue24209.",
            "        server_clazz = CCSimpleHttpServerIPv6",
            "",
            "    http_server = server_clazz(server_addr,",
            "                               RequestHandler,",
            "                               config_directory,",
            "                               config_sql_server,",
            "                               package_data,",
            "                               context,",
            "                               check_env,",
            "                               manager)",
            "",
            "    # If the server was started with the port 0, the OS will pick an available",
            "    # port. For this reason we will update the port variable after server",
            "    # initialization.",
            "    port = http_server.socket.getsockname()[1]",
            "",
            "    processes = []",
            "",
            "    def signal_handler(signum, _):",
            "        \"\"\"",
            "        Handle SIGTERM to stop the server running.",
            "        \"\"\"",
            "        LOG.info(\"Shutting down the WEB server on [%s:%d]\",",
            "                 '[' + listen_address + ']'",
            "                 if server_clazz is CCSimpleHttpServerIPv6 else listen_address,",
            "                 port)",
            "        http_server.terminate()",
            "",
            "        # Terminate child processes.",
            "        for pp in processes:",
            "            pp.terminate()",
            "",
            "        sys.exit(128 + signum)",
            "",
            "    def reload_signal_handler(*_args, **_kwargs):",
            "        \"\"\"",
            "        Reloads server configuration file.",
            "        \"\"\"",
            "        manager.reload_config()",
            "",
            "    try:",
            "        instance_manager.register(os.getpid(),",
            "                                  os.path.abspath(",
            "                                      context.codechecker_workspace),",
            "                                  port)",
            "    except IOError as ex:",
            "        LOG.debug(ex.strerror)",
            "",
            "    LOG.info(\"Server waiting for client requests on [%s:%d]\",",
            "             '[' + listen_address + ']'",
            "             if server_clazz is CCSimpleHttpServerIPv6 else listen_address,",
            "             port)",
            "",
            "    def unregister_handler(pid):",
            "        \"\"\"",
            "        Handle errors during instance unregistration.",
            "        The workspace might be removed so updating the",
            "        config content might fail.",
            "        \"\"\"",
            "        try:",
            "            instance_manager.unregister(pid)",
            "        except IOError as ex:",
            "            LOG.debug(ex.strerror)",
            "",
            "    atexit.register(unregister_handler, os.getpid())",
            "",
            "    for _ in range(manager.worker_processes - 1):",
            "        p = multiprocess.Process(target=http_server.serve_forever)",
            "        processes.append(p)",
            "        p.start()",
            "",
            "    signal.signal(signal.SIGINT, signal_handler)",
            "    signal.signal(signal.SIGTERM, signal_handler)",
            "",
            "    if sys.platform != \"win32\":",
            "        signal.signal(signal.SIGHUP, reload_signal_handler)",
            "",
            "    # Main process also acts as a worker.",
            "    http_server.serve_forever()",
            "",
            "    LOG.info(\"Webserver quit.\")",
            "",
            "",
            "def add_initial_run_database(config_sql_server, product_connection):",
            "    \"\"\"",
            "    Create a default run database as SQLite in the config directory,",
            "    and add it to the list of products in the config database specified by",
            "    db_conn_string.",
            "    \"\"\"",
            "",
            "    # Connect to the configuration database",
            "    LOG.debug(\"Creating database engine for CONFIG DATABASE...\")",
            "    __engine = config_sql_server.create_engine()",
            "    product_session = sessionmaker(bind=__engine)",
            "",
            "    # Load the initial list of products and create the connections.",
            "    sess = product_session()",
            "    products = sess.query(ORMProduct).all()",
            "    if products:",
            "        raise ValueError(\"Called create_initial_run_database on non-empty \"",
            "                         \"config database -- you shouldn't have done this!\")",
            "",
            "    LOG.debug(\"Adding default product to the config db...\")",
            "    product = ORMProduct('Default', product_connection, 'Default',",
            "                         \"Default product created at server start.\")",
            "    sess.add(product)",
            "    sess.commit()",
            "    sess.close()",
            "",
            "    LOG.debug(\"Default product set up.\")"
        ],
        "afterPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "\"\"\"",
            "Main server starts a http server which handles Thrift client",
            "and browser requests.",
            "\"\"\"",
            "",
            "",
            "import atexit",
            "import datetime",
            "from functools import partial",
            "from http.server import HTTPServer, SimpleHTTPRequestHandler",
            "import os",
            "import posixpath",
            "import shutil",
            "import signal",
            "import socket",
            "import ssl",
            "import sys",
            "from typing import List, Optional, Tuple",
            "import urllib",
            "",
            "import multiprocess",
            "from sqlalchemy.orm import sessionmaker",
            "from sqlalchemy.sql.expression import func",
            "from thrift.protocol import TJSONProtocol",
            "from thrift.transport import TTransport",
            "from thrift.Thrift import TApplicationException",
            "from thrift.Thrift import TMessageType",
            "",
            "from codechecker_api_shared.ttypes import DBStatus",
            "from codechecker_api.Authentication_v6 import \\",
            "    codeCheckerAuthentication as AuthAPI_v6",
            "from codechecker_api.Configuration_v6 import \\",
            "    configurationService as ConfigAPI_v6",
            "from codechecker_api.codeCheckerDBAccess_v6 import \\",
            "    codeCheckerDBAccess as ReportAPI_v6",
            "from codechecker_api.ProductManagement_v6 import \\",
            "    codeCheckerProductService as ProductAPI_v6",
            "from codechecker_api.ServerInfo_v6 import \\",
            "    serverInfoService as ServerInfoAPI_v6",
            "",
            "from codechecker_common import util",
            "from codechecker_common.logger import get_logger",
            "from codechecker_common.compatibility.multiprocessing import \\",
            "    Pool, cpu_count",
            "",
            "from codechecker_web.shared import database_status",
            "from codechecker_web.shared.version import get_version_str",
            "",
            "from . import instance_manager, permissions, routing, session_manager",
            "from .api.authentication import ThriftAuthHandler as AuthHandler_v6",
            "from .api.config_handler import ThriftConfigHandler as ConfigHandler_v6",
            "from .api.product_server import ThriftProductHandler as ProductHandler_v6",
            "from .api.report_server import ThriftRequestHandler as ReportHandler_v6",
            "from .api.server_info_handler import \\",
            "    ThriftServerInfoHandler as ServerInfoHandler_v6",
            "from .database import database, db_cleanup",
            "from .database.config_db_model import Product as ORMProduct, \\",
            "    Configuration as ORMConfiguration",
            "from .database.database import DBSession",
            "from .database.run_db_model import IDENTIFIER as RUN_META, Run, RunLock",
            "",
            "LOG = get_logger('server')",
            "",
            "",
            "class RequestHandler(SimpleHTTPRequestHandler):",
            "    \"\"\"",
            "    Handle thrift and browser requests",
            "    Simply modified and extended version of SimpleHTTPRequestHandler",
            "    \"\"\"",
            "    auth_session = None",
            "",
            "    def __init__(self, request, client_address, server):",
            "        self.path = None",
            "        super().__init__(request, client_address, server)",
            "",
            "    def log_message(self, *args):",
            "        \"\"\" Silencing http server. \"\"\"",
            "        return",
            "",
            "    def send_thrift_exception(self, error_msg, iprot, oprot, otrans):",
            "        \"\"\"",
            "        Send an exception response to the client in a proper format which can",
            "        be parsed by the Thrift clients expecting JSON responses.",
            "        \"\"\"",
            "        ex = TApplicationException(TApplicationException.INTERNAL_ERROR,",
            "                                   error_msg)",
            "        fname, _, seqid = iprot.readMessageBegin()",
            "        oprot.writeMessageBegin(fname, TMessageType.EXCEPTION, seqid)",
            "        ex.write(oprot)",
            "        oprot.writeMessageEnd()",
            "        oprot.trans.flush()",
            "        result = otrans.getvalue()",
            "        self.send_response(200)",
            "        self.send_header(\"content-type\", \"application/x-thrift\")",
            "        self.send_header(\"Content-Length\", len(result))",
            "        self.end_headers()",
            "        self.wfile.write(result)",
            "",
            "    def __check_session_cookie(self):",
            "        \"\"\"",
            "        Check the CodeChecker privileged access cookie in the request headers.",
            "",
            "        :returns: A session_manager._Session object if a correct, valid session",
            "        cookie was found in the headers. None, otherwise.",
            "        \"\"\"",
            "",
            "        if not self.server.manager.is_enabled:",
            "            return None",
            "",
            "        session = None",
            "        # Check if the user has presented a privileged access cookie.",
            "        cookies = self.headers.get(\"Cookie\")",
            "        if cookies:",
            "            split = cookies.split(\"; \")",
            "            for cookie in split:",
            "                values = cookie.split(\"=\")",
            "                if len(values) == 2 and \\",
            "                        values[0] == session_manager.SESSION_COOKIE_NAME:",
            "                    session = self.server.manager.get_session(values[1])",
            "",
            "        if session and session.is_alive:",
            "            # If a valid session token was found and it can still be used,",
            "            # mark that the user's last access to the server was the",
            "            # request that resulted in the execution of this function.",
            "            session.revalidate()",
            "            return session",
            "        else:",
            "            # If the user's access cookie is no longer usable (invalid),",
            "            # present an error.",
            "            client_host, client_port, is_ipv6 = \\",
            "                RequestHandler._get_client_host_port(self.client_address)",
            "            LOG.debug(",
            "                \"%s:%s Invalid access, credentials not found - \"",
            "                \"session refused\",",
            "                client_host if not is_ipv6 else '[' + client_host + ']',",
            "                str(client_port))",
            "            return None",
            "",
            "    def __handle_readiness(self):",
            "        \"\"\" Handle readiness probe. \"\"\"",
            "        try:",
            "            cfg_sess = self.server.config_session()",
            "            cfg_sess.query(ORMConfiguration).count()",
            "",
            "            self.send_response(200)",
            "            self.end_headers()",
            "            self.wfile.write(b'CODECHECKER_SERVER_IS_READY')",
            "        except Exception:",
            "            self.send_response(500)",
            "            self.end_headers()",
            "            self.wfile.write(b'CODECHECKER_SERVER_IS_NOT_READY')",
            "        finally:",
            "            if cfg_sess:",
            "                cfg_sess.close()",
            "                cfg_sess.commit()",
            "",
            "    def __handle_liveness(self):",
            "        \"\"\" Handle liveness probe. \"\"\"",
            "        self.send_response(200)",
            "        self.end_headers()",
            "        self.wfile.write(b'CODECHECKER_SERVER_IS_LIVE')",
            "",
            "    def end_headers(self):",
            "        # Sending the authentication cookie",
            "        # in every response if any.",
            "        # This will update the the session cookie",
            "        # on the clients to the newest.",
            "        if self.auth_session:",
            "            token = self.auth_session.token",
            "            if token:",
            "                self.send_header(",
            "                    \"Set-Cookie\",",
            "                    f\"{session_manager.SESSION_COOKIE_NAME}={token}; Path=/\")",
            "",
            "            # Set the current user name in the header.",
            "            user_name = self.auth_session.user",
            "            if user_name:",
            "                self.send_header(\"X-User\", user_name)",
            "",
            "        SimpleHTTPRequestHandler.end_headers(self)",
            "",
            "    @staticmethod",
            "    def _get_client_host_port(address):",
            "        \"\"\"",
            "        Returns the host and port of the request's address, and whether it",
            "        was an IPv6 address.",
            "        \"\"\"",
            "        if len(address) == 2:",
            "            return address[0], address[1], False",
            "        if len(address) == 4:",
            "            return address[0], address[1], True",
            "",
            "        raise IndexError(\"Invalid address tuple given.\")",
            "",
            "    def do_GET(self):",
            "        \"\"\" Handles the SPA browser access (GET requests).",
            "",
            "        It will do the following steps:",
            "         - for requests for index.html ('/'), just respond with the file.",
            "         - if the requested path contains a product endpoint name",
            "           ('/prod/app.js', '/prod/runs'), remove the endpoint from the path.",
            "         - if the requested path is a valid file (e.g: 'app.js'), respond with",
            "           the file.",
            "         - otherwise (e.g: 'runs') respond with index.html.",
            "        \"\"\"",
            "        client_host, client_port, is_ipv6 = \\",
            "            RequestHandler._get_client_host_port(self.client_address)",
            "        self.auth_session = self.__check_session_cookie()",
            "",
            "        username = self.auth_session.user if self.auth_session else 'Anonymous'",
            "        LOG.debug(\"%s:%s -- [%s] GET %s\",",
            "                  client_host if not is_ipv6 else '[' + client_host + ']',",
            "                  client_port, username, self.path)",
            "",
            "        if self.path == '/':",
            "            self.path = 'index.html'",
            "            SimpleHTTPRequestHandler.do_GET(self)",
            "            return",
            "",
            "        if self.path == '/live':",
            "            self.__handle_liveness()",
            "            return",
            "",
            "        if self.path == '/ready':",
            "            self.__handle_readiness()",
            "            return",
            "",
            "        product_endpoint, _ = routing.split_client_GET_request(self.path)",
            "",
            "        # Check that path contains a product endpoint.",
            "        if product_endpoint is not None and product_endpoint != '':",
            "            self.path = self.path.replace(f\"{product_endpoint}/\", \"\", 1)",
            "",
            "        if self.path == '/':",
            "            self.path = \"index.html\"",
            "",
            "        # Check that the given path is a file.",
            "        if not os.path.exists(self.translate_path(self.path)):",
            "            self.path = 'index.html'",
            "",
            "        SimpleHTTPRequestHandler.do_GET(self)",
            "",
            "    def __check_prod_db(self, product_endpoint):",
            "        \"\"\"",
            "        Check the product database status.",
            "        Try to reconnect in some cases.",
            "",
            "        Returns if everything is ok with the database or throw an exception",
            "        with the error message if something is wrong with the database.",
            "        \"\"\"",
            "",
            "        product = self.server.get_product(product_endpoint)",
            "        if not product:",
            "            raise ValueError(",
            "                f\"The product with the given endpoint '{product_endpoint}' \"",
            "                \"does not exist!\")",
            "",
            "        if product.db_status == DBStatus.OK:",
            "            # No reconnect needed.",
            "            return product",
            "",
            "        # Try to reconnect in these cases.",
            "        # Do not try to reconnect if there is a schema mismatch.",
            "        # If the product is not connected, try reconnecting...",
            "        if product.db_status in [DBStatus.FAILED_TO_CONNECT,",
            "                                 DBStatus.MISSING,",
            "                                 DBStatus.SCHEMA_INIT_ERROR]:",
            "            LOG.error(\"Request's product '%s' is not connected! \"",
            "                      \"Attempting reconnect...\", product.endpoint)",
            "            product.connect()",
            "            if product.db_status != DBStatus.OK:",
            "                # If the reconnection fails send an error to the user.",
            "                LOG.debug(\"Product reconnection failed.\")",
            "                error_msg = f\"'{product.endpoint}' database connection failed!\"",
            "                LOG.error(error_msg)",
            "                raise ValueError(error_msg)",
            "        else:",
            "            # Send an error to the user.",
            "            db_stat = DBStatus._VALUES_TO_NAMES.get(product.db_status)",
            "            error_msg = f\"'{product.endpoint}' database connection \" \\",
            "                f\"failed. DB status: {str(db_stat)}\"",
            "            LOG.error(error_msg)",
            "            raise ValueError(error_msg)",
            "",
            "        return product",
            "",
            "    # pylint: disable=invalid-name",
            "    def do_POST(self):",
            "        \"\"\"",
            "        Handles POST queries, which are usually Thrift messages.",
            "        \"\"\"",
            "        protocol_factory = TJSONProtocol.TJSONProtocolFactory()",
            "        input_protocol_factory = protocol_factory",
            "        output_protocol_factory = protocol_factory",
            "",
            "        # Get Thrift API function name to print to the log output.",
            "        itrans = TTransport.TFileObjectTransport(self.rfile)",
            "        itrans = TTransport.TBufferedTransport(itrans,",
            "                                               int(self.headers[",
            "                                                   'Content-Length']))",
            "        iprot = input_protocol_factory.getProtocol(itrans)",
            "        fname, _, _ = iprot.readMessageBegin()",
            "",
            "        client_host, client_port, is_ipv6 = \\",
            "            RequestHandler._get_client_host_port(self.client_address)",
            "        self.auth_session = self.__check_session_cookie()",
            "        auth_user = \\",
            "            self.auth_session.user if self.auth_session else \"Anonymous\"",
            "        host_info = client_host if not is_ipv6 else '[' + client_host + ']'",
            "        api_info = f\"{host_info}:{client_port} -- [{auth_user}] POST \" \\",
            "                   f\"{self.path}@{fname}\"",
            "        LOG.info(api_info)",
            "",
            "        # Create new thrift handler.",
            "        version = self.server.version",
            "",
            "        cstringio_buf = itrans.cstringio_buf.getvalue()",
            "        itrans = TTransport.TMemoryBuffer(cstringio_buf)",
            "        iprot = input_protocol_factory.getProtocol(itrans)",
            "",
            "        otrans = TTransport.TMemoryBuffer()",
            "        oprot = output_protocol_factory.getProtocol(otrans)",
            "",
            "        product_endpoint, api_ver, request_endpoint = \\",
            "            routing.split_client_POST_request(self.path)",
            "",
            "        if product_endpoint is None and api_ver is None and \\",
            "                request_endpoint is None:",
            "            self.send_thrift_exception(\"Invalid request endpoint path.\", iprot,",
            "                                       oprot, otrans)",
            "            return",
            "",
            "        # Only Authentication, Configuration, ServerInof",
            "        # endpoints are allowed for Anonymous users",
            "        # if authentication is required.",
            "        if self.server.manager.is_enabled and \\",
            "                request_endpoint not in \\",
            "                ['Authentication', 'Configuration', 'ServerInfo'] and \\",
            "                not self.auth_session:",
            "            # Bail out if the user is not authenticated...",
            "            # This response has the possibility of melting down Thrift clients,",
            "            # but the user is expected to properly authenticate first.",
            "            LOG.debug(\"%s:%s Invalid access, credentials not found \"",
            "                      \"- session refused.\",",
            "                      client_host if not is_ipv6 else '[' + client_host + ']',",
            "                      str(client_port))",
            "",
            "            self.send_thrift_exception(\"Error code 401: Unauthorized!\", iprot,",
            "                                       oprot, otrans)",
            "            return",
            "",
            "        # Authentication is handled, we may now respond to the user.",
            "        try:",
            "            product = None",
            "            if product_endpoint:",
            "                # The current request came through a product route, and not",
            "                # to the main endpoint.",
            "                product = self.__check_prod_db(product_endpoint)",
            "",
            "            version_supported = routing.is_supported_version(api_ver)",
            "            if version_supported:",
            "                major_version, _ = version_supported",
            "",
            "                if major_version == 6:",
            "                    if request_endpoint == 'Authentication':",
            "                        auth_handler = AuthHandler_v6(",
            "                            self.server.manager,",
            "                            self.auth_session,",
            "                            self.server.config_session)",
            "                        processor = AuthAPI_v6.Processor(auth_handler)",
            "                    elif request_endpoint == 'Configuration':",
            "                        conf_handler = ConfigHandler_v6(",
            "                            self.auth_session,",
            "                            self.server.config_session,",
            "                            self.server.manager)",
            "                        processor = ConfigAPI_v6.Processor(conf_handler)",
            "                    elif request_endpoint == 'ServerInfo':",
            "                        server_info_handler = ServerInfoHandler_v6(version)",
            "                        processor = ServerInfoAPI_v6.Processor(",
            "                            server_info_handler)",
            "                    elif request_endpoint == 'Products':",
            "                        prod_handler = ProductHandler_v6(",
            "                            self.server,",
            "                            self.auth_session,",
            "                            self.server.config_session,",
            "                            product,",
            "                            version)",
            "                        processor = ProductAPI_v6.Processor(prod_handler)",
            "                    elif request_endpoint == 'CodeCheckerService':",
            "                        # This endpoint is a product's report_server.",
            "                        if not product:",
            "                            error_msg = \\",
            "                                \"Requested CodeCheckerService on a \" \\",
            "                                f\"nonexistent product: '{product_endpoint}'.\"",
            "                            LOG.error(error_msg)",
            "                            raise ValueError(error_msg)",
            "",
            "                        if product_endpoint:",
            "                            # The current request came through a",
            "                            # product route, and not to the main endpoint.",
            "                            product = self.__check_prod_db(product_endpoint)",
            "",
            "                        acc_handler = ReportHandler_v6(",
            "                            self.server.manager,",
            "                            product.session_factory,",
            "                            product,",
            "                            self.auth_session,",
            "                            self.server.config_session,",
            "                            version,",
            "                            api_ver,",
            "                            self.server.context)",
            "                        processor = ReportAPI_v6.Processor(acc_handler)",
            "                    else:",
            "                        LOG.debug(\"This API endpoint does not exist.\")",
            "                        error_msg = f\"No API endpoint named '{self.path}'.\"",
            "                        raise ValueError(error_msg)",
            "                else:",
            "                    raise ValueError(",
            "                        f\"API version {major_version} not supported\")",
            "",
            "            else:",
            "                error_msg = \\",
            "                    \"The API version you are using is not supported \" \\",
            "                    \"by this server (server API version: \" \\",
            "                    f\"{get_version_str()})!\"",
            "                self.send_thrift_exception(error_msg, iprot, oprot, otrans)",
            "                return",
            "",
            "            processor.process(iprot, oprot)",
            "            result = otrans.getvalue()",
            "",
            "            self.send_response(200)",
            "            self.send_header(\"content-type\", \"application/x-thrift\")",
            "            self.send_header(\"Content-Length\", len(result))",
            "            self.end_headers()",
            "            self.wfile.write(result)",
            "            return",
            "",
            "        except BrokenPipeError as ex:",
            "            LOG.warning(\"%s failed with BrokenPipeError: %s\",",
            "                        api_info, str(ex))",
            "            import traceback",
            "            traceback.print_exc()",
            "        except Exception as ex:",
            "            LOG.warning(\"%s failed with Exception: %s\", api_info, str(ex))",
            "            import traceback",
            "            traceback.print_exc()",
            "",
            "            cstringio_buf = itrans.cstringio_buf.getvalue()",
            "            if cstringio_buf:",
            "                itrans = TTransport.TMemoryBuffer(cstringio_buf)",
            "                iprot = input_protocol_factory.getProtocol(itrans)",
            "",
            "            self.send_thrift_exception(str(ex), iprot, oprot, otrans)",
            "",
            "    def list_directory(self, path):",
            "        \"\"\" Disable directory listing. \"\"\"",
            "        self.send_error(405, \"No permission to list directory\")",
            "",
            "    def translate_path(self, path):",
            "        \"\"\"",
            "        Modified version from SimpleHTTPRequestHandler.",
            "        Path is set to www_root.",
            "        \"\"\"",
            "        # Abandon query parameters.",
            "        path = path.split('?', 1)[0]",
            "        path = path.split('#', 1)[0]",
            "        path = posixpath.normpath(urllib.parse.unquote(path))",
            "        words = path.split('/')",
            "        words = [_f for _f in words if _f]",
            "        path = self.server.www_root",
            "        for word in words:",
            "            _, word = os.path.splitdrive(word)",
            "            _, word = os.path.split(word)",
            "            if word in (os.curdir, os.pardir):",
            "                continue",
            "            path = os.path.join(path, word)",
            "        return path",
            "",
            "",
            "class Product:",
            "    \"\"\"",
            "    Represents a product, which is a distinct storage of analysis reports in",
            "    a separate database (and database connection) with its own access control.",
            "    \"\"\"",
            "",
            "    # The amount of SECONDS that need to pass after the last unsuccessful",
            "    # connect() call so the next could be made.",
            "    CONNECT_RETRY_TIMEOUT = 300",
            "",
            "    def __init__(self, id_: int, endpoint: str, display_name: str,",
            "                 connection_string: str, context, check_env):",
            "        \"\"\"",
            "        Set up a new managed product object for the configuration given.",
            "        \"\"\"",
            "        self.__id = id_",
            "        self.__endpoint = endpoint",
            "        self.__display_name = display_name",
            "        self.__connection_string = connection_string",
            "        self.__driver_name = None",
            "        self.__context = context",
            "        self.__check_env = check_env",
            "        self.__engine = None",
            "        self.__session = None",
            "        self.__db_status = DBStatus.MISSING",
            "",
            "        self.__last_connect_attempt = None",
            "",
            "    @property",
            "    def id(self):",
            "        return self.__id",
            "",
            "    @property",
            "    def endpoint(self):",
            "        \"\"\"",
            "        Returns the accessible URL endpoint of the product.",
            "        \"\"\"",
            "        return self.__endpoint",
            "",
            "    @property",
            "    def name(self):",
            "        \"\"\"",
            "        Returns the display name of the product.",
            "        \"\"\"",
            "        return self.__display_name",
            "",
            "    @property",
            "    def session_factory(self):",
            "        \"\"\"",
            "        Returns the session maker on this product's database engine which",
            "        can be used to initiate transactional connections.",
            "        \"\"\"",
            "        return self.__session",
            "",
            "    @property",
            "    def driver_name(self):",
            "        \"\"\"",
            "        Returns the name of the sql driver (sqlite, postgres).",
            "        \"\"\"",
            "        return self.__driver_name",
            "",
            "    @property",
            "    def db_status(self):",
            "        \"\"\"",
            "        Returns the status of the database which belongs to this product.",
            "        Call connect to update it.",
            "        \"\"\"",
            "        return self.__db_status",
            "",
            "    @property",
            "    def last_connection_failure(self):",
            "        \"\"\"",
            "        Returns the reason behind the last executed connection attempt's",
            "        failure.",
            "        \"\"\"",
            "        return self.__last_connect_attempt[1] if self.__last_connect_attempt \\",
            "            else None",
            "",
            "    def connect(self, init_db=False):",
            "        \"\"\"",
            "        Initiates the actual connection to the database configured for the",
            "        product.",
            "",
            "        Each time the connect is called the db_status is updated.",
            "        \"\"\"",
            "        LOG.debug(\"Checking '%s' database.\", self.endpoint)",
            "",
            "        sql_server = database.SQLServer.from_connection_string(",
            "            self.__connection_string,",
            "            self.__endpoint,",
            "            RUN_META,",
            "            self.__context.run_migration_root,",
            "            interactive=False,",
            "            env=self.__check_env)",
            "",
            "        if isinstance(sql_server, database.PostgreSQLServer):",
            "            self.__driver_name = 'postgresql'",
            "        elif isinstance(sql_server, database.SQLiteDatabase):",
            "            self.__driver_name = 'sqlite'",
            "",
            "        try:",
            "            LOG.debug(\"Trying to connect to the database\")",
            "",
            "            # Create the SQLAlchemy engine.",
            "            self.__engine = sql_server.create_engine()",
            "            LOG.debug(self.__engine)",
            "",
            "            self.__session = sessionmaker(bind=self.__engine)",
            "",
            "            self.__engine.execute('SELECT 1')",
            "            self.__db_status = sql_server.check_schema()",
            "            self.__last_connect_attempt = None",
            "",
            "            if self.__db_status == DBStatus.SCHEMA_MISSING and init_db:",
            "                LOG.debug(\"Initializing new database schema.\")",
            "                self.__db_status = sql_server.connect(init_db)",
            "",
            "        except Exception as ex:",
            "            LOG.exception(\"The database for product '%s' cannot be\"",
            "                          \" connected to.\", self.endpoint)",
            "            self.__db_status = DBStatus.FAILED_TO_CONNECT",
            "            self.__last_connect_attempt = (datetime.datetime.now(), str(ex))",
            "",
            "    def get_details(self):",
            "        \"\"\"",
            "        Get details for a product from the database.",
            "",
            "        It may throw different error messages depending on the used SQL driver",
            "        adapter in case of connection error.",
            "        \"\"\"",
            "        with DBSession(self.session_factory) as run_db_session:",
            "            run_locks = run_db_session.query(RunLock.name) \\",
            "                .filter(RunLock.locked_at.isnot(None)) \\",
            "                .all()",
            "",
            "            runs_in_progress = set(run_lock[0] for run_lock in run_locks)",
            "",
            "            num_of_runs = run_db_session.query(Run).count()",
            "",
            "            latest_store_to_product = \"\"",
            "            if num_of_runs:",
            "                last_updated_run = run_db_session.query(Run) \\",
            "                    .order_by(Run.date.desc()) \\",
            "                    .limit(1) \\",
            "                    .one_or_none()",
            "",
            "                latest_store_to_product = last_updated_run.date",
            "",
            "        return num_of_runs, runs_in_progress, latest_store_to_product",
            "",
            "    def teardown(self):",
            "        \"\"\"",
            "        Disposes the database connection to the product's backend.",
            "        \"\"\"",
            "        if self.__db_status == DBStatus.FAILED_TO_CONNECT:",
            "            return",
            "",
            "        self.__engine.dispose()",
            "",
            "        self.__session = None",
            "        self.__engine = None",
            "",
            "    def cleanup_run_db(self):",
            "        \"\"\"",
            "        Cleanup the run database which belongs to this product.",
            "        \"\"\"",
            "        LOG.info(\"[%s] Garbage collection started...\", self.endpoint)",
            "",
            "        db_cleanup.remove_expired_data(self)",
            "        db_cleanup.remove_unused_data(self)",
            "        db_cleanup.update_contextual_data(self, self.__context)",
            "",
            "        LOG.info(\"[%s] Garbage collection finished.\", self.endpoint)",
            "        return True",
            "",
            "",
            "def _do_db_cleanup(context, check_env,",
            "                   id_: int, endpoint: str, display_name: str,",
            "                   connection_str: str) -> Tuple[Optional[bool], str]:",
            "    # This functions is a concurrent job handler!",
            "    try:",
            "        prod = Product(id_, endpoint, display_name, connection_str,",
            "                       context, check_env)",
            "        prod.connect(init_db=False)",
            "        if prod.db_status != DBStatus.OK:",
            "            status_str = database_status.db_status_msg.get(prod.db_status)",
            "            return None, \\",
            "                f\"Cleanup not attempted, database status is \\\"{status_str}\\\"\"",
            "",
            "        prod.cleanup_run_db()",
            "        prod.teardown()",
            "",
            "        # Result is hard-wired to True, because the db_cleanup routines",
            "        # swallow and log the potential errors but do not return them.",
            "        return True, \"\"",
            "    except Exception as e:",
            "        import traceback",
            "        traceback.print_exc()",
            "        return False, str(e)",
            "",
            "",
            "def _do_db_cleanups(config_database, context, check_env) \\",
            "        -> Tuple[bool, List[Tuple[str, str]]]:",
            "    \"\"\"",
            "    Performs on-demand start-up database cleanup on all the products present",
            "    in the ``config_database``.",
            "",
            "    Returns whether database clean-up succeeded for all products, and the",
            "    list of products for which it failed, along with the failure reason.",
            "    \"\"\"",
            "    def _get_products() -> List[Product]:",
            "        products = []",
            "        cfg_engine = config_database.create_engine()",
            "        cfg_session_factory = sessionmaker(bind=cfg_engine)",
            "        with DBSession(cfg_session_factory) as cfg_db:",
            "            for row in cfg_db.query(ORMProduct) \\",
            "                    .order_by(ORMProduct.endpoint.asc()) \\",
            "                    .all():",
            "                products.append((row.id, row.endpoint, row.display_name,",
            "                                 row.connection))",
            "        cfg_engine.dispose()",
            "        return products",
            "",
            "    products = _get_products()",
            "    if not products:",
            "        return True, []",
            "",
            "    thr_count = util.clamp(1, len(products), cpu_count())",
            "    overall_result, failures = True, []",
            "    with Pool(max_workers=thr_count) as executor:",
            "        LOG.info(\"Performing database cleanup using %d concurrent jobs...\",",
            "                 thr_count)",
            "        for product, result in \\",
            "                zip(products, executor.map(",
            "                    partial(_do_db_cleanup, context, check_env),",
            "                    *zip(*products))):",
            "            success, reason = result",
            "            if not success:",
            "                _, endpoint, _, _ = product",
            "                overall_result = False",
            "                failures.append((endpoint, reason))",
            "",
            "    return overall_result, failures",
            "",
            "",
            "class CCSimpleHttpServer(HTTPServer):",
            "    \"\"\"",
            "    Simple http server to handle requests from the clients.",
            "    \"\"\"",
            "",
            "    daemon_threads = False",
            "    address_family = socket.AF_INET  # IPv4",
            "",
            "    def __init__(self,",
            "                 server_address,",
            "                 RequestHandlerClass,",
            "                 config_directory,",
            "                 product_db_sql_server,",
            "                 pckg_data,",
            "                 context,",
            "                 check_env,",
            "                 manager):",
            "",
            "        LOG.debug(\"Initializing HTTP server...\")",
            "",
            "        self.config_directory = config_directory",
            "        self.www_root = pckg_data['www_root']",
            "        self.doc_root = pckg_data['doc_root']",
            "        self.version = pckg_data['version']",
            "        self.context = context",
            "        self.check_env = check_env",
            "        self.manager = manager",
            "        self.__products = {}",
            "",
            "        # Create a database engine for the configuration database.",
            "        LOG.debug(\"Creating database engine for CONFIG DATABASE...\")",
            "        self.__engine = product_db_sql_server.create_engine()",
            "        self.config_session = sessionmaker(bind=self.__engine)",
            "        self.manager.set_database_connection(self.config_session)",
            "",
            "        # Load the initial list of products and set up the server.",
            "        cfg_sess = self.config_session()",
            "        permissions.initialise_defaults('SYSTEM', {",
            "            'config_db_session': cfg_sess",
            "        })",
            "        products = cfg_sess.query(ORMProduct).all()",
            "        for product in products:",
            "            self.add_product(product)",
            "            permissions.initialise_defaults('PRODUCT', {",
            "                'config_db_session': cfg_sess,",
            "                'productID': product.id",
            "            })",
            "        cfg_sess.commit()",
            "        cfg_sess.close()",
            "",
            "        try:",
            "            HTTPServer.__init__(self, server_address,",
            "                                RequestHandlerClass,",
            "                                bind_and_activate=True)",
            "            ssl_key_file = os.path.join(config_directory, \"key.pem\")",
            "            ssl_cert_file = os.path.join(config_directory, \"cert.pem\")",
            "",
            "            self.configure_keepalive()",
            "",
            "            if os.path.isfile(ssl_key_file) and os.path.isfile(ssl_cert_file):",
            "                LOG.info(\"Initiating SSL. Server listening on secure socket.\")",
            "                LOG.debug(\"Using cert file: %s\", ssl_cert_file)",
            "                LOG.debug(\"Using key file: %s\", ssl_key_file)",
            "                ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)",
            "                ssl_context.load_cert_chain(certfile=ssl_cert_file,",
            "                                            keyfile=ssl_key_file)",
            "                # FIXME introduce with python 3.7",
            "                # ssl_context.minimum_version = ssl.TLSVersion.TLSv1_2",
            "",
            "                # TLS1 and TLS1.1 were deprecated in RFC8996",
            "                # https://datatracker.ietf.org/doc/html/rfc8996",
            "                ssl_context.options |= (ssl.OP_NO_TLSv1 | ssl.OP_NO_TLSv1_1)",
            "                self.socket = ssl_context.wrap_socket(self.socket,",
            "                                                      server_side=True)",
            "",
            "            else:",
            "                LOG.info(\"Searching for SSL key at %s, cert at %s, \"",
            "                         \"not found...\", ssl_key_file, ssl_cert_file)",
            "                LOG.info(\"Falling back to simple, insecure HTTP.\")",
            "",
            "        except Exception as e:",
            "            LOG.error(\"Couldn't start the server: %s\", e.__str__())",
            "            raise",
            "",
            "    def configure_keepalive(self):",
            "        \"\"\"",
            "        Enable keepalive on the socket and some TCP keepalive configuration",
            "        option based on the server configuration file.",
            "        \"\"\"",
            "        if not self.manager.is_keepalive_enabled():",
            "            return",
            "",
            "        keepalive_is_on = self.socket.getsockopt(socket.SOL_SOCKET,",
            "                                                 socket.SO_KEEPALIVE)",
            "        if keepalive_is_on != 0:",
            "            LOG.debug('Socket keepalive already on.')",
            "        else:",
            "            LOG.debug('Socket keepalive off, turning on.')",
            "",
            "        ret = self.socket.setsockopt(socket.SOL_SOCKET,",
            "                                     socket.SO_KEEPALIVE, 1)",
            "        if ret:",
            "            LOG.error('Failed to set socket keepalive: %s', ret)",
            "",
            "        idle = self.manager.get_keepalive_idle()",
            "        if idle:",
            "            ret = self.socket.setsockopt(socket.IPPROTO_TCP,",
            "                                         socket.TCP_KEEPIDLE, idle)",
            "            if ret:",
            "                LOG.error('Failed to set TCP keepalive idle: %s', ret)",
            "",
            "        interval = self.manager.get_keepalive_interval()",
            "        if interval:",
            "            ret = self.socket.setsockopt(socket.IPPROTO_TCP,",
            "                                         socket.TCP_KEEPINTVL, interval)",
            "            if ret:",
            "                LOG.error('Failed to set TCP keepalive interval: %s', ret)",
            "",
            "        max_probe = self.manager.get_keepalive_max_probe()",
            "        if max_probe:",
            "            ret = self.socket.setsockopt(socket.IPPROTO_TCP,",
            "                                         socket.TCP_KEEPCNT, max_probe)",
            "            if ret:",
            "                LOG.error('Failed to set TCP max keepalive probe: %s', ret)",
            "",
            "    def terminate(self):",
            "        \"\"\"",
            "        Terminating the server.",
            "        \"\"\"",
            "        try:",
            "            self.server_close()",
            "            self.__engine.dispose()",
            "        except Exception as ex:",
            "            LOG.error(\"Failed to shut down the WEB server!\")",
            "            LOG.error(str(ex))",
            "            sys.exit(1)",
            "",
            "    def add_product(self, orm_product, init_db=False):",
            "        \"\"\"",
            "        Adds a product to the list of product databases connected to",
            "        by the server.",
            "        Checks the database connection for the product databases.",
            "        \"\"\"",
            "        if orm_product.endpoint in self.__products:",
            "            LOG.debug(\"This product is already configured!\")",
            "            return",
            "",
            "        LOG.debug(\"Setting up product '%s'\", orm_product.endpoint)",
            "",
            "        prod = Product(orm_product.id,",
            "                       orm_product.endpoint,",
            "                       orm_product.display_name,",
            "                       orm_product.connection,",
            "                       self.context,",
            "                       self.check_env)",
            "",
            "        # Update the product database status.",
            "        prod.connect()",
            "        if prod.db_status == DBStatus.SCHEMA_MISSING and init_db:",
            "            LOG.debug(\"Schema was missing in the database. Initializing new\")",
            "            prod.connect(init_db=True)",
            "",
            "        # The \"num_of_runs\" column of the config database is shown on the",
            "        # product page of the web interface. This is intentionally redundant",
            "        # with a simple query that would count the number of runs in a product:",
            "        # measurements have proven that this caching significantly improves",
            "        # responsibility.",
            "        # This field is incremented whenever a run is added to a product, and",
            "        # decreased when run(s) are removed. However, if these numbers ever",
            "        # diverge, the product page and the bottom right of the run page would",
            "        # display different run counts. To help on this, the num_of_runs column",
            "        # is updated at every server startup.",
            "        # FIXME: Pylint emits a false positive here, and states that",
            "        # session_factory() is not callable, because it initializes to None.",
            "        # More on this:",
            "        # https://github.com/Ericsson/codechecker/pull/3733#issuecomment-1235304179",
            "        # https://github.com/PyCQA/pylint/issues/6005",
            "        orm_product.num_of_runs = \\",
            "            prod.session_factory().query(func.count(Run.id)).one_or_none()[0] \\",
            "            # pylint: disable=not-callable",
            "",
            "        self.__products[prod.endpoint] = prod",
            "",
            "    @property",
            "    def num_products(self):",
            "        \"\"\"",
            "        Returns the number of products currently mounted by the server.",
            "        \"\"\"",
            "        return len(self.__products)",
            "",
            "    def get_product(self, endpoint):",
            "        \"\"\"",
            "        Get the product connection object for the given endpoint, or None.",
            "        \"\"\"",
            "        if endpoint in self.__products:",
            "            return self.__products.get(endpoint)",
            "",
            "        LOG.debug(\"Product with the given endpoint '%s' does not exist in \"",
            "                  \"the local cache. Try to get it from the database.\",",
            "                  endpoint)",
            "",
            "        # If the product doesn't find in the cache, try to get it from the",
            "        # database.",
            "        try:",
            "            cfg_sess = self.config_session()",
            "            product = cfg_sess.query(ORMProduct) \\",
            "                .filter(ORMProduct.endpoint == endpoint) \\",
            "                .limit(1).one_or_none()",
            "",
            "            if not product:",
            "                return None",
            "",
            "            self.add_product(product)",
            "            permissions.initialise_defaults('PRODUCT', {",
            "                'config_db_session': cfg_sess,",
            "                'productID': product.id",
            "            })",
            "",
            "            return self.__products.get(endpoint, None)",
            "        finally:",
            "            if cfg_sess:",
            "                cfg_sess.close()",
            "                cfg_sess.commit()",
            "",
            "    def get_only_product(self):",
            "        \"\"\"",
            "        Returns the Product object for the only product connected to by the",
            "        server, or None, if there are 0 or >= 2 products managed.",
            "        \"\"\"",
            "        return list(self.__products.items())[0][1] if self.num_products == 1 \\",
            "            else None",
            "",
            "    def remove_product(self, endpoint):",
            "        product = self.get_product(endpoint)",
            "        if not product:",
            "            raise ValueError(",
            "                f\"The product with the given endpoint '{endpoint}' does \"",
            "                \"not exist!\")",
            "",
            "        LOG.info(\"Disconnecting product '%s'\", endpoint)",
            "        product.teardown()",
            "",
            "        del self.__products[endpoint]",
            "",
            "    def remove_products_except(self, endpoints_to_keep):",
            "        \"\"\"",
            "        Removes EVERY product connection from the server except those",
            "        endpoints specified in :endpoints_to_keep.",
            "        \"\"\"",
            "        for ep in list(self.__products):",
            "            if ep not in endpoints_to_keep:",
            "                self.remove_product(ep)",
            "",
            "",
            "class CCSimpleHttpServerIPv6(CCSimpleHttpServer):",
            "    \"\"\"",
            "    CodeChecker HTTP simple server that listens over an IPv6 socket.",
            "    \"\"\"",
            "",
            "    address_family = socket.AF_INET6",
            "",
            "",
            "def start_server(config_directory, package_data, port, config_sql_server,",
            "                 listen_address, force_auth, skip_db_cleanup: bool,",
            "                 context, check_env):",
            "    \"\"\"",
            "    Start http server to handle web client and thrift requests.",
            "    \"\"\"",
            "    LOG.debug(\"Starting CodeChecker server...\")",
            "",
            "    server_addr = (listen_address, port)",
            "",
            "    # The root user file is DEPRECATED AND IGNORED",
            "    root_file = os.path.join(config_directory, 'root.user')",
            "    if os.path.exists(root_file):",
            "        LOG.warning(\"The 'root.user' file:  %s\"",
            "                    \" is deprecated and ignored. If you want to\"",
            "                    \" setup an initial user with SUPER_USER permission,\"",
            "                    \" configure the super_user field in the server_config.json\"",
            "                    \" as described in the documentation.\"",
            "                    \" To get rid off this warning,\"",
            "                    \" simply delete the root.user file.\",",
            "                    root_file)",
            "    # Check whether configuration file exists, create an example if not.",
            "    server_cfg_file = os.path.join(config_directory, 'server_config.json')",
            "    if not os.path.exists(server_cfg_file):",
            "        # For backward compatibility reason if the session_config.json file",
            "        # exists we rename it to server_config.json.",
            "        session_cfg_file = os.path.join(config_directory,",
            "                                        'session_config.json')",
            "        example_cfg_file = os.path.join(os.environ['CC_DATA_FILES_DIR'],",
            "                                        'config', 'server_config.json')",
            "        if os.path.exists(session_cfg_file):",
            "            LOG.info(\"Renaming '%s' to '%s'. Please check the example \"",
            "                     \"configuration file ('%s') or the user guide for more \"",
            "                     \"information.\", session_cfg_file,",
            "                     server_cfg_file, example_cfg_file)",
            "            os.rename(session_cfg_file, server_cfg_file)",
            "        else:",
            "            LOG.info(\"CodeChecker server's example configuration file \"",
            "                     \"created at '%s'\", server_cfg_file)",
            "            shutil.copyfile(example_cfg_file, server_cfg_file)",
            "",
            "    try:",
            "        manager = session_manager.SessionManager(",
            "            server_cfg_file,",
            "            force_auth)",
            "    except IOError as ioerr:",
            "        LOG.debug(ioerr)",
            "        LOG.error(\"The server's configuration file \"",
            "                  \"is missing or can not be read!\")",
            "        sys.exit(1)",
            "    except ValueError as verr:",
            "        LOG.debug(verr)",
            "        LOG.error(\"The server's configuration file is invalid!\")",
            "        sys.exit(1)",
            "",
            "    if not skip_db_cleanup:",
            "        all_success, fails = _do_db_cleanups(config_sql_server,",
            "                                             context,",
            "                                             check_env)",
            "        if not all_success:",
            "            LOG.error(\"Failed to perform automatic cleanup on %d products! \"",
            "                      \"Earlier logs might contain additional detailed \"",
            "                      \"reasoning.\\n\\t* %s\", len(fails),",
            "                      \"\\n\\t* \".join(",
            "                          (f\"'{ep}' ({reason})\" for (ep, reason) in fails)",
            "                      ))",
            "    else:",
            "        LOG.debug(\"Skipping db_cleanup, as requested.\")",
            "",
            "    server_clazz = CCSimpleHttpServer",
            "    if ':' in server_addr[0]:",
            "        # IPv6 address specified for listening.",
            "        # FIXME: Python>=3.8 automatically handles IPv6 if ':' is in the bind",
            "        # address, see https://bugs.python.org/issue24209.",
            "        server_clazz = CCSimpleHttpServerIPv6",
            "",
            "    http_server = server_clazz(server_addr,",
            "                               RequestHandler,",
            "                               config_directory,",
            "                               config_sql_server,",
            "                               package_data,",
            "                               context,",
            "                               check_env,",
            "                               manager)",
            "",
            "    # If the server was started with the port 0, the OS will pick an available",
            "    # port. For this reason we will update the port variable after server",
            "    # initialization.",
            "    port = http_server.socket.getsockname()[1]",
            "",
            "    processes = []",
            "",
            "    def signal_handler(signum, _):",
            "        \"\"\"",
            "        Handle SIGTERM to stop the server running.",
            "        \"\"\"",
            "        LOG.info(\"Shutting down the WEB server on [%s:%d]\",",
            "                 '[' + listen_address + ']'",
            "                 if server_clazz is CCSimpleHttpServerIPv6 else listen_address,",
            "                 port)",
            "        http_server.terminate()",
            "",
            "        # Terminate child processes.",
            "        for pp in processes:",
            "            pp.terminate()",
            "",
            "        sys.exit(128 + signum)",
            "",
            "    def reload_signal_handler(*_args, **_kwargs):",
            "        \"\"\"",
            "        Reloads server configuration file.",
            "        \"\"\"",
            "        manager.reload_config()",
            "",
            "    try:",
            "        instance_manager.register(os.getpid(),",
            "                                  os.path.abspath(",
            "                                      context.codechecker_workspace),",
            "                                  port)",
            "    except IOError as ex:",
            "        LOG.debug(ex.strerror)",
            "",
            "    LOG.info(\"Server waiting for client requests on [%s:%d]\",",
            "             '[' + listen_address + ']'",
            "             if server_clazz is CCSimpleHttpServerIPv6 else listen_address,",
            "             port)",
            "",
            "    def unregister_handler(pid):",
            "        \"\"\"",
            "        Handle errors during instance unregistration.",
            "        The workspace might be removed so updating the",
            "        config content might fail.",
            "        \"\"\"",
            "        try:",
            "            instance_manager.unregister(pid)",
            "        except IOError as ex:",
            "            LOG.debug(ex.strerror)",
            "",
            "    atexit.register(unregister_handler, os.getpid())",
            "",
            "    for _ in range(manager.worker_processes - 1):",
            "        p = multiprocess.Process(target=http_server.serve_forever)",
            "        processes.append(p)",
            "        p.start()",
            "",
            "    signal.signal(signal.SIGINT, signal_handler)",
            "    signal.signal(signal.SIGTERM, signal_handler)",
            "",
            "    if sys.platform != \"win32\":",
            "        signal.signal(signal.SIGHUP, reload_signal_handler)",
            "",
            "    # Main process also acts as a worker.",
            "    http_server.serve_forever()",
            "",
            "    LOG.info(\"Webserver quit.\")",
            "",
            "",
            "def add_initial_run_database(config_sql_server, product_connection):",
            "    \"\"\"",
            "    Create a default run database as SQLite in the config directory,",
            "    and add it to the list of products in the config database specified by",
            "    db_conn_string.",
            "    \"\"\"",
            "",
            "    # Connect to the configuration database",
            "    LOG.debug(\"Creating database engine for CONFIG DATABASE...\")",
            "    __engine = config_sql_server.create_engine()",
            "    product_session = sessionmaker(bind=__engine)",
            "",
            "    # Load the initial list of products and create the connections.",
            "    sess = product_session()",
            "    products = sess.query(ORMProduct).all()",
            "    if products:",
            "        raise ValueError(\"Called create_initial_run_database on non-empty \"",
            "                         \"config database -- you shouldn't have done this!\")",
            "",
            "    LOG.debug(\"Adding default product to the config db...\")",
            "    product = ORMProduct('Default', product_connection, 'Default',",
            "                         \"Default product created at server start.\")",
            "    sess.add(product)",
            "    sess.commit()",
            "    sess.close()",
            "",
            "    LOG.debug(\"Default product set up.\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "332": [
                "RequestHandler",
                "do_POST"
            ],
            "333": [
                "RequestHandler",
                "do_POST"
            ],
            "334": [
                "RequestHandler",
                "do_POST"
            ],
            "350": [
                "RequestHandler",
                "do_POST"
            ],
            "351": [
                "RequestHandler",
                "do_POST"
            ],
            "352": [
                "RequestHandler",
                "do_POST"
            ],
            "353": [
                "RequestHandler",
                "do_POST"
            ],
            "354": [
                "RequestHandler",
                "do_POST"
            ],
            "355": [
                "RequestHandler",
                "do_POST"
            ],
            "376": [
                "RequestHandler",
                "do_POST"
            ]
        },
        "addLocation": []
    },
    "web/server/tests/unit/test_request_routing.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "         # It is the server code's responsibility to give a 404 Not Found."
            },
            "1": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "         self.assertEqual(post(''), (None, None, None))"
            },
            "2": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "         self.assertEqual(post('CodeCheckerService'), (None, None, None))"
            },
            "3": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "4": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Raise an exception if URL is malformed, such as contains a"
            },
            "5": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # product-endpoint-like component which is badly encoded version"
            },
            "6": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # string."
            },
            "7": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        with self.assertRaises(Exception):"
            },
            "8": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            post('v6.0')"
            },
            "9": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            post('/v6/CodeCheckerService')"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+        self.assertEqual(post('v6.0'), (None, None, None))"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+        self.assertEqual(post('/v6.0/product/Authentication/Service'),"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+                         (None, None, None))"
            },
            "13": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 58,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "         self.assertEqual(post('/v6.0/Authentication'),"
            },
            "15": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "                          (None, '6.0', 'Authentication'))"
            },
            "16": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.assertEqual(post('/DummyProduct/v0.0/FoobarService'),"
            },
            "18": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                         ('DummyProduct', '0.0', 'FoobarService'))"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+        self.assertEqual(post('/DummyProduct/v6.0/FoobarService'),"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+                         ('DummyProduct', '6.0', 'FoobarService'))"
            }
        },
        "frontPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "",
            "\"\"\" Unit tests for the request_routing module. \"\"\"",
            "",
            "",
            "import unittest",
            "",
            "from codechecker_server.routing import split_client_GET_request",
            "from codechecker_server.routing import split_client_POST_request",
            "",
            "",
            "def get(path, host=\"http://localhost:8001/\"):",
            "    return split_client_GET_request(host + path.lstrip('/'))",
            "",
            "",
            "def post(path):",
            "    return split_client_POST_request(\"http://localhost:8001/\" +",
            "                                     path.lstrip('/'))",
            "",
            "",
            "class RequestRoutingTest(unittest.TestCase):",
            "    \"\"\"",
            "    Testing the router that understands client request queries.",
            "    \"\"\"",
            "",
            "    def test_get(self):",
            "        \"\"\"",
            "        Test if the server properly splits query addresses for GET.",
            "        \"\"\"",
            "",
            "        self.assertEqual(get(''), (None, ''))",
            "        self.assertEqual(get('/', '//'), (None, ''))",
            "        self.assertEqual(get('index.html'), (None, 'index.html'))",
            "        self.assertEqual(get('/images/logo.png'),",
            "                         (None, 'images/logo.png'))",
            "",
            "        self.assertEqual(get('Default'), ('Default', ''))",
            "        self.assertEqual(get('Default/index.html'), ('Default', 'index.html'))",
            "",
            "    def test_post(self):",
            "        \"\"\"",
            "        Test if the server properly splits query addresses for POST.",
            "        \"\"\"",
            "",
            "        # The splitter returns (None, None, None) as these are invalid paths.",
            "        # It is the server code's responsibility to give a 404 Not Found.",
            "        self.assertEqual(post(''), (None, None, None))",
            "        self.assertEqual(post('CodeCheckerService'), (None, None, None))",
            "",
            "        # Raise an exception if URL is malformed, such as contains a",
            "        # product-endpoint-like component which is badly encoded version",
            "        # string.",
            "        with self.assertRaises(Exception):",
            "            post('v6.0')",
            "            post('/v6/CodeCheckerService')",
            "",
            "        self.assertEqual(post('/v6.0/Authentication'),",
            "                         (None, '6.0', 'Authentication'))",
            "",
            "        self.assertEqual(post('/DummyProduct/v0.0/FoobarService'),",
            "                         ('DummyProduct', '0.0', 'FoobarService'))"
        ],
        "afterPatchFile": [
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "",
            "\"\"\" Unit tests for the request_routing module. \"\"\"",
            "",
            "",
            "import unittest",
            "",
            "from codechecker_server.routing import split_client_GET_request",
            "from codechecker_server.routing import split_client_POST_request",
            "",
            "",
            "def get(path, host=\"http://localhost:8001/\"):",
            "    return split_client_GET_request(host + path.lstrip('/'))",
            "",
            "",
            "def post(path):",
            "    return split_client_POST_request(\"http://localhost:8001/\" +",
            "                                     path.lstrip('/'))",
            "",
            "",
            "class RequestRoutingTest(unittest.TestCase):",
            "    \"\"\"",
            "    Testing the router that understands client request queries.",
            "    \"\"\"",
            "",
            "    def test_get(self):",
            "        \"\"\"",
            "        Test if the server properly splits query addresses for GET.",
            "        \"\"\"",
            "",
            "        self.assertEqual(get(''), (None, ''))",
            "        self.assertEqual(get('/', '//'), (None, ''))",
            "        self.assertEqual(get('index.html'), (None, 'index.html'))",
            "        self.assertEqual(get('/images/logo.png'),",
            "                         (None, 'images/logo.png'))",
            "",
            "        self.assertEqual(get('Default'), ('Default', ''))",
            "        self.assertEqual(get('Default/index.html'), ('Default', 'index.html'))",
            "",
            "    def test_post(self):",
            "        \"\"\"",
            "        Test if the server properly splits query addresses for POST.",
            "        \"\"\"",
            "",
            "        # The splitter returns (None, None, None) as these are invalid paths.",
            "        # It is the server code's responsibility to give a 404 Not Found.",
            "        self.assertEqual(post(''), (None, None, None))",
            "        self.assertEqual(post('CodeCheckerService'), (None, None, None))",
            "        self.assertEqual(post('v6.0'), (None, None, None))",
            "        self.assertEqual(post('/v6.0/product/Authentication/Service'),",
            "                         (None, None, None))",
            "",
            "        self.assertEqual(post('/v6.0/Authentication'),",
            "                         (None, '6.0', 'Authentication'))",
            "",
            "        self.assertEqual(post('/DummyProduct/v6.0/FoobarService'),",
            "                         ('DummyProduct', '6.0', 'FoobarService'))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "55": [
                "RequestRoutingTest",
                "test_post"
            ],
            "56": [
                "RequestRoutingTest",
                "test_post"
            ],
            "57": [
                "RequestRoutingTest",
                "test_post"
            ],
            "58": [
                "RequestRoutingTest",
                "test_post"
            ],
            "59": [
                "RequestRoutingTest",
                "test_post"
            ],
            "60": [
                "RequestRoutingTest",
                "test_post"
            ],
            "61": [
                "RequestRoutingTest",
                "test_post"
            ],
            "66": [
                "RequestRoutingTest",
                "test_post"
            ],
            "67": [
                "RequestRoutingTest",
                "test_post"
            ]
        },
        "addLocation": []
    },
    "web/tests/functional/products/test_products.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 150,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": 151,
                "PatchRowcode": "         # Now get the SERVERSPACE (configuration) for the product."
            },
            "2": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "         # TODO: These things usually should only work for superusers!"
            },
            "3": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        pr_conf = self._pr_client.getProductConfiguration(pr_data.id)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+        pr_conf = self._root_client.getProductConfiguration(pr_data.id)"
            },
            "5": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 154,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "         self.assertIsNotNone(pr_conf, \"Product configuration must come.\")"
            },
            "7": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "         self.assertEqual(pr_conf.endpoint, self.product_name,"
            },
            "8": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "         pr_client = env.setup_product_client("
            },
            "9": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "             self.test_workspace, product=self.product_name)"
            },
            "10": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "         product_id = pr_client.getCurrentProduct().id"
            },
            "11": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "13": {
                "beforePatchRowNumber": 193,
                "afterPatchRowNumber": 193,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 194,
                "afterPatchRowNumber": 194,
                "PatchRowcode": "         old_name = config.displayedName_b64"
            },
            "15": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": 195,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 202,
                "afterPatchRowNumber": 202,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "17": {
                "beforePatchRowNumber": 203,
                "afterPatchRowNumber": 203,
                "PatchRowcode": "                         \"Product edit didn't conclude.\")"
            },
            "18": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": 204,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "21": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "         self.assertEqual(config.endpoint, self.product_name,"
            },
            "22": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "                          \"The product edit changed the endpoint, when it \""
            },
            "23": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "                          \"shouldn't have!\")"
            },
            "24": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "25": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "                         \"Product config restore didn't conclude.\")"
            },
            "26": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 216,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 217,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "29": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "         self.assertEqual(config.displayedName_b64, old_name,"
            },
            "30": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "                          \"The product edit didn't change the name back.\")"
            },
            "31": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 220,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "33": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 226,
                "PatchRowcode": "                         \"Product edit didn't conclude.\")"
            },
            "34": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 227,
                "PatchRowcode": " "
            },
            "35": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 228,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "37": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "         self.assertEqual(config.confidentiality,"
            },
            "38": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "                          new_confidentiality,"
            },
            "39": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": 231,
                "PatchRowcode": "                          \"Couldn't change the confidentiality to OPEN\")"
            },
            "40": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "41": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": 236,
                "PatchRowcode": "                         \"Product edit didn't conclude.\")"
            },
            "42": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 237,
                "PatchRowcode": " "
            },
            "43": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 238,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "45": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "         self.assertEqual(config.confidentiality,"
            },
            "46": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 240,
                "PatchRowcode": "                          new_confidentiality,"
            },
            "47": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "                          \"Couldn't change the confidentiality to INTERNAL\")"
            },
            "48": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "49": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "                         \"Product edit didn't conclude.\")"
            },
            "50": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 247,
                "PatchRowcode": " "
            },
            "51": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "53": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 249,
                "PatchRowcode": "         self.assertEqual(config.confidentiality,"
            },
            "54": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 250,
                "PatchRowcode": "                          new_confidentiality,"
            },
            "55": {
                "beforePatchRowNumber": 251,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "                          \"Couldn't change the confidentiality to CONFIDENTIAL\")"
            },
            "56": {
                "beforePatchRowNumber": 255,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "57": {
                "beforePatchRowNumber": 256,
                "afterPatchRowNumber": 256,
                "PatchRowcode": "                         \"Product config restore didn't conclude.\")"
            },
            "58": {
                "beforePatchRowNumber": 257,
                "afterPatchRowNumber": 257,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": 258,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 258,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "61": {
                "beforePatchRowNumber": 259,
                "afterPatchRowNumber": 259,
                "PatchRowcode": "         self.assertEqual(config.confidentiality,"
            },
            "62": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": 260,
                "PatchRowcode": "                          old_confidentiality,"
            },
            "63": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 261,
                "PatchRowcode": "                          \"The edit didn't change back the confidentiality.\")"
            },
            "64": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 271,
                "PatchRowcode": "         pr_client = env.setup_product_client("
            },
            "65": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "             self.test_workspace, product=self.product_name)"
            },
            "66": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 273,
                "PatchRowcode": "         product_id = pr_client.getCurrentProduct().id"
            },
            "67": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 274,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "69": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 275,
                "PatchRowcode": " "
            },
            "70": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 276,
                "PatchRowcode": "         old_db_name = config.connection.database"
            },
            "71": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": 277,
                "PatchRowcode": " "
            },
            "72": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 292,
                "PatchRowcode": "                         \"Product edit didn't conclude.\")"
            },
            "73": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 293,
                "PatchRowcode": " "
            },
            "74": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": 294,
                "PatchRowcode": "         # Check if the configuration now uses the new values."
            },
            "75": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 295,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "77": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 296,
                "PatchRowcode": " "
            },
            "78": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 297,
                "PatchRowcode": "         self.assertEqual(config.connection.database, new_db_name,"
            },
            "79": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": 298,
                "PatchRowcode": "                          \"Server didn't save new database name.\")"
            },
            "80": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 311,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "81": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 312,
                "PatchRowcode": "                         \"Product configuration restore didn't conclude.\")"
            },
            "82": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 313,
                "PatchRowcode": " "
            },
            "83": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 314,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "85": {
                "beforePatchRowNumber": 315,
                "afterPatchRowNumber": 315,
                "PatchRowcode": "         self.assertEqual(config.connection.database, old_db_name,"
            },
            "86": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": 316,
                "PatchRowcode": "                          \"Server didn't save back to old database name.\")"
            },
            "87": {
                "beforePatchRowNumber": 317,
                "afterPatchRowNumber": 317,
                "PatchRowcode": " "
            },
            "88": {
                "beforePatchRowNumber": 336,
                "afterPatchRowNumber": 336,
                "PatchRowcode": "         pr_client = env.setup_product_client("
            },
            "89": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "             self.test_workspace, product=self.product_name)"
            },
            "90": {
                "beforePatchRowNumber": 338,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "         product_id = pr_client.getCurrentProduct().id"
            },
            "91": {
                "beforePatchRowNumber": 339,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 339,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "93": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": 340,
                "PatchRowcode": " "
            },
            "94": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "         old_endpoint = config.endpoint"
            },
            "95": {
                "beforePatchRowNumber": 342,
                "afterPatchRowNumber": 342,
                "PatchRowcode": "         new_endpoint = \"edited_endpoint\""
            },
            "96": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": 347,
                "PatchRowcode": "                         \"Product edit didn't conclude.\")"
            },
            "97": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": 348,
                "PatchRowcode": " "
            },
            "98": {
                "beforePatchRowNumber": 349,
                "afterPatchRowNumber": 349,
                "PatchRowcode": "         # Check if the configuration now uses the new values."
            },
            "99": {
                "beforePatchRowNumber": 350,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 350,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "101": {
                "beforePatchRowNumber": 351,
                "afterPatchRowNumber": 351,
                "PatchRowcode": "         self.assertEqual(config.endpoint, new_endpoint,"
            },
            "102": {
                "beforePatchRowNumber": 352,
                "afterPatchRowNumber": 352,
                "PatchRowcode": "                          \"Server didn't save new endpoint.\")"
            },
            "103": {
                "beforePatchRowNumber": 353,
                "afterPatchRowNumber": 353,
                "PatchRowcode": " "
            },
            "104": {
                "beforePatchRowNumber": 372,
                "afterPatchRowNumber": 372,
                "PatchRowcode": "         self.assertTrue(self._root_client.editProduct(product_id, config),"
            },
            "105": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 373,
                "PatchRowcode": "                         \"Product configuration restore didn't conclude.\")"
            },
            "106": {
                "beforePatchRowNumber": 374,
                "afterPatchRowNumber": 374,
                "PatchRowcode": " "
            },
            "107": {
                "beforePatchRowNumber": 375,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        config = self._pr_client.getProductConfiguration(product_id)"
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 375,
                "PatchRowcode": "+        config = self._root_client.getProductConfiguration(product_id)"
            },
            "109": {
                "beforePatchRowNumber": 376,
                "afterPatchRowNumber": 376,
                "PatchRowcode": "         self.assertEqual(config.endpoint, old_endpoint,"
            },
            "110": {
                "beforePatchRowNumber": 377,
                "afterPatchRowNumber": 377,
                "PatchRowcode": "                          \"Server didn't save back to old endpoint.\")"
            },
            "111": {
                "beforePatchRowNumber": 378,
                "afterPatchRowNumber": 378,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#",
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "",
            "\"\"\"",
            "Test product management related features.",
            "\"\"\"",
            "",
            "",
            "from copy import deepcopy",
            "import os",
            "import unittest",
            "",
            "from codechecker_api_shared.ttypes import RequestFailed",
            "from codechecker_api_shared.ttypes import DBStatus",
            "from codechecker_api.ProductManagement_v6.ttypes import ProductConfiguration",
            "from codechecker_api.ProductManagement_v6.ttypes import DatabaseConnection",
            "from codechecker_api.ProductManagement_v6.ttypes import Confidentiality",
            "",
            "from codechecker_web.shared import convert",
            "",
            "from libtest import env",
            "",
            "from . import setup_class_common, teardown_class_common",
            "",
            "",
            "class TestProducts(unittest.TestCase):",
            "",
            "    def setup_class(self):",
            "        setup_class_common(\"products\")",
            "",
            "    def teardown_class(self):",
            "        teardown_class_common()",
            "",
            "    def setup_method(self, _):",
            "",
            "        # TEST_WORKSPACE is automatically set by test package __init__.py .",
            "        self.test_workspace = os.environ['TEST_WORKSPACE']",
            "",
            "        test_class = self.__class__.__name__",
            "        print('Running ' + test_class + ' tests in ' + self.test_workspace)",
            "",
            "        # Get the test configuration from the prepared int the test workspace.",
            "        self.test_cfg = env.import_test_cfg(self.test_workspace)",
            "",
            "        self.product_name = self.test_cfg['codechecker_cfg']['viewer_product']",
            "",
            "        # Setup a viewer client to test viewer API calls.",
            "        self._cc_client = env.setup_viewer_client(self.test_workspace)",
            "        self.assertIsNotNone(self._cc_client)",
            "",
            "        # Setup an authentication client for creating sessions.",
            "        self._auth_client = env.setup_auth_client(self.test_workspace,",
            "                                                  session_token='_PROHIBIT')",
            "",
            "        # Create a SUPERUSER login.",
            "        root_token = self._auth_client.performLogin(\"Username:Password\",",
            "                                                    \"root:root\")",
            "",
            "        # Setup a product client to test product API calls.",
            "        self._pr_client = env.setup_product_client(self.test_workspace)",
            "        self.assertIsNotNone(self._pr_client)",
            "",
            "        # Setup a product client to test product API calls which requires root.",
            "        self._root_client = env.setup_product_client(self.test_workspace,",
            "                                                     session_token=root_token)",
            "        self.assertIsNotNone(self._pr_client)",
            "",
            "        # Get the run names which belong to this test.",
            "        run_names = env.get_run_names(self.test_workspace)",
            "",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        test_runs = [run for run in runs if run.name in run_names]",
            "",
            "        self.assertEqual(len(test_runs), 1,",
            "                         \"There should be only one run for this test.\")",
            "        self._runid = test_runs[0].runId",
            "",
            "    def test_add_invalid_product(self):",
            "        \"\"\"",
            "        Test the server prohibiting the addition of bogus product configs.",
            "        \"\"\"",
            "        error = convert.to_b64(\"bogus\")",
            "        product_cfg = ProductConfiguration(",
            "            displayedName_b64=error,",
            "            description_b64=error",
            "        )",
            "",
            "        # Test setting up product with valid endpoint but no database",
            "        # connection.",
            "        with self.assertRaises(RequestFailed):",
            "            cfg = deepcopy(product_cfg)",
            "            cfg.endpoint = \"valid\"",
            "            self._root_client.addProduct(cfg)",
            "",
            "        # Test some invalid strings based on pattern.",
            "        dbc = DatabaseConnection(",
            "            engine='sqlite',",
            "            host='',",
            "            port=0,",
            "            username_b64='',",
            "            password_b64='',",
            "            database=\"valid.sqlite\"",
            "        )",
            "        product_cfg.connection = dbc",
            "",
            "        with self.assertRaises(RequestFailed):",
            "            product_cfg.endpoint = \"$$$$$$$\"",
            "            self._root_client.addProduct(product_cfg)",
            "",
            "        # Test some forbidden URI parts.",
            "        with self.assertRaises(RequestFailed):",
            "            product_cfg.endpoint = \"index.html\"",
            "            self._root_client.addProduct(product_cfg)",
            "",
            "        with self.assertRaises(RequestFailed):",
            "            product_cfg.endpoint = \"CodeCheckerService\"",
            "            self._root_client.addProduct(product_cfg)",
            "",
            "    def test_get_product_data(self):",
            "        \"\"\"",
            "        Test getting product configuration from server.",
            "        \"\"\"",
            "",
            "        # First, test calling the API through a product endpoint and not the",
            "        # global endpoint. Also retrieve product ID this way.",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        self.assertIsNotNone(pr_client, \"Couldn't set up client\")",
            "",
            "        # This returns a USERSPACE product data.",
            "        pr_data = pr_client.getCurrentProduct()",
            "",
            "        self.assertIsNotNone(pr_data,",
            "                             \"Couldn't retrieve product data properly\")",
            "",
            "        self.assertEqual(pr_data.endpoint, self.product_name,",
            "                         \"The product's endpoint is improper.\")",
            "        self.assertTrue(pr_data.id > 0, \"Product didn't have valid ID\")",
            "",
            "        # The connected attribute of a product will be always True if",
            "        # database status is OK.",
            "        connected = pr_data.databaseStatus == DBStatus.OK",
            "        self.assertEqual(pr_data.connected, connected)",
            "",
            "        # Now get the SERVERSPACE (configuration) for the product.",
            "        # TODO: These things usually should only work for superusers!",
            "        pr_conf = self._pr_client.getProductConfiguration(pr_data.id)",
            "",
            "        self.assertIsNotNone(pr_conf, \"Product configuration must come.\")",
            "        self.assertEqual(pr_conf.endpoint, self.product_name,",
            "                         \"Product endpoint reproted by server must be the \"",
            "                         \"used endpoint... something doesn't make sense here!\")",
            "",
            "        self.assertIsNotNone(pr_conf.connection, \"Product configuration must \"",
            "                             \"send a database connection.\")",
            "",
            "        self.assertIsNone(pr_conf.connection.password_b64,",
            "                          \"!SECURITY LEAK! Server should NEVER send the \"",
            "                          \"product's database password out!\")",
            "",
            "        self.assertIn(self.product_name, pr_conf.connection.database,",
            "                      \"The product's database (name|file) should contain \"",
            "                      \"the product's endpoint -- in the test context.\")",
            "",
            "        name = convert.from_b64(pr_conf.displayedName_b64) \\",
            "            if pr_conf.displayedName_b64 else ''",
            "        self.assertEqual(name,",
            "                         # libtest/codechecker.py uses the workspace's name.",
            "                         os.path.basename(self.test_workspace),",
            "                         \"The displayed name must == the default value, as \"",
            "                         \"we didn't specify a custom displayed name.\")",
            "        confidentiality = pr_conf.confidentiality",
            "",
            "        self.assertEqual(confidentiality,",
            "                         Confidentiality.CONFIDENTIAL,",
            "                         \"Default Confidentiality was not Confidential\")",
            "",
            "    def test_editing(self):",
            "        \"\"\"",
            "        Test editing the product details (without reconnecting it).",
            "        \"\"\"",
            "",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        product_id = pr_client.getCurrentProduct().id",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "",
            "        old_name = config.displayedName_b64",
            "",
            "        new_name = convert.to_b64(\"edited product name\")",
            "        config.displayedName_b64 = new_name",
            "        with self.assertRaises(RequestFailed):",
            "            self._pr_client.editProduct(product_id, config)",
            "            print(\"Product was edited through non-superuser!\")",
            "",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.endpoint, self.product_name,",
            "                         \"The product edit changed the endpoint, when it \"",
            "                         \"shouldn't have!\")",
            "        self.assertEqual(config.displayedName_b64, new_name,",
            "                         \"The product edit didn't change the name.\")",
            "",
            "        # Restore the configuration of the product.",
            "        config.displayedName_b64 = old_name",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product config restore didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.displayedName_b64, old_name,",
            "                         \"The product edit didn't change the name back.\")",
            "",
            "        # Change confidentiality.",
            "        old_confidentiality = config.confidentiality",
            "        new_confidentiality = Confidentiality.OPEN",
            "        config.confidentiality = new_confidentiality",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         new_confidentiality,",
            "                         \"Couldn't change the confidentiality to OPEN\")",
            "",
            "        new_confidentiality = Confidentiality.INTERNAL",
            "        config.confidentiality = new_confidentiality",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         new_confidentiality,",
            "                         \"Couldn't change the confidentiality to INTERNAL\")",
            "",
            "        new_confidentiality = Confidentiality.CONFIDENTIAL",
            "        config.confidentiality = new_confidentiality",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         new_confidentiality,",
            "                         \"Couldn't change the confidentiality to CONFIDENTIAL\")",
            "",
            "        config.confidentiality = old_confidentiality",
            "",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product config restore didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         old_confidentiality,",
            "                         \"The edit didn't change back the confidentiality.\")",
            "",
            "    @unittest.skip(\"Enable this when local product caches is removed!\")",
            "    def test_editing_reconnect(self):",
            "        \"\"\"",
            "        Test if the product can successfully be set to connect to another db.",
            "",
            "        This requires a SUPERUSER.",
            "        \"\"\"",
            "",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        product_id = pr_client.getCurrentProduct().id",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "",
            "        old_db_name = config.connection.database",
            "",
            "        # Create a new database.",
            "        tenv = self.test_cfg['codechecker_cfg']['check_env']",
            "",
            "        if config.connection.engine == 'sqlite':",
            "            new_db_name = os.path.join(self.test_workspace, 'new.sqlite')",
            "        elif config.connection.engine == 'postgresql':",
            "            new_db_name = 'editeddb'",
            "            env.add_database(new_db_name, tenv)",
            "        else:",
            "            raise ValueError(\"I was not prepared to handle database mode \" +",
            "                             config.connection.engine)",
            "",
            "        config.connection.database = new_db_name",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        # Check if the configuration now uses the new values.",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "",
            "        self.assertEqual(config.connection.database, new_db_name,",
            "                         \"Server didn't save new database name.\")",
            "        self.assertEqual(config.endpoint, self.product_name,",
            "                         \"The endpoint was changed -- perhaps the \"",
            "                         \"temporary connection leaked into the database?\")",
            "",
            "        # There is no schema initialization if the product database",
            "        # was changed. The inital schema needs to be created manually",
            "        # for the new database.",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertIsNone(runs)",
            "",
            "        # Connect back to the old database.",
            "        config.connection.database = old_db_name",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product configuration restore didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.connection.database, old_db_name,",
            "                         \"Server didn't save back to old database name.\")",
            "",
            "        # The old database should have its data available again.",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertEqual(",
            "            len(runs), 1,",
            "            \"We connected to old database but the run was missing.\")",
            "",
            "        # Drop the temporary database. SQLite file will be removed with",
            "        # the test workspace.",
            "        if config.connection.engine == 'postgresql':",
            "            env.del_database(new_db_name, tenv)",
            "",
            "    @unittest.skip(\"Enable this when local product caches is removed!\")",
            "    def test_editing_endpoint(self):",
            "        \"\"\"",
            "        Test if the product can successfully change its endpoint and keep",
            "        the data.",
            "        \"\"\"",
            "",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        product_id = pr_client.getCurrentProduct().id",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "",
            "        old_endpoint = config.endpoint",
            "        new_endpoint = \"edited_endpoint\"",
            "",
            "        # Save a new endpoint.",
            "        config.endpoint = new_endpoint",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        # Check if the configuration now uses the new values.",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.endpoint, new_endpoint,",
            "                         \"Server didn't save new endpoint.\")",
            "",
            "        # The old product is gone. Thus, connection should NOT happen.",
            "        res = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertIsNone(res)",
            "",
            "        # The new product should connect and have the data.",
            "        codechecker_cfg = self.test_cfg['codechecker_cfg']",
            "        token = self._auth_client.performLogin(\"Username:Password\", \"cc:test\")",
            "        new_client = env.get_viewer_client(",
            "            host=codechecker_cfg['viewer_host'],",
            "            port=codechecker_cfg['viewer_port'],",
            "            product=new_endpoint,  # Use the new product URL.",
            "            endpoint='/CodeCheckerService',",
            "            session_token=token)",
            "        self.assertEqual(len(new_client.getRunData(None, None, 0, None)), 1,",
            "                         \"The new product did not serve the stored data.\")",
            "",
            "        # Set back to the old endpoint.",
            "        config.endpoint = old_endpoint",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product configuration restore didn't conclude.\")",
            "",
            "        config = self._pr_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.endpoint, old_endpoint,",
            "                         \"Server didn't save back to old endpoint.\")",
            "",
            "        # The old product should have its data available again.",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertEqual(",
            "            len(runs), 1,",
            "            \"We connected to old database but the run was missing.\")"
        ],
        "afterPatchFile": [
            "#",
            "# -------------------------------------------------------------------------",
            "#",
            "#  Part of the CodeChecker project, under the Apache License v2.0 with",
            "#  LLVM Exceptions. See LICENSE for license information.",
            "#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception",
            "#",
            "# -------------------------------------------------------------------------",
            "",
            "\"\"\"",
            "Test product management related features.",
            "\"\"\"",
            "",
            "",
            "from copy import deepcopy",
            "import os",
            "import unittest",
            "",
            "from codechecker_api_shared.ttypes import RequestFailed",
            "from codechecker_api_shared.ttypes import DBStatus",
            "from codechecker_api.ProductManagement_v6.ttypes import ProductConfiguration",
            "from codechecker_api.ProductManagement_v6.ttypes import DatabaseConnection",
            "from codechecker_api.ProductManagement_v6.ttypes import Confidentiality",
            "",
            "from codechecker_web.shared import convert",
            "",
            "from libtest import env",
            "",
            "from . import setup_class_common, teardown_class_common",
            "",
            "",
            "class TestProducts(unittest.TestCase):",
            "",
            "    def setup_class(self):",
            "        setup_class_common(\"products\")",
            "",
            "    def teardown_class(self):",
            "        teardown_class_common()",
            "",
            "    def setup_method(self, _):",
            "",
            "        # TEST_WORKSPACE is automatically set by test package __init__.py .",
            "        self.test_workspace = os.environ['TEST_WORKSPACE']",
            "",
            "        test_class = self.__class__.__name__",
            "        print('Running ' + test_class + ' tests in ' + self.test_workspace)",
            "",
            "        # Get the test configuration from the prepared int the test workspace.",
            "        self.test_cfg = env.import_test_cfg(self.test_workspace)",
            "",
            "        self.product_name = self.test_cfg['codechecker_cfg']['viewer_product']",
            "",
            "        # Setup a viewer client to test viewer API calls.",
            "        self._cc_client = env.setup_viewer_client(self.test_workspace)",
            "        self.assertIsNotNone(self._cc_client)",
            "",
            "        # Setup an authentication client for creating sessions.",
            "        self._auth_client = env.setup_auth_client(self.test_workspace,",
            "                                                  session_token='_PROHIBIT')",
            "",
            "        # Create a SUPERUSER login.",
            "        root_token = self._auth_client.performLogin(\"Username:Password\",",
            "                                                    \"root:root\")",
            "",
            "        # Setup a product client to test product API calls.",
            "        self._pr_client = env.setup_product_client(self.test_workspace)",
            "        self.assertIsNotNone(self._pr_client)",
            "",
            "        # Setup a product client to test product API calls which requires root.",
            "        self._root_client = env.setup_product_client(self.test_workspace,",
            "                                                     session_token=root_token)",
            "        self.assertIsNotNone(self._pr_client)",
            "",
            "        # Get the run names which belong to this test.",
            "        run_names = env.get_run_names(self.test_workspace)",
            "",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        test_runs = [run for run in runs if run.name in run_names]",
            "",
            "        self.assertEqual(len(test_runs), 1,",
            "                         \"There should be only one run for this test.\")",
            "        self._runid = test_runs[0].runId",
            "",
            "    def test_add_invalid_product(self):",
            "        \"\"\"",
            "        Test the server prohibiting the addition of bogus product configs.",
            "        \"\"\"",
            "        error = convert.to_b64(\"bogus\")",
            "        product_cfg = ProductConfiguration(",
            "            displayedName_b64=error,",
            "            description_b64=error",
            "        )",
            "",
            "        # Test setting up product with valid endpoint but no database",
            "        # connection.",
            "        with self.assertRaises(RequestFailed):",
            "            cfg = deepcopy(product_cfg)",
            "            cfg.endpoint = \"valid\"",
            "            self._root_client.addProduct(cfg)",
            "",
            "        # Test some invalid strings based on pattern.",
            "        dbc = DatabaseConnection(",
            "            engine='sqlite',",
            "            host='',",
            "            port=0,",
            "            username_b64='',",
            "            password_b64='',",
            "            database=\"valid.sqlite\"",
            "        )",
            "        product_cfg.connection = dbc",
            "",
            "        with self.assertRaises(RequestFailed):",
            "            product_cfg.endpoint = \"$$$$$$$\"",
            "            self._root_client.addProduct(product_cfg)",
            "",
            "        # Test some forbidden URI parts.",
            "        with self.assertRaises(RequestFailed):",
            "            product_cfg.endpoint = \"index.html\"",
            "            self._root_client.addProduct(product_cfg)",
            "",
            "        with self.assertRaises(RequestFailed):",
            "            product_cfg.endpoint = \"CodeCheckerService\"",
            "            self._root_client.addProduct(product_cfg)",
            "",
            "    def test_get_product_data(self):",
            "        \"\"\"",
            "        Test getting product configuration from server.",
            "        \"\"\"",
            "",
            "        # First, test calling the API through a product endpoint and not the",
            "        # global endpoint. Also retrieve product ID this way.",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        self.assertIsNotNone(pr_client, \"Couldn't set up client\")",
            "",
            "        # This returns a USERSPACE product data.",
            "        pr_data = pr_client.getCurrentProduct()",
            "",
            "        self.assertIsNotNone(pr_data,",
            "                             \"Couldn't retrieve product data properly\")",
            "",
            "        self.assertEqual(pr_data.endpoint, self.product_name,",
            "                         \"The product's endpoint is improper.\")",
            "        self.assertTrue(pr_data.id > 0, \"Product didn't have valid ID\")",
            "",
            "        # The connected attribute of a product will be always True if",
            "        # database status is OK.",
            "        connected = pr_data.databaseStatus == DBStatus.OK",
            "        self.assertEqual(pr_data.connected, connected)",
            "",
            "        # Now get the SERVERSPACE (configuration) for the product.",
            "        # TODO: These things usually should only work for superusers!",
            "        pr_conf = self._root_client.getProductConfiguration(pr_data.id)",
            "",
            "        self.assertIsNotNone(pr_conf, \"Product configuration must come.\")",
            "        self.assertEqual(pr_conf.endpoint, self.product_name,",
            "                         \"Product endpoint reproted by server must be the \"",
            "                         \"used endpoint... something doesn't make sense here!\")",
            "",
            "        self.assertIsNotNone(pr_conf.connection, \"Product configuration must \"",
            "                             \"send a database connection.\")",
            "",
            "        self.assertIsNone(pr_conf.connection.password_b64,",
            "                          \"!SECURITY LEAK! Server should NEVER send the \"",
            "                          \"product's database password out!\")",
            "",
            "        self.assertIn(self.product_name, pr_conf.connection.database,",
            "                      \"The product's database (name|file) should contain \"",
            "                      \"the product's endpoint -- in the test context.\")",
            "",
            "        name = convert.from_b64(pr_conf.displayedName_b64) \\",
            "            if pr_conf.displayedName_b64 else ''",
            "        self.assertEqual(name,",
            "                         # libtest/codechecker.py uses the workspace's name.",
            "                         os.path.basename(self.test_workspace),",
            "                         \"The displayed name must == the default value, as \"",
            "                         \"we didn't specify a custom displayed name.\")",
            "        confidentiality = pr_conf.confidentiality",
            "",
            "        self.assertEqual(confidentiality,",
            "                         Confidentiality.CONFIDENTIAL,",
            "                         \"Default Confidentiality was not Confidential\")",
            "",
            "    def test_editing(self):",
            "        \"\"\"",
            "        Test editing the product details (without reconnecting it).",
            "        \"\"\"",
            "",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        product_id = pr_client.getCurrentProduct().id",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "",
            "        old_name = config.displayedName_b64",
            "",
            "        new_name = convert.to_b64(\"edited product name\")",
            "        config.displayedName_b64 = new_name",
            "        with self.assertRaises(RequestFailed):",
            "            self._pr_client.editProduct(product_id, config)",
            "            print(\"Product was edited through non-superuser!\")",
            "",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.endpoint, self.product_name,",
            "                         \"The product edit changed the endpoint, when it \"",
            "                         \"shouldn't have!\")",
            "        self.assertEqual(config.displayedName_b64, new_name,",
            "                         \"The product edit didn't change the name.\")",
            "",
            "        # Restore the configuration of the product.",
            "        config.displayedName_b64 = old_name",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product config restore didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.displayedName_b64, old_name,",
            "                         \"The product edit didn't change the name back.\")",
            "",
            "        # Change confidentiality.",
            "        old_confidentiality = config.confidentiality",
            "        new_confidentiality = Confidentiality.OPEN",
            "        config.confidentiality = new_confidentiality",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         new_confidentiality,",
            "                         \"Couldn't change the confidentiality to OPEN\")",
            "",
            "        new_confidentiality = Confidentiality.INTERNAL",
            "        config.confidentiality = new_confidentiality",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         new_confidentiality,",
            "                         \"Couldn't change the confidentiality to INTERNAL\")",
            "",
            "        new_confidentiality = Confidentiality.CONFIDENTIAL",
            "        config.confidentiality = new_confidentiality",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         new_confidentiality,",
            "                         \"Couldn't change the confidentiality to CONFIDENTIAL\")",
            "",
            "        config.confidentiality = old_confidentiality",
            "",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product config restore didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.confidentiality,",
            "                         old_confidentiality,",
            "                         \"The edit didn't change back the confidentiality.\")",
            "",
            "    @unittest.skip(\"Enable this when local product caches is removed!\")",
            "    def test_editing_reconnect(self):",
            "        \"\"\"",
            "        Test if the product can successfully be set to connect to another db.",
            "",
            "        This requires a SUPERUSER.",
            "        \"\"\"",
            "",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        product_id = pr_client.getCurrentProduct().id",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "",
            "        old_db_name = config.connection.database",
            "",
            "        # Create a new database.",
            "        tenv = self.test_cfg['codechecker_cfg']['check_env']",
            "",
            "        if config.connection.engine == 'sqlite':",
            "            new_db_name = os.path.join(self.test_workspace, 'new.sqlite')",
            "        elif config.connection.engine == 'postgresql':",
            "            new_db_name = 'editeddb'",
            "            env.add_database(new_db_name, tenv)",
            "        else:",
            "            raise ValueError(\"I was not prepared to handle database mode \" +",
            "                             config.connection.engine)",
            "",
            "        config.connection.database = new_db_name",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        # Check if the configuration now uses the new values.",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "",
            "        self.assertEqual(config.connection.database, new_db_name,",
            "                         \"Server didn't save new database name.\")",
            "        self.assertEqual(config.endpoint, self.product_name,",
            "                         \"The endpoint was changed -- perhaps the \"",
            "                         \"temporary connection leaked into the database?\")",
            "",
            "        # There is no schema initialization if the product database",
            "        # was changed. The inital schema needs to be created manually",
            "        # for the new database.",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertIsNone(runs)",
            "",
            "        # Connect back to the old database.",
            "        config.connection.database = old_db_name",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product configuration restore didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.connection.database, old_db_name,",
            "                         \"Server didn't save back to old database name.\")",
            "",
            "        # The old database should have its data available again.",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertEqual(",
            "            len(runs), 1,",
            "            \"We connected to old database but the run was missing.\")",
            "",
            "        # Drop the temporary database. SQLite file will be removed with",
            "        # the test workspace.",
            "        if config.connection.engine == 'postgresql':",
            "            env.del_database(new_db_name, tenv)",
            "",
            "    @unittest.skip(\"Enable this when local product caches is removed!\")",
            "    def test_editing_endpoint(self):",
            "        \"\"\"",
            "        Test if the product can successfully change its endpoint and keep",
            "        the data.",
            "        \"\"\"",
            "",
            "        pr_client = env.setup_product_client(",
            "            self.test_workspace, product=self.product_name)",
            "        product_id = pr_client.getCurrentProduct().id",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "",
            "        old_endpoint = config.endpoint",
            "        new_endpoint = \"edited_endpoint\"",
            "",
            "        # Save a new endpoint.",
            "        config.endpoint = new_endpoint",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product edit didn't conclude.\")",
            "",
            "        # Check if the configuration now uses the new values.",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.endpoint, new_endpoint,",
            "                         \"Server didn't save new endpoint.\")",
            "",
            "        # The old product is gone. Thus, connection should NOT happen.",
            "        res = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertIsNone(res)",
            "",
            "        # The new product should connect and have the data.",
            "        codechecker_cfg = self.test_cfg['codechecker_cfg']",
            "        token = self._auth_client.performLogin(\"Username:Password\", \"cc:test\")",
            "        new_client = env.get_viewer_client(",
            "            host=codechecker_cfg['viewer_host'],",
            "            port=codechecker_cfg['viewer_port'],",
            "            product=new_endpoint,  # Use the new product URL.",
            "            endpoint='/CodeCheckerService',",
            "            session_token=token)",
            "        self.assertEqual(len(new_client.getRunData(None, None, 0, None)), 1,",
            "                         \"The new product did not serve the stored data.\")",
            "",
            "        # Set back to the old endpoint.",
            "        config.endpoint = old_endpoint",
            "        self.assertTrue(self._root_client.editProduct(product_id, config),",
            "                        \"Product configuration restore didn't conclude.\")",
            "",
            "        config = self._root_client.getProductConfiguration(product_id)",
            "        self.assertEqual(config.endpoint, old_endpoint,",
            "                         \"Server didn't save back to old endpoint.\")",
            "",
            "        # The old product should have its data available again.",
            "        runs = self._cc_client.getRunData(None, None, 0, None)",
            "        self.assertEqual(",
            "            len(runs), 1,",
            "            \"We connected to old database but the run was missing.\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "153": [
                "TestProducts",
                "test_get_product_data"
            ],
            "192": [
                "TestProducts",
                "test_editing"
            ],
            "205": [
                "TestProducts",
                "test_editing"
            ],
            "217": [
                "TestProducts",
                "test_editing"
            ],
            "228": [
                "TestProducts",
                "test_editing"
            ],
            "238": [
                "TestProducts",
                "test_editing"
            ],
            "248": [
                "TestProducts",
                "test_editing"
            ],
            "258": [
                "TestProducts",
                "test_editing"
            ],
            "274": [
                "TestProducts",
                "test_editing_reconnect"
            ],
            "295": [
                "TestProducts",
                "test_editing_reconnect"
            ],
            "314": [
                "TestProducts",
                "test_editing_reconnect"
            ],
            "339": [
                "TestProducts",
                "test_editing_endpoint"
            ],
            "350": [
                "TestProducts",
                "test_editing_endpoint"
            ],
            "375": [
                "TestProducts",
                "test_editing_endpoint"
            ]
        },
        "addLocation": []
    }
}