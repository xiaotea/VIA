{
    "vyper/builtins/functions.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 781,
                "afterPatchRowNumber": 781,
                "PatchRowcode": "                 [\"mstore\", add_ofst(input_buf, 32), args[1]],"
            },
            "1": {
                "beforePatchRowNumber": 782,
                "afterPatchRowNumber": 782,
                "PatchRowcode": "                 [\"mstore\", add_ofst(input_buf, 64), args[2]],"
            },
            "2": {
                "beforePatchRowNumber": 783,
                "afterPatchRowNumber": 783,
                "PatchRowcode": "                 [\"mstore\", add_ofst(input_buf, 96), args[3]],"
            },
            "3": {
                "beforePatchRowNumber": 784,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                [\"staticcall\", \"gas\", 1, input_buf, 128, output_buf, 32],"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 784,
                "PatchRowcode": "+                [\"assert\", [\"staticcall\", \"gas\", 1, input_buf, 128, output_buf, 32]],"
            },
            "5": {
                "beforePatchRowNumber": 785,
                "afterPatchRowNumber": 785,
                "PatchRowcode": "                 [\"mload\", output_buf],"
            },
            "6": {
                "beforePatchRowNumber": 786,
                "afterPatchRowNumber": 786,
                "PatchRowcode": "             ],"
            },
            "7": {
                "beforePatchRowNumber": 787,
                "afterPatchRowNumber": 787,
                "PatchRowcode": "             typ=AddressT(),"
            }
        },
        "frontPatchFile": [
            "import hashlib",
            "import math",
            "import operator",
            "",
            "from vyper import ast as vy_ast",
            "from vyper.abi_types import ABI_Tuple",
            "from vyper.ast.validation import validate_call_args",
            "from vyper.codegen.abi_encoder import abi_encode",
            "from vyper.codegen.context import Context, VariableRecord",
            "from vyper.codegen.core import (",
            "    LOAD,",
            "    STORE,",
            "    IRnode,",
            "    add_ofst,",
            "    bytes_data_ptr,",
            "    calculate_type_for_external_return,",
            "    check_buffer_overflow_ir,",
            "    check_external_call,",
            "    clamp,",
            "    clamp2,",
            "    clamp_basetype,",
            "    clamp_nonzero,",
            "    copy_bytes,",
            "    dummy_node_for_type,",
            "    ensure_eval_once,",
            "    ensure_in_memory,",
            "    eval_seq,",
            "    get_bytearray_length,",
            "    get_type_for_exact_size,",
            "    ir_tuple_from_args,",
            "    make_setter,",
            "    potential_overlap,",
            "    promote_signed_int,",
            "    sar,",
            "    shl,",
            "    shr,",
            "    unwrap_location,",
            ")",
            "from vyper.codegen.expr import Expr",
            "from vyper.codegen.ir_node import Encoding, scope_multi",
            "from vyper.codegen.keccak256_helper import keccak256_helper",
            "from vyper.evm.address_space import MEMORY",
            "from vyper.evm.opcodes import version_check",
            "from vyper.exceptions import (",
            "    ArgumentException,",
            "    CompilerPanic,",
            "    EvmVersionException,",
            "    InvalidLiteral,",
            "    InvalidType,",
            "    StateAccessViolation,",
            "    StructureException,",
            "    TypeMismatch,",
            "    UnfoldableNode,",
            "    ZeroDivisionException,",
            ")",
            "from vyper.semantics.analysis.base import Modifiability, VarInfo",
            "from vyper.semantics.analysis.utils import (",
            "    get_common_types,",
            "    get_exact_type_from_node,",
            "    get_possible_types_from_node,",
            "    validate_expected_type,",
            ")",
            "from vyper.semantics.types import (",
            "    TYPE_T,",
            "    AddressT,",
            "    BoolT,",
            "    BytesM_T,",
            "    BytesT,",
            "    DArrayT,",
            "    DecimalT,",
            "    HashMapT,",
            "    IntegerT,",
            "    KwargSettings,",
            "    SArrayT,",
            "    StringT,",
            "    TupleT,",
            ")",
            "from vyper.semantics.types.bytestrings import _BytestringT",
            "from vyper.semantics.types.shortcuts import (",
            "    BYTES4_T,",
            "    BYTES32_T,",
            "    INT128_T,",
            "    INT256_T,",
            "    UINT8_T,",
            "    UINT256_T,",
            ")",
            "from vyper.semantics.types.utils import type_from_annotation",
            "from vyper.utils import (",
            "    DECIMAL_DIVISOR,",
            "    EIP_170_LIMIT,",
            "    SHA3_PER_WORD,",
            "    MemoryPositions,",
            "    bytes_to_int,",
            "    ceil32,",
            "    fourbytes_to_int,",
            "    keccak256,",
            "    method_id,",
            "    method_id_int,",
            "    vyper_warn,",
            ")",
            "",
            "from ._convert import convert",
            "from ._signatures import BuiltinFunctionT, process_inputs",
            "",
            "SHA256_ADDRESS = 2",
            "SHA256_BASE_GAS = 60",
            "SHA256_PER_WORD_GAS = 12",
            "",
            "",
            "class FoldedFunctionT(BuiltinFunctionT):",
            "    # Base class for nodes which should always be folded",
            "",
            "    _modifiability = Modifiability.CONSTANT",
            "",
            "",
            "class TypenameFoldedFunctionT(FoldedFunctionT):",
            "    # Base class for builtin functions that:",
            "    # (1) take a typename as the only argument; and",
            "    # (2) should always be folded.",
            "    _inputs = [(\"typename\", TYPE_T.any())]",
            "",
            "    def fetch_call_return(self, node):",
            "        type_ = self.infer_arg_types(node)[0].typedef",
            "        return type_",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        validate_call_args(node, 1)",
            "        input_typedef = TYPE_T(type_from_annotation(node.args[0]))",
            "        return [input_typedef]",
            "",
            "",
            "class Floor(BuiltinFunctionT):",
            "    _id = \"floor\"",
            "    _inputs = [(\"value\", DecimalT())]",
            "    # TODO: maybe use int136?",
            "    _return_type = INT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Decimal):",
            "            raise UnfoldableNode",
            "",
            "        value = math.floor(value.value)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        arg = args[0]",
            "        with arg.cache_when_complex(\"arg\") as (b1, arg):",
            "            ret = IRnode.from_list(",
            "                [",
            "                    \"if\",",
            "                    [\"slt\", arg, 0],",
            "                    [\"sdiv\", [\"sub\", arg, DECIMAL_DIVISOR - 1], DECIMAL_DIVISOR],",
            "                    [\"sdiv\", arg, DECIMAL_DIVISOR],",
            "                ],",
            "                typ=INT256_T,",
            "            )",
            "            return b1.resolve(ret)",
            "",
            "",
            "class Ceil(BuiltinFunctionT):",
            "    _id = \"ceil\"",
            "    _inputs = [(\"value\", DecimalT())]",
            "    # TODO: maybe use int136?",
            "    _return_type = INT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Decimal):",
            "            raise UnfoldableNode",
            "",
            "        value = math.ceil(value.value)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        arg = args[0]",
            "        with arg.cache_when_complex(\"arg\") as (b1, arg):",
            "            ret = IRnode.from_list(",
            "                [",
            "                    \"if\",",
            "                    [\"slt\", arg, 0],",
            "                    [\"sdiv\", arg, DECIMAL_DIVISOR],",
            "                    [\"sdiv\", [\"add\", arg, DECIMAL_DIVISOR - 1], DECIMAL_DIVISOR],",
            "                ],",
            "                typ=INT256_T,",
            "            )",
            "            return b1.resolve(ret)",
            "",
            "",
            "class Convert(BuiltinFunctionT):",
            "    _id = \"convert\"",
            "",
            "    def fetch_call_return(self, node):",
            "        _, target_typedef = self.infer_arg_types(node)",
            "",
            "        # note: more type conversion validation happens in convert.py",
            "        return target_typedef.typedef",
            "",
            "    # TODO: push this down into convert.py for more consistency",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        validate_call_args(node, 2)",
            "",
            "        target_type = type_from_annotation(node.args[1])",
            "        value_types = get_possible_types_from_node(node.args[0])",
            "",
            "        # For `convert` of integer literals, we need to match type inference rules in",
            "        # convert.py codegen routines.",
            "        # TODO: This can probably be removed once constant folding for `convert` is implemented",
            "        if len(value_types) > 1 and all(isinstance(v, IntegerT) for v in value_types):",
            "            # Get the smallest (and unsigned if available) type for non-integer target types",
            "            # (note this is different from the ordering returned by `get_possible_types_from_node`)",
            "            if not isinstance(target_type, IntegerT):",
            "                value_types = sorted(value_types, key=lambda v: (v.is_signed, v.bits), reverse=True)",
            "            else:",
            "                # filter out the target type from list of possible types",
            "                value_types = [i for i in value_types if not target_type.compare_type(i)]",
            "",
            "        value_type = value_types.pop()",
            "",
            "        # block conversions between same type",
            "        if target_type.compare_type(value_type):",
            "            raise InvalidType(f\"Value and target type are both '{target_type}'\", node)",
            "",
            "        return [value_type, TYPE_T(target_type)]",
            "",
            "    def build_IR(self, expr, context):",
            "        return convert(expr, context)",
            "",
            "",
            "ADHOC_SLICE_NODE_MACROS = [\"~calldata\", \"~selfcode\", \"~extcode\"]",
            "",
            "",
            "def _build_adhoc_slice_node(sub: IRnode, start: IRnode, length: IRnode, context: Context) -> IRnode:",
            "    assert length.is_literal, \"typechecker failed\"",
            "    assert isinstance(length.value, int)  # mypy hint",
            "",
            "    dst_typ = BytesT(length.value)",
            "    # allocate a buffer for the return value",
            "    buf = context.new_internal_variable(dst_typ)",
            "",
            "    with scope_multi((start, length), (\"start\", \"length\")) as (b1, (start, length)):",
            "        # `msg.data` by `calldatacopy`",
            "        if sub.value == \"~calldata\":",
            "            node = [",
            "                \"seq\",",
            "                check_buffer_overflow_ir(start, length, \"calldatasize\"),",
            "                [\"mstore\", buf, length],",
            "                [\"calldatacopy\", add_ofst(buf, 32), start, length],",
            "                buf,",
            "            ]",
            "",
            "        # `self.code` by `codecopy`",
            "        elif sub.value == \"~selfcode\":",
            "            node = [",
            "                \"seq\",",
            "                check_buffer_overflow_ir(start, length, \"codesize\"),",
            "                [\"mstore\", buf, length],",
            "                [\"codecopy\", add_ofst(buf, 32), start, length],",
            "                buf,",
            "            ]",
            "",
            "        # `<address>.code` by `extcodecopy`",
            "        else:",
            "            assert sub.value == \"~extcode\" and len(sub.args) == 1",
            "            node = [",
            "                \"with\",",
            "                \"_extcode_address\",",
            "                sub.args[0],",
            "                [",
            "                    \"seq\",",
            "                    check_buffer_overflow_ir(start, length, [\"extcodesize\", \"_extcode_address\"]),",
            "                    [\"mstore\", buf, length],",
            "                    [\"extcodecopy\", \"_extcode_address\", add_ofst(buf, 32), start, length],",
            "                    buf,",
            "                ],",
            "            ]",
            "",
            "        assert isinstance(length.value, int)  # mypy hint",
            "        ret = IRnode.from_list(node, typ=BytesT(length.value), location=MEMORY)",
            "        return b1.resolve(ret)",
            "",
            "",
            "# note: this and a lot of other builtins could be refactored to accept any uint type",
            "class Slice(BuiltinFunctionT):",
            "    _id = \"slice\"",
            "    _inputs = [",
            "        (\"b\", (BYTES32_T, BytesT.any(), StringT.any())),",
            "        (\"start\", UINT256_T),",
            "        (\"length\", UINT256_T),",
            "    ]",
            "",
            "    def fetch_call_return(self, node):",
            "        arg_type, _, _ = self.infer_arg_types(node)",
            "",
            "        if isinstance(arg_type, StringT):",
            "            return_type = StringT()",
            "        else:",
            "            return_type = BytesT()",
            "",
            "        # validate start and length are in bounds",
            "",
            "        arg = node.args[0]",
            "        start_expr = node.args[1]",
            "        length_expr = node.args[2].reduced()",
            "",
            "        # CMC 2022-03-22 NOTE slight code duplication with semantics/analysis/local",
            "        is_adhoc_slice = arg.get(\"attr\") == \"code\" or (",
            "            arg.get(\"value.id\") == \"msg\" and arg.get(\"attr\") == \"data\"",
            "        )",
            "",
            "        start_literal = start_expr.value if isinstance(start_expr, vy_ast.Int) else None",
            "        length_literal = length_expr.value if isinstance(length_expr, vy_ast.Int) else None",
            "",
            "        if not is_adhoc_slice:",
            "            if length_literal is not None:",
            "                if length_literal < 1:",
            "                    raise ArgumentException(\"Length cannot be less than 1\", length_expr)",
            "",
            "                if length_literal > arg_type.length:",
            "                    raise ArgumentException(f\"slice out of bounds for {arg_type}\", length_expr)",
            "",
            "            if start_literal is not None:",
            "                if start_literal > arg_type.length:",
            "                    raise ArgumentException(f\"slice out of bounds for {arg_type}\", start_expr)",
            "                if length_literal is not None and start_literal + length_literal > arg_type.length:",
            "                    raise ArgumentException(f\"slice out of bounds for {arg_type}\", node)",
            "",
            "        # we know the length statically",
            "        if length_literal is not None:",
            "            return_type.set_length(length_literal)",
            "        else:",
            "            return_type.set_min_length(arg_type.length)",
            "",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `b`",
            "        b_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [b_type, self._inputs[1][1], self._inputs[2][1]]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        src, start, length = args",
            "",
            "        # Handle `msg.data`, `self.code`, and `<address>.code`",
            "        if src.value in ADHOC_SLICE_NODE_MACROS:",
            "            return _build_adhoc_slice_node(src, start, length, context)",
            "",
            "        is_bytes32 = src.typ == BYTES32_T",
            "        if src.location is None:",
            "            # it's not a pointer; force it to be one since",
            "            # copy_bytes works on pointers.",
            "            assert is_bytes32, src",
            "            src = ensure_in_memory(src, context)",
            "",
            "        if potential_overlap(src, start) or potential_overlap(src, length):",
            "            raise CompilerPanic(\"risky overlap\")",
            "",
            "        with src.cache_when_complex(\"src\") as (b1, src), start.cache_when_complex(\"start\") as (",
            "            b2,",
            "            start,",
            "        ), length.cache_when_complex(\"length\") as (b3, length):",
            "            if is_bytes32:",
            "                src_maxlen = 32",
            "            else:",
            "                src_maxlen = src.typ.maxlen",
            "",
            "            dst_maxlen = length.value if length.is_literal else src_maxlen",
            "",
            "            buflen = dst_maxlen",
            "",
            "            # add 32 bytes to the buffer size bc word access might",
            "            # be unaligned (see below)",
            "            if src.location.word_addressable:",
            "                buflen += 32",
            "",
            "            # Get returntype string or bytes",
            "            assert isinstance(src.typ, _BytestringT) or is_bytes32",
            "            # TODO: try to get dst_typ from semantic analysis",
            "            if isinstance(src.typ, StringT):",
            "                dst_typ = StringT(dst_maxlen)",
            "            else:",
            "                dst_typ = BytesT(dst_maxlen)",
            "",
            "            # allocate a buffer for the return value",
            "            buf = context.new_internal_variable(BytesT(buflen))",
            "            # assign it the correct return type.",
            "            # (note mismatch between dst_maxlen and buflen)",
            "            dst = IRnode.from_list(buf, typ=dst_typ, location=MEMORY)",
            "",
            "            dst_data = bytes_data_ptr(dst)",
            "",
            "            if is_bytes32:",
            "                src_len = 32",
            "                src_data = src",
            "            else:",
            "                src_len = get_bytearray_length(src)",
            "                src_data = bytes_data_ptr(src)",
            "",
            "            # general case. byte-for-byte copy",
            "            if src.location.word_addressable:",
            "                # because slice uses byte-addressing but storage/tstorage",
            "                # is word-aligned, this algorithm starts at some number",
            "                # of bytes before the data section starts, and might copy",
            "                # an extra word. the pseudocode is:",
            "                #   dst_data = dst + 32",
            "                #   copy_dst = dst_data - start % 32",
            "                #   src_data = src + 32",
            "                #   copy_src = src_data + (start - start % 32) / 32",
            "                #            = src_data + (start // 32)",
            "                #   copy_bytes(copy_dst, copy_src, length)",
            "                #   //set length AFTER copy because the length word has been clobbered!",
            "                #   mstore(src, length)",
            "",
            "                # start at the first word-aligned address before `start`",
            "                # e.g. start == byte 7 -> we start copying from byte 0",
            "                #      start == byte 32 -> we start copying from byte 32",
            "                copy_src = IRnode.from_list(",
            "                    [\"add\", src_data, [\"div\", start, 32]], location=src.location",
            "                )",
            "",
            "                # e.g. start == byte 0 -> we copy to dst_data + 0",
            "                #      start == byte 7 -> we copy to dst_data - 7",
            "                #      start == byte 33 -> we copy to dst_data - 1",
            "                copy_dst = IRnode.from_list(",
            "                    [\"sub\", dst_data, [\"mod\", start, 32]], location=dst.location",
            "                )",
            "",
            "                # len + (32 if start % 32 > 0 else 0)",
            "                copy_len = [\"add\", length, [\"mul\", 32, [\"iszero\", [\"iszero\", [\"mod\", start, 32]]]]]",
            "                copy_maxlen = buflen",
            "",
            "            else:",
            "                # all other address spaces (mem, calldata, code) we have",
            "                # byte-aligned access so we can just do the easy thing,",
            "                # memcopy(dst_data, src_data + dst_data)",
            "",
            "                copy_src = add_ofst(src_data, start)",
            "                copy_dst = dst_data",
            "                copy_len = length",
            "                copy_maxlen = buflen",
            "",
            "            do_copy = copy_bytes(copy_dst, copy_src, copy_len, copy_maxlen)",
            "",
            "            ret = [",
            "                \"seq\",",
            "                check_buffer_overflow_ir(start, length, src_len),",
            "                do_copy,",
            "                [\"mstore\", dst, length],  # set length",
            "                dst,  # return pointer to dst",
            "            ]",
            "            ret = IRnode.from_list(ret, typ=dst_typ, location=MEMORY)",
            "            return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "",
            "class Len(BuiltinFunctionT):",
            "    _id = \"len\"",
            "    _inputs = [(\"b\", (StringT.any(), BytesT.any(), DArrayT.any()))]",
            "    _return_type = UINT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        arg = node.args[0].get_folded_value()",
            "        if isinstance(arg, (vy_ast.Str, vy_ast.Bytes)):",
            "            length = len(arg.value)",
            "        elif isinstance(arg, vy_ast.Hex):",
            "            length = len(arg.bytes_value)",
            "        else:",
            "            raise UnfoldableNode",
            "",
            "        return vy_ast.Int.from_node(node, value=length)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type",
            "        typ = get_possible_types_from_node(node.args[0]).pop()",
            "        return [typ]",
            "",
            "    def build_IR(self, node, context):",
            "        arg = Expr(node.args[0], context).ir_node",
            "        if arg.value == \"~calldata\":",
            "            return IRnode.from_list([\"calldatasize\"], typ=UINT256_T)",
            "        return get_bytearray_length(arg)",
            "",
            "",
            "class Concat(BuiltinFunctionT):",
            "    _id = \"concat\"",
            "",
            "    def fetch_call_return(self, node):",
            "        arg_types = self.infer_arg_types(node)",
            "",
            "        length = 0",
            "        for arg_t in arg_types:",
            "            length += arg_t.length",
            "",
            "        if isinstance(arg_types[0], (StringT)):",
            "            return_type = StringT()",
            "        else:",
            "            return_type = BytesT()",
            "        return_type.set_length(length)",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        if len(node.args) < 2:",
            "            raise ArgumentException(\"Invalid argument count: expected at least 2\", node)",
            "",
            "        if node.keywords:",
            "            raise ArgumentException(\"Keyword arguments are not accepted here\", node.keywords[0])",
            "",
            "        ret = []",
            "        prev_typeclass = None",
            "        for arg in node.args:",
            "            validate_expected_type(arg, (BytesT.any(), StringT.any(), BytesM_T.any()))",
            "            arg_t = get_possible_types_from_node(arg).pop()",
            "            current_typeclass = \"String\" if isinstance(arg_t, StringT) else \"Bytes\"",
            "            if prev_typeclass and current_typeclass != prev_typeclass:",
            "                raise TypeMismatch(",
            "                    (",
            "                        \"Concat expects consistent use of string or bytes types, \"",
            "                        \"use either string or bytes.\"",
            "                    ),",
            "                    arg,",
            "                )",
            "            prev_typeclass = current_typeclass",
            "            ret.append(arg_t)",
            "",
            "        return ret",
            "",
            "    def build_IR(self, expr, context):",
            "        args = [Expr(arg, context).ir_node for arg in expr.args]",
            "        if len(args) < 2:",
            "            raise StructureException(\"Concat expects at least two arguments\", expr)",
            "",
            "        # Maximum length of the output",
            "        dst_maxlen = sum(",
            "            [arg.typ.maxlen if isinstance(arg.typ, _BytestringT) else arg.typ.m for arg in args]",
            "        )",
            "",
            "        # TODO: try to grab these from semantic analysis",
            "        if isinstance(args[0].typ, StringT):",
            "            ret_typ = StringT(dst_maxlen)",
            "        else:",
            "            ret_typ = BytesT(dst_maxlen)",
            "",
            "        # respect API of copy_bytes",
            "        bufsize = dst_maxlen + 32",
            "        dst = context.new_internal_variable(BytesT(bufsize))",
            "        dst.annotation = \"concat destination\"",
            "",
            "        ret = [\"seq\"]",
            "        # stack item representing our current offset in the dst buffer",
            "        ofst = \"concat_ofst\"",
            "",
            "        # TODO: optimize for the case where all lengths are statically known.",
            "        for arg in args:",
            "            dst_data = add_ofst(bytes_data_ptr(dst), ofst)",
            "",
            "            if isinstance(arg.typ, _BytestringT):",
            "                # Ignore empty strings",
            "                if arg.typ.maxlen == 0:",
            "                    continue",
            "",
            "                with arg.cache_when_complex(\"arg\") as (b1, arg):",
            "                    argdata = bytes_data_ptr(arg)",
            "",
            "                    with get_bytearray_length(arg).cache_when_complex(\"len\") as (b2, arglen):",
            "                        do_copy = [",
            "                            \"seq\",",
            "                            copy_bytes(dst_data, argdata, arglen, arg.typ.maxlen),",
            "                            [\"set\", ofst, [\"add\", ofst, arglen]],",
            "                        ]",
            "                        ret.append(b1.resolve(b2.resolve(do_copy)))",
            "",
            "            else:",
            "                ret.append(STORE(dst_data, unwrap_location(arg)))",
            "                ret.append([\"set\", ofst, [\"add\", ofst, arg.typ.m]])",
            "",
            "        ret.append(STORE(dst, ofst))",
            "",
            "        # Memory location of the output",
            "        ret.append(dst)",
            "",
            "        return IRnode.from_list(",
            "            [\"with\", ofst, 0, ret], typ=ret_typ, location=MEMORY, annotation=\"concat\"",
            "        )",
            "",
            "",
            "class Keccak256(BuiltinFunctionT):",
            "    _id = \"keccak256\"",
            "    # TODO allow any BytesM_T",
            "    _inputs = [(\"value\", (BytesT.any(), BYTES32_T, StringT.any()))]",
            "    _return_type = BYTES32_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if isinstance(value, vy_ast.Bytes):",
            "            value = value.value",
            "        elif isinstance(value, vy_ast.Str):",
            "            value = value.value.encode()",
            "        elif isinstance(value, vy_ast.Hex):",
            "            value = value.bytes_value",
            "        else:",
            "            raise UnfoldableNode",
            "",
            "        hash_ = f\"0x{keccak256(value).hex()}\"",
            "        return vy_ast.Hex.from_node(node, value=hash_)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `value`",
            "        value_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [value_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        assert len(args) == 1",
            "        return keccak256_helper(args[0], context)",
            "",
            "",
            "def _make_sha256_call(inp_start, inp_len, out_start, out_len):",
            "    return [",
            "        \"assert\",",
            "        [",
            "            \"staticcall\",",
            "            [\"gas\"],  # gas",
            "            SHA256_ADDRESS,  # address",
            "            inp_start,",
            "            inp_len,",
            "            out_start,",
            "            out_len,",
            "        ],",
            "    ]",
            "",
            "",
            "class Sha256(BuiltinFunctionT):",
            "    _id = \"sha256\"",
            "    _inputs = [(\"value\", (BYTES32_T, BytesT.any(), StringT.any()))]",
            "    _return_type = BYTES32_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if isinstance(value, vy_ast.Bytes):",
            "            value = value.value",
            "        elif isinstance(value, vy_ast.Str):",
            "            value = value.value.encode()",
            "        elif isinstance(value, vy_ast.Hex):",
            "            value = value.bytes_value",
            "        else:",
            "            raise UnfoldableNode",
            "",
            "        hash_ = f\"0x{hashlib.sha256(value).hexdigest()}\"",
            "        return vy_ast.Hex.from_node(node, value=hash_)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `value`",
            "        value_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [value_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        sub = args[0]",
            "        # bytes32 input",
            "        if sub.typ == BYTES32_T:",
            "            return IRnode.from_list(",
            "                [",
            "                    \"seq\",",
            "                    [\"mstore\", MemoryPositions.FREE_VAR_SPACE, sub],",
            "                    _make_sha256_call(",
            "                        inp_start=MemoryPositions.FREE_VAR_SPACE,",
            "                        inp_len=32,",
            "                        out_start=MemoryPositions.FREE_VAR_SPACE,",
            "                        out_len=32,",
            "                    ),",
            "                    [\"mload\", MemoryPositions.FREE_VAR_SPACE],  # push value onto stack",
            "                ],",
            "                typ=BYTES32_T,",
            "                add_gas_estimate=SHA256_BASE_GAS + 1 * SHA256_PER_WORD_GAS,",
            "            )",
            "        # bytearay-like input",
            "        # special case if it's already in memory",
            "        sub = ensure_in_memory(sub, context)",
            "",
            "        return IRnode.from_list(",
            "            [",
            "                \"with\",",
            "                \"_sub\",",
            "                sub,",
            "                [",
            "                    \"seq\",",
            "                    _make_sha256_call(",
            "                        # TODO use add_ofst if sub is statically known",
            "                        inp_start=[\"add\", \"_sub\", 32],",
            "                        inp_len=[\"mload\", \"_sub\"],",
            "                        out_start=MemoryPositions.FREE_VAR_SPACE,",
            "                        out_len=32,",
            "                    ),",
            "                    [\"mload\", MemoryPositions.FREE_VAR_SPACE],",
            "                ],",
            "            ],",
            "            typ=BYTES32_T,",
            "            add_gas_estimate=SHA256_BASE_GAS + sub.typ.maxlen * SHA256_PER_WORD_GAS,",
            "        )",
            "",
            "",
            "class MethodID(FoldedFunctionT):",
            "    _id = \"method_id\"",
            "    _inputs = [(\"value\", StringT.any())]",
            "    _kwargs = {\"output_type\": KwargSettings(TYPE_T.any(), BytesT(4))}",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1, [\"output_type\"])",
            "",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Str):",
            "            raise InvalidType(\"method id must be given as a literal string\", node.args[0])",
            "        if \" \" in value.value:",
            "            raise InvalidLiteral(\"Invalid function signature - no spaces allowed.\", node.args[0])",
            "",
            "        return_type = self.infer_kwarg_types(node)[\"output_type\"].typedef",
            "        value = method_id(value.value)",
            "",
            "        if return_type.compare_type(BYTES4_T):",
            "            return vy_ast.Hex.from_node(node, value=\"0x\" + value.hex())",
            "        else:",
            "            return vy_ast.Bytes.from_node(node, value=value)",
            "",
            "    def fetch_call_return(self, node):",
            "        validate_call_args(node, 1, [\"output_type\"])",
            "",
            "        type_ = self.infer_kwarg_types(node)[\"output_type\"].typedef",
            "        return type_",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        return [self._inputs[0][1]]",
            "",
            "    def infer_kwarg_types(self, node):",
            "        if node.keywords:",
            "            output_type = type_from_annotation(node.keywords[0].value)",
            "            if output_type not in (BytesT(4), BYTES4_T):",
            "                raise ArgumentException(\"output_type must be Bytes[4] or bytes4\", node.keywords[0])",
            "        else:",
            "            # default to `Bytes[4]`",
            "            output_type = BytesT(4)",
            "",
            "        return {\"output_type\": TYPE_T(output_type)}",
            "",
            "",
            "class ECRecover(BuiltinFunctionT):",
            "    _id = \"ecrecover\"",
            "    _inputs = [",
            "        (\"hash\", BYTES32_T),",
            "        (\"v\", (UINT256_T, UINT8_T)),",
            "        (\"r\", (UINT256_T, BYTES32_T)),",
            "        (\"s\", (UINT256_T, BYTES32_T)),",
            "    ]",
            "    _return_type = AddressT()",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        v_t, r_t, s_t = [get_possible_types_from_node(arg).pop() for arg in node.args[1:]]",
            "        return [BYTES32_T, v_t, r_t, s_t]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        input_buf = context.new_internal_variable(get_type_for_exact_size(128))",
            "        output_buf = context.new_internal_variable(get_type_for_exact_size(32))",
            "        return IRnode.from_list(",
            "            [",
            "                \"seq\",",
            "                # clear output memory first, ecrecover can return 0 bytes",
            "                [\"mstore\", output_buf, 0],",
            "                [\"mstore\", input_buf, args[0]],",
            "                [\"mstore\", add_ofst(input_buf, 32), args[1]],",
            "                [\"mstore\", add_ofst(input_buf, 64), args[2]],",
            "                [\"mstore\", add_ofst(input_buf, 96), args[3]],",
            "                [\"staticcall\", \"gas\", 1, input_buf, 128, output_buf, 32],",
            "                [\"mload\", output_buf],",
            "            ],",
            "            typ=AddressT(),",
            "        )",
            "",
            "",
            "class _ECArith(BuiltinFunctionT):",
            "    @process_inputs",
            "    def build_IR(self, expr, _args, kwargs, context):",
            "        args_tuple = ir_tuple_from_args(_args)",
            "",
            "        args_t = args_tuple.typ",
            "        input_buf = context.new_internal_variable(args_t)",
            "        ret_t = self._return_type",
            "",
            "        ret = [\"seq\"]",
            "        ret.append(make_setter(input_buf, args_tuple))",
            "",
            "        output_buf = context.new_internal_variable(ret_t)",
            "",
            "        args_ofst = input_buf",
            "        args_len = args_t.memory_bytes_required",
            "        out_ofst = output_buf",
            "        out_len = ret_t.memory_bytes_required",
            "",
            "        ret.append(",
            "            [",
            "                \"assert\",",
            "                [\"staticcall\", [\"gas\"], self._precompile, args_ofst, args_len, out_ofst, out_len],",
            "            ]",
            "        )",
            "        ret.append(output_buf)",
            "",
            "        return IRnode.from_list(ret, typ=ret_t, location=MEMORY)",
            "",
            "",
            "class ECAdd(_ECArith):",
            "    _id = \"ecadd\"",
            "    _inputs = [(\"a\", SArrayT(UINT256_T, 2)), (\"b\", SArrayT(UINT256_T, 2))]",
            "    _return_type = SArrayT(UINT256_T, 2)",
            "    _precompile = 0x6",
            "",
            "",
            "class ECMul(_ECArith):",
            "    _id = \"ecmul\"",
            "    _inputs = [(\"point\", SArrayT(UINT256_T, 2)), (\"scalar\", UINT256_T)]",
            "    _return_type = SArrayT(UINT256_T, 2)",
            "    _precompile = 0x7",
            "",
            "",
            "class Extract32(BuiltinFunctionT):",
            "    _id = \"extract32\"",
            "    _inputs = [(\"b\", BytesT.any()), (\"start\", IntegerT.unsigneds())]",
            "    _kwargs = {\"output_type\": KwargSettings(TYPE_T.any(), BYTES32_T)}",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "        return_type = self.infer_kwarg_types(node)[\"output_type\"].typedef",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        input_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [input_type, UINT256_T]",
            "",
            "    def infer_kwarg_types(self, node):",
            "        if node.keywords:",
            "            output_type = type_from_annotation(node.keywords[0].value)",
            "            if not isinstance(output_type, (AddressT, BytesM_T, IntegerT)):",
            "                raise InvalidType(",
            "                    \"Output type must be one of integer, bytes32 or address\", node.keywords[0].value",
            "                )",
            "            output_typedef = TYPE_T(output_type)",
            "            node.keywords[0].value._metadata[\"type\"] = output_typedef",
            "        else:",
            "            output_typedef = TYPE_T(BYTES32_T)",
            "",
            "        return {\"output_type\": output_typedef}",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        bytez, index = args",
            "        ret_type = kwargs[\"output_type\"]",
            "",
            "        if potential_overlap(bytez, index):",
            "            raise CompilerPanic(\"risky overlap\")",
            "",
            "        def finalize(ret):",
            "            annotation = \"extract32\"",
            "            ret = IRnode.from_list(ret, typ=ret_type, annotation=annotation)",
            "            return clamp_basetype(ret)",
            "",
            "        with bytez.cache_when_complex(\"_sub\") as (b1, bytez):",
            "            # merge",
            "            length = get_bytearray_length(bytez)",
            "            index = clamp2(0, index, [\"sub\", length, 32], signed=True)",
            "            with index.cache_when_complex(\"_index\") as (b2, index):",
            "                assert not index.typ.is_signed",
            "",
            "                # \"easy\" case, byte- addressed locations:",
            "                if bytez.location.word_scale == 32:",
            "                    word = LOAD(add_ofst(bytes_data_ptr(bytez), index))",
            "                    return finalize(b1.resolve(b2.resolve(word)))",
            "",
            "                # storage and transient storage, word-addressed",
            "                assert bytez.location.word_scale == 1",
            "",
            "                slot = IRnode.from_list([\"div\", index, 32])",
            "                # byte offset within the slot",
            "                byte_ofst = IRnode.from_list([\"mod\", index, 32])",
            "",
            "                with byte_ofst.cache_when_complex(\"byte_ofst\") as (",
            "                    b3,",
            "                    byte_ofst,",
            "                ), slot.cache_when_complex(\"slot\") as (b4, slot):",
            "                    # perform two loads and merge",
            "                    w1 = LOAD(add_ofst(bytes_data_ptr(bytez), slot))",
            "                    w2 = LOAD(add_ofst(bytes_data_ptr(bytez), [\"add\", slot, 1]))",
            "",
            "                    left_bytes = shl([\"mul\", 8, byte_ofst], w1)",
            "                    right_bytes = shr([\"mul\", 8, [\"sub\", 32, byte_ofst]], w2)",
            "                    merged = [\"or\", left_bytes, right_bytes]",
            "",
            "                    ret = [\"if\", byte_ofst, merged, left_bytes]",
            "                    return finalize(b1.resolve(b2.resolve(b3.resolve(b4.resolve(ret)))))",
            "",
            "",
            "class AsWeiValue(BuiltinFunctionT):",
            "    _id = \"as_wei_value\"",
            "    _inputs = [(\"value\", (IntegerT.any(), DecimalT())), (\"unit\", StringT.any())]",
            "    _return_type = UINT256_T",
            "",
            "    wei_denoms = {",
            "        (\"wei\",): 1,",
            "        (\"femtoether\", \"kwei\", \"babbage\"): 10**3,",
            "        (\"picoether\", \"mwei\", \"lovelace\"): 10**6,",
            "        (\"nanoether\", \"gwei\", \"shannon\"): 10**9,",
            "        (\"microether\", \"szabo\"): 10**12,",
            "        (\"milliether\", \"finney\"): 10**15,",
            "        (\"ether\",): 10**18,",
            "        (\"kether\", \"grand\"): 10**21,",
            "    }",
            "",
            "    def get_denomination(self, node):",
            "        value = node.args[1].get_folded_value()",
            "        if not isinstance(value, vy_ast.Str):",
            "            raise ArgumentException(",
            "                \"Wei denomination must be given as a literal string\", node.args[1]",
            "            )",
            "        try:",
            "            denom = next(v for k, v in self.wei_denoms.items() if value.value in k)",
            "        except StopIteration:",
            "            raise ArgumentException(f\"Unknown denomination: {value.value}\", node.args[1]) from None",
            "",
            "        return denom",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 2)",
            "        denom = self.get_denomination(node)",
            "",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, (vy_ast.Decimal, vy_ast.Int)):",
            "            raise UnfoldableNode",
            "        value = value.value",
            "",
            "        if value < 0:",
            "            raise InvalidLiteral(\"Negative wei value not allowed\", node.args[0])",
            "",
            "        return vy_ast.Int.from_node(node, value=int(value * denom))",
            "",
            "    def fetch_call_return(self, node):",
            "        self.infer_arg_types(node)",
            "        return self._return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type instead of abstract type",
            "        value_type = get_possible_types_from_node(node.args[0]).pop()",
            "        unit_type = get_possible_types_from_node(node.args[1]).pop()",
            "        return [value_type, unit_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        value = args[0]",
            "",
            "        denom_divisor = self.get_denomination(expr)",
            "        with value.cache_when_complex(\"value\") as (b1, value):",
            "            if value.typ in (UINT256_T, UINT8_T):",
            "                sub = [",
            "                    \"with\",",
            "                    \"ans\",",
            "                    [\"mul\", value, denom_divisor],",
            "                    [",
            "                        \"seq\",",
            "                        [",
            "                            \"assert\",",
            "                            [\"or\", [\"eq\", [\"div\", \"ans\", value], denom_divisor], [\"iszero\", value]],",
            "                        ],",
            "                        \"ans\",",
            "                    ],",
            "                ]",
            "            elif value.typ == INT128_T:",
            "                # signed types do not require bounds checks because the",
            "                # largest possible converted value will not overflow 2**256",
            "                sub = [\"seq\", [\"assert\", [\"sgt\", value, -1]], [\"mul\", value, denom_divisor]]",
            "            elif value.typ == DecimalT():",
            "                sub = [",
            "                    \"seq\",",
            "                    [\"assert\", [\"sgt\", value, -1]],",
            "                    [\"div\", [\"mul\", value, denom_divisor], DECIMAL_DIVISOR],",
            "                ]",
            "            else:",
            "                raise CompilerPanic(f\"Unexpected type: {value.typ}\")",
            "",
            "            return IRnode.from_list(b1.resolve(sub), typ=UINT256_T)",
            "",
            "",
            "zero_value = IRnode.from_list(0, typ=UINT256_T)",
            "empty_value = IRnode.from_list(0, typ=BYTES32_T)",
            "",
            "",
            "class RawCall(BuiltinFunctionT):",
            "    _id = \"raw_call\"",
            "    _inputs = [(\"to\", AddressT()), (\"data\", BytesT.any())]",
            "    _kwargs = {",
            "        \"max_outsize\": KwargSettings(UINT256_T, 0, require_literal=True),",
            "        \"gas\": KwargSettings(UINT256_T, \"gas\"),",
            "        \"value\": KwargSettings(UINT256_T, zero_value),",
            "        \"is_delegate_call\": KwargSettings(BoolT(), False, require_literal=True),",
            "        \"is_static_call\": KwargSettings(BoolT(), False, require_literal=True),",
            "        \"revert_on_failure\": KwargSettings(BoolT(), True, require_literal=True),",
            "    }",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "",
            "        kwargz = {i.arg: i.value for i in node.keywords}",
            "",
            "        outsize = kwargz.get(\"max_outsize\")",
            "        if outsize is not None:",
            "            outsize = outsize.get_folded_value()",
            "",
            "        revert_on_failure = kwargz.get(\"revert_on_failure\")",
            "        if revert_on_failure is not None:",
            "            revert_on_failure = revert_on_failure.get_folded_value().value",
            "        else:",
            "            revert_on_failure = True",
            "",
            "        if outsize is None or outsize.value == 0:",
            "            if revert_on_failure:",
            "                return None",
            "            return BoolT()",
            "",
            "        if not isinstance(outsize, vy_ast.Int) or outsize.value < 0:",
            "            raise",
            "",
            "        if outsize.value:",
            "            return_type = BytesT()",
            "            return_type.set_min_length(outsize.value)",
            "",
            "            if revert_on_failure:",
            "                return return_type",
            "            return TupleT([BoolT(), return_type])",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `data`",
            "        data_type = get_possible_types_from_node(node.args[1]).pop()",
            "        return [self._inputs[0][1], data_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        to, data = args",
            "        # TODO: must compile in source code order, left-to-right",
            "        gas, value, outsize, delegate_call, static_call, revert_on_failure = (",
            "            kwargs[\"gas\"],",
            "            kwargs[\"value\"],",
            "            kwargs[\"max_outsize\"],",
            "            kwargs[\"is_delegate_call\"],",
            "            kwargs[\"is_static_call\"],",
            "            kwargs[\"revert_on_failure\"],",
            "        )",
            "",
            "        if delegate_call and static_call:",
            "            raise ArgumentException(",
            "                \"Call may use one of `is_delegate_call` or `is_static_call`, not both\"",
            "            )",
            "",
            "        if (delegate_call or static_call) and value.value != 0:",
            "            raise ArgumentException(\"value= may not be passed for static or delegate calls!\")",
            "",
            "        if not static_call and context.is_constant():",
            "            raise StateAccessViolation(",
            "                f\"Cannot make modifying calls from {context.pp_constancy()},\"",
            "                \" use `is_static_call=True` to perform this action\"",
            "            )",
            "",
            "        if data.value == \"~calldata\":",
            "            call_ir = [\"with\", \"mem_ofst\", \"msize\"]",
            "            args_ofst = [\"seq\", [\"calldatacopy\", \"mem_ofst\", 0, \"calldatasize\"], \"mem_ofst\"]",
            "            args_len = \"calldatasize\"",
            "        else:",
            "            # some gymnastics to propagate constants (if eval_input_buf",
            "            # returns a static memory location)",
            "            eval_input_buf = ensure_in_memory(data, context)",
            "",
            "            input_buf = eval_seq(eval_input_buf)",
            "",
            "            if input_buf is None:",
            "                call_ir = [\"with\", \"arg_buf\", eval_input_buf]",
            "                input_buf = IRnode.from_list(\"arg_buf\")",
            "            else:",
            "                call_ir = [\"seq\", eval_input_buf]",
            "",
            "            args_ofst = add_ofst(input_buf, 32)",
            "            args_len = [\"mload\", input_buf]",
            "",
            "        output_node = context.new_internal_variable(BytesT(outsize))",
            "",
            "        bool_ty = BoolT()",
            "",
            "        # build IR for call or delegatecall",
            "        common_call_args = [",
            "            args_ofst,",
            "            args_len,",
            "            # if there is no return value, the return offset can be 0",
            "            add_ofst(output_node, 32) if outsize else 0,",
            "            outsize,",
            "        ]",
            "",
            "        gas, value = IRnode.from_list(gas), IRnode.from_list(value)",
            "        with scope_multi((to, value, gas), (\"_to\", \"_value\", \"_gas\")) as (b1, (to, value, gas)):",
            "            if delegate_call:",
            "                call_op = [\"delegatecall\", gas, to, *common_call_args]",
            "            elif static_call:",
            "                call_op = [\"staticcall\", gas, to, *common_call_args]",
            "            else:",
            "                call_op = [\"call\", gas, to, value, *common_call_args]",
            "",
            "            call_op = ensure_eval_once(\"raw_call_builtin\", call_op)",
            "            call_ir += [call_op]",
            "            call_ir = b1.resolve(call_ir)",
            "",
            "        # build sequence IR",
            "        if outsize:",
            "            # return minimum of outsize and returndatasize",
            "            size = [\"select\", [\"lt\", outsize, \"returndatasize\"], outsize, \"returndatasize\"]",
            "",
            "            # store output size and return output location",
            "            store_output_size = [\"seq\", [\"mstore\", output_node, size], output_node]",
            "",
            "            bytes_ty = BytesT(outsize)",
            "",
            "            if revert_on_failure:",
            "                typ = bytes_ty",
            "                # check the call success flag, and store returndata in memory",
            "                ret_ir = [\"seq\", check_external_call(call_ir), store_output_size]",
            "                return IRnode.from_list(ret_ir, typ=typ, location=MEMORY)",
            "            else:",
            "                typ = TupleT([bool_ty, bytes_ty])",
            "                ret_ir = [",
            "                    \"multi\",",
            "                    # use IRnode.from_list to make sure the types are",
            "                    # set properly on the \"multi\" members",
            "                    IRnode.from_list(call_ir, typ=bool_ty),",
            "                    IRnode.from_list(store_output_size, typ=bytes_ty, location=MEMORY),",
            "                ]",
            "                # return an IR tuple of call success flag and returndata pointer",
            "                return IRnode.from_list(ret_ir, typ=typ)",
            "",
            "        # max_outsize is 0.",
            "",
            "        if not revert_on_failure:",
            "            # return call flag as stack item",
            "            typ = bool_ty",
            "            return IRnode.from_list(call_ir, typ=typ)",
            "",
            "        else:",
            "            # check the call success flag and don't return anything",
            "            ret_ir = check_external_call(call_ir)",
            "            return IRnode.from_list(ret_ir, typ=None)",
            "",
            "        raise CompilerPanic(\"unreachable!\")",
            "",
            "",
            "class Send(BuiltinFunctionT):",
            "    _id = \"send\"",
            "    _inputs = [(\"to\", AddressT()), (\"value\", UINT256_T)]",
            "    # default gas stipend is 0",
            "    _kwargs = {\"gas\": KwargSettings(UINT256_T, 0)}",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        to, value = args",
            "        gas = kwargs[\"gas\"]",
            "        context.check_is_not_constant(\"send ether\", expr)",
            "        send_op = ensure_eval_once(\"send_builtin\", [\"call\", gas, to, value, 0, 0, 0, 0])",
            "        return IRnode.from_list([\"assert\", send_op], error_msg=\"send failed\")",
            "",
            "",
            "class SelfDestruct(BuiltinFunctionT):",
            "    _id = \"selfdestruct\"",
            "    _inputs = [(\"to\", AddressT())]",
            "    _is_terminus = True",
            "    _warned = False",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        if not self._warned:",
            "            vyper_warn(",
            "                \"`selfdestruct` is deprecated! The opcode is no longer recommended for use.\", expr",
            "            )",
            "            self._warned = True",
            "",
            "        context.check_is_not_constant(\"selfdestruct\", expr)",
            "        return IRnode.from_list(ensure_eval_once(\"selfdestruct\", [\"selfdestruct\", args[0]]))",
            "",
            "",
            "class BlockHash(BuiltinFunctionT):",
            "    _id = \"blockhash\"",
            "    _inputs = [(\"block_num\", UINT256_T)]",
            "    _return_type = BYTES32_T",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, contact):",
            "        return IRnode.from_list(",
            "            [\"blockhash\", clamp(\"lt\", clamp(\"sge\", args[0], [\"sub\", [\"number\"], 256]), \"number\")],",
            "            typ=BYTES32_T,",
            "        )",
            "",
            "",
            "class BlobHash(BuiltinFunctionT):",
            "    _id = \"blobhash\"",
            "    _inputs = [(\"index\", UINT256_T)]",
            "    _return_type = BYTES32_T",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, contact):",
            "        if not version_check(begin=\"cancun\"):",
            "            raise EvmVersionException(\"`blobhash` is not available pre-cancun\", expr)",
            "        return IRnode.from_list([\"blobhash\", args[0]], typ=BYTES32_T)",
            "",
            "",
            "class RawRevert(BuiltinFunctionT):",
            "    _id = \"raw_revert\"",
            "    _inputs = [(\"data\", BytesT.any())]",
            "    _return_type = None",
            "    _is_terminus = True",
            "",
            "    def fetch_call_return(self, node):",
            "        return None",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        data_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [data_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        with ensure_in_memory(args[0], context).cache_when_complex(\"err_buf\") as (b, buf):",
            "            data = bytes_data_ptr(buf)",
            "            len_ = get_bytearray_length(buf)",
            "            return b.resolve(IRnode.from_list([\"revert\", data, len_]))",
            "",
            "",
            "class RawLog(BuiltinFunctionT):",
            "    _id = \"raw_log\"",
            "    _inputs = [(\"topics\", DArrayT(BYTES32_T, 4)), (\"data\", (BYTES32_T, BytesT.any()))]",
            "",
            "    def fetch_call_return(self, node):",
            "        self.infer_arg_types(node)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "",
            "        arg = node.args[0].reduced()",
            "        if not isinstance(arg, vy_ast.List) or len(arg.elements) > 4:",
            "            raise InvalidType(\"Expecting a list of 0-4 topics as first argument\", node.args[0])",
            "",
            "        # return a concrete type for `data`",
            "        data_type = get_possible_types_from_node(node.args[1]).pop()",
            "",
            "        return [self._inputs[0][1], data_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        context.check_is_not_constant(f\"use {self._id}\", expr)",
            "",
            "        topics_length = len(expr.args[0].reduced().elements)",
            "        topics = args[0].args",
            "        topics = [unwrap_location(topic) for topic in topics]",
            "",
            "        # sanity check topics is a literal list",
            "        assert args[0].value in (\"~empty\", \"multi\")",
            "",
            "        data = args[1]",
            "",
            "        log_op = \"log\" + str(topics_length)",
            "",
            "        if data.typ == BYTES32_T:",
            "            placeholder = context.new_internal_variable(BYTES32_T)",
            "            log_ir = [log_op, placeholder, 32] + topics",
            "            return IRnode.from_list(",
            "                [\"seq\", make_setter(placeholder, data), ensure_eval_once(\"raw_log\", log_ir)]",
            "            )",
            "",
            "        input_buf = ensure_in_memory(data, context)",
            "",
            "        log_ir = [log_op, [\"add\", \"_sub\", 32], [\"mload\", \"_sub\"], *topics]",
            "        return IRnode.from_list([\"with\", \"_sub\", input_buf, ensure_eval_once(\"raw_log\", log_ir)])",
            "",
            "",
            "class BitwiseAnd(BuiltinFunctionT):",
            "    _id = \"bitwise_and\"",
            "    _inputs = [(\"x\", UINT256_T), (\"y\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_and()` is deprecated! Please use the & operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        for val in values:",
            "            if not isinstance(val, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = values[0].value & values[1].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"and\", args[0], args[1]], typ=UINT256_T)",
            "",
            "",
            "class BitwiseOr(BuiltinFunctionT):",
            "    _id = \"bitwise_or\"",
            "    _inputs = [(\"x\", UINT256_T), (\"y\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_or()` is deprecated! Please use the | operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        for val in values:",
            "            if not isinstance(val, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = values[0].value | values[1].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"or\", args[0], args[1]], typ=UINT256_T)",
            "",
            "",
            "class BitwiseXor(BuiltinFunctionT):",
            "    _id = \"bitwise_xor\"",
            "    _inputs = [(\"x\", UINT256_T), (\"y\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_xor()` is deprecated! Please use the ^ operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        for val in values:",
            "            if not isinstance(val, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = values[0].value ^ values[1].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"xor\", args[0], args[1]], typ=UINT256_T)",
            "",
            "",
            "class BitwiseNot(BuiltinFunctionT):",
            "    _id = \"bitwise_not\"",
            "    _inputs = [(\"x\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_not()` is deprecated! Please use the ~ operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Int):",
            "            raise UnfoldableNode",
            "",
            "        value = value.value",
            "",
            "        value = (2**256 - 1) - value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"not\", args[0]], typ=UINT256_T)",
            "",
            "",
            "class Shift(BuiltinFunctionT):",
            "    _id = \"shift\"",
            "    _inputs = [(\"x\", (UINT256_T, INT256_T)), (\"_shift_bits\", IntegerT.any())]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`shift()` is deprecated! Please use the << or >> operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        args = [i.get_folded_value() for i in node.args]",
            "        if any(not isinstance(i, vy_ast.Int) for i in args):",
            "            raise UnfoldableNode",
            "        value, shift = [i.value for i in args]",
            "        if shift < -256 or shift > 256:",
            "            # this validation is performed to prevent the compiler from hanging",
            "            # rather than for correctness because the post-folded constant would",
            "            # have been validated anyway",
            "            raise InvalidLiteral(\"Shift must be between -256 and 256\", node.args[1])",
            "",
            "        if shift < 0:",
            "            value = value >> -shift",
            "        else:",
            "            value = (value << shift) % (2**256)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    def fetch_call_return(self, node):",
            "        # return type is the type of the first argument",
            "        return self.infer_arg_types(node)[0]",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type instead of SignedIntegerAbstractType",
            "        arg_ty = get_possible_types_from_node(node.args[0])[0]",
            "        shift_ty = get_possible_types_from_node(node.args[1])[0]",
            "        return [arg_ty, shift_ty]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # \"gshr\" -- generalized right shift",
            "        argty = args[0].typ",
            "        GSHR = sar if argty.is_signed else shr",
            "",
            "        with args[0].cache_when_complex(\"to_shift\") as (b1, arg), args[1].cache_when_complex(",
            "            \"bits\"",
            "        ) as (b2, bits):",
            "            neg_bits = [\"sub\", 0, bits]",
            "            ret = [\"if\", [\"slt\", bits, 0], GSHR(neg_bits, arg), shl(bits, arg)]",
            "            return b1.resolve(b2.resolve(IRnode.from_list(ret, typ=argty)))",
            "",
            "",
            "class _AddMulMod(BuiltinFunctionT):",
            "    _inputs = [(\"a\", UINT256_T), (\"b\", UINT256_T), (\"c\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 3)",
            "        args = [i.get_folded_value() for i in node.args]",
            "        if isinstance(args[2], vy_ast.Int) and args[2].value == 0:",
            "            raise ZeroDivisionException(\"Modulo by 0\", node.args[2])",
            "        for arg in args:",
            "            if not isinstance(arg, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = self._eval_fn(args[0].value, args[1].value) % args[2].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        x, y, z = args",
            "        with x.cache_when_complex(\"x\") as (b1, x):",
            "            with y.cache_when_complex(\"y\") as (b2, y):",
            "                with z.cache_when_complex(\"z\") as (b3, z):",
            "                    ret = IRnode.from_list(",
            "                        [\"seq\", [\"assert\", z], [self._opcode, x, y, z]], typ=UINT256_T",
            "                    )",
            "                    return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "",
            "class AddMod(_AddMulMod):",
            "    _id = \"uint256_addmod\"",
            "    _eval_fn = operator.add",
            "    _opcode = \"addmod\"",
            "",
            "",
            "class MulMod(_AddMulMod):",
            "    _id = \"uint256_mulmod\"",
            "    _eval_fn = operator.mul",
            "    _opcode = \"mulmod\"",
            "",
            "",
            "class PowMod256(BuiltinFunctionT):",
            "    _id = \"pow_mod256\"",
            "    _inputs = [(\"a\", UINT256_T), (\"b\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        if any(not isinstance(i, vy_ast.Int) for i in values):",
            "            raise UnfoldableNode",
            "",
            "        left, right = values",
            "        value = pow(left.value, right.value, 2**256)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    def build_IR(self, expr, context):",
            "        left = Expr.parse_value_expr(expr.args[0], context)",
            "        right = Expr.parse_value_expr(expr.args[1], context)",
            "        return IRnode.from_list([\"exp\", left, right], typ=left.typ)",
            "",
            "",
            "class Abs(BuiltinFunctionT):",
            "    _id = \"abs\"",
            "    _inputs = [(\"value\", INT256_T)]",
            "    _return_type = INT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Int):",
            "            raise UnfoldableNode",
            "",
            "        value = abs(value.value)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    def build_IR(self, expr, context):",
            "        value = Expr.parse_value_expr(expr.args[0], context)",
            "        sub = [",
            "            \"with\",",
            "            \"orig\",",
            "            value,",
            "            [",
            "                \"if\",",
            "                [\"slt\", \"orig\", 0],",
            "                # clamp orig != -2**255 (because it maps to itself under negation)",
            "                [\"seq\", [\"assert\", [\"ne\", \"orig\", [\"sub\", 0, \"orig\"]]], [\"sub\", 0, \"orig\"]],",
            "                \"orig\",",
            "            ],",
            "        ]",
            "        return IRnode.from_list(sub, typ=INT256_T)",
            "",
            "",
            "# CREATE* functions",
            "",
            "CREATE2_SENTINEL = dummy_node_for_type(BYTES32_T)",
            "",
            "",
            "# create helper functions",
            "# generates CREATE op sequence + zero check for result",
            "def _create_ir(value, buf, length, salt, revert_on_failure=True):",
            "    args = [value, buf, length]",
            "    create_op = \"create\"",
            "    if salt is not CREATE2_SENTINEL:",
            "        create_op = \"create2\"",
            "        args.append(salt)",
            "",
            "    ret = IRnode.from_list(ensure_eval_once(\"create_builtin\", [create_op, *args]))",
            "",
            "    if not revert_on_failure:",
            "        return ret",
            "",
            "    ret = clamp_nonzero(ret)",
            "    ret.set_error_msg(f\"{create_op} failed\")",
            "    return ret",
            "",
            "",
            "# calculate the gas used by create for a given number of bytes",
            "def _create_addl_gas_estimate(size, should_use_create2):",
            "    ret = 200 * size",
            "    if should_use_create2:",
            "        ret += SHA3_PER_WORD * ceil32(size) // 32",
            "    return ret",
            "",
            "",
            "def eip1167_bytecode():",
            "    # NOTE cyclic import?",
            "    from vyper.ir.compile_ir import assembly_to_evm",
            "",
            "    loader_asm = [",
            "        \"PUSH1\",",
            "        0x2D,",
            "        \"RETURNDATASIZE\",",
            "        \"DUP2\",",
            "        \"PUSH1\",",
            "        0x09,",
            "        \"RETURNDATASIZE\",",
            "        \"CODECOPY\",",
            "        \"RETURN\",",
            "    ]",
            "    forwarder_pre_asm = [",
            "        \"CALLDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"CALLDATACOPY\",",
            "        \"RETURNDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"CALLDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"PUSH20\",  # [address to delegate to]",
            "    ]",
            "    forwarder_post_asm = [",
            "        \"GAS\",",
            "        \"DELEGATECALL\",",
            "        \"RETURNDATASIZE\",",
            "        \"DUP3\",",
            "        \"DUP1\",",
            "        \"RETURNDATACOPY\",",
            "        \"SWAP1\",",
            "        \"RETURNDATASIZE\",",
            "        \"SWAP2\",",
            "        \"PUSH1\",",
            "        0x2B,  # jumpdest of whole program.",
            "        \"JUMPI\",",
            "        \"REVERT\",",
            "        \"JUMPDEST\",",
            "        \"RETURN\",",
            "    ]",
            "    return (",
            "        assembly_to_evm(loader_asm)[0],",
            "        assembly_to_evm(forwarder_pre_asm)[0],",
            "        assembly_to_evm(forwarder_post_asm)[0],",
            "    )",
            "",
            "",
            "# \"standard\" initcode for code which can be larger than 256 bytes.",
            "# returns the code starting from 0x0b with len `codesize`.",
            "# NOTE: it assumes codesize <= 2**24.",
            "def _create_preamble(codesize):",
            "    from vyper.ir.compile_ir import assembly_to_evm",
            "",
            "    evm_len = 0x0B  # 11 bytes",
            "    asm = [",
            "        # use PUSH3 to be able to deal with larger contracts",
            "        \"PUSH3\",",
            "        # blank space for codesize",
            "        0x00,",
            "        0x00,",
            "        0x00,",
            "        \"RETURNDATASIZE\",",
            "        \"DUP2\",",
            "        \"PUSH1\",",
            "        evm_len,",
            "        \"RETURNDATASIZE\",",
            "        \"CODECOPY\",",
            "        \"RETURN\",",
            "    ]",
            "    evm = assembly_to_evm(asm)[0]",
            "    assert len(evm) == evm_len, evm",
            "",
            "    shl_bits = (evm_len - 4) * 8  # codesize needs to go right after the PUSH3",
            "    # mask codesize into the aforementioned \"blank space\"",
            "    return [\"or\", bytes_to_int(evm), shl(shl_bits, codesize)], evm_len",
            "",
            "",
            "class _CreateBase(BuiltinFunctionT):",
            "    _kwargs = {",
            "        \"value\": KwargSettings(UINT256_T, zero_value),",
            "        \"salt\": KwargSettings(BYTES32_T, empty_value),",
            "        \"revert_on_failure\": KwargSettings(BoolT(), True, require_literal=True),",
            "    }",
            "    _return_type = AddressT()",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # errmsg something like f\"Cannot use {self._id} in pure fn\"",
            "        context.check_is_not_constant(f\"use {self._id}\", expr)",
            "",
            "        should_use_create2 = \"salt\" in [kwarg.arg for kwarg in expr.keywords]",
            "",
            "        if not should_use_create2:",
            "            kwargs[\"salt\"] = CREATE2_SENTINEL",
            "",
            "        ir_builder = self._build_create_IR(expr, args, context, **kwargs)",
            "",
            "        add_gas_estimate = self._add_gas_estimate(args, should_use_create2)",
            "",
            "        return IRnode.from_list(",
            "            ir_builder, typ=AddressT(), annotation=self._id, add_gas_estimate=add_gas_estimate",
            "        )",
            "",
            "",
            "class CreateMinimalProxyTo(_CreateBase):",
            "    # create an EIP1167 \"minimal proxy\" to the target contract",
            "",
            "    _id = \"create_minimal_proxy_to\"",
            "    _inputs = [(\"target\", AddressT())]",
            "",
            "    def _add_gas_estimate(self, args, should_use_create2):",
            "        a, b, c = eip1167_bytecode()",
            "        bytecode_len = 20 + len(b) + len(c)",
            "        return _create_addl_gas_estimate(bytecode_len, should_use_create2)",
            "",
            "    def _build_create_IR(self, expr, args, context, value, salt, revert_on_failure):",
            "        target_address = args[0]",
            "",
            "        buf = context.new_internal_variable(BytesT(96))",
            "",
            "        loader_evm, forwarder_pre_evm, forwarder_post_evm = eip1167_bytecode()",
            "        # Adjust to 32-byte boundaries",
            "        preamble_length = len(loader_evm) + len(forwarder_pre_evm)",
            "        forwarder_preamble = bytes_to_int(",
            "            loader_evm + forwarder_pre_evm + b\"\\x00\" * (32 - preamble_length)",
            "        )",
            "        forwarder_post = bytes_to_int(forwarder_post_evm + b\"\\x00\" * (32 - len(forwarder_post_evm)))",
            "",
            "        # left-align the target",
            "        if target_address.is_literal:",
            "            # note: should move to optimizer once we have",
            "            # codesize optimization pipeline",
            "            aligned_target = args[0].value << 96",
            "        else:",
            "            aligned_target = shl(96, target_address)",
            "",
            "        buf_len = preamble_length + 20 + len(forwarder_post_evm)",
            "",
            "        return [",
            "            \"seq\",",
            "            [\"mstore\", buf, forwarder_preamble],",
            "            [\"mstore\", add_ofst(buf, preamble_length), aligned_target],",
            "            [\"mstore\", add_ofst(buf, preamble_length + 20), forwarder_post],",
            "            _create_ir(value, buf, buf_len, salt, revert_on_failure),",
            "        ]",
            "",
            "",
            "class CreateForwarderTo(CreateMinimalProxyTo):",
            "    _warned = False",
            "",
            "    def build_IR(self, expr, context):",
            "        if not self._warned:",
            "            vyper_warn(",
            "                \"`create_forwarder_to` is a deprecated alias of `create_minimal_proxy_to`!\", expr",
            "            )",
            "            self._warned = True",
            "",
            "        return super().build_IR(expr, context)",
            "",
            "",
            "class CreateCopyOf(_CreateBase):",
            "    _id = \"create_copy_of\"",
            "    _inputs = [(\"target\", AddressT())]",
            "",
            "    @property",
            "    def _preamble_len(self):",
            "        return 11",
            "",
            "    def _add_gas_estimate(self, args, should_use_create2):",
            "        # max possible runtime length + preamble length",
            "        return _create_addl_gas_estimate(EIP_170_LIMIT + self._preamble_len, should_use_create2)",
            "",
            "    def _build_create_IR(self, expr, args, context, value, salt, revert_on_failure):",
            "        target = args[0]",
            "",
            "        # something we can pass to scope_multi",
            "        with scope_multi(",
            "            (target, value, salt), (\"create_target\", \"create_value\", \"create_salt\")",
            "        ) as (b1, (target, value, salt)):",
            "            codesize = IRnode.from_list([\"extcodesize\", target])",
            "            msize = IRnode.from_list([\"msize\"])",
            "            with scope_multi((codesize, msize), (\"target_codesize\", \"mem_ofst\")) as (",
            "                b2,",
            "                (codesize, mem_ofst),",
            "            ):",
            "                ir = [\"seq\"]",
            "",
            "                # make sure there is actually code at the target",
            "                check_codesize = [\"assert\", codesize]",
            "                ir.append(",
            "                    IRnode.from_list(check_codesize, error_msg=\"empty target (create_copy_of)\")",
            "                )",
            "",
            "                # store the preamble at msize + 22 (zero padding)",
            "                preamble, preamble_len = _create_preamble(codesize)",
            "                assert preamble_len == self._preamble_len",
            "",
            "                ir.append([\"mstore\", mem_ofst, preamble])",
            "",
            "                # copy the target code into memory. current layout:",
            "                # msize | 00...00 (22 0's) | preamble | bytecode",
            "                ir.append([\"extcodecopy\", target, add_ofst(mem_ofst, 32), 0, codesize])",
            "",
            "                buf = add_ofst(mem_ofst, 32 - preamble_len)",
            "                buf_len = [\"add\", codesize, preamble_len]",
            "",
            "                ir.append(_create_ir(value, buf, buf_len, salt, revert_on_failure))",
            "",
            "                return b1.resolve(b2.resolve(ir))",
            "",
            "",
            "class CreateFromBlueprint(_CreateBase):",
            "    _id = \"create_from_blueprint\"",
            "    _inputs = [(\"target\", AddressT())]",
            "    _kwargs = {",
            "        \"value\": KwargSettings(UINT256_T, zero_value),",
            "        \"salt\": KwargSettings(BYTES32_T, empty_value),",
            "        \"raw_args\": KwargSettings(BoolT(), False, require_literal=True),",
            "        \"code_offset\": KwargSettings(UINT256_T, IRnode.from_list(3, typ=UINT256_T)),",
            "        \"revert_on_failure\": KwargSettings(BoolT(), True, require_literal=True),",
            "    }",
            "    _has_varargs = True",
            "",
            "    def _add_gas_estimate(self, args, should_use_create2):",
            "        ctor_args = ir_tuple_from_args(args[1:])",
            "        # max possible size of init code",
            "        maxlen = EIP_170_LIMIT + ctor_args.typ.abi_type.size_bound()",
            "        return _create_addl_gas_estimate(maxlen, should_use_create2)",
            "",
            "    def _build_create_IR(",
            "        self, expr, args, context, value, salt, code_offset, raw_args, revert_on_failure",
            "    ):",
            "        target = args[0]",
            "        ctor_args = args[1:]",
            "",
            "        ctor_args = [ensure_in_memory(arg, context) for arg in ctor_args]",
            "",
            "        if raw_args:",
            "            if len(ctor_args) != 1 or not isinstance(ctor_args[0].typ, BytesT):",
            "                raise StructureException(\"raw_args must be used with exactly 1 bytes argument\")",
            "",
            "            with ctor_args[0].cache_when_complex(\"arg\") as (b1, arg):",
            "                argbuf = bytes_data_ptr(arg)",
            "                argslen = get_bytearray_length(arg)",
            "                bufsz = arg.typ.maxlen",
            "                return b1.resolve(",
            "                    self._helper(",
            "                        argbuf, bufsz, target, value, salt, argslen, code_offset, revert_on_failure",
            "                    )",
            "                )",
            "        else:",
            "            # encode the varargs",
            "            to_encode = ir_tuple_from_args(ctor_args)",
            "",
            "            # pretend we allocated enough memory for the encoder",
            "            # (we didn't, but we are clobbering unused memory so it's safe.)",
            "            bufsz = to_encode.typ.abi_type.size_bound()",
            "            argbuf = context.new_internal_variable(get_type_for_exact_size(bufsz))",
            "",
            "            # return a complex expression which writes to memory and returns",
            "            # the length of the encoded data",
            "            argslen = abi_encode(argbuf, to_encode, context, bufsz=bufsz, returns_len=True)",
            "            return self._helper(",
            "                argbuf, bufsz, target, value, salt, argslen, code_offset, revert_on_failure",
            "            )",
            "",
            "    def _helper(self, argbuf, bufsz, target, value, salt, argslen, code_offset, revert_on_failure):",
            "        # NOTE: we need to invoke the abi encoder before evaluating MSIZE,",
            "        # then copy the abi encoded buffer to past-the-end of the initcode",
            "        # (since the abi encoder could write to fresh memory).",
            "        # it would be good to not require the memory copy, but need",
            "        # to evaluate memory safety.",
            "        with scope_multi(",
            "            (target, value, salt, argslen, code_offset),",
            "            (\"create_target\", \"create_value\", \"create_salt\", \"encoded_args_len\", \"code_offset\"),",
            "        ) as (b1, (target, value, salt, encoded_args_len, code_offset)):",
            "            codesize = IRnode.from_list([\"sub\", [\"extcodesize\", target], code_offset])",
            "            # copy code to memory starting from msize. we are clobbering",
            "            # unused memory so it's safe.",
            "            msize = IRnode.from_list([\"msize\"], location=MEMORY)",
            "            with scope_multi((codesize, msize), (\"target_codesize\", \"mem_ofst\")) as (",
            "                b2,",
            "                (codesize, mem_ofst),",
            "            ):",
            "                ir = [\"seq\"]",
            "",
            "                # make sure there is code at the target, and that",
            "                # code_ofst <= (extcodesize target).",
            "                # (note if code_ofst > (extcodesize target), would be",
            "                # OOG on the EXTCODECOPY)",
            "                # (code_ofst == (extcodesize target) would be empty",
            "                # initcode, which we disallow for hygiene reasons -",
            "                # same as `create_copy_of` on an empty target).",
            "                check_codesize = [\"assert\", [\"sgt\", codesize, 0]]",
            "                ir.append(",
            "                    IRnode.from_list(",
            "                        check_codesize, error_msg=\"empty target (create_from_blueprint)\"",
            "                    )",
            "                )",
            "",
            "                # copy the target code into memory.",
            "                # layout starting from mem_ofst:",
            "                # <target initcode> | <abi-encoded args OR arg buffer if raw_arg=True>",
            "                ir.append([\"extcodecopy\", target, mem_ofst, code_offset, codesize])",
            "                ir.append(copy_bytes(add_ofst(mem_ofst, codesize), argbuf, encoded_args_len, bufsz))",
            "",
            "                # theoretically, dst = \"msize\", but just be safe.",
            "                # if len(ctor_args) > 0:",
            "                #    dst = add_ofst(mem_ofst, codesize)",
            "                #    encoded_args_len = self._encode_args(dst, ctor_args, context)",
            "                # else:",
            "                #    encoded_args_len = 0",
            "",
            "                length = [\"add\", codesize, encoded_args_len]",
            "",
            "                ir.append(_create_ir(value, mem_ofst, length, salt, revert_on_failure))",
            "",
            "                return b1.resolve(b2.resolve(ir))",
            "",
            "",
            "class _UnsafeMath(BuiltinFunctionT):",
            "    # TODO add unsafe math for `decimal`s",
            "    _inputs = [(\"a\", IntegerT.any()), (\"b\", IntegerT.any())]",
            "",
            "    def __repr__(self):",
            "        return f\"builtin function unsafe_{self.op}\"",
            "",
            "    def fetch_call_return(self, node):",
            "        return_type = self.infer_arg_types(node).pop()",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "",
            "        types_list = get_common_types(*node.args, filter_fn=lambda x: isinstance(x, IntegerT))",
            "        if not types_list:",
            "            raise TypeMismatch(f\"unsafe_{self.op} called on dislike types\", node)",
            "",
            "        type_ = types_list.pop()",
            "        return [type_, type_]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        (a, b) = args",
            "        op = self.op",
            "",
            "        assert a.typ == b.typ, \"unreachable\"",
            "",
            "        otyp = a.typ",
            "",
            "        if op == \"div\" and a.typ.is_signed:",
            "            op = \"sdiv\"",
            "",
            "        ret = [op, a, b]",
            "",
            "        if a.typ.bits < 256:",
            "            # wrap for ops which could under/overflow",
            "            if a.typ.is_signed:",
            "                # e.g. int128 -> (signextend 15 (add x y))",
            "                ret = promote_signed_int(ret, a.typ.bits)",
            "            else:",
            "                # e.g. uint8 -> (mod (add x y) 256)",
            "                # TODO mod_bound could be a really large literal",
            "                ret = [\"mod\", ret, 2**a.typ.bits]",
            "",
            "        return IRnode.from_list(ret, typ=otyp)",
            "",
            "        # TODO handle decimal case",
            "",
            "",
            "class UnsafeAdd(_UnsafeMath):",
            "    _id = \"unsafe_add\"",
            "    op = \"add\"",
            "",
            "",
            "class UnsafeSub(_UnsafeMath):",
            "    _id = \"unsafe_sub\"",
            "    op = \"sub\"",
            "",
            "",
            "class UnsafeMul(_UnsafeMath):",
            "    _id = \"unsafe_mul\"",
            "    op = \"mul\"",
            "",
            "",
            "class UnsafeDiv(_UnsafeMath):",
            "    _id = \"unsafe_div\"",
            "    op = \"div\"",
            "",
            "",
            "class _MinMax(BuiltinFunctionT):",
            "    _inputs = [(\"a\", (DecimalT(), IntegerT.any())), (\"b\", (DecimalT(), IntegerT.any()))]",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 2)",
            "",
            "        left = node.args[0].get_folded_value()",
            "        right = node.args[1].get_folded_value()",
            "        if not isinstance(left, type(right)):",
            "            raise UnfoldableNode",
            "        if not isinstance(left, (vy_ast.Decimal, vy_ast.Int)):",
            "            raise UnfoldableNode",
            "",
            "        types_list = get_common_types(",
            "            *(left, right), filter_fn=lambda x: isinstance(x, (IntegerT, DecimalT))",
            "        )",
            "        if not types_list:",
            "            raise TypeMismatch(\"Cannot perform action between dislike numeric types\", node)",
            "",
            "        value = self._eval_fn(left.value, right.value)",
            "        return type(left).from_node(node, value=value)",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "",
            "        types_list = get_common_types(",
            "            *node.args, filter_fn=lambda x: isinstance(x, (IntegerT, DecimalT))",
            "        )",
            "        if not types_list:",
            "            raise TypeMismatch(\"Cannot perform action between dislike numeric types\", node)",
            "",
            "        return types_list",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        types_list = self.fetch_call_return(node)",
            "        # type mismatch should have been caught in `fetch_call_return`",
            "        assert expected_return_typ in types_list",
            "        return [expected_return_typ, expected_return_typ]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        op = self._opcode",
            "",
            "        with args[0].cache_when_complex(\"_l\") as (b1, left), args[1].cache_when_complex(\"_r\") as (",
            "            b2,",
            "            right,",
            "        ):",
            "            if left.typ == right.typ:",
            "                if left.typ != UINT256_T:",
            "                    # if comparing like types that are not uint256, use SLT or SGT",
            "                    op = f\"s{op}\"",
            "                o = [\"select\", [op, left, right], left, right]",
            "                otyp = left.typ",
            "",
            "            else:",
            "                raise TypeMismatch(f\"Minmax types incompatible: {left.typ.typ} {right.typ.typ}\")",
            "            return IRnode.from_list(b1.resolve(b2.resolve(o)), typ=otyp)",
            "",
            "",
            "class Min(_MinMax):",
            "    _id = \"min\"",
            "    _eval_fn = min",
            "    _opcode = \"lt\"",
            "",
            "",
            "class Max(_MinMax):",
            "    _id = \"max\"",
            "    _eval_fn = max",
            "    _opcode = \"gt\"",
            "",
            "",
            "class Uint2Str(BuiltinFunctionT):",
            "    _id = \"uint2str\"",
            "    _inputs = [(\"x\", IntegerT.unsigneds())]",
            "",
            "    def fetch_call_return(self, node):",
            "        arg_t = self.infer_arg_types(node)[0]",
            "        bits = arg_t.bits",
            "        len_needed = math.ceil(bits * math.log(2) / math.log(10))",
            "        return StringT(len_needed)",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Int):",
            "            raise UnfoldableNode",
            "",
            "        value = value.value",
            "        if value < 0:",
            "            raise InvalidType(\"Only unsigned ints allowed\", node)",
            "        value = str(value)",
            "        return vy_ast.Str.from_node(node, value=value)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        input_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [input_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return_t = self.fetch_call_return(expr)",
            "        n_digits = return_t.maxlen",
            "",
            "        with args[0].cache_when_complex(\"val\") as (b1, val):",
            "            buf = context.new_internal_variable(return_t)",
            "",
            "            i = IRnode.from_list(context.fresh_varname(\"uint2str_i\"), typ=UINT256_T)",
            "",
            "            ret = [\"repeat\", i, 0, n_digits + 1, n_digits + 1]",
            "",
            "            body = [",
            "                \"seq\",",
            "                [",
            "                    \"if\",",
            "                    [\"eq\", val, 0],",
            "                    # clobber val, and return it as a pointer",
            "                    [",
            "                        \"seq\",",
            "                        [\"mstore\", [\"sub\", add_ofst(buf, n_digits), i], i],",
            "                        [\"set\", val, [\"sub\", add_ofst(buf, n_digits), i]],",
            "                        \"break\",",
            "                    ],",
            "                    [",
            "                        \"seq\",",
            "                        [",
            "                            \"mstore\",",
            "                            [\"sub\", add_ofst(buf, n_digits), i],",
            "                            [\"add\", 48, [\"mod\", val, 10]],",
            "                        ],",
            "                        [\"set\", val, [\"div\", val, 10]],",
            "                    ],",
            "                ],",
            "            ]",
            "            ret.append(body)",
            "",
            "            # \"0\" has hex representation 0x00..0130..00",
            "            # if (val == 0) {",
            "            #   return \"0\"",
            "            # } else {",
            "            #   do the loop",
            "            # }",
            "            ret = [",
            "                \"if\",",
            "                [\"eq\", val, 0],",
            "                [\"seq\", [\"mstore\", add_ofst(buf, 1), ord(\"0\")], [\"mstore\", buf, 1], buf],",
            "                [\"seq\", ret, val],",
            "            ]",
            "",
            "            return b1.resolve(IRnode.from_list(ret, location=MEMORY, typ=return_t))",
            "",
            "",
            "class Sqrt(BuiltinFunctionT):",
            "    _id = \"sqrt\"",
            "    _inputs = [(\"d\", DecimalT())]",
            "    _return_type = DecimalT()",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # TODO fix cyclic dependency with codegen/stmt.py",
            "        from ._utils import generate_inline_function",
            "",
            "        arg = args[0]",
            "        # TODO: reify decimal and integer sqrt paths (see isqrt)",
            "        with arg.cache_when_complex(\"x\") as (b1, arg):",
            "            sqrt_code = \"\"\"",
            "assert x >= 0.0",
            "z: decimal = 0.0",
            "",
            "if x == 0.0:",
            "    z = 0.0",
            "else:",
            "    z = x / 2.0 + 0.5",
            "    y: decimal = x",
            "",
            "    for i: uint256 in range(256):",
            "        if z == y:",
            "            break",
            "        y = z",
            "        z = (x / z + z) / 2.0",
            "            \"\"\"",
            "",
            "            x_type = DecimalT()",
            "            placeholder_copy = [\"pass\"]",
            "            # Steal current position if variable is already allocated.",
            "            if arg.value == \"mload\":",
            "                new_var_pos = arg.args[0]",
            "            # Other locations need to be copied.",
            "            else:",
            "                new_var_pos = context.new_internal_variable(x_type)",
            "                placeholder_copy = [\"mstore\", new_var_pos, arg]",
            "            # Create input variables.",
            "            variables = {\"x\": VariableRecord(name=\"x\", pos=new_var_pos, typ=x_type, mutable=False)}",
            "            # Dictionary to update new (i.e. typecheck) namespace",
            "            variables_2 = {\"x\": VarInfo(DecimalT())}",
            "            # Generate inline IR.",
            "            new_ctx, sqrt_ir = generate_inline_function(",
            "                code=sqrt_code,",
            "                variables=variables,",
            "                variables_2=variables_2,",
            "                memory_allocator=context.memory_allocator,",
            "            )",
            "            z_ir = new_ctx.vars[\"z\"].as_ir_node()",
            "            ret = IRnode.from_list(",
            "                [\"seq\", placeholder_copy, sqrt_ir, z_ir], typ=DecimalT(), location=MEMORY",
            "            )",
            "            return b1.resolve(ret)",
            "",
            "",
            "class ISqrt(BuiltinFunctionT):",
            "    _id = \"isqrt\"",
            "    _inputs = [(\"d\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # calculate isqrt using the babylonian method",
            "",
            "        y, z = \"y\", \"z\"",
            "        arg = args[0]",
            "        with arg.cache_when_complex(\"x\") as (b1, x):",
            "            ret = [",
            "                \"seq\",",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (128 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(128, y)], [\"set\", z, shl(64, z)]],",
            "                ],",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (64 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(64, y)], [\"set\", z, shl(32, z)]],",
            "                ],",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (32 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(32, y)], [\"set\", z, shl(16, z)]],",
            "                ],",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (16 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(16, y)], [\"set\", z, shl(8, z)]],",
            "                ],",
            "            ]",
            "            ret.append([\"set\", z, [\"div\", [\"mul\", z, [\"add\", y, 2**16]], 2**18]])",
            "",
            "            for _ in range(7):",
            "                ret.append([\"set\", z, [\"div\", [\"add\", [\"div\", x, z], z], 2]])",
            "",
            "            # note: If ``x+1`` is a perfect square, then the Babylonian",
            "            # algorithm oscillates between floor(sqrt(x)) and ceil(sqrt(x)) in",
            "            # consecutive iterations. return the floor value always.",
            "",
            "            ret.append([\"with\", \"t\", [\"div\", x, z], [\"select\", [\"lt\", z, \"t\"], z, \"t\"]])",
            "",
            "            ret = [\"with\", y, x, [\"with\", z, 181, ret]]",
            "            return b1.resolve(IRnode.from_list(ret, typ=UINT256_T))",
            "",
            "",
            "class Empty(TypenameFoldedFunctionT):",
            "    _id = \"empty\"",
            "",
            "    def fetch_call_return(self, node):",
            "        type_ = self.infer_arg_types(node)[0].typedef",
            "        if isinstance(type_, HashMapT):",
            "            raise TypeMismatch(\"Cannot use empty on HashMap\", node)",
            "        return type_",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        output_type = args[0]",
            "        return IRnode(\"~empty\", typ=output_type)",
            "",
            "",
            "class Breakpoint(BuiltinFunctionT):",
            "    _id = \"breakpoint\"",
            "    _inputs: list = []",
            "",
            "    _warned = False",
            "",
            "    def fetch_call_return(self, node):",
            "        if not self._warned:",
            "            vyper_warn(\"`breakpoint` should only be used for debugging!\", node)",
            "            self._warned = True",
            "",
            "        return None",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list(\"breakpoint\", annotation=\"breakpoint()\")",
            "",
            "",
            "class Print(BuiltinFunctionT):",
            "    _id = \"print\"",
            "    _inputs: list = []",
            "    _has_varargs = True",
            "    _kwargs = {\"hardhat_compat\": KwargSettings(BoolT(), False, require_literal=True)}",
            "",
            "    _warned = False",
            "",
            "    def fetch_call_return(self, node):",
            "        if not self._warned:",
            "            vyper_warn(\"`print` should only be used for debugging!\", node)",
            "            self._warned = True",
            "",
            "        return None",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        args_as_tuple = ir_tuple_from_args(args)",
            "        args_abi_t = args_as_tuple.typ.abi_type",
            "",
            "        # create a signature like \"log(uint256)\"",
            "        sig = \"log\" + \"(\" + \",\".join([arg.typ.abi_type.selector_name() for arg in args]) + \")\"",
            "",
            "        if kwargs[\"hardhat_compat\"] is True:",
            "            method_id = method_id_int(sig)",
            "            buflen = 32 + args_abi_t.size_bound()",
            "",
            "            # 32 bytes extra space for the method id",
            "            buf = context.new_internal_variable(get_type_for_exact_size(buflen))",
            "",
            "            ret = [\"seq\"]",
            "            ret.append([\"mstore\", buf, method_id])",
            "            encode = abi_encode(add_ofst(buf, 32), args_as_tuple, context, buflen, returns_len=True)",
            "",
            "        else:",
            "            method_id = method_id_int(\"log(string,bytes)\")",
            "            schema = args_abi_t.selector_name().encode(\"utf-8\")",
            "            if len(schema) > 32:",
            "                raise CompilerPanic(f\"print signature too long: {schema}\")",
            "",
            "            schema_t = StringT(len(schema))",
            "            schema_buf = context.new_internal_variable(schema_t)",
            "            ret = [\"seq\"]",
            "            ret.append([\"mstore\", schema_buf, len(schema)])",
            "",
            "            # TODO use Expr.make_bytelike, or better have a `bytestring` IRnode type",
            "            ret.append(",
            "                [\"mstore\", add_ofst(schema_buf, 32), bytes_to_int(schema.ljust(32, b\"\\x00\"))]",
            "            )",
            "",
            "            payload_buflen = args_abi_t.size_bound()",
            "            payload_t = BytesT(payload_buflen)",
            "",
            "            # 32 bytes extra space for the method id",
            "            payload_buf = context.new_internal_variable(payload_t)",
            "            encode_payload = abi_encode(",
            "                add_ofst(payload_buf, 32), args_as_tuple, context, payload_buflen, returns_len=True",
            "            )",
            "",
            "            ret.append([\"mstore\", payload_buf, encode_payload])",
            "            args_as_tuple = ir_tuple_from_args(",
            "                [",
            "                    IRnode.from_list(schema_buf, typ=schema_t, location=MEMORY),",
            "                    IRnode.from_list(payload_buf, typ=payload_t, location=MEMORY),",
            "                ]",
            "            )",
            "",
            "            # add 32 for method id padding",
            "            buflen = 32 + args_as_tuple.typ.abi_type.size_bound()",
            "            buf = context.new_internal_variable(get_type_for_exact_size(buflen))",
            "            ret.append([\"mstore\", buf, method_id])",
            "            encode = abi_encode(add_ofst(buf, 32), args_as_tuple, context, buflen, returns_len=True)",
            "",
            "        # debug address that tooling uses",
            "        CONSOLE_ADDRESS = 0x000000000000000000636F6E736F6C652E6C6F67",
            "        ret.append(",
            "            [\"staticcall\", \"gas\", CONSOLE_ADDRESS, add_ofst(buf, 28), [\"add\", 4, encode], 0, 0]",
            "        )",
            "",
            "        return IRnode.from_list(ret, annotation=\"print:\" + sig)",
            "",
            "",
            "class ABIEncode(BuiltinFunctionT):",
            "    _id = \"abi_encode\"",
            "    # signature: *, ensure_tuple=<literal_bool> -> Bytes[<calculated len>]",
            "    # explanation of ensure_tuple:",
            "    # default is to force even a single value into a tuple,",
            "    # e.g. _abi_encode(bytes) -> _abi_encode((bytes,))",
            "    #      _abi_encode((bytes,)) -> _abi_encode(((bytes,),))",
            "    # this follows the encoding convention for functions:",
            "    # ://docs.soliditylang.org/en/v0.8.6/abi-spec.html#function-selector-and-argument-encoding",
            "    # if this is turned off, then bytes will be encoded as bytes.",
            "",
            "    _inputs: list = []",
            "    _has_varargs = True",
            "",
            "    _kwargs = {",
            "        \"ensure_tuple\": KwargSettings(BoolT(), True, require_literal=True),",
            "        \"method_id\": KwargSettings((BYTES4_T, BytesT(4)), None, require_literal=True),",
            "    }",
            "",
            "    def infer_kwarg_types(self, node):",
            "        ret = {}",
            "        for kwarg in node.keywords:",
            "            kwarg_name = kwarg.arg",
            "            validate_expected_type(kwarg.value, self._kwargs[kwarg_name].typ)",
            "",
            "            typ = get_exact_type_from_node(kwarg.value)",
            "            if kwarg_name == \"method_id\" and isinstance(typ, BytesT):",
            "                if typ.length != 4:",
            "                    raise InvalidLiteral(\"method_id must be exactly 4 bytes!\", kwarg.value)",
            "",
            "            ret[kwarg_name] = typ",
            "        return ret",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "        ensure_tuple = next(",
            "            (arg.value.value for arg in node.keywords if arg.arg == \"ensure_tuple\"), True",
            "        )",
            "        assert isinstance(ensure_tuple, bool)",
            "        has_method_id = \"method_id\" in [arg.arg for arg in node.keywords]",
            "",
            "        # figure out the output type by converting",
            "        # the types to ABI_Types and calling size_bound API",
            "        arg_abi_types = []",
            "        arg_types = self.infer_arg_types(node)",
            "        for arg_t in arg_types:",
            "            arg_abi_types.append(arg_t.abi_type)",
            "",
            "        # special case, no tuple",
            "        if len(arg_abi_types) == 1 and not ensure_tuple:",
            "            arg_abi_t = arg_abi_types[0]",
            "        else:",
            "            arg_abi_t = ABI_Tuple(arg_abi_types)",
            "",
            "        maxlen = arg_abi_t.size_bound()",
            "",
            "        if has_method_id:",
            "            # the output includes 4 bytes for the method_id.",
            "            maxlen += 4",
            "",
            "        ret = BytesT()",
            "        ret.set_length(maxlen)",
            "        return ret",
            "",
            "    @staticmethod",
            "    def _parse_method_id(method_id_literal):",
            "        if method_id_literal is None:",
            "            return None",
            "        if isinstance(method_id_literal, bytes):",
            "            assert len(method_id_literal) == 4",
            "            return fourbytes_to_int(method_id_literal)",
            "        if method_id_literal.startswith(\"0x\"):",
            "            method_id_literal = method_id_literal[2:]",
            "        return fourbytes_to_int(bytes.fromhex(method_id_literal))",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        ensure_tuple = kwargs[\"ensure_tuple\"]",
            "        method_id = self._parse_method_id(kwargs[\"method_id\"])",
            "",
            "        if len(args) < 1:",
            "            raise StructureException(\"abi_encode expects at least one argument\", expr)",
            "",
            "        # figure out the required length for the output buffer",
            "        if len(args) == 1 and not ensure_tuple:",
            "            # special case, no tuple",
            "            encode_input = args[0]",
            "        else:",
            "            encode_input = ir_tuple_from_args(args)",
            "",
            "        input_abi_t = encode_input.typ.abi_type",
            "        maxlen = input_abi_t.size_bound()",
            "        if method_id is not None:",
            "            maxlen += 4",
            "",
            "        buf_t = BytesT(maxlen)",
            "        assert self.fetch_call_return(expr).length == maxlen",
            "        buf = context.new_internal_variable(buf_t)",
            "",
            "        ret = [\"seq\"]",
            "        if method_id is not None:",
            "            # <32 bytes length> | <4 bytes method_id> | <everything else>",
            "            # write the unaligned method_id first, then we will",
            "            # overwrite the 28 bytes of zeros with the bytestring length",
            "            ret += [[\"mstore\", add_ofst(buf, 4), method_id]]",
            "            # abi encode, and grab length as stack item",
            "            length = abi_encode(",
            "                add_ofst(buf, 36), encode_input, context, returns_len=True, bufsz=maxlen",
            "            )",
            "            # write the output length to where bytestring stores its length",
            "            ret += [[\"mstore\", buf, [\"add\", length, 4]]]",
            "",
            "        else:",
            "            # abi encode and grab length as stack item",
            "            length = abi_encode(",
            "                add_ofst(buf, 32), encode_input, context, returns_len=True, bufsz=maxlen",
            "            )",
            "            # write the output length to where bytestring stores its length",
            "            ret += [[\"mstore\", buf, length]]",
            "",
            "        # return the buf location",
            "        # TODO location is statically known, optimize this out",
            "        ret += [buf]",
            "",
            "        return IRnode.from_list(ret, location=MEMORY, typ=buf_t)",
            "",
            "",
            "class ABIDecode(BuiltinFunctionT):",
            "    _id = \"abi_decode\"",
            "    _inputs = [(\"data\", BytesT.any()), (\"output_type\", TYPE_T.any())]",
            "    _kwargs = {\"unwrap_tuple\": KwargSettings(BoolT(), True, require_literal=True)}",
            "",
            "    def fetch_call_return(self, node):",
            "        _, output_type = self.infer_arg_types(node)",
            "        return output_type.typedef",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "",
            "        validate_call_args(node, 2, [\"unwrap_tuple\"])",
            "",
            "        data_type = get_exact_type_from_node(node.args[0])",
            "        output_type = type_from_annotation(node.args[1])",
            "",
            "        return [data_type, TYPE_T(output_type)]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        unwrap_tuple = kwargs[\"unwrap_tuple\"]",
            "",
            "        data = args[0]",
            "        output_typ = args[1]",
            "        wrapped_typ = output_typ",
            "",
            "        if unwrap_tuple is True:",
            "            wrapped_typ = calculate_type_for_external_return(output_typ)",
            "",
            "        abi_size_bound = wrapped_typ.abi_type.size_bound()",
            "        abi_min_size = wrapped_typ.abi_type.static_size()",
            "",
            "        # Get the size of data",
            "        input_max_len = data.typ.maxlen",
            "",
            "        assert abi_min_size <= abi_size_bound, \"bad abi type\"",
            "        if input_max_len < abi_size_bound:",
            "            raise StructureException(",
            "                (",
            "                    \"Mismatch between size of input and size of decoded types. \"",
            "                    f\"length of ABI-encoded {wrapped_typ} must be equal to or greater \"",
            "                    f\"than {abi_size_bound}\"",
            "                ),",
            "                expr.args[0],",
            "            )",
            "",
            "        data = ensure_in_memory(data, context)",
            "",
            "        with data.cache_when_complex(\"to_decode\") as (b1, data):",
            "            data_ptr = bytes_data_ptr(data)",
            "            data_len = get_bytearray_length(data)",
            "",
            "            ret = [\"seq\"]",
            "",
            "            # NOTE: we could replace these 4 lines with",
            "            # `[assert [le, abi_min_size, data_len]]`. it depends on",
            "            # what we consider a \"valid\" payload.",
            "            # cf. test_abi_decode_max_size()",
            "            if abi_min_size == abi_size_bound:",
            "                ret.append([\"assert\", [\"eq\", abi_min_size, data_len]])",
            "            else:",
            "                # runtime assert: abi_min_size <= data_len <= abi_size_bound",
            "                ret.append(clamp2(abi_min_size, data_len, abi_size_bound, signed=False))",
            "",
            "            to_decode = IRnode.from_list(",
            "                data_ptr,",
            "                typ=wrapped_typ,",
            "                location=data.location,",
            "                encoding=Encoding.ABI,",
            "                annotation=f\"abi_decode({output_typ})\",",
            "            )",
            "            to_decode.encoding = Encoding.ABI",
            "",
            "            # TODO optimization: skip make_setter when we don't need",
            "            # input validation",
            "",
            "            output_buf = context.new_internal_variable(wrapped_typ)",
            "",
            "            # sanity check buffer size for wrapped output type will not buffer overflow",
            "            assert wrapped_typ.memory_bytes_required == output_typ.memory_bytes_required",
            "",
            "            # pass a buffer bound to make_setter so appropriate oob",
            "            # validation is performed",
            "            buf_bound = add_ofst(data_ptr, data_len)",
            "            ret.append(make_setter(output_buf, to_decode, hi=buf_bound))",
            "",
            "            ret.append(output_buf)",
            "            # finalize. set the type and location for the return buffer.",
            "            # (note: unwraps the tuple type if necessary)",
            "            ret = IRnode.from_list(ret, typ=output_typ, location=MEMORY)",
            "            return b1.resolve(ret)",
            "",
            "",
            "class OldABIEncode(ABIEncode):",
            "    _warned = False",
            "    _id = \"_abi_encode\"",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(f\"`{self._id}()` is deprecated! Please use `{super()._id}()` instead.\", node)",
            "            self.__class__._warned = True",
            "        super()._try_fold(node)",
            "",
            "",
            "class OldABIDecode(ABIDecode):",
            "    _warned = False",
            "    _id = \"_abi_decode\"",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(f\"`{self._id}()` is deprecated! Please use `{super()._id}()` instead.\", node)",
            "            self.__class__._warned = True",
            "        super()._try_fold(node)",
            "",
            "",
            "class _MinMaxValue(TypenameFoldedFunctionT):",
            "    def _try_fold(self, node):",
            "        self._validate_arg_types(node)",
            "        input_type = type_from_annotation(node.args[0])",
            "",
            "        if not isinstance(input_type, (IntegerT, DecimalT)):",
            "            raise InvalidType(f\"Expected numeric type but got {input_type} instead\", node)",
            "",
            "        val = self._eval(input_type)",
            "",
            "        if isinstance(input_type, DecimalT):",
            "            ret = vy_ast.Decimal.from_node(node, value=val)",
            "",
            "        if isinstance(input_type, IntegerT):",
            "            ret = vy_ast.Int.from_node(node, value=val)",
            "",
            "        ret._metadata[\"type\"] = input_type",
            "        return ret",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        input_typedef = TYPE_T(type_from_annotation(node.args[0]))",
            "        return [input_typedef]",
            "",
            "",
            "class MinValue(_MinMaxValue):",
            "    _id = \"min_value\"",
            "",
            "    def _eval(self, type_):",
            "        return type_.ast_bounds[0]",
            "",
            "",
            "class MaxValue(_MinMaxValue):",
            "    _id = \"max_value\"",
            "",
            "    def _eval(self, type_):",
            "        return type_.ast_bounds[1]",
            "",
            "",
            "class Epsilon(TypenameFoldedFunctionT):",
            "    _id = \"epsilon\"",
            "",
            "    def _try_fold(self, node):",
            "        self._validate_arg_types(node)",
            "        input_type = type_from_annotation(node.args[0])",
            "",
            "        if not input_type.compare_type(DecimalT()):",
            "            raise InvalidType(f\"Expected decimal type but got {input_type} instead\", node)",
            "",
            "        return vy_ast.Decimal.from_node(node, value=input_type.epsilon)",
            "",
            "",
            "DISPATCH_TABLE = {",
            "    \"abi_encode\": ABIEncode(),",
            "    \"abi_decode\": ABIDecode(),",
            "    \"_abi_encode\": OldABIEncode(),",
            "    \"_abi_decode\": OldABIDecode(),",
            "    \"floor\": Floor(),",
            "    \"ceil\": Ceil(),",
            "    \"convert\": Convert(),",
            "    \"slice\": Slice(),",
            "    \"len\": Len(),",
            "    \"concat\": Concat(),",
            "    \"sha256\": Sha256(),",
            "    \"method_id\": MethodID(),",
            "    \"keccak256\": Keccak256(),",
            "    \"ecrecover\": ECRecover(),",
            "    \"ecadd\": ECAdd(),",
            "    \"ecmul\": ECMul(),",
            "    \"extract32\": Extract32(),",
            "    \"as_wei_value\": AsWeiValue(),",
            "    \"raw_call\": RawCall(),",
            "    \"blockhash\": BlockHash(),",
            "    \"blobhash\": BlobHash(),",
            "    \"bitwise_and\": BitwiseAnd(),",
            "    \"bitwise_or\": BitwiseOr(),",
            "    \"bitwise_xor\": BitwiseXor(),",
            "    \"bitwise_not\": BitwiseNot(),",
            "    \"uint256_addmod\": AddMod(),",
            "    \"uint256_mulmod\": MulMod(),",
            "    \"unsafe_add\": UnsafeAdd(),",
            "    \"unsafe_sub\": UnsafeSub(),",
            "    \"unsafe_mul\": UnsafeMul(),",
            "    \"unsafe_div\": UnsafeDiv(),",
            "    \"pow_mod256\": PowMod256(),",
            "    \"uint2str\": Uint2Str(),",
            "    \"isqrt\": ISqrt(),",
            "    \"sqrt\": Sqrt(),",
            "    \"shift\": Shift(),",
            "    \"create_minimal_proxy_to\": CreateMinimalProxyTo(),",
            "    \"create_forwarder_to\": CreateForwarderTo(),",
            "    \"create_copy_of\": CreateCopyOf(),",
            "    \"create_from_blueprint\": CreateFromBlueprint(),",
            "    \"min\": Min(),",
            "    \"max\": Max(),",
            "    \"empty\": Empty(),",
            "    \"abs\": Abs(),",
            "    \"min_value\": MinValue(),",
            "    \"max_value\": MaxValue(),",
            "    \"epsilon\": Epsilon(),",
            "}",
            "",
            "STMT_DISPATCH_TABLE = {",
            "    \"send\": Send(),",
            "    \"print\": Print(),",
            "    \"breakpoint\": Breakpoint(),",
            "    \"selfdestruct\": SelfDestruct(),",
            "    \"raw_call\": RawCall(),",
            "    \"raw_log\": RawLog(),",
            "    \"raw_revert\": RawRevert(),",
            "    \"create_minimal_proxy_to\": CreateMinimalProxyTo(),",
            "    \"create_forwarder_to\": CreateForwarderTo(),",
            "    \"create_copy_of\": CreateCopyOf(),",
            "    \"create_from_blueprint\": CreateFromBlueprint(),",
            "}",
            "",
            "BUILTIN_FUNCTIONS = {**STMT_DISPATCH_TABLE, **DISPATCH_TABLE}.keys()",
            "",
            "",
            "def get_builtin_functions():",
            "    return {**STMT_DISPATCH_TABLE, **DISPATCH_TABLE}"
        ],
        "afterPatchFile": [
            "import hashlib",
            "import math",
            "import operator",
            "",
            "from vyper import ast as vy_ast",
            "from vyper.abi_types import ABI_Tuple",
            "from vyper.ast.validation import validate_call_args",
            "from vyper.codegen.abi_encoder import abi_encode",
            "from vyper.codegen.context import Context, VariableRecord",
            "from vyper.codegen.core import (",
            "    LOAD,",
            "    STORE,",
            "    IRnode,",
            "    add_ofst,",
            "    bytes_data_ptr,",
            "    calculate_type_for_external_return,",
            "    check_buffer_overflow_ir,",
            "    check_external_call,",
            "    clamp,",
            "    clamp2,",
            "    clamp_basetype,",
            "    clamp_nonzero,",
            "    copy_bytes,",
            "    dummy_node_for_type,",
            "    ensure_eval_once,",
            "    ensure_in_memory,",
            "    eval_seq,",
            "    get_bytearray_length,",
            "    get_type_for_exact_size,",
            "    ir_tuple_from_args,",
            "    make_setter,",
            "    potential_overlap,",
            "    promote_signed_int,",
            "    sar,",
            "    shl,",
            "    shr,",
            "    unwrap_location,",
            ")",
            "from vyper.codegen.expr import Expr",
            "from vyper.codegen.ir_node import Encoding, scope_multi",
            "from vyper.codegen.keccak256_helper import keccak256_helper",
            "from vyper.evm.address_space import MEMORY",
            "from vyper.evm.opcodes import version_check",
            "from vyper.exceptions import (",
            "    ArgumentException,",
            "    CompilerPanic,",
            "    EvmVersionException,",
            "    InvalidLiteral,",
            "    InvalidType,",
            "    StateAccessViolation,",
            "    StructureException,",
            "    TypeMismatch,",
            "    UnfoldableNode,",
            "    ZeroDivisionException,",
            ")",
            "from vyper.semantics.analysis.base import Modifiability, VarInfo",
            "from vyper.semantics.analysis.utils import (",
            "    get_common_types,",
            "    get_exact_type_from_node,",
            "    get_possible_types_from_node,",
            "    validate_expected_type,",
            ")",
            "from vyper.semantics.types import (",
            "    TYPE_T,",
            "    AddressT,",
            "    BoolT,",
            "    BytesM_T,",
            "    BytesT,",
            "    DArrayT,",
            "    DecimalT,",
            "    HashMapT,",
            "    IntegerT,",
            "    KwargSettings,",
            "    SArrayT,",
            "    StringT,",
            "    TupleT,",
            ")",
            "from vyper.semantics.types.bytestrings import _BytestringT",
            "from vyper.semantics.types.shortcuts import (",
            "    BYTES4_T,",
            "    BYTES32_T,",
            "    INT128_T,",
            "    INT256_T,",
            "    UINT8_T,",
            "    UINT256_T,",
            ")",
            "from vyper.semantics.types.utils import type_from_annotation",
            "from vyper.utils import (",
            "    DECIMAL_DIVISOR,",
            "    EIP_170_LIMIT,",
            "    SHA3_PER_WORD,",
            "    MemoryPositions,",
            "    bytes_to_int,",
            "    ceil32,",
            "    fourbytes_to_int,",
            "    keccak256,",
            "    method_id,",
            "    method_id_int,",
            "    vyper_warn,",
            ")",
            "",
            "from ._convert import convert",
            "from ._signatures import BuiltinFunctionT, process_inputs",
            "",
            "SHA256_ADDRESS = 2",
            "SHA256_BASE_GAS = 60",
            "SHA256_PER_WORD_GAS = 12",
            "",
            "",
            "class FoldedFunctionT(BuiltinFunctionT):",
            "    # Base class for nodes which should always be folded",
            "",
            "    _modifiability = Modifiability.CONSTANT",
            "",
            "",
            "class TypenameFoldedFunctionT(FoldedFunctionT):",
            "    # Base class for builtin functions that:",
            "    # (1) take a typename as the only argument; and",
            "    # (2) should always be folded.",
            "    _inputs = [(\"typename\", TYPE_T.any())]",
            "",
            "    def fetch_call_return(self, node):",
            "        type_ = self.infer_arg_types(node)[0].typedef",
            "        return type_",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        validate_call_args(node, 1)",
            "        input_typedef = TYPE_T(type_from_annotation(node.args[0]))",
            "        return [input_typedef]",
            "",
            "",
            "class Floor(BuiltinFunctionT):",
            "    _id = \"floor\"",
            "    _inputs = [(\"value\", DecimalT())]",
            "    # TODO: maybe use int136?",
            "    _return_type = INT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Decimal):",
            "            raise UnfoldableNode",
            "",
            "        value = math.floor(value.value)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        arg = args[0]",
            "        with arg.cache_when_complex(\"arg\") as (b1, arg):",
            "            ret = IRnode.from_list(",
            "                [",
            "                    \"if\",",
            "                    [\"slt\", arg, 0],",
            "                    [\"sdiv\", [\"sub\", arg, DECIMAL_DIVISOR - 1], DECIMAL_DIVISOR],",
            "                    [\"sdiv\", arg, DECIMAL_DIVISOR],",
            "                ],",
            "                typ=INT256_T,",
            "            )",
            "            return b1.resolve(ret)",
            "",
            "",
            "class Ceil(BuiltinFunctionT):",
            "    _id = \"ceil\"",
            "    _inputs = [(\"value\", DecimalT())]",
            "    # TODO: maybe use int136?",
            "    _return_type = INT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Decimal):",
            "            raise UnfoldableNode",
            "",
            "        value = math.ceil(value.value)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        arg = args[0]",
            "        with arg.cache_when_complex(\"arg\") as (b1, arg):",
            "            ret = IRnode.from_list(",
            "                [",
            "                    \"if\",",
            "                    [\"slt\", arg, 0],",
            "                    [\"sdiv\", arg, DECIMAL_DIVISOR],",
            "                    [\"sdiv\", [\"add\", arg, DECIMAL_DIVISOR - 1], DECIMAL_DIVISOR],",
            "                ],",
            "                typ=INT256_T,",
            "            )",
            "            return b1.resolve(ret)",
            "",
            "",
            "class Convert(BuiltinFunctionT):",
            "    _id = \"convert\"",
            "",
            "    def fetch_call_return(self, node):",
            "        _, target_typedef = self.infer_arg_types(node)",
            "",
            "        # note: more type conversion validation happens in convert.py",
            "        return target_typedef.typedef",
            "",
            "    # TODO: push this down into convert.py for more consistency",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        validate_call_args(node, 2)",
            "",
            "        target_type = type_from_annotation(node.args[1])",
            "        value_types = get_possible_types_from_node(node.args[0])",
            "",
            "        # For `convert` of integer literals, we need to match type inference rules in",
            "        # convert.py codegen routines.",
            "        # TODO: This can probably be removed once constant folding for `convert` is implemented",
            "        if len(value_types) > 1 and all(isinstance(v, IntegerT) for v in value_types):",
            "            # Get the smallest (and unsigned if available) type for non-integer target types",
            "            # (note this is different from the ordering returned by `get_possible_types_from_node`)",
            "            if not isinstance(target_type, IntegerT):",
            "                value_types = sorted(value_types, key=lambda v: (v.is_signed, v.bits), reverse=True)",
            "            else:",
            "                # filter out the target type from list of possible types",
            "                value_types = [i for i in value_types if not target_type.compare_type(i)]",
            "",
            "        value_type = value_types.pop()",
            "",
            "        # block conversions between same type",
            "        if target_type.compare_type(value_type):",
            "            raise InvalidType(f\"Value and target type are both '{target_type}'\", node)",
            "",
            "        return [value_type, TYPE_T(target_type)]",
            "",
            "    def build_IR(self, expr, context):",
            "        return convert(expr, context)",
            "",
            "",
            "ADHOC_SLICE_NODE_MACROS = [\"~calldata\", \"~selfcode\", \"~extcode\"]",
            "",
            "",
            "def _build_adhoc_slice_node(sub: IRnode, start: IRnode, length: IRnode, context: Context) -> IRnode:",
            "    assert length.is_literal, \"typechecker failed\"",
            "    assert isinstance(length.value, int)  # mypy hint",
            "",
            "    dst_typ = BytesT(length.value)",
            "    # allocate a buffer for the return value",
            "    buf = context.new_internal_variable(dst_typ)",
            "",
            "    with scope_multi((start, length), (\"start\", \"length\")) as (b1, (start, length)):",
            "        # `msg.data` by `calldatacopy`",
            "        if sub.value == \"~calldata\":",
            "            node = [",
            "                \"seq\",",
            "                check_buffer_overflow_ir(start, length, \"calldatasize\"),",
            "                [\"mstore\", buf, length],",
            "                [\"calldatacopy\", add_ofst(buf, 32), start, length],",
            "                buf,",
            "            ]",
            "",
            "        # `self.code` by `codecopy`",
            "        elif sub.value == \"~selfcode\":",
            "            node = [",
            "                \"seq\",",
            "                check_buffer_overflow_ir(start, length, \"codesize\"),",
            "                [\"mstore\", buf, length],",
            "                [\"codecopy\", add_ofst(buf, 32), start, length],",
            "                buf,",
            "            ]",
            "",
            "        # `<address>.code` by `extcodecopy`",
            "        else:",
            "            assert sub.value == \"~extcode\" and len(sub.args) == 1",
            "            node = [",
            "                \"with\",",
            "                \"_extcode_address\",",
            "                sub.args[0],",
            "                [",
            "                    \"seq\",",
            "                    check_buffer_overflow_ir(start, length, [\"extcodesize\", \"_extcode_address\"]),",
            "                    [\"mstore\", buf, length],",
            "                    [\"extcodecopy\", \"_extcode_address\", add_ofst(buf, 32), start, length],",
            "                    buf,",
            "                ],",
            "            ]",
            "",
            "        assert isinstance(length.value, int)  # mypy hint",
            "        ret = IRnode.from_list(node, typ=BytesT(length.value), location=MEMORY)",
            "        return b1.resolve(ret)",
            "",
            "",
            "# note: this and a lot of other builtins could be refactored to accept any uint type",
            "class Slice(BuiltinFunctionT):",
            "    _id = \"slice\"",
            "    _inputs = [",
            "        (\"b\", (BYTES32_T, BytesT.any(), StringT.any())),",
            "        (\"start\", UINT256_T),",
            "        (\"length\", UINT256_T),",
            "    ]",
            "",
            "    def fetch_call_return(self, node):",
            "        arg_type, _, _ = self.infer_arg_types(node)",
            "",
            "        if isinstance(arg_type, StringT):",
            "            return_type = StringT()",
            "        else:",
            "            return_type = BytesT()",
            "",
            "        # validate start and length are in bounds",
            "",
            "        arg = node.args[0]",
            "        start_expr = node.args[1]",
            "        length_expr = node.args[2].reduced()",
            "",
            "        # CMC 2022-03-22 NOTE slight code duplication with semantics/analysis/local",
            "        is_adhoc_slice = arg.get(\"attr\") == \"code\" or (",
            "            arg.get(\"value.id\") == \"msg\" and arg.get(\"attr\") == \"data\"",
            "        )",
            "",
            "        start_literal = start_expr.value if isinstance(start_expr, vy_ast.Int) else None",
            "        length_literal = length_expr.value if isinstance(length_expr, vy_ast.Int) else None",
            "",
            "        if not is_adhoc_slice:",
            "            if length_literal is not None:",
            "                if length_literal < 1:",
            "                    raise ArgumentException(\"Length cannot be less than 1\", length_expr)",
            "",
            "                if length_literal > arg_type.length:",
            "                    raise ArgumentException(f\"slice out of bounds for {arg_type}\", length_expr)",
            "",
            "            if start_literal is not None:",
            "                if start_literal > arg_type.length:",
            "                    raise ArgumentException(f\"slice out of bounds for {arg_type}\", start_expr)",
            "                if length_literal is not None and start_literal + length_literal > arg_type.length:",
            "                    raise ArgumentException(f\"slice out of bounds for {arg_type}\", node)",
            "",
            "        # we know the length statically",
            "        if length_literal is not None:",
            "            return_type.set_length(length_literal)",
            "        else:",
            "            return_type.set_min_length(arg_type.length)",
            "",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `b`",
            "        b_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [b_type, self._inputs[1][1], self._inputs[2][1]]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        src, start, length = args",
            "",
            "        # Handle `msg.data`, `self.code`, and `<address>.code`",
            "        if src.value in ADHOC_SLICE_NODE_MACROS:",
            "            return _build_adhoc_slice_node(src, start, length, context)",
            "",
            "        is_bytes32 = src.typ == BYTES32_T",
            "        if src.location is None:",
            "            # it's not a pointer; force it to be one since",
            "            # copy_bytes works on pointers.",
            "            assert is_bytes32, src",
            "            src = ensure_in_memory(src, context)",
            "",
            "        if potential_overlap(src, start) or potential_overlap(src, length):",
            "            raise CompilerPanic(\"risky overlap\")",
            "",
            "        with src.cache_when_complex(\"src\") as (b1, src), start.cache_when_complex(\"start\") as (",
            "            b2,",
            "            start,",
            "        ), length.cache_when_complex(\"length\") as (b3, length):",
            "            if is_bytes32:",
            "                src_maxlen = 32",
            "            else:",
            "                src_maxlen = src.typ.maxlen",
            "",
            "            dst_maxlen = length.value if length.is_literal else src_maxlen",
            "",
            "            buflen = dst_maxlen",
            "",
            "            # add 32 bytes to the buffer size bc word access might",
            "            # be unaligned (see below)",
            "            if src.location.word_addressable:",
            "                buflen += 32",
            "",
            "            # Get returntype string or bytes",
            "            assert isinstance(src.typ, _BytestringT) or is_bytes32",
            "            # TODO: try to get dst_typ from semantic analysis",
            "            if isinstance(src.typ, StringT):",
            "                dst_typ = StringT(dst_maxlen)",
            "            else:",
            "                dst_typ = BytesT(dst_maxlen)",
            "",
            "            # allocate a buffer for the return value",
            "            buf = context.new_internal_variable(BytesT(buflen))",
            "            # assign it the correct return type.",
            "            # (note mismatch between dst_maxlen and buflen)",
            "            dst = IRnode.from_list(buf, typ=dst_typ, location=MEMORY)",
            "",
            "            dst_data = bytes_data_ptr(dst)",
            "",
            "            if is_bytes32:",
            "                src_len = 32",
            "                src_data = src",
            "            else:",
            "                src_len = get_bytearray_length(src)",
            "                src_data = bytes_data_ptr(src)",
            "",
            "            # general case. byte-for-byte copy",
            "            if src.location.word_addressable:",
            "                # because slice uses byte-addressing but storage/tstorage",
            "                # is word-aligned, this algorithm starts at some number",
            "                # of bytes before the data section starts, and might copy",
            "                # an extra word. the pseudocode is:",
            "                #   dst_data = dst + 32",
            "                #   copy_dst = dst_data - start % 32",
            "                #   src_data = src + 32",
            "                #   copy_src = src_data + (start - start % 32) / 32",
            "                #            = src_data + (start // 32)",
            "                #   copy_bytes(copy_dst, copy_src, length)",
            "                #   //set length AFTER copy because the length word has been clobbered!",
            "                #   mstore(src, length)",
            "",
            "                # start at the first word-aligned address before `start`",
            "                # e.g. start == byte 7 -> we start copying from byte 0",
            "                #      start == byte 32 -> we start copying from byte 32",
            "                copy_src = IRnode.from_list(",
            "                    [\"add\", src_data, [\"div\", start, 32]], location=src.location",
            "                )",
            "",
            "                # e.g. start == byte 0 -> we copy to dst_data + 0",
            "                #      start == byte 7 -> we copy to dst_data - 7",
            "                #      start == byte 33 -> we copy to dst_data - 1",
            "                copy_dst = IRnode.from_list(",
            "                    [\"sub\", dst_data, [\"mod\", start, 32]], location=dst.location",
            "                )",
            "",
            "                # len + (32 if start % 32 > 0 else 0)",
            "                copy_len = [\"add\", length, [\"mul\", 32, [\"iszero\", [\"iszero\", [\"mod\", start, 32]]]]]",
            "                copy_maxlen = buflen",
            "",
            "            else:",
            "                # all other address spaces (mem, calldata, code) we have",
            "                # byte-aligned access so we can just do the easy thing,",
            "                # memcopy(dst_data, src_data + dst_data)",
            "",
            "                copy_src = add_ofst(src_data, start)",
            "                copy_dst = dst_data",
            "                copy_len = length",
            "                copy_maxlen = buflen",
            "",
            "            do_copy = copy_bytes(copy_dst, copy_src, copy_len, copy_maxlen)",
            "",
            "            ret = [",
            "                \"seq\",",
            "                check_buffer_overflow_ir(start, length, src_len),",
            "                do_copy,",
            "                [\"mstore\", dst, length],  # set length",
            "                dst,  # return pointer to dst",
            "            ]",
            "            ret = IRnode.from_list(ret, typ=dst_typ, location=MEMORY)",
            "            return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "",
            "class Len(BuiltinFunctionT):",
            "    _id = \"len\"",
            "    _inputs = [(\"b\", (StringT.any(), BytesT.any(), DArrayT.any()))]",
            "    _return_type = UINT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        arg = node.args[0].get_folded_value()",
            "        if isinstance(arg, (vy_ast.Str, vy_ast.Bytes)):",
            "            length = len(arg.value)",
            "        elif isinstance(arg, vy_ast.Hex):",
            "            length = len(arg.bytes_value)",
            "        else:",
            "            raise UnfoldableNode",
            "",
            "        return vy_ast.Int.from_node(node, value=length)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type",
            "        typ = get_possible_types_from_node(node.args[0]).pop()",
            "        return [typ]",
            "",
            "    def build_IR(self, node, context):",
            "        arg = Expr(node.args[0], context).ir_node",
            "        if arg.value == \"~calldata\":",
            "            return IRnode.from_list([\"calldatasize\"], typ=UINT256_T)",
            "        return get_bytearray_length(arg)",
            "",
            "",
            "class Concat(BuiltinFunctionT):",
            "    _id = \"concat\"",
            "",
            "    def fetch_call_return(self, node):",
            "        arg_types = self.infer_arg_types(node)",
            "",
            "        length = 0",
            "        for arg_t in arg_types:",
            "            length += arg_t.length",
            "",
            "        if isinstance(arg_types[0], (StringT)):",
            "            return_type = StringT()",
            "        else:",
            "            return_type = BytesT()",
            "        return_type.set_length(length)",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        if len(node.args) < 2:",
            "            raise ArgumentException(\"Invalid argument count: expected at least 2\", node)",
            "",
            "        if node.keywords:",
            "            raise ArgumentException(\"Keyword arguments are not accepted here\", node.keywords[0])",
            "",
            "        ret = []",
            "        prev_typeclass = None",
            "        for arg in node.args:",
            "            validate_expected_type(arg, (BytesT.any(), StringT.any(), BytesM_T.any()))",
            "            arg_t = get_possible_types_from_node(arg).pop()",
            "            current_typeclass = \"String\" if isinstance(arg_t, StringT) else \"Bytes\"",
            "            if prev_typeclass and current_typeclass != prev_typeclass:",
            "                raise TypeMismatch(",
            "                    (",
            "                        \"Concat expects consistent use of string or bytes types, \"",
            "                        \"use either string or bytes.\"",
            "                    ),",
            "                    arg,",
            "                )",
            "            prev_typeclass = current_typeclass",
            "            ret.append(arg_t)",
            "",
            "        return ret",
            "",
            "    def build_IR(self, expr, context):",
            "        args = [Expr(arg, context).ir_node for arg in expr.args]",
            "        if len(args) < 2:",
            "            raise StructureException(\"Concat expects at least two arguments\", expr)",
            "",
            "        # Maximum length of the output",
            "        dst_maxlen = sum(",
            "            [arg.typ.maxlen if isinstance(arg.typ, _BytestringT) else arg.typ.m for arg in args]",
            "        )",
            "",
            "        # TODO: try to grab these from semantic analysis",
            "        if isinstance(args[0].typ, StringT):",
            "            ret_typ = StringT(dst_maxlen)",
            "        else:",
            "            ret_typ = BytesT(dst_maxlen)",
            "",
            "        # respect API of copy_bytes",
            "        bufsize = dst_maxlen + 32",
            "        dst = context.new_internal_variable(BytesT(bufsize))",
            "        dst.annotation = \"concat destination\"",
            "",
            "        ret = [\"seq\"]",
            "        # stack item representing our current offset in the dst buffer",
            "        ofst = \"concat_ofst\"",
            "",
            "        # TODO: optimize for the case where all lengths are statically known.",
            "        for arg in args:",
            "            dst_data = add_ofst(bytes_data_ptr(dst), ofst)",
            "",
            "            if isinstance(arg.typ, _BytestringT):",
            "                # Ignore empty strings",
            "                if arg.typ.maxlen == 0:",
            "                    continue",
            "",
            "                with arg.cache_when_complex(\"arg\") as (b1, arg):",
            "                    argdata = bytes_data_ptr(arg)",
            "",
            "                    with get_bytearray_length(arg).cache_when_complex(\"len\") as (b2, arglen):",
            "                        do_copy = [",
            "                            \"seq\",",
            "                            copy_bytes(dst_data, argdata, arglen, arg.typ.maxlen),",
            "                            [\"set\", ofst, [\"add\", ofst, arglen]],",
            "                        ]",
            "                        ret.append(b1.resolve(b2.resolve(do_copy)))",
            "",
            "            else:",
            "                ret.append(STORE(dst_data, unwrap_location(arg)))",
            "                ret.append([\"set\", ofst, [\"add\", ofst, arg.typ.m]])",
            "",
            "        ret.append(STORE(dst, ofst))",
            "",
            "        # Memory location of the output",
            "        ret.append(dst)",
            "",
            "        return IRnode.from_list(",
            "            [\"with\", ofst, 0, ret], typ=ret_typ, location=MEMORY, annotation=\"concat\"",
            "        )",
            "",
            "",
            "class Keccak256(BuiltinFunctionT):",
            "    _id = \"keccak256\"",
            "    # TODO allow any BytesM_T",
            "    _inputs = [(\"value\", (BytesT.any(), BYTES32_T, StringT.any()))]",
            "    _return_type = BYTES32_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if isinstance(value, vy_ast.Bytes):",
            "            value = value.value",
            "        elif isinstance(value, vy_ast.Str):",
            "            value = value.value.encode()",
            "        elif isinstance(value, vy_ast.Hex):",
            "            value = value.bytes_value",
            "        else:",
            "            raise UnfoldableNode",
            "",
            "        hash_ = f\"0x{keccak256(value).hex()}\"",
            "        return vy_ast.Hex.from_node(node, value=hash_)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `value`",
            "        value_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [value_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        assert len(args) == 1",
            "        return keccak256_helper(args[0], context)",
            "",
            "",
            "def _make_sha256_call(inp_start, inp_len, out_start, out_len):",
            "    return [",
            "        \"assert\",",
            "        [",
            "            \"staticcall\",",
            "            [\"gas\"],  # gas",
            "            SHA256_ADDRESS,  # address",
            "            inp_start,",
            "            inp_len,",
            "            out_start,",
            "            out_len,",
            "        ],",
            "    ]",
            "",
            "",
            "class Sha256(BuiltinFunctionT):",
            "    _id = \"sha256\"",
            "    _inputs = [(\"value\", (BYTES32_T, BytesT.any(), StringT.any()))]",
            "    _return_type = BYTES32_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if isinstance(value, vy_ast.Bytes):",
            "            value = value.value",
            "        elif isinstance(value, vy_ast.Str):",
            "            value = value.value.encode()",
            "        elif isinstance(value, vy_ast.Hex):",
            "            value = value.bytes_value",
            "        else:",
            "            raise UnfoldableNode",
            "",
            "        hash_ = f\"0x{hashlib.sha256(value).hexdigest()}\"",
            "        return vy_ast.Hex.from_node(node, value=hash_)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `value`",
            "        value_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [value_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        sub = args[0]",
            "        # bytes32 input",
            "        if sub.typ == BYTES32_T:",
            "            return IRnode.from_list(",
            "                [",
            "                    \"seq\",",
            "                    [\"mstore\", MemoryPositions.FREE_VAR_SPACE, sub],",
            "                    _make_sha256_call(",
            "                        inp_start=MemoryPositions.FREE_VAR_SPACE,",
            "                        inp_len=32,",
            "                        out_start=MemoryPositions.FREE_VAR_SPACE,",
            "                        out_len=32,",
            "                    ),",
            "                    [\"mload\", MemoryPositions.FREE_VAR_SPACE],  # push value onto stack",
            "                ],",
            "                typ=BYTES32_T,",
            "                add_gas_estimate=SHA256_BASE_GAS + 1 * SHA256_PER_WORD_GAS,",
            "            )",
            "        # bytearay-like input",
            "        # special case if it's already in memory",
            "        sub = ensure_in_memory(sub, context)",
            "",
            "        return IRnode.from_list(",
            "            [",
            "                \"with\",",
            "                \"_sub\",",
            "                sub,",
            "                [",
            "                    \"seq\",",
            "                    _make_sha256_call(",
            "                        # TODO use add_ofst if sub is statically known",
            "                        inp_start=[\"add\", \"_sub\", 32],",
            "                        inp_len=[\"mload\", \"_sub\"],",
            "                        out_start=MemoryPositions.FREE_VAR_SPACE,",
            "                        out_len=32,",
            "                    ),",
            "                    [\"mload\", MemoryPositions.FREE_VAR_SPACE],",
            "                ],",
            "            ],",
            "            typ=BYTES32_T,",
            "            add_gas_estimate=SHA256_BASE_GAS + sub.typ.maxlen * SHA256_PER_WORD_GAS,",
            "        )",
            "",
            "",
            "class MethodID(FoldedFunctionT):",
            "    _id = \"method_id\"",
            "    _inputs = [(\"value\", StringT.any())]",
            "    _kwargs = {\"output_type\": KwargSettings(TYPE_T.any(), BytesT(4))}",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1, [\"output_type\"])",
            "",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Str):",
            "            raise InvalidType(\"method id must be given as a literal string\", node.args[0])",
            "        if \" \" in value.value:",
            "            raise InvalidLiteral(\"Invalid function signature - no spaces allowed.\", node.args[0])",
            "",
            "        return_type = self.infer_kwarg_types(node)[\"output_type\"].typedef",
            "        value = method_id(value.value)",
            "",
            "        if return_type.compare_type(BYTES4_T):",
            "            return vy_ast.Hex.from_node(node, value=\"0x\" + value.hex())",
            "        else:",
            "            return vy_ast.Bytes.from_node(node, value=value)",
            "",
            "    def fetch_call_return(self, node):",
            "        validate_call_args(node, 1, [\"output_type\"])",
            "",
            "        type_ = self.infer_kwarg_types(node)[\"output_type\"].typedef",
            "        return type_",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        return [self._inputs[0][1]]",
            "",
            "    def infer_kwarg_types(self, node):",
            "        if node.keywords:",
            "            output_type = type_from_annotation(node.keywords[0].value)",
            "            if output_type not in (BytesT(4), BYTES4_T):",
            "                raise ArgumentException(\"output_type must be Bytes[4] or bytes4\", node.keywords[0])",
            "        else:",
            "            # default to `Bytes[4]`",
            "            output_type = BytesT(4)",
            "",
            "        return {\"output_type\": TYPE_T(output_type)}",
            "",
            "",
            "class ECRecover(BuiltinFunctionT):",
            "    _id = \"ecrecover\"",
            "    _inputs = [",
            "        (\"hash\", BYTES32_T),",
            "        (\"v\", (UINT256_T, UINT8_T)),",
            "        (\"r\", (UINT256_T, BYTES32_T)),",
            "        (\"s\", (UINT256_T, BYTES32_T)),",
            "    ]",
            "    _return_type = AddressT()",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        v_t, r_t, s_t = [get_possible_types_from_node(arg).pop() for arg in node.args[1:]]",
            "        return [BYTES32_T, v_t, r_t, s_t]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        input_buf = context.new_internal_variable(get_type_for_exact_size(128))",
            "        output_buf = context.new_internal_variable(get_type_for_exact_size(32))",
            "        return IRnode.from_list(",
            "            [",
            "                \"seq\",",
            "                # clear output memory first, ecrecover can return 0 bytes",
            "                [\"mstore\", output_buf, 0],",
            "                [\"mstore\", input_buf, args[0]],",
            "                [\"mstore\", add_ofst(input_buf, 32), args[1]],",
            "                [\"mstore\", add_ofst(input_buf, 64), args[2]],",
            "                [\"mstore\", add_ofst(input_buf, 96), args[3]],",
            "                [\"assert\", [\"staticcall\", \"gas\", 1, input_buf, 128, output_buf, 32]],",
            "                [\"mload\", output_buf],",
            "            ],",
            "            typ=AddressT(),",
            "        )",
            "",
            "",
            "class _ECArith(BuiltinFunctionT):",
            "    @process_inputs",
            "    def build_IR(self, expr, _args, kwargs, context):",
            "        args_tuple = ir_tuple_from_args(_args)",
            "",
            "        args_t = args_tuple.typ",
            "        input_buf = context.new_internal_variable(args_t)",
            "        ret_t = self._return_type",
            "",
            "        ret = [\"seq\"]",
            "        ret.append(make_setter(input_buf, args_tuple))",
            "",
            "        output_buf = context.new_internal_variable(ret_t)",
            "",
            "        args_ofst = input_buf",
            "        args_len = args_t.memory_bytes_required",
            "        out_ofst = output_buf",
            "        out_len = ret_t.memory_bytes_required",
            "",
            "        ret.append(",
            "            [",
            "                \"assert\",",
            "                [\"staticcall\", [\"gas\"], self._precompile, args_ofst, args_len, out_ofst, out_len],",
            "            ]",
            "        )",
            "        ret.append(output_buf)",
            "",
            "        return IRnode.from_list(ret, typ=ret_t, location=MEMORY)",
            "",
            "",
            "class ECAdd(_ECArith):",
            "    _id = \"ecadd\"",
            "    _inputs = [(\"a\", SArrayT(UINT256_T, 2)), (\"b\", SArrayT(UINT256_T, 2))]",
            "    _return_type = SArrayT(UINT256_T, 2)",
            "    _precompile = 0x6",
            "",
            "",
            "class ECMul(_ECArith):",
            "    _id = \"ecmul\"",
            "    _inputs = [(\"point\", SArrayT(UINT256_T, 2)), (\"scalar\", UINT256_T)]",
            "    _return_type = SArrayT(UINT256_T, 2)",
            "    _precompile = 0x7",
            "",
            "",
            "class Extract32(BuiltinFunctionT):",
            "    _id = \"extract32\"",
            "    _inputs = [(\"b\", BytesT.any()), (\"start\", IntegerT.unsigneds())]",
            "    _kwargs = {\"output_type\": KwargSettings(TYPE_T.any(), BYTES32_T)}",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "        return_type = self.infer_kwarg_types(node)[\"output_type\"].typedef",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        input_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [input_type, UINT256_T]",
            "",
            "    def infer_kwarg_types(self, node):",
            "        if node.keywords:",
            "            output_type = type_from_annotation(node.keywords[0].value)",
            "            if not isinstance(output_type, (AddressT, BytesM_T, IntegerT)):",
            "                raise InvalidType(",
            "                    \"Output type must be one of integer, bytes32 or address\", node.keywords[0].value",
            "                )",
            "            output_typedef = TYPE_T(output_type)",
            "            node.keywords[0].value._metadata[\"type\"] = output_typedef",
            "        else:",
            "            output_typedef = TYPE_T(BYTES32_T)",
            "",
            "        return {\"output_type\": output_typedef}",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        bytez, index = args",
            "        ret_type = kwargs[\"output_type\"]",
            "",
            "        if potential_overlap(bytez, index):",
            "            raise CompilerPanic(\"risky overlap\")",
            "",
            "        def finalize(ret):",
            "            annotation = \"extract32\"",
            "            ret = IRnode.from_list(ret, typ=ret_type, annotation=annotation)",
            "            return clamp_basetype(ret)",
            "",
            "        with bytez.cache_when_complex(\"_sub\") as (b1, bytez):",
            "            # merge",
            "            length = get_bytearray_length(bytez)",
            "            index = clamp2(0, index, [\"sub\", length, 32], signed=True)",
            "            with index.cache_when_complex(\"_index\") as (b2, index):",
            "                assert not index.typ.is_signed",
            "",
            "                # \"easy\" case, byte- addressed locations:",
            "                if bytez.location.word_scale == 32:",
            "                    word = LOAD(add_ofst(bytes_data_ptr(bytez), index))",
            "                    return finalize(b1.resolve(b2.resolve(word)))",
            "",
            "                # storage and transient storage, word-addressed",
            "                assert bytez.location.word_scale == 1",
            "",
            "                slot = IRnode.from_list([\"div\", index, 32])",
            "                # byte offset within the slot",
            "                byte_ofst = IRnode.from_list([\"mod\", index, 32])",
            "",
            "                with byte_ofst.cache_when_complex(\"byte_ofst\") as (",
            "                    b3,",
            "                    byte_ofst,",
            "                ), slot.cache_when_complex(\"slot\") as (b4, slot):",
            "                    # perform two loads and merge",
            "                    w1 = LOAD(add_ofst(bytes_data_ptr(bytez), slot))",
            "                    w2 = LOAD(add_ofst(bytes_data_ptr(bytez), [\"add\", slot, 1]))",
            "",
            "                    left_bytes = shl([\"mul\", 8, byte_ofst], w1)",
            "                    right_bytes = shr([\"mul\", 8, [\"sub\", 32, byte_ofst]], w2)",
            "                    merged = [\"or\", left_bytes, right_bytes]",
            "",
            "                    ret = [\"if\", byte_ofst, merged, left_bytes]",
            "                    return finalize(b1.resolve(b2.resolve(b3.resolve(b4.resolve(ret)))))",
            "",
            "",
            "class AsWeiValue(BuiltinFunctionT):",
            "    _id = \"as_wei_value\"",
            "    _inputs = [(\"value\", (IntegerT.any(), DecimalT())), (\"unit\", StringT.any())]",
            "    _return_type = UINT256_T",
            "",
            "    wei_denoms = {",
            "        (\"wei\",): 1,",
            "        (\"femtoether\", \"kwei\", \"babbage\"): 10**3,",
            "        (\"picoether\", \"mwei\", \"lovelace\"): 10**6,",
            "        (\"nanoether\", \"gwei\", \"shannon\"): 10**9,",
            "        (\"microether\", \"szabo\"): 10**12,",
            "        (\"milliether\", \"finney\"): 10**15,",
            "        (\"ether\",): 10**18,",
            "        (\"kether\", \"grand\"): 10**21,",
            "    }",
            "",
            "    def get_denomination(self, node):",
            "        value = node.args[1].get_folded_value()",
            "        if not isinstance(value, vy_ast.Str):",
            "            raise ArgumentException(",
            "                \"Wei denomination must be given as a literal string\", node.args[1]",
            "            )",
            "        try:",
            "            denom = next(v for k, v in self.wei_denoms.items() if value.value in k)",
            "        except StopIteration:",
            "            raise ArgumentException(f\"Unknown denomination: {value.value}\", node.args[1]) from None",
            "",
            "        return denom",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 2)",
            "        denom = self.get_denomination(node)",
            "",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, (vy_ast.Decimal, vy_ast.Int)):",
            "            raise UnfoldableNode",
            "        value = value.value",
            "",
            "        if value < 0:",
            "            raise InvalidLiteral(\"Negative wei value not allowed\", node.args[0])",
            "",
            "        return vy_ast.Int.from_node(node, value=int(value * denom))",
            "",
            "    def fetch_call_return(self, node):",
            "        self.infer_arg_types(node)",
            "        return self._return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type instead of abstract type",
            "        value_type = get_possible_types_from_node(node.args[0]).pop()",
            "        unit_type = get_possible_types_from_node(node.args[1]).pop()",
            "        return [value_type, unit_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        value = args[0]",
            "",
            "        denom_divisor = self.get_denomination(expr)",
            "        with value.cache_when_complex(\"value\") as (b1, value):",
            "            if value.typ in (UINT256_T, UINT8_T):",
            "                sub = [",
            "                    \"with\",",
            "                    \"ans\",",
            "                    [\"mul\", value, denom_divisor],",
            "                    [",
            "                        \"seq\",",
            "                        [",
            "                            \"assert\",",
            "                            [\"or\", [\"eq\", [\"div\", \"ans\", value], denom_divisor], [\"iszero\", value]],",
            "                        ],",
            "                        \"ans\",",
            "                    ],",
            "                ]",
            "            elif value.typ == INT128_T:",
            "                # signed types do not require bounds checks because the",
            "                # largest possible converted value will not overflow 2**256",
            "                sub = [\"seq\", [\"assert\", [\"sgt\", value, -1]], [\"mul\", value, denom_divisor]]",
            "            elif value.typ == DecimalT():",
            "                sub = [",
            "                    \"seq\",",
            "                    [\"assert\", [\"sgt\", value, -1]],",
            "                    [\"div\", [\"mul\", value, denom_divisor], DECIMAL_DIVISOR],",
            "                ]",
            "            else:",
            "                raise CompilerPanic(f\"Unexpected type: {value.typ}\")",
            "",
            "            return IRnode.from_list(b1.resolve(sub), typ=UINT256_T)",
            "",
            "",
            "zero_value = IRnode.from_list(0, typ=UINT256_T)",
            "empty_value = IRnode.from_list(0, typ=BYTES32_T)",
            "",
            "",
            "class RawCall(BuiltinFunctionT):",
            "    _id = \"raw_call\"",
            "    _inputs = [(\"to\", AddressT()), (\"data\", BytesT.any())]",
            "    _kwargs = {",
            "        \"max_outsize\": KwargSettings(UINT256_T, 0, require_literal=True),",
            "        \"gas\": KwargSettings(UINT256_T, \"gas\"),",
            "        \"value\": KwargSettings(UINT256_T, zero_value),",
            "        \"is_delegate_call\": KwargSettings(BoolT(), False, require_literal=True),",
            "        \"is_static_call\": KwargSettings(BoolT(), False, require_literal=True),",
            "        \"revert_on_failure\": KwargSettings(BoolT(), True, require_literal=True),",
            "    }",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "",
            "        kwargz = {i.arg: i.value for i in node.keywords}",
            "",
            "        outsize = kwargz.get(\"max_outsize\")",
            "        if outsize is not None:",
            "            outsize = outsize.get_folded_value()",
            "",
            "        revert_on_failure = kwargz.get(\"revert_on_failure\")",
            "        if revert_on_failure is not None:",
            "            revert_on_failure = revert_on_failure.get_folded_value().value",
            "        else:",
            "            revert_on_failure = True",
            "",
            "        if outsize is None or outsize.value == 0:",
            "            if revert_on_failure:",
            "                return None",
            "            return BoolT()",
            "",
            "        if not isinstance(outsize, vy_ast.Int) or outsize.value < 0:",
            "            raise",
            "",
            "        if outsize.value:",
            "            return_type = BytesT()",
            "            return_type.set_min_length(outsize.value)",
            "",
            "            if revert_on_failure:",
            "                return return_type",
            "            return TupleT([BoolT(), return_type])",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type for `data`",
            "        data_type = get_possible_types_from_node(node.args[1]).pop()",
            "        return [self._inputs[0][1], data_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        to, data = args",
            "        # TODO: must compile in source code order, left-to-right",
            "        gas, value, outsize, delegate_call, static_call, revert_on_failure = (",
            "            kwargs[\"gas\"],",
            "            kwargs[\"value\"],",
            "            kwargs[\"max_outsize\"],",
            "            kwargs[\"is_delegate_call\"],",
            "            kwargs[\"is_static_call\"],",
            "            kwargs[\"revert_on_failure\"],",
            "        )",
            "",
            "        if delegate_call and static_call:",
            "            raise ArgumentException(",
            "                \"Call may use one of `is_delegate_call` or `is_static_call`, not both\"",
            "            )",
            "",
            "        if (delegate_call or static_call) and value.value != 0:",
            "            raise ArgumentException(\"value= may not be passed for static or delegate calls!\")",
            "",
            "        if not static_call and context.is_constant():",
            "            raise StateAccessViolation(",
            "                f\"Cannot make modifying calls from {context.pp_constancy()},\"",
            "                \" use `is_static_call=True` to perform this action\"",
            "            )",
            "",
            "        if data.value == \"~calldata\":",
            "            call_ir = [\"with\", \"mem_ofst\", \"msize\"]",
            "            args_ofst = [\"seq\", [\"calldatacopy\", \"mem_ofst\", 0, \"calldatasize\"], \"mem_ofst\"]",
            "            args_len = \"calldatasize\"",
            "        else:",
            "            # some gymnastics to propagate constants (if eval_input_buf",
            "            # returns a static memory location)",
            "            eval_input_buf = ensure_in_memory(data, context)",
            "",
            "            input_buf = eval_seq(eval_input_buf)",
            "",
            "            if input_buf is None:",
            "                call_ir = [\"with\", \"arg_buf\", eval_input_buf]",
            "                input_buf = IRnode.from_list(\"arg_buf\")",
            "            else:",
            "                call_ir = [\"seq\", eval_input_buf]",
            "",
            "            args_ofst = add_ofst(input_buf, 32)",
            "            args_len = [\"mload\", input_buf]",
            "",
            "        output_node = context.new_internal_variable(BytesT(outsize))",
            "",
            "        bool_ty = BoolT()",
            "",
            "        # build IR for call or delegatecall",
            "        common_call_args = [",
            "            args_ofst,",
            "            args_len,",
            "            # if there is no return value, the return offset can be 0",
            "            add_ofst(output_node, 32) if outsize else 0,",
            "            outsize,",
            "        ]",
            "",
            "        gas, value = IRnode.from_list(gas), IRnode.from_list(value)",
            "        with scope_multi((to, value, gas), (\"_to\", \"_value\", \"_gas\")) as (b1, (to, value, gas)):",
            "            if delegate_call:",
            "                call_op = [\"delegatecall\", gas, to, *common_call_args]",
            "            elif static_call:",
            "                call_op = [\"staticcall\", gas, to, *common_call_args]",
            "            else:",
            "                call_op = [\"call\", gas, to, value, *common_call_args]",
            "",
            "            call_op = ensure_eval_once(\"raw_call_builtin\", call_op)",
            "            call_ir += [call_op]",
            "            call_ir = b1.resolve(call_ir)",
            "",
            "        # build sequence IR",
            "        if outsize:",
            "            # return minimum of outsize and returndatasize",
            "            size = [\"select\", [\"lt\", outsize, \"returndatasize\"], outsize, \"returndatasize\"]",
            "",
            "            # store output size and return output location",
            "            store_output_size = [\"seq\", [\"mstore\", output_node, size], output_node]",
            "",
            "            bytes_ty = BytesT(outsize)",
            "",
            "            if revert_on_failure:",
            "                typ = bytes_ty",
            "                # check the call success flag, and store returndata in memory",
            "                ret_ir = [\"seq\", check_external_call(call_ir), store_output_size]",
            "                return IRnode.from_list(ret_ir, typ=typ, location=MEMORY)",
            "            else:",
            "                typ = TupleT([bool_ty, bytes_ty])",
            "                ret_ir = [",
            "                    \"multi\",",
            "                    # use IRnode.from_list to make sure the types are",
            "                    # set properly on the \"multi\" members",
            "                    IRnode.from_list(call_ir, typ=bool_ty),",
            "                    IRnode.from_list(store_output_size, typ=bytes_ty, location=MEMORY),",
            "                ]",
            "                # return an IR tuple of call success flag and returndata pointer",
            "                return IRnode.from_list(ret_ir, typ=typ)",
            "",
            "        # max_outsize is 0.",
            "",
            "        if not revert_on_failure:",
            "            # return call flag as stack item",
            "            typ = bool_ty",
            "            return IRnode.from_list(call_ir, typ=typ)",
            "",
            "        else:",
            "            # check the call success flag and don't return anything",
            "            ret_ir = check_external_call(call_ir)",
            "            return IRnode.from_list(ret_ir, typ=None)",
            "",
            "        raise CompilerPanic(\"unreachable!\")",
            "",
            "",
            "class Send(BuiltinFunctionT):",
            "    _id = \"send\"",
            "    _inputs = [(\"to\", AddressT()), (\"value\", UINT256_T)]",
            "    # default gas stipend is 0",
            "    _kwargs = {\"gas\": KwargSettings(UINT256_T, 0)}",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        to, value = args",
            "        gas = kwargs[\"gas\"]",
            "        context.check_is_not_constant(\"send ether\", expr)",
            "        send_op = ensure_eval_once(\"send_builtin\", [\"call\", gas, to, value, 0, 0, 0, 0])",
            "        return IRnode.from_list([\"assert\", send_op], error_msg=\"send failed\")",
            "",
            "",
            "class SelfDestruct(BuiltinFunctionT):",
            "    _id = \"selfdestruct\"",
            "    _inputs = [(\"to\", AddressT())]",
            "    _is_terminus = True",
            "    _warned = False",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        if not self._warned:",
            "            vyper_warn(",
            "                \"`selfdestruct` is deprecated! The opcode is no longer recommended for use.\", expr",
            "            )",
            "            self._warned = True",
            "",
            "        context.check_is_not_constant(\"selfdestruct\", expr)",
            "        return IRnode.from_list(ensure_eval_once(\"selfdestruct\", [\"selfdestruct\", args[0]]))",
            "",
            "",
            "class BlockHash(BuiltinFunctionT):",
            "    _id = \"blockhash\"",
            "    _inputs = [(\"block_num\", UINT256_T)]",
            "    _return_type = BYTES32_T",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, contact):",
            "        return IRnode.from_list(",
            "            [\"blockhash\", clamp(\"lt\", clamp(\"sge\", args[0], [\"sub\", [\"number\"], 256]), \"number\")],",
            "            typ=BYTES32_T,",
            "        )",
            "",
            "",
            "class BlobHash(BuiltinFunctionT):",
            "    _id = \"blobhash\"",
            "    _inputs = [(\"index\", UINT256_T)]",
            "    _return_type = BYTES32_T",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, contact):",
            "        if not version_check(begin=\"cancun\"):",
            "            raise EvmVersionException(\"`blobhash` is not available pre-cancun\", expr)",
            "        return IRnode.from_list([\"blobhash\", args[0]], typ=BYTES32_T)",
            "",
            "",
            "class RawRevert(BuiltinFunctionT):",
            "    _id = \"raw_revert\"",
            "    _inputs = [(\"data\", BytesT.any())]",
            "    _return_type = None",
            "    _is_terminus = True",
            "",
            "    def fetch_call_return(self, node):",
            "        return None",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        data_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [data_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        with ensure_in_memory(args[0], context).cache_when_complex(\"err_buf\") as (b, buf):",
            "            data = bytes_data_ptr(buf)",
            "            len_ = get_bytearray_length(buf)",
            "            return b.resolve(IRnode.from_list([\"revert\", data, len_]))",
            "",
            "",
            "class RawLog(BuiltinFunctionT):",
            "    _id = \"raw_log\"",
            "    _inputs = [(\"topics\", DArrayT(BYTES32_T, 4)), (\"data\", (BYTES32_T, BytesT.any()))]",
            "",
            "    def fetch_call_return(self, node):",
            "        self.infer_arg_types(node)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "",
            "        arg = node.args[0].reduced()",
            "        if not isinstance(arg, vy_ast.List) or len(arg.elements) > 4:",
            "            raise InvalidType(\"Expecting a list of 0-4 topics as first argument\", node.args[0])",
            "",
            "        # return a concrete type for `data`",
            "        data_type = get_possible_types_from_node(node.args[1]).pop()",
            "",
            "        return [self._inputs[0][1], data_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        context.check_is_not_constant(f\"use {self._id}\", expr)",
            "",
            "        topics_length = len(expr.args[0].reduced().elements)",
            "        topics = args[0].args",
            "        topics = [unwrap_location(topic) for topic in topics]",
            "",
            "        # sanity check topics is a literal list",
            "        assert args[0].value in (\"~empty\", \"multi\")",
            "",
            "        data = args[1]",
            "",
            "        log_op = \"log\" + str(topics_length)",
            "",
            "        if data.typ == BYTES32_T:",
            "            placeholder = context.new_internal_variable(BYTES32_T)",
            "            log_ir = [log_op, placeholder, 32] + topics",
            "            return IRnode.from_list(",
            "                [\"seq\", make_setter(placeholder, data), ensure_eval_once(\"raw_log\", log_ir)]",
            "            )",
            "",
            "        input_buf = ensure_in_memory(data, context)",
            "",
            "        log_ir = [log_op, [\"add\", \"_sub\", 32], [\"mload\", \"_sub\"], *topics]",
            "        return IRnode.from_list([\"with\", \"_sub\", input_buf, ensure_eval_once(\"raw_log\", log_ir)])",
            "",
            "",
            "class BitwiseAnd(BuiltinFunctionT):",
            "    _id = \"bitwise_and\"",
            "    _inputs = [(\"x\", UINT256_T), (\"y\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_and()` is deprecated! Please use the & operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        for val in values:",
            "            if not isinstance(val, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = values[0].value & values[1].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"and\", args[0], args[1]], typ=UINT256_T)",
            "",
            "",
            "class BitwiseOr(BuiltinFunctionT):",
            "    _id = \"bitwise_or\"",
            "    _inputs = [(\"x\", UINT256_T), (\"y\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_or()` is deprecated! Please use the | operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        for val in values:",
            "            if not isinstance(val, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = values[0].value | values[1].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"or\", args[0], args[1]], typ=UINT256_T)",
            "",
            "",
            "class BitwiseXor(BuiltinFunctionT):",
            "    _id = \"bitwise_xor\"",
            "    _inputs = [(\"x\", UINT256_T), (\"y\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_xor()` is deprecated! Please use the ^ operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        for val in values:",
            "            if not isinstance(val, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = values[0].value ^ values[1].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"xor\", args[0], args[1]], typ=UINT256_T)",
            "",
            "",
            "class BitwiseNot(BuiltinFunctionT):",
            "    _id = \"bitwise_not\"",
            "    _inputs = [(\"x\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`bitwise_not()` is deprecated! Please use the ~ operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Int):",
            "            raise UnfoldableNode",
            "",
            "        value = value.value",
            "",
            "        value = (2**256 - 1) - value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list([\"not\", args[0]], typ=UINT256_T)",
            "",
            "",
            "class Shift(BuiltinFunctionT):",
            "    _id = \"shift\"",
            "    _inputs = [(\"x\", (UINT256_T, INT256_T)), (\"_shift_bits\", IntegerT.any())]",
            "    _return_type = UINT256_T",
            "    _warned = False",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(\"`shift()` is deprecated! Please use the << or >> operator instead.\", node)",
            "            self.__class__._warned = True",
            "",
            "        validate_call_args(node, 2)",
            "        args = [i.get_folded_value() for i in node.args]",
            "        if any(not isinstance(i, vy_ast.Int) for i in args):",
            "            raise UnfoldableNode",
            "        value, shift = [i.value for i in args]",
            "        if shift < -256 or shift > 256:",
            "            # this validation is performed to prevent the compiler from hanging",
            "            # rather than for correctness because the post-folded constant would",
            "            # have been validated anyway",
            "            raise InvalidLiteral(\"Shift must be between -256 and 256\", node.args[1])",
            "",
            "        if shift < 0:",
            "            value = value >> -shift",
            "        else:",
            "            value = (value << shift) % (2**256)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    def fetch_call_return(self, node):",
            "        # return type is the type of the first argument",
            "        return self.infer_arg_types(node)[0]",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        # return a concrete type instead of SignedIntegerAbstractType",
            "        arg_ty = get_possible_types_from_node(node.args[0])[0]",
            "        shift_ty = get_possible_types_from_node(node.args[1])[0]",
            "        return [arg_ty, shift_ty]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # \"gshr\" -- generalized right shift",
            "        argty = args[0].typ",
            "        GSHR = sar if argty.is_signed else shr",
            "",
            "        with args[0].cache_when_complex(\"to_shift\") as (b1, arg), args[1].cache_when_complex(",
            "            \"bits\"",
            "        ) as (b2, bits):",
            "            neg_bits = [\"sub\", 0, bits]",
            "            ret = [\"if\", [\"slt\", bits, 0], GSHR(neg_bits, arg), shl(bits, arg)]",
            "            return b1.resolve(b2.resolve(IRnode.from_list(ret, typ=argty)))",
            "",
            "",
            "class _AddMulMod(BuiltinFunctionT):",
            "    _inputs = [(\"a\", UINT256_T), (\"b\", UINT256_T), (\"c\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 3)",
            "        args = [i.get_folded_value() for i in node.args]",
            "        if isinstance(args[2], vy_ast.Int) and args[2].value == 0:",
            "            raise ZeroDivisionException(\"Modulo by 0\", node.args[2])",
            "        for arg in args:",
            "            if not isinstance(arg, vy_ast.Int):",
            "                raise UnfoldableNode",
            "",
            "        value = self._eval_fn(args[0].value, args[1].value) % args[2].value",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        x, y, z = args",
            "        with x.cache_when_complex(\"x\") as (b1, x):",
            "            with y.cache_when_complex(\"y\") as (b2, y):",
            "                with z.cache_when_complex(\"z\") as (b3, z):",
            "                    ret = IRnode.from_list(",
            "                        [\"seq\", [\"assert\", z], [self._opcode, x, y, z]], typ=UINT256_T",
            "                    )",
            "                    return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "",
            "class AddMod(_AddMulMod):",
            "    _id = \"uint256_addmod\"",
            "    _eval_fn = operator.add",
            "    _opcode = \"addmod\"",
            "",
            "",
            "class MulMod(_AddMulMod):",
            "    _id = \"uint256_mulmod\"",
            "    _eval_fn = operator.mul",
            "    _opcode = \"mulmod\"",
            "",
            "",
            "class PowMod256(BuiltinFunctionT):",
            "    _id = \"pow_mod256\"",
            "    _inputs = [(\"a\", UINT256_T), (\"b\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 2)",
            "        values = [i.get_folded_value() for i in node.args]",
            "        if any(not isinstance(i, vy_ast.Int) for i in values):",
            "            raise UnfoldableNode",
            "",
            "        left, right = values",
            "        value = pow(left.value, right.value, 2**256)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    def build_IR(self, expr, context):",
            "        left = Expr.parse_value_expr(expr.args[0], context)",
            "        right = Expr.parse_value_expr(expr.args[1], context)",
            "        return IRnode.from_list([\"exp\", left, right], typ=left.typ)",
            "",
            "",
            "class Abs(BuiltinFunctionT):",
            "    _id = \"abs\"",
            "    _inputs = [(\"value\", INT256_T)]",
            "    _return_type = INT256_T",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Int):",
            "            raise UnfoldableNode",
            "",
            "        value = abs(value.value)",
            "        return vy_ast.Int.from_node(node, value=value)",
            "",
            "    def build_IR(self, expr, context):",
            "        value = Expr.parse_value_expr(expr.args[0], context)",
            "        sub = [",
            "            \"with\",",
            "            \"orig\",",
            "            value,",
            "            [",
            "                \"if\",",
            "                [\"slt\", \"orig\", 0],",
            "                # clamp orig != -2**255 (because it maps to itself under negation)",
            "                [\"seq\", [\"assert\", [\"ne\", \"orig\", [\"sub\", 0, \"orig\"]]], [\"sub\", 0, \"orig\"]],",
            "                \"orig\",",
            "            ],",
            "        ]",
            "        return IRnode.from_list(sub, typ=INT256_T)",
            "",
            "",
            "# CREATE* functions",
            "",
            "CREATE2_SENTINEL = dummy_node_for_type(BYTES32_T)",
            "",
            "",
            "# create helper functions",
            "# generates CREATE op sequence + zero check for result",
            "def _create_ir(value, buf, length, salt, revert_on_failure=True):",
            "    args = [value, buf, length]",
            "    create_op = \"create\"",
            "    if salt is not CREATE2_SENTINEL:",
            "        create_op = \"create2\"",
            "        args.append(salt)",
            "",
            "    ret = IRnode.from_list(ensure_eval_once(\"create_builtin\", [create_op, *args]))",
            "",
            "    if not revert_on_failure:",
            "        return ret",
            "",
            "    ret = clamp_nonzero(ret)",
            "    ret.set_error_msg(f\"{create_op} failed\")",
            "    return ret",
            "",
            "",
            "# calculate the gas used by create for a given number of bytes",
            "def _create_addl_gas_estimate(size, should_use_create2):",
            "    ret = 200 * size",
            "    if should_use_create2:",
            "        ret += SHA3_PER_WORD * ceil32(size) // 32",
            "    return ret",
            "",
            "",
            "def eip1167_bytecode():",
            "    # NOTE cyclic import?",
            "    from vyper.ir.compile_ir import assembly_to_evm",
            "",
            "    loader_asm = [",
            "        \"PUSH1\",",
            "        0x2D,",
            "        \"RETURNDATASIZE\",",
            "        \"DUP2\",",
            "        \"PUSH1\",",
            "        0x09,",
            "        \"RETURNDATASIZE\",",
            "        \"CODECOPY\",",
            "        \"RETURN\",",
            "    ]",
            "    forwarder_pre_asm = [",
            "        \"CALLDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"CALLDATACOPY\",",
            "        \"RETURNDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"CALLDATASIZE\",",
            "        \"RETURNDATASIZE\",",
            "        \"PUSH20\",  # [address to delegate to]",
            "    ]",
            "    forwarder_post_asm = [",
            "        \"GAS\",",
            "        \"DELEGATECALL\",",
            "        \"RETURNDATASIZE\",",
            "        \"DUP3\",",
            "        \"DUP1\",",
            "        \"RETURNDATACOPY\",",
            "        \"SWAP1\",",
            "        \"RETURNDATASIZE\",",
            "        \"SWAP2\",",
            "        \"PUSH1\",",
            "        0x2B,  # jumpdest of whole program.",
            "        \"JUMPI\",",
            "        \"REVERT\",",
            "        \"JUMPDEST\",",
            "        \"RETURN\",",
            "    ]",
            "    return (",
            "        assembly_to_evm(loader_asm)[0],",
            "        assembly_to_evm(forwarder_pre_asm)[0],",
            "        assembly_to_evm(forwarder_post_asm)[0],",
            "    )",
            "",
            "",
            "# \"standard\" initcode for code which can be larger than 256 bytes.",
            "# returns the code starting from 0x0b with len `codesize`.",
            "# NOTE: it assumes codesize <= 2**24.",
            "def _create_preamble(codesize):",
            "    from vyper.ir.compile_ir import assembly_to_evm",
            "",
            "    evm_len = 0x0B  # 11 bytes",
            "    asm = [",
            "        # use PUSH3 to be able to deal with larger contracts",
            "        \"PUSH3\",",
            "        # blank space for codesize",
            "        0x00,",
            "        0x00,",
            "        0x00,",
            "        \"RETURNDATASIZE\",",
            "        \"DUP2\",",
            "        \"PUSH1\",",
            "        evm_len,",
            "        \"RETURNDATASIZE\",",
            "        \"CODECOPY\",",
            "        \"RETURN\",",
            "    ]",
            "    evm = assembly_to_evm(asm)[0]",
            "    assert len(evm) == evm_len, evm",
            "",
            "    shl_bits = (evm_len - 4) * 8  # codesize needs to go right after the PUSH3",
            "    # mask codesize into the aforementioned \"blank space\"",
            "    return [\"or\", bytes_to_int(evm), shl(shl_bits, codesize)], evm_len",
            "",
            "",
            "class _CreateBase(BuiltinFunctionT):",
            "    _kwargs = {",
            "        \"value\": KwargSettings(UINT256_T, zero_value),",
            "        \"salt\": KwargSettings(BYTES32_T, empty_value),",
            "        \"revert_on_failure\": KwargSettings(BoolT(), True, require_literal=True),",
            "    }",
            "    _return_type = AddressT()",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # errmsg something like f\"Cannot use {self._id} in pure fn\"",
            "        context.check_is_not_constant(f\"use {self._id}\", expr)",
            "",
            "        should_use_create2 = \"salt\" in [kwarg.arg for kwarg in expr.keywords]",
            "",
            "        if not should_use_create2:",
            "            kwargs[\"salt\"] = CREATE2_SENTINEL",
            "",
            "        ir_builder = self._build_create_IR(expr, args, context, **kwargs)",
            "",
            "        add_gas_estimate = self._add_gas_estimate(args, should_use_create2)",
            "",
            "        return IRnode.from_list(",
            "            ir_builder, typ=AddressT(), annotation=self._id, add_gas_estimate=add_gas_estimate",
            "        )",
            "",
            "",
            "class CreateMinimalProxyTo(_CreateBase):",
            "    # create an EIP1167 \"minimal proxy\" to the target contract",
            "",
            "    _id = \"create_minimal_proxy_to\"",
            "    _inputs = [(\"target\", AddressT())]",
            "",
            "    def _add_gas_estimate(self, args, should_use_create2):",
            "        a, b, c = eip1167_bytecode()",
            "        bytecode_len = 20 + len(b) + len(c)",
            "        return _create_addl_gas_estimate(bytecode_len, should_use_create2)",
            "",
            "    def _build_create_IR(self, expr, args, context, value, salt, revert_on_failure):",
            "        target_address = args[0]",
            "",
            "        buf = context.new_internal_variable(BytesT(96))",
            "",
            "        loader_evm, forwarder_pre_evm, forwarder_post_evm = eip1167_bytecode()",
            "        # Adjust to 32-byte boundaries",
            "        preamble_length = len(loader_evm) + len(forwarder_pre_evm)",
            "        forwarder_preamble = bytes_to_int(",
            "            loader_evm + forwarder_pre_evm + b\"\\x00\" * (32 - preamble_length)",
            "        )",
            "        forwarder_post = bytes_to_int(forwarder_post_evm + b\"\\x00\" * (32 - len(forwarder_post_evm)))",
            "",
            "        # left-align the target",
            "        if target_address.is_literal:",
            "            # note: should move to optimizer once we have",
            "            # codesize optimization pipeline",
            "            aligned_target = args[0].value << 96",
            "        else:",
            "            aligned_target = shl(96, target_address)",
            "",
            "        buf_len = preamble_length + 20 + len(forwarder_post_evm)",
            "",
            "        return [",
            "            \"seq\",",
            "            [\"mstore\", buf, forwarder_preamble],",
            "            [\"mstore\", add_ofst(buf, preamble_length), aligned_target],",
            "            [\"mstore\", add_ofst(buf, preamble_length + 20), forwarder_post],",
            "            _create_ir(value, buf, buf_len, salt, revert_on_failure),",
            "        ]",
            "",
            "",
            "class CreateForwarderTo(CreateMinimalProxyTo):",
            "    _warned = False",
            "",
            "    def build_IR(self, expr, context):",
            "        if not self._warned:",
            "            vyper_warn(",
            "                \"`create_forwarder_to` is a deprecated alias of `create_minimal_proxy_to`!\", expr",
            "            )",
            "            self._warned = True",
            "",
            "        return super().build_IR(expr, context)",
            "",
            "",
            "class CreateCopyOf(_CreateBase):",
            "    _id = \"create_copy_of\"",
            "    _inputs = [(\"target\", AddressT())]",
            "",
            "    @property",
            "    def _preamble_len(self):",
            "        return 11",
            "",
            "    def _add_gas_estimate(self, args, should_use_create2):",
            "        # max possible runtime length + preamble length",
            "        return _create_addl_gas_estimate(EIP_170_LIMIT + self._preamble_len, should_use_create2)",
            "",
            "    def _build_create_IR(self, expr, args, context, value, salt, revert_on_failure):",
            "        target = args[0]",
            "",
            "        # something we can pass to scope_multi",
            "        with scope_multi(",
            "            (target, value, salt), (\"create_target\", \"create_value\", \"create_salt\")",
            "        ) as (b1, (target, value, salt)):",
            "            codesize = IRnode.from_list([\"extcodesize\", target])",
            "            msize = IRnode.from_list([\"msize\"])",
            "            with scope_multi((codesize, msize), (\"target_codesize\", \"mem_ofst\")) as (",
            "                b2,",
            "                (codesize, mem_ofst),",
            "            ):",
            "                ir = [\"seq\"]",
            "",
            "                # make sure there is actually code at the target",
            "                check_codesize = [\"assert\", codesize]",
            "                ir.append(",
            "                    IRnode.from_list(check_codesize, error_msg=\"empty target (create_copy_of)\")",
            "                )",
            "",
            "                # store the preamble at msize + 22 (zero padding)",
            "                preamble, preamble_len = _create_preamble(codesize)",
            "                assert preamble_len == self._preamble_len",
            "",
            "                ir.append([\"mstore\", mem_ofst, preamble])",
            "",
            "                # copy the target code into memory. current layout:",
            "                # msize | 00...00 (22 0's) | preamble | bytecode",
            "                ir.append([\"extcodecopy\", target, add_ofst(mem_ofst, 32), 0, codesize])",
            "",
            "                buf = add_ofst(mem_ofst, 32 - preamble_len)",
            "                buf_len = [\"add\", codesize, preamble_len]",
            "",
            "                ir.append(_create_ir(value, buf, buf_len, salt, revert_on_failure))",
            "",
            "                return b1.resolve(b2.resolve(ir))",
            "",
            "",
            "class CreateFromBlueprint(_CreateBase):",
            "    _id = \"create_from_blueprint\"",
            "    _inputs = [(\"target\", AddressT())]",
            "    _kwargs = {",
            "        \"value\": KwargSettings(UINT256_T, zero_value),",
            "        \"salt\": KwargSettings(BYTES32_T, empty_value),",
            "        \"raw_args\": KwargSettings(BoolT(), False, require_literal=True),",
            "        \"code_offset\": KwargSettings(UINT256_T, IRnode.from_list(3, typ=UINT256_T)),",
            "        \"revert_on_failure\": KwargSettings(BoolT(), True, require_literal=True),",
            "    }",
            "    _has_varargs = True",
            "",
            "    def _add_gas_estimate(self, args, should_use_create2):",
            "        ctor_args = ir_tuple_from_args(args[1:])",
            "        # max possible size of init code",
            "        maxlen = EIP_170_LIMIT + ctor_args.typ.abi_type.size_bound()",
            "        return _create_addl_gas_estimate(maxlen, should_use_create2)",
            "",
            "    def _build_create_IR(",
            "        self, expr, args, context, value, salt, code_offset, raw_args, revert_on_failure",
            "    ):",
            "        target = args[0]",
            "        ctor_args = args[1:]",
            "",
            "        ctor_args = [ensure_in_memory(arg, context) for arg in ctor_args]",
            "",
            "        if raw_args:",
            "            if len(ctor_args) != 1 or not isinstance(ctor_args[0].typ, BytesT):",
            "                raise StructureException(\"raw_args must be used with exactly 1 bytes argument\")",
            "",
            "            with ctor_args[0].cache_when_complex(\"arg\") as (b1, arg):",
            "                argbuf = bytes_data_ptr(arg)",
            "                argslen = get_bytearray_length(arg)",
            "                bufsz = arg.typ.maxlen",
            "                return b1.resolve(",
            "                    self._helper(",
            "                        argbuf, bufsz, target, value, salt, argslen, code_offset, revert_on_failure",
            "                    )",
            "                )",
            "        else:",
            "            # encode the varargs",
            "            to_encode = ir_tuple_from_args(ctor_args)",
            "",
            "            # pretend we allocated enough memory for the encoder",
            "            # (we didn't, but we are clobbering unused memory so it's safe.)",
            "            bufsz = to_encode.typ.abi_type.size_bound()",
            "            argbuf = context.new_internal_variable(get_type_for_exact_size(bufsz))",
            "",
            "            # return a complex expression which writes to memory and returns",
            "            # the length of the encoded data",
            "            argslen = abi_encode(argbuf, to_encode, context, bufsz=bufsz, returns_len=True)",
            "            return self._helper(",
            "                argbuf, bufsz, target, value, salt, argslen, code_offset, revert_on_failure",
            "            )",
            "",
            "    def _helper(self, argbuf, bufsz, target, value, salt, argslen, code_offset, revert_on_failure):",
            "        # NOTE: we need to invoke the abi encoder before evaluating MSIZE,",
            "        # then copy the abi encoded buffer to past-the-end of the initcode",
            "        # (since the abi encoder could write to fresh memory).",
            "        # it would be good to not require the memory copy, but need",
            "        # to evaluate memory safety.",
            "        with scope_multi(",
            "            (target, value, salt, argslen, code_offset),",
            "            (\"create_target\", \"create_value\", \"create_salt\", \"encoded_args_len\", \"code_offset\"),",
            "        ) as (b1, (target, value, salt, encoded_args_len, code_offset)):",
            "            codesize = IRnode.from_list([\"sub\", [\"extcodesize\", target], code_offset])",
            "            # copy code to memory starting from msize. we are clobbering",
            "            # unused memory so it's safe.",
            "            msize = IRnode.from_list([\"msize\"], location=MEMORY)",
            "            with scope_multi((codesize, msize), (\"target_codesize\", \"mem_ofst\")) as (",
            "                b2,",
            "                (codesize, mem_ofst),",
            "            ):",
            "                ir = [\"seq\"]",
            "",
            "                # make sure there is code at the target, and that",
            "                # code_ofst <= (extcodesize target).",
            "                # (note if code_ofst > (extcodesize target), would be",
            "                # OOG on the EXTCODECOPY)",
            "                # (code_ofst == (extcodesize target) would be empty",
            "                # initcode, which we disallow for hygiene reasons -",
            "                # same as `create_copy_of` on an empty target).",
            "                check_codesize = [\"assert\", [\"sgt\", codesize, 0]]",
            "                ir.append(",
            "                    IRnode.from_list(",
            "                        check_codesize, error_msg=\"empty target (create_from_blueprint)\"",
            "                    )",
            "                )",
            "",
            "                # copy the target code into memory.",
            "                # layout starting from mem_ofst:",
            "                # <target initcode> | <abi-encoded args OR arg buffer if raw_arg=True>",
            "                ir.append([\"extcodecopy\", target, mem_ofst, code_offset, codesize])",
            "                ir.append(copy_bytes(add_ofst(mem_ofst, codesize), argbuf, encoded_args_len, bufsz))",
            "",
            "                # theoretically, dst = \"msize\", but just be safe.",
            "                # if len(ctor_args) > 0:",
            "                #    dst = add_ofst(mem_ofst, codesize)",
            "                #    encoded_args_len = self._encode_args(dst, ctor_args, context)",
            "                # else:",
            "                #    encoded_args_len = 0",
            "",
            "                length = [\"add\", codesize, encoded_args_len]",
            "",
            "                ir.append(_create_ir(value, mem_ofst, length, salt, revert_on_failure))",
            "",
            "                return b1.resolve(b2.resolve(ir))",
            "",
            "",
            "class _UnsafeMath(BuiltinFunctionT):",
            "    # TODO add unsafe math for `decimal`s",
            "    _inputs = [(\"a\", IntegerT.any()), (\"b\", IntegerT.any())]",
            "",
            "    def __repr__(self):",
            "        return f\"builtin function unsafe_{self.op}\"",
            "",
            "    def fetch_call_return(self, node):",
            "        return_type = self.infer_arg_types(node).pop()",
            "        return return_type",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "",
            "        types_list = get_common_types(*node.args, filter_fn=lambda x: isinstance(x, IntegerT))",
            "        if not types_list:",
            "            raise TypeMismatch(f\"unsafe_{self.op} called on dislike types\", node)",
            "",
            "        type_ = types_list.pop()",
            "        return [type_, type_]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        (a, b) = args",
            "        op = self.op",
            "",
            "        assert a.typ == b.typ, \"unreachable\"",
            "",
            "        otyp = a.typ",
            "",
            "        if op == \"div\" and a.typ.is_signed:",
            "            op = \"sdiv\"",
            "",
            "        ret = [op, a, b]",
            "",
            "        if a.typ.bits < 256:",
            "            # wrap for ops which could under/overflow",
            "            if a.typ.is_signed:",
            "                # e.g. int128 -> (signextend 15 (add x y))",
            "                ret = promote_signed_int(ret, a.typ.bits)",
            "            else:",
            "                # e.g. uint8 -> (mod (add x y) 256)",
            "                # TODO mod_bound could be a really large literal",
            "                ret = [\"mod\", ret, 2**a.typ.bits]",
            "",
            "        return IRnode.from_list(ret, typ=otyp)",
            "",
            "        # TODO handle decimal case",
            "",
            "",
            "class UnsafeAdd(_UnsafeMath):",
            "    _id = \"unsafe_add\"",
            "    op = \"add\"",
            "",
            "",
            "class UnsafeSub(_UnsafeMath):",
            "    _id = \"unsafe_sub\"",
            "    op = \"sub\"",
            "",
            "",
            "class UnsafeMul(_UnsafeMath):",
            "    _id = \"unsafe_mul\"",
            "    op = \"mul\"",
            "",
            "",
            "class UnsafeDiv(_UnsafeMath):",
            "    _id = \"unsafe_div\"",
            "    op = \"div\"",
            "",
            "",
            "class _MinMax(BuiltinFunctionT):",
            "    _inputs = [(\"a\", (DecimalT(), IntegerT.any())), (\"b\", (DecimalT(), IntegerT.any()))]",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 2)",
            "",
            "        left = node.args[0].get_folded_value()",
            "        right = node.args[1].get_folded_value()",
            "        if not isinstance(left, type(right)):",
            "            raise UnfoldableNode",
            "        if not isinstance(left, (vy_ast.Decimal, vy_ast.Int)):",
            "            raise UnfoldableNode",
            "",
            "        types_list = get_common_types(",
            "            *(left, right), filter_fn=lambda x: isinstance(x, (IntegerT, DecimalT))",
            "        )",
            "        if not types_list:",
            "            raise TypeMismatch(\"Cannot perform action between dislike numeric types\", node)",
            "",
            "        value = self._eval_fn(left.value, right.value)",
            "        return type(left).from_node(node, value=value)",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "",
            "        types_list = get_common_types(",
            "            *node.args, filter_fn=lambda x: isinstance(x, (IntegerT, DecimalT))",
            "        )",
            "        if not types_list:",
            "            raise TypeMismatch(\"Cannot perform action between dislike numeric types\", node)",
            "",
            "        return types_list",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        types_list = self.fetch_call_return(node)",
            "        # type mismatch should have been caught in `fetch_call_return`",
            "        assert expected_return_typ in types_list",
            "        return [expected_return_typ, expected_return_typ]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        op = self._opcode",
            "",
            "        with args[0].cache_when_complex(\"_l\") as (b1, left), args[1].cache_when_complex(\"_r\") as (",
            "            b2,",
            "            right,",
            "        ):",
            "            if left.typ == right.typ:",
            "                if left.typ != UINT256_T:",
            "                    # if comparing like types that are not uint256, use SLT or SGT",
            "                    op = f\"s{op}\"",
            "                o = [\"select\", [op, left, right], left, right]",
            "                otyp = left.typ",
            "",
            "            else:",
            "                raise TypeMismatch(f\"Minmax types incompatible: {left.typ.typ} {right.typ.typ}\")",
            "            return IRnode.from_list(b1.resolve(b2.resolve(o)), typ=otyp)",
            "",
            "",
            "class Min(_MinMax):",
            "    _id = \"min\"",
            "    _eval_fn = min",
            "    _opcode = \"lt\"",
            "",
            "",
            "class Max(_MinMax):",
            "    _id = \"max\"",
            "    _eval_fn = max",
            "    _opcode = \"gt\"",
            "",
            "",
            "class Uint2Str(BuiltinFunctionT):",
            "    _id = \"uint2str\"",
            "    _inputs = [(\"x\", IntegerT.unsigneds())]",
            "",
            "    def fetch_call_return(self, node):",
            "        arg_t = self.infer_arg_types(node)[0]",
            "        bits = arg_t.bits",
            "        len_needed = math.ceil(bits * math.log(2) / math.log(10))",
            "        return StringT(len_needed)",
            "",
            "    def _try_fold(self, node):",
            "        validate_call_args(node, 1)",
            "        value = node.args[0].get_folded_value()",
            "        if not isinstance(value, vy_ast.Int):",
            "            raise UnfoldableNode",
            "",
            "        value = value.value",
            "        if value < 0:",
            "            raise InvalidType(\"Only unsigned ints allowed\", node)",
            "        value = str(value)",
            "        return vy_ast.Str.from_node(node, value=value)",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "        input_type = get_possible_types_from_node(node.args[0]).pop()",
            "        return [input_type]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return_t = self.fetch_call_return(expr)",
            "        n_digits = return_t.maxlen",
            "",
            "        with args[0].cache_when_complex(\"val\") as (b1, val):",
            "            buf = context.new_internal_variable(return_t)",
            "",
            "            i = IRnode.from_list(context.fresh_varname(\"uint2str_i\"), typ=UINT256_T)",
            "",
            "            ret = [\"repeat\", i, 0, n_digits + 1, n_digits + 1]",
            "",
            "            body = [",
            "                \"seq\",",
            "                [",
            "                    \"if\",",
            "                    [\"eq\", val, 0],",
            "                    # clobber val, and return it as a pointer",
            "                    [",
            "                        \"seq\",",
            "                        [\"mstore\", [\"sub\", add_ofst(buf, n_digits), i], i],",
            "                        [\"set\", val, [\"sub\", add_ofst(buf, n_digits), i]],",
            "                        \"break\",",
            "                    ],",
            "                    [",
            "                        \"seq\",",
            "                        [",
            "                            \"mstore\",",
            "                            [\"sub\", add_ofst(buf, n_digits), i],",
            "                            [\"add\", 48, [\"mod\", val, 10]],",
            "                        ],",
            "                        [\"set\", val, [\"div\", val, 10]],",
            "                    ],",
            "                ],",
            "            ]",
            "            ret.append(body)",
            "",
            "            # \"0\" has hex representation 0x00..0130..00",
            "            # if (val == 0) {",
            "            #   return \"0\"",
            "            # } else {",
            "            #   do the loop",
            "            # }",
            "            ret = [",
            "                \"if\",",
            "                [\"eq\", val, 0],",
            "                [\"seq\", [\"mstore\", add_ofst(buf, 1), ord(\"0\")], [\"mstore\", buf, 1], buf],",
            "                [\"seq\", ret, val],",
            "            ]",
            "",
            "            return b1.resolve(IRnode.from_list(ret, location=MEMORY, typ=return_t))",
            "",
            "",
            "class Sqrt(BuiltinFunctionT):",
            "    _id = \"sqrt\"",
            "    _inputs = [(\"d\", DecimalT())]",
            "    _return_type = DecimalT()",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # TODO fix cyclic dependency with codegen/stmt.py",
            "        from ._utils import generate_inline_function",
            "",
            "        arg = args[0]",
            "        # TODO: reify decimal and integer sqrt paths (see isqrt)",
            "        with arg.cache_when_complex(\"x\") as (b1, arg):",
            "            sqrt_code = \"\"\"",
            "assert x >= 0.0",
            "z: decimal = 0.0",
            "",
            "if x == 0.0:",
            "    z = 0.0",
            "else:",
            "    z = x / 2.0 + 0.5",
            "    y: decimal = x",
            "",
            "    for i: uint256 in range(256):",
            "        if z == y:",
            "            break",
            "        y = z",
            "        z = (x / z + z) / 2.0",
            "            \"\"\"",
            "",
            "            x_type = DecimalT()",
            "            placeholder_copy = [\"pass\"]",
            "            # Steal current position if variable is already allocated.",
            "            if arg.value == \"mload\":",
            "                new_var_pos = arg.args[0]",
            "            # Other locations need to be copied.",
            "            else:",
            "                new_var_pos = context.new_internal_variable(x_type)",
            "                placeholder_copy = [\"mstore\", new_var_pos, arg]",
            "            # Create input variables.",
            "            variables = {\"x\": VariableRecord(name=\"x\", pos=new_var_pos, typ=x_type, mutable=False)}",
            "            # Dictionary to update new (i.e. typecheck) namespace",
            "            variables_2 = {\"x\": VarInfo(DecimalT())}",
            "            # Generate inline IR.",
            "            new_ctx, sqrt_ir = generate_inline_function(",
            "                code=sqrt_code,",
            "                variables=variables,",
            "                variables_2=variables_2,",
            "                memory_allocator=context.memory_allocator,",
            "            )",
            "            z_ir = new_ctx.vars[\"z\"].as_ir_node()",
            "            ret = IRnode.from_list(",
            "                [\"seq\", placeholder_copy, sqrt_ir, z_ir], typ=DecimalT(), location=MEMORY",
            "            )",
            "            return b1.resolve(ret)",
            "",
            "",
            "class ISqrt(BuiltinFunctionT):",
            "    _id = \"isqrt\"",
            "    _inputs = [(\"d\", UINT256_T)]",
            "    _return_type = UINT256_T",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        # calculate isqrt using the babylonian method",
            "",
            "        y, z = \"y\", \"z\"",
            "        arg = args[0]",
            "        with arg.cache_when_complex(\"x\") as (b1, x):",
            "            ret = [",
            "                \"seq\",",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (128 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(128, y)], [\"set\", z, shl(64, z)]],",
            "                ],",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (64 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(64, y)], [\"set\", z, shl(32, z)]],",
            "                ],",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (32 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(32, y)], [\"set\", z, shl(16, z)]],",
            "                ],",
            "                [",
            "                    \"if\",",
            "                    [\"ge\", y, 2 ** (16 + 8)],",
            "                    [\"seq\", [\"set\", y, shr(16, y)], [\"set\", z, shl(8, z)]],",
            "                ],",
            "            ]",
            "            ret.append([\"set\", z, [\"div\", [\"mul\", z, [\"add\", y, 2**16]], 2**18]])",
            "",
            "            for _ in range(7):",
            "                ret.append([\"set\", z, [\"div\", [\"add\", [\"div\", x, z], z], 2]])",
            "",
            "            # note: If ``x+1`` is a perfect square, then the Babylonian",
            "            # algorithm oscillates between floor(sqrt(x)) and ceil(sqrt(x)) in",
            "            # consecutive iterations. return the floor value always.",
            "",
            "            ret.append([\"with\", \"t\", [\"div\", x, z], [\"select\", [\"lt\", z, \"t\"], z, \"t\"]])",
            "",
            "            ret = [\"with\", y, x, [\"with\", z, 181, ret]]",
            "            return b1.resolve(IRnode.from_list(ret, typ=UINT256_T))",
            "",
            "",
            "class Empty(TypenameFoldedFunctionT):",
            "    _id = \"empty\"",
            "",
            "    def fetch_call_return(self, node):",
            "        type_ = self.infer_arg_types(node)[0].typedef",
            "        if isinstance(type_, HashMapT):",
            "            raise TypeMismatch(\"Cannot use empty on HashMap\", node)",
            "        return type_",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        output_type = args[0]",
            "        return IRnode(\"~empty\", typ=output_type)",
            "",
            "",
            "class Breakpoint(BuiltinFunctionT):",
            "    _id = \"breakpoint\"",
            "    _inputs: list = []",
            "",
            "    _warned = False",
            "",
            "    def fetch_call_return(self, node):",
            "        if not self._warned:",
            "            vyper_warn(\"`breakpoint` should only be used for debugging!\", node)",
            "            self._warned = True",
            "",
            "        return None",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        return IRnode.from_list(\"breakpoint\", annotation=\"breakpoint()\")",
            "",
            "",
            "class Print(BuiltinFunctionT):",
            "    _id = \"print\"",
            "    _inputs: list = []",
            "    _has_varargs = True",
            "    _kwargs = {\"hardhat_compat\": KwargSettings(BoolT(), False, require_literal=True)}",
            "",
            "    _warned = False",
            "",
            "    def fetch_call_return(self, node):",
            "        if not self._warned:",
            "            vyper_warn(\"`print` should only be used for debugging!\", node)",
            "            self._warned = True",
            "",
            "        return None",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        args_as_tuple = ir_tuple_from_args(args)",
            "        args_abi_t = args_as_tuple.typ.abi_type",
            "",
            "        # create a signature like \"log(uint256)\"",
            "        sig = \"log\" + \"(\" + \",\".join([arg.typ.abi_type.selector_name() for arg in args]) + \")\"",
            "",
            "        if kwargs[\"hardhat_compat\"] is True:",
            "            method_id = method_id_int(sig)",
            "            buflen = 32 + args_abi_t.size_bound()",
            "",
            "            # 32 bytes extra space for the method id",
            "            buf = context.new_internal_variable(get_type_for_exact_size(buflen))",
            "",
            "            ret = [\"seq\"]",
            "            ret.append([\"mstore\", buf, method_id])",
            "            encode = abi_encode(add_ofst(buf, 32), args_as_tuple, context, buflen, returns_len=True)",
            "",
            "        else:",
            "            method_id = method_id_int(\"log(string,bytes)\")",
            "            schema = args_abi_t.selector_name().encode(\"utf-8\")",
            "            if len(schema) > 32:",
            "                raise CompilerPanic(f\"print signature too long: {schema}\")",
            "",
            "            schema_t = StringT(len(schema))",
            "            schema_buf = context.new_internal_variable(schema_t)",
            "            ret = [\"seq\"]",
            "            ret.append([\"mstore\", schema_buf, len(schema)])",
            "",
            "            # TODO use Expr.make_bytelike, or better have a `bytestring` IRnode type",
            "            ret.append(",
            "                [\"mstore\", add_ofst(schema_buf, 32), bytes_to_int(schema.ljust(32, b\"\\x00\"))]",
            "            )",
            "",
            "            payload_buflen = args_abi_t.size_bound()",
            "            payload_t = BytesT(payload_buflen)",
            "",
            "            # 32 bytes extra space for the method id",
            "            payload_buf = context.new_internal_variable(payload_t)",
            "            encode_payload = abi_encode(",
            "                add_ofst(payload_buf, 32), args_as_tuple, context, payload_buflen, returns_len=True",
            "            )",
            "",
            "            ret.append([\"mstore\", payload_buf, encode_payload])",
            "            args_as_tuple = ir_tuple_from_args(",
            "                [",
            "                    IRnode.from_list(schema_buf, typ=schema_t, location=MEMORY),",
            "                    IRnode.from_list(payload_buf, typ=payload_t, location=MEMORY),",
            "                ]",
            "            )",
            "",
            "            # add 32 for method id padding",
            "            buflen = 32 + args_as_tuple.typ.abi_type.size_bound()",
            "            buf = context.new_internal_variable(get_type_for_exact_size(buflen))",
            "            ret.append([\"mstore\", buf, method_id])",
            "            encode = abi_encode(add_ofst(buf, 32), args_as_tuple, context, buflen, returns_len=True)",
            "",
            "        # debug address that tooling uses",
            "        CONSOLE_ADDRESS = 0x000000000000000000636F6E736F6C652E6C6F67",
            "        ret.append(",
            "            [\"staticcall\", \"gas\", CONSOLE_ADDRESS, add_ofst(buf, 28), [\"add\", 4, encode], 0, 0]",
            "        )",
            "",
            "        return IRnode.from_list(ret, annotation=\"print:\" + sig)",
            "",
            "",
            "class ABIEncode(BuiltinFunctionT):",
            "    _id = \"abi_encode\"",
            "    # signature: *, ensure_tuple=<literal_bool> -> Bytes[<calculated len>]",
            "    # explanation of ensure_tuple:",
            "    # default is to force even a single value into a tuple,",
            "    # e.g. _abi_encode(bytes) -> _abi_encode((bytes,))",
            "    #      _abi_encode((bytes,)) -> _abi_encode(((bytes,),))",
            "    # this follows the encoding convention for functions:",
            "    # ://docs.soliditylang.org/en/v0.8.6/abi-spec.html#function-selector-and-argument-encoding",
            "    # if this is turned off, then bytes will be encoded as bytes.",
            "",
            "    _inputs: list = []",
            "    _has_varargs = True",
            "",
            "    _kwargs = {",
            "        \"ensure_tuple\": KwargSettings(BoolT(), True, require_literal=True),",
            "        \"method_id\": KwargSettings((BYTES4_T, BytesT(4)), None, require_literal=True),",
            "    }",
            "",
            "    def infer_kwarg_types(self, node):",
            "        ret = {}",
            "        for kwarg in node.keywords:",
            "            kwarg_name = kwarg.arg",
            "            validate_expected_type(kwarg.value, self._kwargs[kwarg_name].typ)",
            "",
            "            typ = get_exact_type_from_node(kwarg.value)",
            "            if kwarg_name == \"method_id\" and isinstance(typ, BytesT):",
            "                if typ.length != 4:",
            "                    raise InvalidLiteral(\"method_id must be exactly 4 bytes!\", kwarg.value)",
            "",
            "            ret[kwarg_name] = typ",
            "        return ret",
            "",
            "    def fetch_call_return(self, node):",
            "        self._validate_arg_types(node)",
            "        ensure_tuple = next(",
            "            (arg.value.value for arg in node.keywords if arg.arg == \"ensure_tuple\"), True",
            "        )",
            "        assert isinstance(ensure_tuple, bool)",
            "        has_method_id = \"method_id\" in [arg.arg for arg in node.keywords]",
            "",
            "        # figure out the output type by converting",
            "        # the types to ABI_Types and calling size_bound API",
            "        arg_abi_types = []",
            "        arg_types = self.infer_arg_types(node)",
            "        for arg_t in arg_types:",
            "            arg_abi_types.append(arg_t.abi_type)",
            "",
            "        # special case, no tuple",
            "        if len(arg_abi_types) == 1 and not ensure_tuple:",
            "            arg_abi_t = arg_abi_types[0]",
            "        else:",
            "            arg_abi_t = ABI_Tuple(arg_abi_types)",
            "",
            "        maxlen = arg_abi_t.size_bound()",
            "",
            "        if has_method_id:",
            "            # the output includes 4 bytes for the method_id.",
            "            maxlen += 4",
            "",
            "        ret = BytesT()",
            "        ret.set_length(maxlen)",
            "        return ret",
            "",
            "    @staticmethod",
            "    def _parse_method_id(method_id_literal):",
            "        if method_id_literal is None:",
            "            return None",
            "        if isinstance(method_id_literal, bytes):",
            "            assert len(method_id_literal) == 4",
            "            return fourbytes_to_int(method_id_literal)",
            "        if method_id_literal.startswith(\"0x\"):",
            "            method_id_literal = method_id_literal[2:]",
            "        return fourbytes_to_int(bytes.fromhex(method_id_literal))",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        ensure_tuple = kwargs[\"ensure_tuple\"]",
            "        method_id = self._parse_method_id(kwargs[\"method_id\"])",
            "",
            "        if len(args) < 1:",
            "            raise StructureException(\"abi_encode expects at least one argument\", expr)",
            "",
            "        # figure out the required length for the output buffer",
            "        if len(args) == 1 and not ensure_tuple:",
            "            # special case, no tuple",
            "            encode_input = args[0]",
            "        else:",
            "            encode_input = ir_tuple_from_args(args)",
            "",
            "        input_abi_t = encode_input.typ.abi_type",
            "        maxlen = input_abi_t.size_bound()",
            "        if method_id is not None:",
            "            maxlen += 4",
            "",
            "        buf_t = BytesT(maxlen)",
            "        assert self.fetch_call_return(expr).length == maxlen",
            "        buf = context.new_internal_variable(buf_t)",
            "",
            "        ret = [\"seq\"]",
            "        if method_id is not None:",
            "            # <32 bytes length> | <4 bytes method_id> | <everything else>",
            "            # write the unaligned method_id first, then we will",
            "            # overwrite the 28 bytes of zeros with the bytestring length",
            "            ret += [[\"mstore\", add_ofst(buf, 4), method_id]]",
            "            # abi encode, and grab length as stack item",
            "            length = abi_encode(",
            "                add_ofst(buf, 36), encode_input, context, returns_len=True, bufsz=maxlen",
            "            )",
            "            # write the output length to where bytestring stores its length",
            "            ret += [[\"mstore\", buf, [\"add\", length, 4]]]",
            "",
            "        else:",
            "            # abi encode and grab length as stack item",
            "            length = abi_encode(",
            "                add_ofst(buf, 32), encode_input, context, returns_len=True, bufsz=maxlen",
            "            )",
            "            # write the output length to where bytestring stores its length",
            "            ret += [[\"mstore\", buf, length]]",
            "",
            "        # return the buf location",
            "        # TODO location is statically known, optimize this out",
            "        ret += [buf]",
            "",
            "        return IRnode.from_list(ret, location=MEMORY, typ=buf_t)",
            "",
            "",
            "class ABIDecode(BuiltinFunctionT):",
            "    _id = \"abi_decode\"",
            "    _inputs = [(\"data\", BytesT.any()), (\"output_type\", TYPE_T.any())]",
            "    _kwargs = {\"unwrap_tuple\": KwargSettings(BoolT(), True, require_literal=True)}",
            "",
            "    def fetch_call_return(self, node):",
            "        _, output_type = self.infer_arg_types(node)",
            "        return output_type.typedef",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        self._validate_arg_types(node)",
            "",
            "        validate_call_args(node, 2, [\"unwrap_tuple\"])",
            "",
            "        data_type = get_exact_type_from_node(node.args[0])",
            "        output_type = type_from_annotation(node.args[1])",
            "",
            "        return [data_type, TYPE_T(output_type)]",
            "",
            "    @process_inputs",
            "    def build_IR(self, expr, args, kwargs, context):",
            "        unwrap_tuple = kwargs[\"unwrap_tuple\"]",
            "",
            "        data = args[0]",
            "        output_typ = args[1]",
            "        wrapped_typ = output_typ",
            "",
            "        if unwrap_tuple is True:",
            "            wrapped_typ = calculate_type_for_external_return(output_typ)",
            "",
            "        abi_size_bound = wrapped_typ.abi_type.size_bound()",
            "        abi_min_size = wrapped_typ.abi_type.static_size()",
            "",
            "        # Get the size of data",
            "        input_max_len = data.typ.maxlen",
            "",
            "        assert abi_min_size <= abi_size_bound, \"bad abi type\"",
            "        if input_max_len < abi_size_bound:",
            "            raise StructureException(",
            "                (",
            "                    \"Mismatch between size of input and size of decoded types. \"",
            "                    f\"length of ABI-encoded {wrapped_typ} must be equal to or greater \"",
            "                    f\"than {abi_size_bound}\"",
            "                ),",
            "                expr.args[0],",
            "            )",
            "",
            "        data = ensure_in_memory(data, context)",
            "",
            "        with data.cache_when_complex(\"to_decode\") as (b1, data):",
            "            data_ptr = bytes_data_ptr(data)",
            "            data_len = get_bytearray_length(data)",
            "",
            "            ret = [\"seq\"]",
            "",
            "            # NOTE: we could replace these 4 lines with",
            "            # `[assert [le, abi_min_size, data_len]]`. it depends on",
            "            # what we consider a \"valid\" payload.",
            "            # cf. test_abi_decode_max_size()",
            "            if abi_min_size == abi_size_bound:",
            "                ret.append([\"assert\", [\"eq\", abi_min_size, data_len]])",
            "            else:",
            "                # runtime assert: abi_min_size <= data_len <= abi_size_bound",
            "                ret.append(clamp2(abi_min_size, data_len, abi_size_bound, signed=False))",
            "",
            "            to_decode = IRnode.from_list(",
            "                data_ptr,",
            "                typ=wrapped_typ,",
            "                location=data.location,",
            "                encoding=Encoding.ABI,",
            "                annotation=f\"abi_decode({output_typ})\",",
            "            )",
            "            to_decode.encoding = Encoding.ABI",
            "",
            "            # TODO optimization: skip make_setter when we don't need",
            "            # input validation",
            "",
            "            output_buf = context.new_internal_variable(wrapped_typ)",
            "",
            "            # sanity check buffer size for wrapped output type will not buffer overflow",
            "            assert wrapped_typ.memory_bytes_required == output_typ.memory_bytes_required",
            "",
            "            # pass a buffer bound to make_setter so appropriate oob",
            "            # validation is performed",
            "            buf_bound = add_ofst(data_ptr, data_len)",
            "            ret.append(make_setter(output_buf, to_decode, hi=buf_bound))",
            "",
            "            ret.append(output_buf)",
            "            # finalize. set the type and location for the return buffer.",
            "            # (note: unwraps the tuple type if necessary)",
            "            ret = IRnode.from_list(ret, typ=output_typ, location=MEMORY)",
            "            return b1.resolve(ret)",
            "",
            "",
            "class OldABIEncode(ABIEncode):",
            "    _warned = False",
            "    _id = \"_abi_encode\"",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(f\"`{self._id}()` is deprecated! Please use `{super()._id}()` instead.\", node)",
            "            self.__class__._warned = True",
            "        super()._try_fold(node)",
            "",
            "",
            "class OldABIDecode(ABIDecode):",
            "    _warned = False",
            "    _id = \"_abi_decode\"",
            "",
            "    def _try_fold(self, node):",
            "        if not self.__class__._warned:",
            "            vyper_warn(f\"`{self._id}()` is deprecated! Please use `{super()._id}()` instead.\", node)",
            "            self.__class__._warned = True",
            "        super()._try_fold(node)",
            "",
            "",
            "class _MinMaxValue(TypenameFoldedFunctionT):",
            "    def _try_fold(self, node):",
            "        self._validate_arg_types(node)",
            "        input_type = type_from_annotation(node.args[0])",
            "",
            "        if not isinstance(input_type, (IntegerT, DecimalT)):",
            "            raise InvalidType(f\"Expected numeric type but got {input_type} instead\", node)",
            "",
            "        val = self._eval(input_type)",
            "",
            "        if isinstance(input_type, DecimalT):",
            "            ret = vy_ast.Decimal.from_node(node, value=val)",
            "",
            "        if isinstance(input_type, IntegerT):",
            "            ret = vy_ast.Int.from_node(node, value=val)",
            "",
            "        ret._metadata[\"type\"] = input_type",
            "        return ret",
            "",
            "    def infer_arg_types(self, node, expected_return_typ=None):",
            "        input_typedef = TYPE_T(type_from_annotation(node.args[0]))",
            "        return [input_typedef]",
            "",
            "",
            "class MinValue(_MinMaxValue):",
            "    _id = \"min_value\"",
            "",
            "    def _eval(self, type_):",
            "        return type_.ast_bounds[0]",
            "",
            "",
            "class MaxValue(_MinMaxValue):",
            "    _id = \"max_value\"",
            "",
            "    def _eval(self, type_):",
            "        return type_.ast_bounds[1]",
            "",
            "",
            "class Epsilon(TypenameFoldedFunctionT):",
            "    _id = \"epsilon\"",
            "",
            "    def _try_fold(self, node):",
            "        self._validate_arg_types(node)",
            "        input_type = type_from_annotation(node.args[0])",
            "",
            "        if not input_type.compare_type(DecimalT()):",
            "            raise InvalidType(f\"Expected decimal type but got {input_type} instead\", node)",
            "",
            "        return vy_ast.Decimal.from_node(node, value=input_type.epsilon)",
            "",
            "",
            "DISPATCH_TABLE = {",
            "    \"abi_encode\": ABIEncode(),",
            "    \"abi_decode\": ABIDecode(),",
            "    \"_abi_encode\": OldABIEncode(),",
            "    \"_abi_decode\": OldABIDecode(),",
            "    \"floor\": Floor(),",
            "    \"ceil\": Ceil(),",
            "    \"convert\": Convert(),",
            "    \"slice\": Slice(),",
            "    \"len\": Len(),",
            "    \"concat\": Concat(),",
            "    \"sha256\": Sha256(),",
            "    \"method_id\": MethodID(),",
            "    \"keccak256\": Keccak256(),",
            "    \"ecrecover\": ECRecover(),",
            "    \"ecadd\": ECAdd(),",
            "    \"ecmul\": ECMul(),",
            "    \"extract32\": Extract32(),",
            "    \"as_wei_value\": AsWeiValue(),",
            "    \"raw_call\": RawCall(),",
            "    \"blockhash\": BlockHash(),",
            "    \"blobhash\": BlobHash(),",
            "    \"bitwise_and\": BitwiseAnd(),",
            "    \"bitwise_or\": BitwiseOr(),",
            "    \"bitwise_xor\": BitwiseXor(),",
            "    \"bitwise_not\": BitwiseNot(),",
            "    \"uint256_addmod\": AddMod(),",
            "    \"uint256_mulmod\": MulMod(),",
            "    \"unsafe_add\": UnsafeAdd(),",
            "    \"unsafe_sub\": UnsafeSub(),",
            "    \"unsafe_mul\": UnsafeMul(),",
            "    \"unsafe_div\": UnsafeDiv(),",
            "    \"pow_mod256\": PowMod256(),",
            "    \"uint2str\": Uint2Str(),",
            "    \"isqrt\": ISqrt(),",
            "    \"sqrt\": Sqrt(),",
            "    \"shift\": Shift(),",
            "    \"create_minimal_proxy_to\": CreateMinimalProxyTo(),",
            "    \"create_forwarder_to\": CreateForwarderTo(),",
            "    \"create_copy_of\": CreateCopyOf(),",
            "    \"create_from_blueprint\": CreateFromBlueprint(),",
            "    \"min\": Min(),",
            "    \"max\": Max(),",
            "    \"empty\": Empty(),",
            "    \"abs\": Abs(),",
            "    \"min_value\": MinValue(),",
            "    \"max_value\": MaxValue(),",
            "    \"epsilon\": Epsilon(),",
            "}",
            "",
            "STMT_DISPATCH_TABLE = {",
            "    \"send\": Send(),",
            "    \"print\": Print(),",
            "    \"breakpoint\": Breakpoint(),",
            "    \"selfdestruct\": SelfDestruct(),",
            "    \"raw_call\": RawCall(),",
            "    \"raw_log\": RawLog(),",
            "    \"raw_revert\": RawRevert(),",
            "    \"create_minimal_proxy_to\": CreateMinimalProxyTo(),",
            "    \"create_forwarder_to\": CreateForwarderTo(),",
            "    \"create_copy_of\": CreateCopyOf(),",
            "    \"create_from_blueprint\": CreateFromBlueprint(),",
            "}",
            "",
            "BUILTIN_FUNCTIONS = {**STMT_DISPATCH_TABLE, **DISPATCH_TABLE}.keys()",
            "",
            "",
            "def get_builtin_functions():",
            "    return {**STMT_DISPATCH_TABLE, **DISPATCH_TABLE}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "784": [
                "ECRecover",
                "build_IR"
            ]
        },
        "addLocation": []
    },
    "vyper/codegen/core.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 326,
                "afterPatchRowNumber": 326,
                "PatchRowcode": "                     copy_op = [\"mcopy\", dst, src, length]"
            },
            "1": {
                "beforePatchRowNumber": 327,
                "afterPatchRowNumber": 327,
                "PatchRowcode": "                     gas_bound = _mcopy_gas_bound(length_bound)"
            },
            "2": {
                "beforePatchRowNumber": 328,
                "afterPatchRowNumber": 328,
                "PatchRowcode": "                 else:"
            },
            "3": {
                "beforePatchRowNumber": 329,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    copy_op = [\"staticcall\", \"gas\", 4, src, length, dst, length]"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 329,
                "PatchRowcode": "+                    copy_op = [\"assert\", [\"staticcall\", \"gas\", 4, src, length, dst, length]]"
            },
            "5": {
                "beforePatchRowNumber": 330,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "                     gas_bound = _identity_gas_bound(length_bound)"
            },
            "6": {
                "beforePatchRowNumber": 331,
                "afterPatchRowNumber": 331,
                "PatchRowcode": "             elif src.location == CALLDATA:"
            },
            "7": {
                "beforePatchRowNumber": 332,
                "afterPatchRowNumber": 332,
                "PatchRowcode": "                 copy_op = [\"calldatacopy\", dst, src, length]"
            }
        },
        "frontPatchFile": [
            "import vyper.codegen.context as ctx",
            "from vyper.codegen.ir_node import Encoding, IRnode",
            "from vyper.compiler.settings import _opt_codesize, _opt_gas, _opt_none",
            "from vyper.evm.address_space import (",
            "    CALLDATA,",
            "    DATA,",
            "    IMMUTABLES,",
            "    MEMORY,",
            "    STORAGE,",
            "    TRANSIENT,",
            "    AddrSpace,",
            "    legal_in_staticcall,",
            ")",
            "from vyper.evm.opcodes import version_check",
            "from vyper.exceptions import CompilerPanic, TypeCheckFailure, TypeMismatch",
            "from vyper.semantics.data_locations import DataLocation",
            "from vyper.semantics.types import (",
            "    AddressT,",
            "    BoolT,",
            "    BytesM_T,",
            "    BytesT,",
            "    DArrayT,",
            "    DecimalT,",
            "    HashMapT,",
            "    IntegerT,",
            "    InterfaceT,",
            "    StructT,",
            "    TupleT,",
            "    _BytestringT,",
            ")",
            "from vyper.semantics.types.shortcuts import BYTES32_T, INT256_T, UINT256_T",
            "from vyper.semantics.types.subscriptable import SArrayT",
            "from vyper.semantics.types.user import FlagT",
            "from vyper.utils import GAS_COPY_WORD, GAS_IDENTITY, GAS_IDENTITYWORD, ceil32",
            "",
            "DYNAMIC_ARRAY_OVERHEAD = 1",
            "",
            "",
            "def is_bytes_m_type(typ):",
            "    return isinstance(typ, BytesM_T)",
            "",
            "",
            "def is_numeric_type(typ):",
            "    return isinstance(typ, (IntegerT, DecimalT))",
            "",
            "",
            "def is_integer_type(typ):",
            "    return isinstance(typ, IntegerT)",
            "",
            "",
            "def is_decimal_type(typ):",
            "    return isinstance(typ, DecimalT)",
            "",
            "",
            "def is_flag_type(typ):",
            "    return isinstance(typ, FlagT)",
            "",
            "",
            "def is_tuple_like(typ):",
            "    # A lot of code paths treat tuples and structs similarly",
            "    # so we have a convenience function to detect it",
            "    ret = isinstance(typ, (TupleT, StructT))",
            "    assert ret == hasattr(typ, \"tuple_items\")",
            "    return ret",
            "",
            "",
            "def is_array_like(typ):",
            "    # For convenience static and dynamic arrays share some code paths",
            "    ret = isinstance(typ, (DArrayT, SArrayT))",
            "    assert ret == typ._is_array_type",
            "    return ret",
            "",
            "",
            "def get_type_for_exact_size(n_bytes):",
            "    \"\"\"Create a type which will take up exactly n_bytes. Used for allocating internal buffers.",
            "",
            "    Parameters:",
            "      n_bytes: the number of bytes to allocate",
            "    Returns:",
            "      type: A type which can be passed to context.new_variable",
            "    \"\"\"",
            "    return BytesT(n_bytes - 32 * DYNAMIC_ARRAY_OVERHEAD)",
            "",
            "",
            "# propagate revert message when calls to external contracts fail",
            "def check_external_call(call_ir):",
            "    copy_revertdata = [\"returndatacopy\", 0, 0, \"returndatasize\"]",
            "    revert = IRnode.from_list([\"revert\", 0, \"returndatasize\"], error_msg=\"external call failed\")",
            "",
            "    propagate_revert_ir = [\"seq\", copy_revertdata, revert]",
            "    return [\"if\", [\"iszero\", call_ir], propagate_revert_ir]",
            "",
            "",
            "# cost per byte of the identity precompile",
            "def _identity_gas_bound(num_bytes):",
            "    return GAS_IDENTITY + GAS_IDENTITYWORD * (ceil32(num_bytes) // 32)",
            "",
            "",
            "def _mcopy_gas_bound(num_bytes):",
            "    return GAS_COPY_WORD * ceil32(num_bytes) // 32",
            "",
            "",
            "def _calldatacopy_gas_bound(num_bytes):",
            "    return GAS_COPY_WORD * ceil32(num_bytes) // 32",
            "",
            "",
            "def _codecopy_gas_bound(num_bytes):",
            "    return GAS_COPY_WORD * ceil32(num_bytes) // 32",
            "",
            "",
            "def data_location_to_address_space(s: DataLocation, is_ctor_ctx: bool) -> AddrSpace:",
            "    if s == DataLocation.MEMORY:",
            "        return MEMORY",
            "    if s == DataLocation.STORAGE:",
            "        return STORAGE",
            "    if s == DataLocation.TRANSIENT:",
            "        return TRANSIENT",
            "    if s == DataLocation.CODE:",
            "        if is_ctor_ctx:",
            "            return IMMUTABLES",
            "        return DATA",
            "",
            "    raise CompilerPanic(\"unreachable!\")  # pragma: nocover",
            "",
            "",
            "def address_space_to_data_location(s: AddrSpace) -> DataLocation:",
            "    if s == MEMORY:",
            "        return DataLocation.MEMORY",
            "    if s == STORAGE:",
            "        return DataLocation.STORAGE",
            "    if s == TRANSIENT:",
            "        return DataLocation.TRANSIENT",
            "    if s in (IMMUTABLES, DATA):",
            "        return DataLocation.CODE",
            "    if s == CALLDATA:",
            "        return DataLocation.CALLDATA",
            "",
            "    raise CompilerPanic(\"unreachable!\")  # pragma: nocover",
            "",
            "",
            "def writeable(context, ir_node):",
            "    assert ir_node.is_pointer  # sanity check",
            "",
            "    if context.is_constant() and not legal_in_staticcall(ir_node.location):",
            "        return False",
            "    return ir_node.mutable",
            "",
            "",
            "# Copy byte array word-for-word (including layout)",
            "# TODO make this a private function",
            "def make_byte_array_copier(dst, src):",
            "    assert isinstance(src.typ, _BytestringT)",
            "    assert isinstance(dst.typ, _BytestringT)",
            "",
            "    _check_assign_bytes(dst, src)",
            "",
            "    # TODO: remove this branch, copy_bytes and get_bytearray_length should handle",
            "    if src.value == \"~empty\" or src.typ.maxlen == 0:",
            "        # set length word to 0.",
            "        return STORE(dst, 0)",
            "",
            "    with src.cache_when_complex(\"src\") as (b1, src):",
            "        if src.typ.maxlen <= 32 and not copy_opcode_available(dst, src):",
            "            # if there is no batch copy opcode available,",
            "            # it's cheaper to run two load/stores instead of copy_bytes",
            "            ret = [\"seq\"]",
            "            # store length word",
            "            len_ = get_bytearray_length(src)",
            "            ret.append(STORE(dst, len_))",
            "",
            "            # store the single data word.",
            "            dst_data_ptr = bytes_data_ptr(dst)",
            "            src_data_ptr = bytes_data_ptr(src)",
            "            ret.append(STORE(dst_data_ptr, LOAD(src_data_ptr)))",
            "            return b1.resolve(ret)",
            "",
            "        # batch copy the bytearray (including length word) using copy_bytes",
            "        len_ = add_ofst(get_bytearray_length(src), 32)",
            "        max_bytes = src.typ.maxlen + 32",
            "        ret = copy_bytes(dst, src, len_, max_bytes)",
            "        return b1.resolve(ret)",
            "",
            "",
            "def bytes_data_ptr(ptr):",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"tried to modify non-pointer type\")",
            "    assert isinstance(ptr.typ, _BytestringT)",
            "    return add_ofst(ptr, ptr.location.word_scale)",
            "",
            "",
            "def dynarray_data_ptr(ptr):",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"tried to modify non-pointer type\")",
            "    assert isinstance(ptr.typ, DArrayT)",
            "    return add_ofst(ptr, ptr.location.word_scale)",
            "",
            "",
            "def _dynarray_make_setter(dst, src, hi=None):",
            "    assert isinstance(src.typ, DArrayT)",
            "    assert isinstance(dst.typ, DArrayT)",
            "",
            "    if src.value == \"~empty\":",
            "        return IRnode.from_list(STORE(dst, 0))",
            "",
            "    # copy contents of src dynarray to dst.",
            "    # note that in case src and dst refer to the same dynarray,",
            "    # in order for get_element_ptr oob checks on the src dynarray",
            "    # to work, we need to wait until after the data is copied",
            "    # before we clobber the length word.",
            "",
            "    if src.value == \"multi\":",
            "        # validation is only performed on unsafe data, but we are dealing with",
            "        # a literal here.",
            "        assert hi is None",
            "        ret = [\"seq\"]",
            "        # handle literals",
            "",
            "        # copy each item",
            "        n_items = len(src.args)",
            "",
            "        for i in range(n_items):",
            "            k = IRnode.from_list(i, typ=UINT256_T)",
            "            dst_i = get_element_ptr(dst, k, array_bounds_check=False)",
            "            src_i = get_element_ptr(src, k, array_bounds_check=False)",
            "            ret.append(make_setter(dst_i, src_i))",
            "",
            "        # write the length word after data is copied",
            "        store_length = STORE(dst, n_items)",
            "        ann = None",
            "        if src.annotation is not None:",
            "            ann = f\"len({src.annotation})\"",
            "        store_length = IRnode.from_list(store_length, annotation=ann)",
            "",
            "        ret.append(store_length)",
            "",
            "        return ret",
            "",
            "    with src.cache_when_complex(\"darray_src\") as (b1, src):",
            "        # for ABI-encoded dynamic data, we must loop to unpack, since",
            "        # the layout does not match our memory layout",
            "        should_loop = src.encoding == Encoding.ABI and src.typ.value_type.abi_type.is_dynamic()",
            "",
            "        # if the data is not validated, we must loop to unpack",
            "        should_loop |= needs_clamp(src.typ.value_type, src.encoding)",
            "",
            "        # performance: if the subtype is dynamic, there might be a lot",
            "        # of unused space inside of each element. for instance",
            "        # DynArray[DynArray[uint256, 100], 5] where all the child",
            "        # arrays are empty - for this case, we recursively call",
            "        # into make_setter instead of straight bytes copy",
            "        # TODO we can make this heuristic more precise, e.g.",
            "        # loop when subtype.is_dynamic AND location == storage",
            "        # OR array_size <= /bound where loop is cheaper than memcpy/",
            "        should_loop |= src.typ.value_type.abi_type.is_dynamic()",
            "",
            "        with get_dyn_array_count(src).cache_when_complex(\"darray_count\") as (b2, count):",
            "            ret = [\"seq\"]",
            "",
            "            if should_loop:",
            "                i = IRnode.from_list(_freshname(\"copy_darray_ix\"), typ=UINT256_T)",
            "",
            "                loop_body = make_setter(",
            "                    get_element_ptr(dst, i, array_bounds_check=False),",
            "                    get_element_ptr(src, i, array_bounds_check=False),",
            "                    hi=hi,",
            "                )",
            "                loop_body.annotation = f\"{dst}[i] = {src}[i]\"",
            "",
            "                ret.append([\"repeat\", i, 0, count, src.typ.count, loop_body])",
            "                # write the length word after data is copied",
            "                ret.append(STORE(dst, count))",
            "",
            "            else:",
            "                element_size = src.typ.value_type.memory_bytes_required",
            "                # number of elements * size of element in bytes + length word",
            "                n_bytes = add_ofst(_mul(count, element_size), 32)",
            "                max_bytes = 32 + src.typ.count * element_size",
            "",
            "                # batch copy the entire dynarray, including length word",
            "                ret.append(copy_bytes(dst, src, n_bytes, max_bytes))",
            "",
            "            return b1.resolve(b2.resolve(ret))",
            "",
            "",
            "# Copy bytes",
            "# Accepts 4 arguments:",
            "# (i) an IR node for the start position of the source",
            "# (ii) an IR node for the start position of the destination",
            "# (iii) an IR node for the length (in bytes)",
            "# (iv) a constant for the max length (in bytes)",
            "# NOTE: may pad to ceil32 of `length`! If you ask to copy 1 byte, it may",
            "# copy an entire (32-byte) word, depending on the copy routine chosen.",
            "# TODO maybe always pad to ceil32, to reduce dirty bytes bugs",
            "def copy_bytes(dst, src, length, length_bound):",
            "    annotation = f\"copy up to {length_bound} bytes from {src} to {dst}\"",
            "",
            "    src = IRnode.from_list(src)",
            "    dst = IRnode.from_list(dst)",
            "    length = IRnode.from_list(length)",
            "",
            "    with src.cache_when_complex(\"src\") as (b1, src), length.cache_when_complex(",
            "        \"copy_bytes_count\"",
            "    ) as (b2, length), dst.cache_when_complex(\"dst\") as (b3, dst):",
            "        assert isinstance(length_bound, int) and length_bound >= 0",
            "",
            "        # correctness: do not clobber dst",
            "        if length_bound == 0:",
            "            return IRnode.from_list([\"seq\"], annotation=annotation)",
            "        # performance: if we know that length is 0, do not copy anything",
            "        if length.value == 0:",
            "            return IRnode.from_list([\"seq\"], annotation=annotation)",
            "",
            "        assert src.is_pointer and dst.is_pointer",
            "",
            "        # fast code for common case where num bytes is small",
            "        if length_bound <= 32:",
            "            copy_op = STORE(dst, LOAD(src))",
            "            ret = IRnode.from_list(copy_op, annotation=annotation)",
            "            return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "        if dst.location == MEMORY and src.location in (MEMORY, CALLDATA, DATA):",
            "            # special cases: batch copy to memory",
            "            # TODO: iloadbytes",
            "            if src.location == MEMORY:",
            "                if version_check(begin=\"cancun\"):",
            "                    copy_op = [\"mcopy\", dst, src, length]",
            "                    gas_bound = _mcopy_gas_bound(length_bound)",
            "                else:",
            "                    copy_op = [\"staticcall\", \"gas\", 4, src, length, dst, length]",
            "                    gas_bound = _identity_gas_bound(length_bound)",
            "            elif src.location == CALLDATA:",
            "                copy_op = [\"calldatacopy\", dst, src, length]",
            "                gas_bound = _calldatacopy_gas_bound(length_bound)",
            "            elif src.location == DATA:",
            "                copy_op = [\"dloadbytes\", dst, src, length]",
            "                # note: dloadbytes compiles to CODECOPY",
            "                gas_bound = _codecopy_gas_bound(length_bound)",
            "",
            "            ret = IRnode.from_list(copy_op, annotation=annotation, add_gas_estimate=gas_bound)",
            "            return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "        if dst.location == IMMUTABLES and src.location in (MEMORY, DATA):",
            "            # TODO istorebytes-from-mem, istorebytes-from-calldata(?)",
            "            # compile to identity, CODECOPY respectively.",
            "            pass",
            "",
            "        # general case, copy word-for-word",
            "        # pseudocode for our approach (memory-storage as example):",
            "        # for i in range(len, bound=MAX_LEN):",
            "        #   sstore(_dst + i, mload(src + i * 32))",
            "        i = IRnode.from_list(_freshname(\"copy_bytes_ix\"), typ=UINT256_T)",
            "",
            "        # optimized form of (div (ceil32 len) 32)",
            "        n = [\"div\", [\"add\", 31, length], 32]",
            "        n_bound = ceil32(length_bound) // 32",
            "",
            "        dst_i = add_ofst(dst, _mul(i, dst.location.word_scale))",
            "        src_i = add_ofst(src, _mul(i, src.location.word_scale))",
            "",
            "        copy_one_word = STORE(dst_i, LOAD(src_i))",
            "",
            "        main_loop = [\"repeat\", i, 0, n, n_bound, copy_one_word]",
            "",
            "        return b1.resolve(",
            "            b2.resolve(b3.resolve(IRnode.from_list(main_loop, annotation=annotation)))",
            "        )",
            "",
            "",
            "# get the number of bytes at runtime",
            "def get_bytearray_length(arg):",
            "    typ = UINT256_T",
            "",
            "    # TODO: it would be nice to merge the implementations of get_bytearray_length and",
            "    # get_dynarray_count",
            "    if arg.value == \"~empty\":",
            "        return IRnode.from_list(0, typ=typ)",
            "",
            "    return IRnode.from_list(LOAD(arg), typ=typ)",
            "",
            "",
            "# get the number of elements at runtime",
            "def get_dyn_array_count(arg):",
            "    assert isinstance(arg.typ, DArrayT)",
            "",
            "    typ = UINT256_T",
            "",
            "    if arg.value == \"multi\":",
            "        return IRnode.from_list(len(arg.args), typ=typ)",
            "",
            "    if arg.value == \"~empty\":",
            "        # empty(DynArray[...])",
            "        return IRnode.from_list(0, typ=typ)",
            "",
            "    return IRnode.from_list(LOAD(arg), typ=typ)",
            "",
            "",
            "def append_dyn_array(darray_node, elem_node):",
            "    assert isinstance(darray_node.typ, DArrayT)",
            "",
            "    assert darray_node.typ.count > 0, \"jerk boy u r out\"",
            "",
            "    ret = [\"seq\"]",
            "    with darray_node.cache_when_complex(\"darray\") as (b1, darray_node):",
            "        len_ = get_dyn_array_count(darray_node)",
            "        with len_.cache_when_complex(\"old_darray_len\") as (b2, len_):",
            "            assertion = [\"assert\", [\"lt\", len_, darray_node.typ.count]]",
            "            ret.append(IRnode.from_list(assertion, error_msg=f\"{darray_node.typ} bounds check\"))",
            "            # NOTE: typechecks elem_node",
            "            # NOTE skip array bounds check bc we already asserted len two lines up",
            "            ret.append(",
            "                make_setter(get_element_ptr(darray_node, len_, array_bounds_check=False), elem_node)",
            "            )",
            "",
            "            # store new length",
            "            ret.append(ensure_eval_once(\"append_dynarray\", STORE(darray_node, [\"add\", len_, 1])))",
            "",
            "            return IRnode.from_list(b1.resolve(b2.resolve(ret)))",
            "",
            "",
            "def pop_dyn_array(darray_node, return_popped_item):",
            "    assert isinstance(darray_node.typ, DArrayT)",
            "    assert darray_node.encoding == Encoding.VYPER",
            "    ret = [\"seq\"]",
            "    with darray_node.cache_when_complex(\"darray\") as (b1, darray_node):",
            "        old_len = clamp(\"gt\", get_dyn_array_count(darray_node), 0)",
            "        new_len = IRnode.from_list([\"sub\", old_len, 1], typ=UINT256_T)",
            "",
            "        with new_len.cache_when_complex(\"new_len\") as (b2, new_len):",
            "            # store new length",
            "            ret.append(ensure_eval_once(\"pop_dynarray\", STORE(darray_node, new_len)))",
            "",
            "            # NOTE skip array bounds check bc we already asserted len two lines up",
            "            if return_popped_item:",
            "                popped_item = get_element_ptr(darray_node, new_len, array_bounds_check=False)",
            "                ret.append(popped_item)",
            "                typ = popped_item.typ",
            "                location = popped_item.location",
            "            else:",
            "                typ, location = None, None",
            "",
            "            return IRnode.from_list(b1.resolve(b2.resolve(ret)), typ=typ, location=location)",
            "",
            "",
            "# add an offset to a pointer, keeping location and encoding info",
            "def add_ofst(ptr, ofst):",
            "    ret = [\"add\", ptr, ofst]",
            "    return IRnode.from_list(ret, location=ptr.location, encoding=ptr.encoding)",
            "",
            "",
            "# shorthand util",
            "def _mul(x, y):",
            "    ret = [\"mul\", x, y]",
            "    return IRnode.from_list(ret)",
            "",
            "",
            "# Resolve pointer locations for ABI-encoded data",
            "def _getelemptr_abi_helper(parent, member_t, ofst):",
            "    member_abi_t = member_t.abi_type",
            "",
            "    # ABI encoding has length word and then pretends length is not there",
            "    # e.g. [[1,2]] is encoded as 0x01 <len> 0x20 <inner array ofst> <encode(inner array)>",
            "    # note that inner array ofst is 0x20, not 0x40.",
            "    if has_length_word(parent.typ):",
            "        parent = add_ofst(parent, parent.location.word_scale * DYNAMIC_ARRAY_OVERHEAD)",
            "",
            "    ofst_ir = add_ofst(parent, ofst)",
            "",
            "    if member_abi_t.is_dynamic():",
            "        # double dereference, according to ABI spec",
            "        ofst_ir = add_ofst(parent, unwrap_location(ofst_ir))",
            "        if _dirty_read_risk(ofst_ir):",
            "            # check no arithmetic overflow",
            "            ofst_ir = [\"seq\", [\"assert\", [\"ge\", ofst_ir, parent]], ofst_ir]",
            "",
            "    return IRnode.from_list(",
            "        ofst_ir,",
            "        typ=member_t,",
            "        location=parent.location,",
            "        encoding=parent.encoding,",
            "        annotation=f\"{parent}{ofst}\",",
            "    )",
            "",
            "",
            "# TODO simplify this code, especially the ABI decoding",
            "def _get_element_ptr_tuplelike(parent, key, hi=None):",
            "    typ = parent.typ",
            "    assert is_tuple_like(typ)",
            "",
            "    if isinstance(typ, StructT):",
            "        assert isinstance(key, str)",
            "        subtype = typ.member_types[key]",
            "        attrs = list(typ.tuple_keys())",
            "        index = attrs.index(key)",
            "        annotation = key",
            "    else:",
            "        assert isinstance(typ, TupleT)",
            "        assert isinstance(key, int)",
            "        subtype = typ.member_types[key]",
            "        attrs = list(typ.tuple_keys())",
            "        index = key",
            "        annotation = None",
            "",
            "    # generated by empty() + make_setter",
            "    if parent.value == \"~empty\":",
            "        return IRnode.from_list(\"~empty\", typ=subtype)",
            "",
            "    if parent.value == \"multi\":",
            "        assert parent.encoding != Encoding.ABI, \"no abi-encoded literals\"",
            "        return parent.args[index]",
            "",
            "    ofst = 0  # offset from parent start",
            "",
            "    if parent.encoding == Encoding.ABI:",
            "        if parent.location in (STORAGE, TRANSIENT):  # pragma: nocover",
            "            raise CompilerPanic(\"storage variables should not be abi encoded\")",
            "",
            "        member_t = typ.member_types[attrs[index]]",
            "",
            "        for i in range(index):",
            "            member_abi_t = typ.member_types[attrs[i]].abi_type",
            "            ofst += member_abi_t.embedded_static_size()",
            "",
            "        return _getelemptr_abi_helper(parent, member_t, ofst)",
            "",
            "    data_location = address_space_to_data_location(parent.location)",
            "    for i in range(index):",
            "        t = typ.member_types[attrs[i]]",
            "        ofst += t.get_size_in(data_location)",
            "",
            "    return IRnode.from_list(",
            "        add_ofst(parent, ofst),",
            "        typ=subtype,",
            "        location=parent.location,",
            "        encoding=parent.encoding,",
            "        annotation=annotation,",
            "    )",
            "",
            "",
            "def has_length_word(typ):",
            "    # Consider moving this to an attribute on typ",
            "    return isinstance(typ, (DArrayT, _BytestringT))",
            "",
            "",
            "# TODO simplify this code, especially the ABI decoding",
            "def _get_element_ptr_array(parent, key, array_bounds_check):",
            "    assert is_array_like(parent.typ)",
            "",
            "    if not is_integer_type(key.typ):  # pragma: nocover",
            "        raise TypeCheckFailure(f\"{key.typ} used as array index\")",
            "",
            "    subtype = parent.typ.value_type",
            "",
            "    if parent.value == \"~empty\":",
            "        if array_bounds_check:",
            "            # this case was previously missing a bounds check. codegen",
            "            # is a bit complicated when bounds check is required, so",
            "            # block it. there is no reason to index into a literal empty",
            "            # array anyways!",
            "            raise TypeCheckFailure(\"indexing into zero array not allowed\")",
            "        return IRnode.from_list(\"~empty\", subtype)",
            "",
            "    if parent.value == \"multi\":",
            "        assert isinstance(key.value, int)",
            "        return parent.args[key.value]",
            "",
            "    ix = unwrap_location(key)",
            "",
            "    if array_bounds_check:",
            "        is_darray = isinstance(parent.typ, DArrayT)",
            "        bound = get_dyn_array_count(parent) if is_darray else parent.typ.count",
            "        # NOTE: there are optimization rules for the bounds check when",
            "        # ix or bound is literal",
            "        with ix.cache_when_complex(\"ix\") as (b1, ix):",
            "            LT = \"slt\" if ix.typ.is_signed else \"lt\"",
            "            # note: this is optimized out for unsigned integers",
            "            is_negative = [LT, ix, 0]",
            "            # always use unsigned ge, since bound is always an unsigned quantity",
            "            is_oob = [\"ge\", ix, bound]",
            "            checked_ix = [\"seq\", [\"assert\", [\"iszero\", [\"or\", is_negative, is_oob]]], ix]",
            "            ix = b1.resolve(IRnode.from_list(checked_ix))",
            "        ix.set_error_msg(f\"{parent.typ} bounds check\")",
            "",
            "    if parent.encoding == Encoding.ABI:",
            "        if parent.location in (STORAGE, TRANSIENT):  # pragma: nocover",
            "            raise CompilerPanic(\"storage variables should not be abi encoded\")",
            "",
            "        member_abi_t = subtype.abi_type",
            "",
            "        ofst = _mul(ix, member_abi_t.embedded_static_size())",
            "",
            "        return _getelemptr_abi_helper(parent, subtype, ofst)",
            "",
            "    data_location = address_space_to_data_location(parent.location)",
            "    element_size = subtype.get_size_in(data_location)",
            "",
            "    ofst = _mul(ix, element_size)",
            "",
            "    if has_length_word(parent.typ):",
            "        data_ptr = add_ofst(parent, parent.location.word_scale * DYNAMIC_ARRAY_OVERHEAD)",
            "    else:",
            "        data_ptr = parent",
            "",
            "    return IRnode.from_list(add_ofst(data_ptr, ofst), typ=subtype, location=parent.location)",
            "",
            "",
            "def _get_element_ptr_mapping(parent, key):",
            "    assert isinstance(parent.typ, HashMapT)",
            "    subtype = parent.typ.value_type",
            "    key = unwrap_location(key)",
            "",
            "    if parent.location not in (STORAGE, TRANSIENT):  # pragma: nocover",
            "        raise TypeCheckFailure(f\"bad dereference on mapping {parent}[{key}]\")",
            "",
            "    return IRnode.from_list([\"sha3_64\", parent, key], typ=subtype, location=parent.location)",
            "",
            "",
            "# Take a value representing a memory or storage location, and descend down to",
            "# an element or member variable",
            "# This is analogous (but not necessarily equivalent to) getelementptr in LLVM.",
            "def get_element_ptr(parent, key, array_bounds_check=True):",
            "    with parent.cache_when_complex(\"val\") as (b, parent):",
            "        typ = parent.typ",
            "",
            "        if is_tuple_like(typ):",
            "            ret = _get_element_ptr_tuplelike(parent, key)",
            "",
            "        elif isinstance(typ, HashMapT):",
            "            ret = _get_element_ptr_mapping(parent, key)",
            "",
            "        elif is_array_like(typ):",
            "            ret = _get_element_ptr_array(parent, key, array_bounds_check)",
            "",
            "        else:  # pragma: nocover",
            "            raise CompilerPanic(f\"get_element_ptr cannot be called on {typ}\")",
            "",
            "        return b.resolve(ret)",
            "",
            "",
            "def LOAD(ptr: IRnode) -> IRnode:",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"cannot dereference non-pointer type\")",
            "    op = ptr.location.load_op",
            "    if op is None:  # pragma: nocover",
            "        raise CompilerPanic(f\"unreachable {ptr.location}\")",
            "    return IRnode.from_list([op, ptr])",
            "",
            "",
            "def eval_once_check(name):",
            "    # an IRnode which enforces uniqueness. include with a side-effecting",
            "    # operation to sanity check that the codegen pipeline only generates",
            "    # the side-effecting operation once (otherwise, IR-to-assembly will",
            "    # throw a duplicate label exception). there is no runtime overhead",
            "    # since the jumpdest gets optimized out in the final stage of assembly.",
            "    return IRnode.from_list([\"unique_symbol\", name])",
            "",
            "",
            "def ensure_eval_once(name, irnode):",
            "    return [\"seq\", eval_once_check(_freshname(name)), irnode]",
            "",
            "",
            "def STORE(ptr: IRnode, val: IRnode) -> IRnode:",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"cannot dereference non-pointer type\")",
            "    op = ptr.location.store_op",
            "    if op is None:  # pragma: nocover",
            "        raise CompilerPanic(f\"unreachable {ptr.location}\")",
            "",
            "    store = [op, ptr, val]",
            "    # don't use eval_once_check for memory, immutables because it interferes",
            "    # with optimizer",
            "    if ptr.location in (MEMORY, IMMUTABLES):",
            "        return IRnode.from_list(store)",
            "",
            "    return IRnode.from_list(ensure_eval_once(f\"{op}_\", store))",
            "",
            "",
            "# Unwrap location",
            "def unwrap_location(orig):",
            "    if orig.location is not None:",
            "        return IRnode.from_list(LOAD(orig), typ=orig.typ)",
            "    else:",
            "        # CMC 2022-03-24 TODO refactor so this branch can be removed",
            "        if orig.value == \"~empty\":",
            "            # must be word type",
            "            return IRnode.from_list(0, typ=orig.typ)",
            "        return orig",
            "",
            "",
            "# utility function, constructs an IR tuple out of a list of IR nodes",
            "def ir_tuple_from_args(args):",
            "    typ = TupleT([x.typ for x in args])",
            "    return IRnode.from_list([\"multi\"] + [x for x in args], typ=typ)",
            "",
            "",
            "def needs_external_call_wrap(typ):",
            "    # for calls to ABI conforming contracts.",
            "    # according to the ABI spec, return types are ALWAYS tuples even",
            "    # if only one element is being returned.",
            "    # https://solidity.readthedocs.io/en/latest/abi-spec.html#function-selector-and-argument-encoding",
            "    # \"and the return values v_1, ..., v_k of f are encoded as",
            "    #",
            "    #    enc((v_1, ..., v_k))",
            "    #    i.e. the values are combined into a tuple and encoded.",
            "    # \"",
            "    # therefore, wrap it in a tuple if it's not already a tuple.",
            "    # for example, `bytes` is returned as abi-encoded (bytes,)",
            "    # and `(bytes,)` is returned as abi-encoded ((bytes,),)",
            "    # In general `-> X` gets returned as (X,)",
            "    # including structs. MyStruct is returned as abi-encoded (MyStruct,).",
            "    # (Sorry this is so confusing. I didn't make these rules.)",
            "",
            "    return not (isinstance(typ, TupleT) and typ.length > 1)",
            "",
            "",
            "def calculate_type_for_external_return(typ):",
            "    if needs_external_call_wrap(typ):",
            "        return TupleT([typ])",
            "    return typ",
            "",
            "",
            "def wrap_value_for_external_return(ir_val):",
            "    # used for LHS promotion",
            "    if needs_external_call_wrap(ir_val.typ):",
            "        return ir_tuple_from_args([ir_val])",
            "    else:",
            "        return ir_val",
            "",
            "",
            "def set_type_for_external_return(ir_val):",
            "    # used for RHS promotion",
            "    ir_val.typ = calculate_type_for_external_return(ir_val.typ)",
            "",
            "",
            "# return a dummy IRnode with the given type",
            "def dummy_node_for_type(typ):",
            "    return IRnode(\"fake_node\", typ=typ)",
            "",
            "",
            "def _check_assign_bytes(left, right):",
            "    if right.typ.maxlen > left.typ.maxlen:  # pragma: nocover",
            "        raise TypeMismatch(f\"Cannot cast from {right.typ} to {left.typ}\")",
            "",
            "    # stricter check for zeroing a byte array.",
            "    # TODO: these should be TypeCheckFailure instead of TypeMismatch",
            "    if right.value == \"~empty\" and right.typ.maxlen != left.typ.maxlen:  # pragma: nocover",
            "        raise TypeMismatch(f\"Cannot cast from empty({right.typ}) to {left.typ}\")",
            "",
            "",
            "def _check_assign_list(left, right):",
            "    def FAIL():  # pragma: no cover",
            "        raise TypeCheckFailure(f\"assigning {right.typ} to {left.typ}\")",
            "",
            "    if left.value == \"multi\":  # pragma: nocover",
            "        # Cannot do something like [a, b, c] = [1, 2, 3]",
            "        FAIL()",
            "",
            "    if isinstance(left.typ, SArrayT):",
            "        if not is_array_like(right.typ):  # pragma: nocover",
            "            FAIL()",
            "        if left.typ.count != right.typ.count:  # pragma: nocover",
            "            FAIL()",
            "",
            "        # TODO recurse into left, right if literals?",
            "        check_assign(",
            "            dummy_node_for_type(left.typ.value_type), dummy_node_for_type(right.typ.value_type)",
            "        )",
            "",
            "    if isinstance(left.typ, DArrayT):",
            "        if not isinstance(right.typ, DArrayT):  # pragma: nocover",
            "            FAIL()",
            "",
            "        if left.typ.count < right.typ.count:  # pragma: nocover",
            "            FAIL()",
            "",
            "        # stricter check for zeroing",
            "        if right.value == \"~empty\" and right.typ.count != left.typ.count:  # pragma: nocover",
            "            raise TypeCheckFailure(",
            "                f\"Bad type for clearing bytes: expected {left.typ} but got {right.typ}\"",
            "            )",
            "",
            "        # TODO recurse into left, right if literals?",
            "        check_assign(",
            "            dummy_node_for_type(left.typ.value_type), dummy_node_for_type(right.typ.value_type)",
            "        )",
            "",
            "",
            "def _check_assign_tuple(left, right):",
            "    def FAIL():  # pragma: no cover",
            "        raise TypeCheckFailure(f\"assigning {right.typ} to {left.typ}\")",
            "",
            "    if not isinstance(right.typ, left.typ.__class__):  # pragma: nocover",
            "        FAIL()",
            "",
            "    if isinstance(left.typ, StructT):",
            "        for k in left.typ.member_types:",
            "            if k not in right.typ.member_types:  # pragma: nocover",
            "                FAIL()",
            "            # TODO recurse into left, right if literals?",
            "            check_assign(",
            "                dummy_node_for_type(left.typ.member_types[k]),",
            "                dummy_node_for_type(right.typ.member_types[k]),",
            "            )",
            "",
            "        for k in right.typ.member_types:",
            "            if k not in left.typ.member_types:  # pragma: nocover",
            "                FAIL()",
            "",
            "        if left.typ.name != right.typ.name:  # pragma: nocover",
            "            FAIL()",
            "",
            "    else:",
            "        if len(left.typ.member_types) != len(right.typ.member_types):  # pragma: nocover",
            "            FAIL()",
            "        for left_, right_ in zip(left.typ.member_types, right.typ.member_types):",
            "            # TODO recurse into left, right if literals?",
            "            check_assign(dummy_node_for_type(left_), dummy_node_for_type(right_))",
            "",
            "",
            "# sanity check an assignment",
            "# typechecking source code is done at an earlier phase",
            "# this function is more of a sanity check for typechecking internally",
            "# generated assignments",
            "# TODO: do we still need this?",
            "def check_assign(left, right):",
            "    def FAIL():  # pragma: no cover",
            "        raise TypeCheckFailure(f\"assigning {right.typ} to {left.typ} {left} {right}\")",
            "",
            "    if isinstance(left.typ, _BytestringT):",
            "        _check_assign_bytes(left, right)",
            "    elif is_array_like(left.typ):",
            "        _check_assign_list(left, right)",
            "    elif is_tuple_like(left.typ):",
            "        _check_assign_tuple(left, right)",
            "",
            "    elif left.typ._is_prim_word:",
            "        # TODO once we propagate types from typechecker, introduce this check:",
            "        # if left.typ != right.typ:  # pragma: nocover",
            "        #    FAIL()",
            "        pass",
            "",
            "    else:  # pragma: no cover",
            "        FAIL()",
            "",
            "",
            "_label = 0",
            "",
            "",
            "# TODO might want to coalesce with Context.fresh_varname and compile_ir.mksymbol",
            "def _freshname(name):",
            "    global _label",
            "    _label += 1",
            "    return f\"{name}{_label}\"",
            "",
            "",
            "def reset_names():",
            "    global _label",
            "    _label = 0",
            "",
            "    # could be refactored",
            "    ctx._alloca_id = 0",
            "",
            "",
            "# returns True if t is ABI encoded and is a type that needs any kind of",
            "# validation",
            "def needs_clamp(t, encoding):",
            "    if encoding == Encoding.VYPER:",
            "        return False",
            "    if encoding != Encoding.ABI:  # pragma: nocover",
            "        raise CompilerPanic(\"unreachable\")",
            "    if isinstance(t, (_BytestringT, DArrayT)):",
            "        return True",
            "    if isinstance(t, FlagT):",
            "        return len(t._flag_members) < 256",
            "    if isinstance(t, SArrayT):",
            "        return needs_clamp(t.value_type, encoding)",
            "    if is_tuple_like(t):",
            "        return any(needs_clamp(m, encoding) for m in t.tuple_members())",
            "    if t._is_prim_word:",
            "        return t not in (INT256_T, UINT256_T, BYTES32_T)",
            "",
            "    raise CompilerPanic(\"unreachable\")  # pragma: nocover",
            "",
            "",
            "# when abi encoded data is user provided and lives in memory,",
            "# we risk either reading oob of the buffer or oob of the payload data.",
            "# in these cases, we need additional validation.",
            "def _dirty_read_risk(ir_node):",
            "    return ir_node.encoding == Encoding.ABI and ir_node.location == MEMORY",
            "",
            "",
            "# child elements which have dynamic length, and could overflow the buffer",
            "# even if the start of the item is in-bounds.",
            "def _abi_payload_size(ir_node):",
            "    SCALE = ir_node.location.word_scale",
            "    assert SCALE == 32  # we must be in some byte-addressable region, like memory",
            "    OFFSET = DYNAMIC_ARRAY_OVERHEAD * SCALE",
            "",
            "    if isinstance(ir_node.typ, DArrayT):",
            "        # the amount of size each value occupies in static section",
            "        # (the amount of size it occupies in the dynamic section is handled in",
            "        # make_setter recursion)",
            "        item_size = ir_node.typ.value_type.abi_type.embedded_static_size()",
            "        return [\"add\", OFFSET, [\"mul\", get_dyn_array_count(ir_node), item_size]]",
            "",
            "    if isinstance(ir_node.typ, _BytestringT):",
            "        return [\"add\", OFFSET, get_bytearray_length(ir_node)]",
            "",
            "    raise CompilerPanic(\"unreachable\")  # pragma: nocover",
            "",
            "",
            "def potential_overlap(left, right):",
            "    \"\"\"",
            "    Return true if make_setter(left, right) could potentially trample",
            "    src or dst during evaluation.",
            "    \"\"\"",
            "    if left.typ._is_prim_word and right.typ._is_prim_word:",
            "        return False",
            "",
            "    if len(left.referenced_variables & right.referenced_variables) > 0:",
            "        return True",
            "",
            "    if len(left.referenced_variables) > 0 and right.contains_risky_call:",
            "        return True",
            "",
            "    if left.contains_risky_call and len(right.referenced_variables) > 0:",
            "        return True",
            "",
            "    return False",
            "",
            "",
            "# similar to `potential_overlap()`, but compares left's _reads_ vs",
            "# right's _writes_.",
            "# TODO: `potential_overlap()` can probably be replaced by this function,",
            "# but all the cases need to be checked.",
            "def read_write_overlap(left, right):",
            "    if not isinstance(left, IRnode) or not isinstance(right, IRnode):",
            "        return False",
            "",
            "    if left.typ._is_prim_word and right.typ._is_prim_word:",
            "        return False",
            "",
            "    if len(left.referenced_variables & right.variable_writes) > 0:",
            "        return True",
            "",
            "    if len(left.referenced_variables) > 0 and right.contains_risky_call:",
            "        return True",
            "",
            "    return False",
            "",
            "",
            "# Create an x=y statement, where the types may be compound",
            "def make_setter(left, right, hi=None):",
            "    check_assign(left, right)",
            "",
            "    if potential_overlap(left, right):",
            "        raise CompilerPanic(\"overlap between src and dst!\")",
            "",
            "    # we need bounds checks when decoding from memory, otherwise we can",
            "    # get oob reads.",
            "    #",
            "    # the caller is responsible for calculating the bound;",
            "    # sanity check that there is a bound if there is dirty read risk",
            "    assert (hi is not None) == _dirty_read_risk(right)",
            "",
            "    # For types which occupy just one word we can use single load/store",
            "    if left.typ._is_prim_word:",
            "        enc = right.encoding  # unwrap_location butchers encoding",
            "        right = unwrap_location(right)",
            "        # TODO rethink/streamline the clamp_basetype logic",
            "        if needs_clamp(right.typ, enc):",
            "            right = clamp_basetype(right)",
            "",
            "        return STORE(left, right)",
            "",
            "    # Byte arrays",
            "    elif isinstance(left.typ, _BytestringT):",
            "        # TODO rethink/streamline the clamp_basetype logic",
            "        if needs_clamp(right.typ, right.encoding):",
            "            with right.cache_when_complex(\"bs_ptr\") as (b, right):",
            "                copier = make_byte_array_copier(left, right)",
            "                ret = b.resolve([\"seq\", clamp_bytestring(right, hi=hi), copier])",
            "        else:",
            "            ret = make_byte_array_copier(left, right)",
            "",
            "        return IRnode.from_list(ret)",
            "",
            "    elif isinstance(left.typ, DArrayT):",
            "        # TODO should we enable this?",
            "        # implicit conversion from sarray to darray",
            "        # if isinstance(right.typ, SArrayType):",
            "        #    return _complex_make_setter(left, right)",
            "",
            "        # TODO rethink/streamline the clamp_basetype logic",
            "        if needs_clamp(right.typ, right.encoding):",
            "            with right.cache_when_complex(\"arr_ptr\") as (b, right):",
            "                copier = _dynarray_make_setter(left, right, hi=hi)",
            "                ret = b.resolve([\"seq\", clamp_dyn_array(right, hi=hi), copier])",
            "        else:",
            "            ret = _dynarray_make_setter(left, right)",
            "",
            "        return IRnode.from_list(ret)",
            "",
            "    # Complex Types",
            "    assert isinstance(left.typ, (SArrayT, TupleT, StructT))",
            "",
            "    with right.cache_when_complex(\"c_right\") as (b1, right):",
            "        ret = [\"seq\"]",
            "        if hi is not None:",
            "            item_end = add_ofst(right, right.typ.abi_type.static_size())",
            "            len_check = [\"assert\", [\"le\", item_end, hi]]",
            "            ret.append(len_check)",
            "",
            "        ret.append(_complex_make_setter(left, right, hi=hi))",
            "        return b1.resolve(IRnode.from_list(ret))",
            "",
            "",
            "# locations with no dedicated copy opcode",
            "# (i.e. storage and transient storage)",
            "def copy_opcode_available(left, right):",
            "    if left.location == MEMORY and right.location == MEMORY:",
            "        return version_check(begin=\"cancun\")",
            "",
            "    return left.location == MEMORY and right.location.has_copy_opcode",
            "",
            "",
            "def _complex_make_setter(left, right, hi=None):",
            "    if right.value == \"~empty\" and left.location == MEMORY:",
            "        # optimized memzero",
            "        return mzero(left, left.typ.memory_bytes_required)",
            "",
            "    ret = [\"seq\"]",
            "",
            "    if isinstance(left.typ, SArrayT):",
            "        n_items = right.typ.count",
            "        keys = [IRnode.from_list(i, typ=UINT256_T) for i in range(n_items)]",
            "",
            "    else:",
            "        assert is_tuple_like(left.typ)",
            "        keys = left.typ.tuple_keys()",
            "",
            "    if left.is_pointer and right.is_pointer and right.encoding == Encoding.VYPER:",
            "        # both left and right are pointers, see if we want to batch copy",
            "        # instead of unrolling the loop.",
            "        assert left.encoding == Encoding.VYPER",
            "        len_ = left.typ.memory_bytes_required",
            "",
            "        # special logic for identity precompile (pre-cancun) in the else branch",
            "        mem2mem = left.location == right.location == MEMORY",
            "",
            "        if not copy_opcode_available(left, right) and not mem2mem:",
            "            if _opt_codesize():",
            "                # assuming PUSH2, a single sstore(dst (sload src)) is 8 bytes,",
            "                # sstore(add (dst ofst), (sload (add (src ofst)))) is 16 bytes,",
            "                # whereas loop overhead is 16-17 bytes.",
            "                base_cost = 3",
            "                if left._optimized.is_literal:",
            "                    # code size is smaller since add is performed at compile-time",
            "                    base_cost += 1",
            "                if right._optimized.is_literal:",
            "                    base_cost += 1",
            "                # the formula is a heuristic, but it works.",
            "                # (CMC 2023-07-14 could get more detailed for PUSH1 vs",
            "                # PUSH2 etc but not worried about that too much now,",
            "                # it's probably better to add a proper unroll rule in the",
            "                # optimizer.)",
            "                should_batch_copy = len_ >= 32 * base_cost",
            "            elif _opt_gas():",
            "                # kind of arbitrary, but cut off when code used > ~160 bytes",
            "                should_batch_copy = len_ >= 32 * 10",
            "            else:",
            "                assert _opt_none()",
            "                # don't care, just generate the most readable version",
            "                should_batch_copy = True",
            "        else:",
            "            # find a cutoff for memory copy where identity is cheaper",
            "            # than unrolled mloads/mstores",
            "            # if MCOPY is available, mcopy is *always* better (except in",
            "            # the 1 word case, but that is already handled by copy_bytes).",
            "            if right.location == MEMORY and _opt_gas() and not version_check(begin=\"cancun\"):",
            "                # cost for 0th word - (mstore dst (mload src))",
            "                base_unroll_cost = 12",
            "                nth_word_cost = base_unroll_cost",
            "                if not left._optimized.is_literal:",
            "                    # (mstore (add N dst) (mload src))",
            "                    nth_word_cost += 6",
            "                if not right._optimized.is_literal:",
            "                    # (mstore dst (mload (add N src)))",
            "                    nth_word_cost += 6",
            "",
            "                identity_base_cost = 115  # staticcall 4 gas dst len src len",
            "",
            "                n_words = ceil32(len_) // 32",
            "                should_batch_copy = (",
            "                    base_unroll_cost + (nth_word_cost * (n_words - 1)) >= identity_base_cost",
            "                )",
            "",
            "            # calldata to memory, code to memory, cancun, or opt-codesize -",
            "            # batch copy is always better.",
            "            else:",
            "                should_batch_copy = True",
            "",
            "        if should_batch_copy:",
            "            return copy_bytes(left, right, len_, len_)",
            "",
            "    # general case, unroll",
            "    with left.cache_when_complex(\"_L\") as (b1, left), right.cache_when_complex(\"_R\") as (b2, right):",
            "        for k in keys:",
            "            l_i = get_element_ptr(left, k, array_bounds_check=False)",
            "            r_i = get_element_ptr(right, k, array_bounds_check=False)",
            "            ret.append(make_setter(l_i, r_i, hi=hi))",
            "",
            "        return b1.resolve(b2.resolve(IRnode.from_list(ret)))",
            "",
            "",
            "def ensure_in_memory(ir_var, context):",
            "    \"\"\"",
            "    Ensure a variable is in memory. This is useful for functions",
            "    which expect to operate on memory variables.",
            "    \"\"\"",
            "    if ir_var.location == MEMORY:",
            "        return ir_var",
            "",
            "    typ = ir_var.typ",
            "    buf = context.new_internal_variable(typ)",
            "    do_copy = make_setter(buf, ir_var)",
            "",
            "    return IRnode.from_list([\"seq\", do_copy, buf], typ=typ, location=MEMORY)",
            "",
            "",
            "def eval_seq(ir_node):",
            "    \"\"\"Tries to find the \"return\" value of a `seq` statement, in order so",
            "    that the value can be known without possibly evaluating side effects",
            "    \"\"\"",
            "    if ir_node.value in (\"seq\", \"with\") and len(ir_node.args) > 0:",
            "        return eval_seq(ir_node.args[-1])",
            "    if isinstance(ir_node.value, int):",
            "        return IRnode.from_list(ir_node)",
            "    return None",
            "",
            "",
            "def mzero(dst, nbytes):",
            "    # calldatacopy from past-the-end gives zero bytes.",
            "    # cf. YP H.2 (ops section) with CALLDATACOPY spec.",
            "    return IRnode.from_list(",
            "        # calldatacopy mempos calldatapos len",
            "        [\"calldatacopy\", dst, \"calldatasize\", nbytes],",
            "        annotation=\"mzero\",",
            "    )",
            "",
            "",
            "# zero pad a bytearray according to the ABI spec. The last word",
            "# of the byte array needs to be right-padded with zeroes.",
            "def zero_pad(bytez_placeholder):",
            "    len_ = [\"mload\", bytez_placeholder]",
            "    dst = [\"add\", [\"add\", bytez_placeholder, 32], \"len\"]",
            "    # the runtime length of the data rounded up to nearest 32",
            "    # from spec:",
            "    #   the actual value of X as a byte sequence,",
            "    #   followed by the *minimum* number of zero-bytes",
            "    #   such that len(enc(X)) is a multiple of 32.",
            "    # optimized form of ceil32(len) - len:",
            "    num_zero_bytes = [\"mod\", [\"sub\", 0, \"len\"], 32]",
            "    return IRnode.from_list(",
            "        [\"with\", \"len\", len_, [\"with\", \"dst\", dst, mzero(\"dst\", num_zero_bytes)]],",
            "        annotation=\"Zero pad\",",
            "    )",
            "",
            "",
            "# convenience rewrites for shr/sar/shl",
            "def shr(bits, x):",
            "    return [\"shr\", bits, x]",
            "",
            "",
            "# convenience rewrites for shr/sar/shl",
            "def shl(bits, x):",
            "    return [\"shl\", bits, x]",
            "",
            "",
            "def sar(bits, x):",
            "    return [\"sar\", bits, x]",
            "",
            "",
            "def clamp_bytestring(ir_node, hi=None):",
            "    t = ir_node.typ",
            "    if not isinstance(t, _BytestringT):  # pragma: nocover",
            "        raise CompilerPanic(f\"{t} passed to clamp_bytestring\")",
            "",
            "    # check if byte array length is within type max",
            "    with get_bytearray_length(ir_node).cache_when_complex(\"length\") as (b1, length):",
            "        len_check = [\"assert\", [\"le\", length, t.maxlen]]",
            "",
            "        assert (hi is not None) == _dirty_read_risk(ir_node)",
            "        if hi is not None:",
            "            assert t.maxlen < 2**64  # sanity check",
            "",
            "            # NOTE: this add does not risk arithmetic overflow because",
            "            # length is bounded by maxlen.",
            "            # however(!) _abi_payload_size can OOG, since it loads the word",
            "            # at `ir_node` to find the length of the bytearray, which could",
            "            # be out-of-bounds.",
            "            # if we didn't get OOG, we could overflow in `add`.",
            "            item_end = add_ofst(ir_node, _abi_payload_size(ir_node))",
            "",
            "            len_check = [\"seq\", [\"assert\", [\"le\", item_end, hi]], len_check]",
            "",
            "        return IRnode.from_list(b1.resolve(len_check), error_msg=f\"{ir_node.typ} bounds check\")",
            "",
            "",
            "def clamp_dyn_array(ir_node, hi=None):",
            "    t = ir_node.typ",
            "    assert isinstance(t, DArrayT)",
            "",
            "    len_check = [\"assert\", [\"le\", get_dyn_array_count(ir_node), t.count]]",
            "",
            "    assert (hi is not None) == _dirty_read_risk(ir_node)",
            "",
            "    if hi is not None:",
            "        assert t.count < 2**64  # sanity check",
            "",
            "        # NOTE: this add does not risk arithmetic overflow because",
            "        # length is bounded by count * elemsize.",
            "        # however(!) _abi_payload_size can OOG, since it loads the word",
            "        # at `ir_node` to find the length of the bytearray, which could",
            "        # be out-of-bounds.",
            "        # if we didn't get OOG, we could overflow in `add`.",
            "        item_end = add_ofst(ir_node, _abi_payload_size(ir_node))",
            "",
            "        # if the subtype is dynamic, the length check is performed in",
            "        # the recursion, UNLESS the count is zero. here we perform the",
            "        # check all the time, but it could maybe be optimized out in the",
            "        # make_setter loop (in the common case that runtime count > 0).",
            "        len_check = [\"seq\", [\"assert\", [\"le\", item_end, hi]], len_check]",
            "",
            "    return IRnode.from_list(len_check, error_msg=f\"{ir_node.typ} bounds check\")",
            "",
            "",
            "# clampers for basetype",
            "def clamp_basetype(ir_node):",
            "    t = ir_node.typ",
            "    if not t._is_prim_word:  # pragma: nocover",
            "        raise CompilerPanic(f\"{t} passed to clamp_basetype\")",
            "",
            "    # copy of the input",
            "    ir_node = unwrap_location(ir_node)",
            "",
            "    if isinstance(t, FlagT):",
            "        bits = len(t._flag_members)",
            "        # assert x >> bits == 0",
            "        ret = int_clamp(ir_node, bits, signed=False)",
            "",
            "    elif isinstance(t, (IntegerT, DecimalT)):",
            "        if t.bits == 256:",
            "            ret = ir_node",
            "        else:",
            "            ret = int_clamp(ir_node, t.bits, signed=t.is_signed)",
            "",
            "    elif isinstance(t, BytesM_T):",
            "        if t.m == 32:",
            "            ret = ir_node  # special case, no clamp.",
            "        else:",
            "            ret = bytes_clamp(ir_node, t.m)",
            "",
            "    elif isinstance(t, (AddressT, InterfaceT)):",
            "        ret = int_clamp(ir_node, 160)",
            "    elif t in (BoolT(),):",
            "        ret = int_clamp(ir_node, 1)",
            "    else:  # pragma: no cover",
            "        raise CompilerPanic(f\"{t} passed to clamp_basetype\")",
            "",
            "    return IRnode.from_list(ret, typ=ir_node.typ, error_msg=f\"validate {t}\")",
            "",
            "",
            "def int_clamp(ir_node, bits, signed=False):",
            "    \"\"\"Generalized clamper for integer types. Takes the number of bits,",
            "    whether it's signed, and returns an IR node which checks it is",
            "    in bounds. (Consumers should use clamp_basetype instead which uses",
            "    type-based dispatch and is a little safer.)",
            "    \"\"\"",
            "    if bits >= 256:  # pragma: nocover",
            "        raise CompilerPanic(f\"invalid clamp: {bits}>=256 ({ir_node})\")",
            "",
            "    u = \"u\" if not signed else \"\"",
            "    msg = f\"{u}int{bits} bounds check\"",
            "    with ir_node.cache_when_complex(\"val\") as (b, val):",
            "        if signed:",
            "            # example for bits==128:",
            "            # promote_signed_int(val, bits) is the \"canonical\" version of val",
            "            # if val is in bounds, the bits above bit 128 should be equal.",
            "            # (this works for both val >= 0 and val < 0. in the first case,",
            "            # all upper bits should be 0 if val is a valid int128,",
            "            # in the latter case, all upper bits should be 1.)",
            "            assertion = [\"assert\", [\"eq\", val, promote_signed_int(val, bits)]]",
            "        else:",
            "            assertion = [\"assert\", [\"iszero\", shr(bits, val)]]",
            "",
            "        assertion = IRnode.from_list(assertion, error_msg=msg)",
            "",
            "        ret = b.resolve([\"seq\", assertion, val])",
            "",
            "    return IRnode.from_list(ret, annotation=msg)",
            "",
            "",
            "def bytes_clamp(ir_node: IRnode, n_bytes: int) -> IRnode:",
            "    if not (0 < n_bytes <= 32):  # pragma: nocover",
            "        raise CompilerPanic(f\"bad type: bytes{n_bytes}\")",
            "    msg = f\"bytes{n_bytes} bounds check\"",
            "    with ir_node.cache_when_complex(\"val\") as (b, val):",
            "        assertion = IRnode.from_list([\"assert\", [\"iszero\", shl(n_bytes * 8, val)]], error_msg=msg)",
            "        ret = b.resolve([\"seq\", assertion, val])",
            "",
            "    return IRnode.from_list(ret, annotation=msg)",
            "",
            "",
            "# e.g. for int8, promote 255 to -1",
            "def promote_signed_int(x, bits):",
            "    assert bits % 8 == 0",
            "    ret = [\"signextend\", bits // 8 - 1, x]",
            "    return IRnode.from_list(ret, annotation=f\"promote int{bits}\")",
            "",
            "",
            "# general clamp function for all ops and numbers",
            "def clamp(op, arg, bound):",
            "    with IRnode.from_list(arg).cache_when_complex(\"clamp_arg\") as (b1, arg):",
            "        check = IRnode.from_list([\"assert\", [op, arg, bound]], error_msg=f\"clamp {op} {bound}\")",
            "        ret = [\"seq\", check, arg]",
            "        return IRnode.from_list(b1.resolve(ret), typ=arg.typ)",
            "",
            "",
            "def clamp_nonzero(arg):",
            "    # TODO: use clamp(\"ne\", arg, 0) once optimizer rules can handle it",
            "    with IRnode.from_list(arg).cache_when_complex(\"should_nonzero\") as (b1, arg):",
            "        check = IRnode.from_list([\"assert\", arg], error_msg=\"check nonzero\")",
            "        ret = [\"seq\", check, arg]",
            "        return IRnode.from_list(b1.resolve(ret), typ=arg.typ)",
            "",
            "",
            "def clamp_le(arg, hi, signed):",
            "    LE = \"sle\" if signed else \"le\"",
            "    return clamp(LE, arg, hi)",
            "",
            "",
            "def clamp2(lo, arg, hi, signed):",
            "    with IRnode.from_list(arg).cache_when_complex(\"clamp2_arg\") as (b1, arg):",
            "        GE = \"sge\" if signed else \"ge\"",
            "        LE = \"sle\" if signed else \"le\"",
            "        ret = [\"seq\", [\"assert\", [\"and\", [GE, arg, lo], [LE, arg, hi]]], arg]",
            "        return IRnode.from_list(b1.resolve(ret), typ=arg.typ)",
            "",
            "",
            "# make sure we don't overrun the source buffer, checking for overflow:",
            "# valid inputs satisfy:",
            "#   `assert !(start+length > src_len || start+length < start)`",
            "def check_buffer_overflow_ir(start, length, src_len):",
            "    with start.cache_when_complex(\"start\") as (b1, start):",
            "        with add_ofst(start, length).cache_when_complex(\"end\") as (b2, end):",
            "            arithmetic_overflow = [\"lt\", end, start]",
            "            buffer_oob = [\"gt\", end, src_len]",
            "            ok = [\"iszero\", [\"or\", arithmetic_overflow, buffer_oob]]",
            "            return b1.resolve(b2.resolve([\"assert\", ok]))"
        ],
        "afterPatchFile": [
            "import vyper.codegen.context as ctx",
            "from vyper.codegen.ir_node import Encoding, IRnode",
            "from vyper.compiler.settings import _opt_codesize, _opt_gas, _opt_none",
            "from vyper.evm.address_space import (",
            "    CALLDATA,",
            "    DATA,",
            "    IMMUTABLES,",
            "    MEMORY,",
            "    STORAGE,",
            "    TRANSIENT,",
            "    AddrSpace,",
            "    legal_in_staticcall,",
            ")",
            "from vyper.evm.opcodes import version_check",
            "from vyper.exceptions import CompilerPanic, TypeCheckFailure, TypeMismatch",
            "from vyper.semantics.data_locations import DataLocation",
            "from vyper.semantics.types import (",
            "    AddressT,",
            "    BoolT,",
            "    BytesM_T,",
            "    BytesT,",
            "    DArrayT,",
            "    DecimalT,",
            "    HashMapT,",
            "    IntegerT,",
            "    InterfaceT,",
            "    StructT,",
            "    TupleT,",
            "    _BytestringT,",
            ")",
            "from vyper.semantics.types.shortcuts import BYTES32_T, INT256_T, UINT256_T",
            "from vyper.semantics.types.subscriptable import SArrayT",
            "from vyper.semantics.types.user import FlagT",
            "from vyper.utils import GAS_COPY_WORD, GAS_IDENTITY, GAS_IDENTITYWORD, ceil32",
            "",
            "DYNAMIC_ARRAY_OVERHEAD = 1",
            "",
            "",
            "def is_bytes_m_type(typ):",
            "    return isinstance(typ, BytesM_T)",
            "",
            "",
            "def is_numeric_type(typ):",
            "    return isinstance(typ, (IntegerT, DecimalT))",
            "",
            "",
            "def is_integer_type(typ):",
            "    return isinstance(typ, IntegerT)",
            "",
            "",
            "def is_decimal_type(typ):",
            "    return isinstance(typ, DecimalT)",
            "",
            "",
            "def is_flag_type(typ):",
            "    return isinstance(typ, FlagT)",
            "",
            "",
            "def is_tuple_like(typ):",
            "    # A lot of code paths treat tuples and structs similarly",
            "    # so we have a convenience function to detect it",
            "    ret = isinstance(typ, (TupleT, StructT))",
            "    assert ret == hasattr(typ, \"tuple_items\")",
            "    return ret",
            "",
            "",
            "def is_array_like(typ):",
            "    # For convenience static and dynamic arrays share some code paths",
            "    ret = isinstance(typ, (DArrayT, SArrayT))",
            "    assert ret == typ._is_array_type",
            "    return ret",
            "",
            "",
            "def get_type_for_exact_size(n_bytes):",
            "    \"\"\"Create a type which will take up exactly n_bytes. Used for allocating internal buffers.",
            "",
            "    Parameters:",
            "      n_bytes: the number of bytes to allocate",
            "    Returns:",
            "      type: A type which can be passed to context.new_variable",
            "    \"\"\"",
            "    return BytesT(n_bytes - 32 * DYNAMIC_ARRAY_OVERHEAD)",
            "",
            "",
            "# propagate revert message when calls to external contracts fail",
            "def check_external_call(call_ir):",
            "    copy_revertdata = [\"returndatacopy\", 0, 0, \"returndatasize\"]",
            "    revert = IRnode.from_list([\"revert\", 0, \"returndatasize\"], error_msg=\"external call failed\")",
            "",
            "    propagate_revert_ir = [\"seq\", copy_revertdata, revert]",
            "    return [\"if\", [\"iszero\", call_ir], propagate_revert_ir]",
            "",
            "",
            "# cost per byte of the identity precompile",
            "def _identity_gas_bound(num_bytes):",
            "    return GAS_IDENTITY + GAS_IDENTITYWORD * (ceil32(num_bytes) // 32)",
            "",
            "",
            "def _mcopy_gas_bound(num_bytes):",
            "    return GAS_COPY_WORD * ceil32(num_bytes) // 32",
            "",
            "",
            "def _calldatacopy_gas_bound(num_bytes):",
            "    return GAS_COPY_WORD * ceil32(num_bytes) // 32",
            "",
            "",
            "def _codecopy_gas_bound(num_bytes):",
            "    return GAS_COPY_WORD * ceil32(num_bytes) // 32",
            "",
            "",
            "def data_location_to_address_space(s: DataLocation, is_ctor_ctx: bool) -> AddrSpace:",
            "    if s == DataLocation.MEMORY:",
            "        return MEMORY",
            "    if s == DataLocation.STORAGE:",
            "        return STORAGE",
            "    if s == DataLocation.TRANSIENT:",
            "        return TRANSIENT",
            "    if s == DataLocation.CODE:",
            "        if is_ctor_ctx:",
            "            return IMMUTABLES",
            "        return DATA",
            "",
            "    raise CompilerPanic(\"unreachable!\")  # pragma: nocover",
            "",
            "",
            "def address_space_to_data_location(s: AddrSpace) -> DataLocation:",
            "    if s == MEMORY:",
            "        return DataLocation.MEMORY",
            "    if s == STORAGE:",
            "        return DataLocation.STORAGE",
            "    if s == TRANSIENT:",
            "        return DataLocation.TRANSIENT",
            "    if s in (IMMUTABLES, DATA):",
            "        return DataLocation.CODE",
            "    if s == CALLDATA:",
            "        return DataLocation.CALLDATA",
            "",
            "    raise CompilerPanic(\"unreachable!\")  # pragma: nocover",
            "",
            "",
            "def writeable(context, ir_node):",
            "    assert ir_node.is_pointer  # sanity check",
            "",
            "    if context.is_constant() and not legal_in_staticcall(ir_node.location):",
            "        return False",
            "    return ir_node.mutable",
            "",
            "",
            "# Copy byte array word-for-word (including layout)",
            "# TODO make this a private function",
            "def make_byte_array_copier(dst, src):",
            "    assert isinstance(src.typ, _BytestringT)",
            "    assert isinstance(dst.typ, _BytestringT)",
            "",
            "    _check_assign_bytes(dst, src)",
            "",
            "    # TODO: remove this branch, copy_bytes and get_bytearray_length should handle",
            "    if src.value == \"~empty\" or src.typ.maxlen == 0:",
            "        # set length word to 0.",
            "        return STORE(dst, 0)",
            "",
            "    with src.cache_when_complex(\"src\") as (b1, src):",
            "        if src.typ.maxlen <= 32 and not copy_opcode_available(dst, src):",
            "            # if there is no batch copy opcode available,",
            "            # it's cheaper to run two load/stores instead of copy_bytes",
            "            ret = [\"seq\"]",
            "            # store length word",
            "            len_ = get_bytearray_length(src)",
            "            ret.append(STORE(dst, len_))",
            "",
            "            # store the single data word.",
            "            dst_data_ptr = bytes_data_ptr(dst)",
            "            src_data_ptr = bytes_data_ptr(src)",
            "            ret.append(STORE(dst_data_ptr, LOAD(src_data_ptr)))",
            "            return b1.resolve(ret)",
            "",
            "        # batch copy the bytearray (including length word) using copy_bytes",
            "        len_ = add_ofst(get_bytearray_length(src), 32)",
            "        max_bytes = src.typ.maxlen + 32",
            "        ret = copy_bytes(dst, src, len_, max_bytes)",
            "        return b1.resolve(ret)",
            "",
            "",
            "def bytes_data_ptr(ptr):",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"tried to modify non-pointer type\")",
            "    assert isinstance(ptr.typ, _BytestringT)",
            "    return add_ofst(ptr, ptr.location.word_scale)",
            "",
            "",
            "def dynarray_data_ptr(ptr):",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"tried to modify non-pointer type\")",
            "    assert isinstance(ptr.typ, DArrayT)",
            "    return add_ofst(ptr, ptr.location.word_scale)",
            "",
            "",
            "def _dynarray_make_setter(dst, src, hi=None):",
            "    assert isinstance(src.typ, DArrayT)",
            "    assert isinstance(dst.typ, DArrayT)",
            "",
            "    if src.value == \"~empty\":",
            "        return IRnode.from_list(STORE(dst, 0))",
            "",
            "    # copy contents of src dynarray to dst.",
            "    # note that in case src and dst refer to the same dynarray,",
            "    # in order for get_element_ptr oob checks on the src dynarray",
            "    # to work, we need to wait until after the data is copied",
            "    # before we clobber the length word.",
            "",
            "    if src.value == \"multi\":",
            "        # validation is only performed on unsafe data, but we are dealing with",
            "        # a literal here.",
            "        assert hi is None",
            "        ret = [\"seq\"]",
            "        # handle literals",
            "",
            "        # copy each item",
            "        n_items = len(src.args)",
            "",
            "        for i in range(n_items):",
            "            k = IRnode.from_list(i, typ=UINT256_T)",
            "            dst_i = get_element_ptr(dst, k, array_bounds_check=False)",
            "            src_i = get_element_ptr(src, k, array_bounds_check=False)",
            "            ret.append(make_setter(dst_i, src_i))",
            "",
            "        # write the length word after data is copied",
            "        store_length = STORE(dst, n_items)",
            "        ann = None",
            "        if src.annotation is not None:",
            "            ann = f\"len({src.annotation})\"",
            "        store_length = IRnode.from_list(store_length, annotation=ann)",
            "",
            "        ret.append(store_length)",
            "",
            "        return ret",
            "",
            "    with src.cache_when_complex(\"darray_src\") as (b1, src):",
            "        # for ABI-encoded dynamic data, we must loop to unpack, since",
            "        # the layout does not match our memory layout",
            "        should_loop = src.encoding == Encoding.ABI and src.typ.value_type.abi_type.is_dynamic()",
            "",
            "        # if the data is not validated, we must loop to unpack",
            "        should_loop |= needs_clamp(src.typ.value_type, src.encoding)",
            "",
            "        # performance: if the subtype is dynamic, there might be a lot",
            "        # of unused space inside of each element. for instance",
            "        # DynArray[DynArray[uint256, 100], 5] where all the child",
            "        # arrays are empty - for this case, we recursively call",
            "        # into make_setter instead of straight bytes copy",
            "        # TODO we can make this heuristic more precise, e.g.",
            "        # loop when subtype.is_dynamic AND location == storage",
            "        # OR array_size <= /bound where loop is cheaper than memcpy/",
            "        should_loop |= src.typ.value_type.abi_type.is_dynamic()",
            "",
            "        with get_dyn_array_count(src).cache_when_complex(\"darray_count\") as (b2, count):",
            "            ret = [\"seq\"]",
            "",
            "            if should_loop:",
            "                i = IRnode.from_list(_freshname(\"copy_darray_ix\"), typ=UINT256_T)",
            "",
            "                loop_body = make_setter(",
            "                    get_element_ptr(dst, i, array_bounds_check=False),",
            "                    get_element_ptr(src, i, array_bounds_check=False),",
            "                    hi=hi,",
            "                )",
            "                loop_body.annotation = f\"{dst}[i] = {src}[i]\"",
            "",
            "                ret.append([\"repeat\", i, 0, count, src.typ.count, loop_body])",
            "                # write the length word after data is copied",
            "                ret.append(STORE(dst, count))",
            "",
            "            else:",
            "                element_size = src.typ.value_type.memory_bytes_required",
            "                # number of elements * size of element in bytes + length word",
            "                n_bytes = add_ofst(_mul(count, element_size), 32)",
            "                max_bytes = 32 + src.typ.count * element_size",
            "",
            "                # batch copy the entire dynarray, including length word",
            "                ret.append(copy_bytes(dst, src, n_bytes, max_bytes))",
            "",
            "            return b1.resolve(b2.resolve(ret))",
            "",
            "",
            "# Copy bytes",
            "# Accepts 4 arguments:",
            "# (i) an IR node for the start position of the source",
            "# (ii) an IR node for the start position of the destination",
            "# (iii) an IR node for the length (in bytes)",
            "# (iv) a constant for the max length (in bytes)",
            "# NOTE: may pad to ceil32 of `length`! If you ask to copy 1 byte, it may",
            "# copy an entire (32-byte) word, depending on the copy routine chosen.",
            "# TODO maybe always pad to ceil32, to reduce dirty bytes bugs",
            "def copy_bytes(dst, src, length, length_bound):",
            "    annotation = f\"copy up to {length_bound} bytes from {src} to {dst}\"",
            "",
            "    src = IRnode.from_list(src)",
            "    dst = IRnode.from_list(dst)",
            "    length = IRnode.from_list(length)",
            "",
            "    with src.cache_when_complex(\"src\") as (b1, src), length.cache_when_complex(",
            "        \"copy_bytes_count\"",
            "    ) as (b2, length), dst.cache_when_complex(\"dst\") as (b3, dst):",
            "        assert isinstance(length_bound, int) and length_bound >= 0",
            "",
            "        # correctness: do not clobber dst",
            "        if length_bound == 0:",
            "            return IRnode.from_list([\"seq\"], annotation=annotation)",
            "        # performance: if we know that length is 0, do not copy anything",
            "        if length.value == 0:",
            "            return IRnode.from_list([\"seq\"], annotation=annotation)",
            "",
            "        assert src.is_pointer and dst.is_pointer",
            "",
            "        # fast code for common case where num bytes is small",
            "        if length_bound <= 32:",
            "            copy_op = STORE(dst, LOAD(src))",
            "            ret = IRnode.from_list(copy_op, annotation=annotation)",
            "            return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "        if dst.location == MEMORY and src.location in (MEMORY, CALLDATA, DATA):",
            "            # special cases: batch copy to memory",
            "            # TODO: iloadbytes",
            "            if src.location == MEMORY:",
            "                if version_check(begin=\"cancun\"):",
            "                    copy_op = [\"mcopy\", dst, src, length]",
            "                    gas_bound = _mcopy_gas_bound(length_bound)",
            "                else:",
            "                    copy_op = [\"assert\", [\"staticcall\", \"gas\", 4, src, length, dst, length]]",
            "                    gas_bound = _identity_gas_bound(length_bound)",
            "            elif src.location == CALLDATA:",
            "                copy_op = [\"calldatacopy\", dst, src, length]",
            "                gas_bound = _calldatacopy_gas_bound(length_bound)",
            "            elif src.location == DATA:",
            "                copy_op = [\"dloadbytes\", dst, src, length]",
            "                # note: dloadbytes compiles to CODECOPY",
            "                gas_bound = _codecopy_gas_bound(length_bound)",
            "",
            "            ret = IRnode.from_list(copy_op, annotation=annotation, add_gas_estimate=gas_bound)",
            "            return b1.resolve(b2.resolve(b3.resolve(ret)))",
            "",
            "        if dst.location == IMMUTABLES and src.location in (MEMORY, DATA):",
            "            # TODO istorebytes-from-mem, istorebytes-from-calldata(?)",
            "            # compile to identity, CODECOPY respectively.",
            "            pass",
            "",
            "        # general case, copy word-for-word",
            "        # pseudocode for our approach (memory-storage as example):",
            "        # for i in range(len, bound=MAX_LEN):",
            "        #   sstore(_dst + i, mload(src + i * 32))",
            "        i = IRnode.from_list(_freshname(\"copy_bytes_ix\"), typ=UINT256_T)",
            "",
            "        # optimized form of (div (ceil32 len) 32)",
            "        n = [\"div\", [\"add\", 31, length], 32]",
            "        n_bound = ceil32(length_bound) // 32",
            "",
            "        dst_i = add_ofst(dst, _mul(i, dst.location.word_scale))",
            "        src_i = add_ofst(src, _mul(i, src.location.word_scale))",
            "",
            "        copy_one_word = STORE(dst_i, LOAD(src_i))",
            "",
            "        main_loop = [\"repeat\", i, 0, n, n_bound, copy_one_word]",
            "",
            "        return b1.resolve(",
            "            b2.resolve(b3.resolve(IRnode.from_list(main_loop, annotation=annotation)))",
            "        )",
            "",
            "",
            "# get the number of bytes at runtime",
            "def get_bytearray_length(arg):",
            "    typ = UINT256_T",
            "",
            "    # TODO: it would be nice to merge the implementations of get_bytearray_length and",
            "    # get_dynarray_count",
            "    if arg.value == \"~empty\":",
            "        return IRnode.from_list(0, typ=typ)",
            "",
            "    return IRnode.from_list(LOAD(arg), typ=typ)",
            "",
            "",
            "# get the number of elements at runtime",
            "def get_dyn_array_count(arg):",
            "    assert isinstance(arg.typ, DArrayT)",
            "",
            "    typ = UINT256_T",
            "",
            "    if arg.value == \"multi\":",
            "        return IRnode.from_list(len(arg.args), typ=typ)",
            "",
            "    if arg.value == \"~empty\":",
            "        # empty(DynArray[...])",
            "        return IRnode.from_list(0, typ=typ)",
            "",
            "    return IRnode.from_list(LOAD(arg), typ=typ)",
            "",
            "",
            "def append_dyn_array(darray_node, elem_node):",
            "    assert isinstance(darray_node.typ, DArrayT)",
            "",
            "    assert darray_node.typ.count > 0, \"jerk boy u r out\"",
            "",
            "    ret = [\"seq\"]",
            "    with darray_node.cache_when_complex(\"darray\") as (b1, darray_node):",
            "        len_ = get_dyn_array_count(darray_node)",
            "        with len_.cache_when_complex(\"old_darray_len\") as (b2, len_):",
            "            assertion = [\"assert\", [\"lt\", len_, darray_node.typ.count]]",
            "            ret.append(IRnode.from_list(assertion, error_msg=f\"{darray_node.typ} bounds check\"))",
            "            # NOTE: typechecks elem_node",
            "            # NOTE skip array bounds check bc we already asserted len two lines up",
            "            ret.append(",
            "                make_setter(get_element_ptr(darray_node, len_, array_bounds_check=False), elem_node)",
            "            )",
            "",
            "            # store new length",
            "            ret.append(ensure_eval_once(\"append_dynarray\", STORE(darray_node, [\"add\", len_, 1])))",
            "",
            "            return IRnode.from_list(b1.resolve(b2.resolve(ret)))",
            "",
            "",
            "def pop_dyn_array(darray_node, return_popped_item):",
            "    assert isinstance(darray_node.typ, DArrayT)",
            "    assert darray_node.encoding == Encoding.VYPER",
            "    ret = [\"seq\"]",
            "    with darray_node.cache_when_complex(\"darray\") as (b1, darray_node):",
            "        old_len = clamp(\"gt\", get_dyn_array_count(darray_node), 0)",
            "        new_len = IRnode.from_list([\"sub\", old_len, 1], typ=UINT256_T)",
            "",
            "        with new_len.cache_when_complex(\"new_len\") as (b2, new_len):",
            "            # store new length",
            "            ret.append(ensure_eval_once(\"pop_dynarray\", STORE(darray_node, new_len)))",
            "",
            "            # NOTE skip array bounds check bc we already asserted len two lines up",
            "            if return_popped_item:",
            "                popped_item = get_element_ptr(darray_node, new_len, array_bounds_check=False)",
            "                ret.append(popped_item)",
            "                typ = popped_item.typ",
            "                location = popped_item.location",
            "            else:",
            "                typ, location = None, None",
            "",
            "            return IRnode.from_list(b1.resolve(b2.resolve(ret)), typ=typ, location=location)",
            "",
            "",
            "# add an offset to a pointer, keeping location and encoding info",
            "def add_ofst(ptr, ofst):",
            "    ret = [\"add\", ptr, ofst]",
            "    return IRnode.from_list(ret, location=ptr.location, encoding=ptr.encoding)",
            "",
            "",
            "# shorthand util",
            "def _mul(x, y):",
            "    ret = [\"mul\", x, y]",
            "    return IRnode.from_list(ret)",
            "",
            "",
            "# Resolve pointer locations for ABI-encoded data",
            "def _getelemptr_abi_helper(parent, member_t, ofst):",
            "    member_abi_t = member_t.abi_type",
            "",
            "    # ABI encoding has length word and then pretends length is not there",
            "    # e.g. [[1,2]] is encoded as 0x01 <len> 0x20 <inner array ofst> <encode(inner array)>",
            "    # note that inner array ofst is 0x20, not 0x40.",
            "    if has_length_word(parent.typ):",
            "        parent = add_ofst(parent, parent.location.word_scale * DYNAMIC_ARRAY_OVERHEAD)",
            "",
            "    ofst_ir = add_ofst(parent, ofst)",
            "",
            "    if member_abi_t.is_dynamic():",
            "        # double dereference, according to ABI spec",
            "        ofst_ir = add_ofst(parent, unwrap_location(ofst_ir))",
            "        if _dirty_read_risk(ofst_ir):",
            "            # check no arithmetic overflow",
            "            ofst_ir = [\"seq\", [\"assert\", [\"ge\", ofst_ir, parent]], ofst_ir]",
            "",
            "    return IRnode.from_list(",
            "        ofst_ir,",
            "        typ=member_t,",
            "        location=parent.location,",
            "        encoding=parent.encoding,",
            "        annotation=f\"{parent}{ofst}\",",
            "    )",
            "",
            "",
            "# TODO simplify this code, especially the ABI decoding",
            "def _get_element_ptr_tuplelike(parent, key, hi=None):",
            "    typ = parent.typ",
            "    assert is_tuple_like(typ)",
            "",
            "    if isinstance(typ, StructT):",
            "        assert isinstance(key, str)",
            "        subtype = typ.member_types[key]",
            "        attrs = list(typ.tuple_keys())",
            "        index = attrs.index(key)",
            "        annotation = key",
            "    else:",
            "        assert isinstance(typ, TupleT)",
            "        assert isinstance(key, int)",
            "        subtype = typ.member_types[key]",
            "        attrs = list(typ.tuple_keys())",
            "        index = key",
            "        annotation = None",
            "",
            "    # generated by empty() + make_setter",
            "    if parent.value == \"~empty\":",
            "        return IRnode.from_list(\"~empty\", typ=subtype)",
            "",
            "    if parent.value == \"multi\":",
            "        assert parent.encoding != Encoding.ABI, \"no abi-encoded literals\"",
            "        return parent.args[index]",
            "",
            "    ofst = 0  # offset from parent start",
            "",
            "    if parent.encoding == Encoding.ABI:",
            "        if parent.location in (STORAGE, TRANSIENT):  # pragma: nocover",
            "            raise CompilerPanic(\"storage variables should not be abi encoded\")",
            "",
            "        member_t = typ.member_types[attrs[index]]",
            "",
            "        for i in range(index):",
            "            member_abi_t = typ.member_types[attrs[i]].abi_type",
            "            ofst += member_abi_t.embedded_static_size()",
            "",
            "        return _getelemptr_abi_helper(parent, member_t, ofst)",
            "",
            "    data_location = address_space_to_data_location(parent.location)",
            "    for i in range(index):",
            "        t = typ.member_types[attrs[i]]",
            "        ofst += t.get_size_in(data_location)",
            "",
            "    return IRnode.from_list(",
            "        add_ofst(parent, ofst),",
            "        typ=subtype,",
            "        location=parent.location,",
            "        encoding=parent.encoding,",
            "        annotation=annotation,",
            "    )",
            "",
            "",
            "def has_length_word(typ):",
            "    # Consider moving this to an attribute on typ",
            "    return isinstance(typ, (DArrayT, _BytestringT))",
            "",
            "",
            "# TODO simplify this code, especially the ABI decoding",
            "def _get_element_ptr_array(parent, key, array_bounds_check):",
            "    assert is_array_like(parent.typ)",
            "",
            "    if not is_integer_type(key.typ):  # pragma: nocover",
            "        raise TypeCheckFailure(f\"{key.typ} used as array index\")",
            "",
            "    subtype = parent.typ.value_type",
            "",
            "    if parent.value == \"~empty\":",
            "        if array_bounds_check:",
            "            # this case was previously missing a bounds check. codegen",
            "            # is a bit complicated when bounds check is required, so",
            "            # block it. there is no reason to index into a literal empty",
            "            # array anyways!",
            "            raise TypeCheckFailure(\"indexing into zero array not allowed\")",
            "        return IRnode.from_list(\"~empty\", subtype)",
            "",
            "    if parent.value == \"multi\":",
            "        assert isinstance(key.value, int)",
            "        return parent.args[key.value]",
            "",
            "    ix = unwrap_location(key)",
            "",
            "    if array_bounds_check:",
            "        is_darray = isinstance(parent.typ, DArrayT)",
            "        bound = get_dyn_array_count(parent) if is_darray else parent.typ.count",
            "        # NOTE: there are optimization rules for the bounds check when",
            "        # ix or bound is literal",
            "        with ix.cache_when_complex(\"ix\") as (b1, ix):",
            "            LT = \"slt\" if ix.typ.is_signed else \"lt\"",
            "            # note: this is optimized out for unsigned integers",
            "            is_negative = [LT, ix, 0]",
            "            # always use unsigned ge, since bound is always an unsigned quantity",
            "            is_oob = [\"ge\", ix, bound]",
            "            checked_ix = [\"seq\", [\"assert\", [\"iszero\", [\"or\", is_negative, is_oob]]], ix]",
            "            ix = b1.resolve(IRnode.from_list(checked_ix))",
            "        ix.set_error_msg(f\"{parent.typ} bounds check\")",
            "",
            "    if parent.encoding == Encoding.ABI:",
            "        if parent.location in (STORAGE, TRANSIENT):  # pragma: nocover",
            "            raise CompilerPanic(\"storage variables should not be abi encoded\")",
            "",
            "        member_abi_t = subtype.abi_type",
            "",
            "        ofst = _mul(ix, member_abi_t.embedded_static_size())",
            "",
            "        return _getelemptr_abi_helper(parent, subtype, ofst)",
            "",
            "    data_location = address_space_to_data_location(parent.location)",
            "    element_size = subtype.get_size_in(data_location)",
            "",
            "    ofst = _mul(ix, element_size)",
            "",
            "    if has_length_word(parent.typ):",
            "        data_ptr = add_ofst(parent, parent.location.word_scale * DYNAMIC_ARRAY_OVERHEAD)",
            "    else:",
            "        data_ptr = parent",
            "",
            "    return IRnode.from_list(add_ofst(data_ptr, ofst), typ=subtype, location=parent.location)",
            "",
            "",
            "def _get_element_ptr_mapping(parent, key):",
            "    assert isinstance(parent.typ, HashMapT)",
            "    subtype = parent.typ.value_type",
            "    key = unwrap_location(key)",
            "",
            "    if parent.location not in (STORAGE, TRANSIENT):  # pragma: nocover",
            "        raise TypeCheckFailure(f\"bad dereference on mapping {parent}[{key}]\")",
            "",
            "    return IRnode.from_list([\"sha3_64\", parent, key], typ=subtype, location=parent.location)",
            "",
            "",
            "# Take a value representing a memory or storage location, and descend down to",
            "# an element or member variable",
            "# This is analogous (but not necessarily equivalent to) getelementptr in LLVM.",
            "def get_element_ptr(parent, key, array_bounds_check=True):",
            "    with parent.cache_when_complex(\"val\") as (b, parent):",
            "        typ = parent.typ",
            "",
            "        if is_tuple_like(typ):",
            "            ret = _get_element_ptr_tuplelike(parent, key)",
            "",
            "        elif isinstance(typ, HashMapT):",
            "            ret = _get_element_ptr_mapping(parent, key)",
            "",
            "        elif is_array_like(typ):",
            "            ret = _get_element_ptr_array(parent, key, array_bounds_check)",
            "",
            "        else:  # pragma: nocover",
            "            raise CompilerPanic(f\"get_element_ptr cannot be called on {typ}\")",
            "",
            "        return b.resolve(ret)",
            "",
            "",
            "def LOAD(ptr: IRnode) -> IRnode:",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"cannot dereference non-pointer type\")",
            "    op = ptr.location.load_op",
            "    if op is None:  # pragma: nocover",
            "        raise CompilerPanic(f\"unreachable {ptr.location}\")",
            "    return IRnode.from_list([op, ptr])",
            "",
            "",
            "def eval_once_check(name):",
            "    # an IRnode which enforces uniqueness. include with a side-effecting",
            "    # operation to sanity check that the codegen pipeline only generates",
            "    # the side-effecting operation once (otherwise, IR-to-assembly will",
            "    # throw a duplicate label exception). there is no runtime overhead",
            "    # since the jumpdest gets optimized out in the final stage of assembly.",
            "    return IRnode.from_list([\"unique_symbol\", name])",
            "",
            "",
            "def ensure_eval_once(name, irnode):",
            "    return [\"seq\", eval_once_check(_freshname(name)), irnode]",
            "",
            "",
            "def STORE(ptr: IRnode, val: IRnode) -> IRnode:",
            "    if ptr.location is None:  # pragma: nocover",
            "        raise CompilerPanic(\"cannot dereference non-pointer type\")",
            "    op = ptr.location.store_op",
            "    if op is None:  # pragma: nocover",
            "        raise CompilerPanic(f\"unreachable {ptr.location}\")",
            "",
            "    store = [op, ptr, val]",
            "    # don't use eval_once_check for memory, immutables because it interferes",
            "    # with optimizer",
            "    if ptr.location in (MEMORY, IMMUTABLES):",
            "        return IRnode.from_list(store)",
            "",
            "    return IRnode.from_list(ensure_eval_once(f\"{op}_\", store))",
            "",
            "",
            "# Unwrap location",
            "def unwrap_location(orig):",
            "    if orig.location is not None:",
            "        return IRnode.from_list(LOAD(orig), typ=orig.typ)",
            "    else:",
            "        # CMC 2022-03-24 TODO refactor so this branch can be removed",
            "        if orig.value == \"~empty\":",
            "            # must be word type",
            "            return IRnode.from_list(0, typ=orig.typ)",
            "        return orig",
            "",
            "",
            "# utility function, constructs an IR tuple out of a list of IR nodes",
            "def ir_tuple_from_args(args):",
            "    typ = TupleT([x.typ for x in args])",
            "    return IRnode.from_list([\"multi\"] + [x for x in args], typ=typ)",
            "",
            "",
            "def needs_external_call_wrap(typ):",
            "    # for calls to ABI conforming contracts.",
            "    # according to the ABI spec, return types are ALWAYS tuples even",
            "    # if only one element is being returned.",
            "    # https://solidity.readthedocs.io/en/latest/abi-spec.html#function-selector-and-argument-encoding",
            "    # \"and the return values v_1, ..., v_k of f are encoded as",
            "    #",
            "    #    enc((v_1, ..., v_k))",
            "    #    i.e. the values are combined into a tuple and encoded.",
            "    # \"",
            "    # therefore, wrap it in a tuple if it's not already a tuple.",
            "    # for example, `bytes` is returned as abi-encoded (bytes,)",
            "    # and `(bytes,)` is returned as abi-encoded ((bytes,),)",
            "    # In general `-> X` gets returned as (X,)",
            "    # including structs. MyStruct is returned as abi-encoded (MyStruct,).",
            "    # (Sorry this is so confusing. I didn't make these rules.)",
            "",
            "    return not (isinstance(typ, TupleT) and typ.length > 1)",
            "",
            "",
            "def calculate_type_for_external_return(typ):",
            "    if needs_external_call_wrap(typ):",
            "        return TupleT([typ])",
            "    return typ",
            "",
            "",
            "def wrap_value_for_external_return(ir_val):",
            "    # used for LHS promotion",
            "    if needs_external_call_wrap(ir_val.typ):",
            "        return ir_tuple_from_args([ir_val])",
            "    else:",
            "        return ir_val",
            "",
            "",
            "def set_type_for_external_return(ir_val):",
            "    # used for RHS promotion",
            "    ir_val.typ = calculate_type_for_external_return(ir_val.typ)",
            "",
            "",
            "# return a dummy IRnode with the given type",
            "def dummy_node_for_type(typ):",
            "    return IRnode(\"fake_node\", typ=typ)",
            "",
            "",
            "def _check_assign_bytes(left, right):",
            "    if right.typ.maxlen > left.typ.maxlen:  # pragma: nocover",
            "        raise TypeMismatch(f\"Cannot cast from {right.typ} to {left.typ}\")",
            "",
            "    # stricter check for zeroing a byte array.",
            "    # TODO: these should be TypeCheckFailure instead of TypeMismatch",
            "    if right.value == \"~empty\" and right.typ.maxlen != left.typ.maxlen:  # pragma: nocover",
            "        raise TypeMismatch(f\"Cannot cast from empty({right.typ}) to {left.typ}\")",
            "",
            "",
            "def _check_assign_list(left, right):",
            "    def FAIL():  # pragma: no cover",
            "        raise TypeCheckFailure(f\"assigning {right.typ} to {left.typ}\")",
            "",
            "    if left.value == \"multi\":  # pragma: nocover",
            "        # Cannot do something like [a, b, c] = [1, 2, 3]",
            "        FAIL()",
            "",
            "    if isinstance(left.typ, SArrayT):",
            "        if not is_array_like(right.typ):  # pragma: nocover",
            "            FAIL()",
            "        if left.typ.count != right.typ.count:  # pragma: nocover",
            "            FAIL()",
            "",
            "        # TODO recurse into left, right if literals?",
            "        check_assign(",
            "            dummy_node_for_type(left.typ.value_type), dummy_node_for_type(right.typ.value_type)",
            "        )",
            "",
            "    if isinstance(left.typ, DArrayT):",
            "        if not isinstance(right.typ, DArrayT):  # pragma: nocover",
            "            FAIL()",
            "",
            "        if left.typ.count < right.typ.count:  # pragma: nocover",
            "            FAIL()",
            "",
            "        # stricter check for zeroing",
            "        if right.value == \"~empty\" and right.typ.count != left.typ.count:  # pragma: nocover",
            "            raise TypeCheckFailure(",
            "                f\"Bad type for clearing bytes: expected {left.typ} but got {right.typ}\"",
            "            )",
            "",
            "        # TODO recurse into left, right if literals?",
            "        check_assign(",
            "            dummy_node_for_type(left.typ.value_type), dummy_node_for_type(right.typ.value_type)",
            "        )",
            "",
            "",
            "def _check_assign_tuple(left, right):",
            "    def FAIL():  # pragma: no cover",
            "        raise TypeCheckFailure(f\"assigning {right.typ} to {left.typ}\")",
            "",
            "    if not isinstance(right.typ, left.typ.__class__):  # pragma: nocover",
            "        FAIL()",
            "",
            "    if isinstance(left.typ, StructT):",
            "        for k in left.typ.member_types:",
            "            if k not in right.typ.member_types:  # pragma: nocover",
            "                FAIL()",
            "            # TODO recurse into left, right if literals?",
            "            check_assign(",
            "                dummy_node_for_type(left.typ.member_types[k]),",
            "                dummy_node_for_type(right.typ.member_types[k]),",
            "            )",
            "",
            "        for k in right.typ.member_types:",
            "            if k not in left.typ.member_types:  # pragma: nocover",
            "                FAIL()",
            "",
            "        if left.typ.name != right.typ.name:  # pragma: nocover",
            "            FAIL()",
            "",
            "    else:",
            "        if len(left.typ.member_types) != len(right.typ.member_types):  # pragma: nocover",
            "            FAIL()",
            "        for left_, right_ in zip(left.typ.member_types, right.typ.member_types):",
            "            # TODO recurse into left, right if literals?",
            "            check_assign(dummy_node_for_type(left_), dummy_node_for_type(right_))",
            "",
            "",
            "# sanity check an assignment",
            "# typechecking source code is done at an earlier phase",
            "# this function is more of a sanity check for typechecking internally",
            "# generated assignments",
            "# TODO: do we still need this?",
            "def check_assign(left, right):",
            "    def FAIL():  # pragma: no cover",
            "        raise TypeCheckFailure(f\"assigning {right.typ} to {left.typ} {left} {right}\")",
            "",
            "    if isinstance(left.typ, _BytestringT):",
            "        _check_assign_bytes(left, right)",
            "    elif is_array_like(left.typ):",
            "        _check_assign_list(left, right)",
            "    elif is_tuple_like(left.typ):",
            "        _check_assign_tuple(left, right)",
            "",
            "    elif left.typ._is_prim_word:",
            "        # TODO once we propagate types from typechecker, introduce this check:",
            "        # if left.typ != right.typ:  # pragma: nocover",
            "        #    FAIL()",
            "        pass",
            "",
            "    else:  # pragma: no cover",
            "        FAIL()",
            "",
            "",
            "_label = 0",
            "",
            "",
            "# TODO might want to coalesce with Context.fresh_varname and compile_ir.mksymbol",
            "def _freshname(name):",
            "    global _label",
            "    _label += 1",
            "    return f\"{name}{_label}\"",
            "",
            "",
            "def reset_names():",
            "    global _label",
            "    _label = 0",
            "",
            "    # could be refactored",
            "    ctx._alloca_id = 0",
            "",
            "",
            "# returns True if t is ABI encoded and is a type that needs any kind of",
            "# validation",
            "def needs_clamp(t, encoding):",
            "    if encoding == Encoding.VYPER:",
            "        return False",
            "    if encoding != Encoding.ABI:  # pragma: nocover",
            "        raise CompilerPanic(\"unreachable\")",
            "    if isinstance(t, (_BytestringT, DArrayT)):",
            "        return True",
            "    if isinstance(t, FlagT):",
            "        return len(t._flag_members) < 256",
            "    if isinstance(t, SArrayT):",
            "        return needs_clamp(t.value_type, encoding)",
            "    if is_tuple_like(t):",
            "        return any(needs_clamp(m, encoding) for m in t.tuple_members())",
            "    if t._is_prim_word:",
            "        return t not in (INT256_T, UINT256_T, BYTES32_T)",
            "",
            "    raise CompilerPanic(\"unreachable\")  # pragma: nocover",
            "",
            "",
            "# when abi encoded data is user provided and lives in memory,",
            "# we risk either reading oob of the buffer or oob of the payload data.",
            "# in these cases, we need additional validation.",
            "def _dirty_read_risk(ir_node):",
            "    return ir_node.encoding == Encoding.ABI and ir_node.location == MEMORY",
            "",
            "",
            "# child elements which have dynamic length, and could overflow the buffer",
            "# even if the start of the item is in-bounds.",
            "def _abi_payload_size(ir_node):",
            "    SCALE = ir_node.location.word_scale",
            "    assert SCALE == 32  # we must be in some byte-addressable region, like memory",
            "    OFFSET = DYNAMIC_ARRAY_OVERHEAD * SCALE",
            "",
            "    if isinstance(ir_node.typ, DArrayT):",
            "        # the amount of size each value occupies in static section",
            "        # (the amount of size it occupies in the dynamic section is handled in",
            "        # make_setter recursion)",
            "        item_size = ir_node.typ.value_type.abi_type.embedded_static_size()",
            "        return [\"add\", OFFSET, [\"mul\", get_dyn_array_count(ir_node), item_size]]",
            "",
            "    if isinstance(ir_node.typ, _BytestringT):",
            "        return [\"add\", OFFSET, get_bytearray_length(ir_node)]",
            "",
            "    raise CompilerPanic(\"unreachable\")  # pragma: nocover",
            "",
            "",
            "def potential_overlap(left, right):",
            "    \"\"\"",
            "    Return true if make_setter(left, right) could potentially trample",
            "    src or dst during evaluation.",
            "    \"\"\"",
            "    if left.typ._is_prim_word and right.typ._is_prim_word:",
            "        return False",
            "",
            "    if len(left.referenced_variables & right.referenced_variables) > 0:",
            "        return True",
            "",
            "    if len(left.referenced_variables) > 0 and right.contains_risky_call:",
            "        return True",
            "",
            "    if left.contains_risky_call and len(right.referenced_variables) > 0:",
            "        return True",
            "",
            "    return False",
            "",
            "",
            "# similar to `potential_overlap()`, but compares left's _reads_ vs",
            "# right's _writes_.",
            "# TODO: `potential_overlap()` can probably be replaced by this function,",
            "# but all the cases need to be checked.",
            "def read_write_overlap(left, right):",
            "    if not isinstance(left, IRnode) or not isinstance(right, IRnode):",
            "        return False",
            "",
            "    if left.typ._is_prim_word and right.typ._is_prim_word:",
            "        return False",
            "",
            "    if len(left.referenced_variables & right.variable_writes) > 0:",
            "        return True",
            "",
            "    if len(left.referenced_variables) > 0 and right.contains_risky_call:",
            "        return True",
            "",
            "    return False",
            "",
            "",
            "# Create an x=y statement, where the types may be compound",
            "def make_setter(left, right, hi=None):",
            "    check_assign(left, right)",
            "",
            "    if potential_overlap(left, right):",
            "        raise CompilerPanic(\"overlap between src and dst!\")",
            "",
            "    # we need bounds checks when decoding from memory, otherwise we can",
            "    # get oob reads.",
            "    #",
            "    # the caller is responsible for calculating the bound;",
            "    # sanity check that there is a bound if there is dirty read risk",
            "    assert (hi is not None) == _dirty_read_risk(right)",
            "",
            "    # For types which occupy just one word we can use single load/store",
            "    if left.typ._is_prim_word:",
            "        enc = right.encoding  # unwrap_location butchers encoding",
            "        right = unwrap_location(right)",
            "        # TODO rethink/streamline the clamp_basetype logic",
            "        if needs_clamp(right.typ, enc):",
            "            right = clamp_basetype(right)",
            "",
            "        return STORE(left, right)",
            "",
            "    # Byte arrays",
            "    elif isinstance(left.typ, _BytestringT):",
            "        # TODO rethink/streamline the clamp_basetype logic",
            "        if needs_clamp(right.typ, right.encoding):",
            "            with right.cache_when_complex(\"bs_ptr\") as (b, right):",
            "                copier = make_byte_array_copier(left, right)",
            "                ret = b.resolve([\"seq\", clamp_bytestring(right, hi=hi), copier])",
            "        else:",
            "            ret = make_byte_array_copier(left, right)",
            "",
            "        return IRnode.from_list(ret)",
            "",
            "    elif isinstance(left.typ, DArrayT):",
            "        # TODO should we enable this?",
            "        # implicit conversion from sarray to darray",
            "        # if isinstance(right.typ, SArrayType):",
            "        #    return _complex_make_setter(left, right)",
            "",
            "        # TODO rethink/streamline the clamp_basetype logic",
            "        if needs_clamp(right.typ, right.encoding):",
            "            with right.cache_when_complex(\"arr_ptr\") as (b, right):",
            "                copier = _dynarray_make_setter(left, right, hi=hi)",
            "                ret = b.resolve([\"seq\", clamp_dyn_array(right, hi=hi), copier])",
            "        else:",
            "            ret = _dynarray_make_setter(left, right)",
            "",
            "        return IRnode.from_list(ret)",
            "",
            "    # Complex Types",
            "    assert isinstance(left.typ, (SArrayT, TupleT, StructT))",
            "",
            "    with right.cache_when_complex(\"c_right\") as (b1, right):",
            "        ret = [\"seq\"]",
            "        if hi is not None:",
            "            item_end = add_ofst(right, right.typ.abi_type.static_size())",
            "            len_check = [\"assert\", [\"le\", item_end, hi]]",
            "            ret.append(len_check)",
            "",
            "        ret.append(_complex_make_setter(left, right, hi=hi))",
            "        return b1.resolve(IRnode.from_list(ret))",
            "",
            "",
            "# locations with no dedicated copy opcode",
            "# (i.e. storage and transient storage)",
            "def copy_opcode_available(left, right):",
            "    if left.location == MEMORY and right.location == MEMORY:",
            "        return version_check(begin=\"cancun\")",
            "",
            "    return left.location == MEMORY and right.location.has_copy_opcode",
            "",
            "",
            "def _complex_make_setter(left, right, hi=None):",
            "    if right.value == \"~empty\" and left.location == MEMORY:",
            "        # optimized memzero",
            "        return mzero(left, left.typ.memory_bytes_required)",
            "",
            "    ret = [\"seq\"]",
            "",
            "    if isinstance(left.typ, SArrayT):",
            "        n_items = right.typ.count",
            "        keys = [IRnode.from_list(i, typ=UINT256_T) for i in range(n_items)]",
            "",
            "    else:",
            "        assert is_tuple_like(left.typ)",
            "        keys = left.typ.tuple_keys()",
            "",
            "    if left.is_pointer and right.is_pointer and right.encoding == Encoding.VYPER:",
            "        # both left and right are pointers, see if we want to batch copy",
            "        # instead of unrolling the loop.",
            "        assert left.encoding == Encoding.VYPER",
            "        len_ = left.typ.memory_bytes_required",
            "",
            "        # special logic for identity precompile (pre-cancun) in the else branch",
            "        mem2mem = left.location == right.location == MEMORY",
            "",
            "        if not copy_opcode_available(left, right) and not mem2mem:",
            "            if _opt_codesize():",
            "                # assuming PUSH2, a single sstore(dst (sload src)) is 8 bytes,",
            "                # sstore(add (dst ofst), (sload (add (src ofst)))) is 16 bytes,",
            "                # whereas loop overhead is 16-17 bytes.",
            "                base_cost = 3",
            "                if left._optimized.is_literal:",
            "                    # code size is smaller since add is performed at compile-time",
            "                    base_cost += 1",
            "                if right._optimized.is_literal:",
            "                    base_cost += 1",
            "                # the formula is a heuristic, but it works.",
            "                # (CMC 2023-07-14 could get more detailed for PUSH1 vs",
            "                # PUSH2 etc but not worried about that too much now,",
            "                # it's probably better to add a proper unroll rule in the",
            "                # optimizer.)",
            "                should_batch_copy = len_ >= 32 * base_cost",
            "            elif _opt_gas():",
            "                # kind of arbitrary, but cut off when code used > ~160 bytes",
            "                should_batch_copy = len_ >= 32 * 10",
            "            else:",
            "                assert _opt_none()",
            "                # don't care, just generate the most readable version",
            "                should_batch_copy = True",
            "        else:",
            "            # find a cutoff for memory copy where identity is cheaper",
            "            # than unrolled mloads/mstores",
            "            # if MCOPY is available, mcopy is *always* better (except in",
            "            # the 1 word case, but that is already handled by copy_bytes).",
            "            if right.location == MEMORY and _opt_gas() and not version_check(begin=\"cancun\"):",
            "                # cost for 0th word - (mstore dst (mload src))",
            "                base_unroll_cost = 12",
            "                nth_word_cost = base_unroll_cost",
            "                if not left._optimized.is_literal:",
            "                    # (mstore (add N dst) (mload src))",
            "                    nth_word_cost += 6",
            "                if not right._optimized.is_literal:",
            "                    # (mstore dst (mload (add N src)))",
            "                    nth_word_cost += 6",
            "",
            "                identity_base_cost = 115  # staticcall 4 gas dst len src len",
            "",
            "                n_words = ceil32(len_) // 32",
            "                should_batch_copy = (",
            "                    base_unroll_cost + (nth_word_cost * (n_words - 1)) >= identity_base_cost",
            "                )",
            "",
            "            # calldata to memory, code to memory, cancun, or opt-codesize -",
            "            # batch copy is always better.",
            "            else:",
            "                should_batch_copy = True",
            "",
            "        if should_batch_copy:",
            "            return copy_bytes(left, right, len_, len_)",
            "",
            "    # general case, unroll",
            "    with left.cache_when_complex(\"_L\") as (b1, left), right.cache_when_complex(\"_R\") as (b2, right):",
            "        for k in keys:",
            "            l_i = get_element_ptr(left, k, array_bounds_check=False)",
            "            r_i = get_element_ptr(right, k, array_bounds_check=False)",
            "            ret.append(make_setter(l_i, r_i, hi=hi))",
            "",
            "        return b1.resolve(b2.resolve(IRnode.from_list(ret)))",
            "",
            "",
            "def ensure_in_memory(ir_var, context):",
            "    \"\"\"",
            "    Ensure a variable is in memory. This is useful for functions",
            "    which expect to operate on memory variables.",
            "    \"\"\"",
            "    if ir_var.location == MEMORY:",
            "        return ir_var",
            "",
            "    typ = ir_var.typ",
            "    buf = context.new_internal_variable(typ)",
            "    do_copy = make_setter(buf, ir_var)",
            "",
            "    return IRnode.from_list([\"seq\", do_copy, buf], typ=typ, location=MEMORY)",
            "",
            "",
            "def eval_seq(ir_node):",
            "    \"\"\"Tries to find the \"return\" value of a `seq` statement, in order so",
            "    that the value can be known without possibly evaluating side effects",
            "    \"\"\"",
            "    if ir_node.value in (\"seq\", \"with\") and len(ir_node.args) > 0:",
            "        return eval_seq(ir_node.args[-1])",
            "    if isinstance(ir_node.value, int):",
            "        return IRnode.from_list(ir_node)",
            "    return None",
            "",
            "",
            "def mzero(dst, nbytes):",
            "    # calldatacopy from past-the-end gives zero bytes.",
            "    # cf. YP H.2 (ops section) with CALLDATACOPY spec.",
            "    return IRnode.from_list(",
            "        # calldatacopy mempos calldatapos len",
            "        [\"calldatacopy\", dst, \"calldatasize\", nbytes],",
            "        annotation=\"mzero\",",
            "    )",
            "",
            "",
            "# zero pad a bytearray according to the ABI spec. The last word",
            "# of the byte array needs to be right-padded with zeroes.",
            "def zero_pad(bytez_placeholder):",
            "    len_ = [\"mload\", bytez_placeholder]",
            "    dst = [\"add\", [\"add\", bytez_placeholder, 32], \"len\"]",
            "    # the runtime length of the data rounded up to nearest 32",
            "    # from spec:",
            "    #   the actual value of X as a byte sequence,",
            "    #   followed by the *minimum* number of zero-bytes",
            "    #   such that len(enc(X)) is a multiple of 32.",
            "    # optimized form of ceil32(len) - len:",
            "    num_zero_bytes = [\"mod\", [\"sub\", 0, \"len\"], 32]",
            "    return IRnode.from_list(",
            "        [\"with\", \"len\", len_, [\"with\", \"dst\", dst, mzero(\"dst\", num_zero_bytes)]],",
            "        annotation=\"Zero pad\",",
            "    )",
            "",
            "",
            "# convenience rewrites for shr/sar/shl",
            "def shr(bits, x):",
            "    return [\"shr\", bits, x]",
            "",
            "",
            "# convenience rewrites for shr/sar/shl",
            "def shl(bits, x):",
            "    return [\"shl\", bits, x]",
            "",
            "",
            "def sar(bits, x):",
            "    return [\"sar\", bits, x]",
            "",
            "",
            "def clamp_bytestring(ir_node, hi=None):",
            "    t = ir_node.typ",
            "    if not isinstance(t, _BytestringT):  # pragma: nocover",
            "        raise CompilerPanic(f\"{t} passed to clamp_bytestring\")",
            "",
            "    # check if byte array length is within type max",
            "    with get_bytearray_length(ir_node).cache_when_complex(\"length\") as (b1, length):",
            "        len_check = [\"assert\", [\"le\", length, t.maxlen]]",
            "",
            "        assert (hi is not None) == _dirty_read_risk(ir_node)",
            "        if hi is not None:",
            "            assert t.maxlen < 2**64  # sanity check",
            "",
            "            # NOTE: this add does not risk arithmetic overflow because",
            "            # length is bounded by maxlen.",
            "            # however(!) _abi_payload_size can OOG, since it loads the word",
            "            # at `ir_node` to find the length of the bytearray, which could",
            "            # be out-of-bounds.",
            "            # if we didn't get OOG, we could overflow in `add`.",
            "            item_end = add_ofst(ir_node, _abi_payload_size(ir_node))",
            "",
            "            len_check = [\"seq\", [\"assert\", [\"le\", item_end, hi]], len_check]",
            "",
            "        return IRnode.from_list(b1.resolve(len_check), error_msg=f\"{ir_node.typ} bounds check\")",
            "",
            "",
            "def clamp_dyn_array(ir_node, hi=None):",
            "    t = ir_node.typ",
            "    assert isinstance(t, DArrayT)",
            "",
            "    len_check = [\"assert\", [\"le\", get_dyn_array_count(ir_node), t.count]]",
            "",
            "    assert (hi is not None) == _dirty_read_risk(ir_node)",
            "",
            "    if hi is not None:",
            "        assert t.count < 2**64  # sanity check",
            "",
            "        # NOTE: this add does not risk arithmetic overflow because",
            "        # length is bounded by count * elemsize.",
            "        # however(!) _abi_payload_size can OOG, since it loads the word",
            "        # at `ir_node` to find the length of the bytearray, which could",
            "        # be out-of-bounds.",
            "        # if we didn't get OOG, we could overflow in `add`.",
            "        item_end = add_ofst(ir_node, _abi_payload_size(ir_node))",
            "",
            "        # if the subtype is dynamic, the length check is performed in",
            "        # the recursion, UNLESS the count is zero. here we perform the",
            "        # check all the time, but it could maybe be optimized out in the",
            "        # make_setter loop (in the common case that runtime count > 0).",
            "        len_check = [\"seq\", [\"assert\", [\"le\", item_end, hi]], len_check]",
            "",
            "    return IRnode.from_list(len_check, error_msg=f\"{ir_node.typ} bounds check\")",
            "",
            "",
            "# clampers for basetype",
            "def clamp_basetype(ir_node):",
            "    t = ir_node.typ",
            "    if not t._is_prim_word:  # pragma: nocover",
            "        raise CompilerPanic(f\"{t} passed to clamp_basetype\")",
            "",
            "    # copy of the input",
            "    ir_node = unwrap_location(ir_node)",
            "",
            "    if isinstance(t, FlagT):",
            "        bits = len(t._flag_members)",
            "        # assert x >> bits == 0",
            "        ret = int_clamp(ir_node, bits, signed=False)",
            "",
            "    elif isinstance(t, (IntegerT, DecimalT)):",
            "        if t.bits == 256:",
            "            ret = ir_node",
            "        else:",
            "            ret = int_clamp(ir_node, t.bits, signed=t.is_signed)",
            "",
            "    elif isinstance(t, BytesM_T):",
            "        if t.m == 32:",
            "            ret = ir_node  # special case, no clamp.",
            "        else:",
            "            ret = bytes_clamp(ir_node, t.m)",
            "",
            "    elif isinstance(t, (AddressT, InterfaceT)):",
            "        ret = int_clamp(ir_node, 160)",
            "    elif t in (BoolT(),):",
            "        ret = int_clamp(ir_node, 1)",
            "    else:  # pragma: no cover",
            "        raise CompilerPanic(f\"{t} passed to clamp_basetype\")",
            "",
            "    return IRnode.from_list(ret, typ=ir_node.typ, error_msg=f\"validate {t}\")",
            "",
            "",
            "def int_clamp(ir_node, bits, signed=False):",
            "    \"\"\"Generalized clamper for integer types. Takes the number of bits,",
            "    whether it's signed, and returns an IR node which checks it is",
            "    in bounds. (Consumers should use clamp_basetype instead which uses",
            "    type-based dispatch and is a little safer.)",
            "    \"\"\"",
            "    if bits >= 256:  # pragma: nocover",
            "        raise CompilerPanic(f\"invalid clamp: {bits}>=256 ({ir_node})\")",
            "",
            "    u = \"u\" if not signed else \"\"",
            "    msg = f\"{u}int{bits} bounds check\"",
            "    with ir_node.cache_when_complex(\"val\") as (b, val):",
            "        if signed:",
            "            # example for bits==128:",
            "            # promote_signed_int(val, bits) is the \"canonical\" version of val",
            "            # if val is in bounds, the bits above bit 128 should be equal.",
            "            # (this works for both val >= 0 and val < 0. in the first case,",
            "            # all upper bits should be 0 if val is a valid int128,",
            "            # in the latter case, all upper bits should be 1.)",
            "            assertion = [\"assert\", [\"eq\", val, promote_signed_int(val, bits)]]",
            "        else:",
            "            assertion = [\"assert\", [\"iszero\", shr(bits, val)]]",
            "",
            "        assertion = IRnode.from_list(assertion, error_msg=msg)",
            "",
            "        ret = b.resolve([\"seq\", assertion, val])",
            "",
            "    return IRnode.from_list(ret, annotation=msg)",
            "",
            "",
            "def bytes_clamp(ir_node: IRnode, n_bytes: int) -> IRnode:",
            "    if not (0 < n_bytes <= 32):  # pragma: nocover",
            "        raise CompilerPanic(f\"bad type: bytes{n_bytes}\")",
            "    msg = f\"bytes{n_bytes} bounds check\"",
            "    with ir_node.cache_when_complex(\"val\") as (b, val):",
            "        assertion = IRnode.from_list([\"assert\", [\"iszero\", shl(n_bytes * 8, val)]], error_msg=msg)",
            "        ret = b.resolve([\"seq\", assertion, val])",
            "",
            "    return IRnode.from_list(ret, annotation=msg)",
            "",
            "",
            "# e.g. for int8, promote 255 to -1",
            "def promote_signed_int(x, bits):",
            "    assert bits % 8 == 0",
            "    ret = [\"signextend\", bits // 8 - 1, x]",
            "    return IRnode.from_list(ret, annotation=f\"promote int{bits}\")",
            "",
            "",
            "# general clamp function for all ops and numbers",
            "def clamp(op, arg, bound):",
            "    with IRnode.from_list(arg).cache_when_complex(\"clamp_arg\") as (b1, arg):",
            "        check = IRnode.from_list([\"assert\", [op, arg, bound]], error_msg=f\"clamp {op} {bound}\")",
            "        ret = [\"seq\", check, arg]",
            "        return IRnode.from_list(b1.resolve(ret), typ=arg.typ)",
            "",
            "",
            "def clamp_nonzero(arg):",
            "    # TODO: use clamp(\"ne\", arg, 0) once optimizer rules can handle it",
            "    with IRnode.from_list(arg).cache_when_complex(\"should_nonzero\") as (b1, arg):",
            "        check = IRnode.from_list([\"assert\", arg], error_msg=\"check nonzero\")",
            "        ret = [\"seq\", check, arg]",
            "        return IRnode.from_list(b1.resolve(ret), typ=arg.typ)",
            "",
            "",
            "def clamp_le(arg, hi, signed):",
            "    LE = \"sle\" if signed else \"le\"",
            "    return clamp(LE, arg, hi)",
            "",
            "",
            "def clamp2(lo, arg, hi, signed):",
            "    with IRnode.from_list(arg).cache_when_complex(\"clamp2_arg\") as (b1, arg):",
            "        GE = \"sge\" if signed else \"ge\"",
            "        LE = \"sle\" if signed else \"le\"",
            "        ret = [\"seq\", [\"assert\", [\"and\", [GE, arg, lo], [LE, arg, hi]]], arg]",
            "        return IRnode.from_list(b1.resolve(ret), typ=arg.typ)",
            "",
            "",
            "# make sure we don't overrun the source buffer, checking for overflow:",
            "# valid inputs satisfy:",
            "#   `assert !(start+length > src_len || start+length < start)`",
            "def check_buffer_overflow_ir(start, length, src_len):",
            "    with start.cache_when_complex(\"start\") as (b1, start):",
            "        with add_ofst(start, length).cache_when_complex(\"end\") as (b2, end):",
            "            arithmetic_overflow = [\"lt\", end, start]",
            "            buffer_oob = [\"gt\", end, src_len]",
            "            ok = [\"iszero\", [\"or\", arithmetic_overflow, buffer_oob]]",
            "            return b1.resolve(b2.resolve([\"assert\", ok]))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "329": [
                "copy_bytes"
            ]
        },
        "addLocation": []
    }
}