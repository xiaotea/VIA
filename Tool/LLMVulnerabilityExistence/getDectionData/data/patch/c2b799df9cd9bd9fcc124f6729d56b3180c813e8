{
    "pip/cmdoptions.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "     metavar='url',"
            },
            "1": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     help=\"If a url or path to an html file, then parse for links to archives. If a local path or file:// url that's a directory, then look for archives in the directory listing.\")"
            },
            "2": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+# TODO: Remove after 1.6"
            },
            "4": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 52,
                "PatchRowcode": " use_mirrors = make_option("
            },
            "5": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "     '-M', '--use-mirrors',"
            },
            "6": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "     dest='use_mirrors',"
            },
            "7": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "     action='store_true',"
            },
            "8": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "     default=False,"
            },
            "9": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    help='Use the PyPI mirrors as a fallback in case the main index is down.')"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    help=SUPPRESS_HELP)"
            },
            "11": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 58,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+# TODO: Remove after 1.6"
            },
            "13": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 60,
                "PatchRowcode": " mirrors = make_option("
            },
            "14": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "     '--mirrors',"
            },
            "15": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "     dest='mirrors',"
            },
            "16": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "     metavar='URL',"
            },
            "17": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "     action='append',"
            },
            "18": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     default=[],"
            },
            "19": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    help='Specific mirror URLs to query when --use-mirrors is used.')"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+    help=SUPPRESS_HELP)"
            },
            "21": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 67,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 68,
                "PatchRowcode": " allow_external = make_option("
            },
            "23": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "     \"--allow-external\","
            }
        },
        "frontPatchFile": [
            "\"\"\"shared options and groups\"\"\"",
            "from optparse import make_option, OptionGroup, SUPPRESS_HELP",
            "from pip.locations import build_prefix",
            "",
            "",
            "def make_option_group(group, parser):",
            "    \"\"\"",
            "    Return an OptionGroup object",
            "    group  -- assumed to be dict with 'name' and 'options' keys",
            "    parser -- an optparse Parser",
            "    \"\"\"",
            "    option_group = OptionGroup(parser, group['name'])",
            "    for option in group['options']:",
            "        option_group.add_option(option)",
            "    return option_group",
            "",
            "###########",
            "# options #",
            "###########",
            "",
            "index_url = make_option(",
            "    '-i', '--index-url', '--pypi-url',",
            "    dest='index_url',",
            "    metavar='URL',",
            "    default='https://pypi.python.org/simple/',",
            "    help='Base URL of Python Package Index (default %default).')",
            "",
            "extra_index_url = make_option(",
            "    '--extra-index-url',",
            "    dest='extra_index_urls',",
            "    metavar='URL',",
            "    action='append',",
            "    default=[],",
            "    help='Extra URLs of package indexes to use in addition to --index-url.')",
            "",
            "no_index = make_option(",
            "    '--no-index',",
            "    dest='no_index',",
            "    action='store_true',",
            "    default=False,",
            "    help='Ignore package index (only looking at --find-links URLs instead).')",
            "",
            "find_links =  make_option(",
            "    '-f', '--find-links',",
            "    dest='find_links',",
            "    action='append',",
            "    default=[],",
            "    metavar='url',",
            "    help=\"If a url or path to an html file, then parse for links to archives. If a local path or file:// url that's a directory, then look for archives in the directory listing.\")",
            "",
            "use_mirrors = make_option(",
            "    '-M', '--use-mirrors',",
            "    dest='use_mirrors',",
            "    action='store_true',",
            "    default=False,",
            "    help='Use the PyPI mirrors as a fallback in case the main index is down.')",
            "",
            "mirrors = make_option(",
            "    '--mirrors',",
            "    dest='mirrors',",
            "    metavar='URL',",
            "    action='append',",
            "    default=[],",
            "    help='Specific mirror URLs to query when --use-mirrors is used.')",
            "",
            "allow_external = make_option(",
            "    \"--allow-external\",",
            "    dest=\"allow_external\",",
            "    action=\"append\",",
            "    default=[],",
            "    metavar=\"PACKAGE\",",
            "    help=\"Allow the installation of externally hosted files\",",
            ")",
            "",
            "allow_all_external = make_option(",
            "    \"--allow-all-external\",",
            "    dest=\"allow_all_external\",",
            "    action=\"store_true\",",
            "    default=False,",
            "    help=\"Allow the installation of all externally hosted files\",",
            ")",
            "",
            "no_allow_external = make_option(",
            "    \"--no-allow-external\",",
            "    dest=\"allow_all_external\",",
            "    action=\"store_false\",",
            "    default=False,",
            "    help=SUPPRESS_HELP,",
            ")",
            "",
            "allow_unsafe = make_option(",
            "    \"--allow-insecure\",",
            "    dest=\"allow_insecure\",",
            "    action=\"append\",",
            "    default=[],",
            "    metavar=\"PACKAGE\",",
            "    help=\"Allow the installation of insecure and unverifiable files\",",
            ")",
            "",
            "no_allow_unsafe = make_option(",
            "    \"--no-allow-insecure\",",
            "    dest=\"allow_all_insecure\",",
            "    action=\"store_false\",",
            "    default=False,",
            "    help=SUPPRESS_HELP",
            ")",
            "",
            "requirements = make_option(",
            "    '-r', '--requirement',",
            "    dest='requirements',",
            "    action='append',",
            "    default=[],",
            "    metavar='file',",
            "    help='Install from the given requirements file. '",
            "    'This option can be used multiple times.')",
            "",
            "use_wheel = make_option(",
            "    '--use-wheel',",
            "    dest='use_wheel',",
            "    action='store_true',",
            "    help='Find and prefer wheel archives when searching indexes and find-links locations. Default to accepting source archives.')",
            "",
            "download_cache = make_option(",
            "    '--download-cache',",
            "    dest='download_cache',",
            "    metavar='dir',",
            "    default=None,",
            "    help='Cache downloaded packages in <dir>.')",
            "",
            "no_deps = make_option(",
            "    '--no-deps', '--no-dependencies',",
            "    dest='ignore_dependencies',",
            "    action='store_true',",
            "    default=False,",
            "    help=\"Don't install package dependencies.\")",
            "",
            "build_dir = make_option(",
            "    '-b', '--build', '--build-dir', '--build-directory',",
            "    dest='build_dir',",
            "    metavar='dir',",
            "    default=build_prefix,",
            "    help='Directory to unpack packages into and build in. '",
            "    'The default in a virtualenv is \"<venv path>/build\". '",
            "    'The default for global installs is \"<OS temp dir>/pip-build-<username>\".')",
            "",
            "install_options = make_option(",
            "    '--install-option',",
            "    dest='install_options',",
            "    action='append',",
            "    metavar='options',",
            "    help=\"Extra arguments to be supplied to the setup.py install \"",
            "    \"command (use like --install-option=\\\"--install-scripts=/usr/local/bin\\\"). \"",
            "    \"Use multiple --install-option options to pass multiple options to setup.py install. \"",
            "    \"If you are using an option with a directory path, be sure to use absolute path.\")",
            "",
            "global_options = make_option(",
            "    '--global-option',",
            "    dest='global_options',",
            "    action='append',",
            "    metavar='options',",
            "    help=\"Extra global options to be supplied to the setup.py \"",
            "    \"call before the install command.\")",
            "",
            "no_clean = make_option(",
            "    '--no-clean',",
            "    action='store_true',",
            "    default=False,",
            "    help=\"Don't clean up build directories.\")",
            "",
            "",
            "##########",
            "# groups #",
            "##########",
            "",
            "index_group = {",
            "    'name': 'Package Index Options',",
            "    'options': [",
            "        index_url,",
            "        extra_index_url,",
            "        no_index,",
            "        find_links,",
            "        use_mirrors,",
            "        mirrors,",
            "        allow_external,",
            "        allow_all_external,",
            "        no_allow_external,",
            "        allow_unsafe,",
            "        no_allow_unsafe,",
            "        ]",
            "    }"
        ],
        "afterPatchFile": [
            "\"\"\"shared options and groups\"\"\"",
            "from optparse import make_option, OptionGroup, SUPPRESS_HELP",
            "from pip.locations import build_prefix",
            "",
            "",
            "def make_option_group(group, parser):",
            "    \"\"\"",
            "    Return an OptionGroup object",
            "    group  -- assumed to be dict with 'name' and 'options' keys",
            "    parser -- an optparse Parser",
            "    \"\"\"",
            "    option_group = OptionGroup(parser, group['name'])",
            "    for option in group['options']:",
            "        option_group.add_option(option)",
            "    return option_group",
            "",
            "###########",
            "# options #",
            "###########",
            "",
            "index_url = make_option(",
            "    '-i', '--index-url', '--pypi-url',",
            "    dest='index_url',",
            "    metavar='URL',",
            "    default='https://pypi.python.org/simple/',",
            "    help='Base URL of Python Package Index (default %default).')",
            "",
            "extra_index_url = make_option(",
            "    '--extra-index-url',",
            "    dest='extra_index_urls',",
            "    metavar='URL',",
            "    action='append',",
            "    default=[],",
            "    help='Extra URLs of package indexes to use in addition to --index-url.')",
            "",
            "no_index = make_option(",
            "    '--no-index',",
            "    dest='no_index',",
            "    action='store_true',",
            "    default=False,",
            "    help='Ignore package index (only looking at --find-links URLs instead).')",
            "",
            "find_links =  make_option(",
            "    '-f', '--find-links',",
            "    dest='find_links',",
            "    action='append',",
            "    default=[],",
            "    metavar='url',",
            "    help=\"If a url or path to an html file, then parse for links to archives. If a local path or file:// url that's a directory, then look for archives in the directory listing.\")",
            "",
            "# TODO: Remove after 1.6",
            "use_mirrors = make_option(",
            "    '-M', '--use-mirrors',",
            "    dest='use_mirrors',",
            "    action='store_true',",
            "    default=False,",
            "    help=SUPPRESS_HELP)",
            "",
            "# TODO: Remove after 1.6",
            "mirrors = make_option(",
            "    '--mirrors',",
            "    dest='mirrors',",
            "    metavar='URL',",
            "    action='append',",
            "    default=[],",
            "    help=SUPPRESS_HELP)",
            "",
            "allow_external = make_option(",
            "    \"--allow-external\",",
            "    dest=\"allow_external\",",
            "    action=\"append\",",
            "    default=[],",
            "    metavar=\"PACKAGE\",",
            "    help=\"Allow the installation of externally hosted files\",",
            ")",
            "",
            "allow_all_external = make_option(",
            "    \"--allow-all-external\",",
            "    dest=\"allow_all_external\",",
            "    action=\"store_true\",",
            "    default=False,",
            "    help=\"Allow the installation of all externally hosted files\",",
            ")",
            "",
            "no_allow_external = make_option(",
            "    \"--no-allow-external\",",
            "    dest=\"allow_all_external\",",
            "    action=\"store_false\",",
            "    default=False,",
            "    help=SUPPRESS_HELP,",
            ")",
            "",
            "allow_unsafe = make_option(",
            "    \"--allow-insecure\",",
            "    dest=\"allow_insecure\",",
            "    action=\"append\",",
            "    default=[],",
            "    metavar=\"PACKAGE\",",
            "    help=\"Allow the installation of insecure and unverifiable files\",",
            ")",
            "",
            "no_allow_unsafe = make_option(",
            "    \"--no-allow-insecure\",",
            "    dest=\"allow_all_insecure\",",
            "    action=\"store_false\",",
            "    default=False,",
            "    help=SUPPRESS_HELP",
            ")",
            "",
            "requirements = make_option(",
            "    '-r', '--requirement',",
            "    dest='requirements',",
            "    action='append',",
            "    default=[],",
            "    metavar='file',",
            "    help='Install from the given requirements file. '",
            "    'This option can be used multiple times.')",
            "",
            "use_wheel = make_option(",
            "    '--use-wheel',",
            "    dest='use_wheel',",
            "    action='store_true',",
            "    help='Find and prefer wheel archives when searching indexes and find-links locations. Default to accepting source archives.')",
            "",
            "download_cache = make_option(",
            "    '--download-cache',",
            "    dest='download_cache',",
            "    metavar='dir',",
            "    default=None,",
            "    help='Cache downloaded packages in <dir>.')",
            "",
            "no_deps = make_option(",
            "    '--no-deps', '--no-dependencies',",
            "    dest='ignore_dependencies',",
            "    action='store_true',",
            "    default=False,",
            "    help=\"Don't install package dependencies.\")",
            "",
            "build_dir = make_option(",
            "    '-b', '--build', '--build-dir', '--build-directory',",
            "    dest='build_dir',",
            "    metavar='dir',",
            "    default=build_prefix,",
            "    help='Directory to unpack packages into and build in. '",
            "    'The default in a virtualenv is \"<venv path>/build\". '",
            "    'The default for global installs is \"<OS temp dir>/pip-build-<username>\".')",
            "",
            "install_options = make_option(",
            "    '--install-option',",
            "    dest='install_options',",
            "    action='append',",
            "    metavar='options',",
            "    help=\"Extra arguments to be supplied to the setup.py install \"",
            "    \"command (use like --install-option=\\\"--install-scripts=/usr/local/bin\\\"). \"",
            "    \"Use multiple --install-option options to pass multiple options to setup.py install. \"",
            "    \"If you are using an option with a directory path, be sure to use absolute path.\")",
            "",
            "global_options = make_option(",
            "    '--global-option',",
            "    dest='global_options',",
            "    action='append',",
            "    metavar='options',",
            "    help=\"Extra global options to be supplied to the setup.py \"",
            "    \"call before the install command.\")",
            "",
            "no_clean = make_option(",
            "    '--no-clean',",
            "    action='store_true',",
            "    default=False,",
            "    help=\"Don't clean up build directories.\")",
            "",
            "",
            "##########",
            "# groups #",
            "##########",
            "",
            "index_group = {",
            "    'name': 'Package Index Options',",
            "    'options': [",
            "        index_url,",
            "        extra_index_url,",
            "        no_index,",
            "        find_links,",
            "        use_mirrors,",
            "        mirrors,",
            "        allow_external,",
            "        allow_all_external,",
            "        no_allow_external,",
            "        allow_unsafe,",
            "        no_allow_unsafe,",
            "        ]",
            "    }"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "56": [],
            "64": []
        },
        "addLocation": []
    },
    "pip/commands/install.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 159,
                "PatchRowcode": "         \"\"\""
            },
            "1": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": 160,
                "PatchRowcode": "         return PackageFinder(find_links=options.find_links,"
            },
            "2": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": 161,
                "PatchRowcode": "                              index_urls=index_urls,"
            },
            "3": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                             use_mirrors=options.use_mirrors,"
            },
            "4": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                             mirrors=options.mirrors,"
            },
            "5": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 162,
                "PatchRowcode": "                              use_wheel=options.use_wheel,"
            },
            "6": {
                "beforePatchRowNumber": 165,
                "afterPatchRowNumber": 163,
                "PatchRowcode": "                              allow_external=options.allow_external,"
            },
            "7": {
                "beforePatchRowNumber": 166,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "                              allow_insecure=options.allow_insecure,"
            },
            "8": {
                "beforePatchRowNumber": 195,
                "afterPatchRowNumber": 193,
                "PatchRowcode": "             logger.notify('Ignoring indexes: %s' % ','.join(index_urls))"
            },
            "9": {
                "beforePatchRowNumber": 196,
                "afterPatchRowNumber": 194,
                "PatchRowcode": "             index_urls = []"
            },
            "10": {
                "beforePatchRowNumber": 197,
                "afterPatchRowNumber": 195,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 196,
                "PatchRowcode": "+        if options.use_mirrors:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 197,
                "PatchRowcode": "+            logger.warn(\"--use-mirrors has been deprecated and will be removed\""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+                        \" in the future. Explicit uses of --index-url and/or \""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 199,
                "PatchRowcode": "+                        \"--extra-index-url is suggested.\")"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 200,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 201,
                "PatchRowcode": "+        if options.mirrors:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 202,
                "PatchRowcode": "+            logger.warn(\"--mirrors has been deprecated and will be removed in \""
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+                        \" the future. Explicit uses of --index-url and/or \""
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 204,
                "PatchRowcode": "+                        \"--extra-index-url is suggested.\")"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 205,
                "PatchRowcode": "+            index_urls += options.mirrors"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 206,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "         finder = self._build_package_finder(options, index_urls)"
            },
            "23": {
                "beforePatchRowNumber": 199,
                "afterPatchRowNumber": 208,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 200,
                "afterPatchRowNumber": 209,
                "PatchRowcode": "         requirement_set = RequirementSet("
            }
        },
        "frontPatchFile": [
            "import os",
            "import sys",
            "import tempfile",
            "import shutil",
            "from pip.req import InstallRequirement, RequirementSet, parse_requirements",
            "from pip.log import logger",
            "from pip.locations import src_prefix, virtualenv_no_global, distutils_scheme",
            "from pip.basecommand import Command",
            "from pip.index import PackageFinder",
            "from pip.exceptions import InstallationError, CommandError",
            "from pip import cmdoptions",
            "",
            "",
            "class InstallCommand(Command):",
            "    \"\"\"",
            "    Install packages from:",
            "",
            "    - PyPI (and other indexes) using requirement specifiers.",
            "    - VCS project urls.",
            "    - Local project directories.",
            "    - Local or remote source archives.",
            "",
            "    pip also supports installing from \"requirements files\", which provide",
            "    an easy way to specify a whole environment to be installed.",
            "",
            "    See http://www.pip-installer.org for details on VCS url formats and",
            "    requirements files.",
            "    \"\"\"",
            "    name = 'install'",
            "",
            "    usage = \"\"\"",
            "      %prog [options] <requirement specifier> ...",
            "      %prog [options] -r <requirements file> ...",
            "      %prog [options] [-e] <vcs project url> ...",
            "      %prog [options] [-e] <local project path> ...",
            "      %prog [options] <archive url/path> ...\"\"\"",
            "",
            "    summary = 'Install packages.'",
            "    bundle = False",
            "",
            "    def __init__(self, *args, **kw):",
            "        super(InstallCommand, self).__init__(*args, **kw)",
            "",
            "        cmd_opts = self.cmd_opts",
            "",
            "        cmd_opts.add_option(",
            "            '-e', '--editable',",
            "            dest='editables',",
            "            action='append',",
            "            default=[],",
            "            metavar='path/url',",
            "            help='Install a project in editable mode (i.e. setuptools \"develop mode\") from a local project path or a VCS url.')",
            "",
            "        cmd_opts.add_option(cmdoptions.requirements)",
            "        cmd_opts.add_option(cmdoptions.build_dir)",
            "",
            "        cmd_opts.add_option(",
            "            '-t', '--target',",
            "            dest='target_dir',",
            "            metavar='dir',",
            "            default=None,",
            "            help='Install packages into <dir>.')",
            "",
            "        cmd_opts.add_option(",
            "            '-d', '--download', '--download-dir', '--download-directory',",
            "            dest='download_dir',",
            "            metavar='dir',",
            "            default=None,",
            "            help=\"Download packages into <dir> instead of installing them, regardless of what's already installed.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.download_cache)",
            "",
            "        cmd_opts.add_option(",
            "            '--src', '--source', '--source-dir', '--source-directory',",
            "            dest='src_dir',",
            "            metavar='dir',",
            "            default=src_prefix,",
            "            help='Directory to check out editable projects into. '",
            "            'The default in a virtualenv is \"<venv path>/src\". '",
            "            'The default for global installs is \"<current dir>/src\".')",
            "",
            "        cmd_opts.add_option(",
            "            '-U', '--upgrade',",
            "            dest='upgrade',",
            "            action='store_true',",
            "            help='Upgrade all packages to the newest available version. '",
            "            'This process is recursive regardless of whether a dependency is already satisfied.')",
            "",
            "        cmd_opts.add_option(",
            "            '--force-reinstall',",
            "            dest='force_reinstall',",
            "            action='store_true',",
            "            help='When upgrading, reinstall all packages even if they are '",
            "                 'already up-to-date.')",
            "",
            "        cmd_opts.add_option(",
            "            '-I', '--ignore-installed',",
            "            dest='ignore_installed',",
            "            action='store_true',",
            "            help='Ignore the installed packages (reinstalling instead).')",
            "",
            "        cmd_opts.add_option(cmdoptions.no_deps)",
            "",
            "        cmd_opts.add_option(",
            "            '--no-install',",
            "            dest='no_install',",
            "            action='store_true',",
            "            help=\"Download and unpack all packages, but don't actually install them.\")",
            "",
            "        cmd_opts.add_option(",
            "            '--no-download',",
            "            dest='no_download',",
            "            action=\"store_true\",",
            "            help=\"Don't download any packages, just install the ones already downloaded \"",
            "            \"(completes an install run with --no-install).\")",
            "",
            "        cmd_opts.add_option(cmdoptions.install_options)",
            "        cmd_opts.add_option(cmdoptions.global_options)",
            "",
            "        cmd_opts.add_option(",
            "            '--user',",
            "            dest='use_user_site',",
            "            action='store_true',",
            "            help='Install using the user scheme.')",
            "",
            "        cmd_opts.add_option(",
            "            '--egg',",
            "            dest='as_egg',",
            "            action='store_true',",
            "            help=\"Install as self contained egg file, like easy_install does.\")",
            "",
            "        cmd_opts.add_option(",
            "            '--root',",
            "            dest='root_path',",
            "            metavar='dir',",
            "            default=None,",
            "            help=\"Install everything relative to this alternate root directory.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.use_wheel)",
            "",
            "        cmd_opts.add_option(",
            "            '--pre',",
            "            action='store_true',",
            "            default=False,",
            "            help=\"Include pre-release and development versions. By default, pip only finds stable versions.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.no_clean)",
            "",
            "        index_opts = cmdoptions.make_option_group(cmdoptions.index_group, self.parser)",
            "",
            "        self.parser.insert_option_group(0, index_opts)",
            "        self.parser.insert_option_group(0, cmd_opts)",
            "",
            "    def _build_package_finder(self, options, index_urls):",
            "        \"\"\"",
            "        Create a package finder appropriate to this install command.",
            "        This method is meant to be overridden by subclasses, not",
            "        called directly.",
            "        \"\"\"",
            "        return PackageFinder(find_links=options.find_links,",
            "                             index_urls=index_urls,",
            "                             use_mirrors=options.use_mirrors,",
            "                             mirrors=options.mirrors,",
            "                             use_wheel=options.use_wheel,",
            "                             allow_external=options.allow_external,",
            "                             allow_insecure=options.allow_insecure,",
            "                             allow_all_external=options.allow_all_external,",
            "                             allow_all_prereleases=options.pre,",
            "                            )",
            "",
            "    def run(self, options, args):",
            "        if options.download_dir:",
            "            options.no_install = True",
            "            options.ignore_installed = True",
            "        options.build_dir = os.path.abspath(options.build_dir)",
            "        options.src_dir = os.path.abspath(options.src_dir)",
            "        install_options = options.install_options or []",
            "        if options.use_user_site:",
            "            if virtualenv_no_global():",
            "                raise InstallationError(\"Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\")",
            "            install_options.append('--user')",
            "",
            "        temp_target_dir = None",
            "        if options.target_dir:",
            "            options.ignore_installed = True",
            "            temp_target_dir = tempfile.mkdtemp()",
            "            options.target_dir = os.path.abspath(options.target_dir)",
            "            if os.path.exists(options.target_dir) and not os.path.isdir(options.target_dir):",
            "                raise CommandError(\"Target path exists but is not a directory, will not continue.\")",
            "            install_options.append('--home=' + temp_target_dir)",
            "",
            "        global_options = options.global_options or []",
            "        index_urls = [options.index_url] + options.extra_index_urls",
            "        if options.no_index:",
            "            logger.notify('Ignoring indexes: %s' % ','.join(index_urls))",
            "            index_urls = []",
            "",
            "        finder = self._build_package_finder(options, index_urls)",
            "",
            "        requirement_set = RequirementSet(",
            "            build_dir=options.build_dir,",
            "            src_dir=options.src_dir,",
            "            download_dir=options.download_dir,",
            "            download_cache=options.download_cache,",
            "            upgrade=options.upgrade,",
            "            as_egg=options.as_egg,",
            "            ignore_installed=options.ignore_installed,",
            "            ignore_dependencies=options.ignore_dependencies,",
            "            force_reinstall=options.force_reinstall,",
            "            use_user_site=options.use_user_site,",
            "            target_dir=temp_target_dir)",
            "        for name in args:",
            "            requirement_set.add_requirement(",
            "                InstallRequirement.from_line(name, None))",
            "        for name in options.editables:",
            "            requirement_set.add_requirement(",
            "                InstallRequirement.from_editable(name, default_vcs=options.default_vcs))",
            "        for filename in options.requirements:",
            "            for req in parse_requirements(filename, finder=finder, options=options):",
            "                requirement_set.add_requirement(req)",
            "        if not requirement_set.has_requirements:",
            "            opts = {'name': self.name}",
            "            if options.find_links:",
            "                msg = ('You must give at least one requirement to %(name)s '",
            "                       '(maybe you meant \"pip %(name)s %(links)s\"?)' %",
            "                       dict(opts, links=' '.join(options.find_links)))",
            "            else:",
            "                msg = ('You must give at least one requirement '",
            "                       'to %(name)s (see \"pip help %(name)s\")' % opts)",
            "            logger.warn(msg)",
            "            return",
            "",
            "        try:",
            "            if not options.no_download:",
            "                requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)",
            "            else:",
            "                requirement_set.locate_files()",
            "",
            "            if not options.no_install and not self.bundle:",
            "                requirement_set.install(install_options, global_options, root=options.root_path)",
            "                installed = ' '.join([req.name for req in",
            "                                      requirement_set.successfully_installed])",
            "                if installed:",
            "                    logger.notify('Successfully installed %s' % installed)",
            "            elif not self.bundle:",
            "                downloaded = ' '.join([req.name for req in",
            "                                       requirement_set.successfully_downloaded])",
            "                if downloaded:",
            "                    logger.notify('Successfully downloaded %s' % downloaded)",
            "            elif self.bundle:",
            "                requirement_set.create_bundle(self.bundle_filename)",
            "                logger.notify('Created bundle in %s' % self.bundle_filename)",
            "        finally:",
            "            # Clean up",
            "            if (not options.no_clean) and ((not options.no_install) or options.download_dir):",
            "                requirement_set.cleanup_files(bundle=self.bundle)",
            "",
            "        if options.target_dir:",
            "            if not os.path.exists(options.target_dir):",
            "                os.makedirs(options.target_dir)",
            "            lib_dir = distutils_scheme('', home=temp_target_dir)['purelib']",
            "            for item in os.listdir(lib_dir):",
            "                shutil.move(",
            "                    os.path.join(lib_dir, item),",
            "                    os.path.join(options.target_dir, item)",
            "                    )",
            "            shutil.rmtree(temp_target_dir)",
            "        return requirement_set"
        ],
        "afterPatchFile": [
            "import os",
            "import sys",
            "import tempfile",
            "import shutil",
            "from pip.req import InstallRequirement, RequirementSet, parse_requirements",
            "from pip.log import logger",
            "from pip.locations import src_prefix, virtualenv_no_global, distutils_scheme",
            "from pip.basecommand import Command",
            "from pip.index import PackageFinder",
            "from pip.exceptions import InstallationError, CommandError",
            "from pip import cmdoptions",
            "",
            "",
            "class InstallCommand(Command):",
            "    \"\"\"",
            "    Install packages from:",
            "",
            "    - PyPI (and other indexes) using requirement specifiers.",
            "    - VCS project urls.",
            "    - Local project directories.",
            "    - Local or remote source archives.",
            "",
            "    pip also supports installing from \"requirements files\", which provide",
            "    an easy way to specify a whole environment to be installed.",
            "",
            "    See http://www.pip-installer.org for details on VCS url formats and",
            "    requirements files.",
            "    \"\"\"",
            "    name = 'install'",
            "",
            "    usage = \"\"\"",
            "      %prog [options] <requirement specifier> ...",
            "      %prog [options] -r <requirements file> ...",
            "      %prog [options] [-e] <vcs project url> ...",
            "      %prog [options] [-e] <local project path> ...",
            "      %prog [options] <archive url/path> ...\"\"\"",
            "",
            "    summary = 'Install packages.'",
            "    bundle = False",
            "",
            "    def __init__(self, *args, **kw):",
            "        super(InstallCommand, self).__init__(*args, **kw)",
            "",
            "        cmd_opts = self.cmd_opts",
            "",
            "        cmd_opts.add_option(",
            "            '-e', '--editable',",
            "            dest='editables',",
            "            action='append',",
            "            default=[],",
            "            metavar='path/url',",
            "            help='Install a project in editable mode (i.e. setuptools \"develop mode\") from a local project path or a VCS url.')",
            "",
            "        cmd_opts.add_option(cmdoptions.requirements)",
            "        cmd_opts.add_option(cmdoptions.build_dir)",
            "",
            "        cmd_opts.add_option(",
            "            '-t', '--target',",
            "            dest='target_dir',",
            "            metavar='dir',",
            "            default=None,",
            "            help='Install packages into <dir>.')",
            "",
            "        cmd_opts.add_option(",
            "            '-d', '--download', '--download-dir', '--download-directory',",
            "            dest='download_dir',",
            "            metavar='dir',",
            "            default=None,",
            "            help=\"Download packages into <dir> instead of installing them, regardless of what's already installed.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.download_cache)",
            "",
            "        cmd_opts.add_option(",
            "            '--src', '--source', '--source-dir', '--source-directory',",
            "            dest='src_dir',",
            "            metavar='dir',",
            "            default=src_prefix,",
            "            help='Directory to check out editable projects into. '",
            "            'The default in a virtualenv is \"<venv path>/src\". '",
            "            'The default for global installs is \"<current dir>/src\".')",
            "",
            "        cmd_opts.add_option(",
            "            '-U', '--upgrade',",
            "            dest='upgrade',",
            "            action='store_true',",
            "            help='Upgrade all packages to the newest available version. '",
            "            'This process is recursive regardless of whether a dependency is already satisfied.')",
            "",
            "        cmd_opts.add_option(",
            "            '--force-reinstall',",
            "            dest='force_reinstall',",
            "            action='store_true',",
            "            help='When upgrading, reinstall all packages even if they are '",
            "                 'already up-to-date.')",
            "",
            "        cmd_opts.add_option(",
            "            '-I', '--ignore-installed',",
            "            dest='ignore_installed',",
            "            action='store_true',",
            "            help='Ignore the installed packages (reinstalling instead).')",
            "",
            "        cmd_opts.add_option(cmdoptions.no_deps)",
            "",
            "        cmd_opts.add_option(",
            "            '--no-install',",
            "            dest='no_install',",
            "            action='store_true',",
            "            help=\"Download and unpack all packages, but don't actually install them.\")",
            "",
            "        cmd_opts.add_option(",
            "            '--no-download',",
            "            dest='no_download',",
            "            action=\"store_true\",",
            "            help=\"Don't download any packages, just install the ones already downloaded \"",
            "            \"(completes an install run with --no-install).\")",
            "",
            "        cmd_opts.add_option(cmdoptions.install_options)",
            "        cmd_opts.add_option(cmdoptions.global_options)",
            "",
            "        cmd_opts.add_option(",
            "            '--user',",
            "            dest='use_user_site',",
            "            action='store_true',",
            "            help='Install using the user scheme.')",
            "",
            "        cmd_opts.add_option(",
            "            '--egg',",
            "            dest='as_egg',",
            "            action='store_true',",
            "            help=\"Install as self contained egg file, like easy_install does.\")",
            "",
            "        cmd_opts.add_option(",
            "            '--root',",
            "            dest='root_path',",
            "            metavar='dir',",
            "            default=None,",
            "            help=\"Install everything relative to this alternate root directory.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.use_wheel)",
            "",
            "        cmd_opts.add_option(",
            "            '--pre',",
            "            action='store_true',",
            "            default=False,",
            "            help=\"Include pre-release and development versions. By default, pip only finds stable versions.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.no_clean)",
            "",
            "        index_opts = cmdoptions.make_option_group(cmdoptions.index_group, self.parser)",
            "",
            "        self.parser.insert_option_group(0, index_opts)",
            "        self.parser.insert_option_group(0, cmd_opts)",
            "",
            "    def _build_package_finder(self, options, index_urls):",
            "        \"\"\"",
            "        Create a package finder appropriate to this install command.",
            "        This method is meant to be overridden by subclasses, not",
            "        called directly.",
            "        \"\"\"",
            "        return PackageFinder(find_links=options.find_links,",
            "                             index_urls=index_urls,",
            "                             use_wheel=options.use_wheel,",
            "                             allow_external=options.allow_external,",
            "                             allow_insecure=options.allow_insecure,",
            "                             allow_all_external=options.allow_all_external,",
            "                             allow_all_prereleases=options.pre,",
            "                            )",
            "",
            "    def run(self, options, args):",
            "        if options.download_dir:",
            "            options.no_install = True",
            "            options.ignore_installed = True",
            "        options.build_dir = os.path.abspath(options.build_dir)",
            "        options.src_dir = os.path.abspath(options.src_dir)",
            "        install_options = options.install_options or []",
            "        if options.use_user_site:",
            "            if virtualenv_no_global():",
            "                raise InstallationError(\"Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\")",
            "            install_options.append('--user')",
            "",
            "        temp_target_dir = None",
            "        if options.target_dir:",
            "            options.ignore_installed = True",
            "            temp_target_dir = tempfile.mkdtemp()",
            "            options.target_dir = os.path.abspath(options.target_dir)",
            "            if os.path.exists(options.target_dir) and not os.path.isdir(options.target_dir):",
            "                raise CommandError(\"Target path exists but is not a directory, will not continue.\")",
            "            install_options.append('--home=' + temp_target_dir)",
            "",
            "        global_options = options.global_options or []",
            "        index_urls = [options.index_url] + options.extra_index_urls",
            "        if options.no_index:",
            "            logger.notify('Ignoring indexes: %s' % ','.join(index_urls))",
            "            index_urls = []",
            "",
            "        if options.use_mirrors:",
            "            logger.warn(\"--use-mirrors has been deprecated and will be removed\"",
            "                        \" in the future. Explicit uses of --index-url and/or \"",
            "                        \"--extra-index-url is suggested.\")",
            "",
            "        if options.mirrors:",
            "            logger.warn(\"--mirrors has been deprecated and will be removed in \"",
            "                        \" the future. Explicit uses of --index-url and/or \"",
            "                        \"--extra-index-url is suggested.\")",
            "            index_urls += options.mirrors",
            "",
            "        finder = self._build_package_finder(options, index_urls)",
            "",
            "        requirement_set = RequirementSet(",
            "            build_dir=options.build_dir,",
            "            src_dir=options.src_dir,",
            "            download_dir=options.download_dir,",
            "            download_cache=options.download_cache,",
            "            upgrade=options.upgrade,",
            "            as_egg=options.as_egg,",
            "            ignore_installed=options.ignore_installed,",
            "            ignore_dependencies=options.ignore_dependencies,",
            "            force_reinstall=options.force_reinstall,",
            "            use_user_site=options.use_user_site,",
            "            target_dir=temp_target_dir)",
            "        for name in args:",
            "            requirement_set.add_requirement(",
            "                InstallRequirement.from_line(name, None))",
            "        for name in options.editables:",
            "            requirement_set.add_requirement(",
            "                InstallRequirement.from_editable(name, default_vcs=options.default_vcs))",
            "        for filename in options.requirements:",
            "            for req in parse_requirements(filename, finder=finder, options=options):",
            "                requirement_set.add_requirement(req)",
            "        if not requirement_set.has_requirements:",
            "            opts = {'name': self.name}",
            "            if options.find_links:",
            "                msg = ('You must give at least one requirement to %(name)s '",
            "                       '(maybe you meant \"pip %(name)s %(links)s\"?)' %",
            "                       dict(opts, links=' '.join(options.find_links)))",
            "            else:",
            "                msg = ('You must give at least one requirement '",
            "                       'to %(name)s (see \"pip help %(name)s\")' % opts)",
            "            logger.warn(msg)",
            "            return",
            "",
            "        try:",
            "            if not options.no_download:",
            "                requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)",
            "            else:",
            "                requirement_set.locate_files()",
            "",
            "            if not options.no_install and not self.bundle:",
            "                requirement_set.install(install_options, global_options, root=options.root_path)",
            "                installed = ' '.join([req.name for req in",
            "                                      requirement_set.successfully_installed])",
            "                if installed:",
            "                    logger.notify('Successfully installed %s' % installed)",
            "            elif not self.bundle:",
            "                downloaded = ' '.join([req.name for req in",
            "                                       requirement_set.successfully_downloaded])",
            "                if downloaded:",
            "                    logger.notify('Successfully downloaded %s' % downloaded)",
            "            elif self.bundle:",
            "                requirement_set.create_bundle(self.bundle_filename)",
            "                logger.notify('Created bundle in %s' % self.bundle_filename)",
            "        finally:",
            "            # Clean up",
            "            if (not options.no_clean) and ((not options.no_install) or options.download_dir):",
            "                requirement_set.cleanup_files(bundle=self.bundle)",
            "",
            "        if options.target_dir:",
            "            if not os.path.exists(options.target_dir):",
            "                os.makedirs(options.target_dir)",
            "            lib_dir = distutils_scheme('', home=temp_target_dir)['purelib']",
            "            for item in os.listdir(lib_dir):",
            "                shutil.move(",
            "                    os.path.join(lib_dir, item),",
            "                    os.path.join(options.target_dir, item)",
            "                    )",
            "            shutil.rmtree(temp_target_dir)",
            "        return requirement_set"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "162": [
                "InstallCommand",
                "_build_package_finder"
            ],
            "163": [
                "InstallCommand",
                "_build_package_finder"
            ]
        },
        "addLocation": []
    },
    "pip/commands/list.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "         \"\"\""
            },
            "1": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "         return PackageFinder(find_links=options.find_links,"
            },
            "2": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "                              index_urls=index_urls,"
            },
            "3": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                             use_mirrors=options.use_mirrors,"
            },
            "4": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                             mirrors=options.mirrors,"
            },
            "5": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "                              allow_external=options.allow_external,"
            },
            "6": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 64,
                "PatchRowcode": "                              allow_insecure=options.allow_insecure,"
            },
            "7": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "                              allow_all_external=options.allow_all_external,"
            },
            "8": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "             logger.notify('Ignoring indexes: %s' % ','.join(index_urls))"
            },
            "9": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "             index_urls = []"
            },
            "10": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 90,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+        if options.use_mirrors:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+            logger.warn(\"--use-mirrors has been deprecated and will be removed\""
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+                        \" in the future. Explicit uses of --index-url and/or \""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+                        \"--extra-index-url is suggested.\")"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+        if options.mirrors:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+            logger.warn(\"--mirrors has been deprecated and will be removed in \""
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+                        \" the future. Explicit uses of --index-url and/or \""
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                        \"--extra-index-url is suggested.\")"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+            index_urls += options.mirrors"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "         dependency_links = []"
            },
            "23": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "         for dist in get_installed_distributions(local_only=options.local, skip=self.skip):"
            },
            "24": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "             if dist.has_metadata('dependency_links.txt'):"
            }
        },
        "frontPatchFile": [
            "from pip.basecommand import Command",
            "from pip.exceptions import DistributionNotFound, BestVersionAlreadyInstalled",
            "from pip.index import PackageFinder",
            "from pip.log import logger",
            "from pip.req import InstallRequirement",
            "from pip.util import get_installed_distributions, dist_is_editable",
            "from pip.cmdoptions import make_option_group, index_group",
            "",
            "",
            "class ListCommand(Command):",
            "    \"\"\"List installed packages, including editables.\"\"\"",
            "    name = 'list'",
            "    usage = \"\"\"",
            "      %prog [options]\"\"\"",
            "    summary = 'List installed packages.'",
            "",
            "    # distributions to skip (python itself is reported by pkg_resources.working_set)",
            "    skip = ['python']",
            "",
            "    def __init__(self, *args, **kw):",
            "        super(ListCommand, self).__init__(*args, **kw)",
            "",
            "        cmd_opts = self.cmd_opts",
            "",
            "        cmd_opts.add_option(",
            "            '-o', '--outdated',",
            "            action='store_true',",
            "            default=False,",
            "            help='List outdated packages (excluding editables)')",
            "        cmd_opts.add_option(",
            "            '-u', '--uptodate',",
            "            action='store_true',",
            "            default=False,",
            "            help='List uptodate packages (excluding editables)')",
            "        cmd_opts.add_option(",
            "            '-e', '--editable',",
            "            action='store_true',",
            "            default=False,",
            "            help='List editable projects.')",
            "        cmd_opts.add_option(",
            "            '-l', '--local',",
            "            action='store_true',",
            "            default=False,",
            "            help='If in a virtualenv that has global access, do not list globally-installed packages.')",
            "",
            "        cmd_opts.add_option(",
            "            '--pre',",
            "            action='store_true',",
            "            default=False,",
            "            help=\"Include pre-release and development versions. By default, pip only finds stable versions.\")",
            "",
            "        index_opts = make_option_group(index_group, self.parser)",
            "",
            "        self.parser.insert_option_group(0, index_opts)",
            "        self.parser.insert_option_group(0, cmd_opts)",
            "",
            "    def _build_package_finder(self, options, index_urls):",
            "        \"\"\"",
            "        Create a package finder appropriate to this list command.",
            "        \"\"\"",
            "        return PackageFinder(find_links=options.find_links,",
            "                             index_urls=index_urls,",
            "                             use_mirrors=options.use_mirrors,",
            "                             mirrors=options.mirrors,",
            "                             allow_external=options.allow_external,",
            "                             allow_insecure=options.allow_insecure,",
            "                             allow_all_external=options.allow_all_external,",
            "                             allow_all_prereleases=options.pre,",
            "                        )",
            "",
            "    def run(self, options, args):",
            "        if options.outdated:",
            "            self.run_outdated(options)",
            "        elif options.uptodate:",
            "            self.run_uptodate(options)",
            "        elif options.editable:",
            "            self.run_editables(options)",
            "        else:",
            "            self.run_listing(options)",
            "",
            "    def run_outdated(self, options):",
            "        for dist, remote_version_raw, remote_version_parsed in self.find_packages_latests_versions(options):",
            "            if remote_version_parsed > dist.parsed_version:",
            "                logger.notify('%s (Current: %s Latest: %s)' % (dist.project_name,",
            "                    dist.version, remote_version_raw))",
            "",
            "    def find_packages_latests_versions(self, options):",
            "        index_urls = [options.index_url] + options.extra_index_urls",
            "        if options.no_index:",
            "            logger.notify('Ignoring indexes: %s' % ','.join(index_urls))",
            "            index_urls = []",
            "",
            "        dependency_links = []",
            "        for dist in get_installed_distributions(local_only=options.local, skip=self.skip):",
            "            if dist.has_metadata('dependency_links.txt'):",
            "                dependency_links.extend(",
            "                    dist.get_metadata_lines('dependency_links.txt'),",
            "                )",
            "",
            "        finder = self._build_package_finder(options, index_urls)",
            "        finder.add_dependency_links(dependency_links)",
            "",
            "        installed_packages = get_installed_distributions(local_only=options.local, include_editables=False, skip=self.skip)",
            "        for dist in installed_packages:",
            "            req = InstallRequirement.from_line(dist.key, None)",
            "            try:",
            "                link = finder.find_requirement(req, True)",
            "",
            "                # If link is None, means installed version is most up-to-date",
            "                if link is None:",
            "                    continue",
            "            except DistributionNotFound:",
            "                continue",
            "            except BestVersionAlreadyInstalled:",
            "                remote_version = req.installed_version",
            "            else:",
            "                # It might be a good idea that link or finder had a public method",
            "                # that returned version",
            "                remote_version = finder._link_package_versions(link, req.name)[0]",
            "                remote_version_raw = remote_version[2]",
            "                remote_version_parsed = remote_version[0]",
            "            yield dist, remote_version_raw, remote_version_parsed",
            "",
            "    def run_listing(self, options):",
            "        installed_packages = get_installed_distributions(local_only=options.local, skip=self.skip)",
            "        self.output_package_listing(installed_packages)",
            "",
            "    def run_editables(self, options):",
            "        installed_packages = get_installed_distributions(local_only=options.local, editables_only=True)",
            "        self.output_package_listing(installed_packages)",
            "",
            "    def output_package_listing(self, installed_packages):",
            "        installed_packages = sorted(installed_packages, key=lambda dist: dist.project_name.lower())",
            "        for dist in installed_packages:",
            "            if dist_is_editable(dist):",
            "                line = '%s (%s, %s)' % (dist.project_name, dist.version, dist.location)",
            "            else:",
            "                line = '%s (%s)' % (dist.project_name, dist.version)",
            "            logger.notify(line)",
            "",
            "    def run_uptodate(self, options):",
            "        uptodate = []",
            "        for dist, remote_version_raw, remote_version_parsed in self.find_packages_latests_versions(options):",
            "            if dist.parsed_version == remote_version_parsed:",
            "                uptodate.append(dist)",
            "        self.output_package_listing(uptodate)"
        ],
        "afterPatchFile": [
            "from pip.basecommand import Command",
            "from pip.exceptions import DistributionNotFound, BestVersionAlreadyInstalled",
            "from pip.index import PackageFinder",
            "from pip.log import logger",
            "from pip.req import InstallRequirement",
            "from pip.util import get_installed_distributions, dist_is_editable",
            "from pip.cmdoptions import make_option_group, index_group",
            "",
            "",
            "class ListCommand(Command):",
            "    \"\"\"List installed packages, including editables.\"\"\"",
            "    name = 'list'",
            "    usage = \"\"\"",
            "      %prog [options]\"\"\"",
            "    summary = 'List installed packages.'",
            "",
            "    # distributions to skip (python itself is reported by pkg_resources.working_set)",
            "    skip = ['python']",
            "",
            "    def __init__(self, *args, **kw):",
            "        super(ListCommand, self).__init__(*args, **kw)",
            "",
            "        cmd_opts = self.cmd_opts",
            "",
            "        cmd_opts.add_option(",
            "            '-o', '--outdated',",
            "            action='store_true',",
            "            default=False,",
            "            help='List outdated packages (excluding editables)')",
            "        cmd_opts.add_option(",
            "            '-u', '--uptodate',",
            "            action='store_true',",
            "            default=False,",
            "            help='List uptodate packages (excluding editables)')",
            "        cmd_opts.add_option(",
            "            '-e', '--editable',",
            "            action='store_true',",
            "            default=False,",
            "            help='List editable projects.')",
            "        cmd_opts.add_option(",
            "            '-l', '--local',",
            "            action='store_true',",
            "            default=False,",
            "            help='If in a virtualenv that has global access, do not list globally-installed packages.')",
            "",
            "        cmd_opts.add_option(",
            "            '--pre',",
            "            action='store_true',",
            "            default=False,",
            "            help=\"Include pre-release and development versions. By default, pip only finds stable versions.\")",
            "",
            "        index_opts = make_option_group(index_group, self.parser)",
            "",
            "        self.parser.insert_option_group(0, index_opts)",
            "        self.parser.insert_option_group(0, cmd_opts)",
            "",
            "    def _build_package_finder(self, options, index_urls):",
            "        \"\"\"",
            "        Create a package finder appropriate to this list command.",
            "        \"\"\"",
            "        return PackageFinder(find_links=options.find_links,",
            "                             index_urls=index_urls,",
            "                             allow_external=options.allow_external,",
            "                             allow_insecure=options.allow_insecure,",
            "                             allow_all_external=options.allow_all_external,",
            "                             allow_all_prereleases=options.pre,",
            "                        )",
            "",
            "    def run(self, options, args):",
            "        if options.outdated:",
            "            self.run_outdated(options)",
            "        elif options.uptodate:",
            "            self.run_uptodate(options)",
            "        elif options.editable:",
            "            self.run_editables(options)",
            "        else:",
            "            self.run_listing(options)",
            "",
            "    def run_outdated(self, options):",
            "        for dist, remote_version_raw, remote_version_parsed in self.find_packages_latests_versions(options):",
            "            if remote_version_parsed > dist.parsed_version:",
            "                logger.notify('%s (Current: %s Latest: %s)' % (dist.project_name,",
            "                    dist.version, remote_version_raw))",
            "",
            "    def find_packages_latests_versions(self, options):",
            "        index_urls = [options.index_url] + options.extra_index_urls",
            "        if options.no_index:",
            "            logger.notify('Ignoring indexes: %s' % ','.join(index_urls))",
            "            index_urls = []",
            "",
            "        if options.use_mirrors:",
            "            logger.warn(\"--use-mirrors has been deprecated and will be removed\"",
            "                        \" in the future. Explicit uses of --index-url and/or \"",
            "                        \"--extra-index-url is suggested.\")",
            "",
            "        if options.mirrors:",
            "            logger.warn(\"--mirrors has been deprecated and will be removed in \"",
            "                        \" the future. Explicit uses of --index-url and/or \"",
            "                        \"--extra-index-url is suggested.\")",
            "            index_urls += options.mirrors",
            "",
            "        dependency_links = []",
            "        for dist in get_installed_distributions(local_only=options.local, skip=self.skip):",
            "            if dist.has_metadata('dependency_links.txt'):",
            "                dependency_links.extend(",
            "                    dist.get_metadata_lines('dependency_links.txt'),",
            "                )",
            "",
            "        finder = self._build_package_finder(options, index_urls)",
            "        finder.add_dependency_links(dependency_links)",
            "",
            "        installed_packages = get_installed_distributions(local_only=options.local, include_editables=False, skip=self.skip)",
            "        for dist in installed_packages:",
            "            req = InstallRequirement.from_line(dist.key, None)",
            "            try:",
            "                link = finder.find_requirement(req, True)",
            "",
            "                # If link is None, means installed version is most up-to-date",
            "                if link is None:",
            "                    continue",
            "            except DistributionNotFound:",
            "                continue",
            "            except BestVersionAlreadyInstalled:",
            "                remote_version = req.installed_version",
            "            else:",
            "                # It might be a good idea that link or finder had a public method",
            "                # that returned version",
            "                remote_version = finder._link_package_versions(link, req.name)[0]",
            "                remote_version_raw = remote_version[2]",
            "                remote_version_parsed = remote_version[0]",
            "            yield dist, remote_version_raw, remote_version_parsed",
            "",
            "    def run_listing(self, options):",
            "        installed_packages = get_installed_distributions(local_only=options.local, skip=self.skip)",
            "        self.output_package_listing(installed_packages)",
            "",
            "    def run_editables(self, options):",
            "        installed_packages = get_installed_distributions(local_only=options.local, editables_only=True)",
            "        self.output_package_listing(installed_packages)",
            "",
            "    def output_package_listing(self, installed_packages):",
            "        installed_packages = sorted(installed_packages, key=lambda dist: dist.project_name.lower())",
            "        for dist in installed_packages:",
            "            if dist_is_editable(dist):",
            "                line = '%s (%s, %s)' % (dist.project_name, dist.version, dist.location)",
            "            else:",
            "                line = '%s (%s)' % (dist.project_name, dist.version)",
            "            logger.notify(line)",
            "",
            "    def run_uptodate(self, options):",
            "        uptodate = []",
            "        for dist, remote_version_raw, remote_version_parsed in self.find_packages_latests_versions(options):",
            "            if dist.parsed_version == remote_version_parsed:",
            "                uptodate.append(dist)",
            "        self.output_package_listing(uptodate)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "63": [
                "ListCommand",
                "_build_package_finder"
            ],
            "64": [
                "ListCommand",
                "_build_package_finder"
            ]
        },
        "addLocation": []
    },
    "pip/commands/wheel.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "             logger.notify('Ignoring indexes: %s' % ','.join(index_urls))"
            },
            "1": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "             index_urls = []"
            },
            "2": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 98,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+        if options.use_mirrors:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+            logger.warn(\"--use-mirrors has been deprecated and will be removed\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+                        \" in the future. Explicit uses of --index-url and/or \""
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+                        \"--extra-index-url is suggested.\")"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+        if options.mirrors:"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+            logger.warn(\"--mirrors has been deprecated and will be removed in \""
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+                        \" the future. Explicit uses of --index-url and/or \""
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+                        \"--extra-index-url is suggested.\")"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+            index_urls += options.mirrors"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "         finder = PackageFinder(find_links=options.find_links,"
            },
            "15": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "                                index_urls=index_urls,"
            },
            "16": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                               use_mirrors=options.use_mirrors,"
            },
            "17": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                               mirrors=options.mirrors,"
            },
            "18": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "                                use_wheel=options.use_wheel,"
            },
            "19": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "                                allow_external=options.allow_external,"
            },
            "20": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "                                allow_insecure=options.allow_insecure,"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "from __future__ import absolute_import",
            "",
            "import os",
            "import sys",
            "from pip.basecommand import Command",
            "from pip.index import PackageFinder",
            "from pip.log import logger",
            "from pip.exceptions import CommandError",
            "from pip.req import InstallRequirement, RequirementSet, parse_requirements",
            "from pip.util import normalize_path",
            "from pip.wheel import WheelBuilder, wheel_setuptools_support, setuptools_requirement",
            "from pip import cmdoptions",
            "",
            "DEFAULT_WHEEL_DIR = os.path.join(normalize_path(os.curdir), 'wheelhouse')",
            "",
            "class WheelCommand(Command):",
            "    \"\"\"",
            "    Build Wheel archives for your requirements and dependencies.",
            "",
            "    Wheel is a built-package format, and offers the advantage of not recompiling your software during every install.",
            "    For more details, see the wheel docs: http://wheel.readthedocs.org/en/latest.",
            "",
            "    Requirements: setuptools>=0.8, and wheel.",
            "",
            "    'pip wheel' uses the bdist_wheel setuptools extension from the wheel package to build individual wheels.",
            "",
            "    \"\"\"",
            "",
            "    name = 'wheel'",
            "    usage = \"\"\"",
            "      %prog [options] <requirement specifier> ...",
            "      %prog [options] -r <requirements file> ...",
            "      %prog [options] <vcs project url> ...",
            "      %prog [options] <local project path> ...",
            "      %prog [options] <archive url/path> ...\"\"\"",
            "",
            "    summary = 'Build wheels from your requirements.'",
            "",
            "    def __init__(self, *args, **kw):",
            "        super(WheelCommand, self).__init__(*args, **kw)",
            "",
            "        cmd_opts = self.cmd_opts",
            "",
            "        cmd_opts.add_option(",
            "            '-w', '--wheel-dir',",
            "            dest='wheel_dir',",
            "            metavar='dir',",
            "            default=DEFAULT_WHEEL_DIR,",
            "            help=\"Build wheels into <dir>, where the default is '<cwd>/wheelhouse'.\")",
            "        cmd_opts.add_option(cmdoptions.use_wheel)",
            "        cmd_opts.add_option(",
            "            '--build-option',",
            "            dest='build_options',",
            "            metavar='options',",
            "            action='append',",
            "            help=\"Extra arguments to be supplied to 'setup.py bdist_wheel'.\")",
            "        cmd_opts.add_option(cmdoptions.requirements)",
            "        cmd_opts.add_option(cmdoptions.download_cache)",
            "        cmd_opts.add_option(cmdoptions.no_deps)",
            "        cmd_opts.add_option(cmdoptions.build_dir)",
            "",
            "        cmd_opts.add_option(",
            "            '--global-option',",
            "            dest='global_options',",
            "            action='append',",
            "            metavar='options',",
            "            help=\"Extra global options to be supplied to the setup.py \"",
            "            \"call before the 'bdist_wheel' command.\")",
            "",
            "        cmd_opts.add_option(",
            "            '--pre',",
            "            action='store_true',",
            "            default=False,",
            "            help=\"Include pre-release and development versions. By default, pip only finds stable versions.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.no_clean)",
            "",
            "        index_opts = cmdoptions.make_option_group(cmdoptions.index_group, self.parser)",
            "",
            "        self.parser.insert_option_group(0, index_opts)",
            "        self.parser.insert_option_group(0, cmd_opts)",
            "",
            "    def run(self, options, args):",
            "",
            "        # confirm requirements",
            "        try:",
            "            import wheel.bdist_wheel",
            "        except ImportError:",
            "            raise CommandError(\"'pip wheel' requires bdist_wheel from the 'wheel' distribution.\")",
            "        if not wheel_setuptools_support():",
            "            raise CommandError(\"'pip wheel' requires %s.\" % setuptools_requirement)",
            "",
            "        index_urls = [options.index_url] + options.extra_index_urls",
            "        if options.no_index:",
            "            logger.notify('Ignoring indexes: %s' % ','.join(index_urls))",
            "            index_urls = []",
            "",
            "        finder = PackageFinder(find_links=options.find_links,",
            "                               index_urls=index_urls,",
            "                               use_mirrors=options.use_mirrors,",
            "                               mirrors=options.mirrors,",
            "                               use_wheel=options.use_wheel,",
            "                               allow_external=options.allow_external,",
            "                               allow_insecure=options.allow_insecure,",
            "                               allow_all_external=options.allow_all_external,",
            "                               allow_all_prereleases=options.pre,",
            "                            )",
            "",
            "        options.build_dir = os.path.abspath(options.build_dir)",
            "        requirement_set = RequirementSet(",
            "            build_dir=options.build_dir,",
            "            src_dir=None,",
            "            download_dir=None,",
            "            download_cache=options.download_cache,",
            "            ignore_dependencies=options.ignore_dependencies,",
            "            ignore_installed=True)",
            "",
            "        #parse args and/or requirements files",
            "        for name in args:",
            "            if name.endswith(\".whl\"):",
            "                logger.notify(\"ignoring %s\" % name)",
            "                continue",
            "            requirement_set.add_requirement(",
            "                InstallRequirement.from_line(name, None))",
            "",
            "        for filename in options.requirements:",
            "            for req in parse_requirements(filename, finder=finder, options=options):",
            "                if req.editable or (req.name is None and req.url.endswith(\".whl\")):",
            "                    logger.notify(\"ignoring %s\" % req.url)",
            "                    continue",
            "                requirement_set.add_requirement(req)",
            "",
            "        #fail if no requirements",
            "        if not requirement_set.has_requirements:",
            "            opts = {'name': self.name}",
            "            msg = ('You must give at least one requirement '",
            "                   'to %(name)s (see \"pip help %(name)s\")' % opts)",
            "            logger.error(msg)",
            "            return",
            "",
            "        try:",
            "            #build wheels",
            "            wb = WheelBuilder(",
            "                requirement_set,",
            "                finder,",
            "                options.wheel_dir,",
            "                build_options = options.build_options or [],",
            "                global_options = options.global_options or []",
            "                )",
            "            wb.build()",
            "        finally:",
            "            if not options.no_clean:",
            "                requirement_set.cleanup_files()"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "from __future__ import absolute_import",
            "",
            "import os",
            "import sys",
            "from pip.basecommand import Command",
            "from pip.index import PackageFinder",
            "from pip.log import logger",
            "from pip.exceptions import CommandError",
            "from pip.req import InstallRequirement, RequirementSet, parse_requirements",
            "from pip.util import normalize_path",
            "from pip.wheel import WheelBuilder, wheel_setuptools_support, setuptools_requirement",
            "from pip import cmdoptions",
            "",
            "DEFAULT_WHEEL_DIR = os.path.join(normalize_path(os.curdir), 'wheelhouse')",
            "",
            "class WheelCommand(Command):",
            "    \"\"\"",
            "    Build Wheel archives for your requirements and dependencies.",
            "",
            "    Wheel is a built-package format, and offers the advantage of not recompiling your software during every install.",
            "    For more details, see the wheel docs: http://wheel.readthedocs.org/en/latest.",
            "",
            "    Requirements: setuptools>=0.8, and wheel.",
            "",
            "    'pip wheel' uses the bdist_wheel setuptools extension from the wheel package to build individual wheels.",
            "",
            "    \"\"\"",
            "",
            "    name = 'wheel'",
            "    usage = \"\"\"",
            "      %prog [options] <requirement specifier> ...",
            "      %prog [options] -r <requirements file> ...",
            "      %prog [options] <vcs project url> ...",
            "      %prog [options] <local project path> ...",
            "      %prog [options] <archive url/path> ...\"\"\"",
            "",
            "    summary = 'Build wheels from your requirements.'",
            "",
            "    def __init__(self, *args, **kw):",
            "        super(WheelCommand, self).__init__(*args, **kw)",
            "",
            "        cmd_opts = self.cmd_opts",
            "",
            "        cmd_opts.add_option(",
            "            '-w', '--wheel-dir',",
            "            dest='wheel_dir',",
            "            metavar='dir',",
            "            default=DEFAULT_WHEEL_DIR,",
            "            help=\"Build wheels into <dir>, where the default is '<cwd>/wheelhouse'.\")",
            "        cmd_opts.add_option(cmdoptions.use_wheel)",
            "        cmd_opts.add_option(",
            "            '--build-option',",
            "            dest='build_options',",
            "            metavar='options',",
            "            action='append',",
            "            help=\"Extra arguments to be supplied to 'setup.py bdist_wheel'.\")",
            "        cmd_opts.add_option(cmdoptions.requirements)",
            "        cmd_opts.add_option(cmdoptions.download_cache)",
            "        cmd_opts.add_option(cmdoptions.no_deps)",
            "        cmd_opts.add_option(cmdoptions.build_dir)",
            "",
            "        cmd_opts.add_option(",
            "            '--global-option',",
            "            dest='global_options',",
            "            action='append',",
            "            metavar='options',",
            "            help=\"Extra global options to be supplied to the setup.py \"",
            "            \"call before the 'bdist_wheel' command.\")",
            "",
            "        cmd_opts.add_option(",
            "            '--pre',",
            "            action='store_true',",
            "            default=False,",
            "            help=\"Include pre-release and development versions. By default, pip only finds stable versions.\")",
            "",
            "        cmd_opts.add_option(cmdoptions.no_clean)",
            "",
            "        index_opts = cmdoptions.make_option_group(cmdoptions.index_group, self.parser)",
            "",
            "        self.parser.insert_option_group(0, index_opts)",
            "        self.parser.insert_option_group(0, cmd_opts)",
            "",
            "    def run(self, options, args):",
            "",
            "        # confirm requirements",
            "        try:",
            "            import wheel.bdist_wheel",
            "        except ImportError:",
            "            raise CommandError(\"'pip wheel' requires bdist_wheel from the 'wheel' distribution.\")",
            "        if not wheel_setuptools_support():",
            "            raise CommandError(\"'pip wheel' requires %s.\" % setuptools_requirement)",
            "",
            "        index_urls = [options.index_url] + options.extra_index_urls",
            "        if options.no_index:",
            "            logger.notify('Ignoring indexes: %s' % ','.join(index_urls))",
            "            index_urls = []",
            "",
            "        if options.use_mirrors:",
            "            logger.warn(\"--use-mirrors has been deprecated and will be removed\"",
            "                        \" in the future. Explicit uses of --index-url and/or \"",
            "                        \"--extra-index-url is suggested.\")",
            "",
            "        if options.mirrors:",
            "            logger.warn(\"--mirrors has been deprecated and will be removed in \"",
            "                        \" the future. Explicit uses of --index-url and/or \"",
            "                        \"--extra-index-url is suggested.\")",
            "            index_urls += options.mirrors",
            "",
            "        finder = PackageFinder(find_links=options.find_links,",
            "                               index_urls=index_urls,",
            "                               use_wheel=options.use_wheel,",
            "                               allow_external=options.allow_external,",
            "                               allow_insecure=options.allow_insecure,",
            "                               allow_all_external=options.allow_all_external,",
            "                               allow_all_prereleases=options.pre,",
            "                            )",
            "",
            "        options.build_dir = os.path.abspath(options.build_dir)",
            "        requirement_set = RequirementSet(",
            "            build_dir=options.build_dir,",
            "            src_dir=None,",
            "            download_dir=None,",
            "            download_cache=options.download_cache,",
            "            ignore_dependencies=options.ignore_dependencies,",
            "            ignore_installed=True)",
            "",
            "        #parse args and/or requirements files",
            "        for name in args:",
            "            if name.endswith(\".whl\"):",
            "                logger.notify(\"ignoring %s\" % name)",
            "                continue",
            "            requirement_set.add_requirement(",
            "                InstallRequirement.from_line(name, None))",
            "",
            "        for filename in options.requirements:",
            "            for req in parse_requirements(filename, finder=finder, options=options):",
            "                if req.editable or (req.name is None and req.url.endswith(\".whl\")):",
            "                    logger.notify(\"ignoring %s\" % req.url)",
            "                    continue",
            "                requirement_set.add_requirement(req)",
            "",
            "        #fail if no requirements",
            "        if not requirement_set.has_requirements:",
            "            opts = {'name': self.name}",
            "            msg = ('You must give at least one requirement '",
            "                   'to %(name)s (see \"pip help %(name)s\")' % opts)",
            "            logger.error(msg)",
            "            return",
            "",
            "        try:",
            "            #build wheels",
            "            wb = WheelBuilder(",
            "                requirement_set,",
            "                finder,",
            "                options.wheel_dir,",
            "                build_options = options.build_options or [],",
            "                global_options = options.global_options or []",
            "                )",
            "            wb.build()",
            "        finally:",
            "            if not options.no_clean:",
            "                requirement_set.cleanup_files()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "101": [
                "WheelCommand",
                "run"
            ],
            "102": [
                "WheelCommand",
                "run"
            ]
        },
        "addLocation": [
            "rdiffweb.controller.page_pref_sshkeys.ApiSshKeys.post",
            "pip.commands.wheel.WheelCommand.run.opts",
            "pip.commands.wheel.WheelCommand.run.index_urls"
        ]
    },
    "pip/index.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "     \"\"\""
            },
            "1": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     def __init__(self, find_links, index_urls,"
            },
            "3": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            use_mirrors=False, mirrors=None, main_mirror_url=None,"
            },
            "4": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "             use_wheel=False, allow_external=[], allow_insecure=[],"
            },
            "5": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "             allow_all_external=False, allow_all_insecure=False,"
            },
            "6": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "             allow_all_prereleases=False):"
            },
            "7": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "         self.cache = PageCache()"
            },
            "8": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "         # These are boring links that have already been logged somehow:"
            },
            "9": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "         self.logged_links = set()"
            },
            "10": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if use_mirrors:"
            },
            "11": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.mirror_urls = self._get_mirror_urls(mirrors, main_mirror_url)"
            },
            "12": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            logger.info('Using PyPI mirrors: %s' % ', '.join(self.mirror_urls))"
            },
            "13": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:"
            },
            "14": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.mirror_urls = []"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "         self.use_wheel = use_wheel"
            },
            "17": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "         # Do we allow (safe and verifiable) externally hosted files?"
            },
            "19": {
                "beforePatchRowNumber": 202,
                "afterPatchRowNumber": 197,
                "PatchRowcode": "             if page is None:"
            },
            "20": {
                "beforePatchRowNumber": 203,
                "afterPatchRowNumber": 198,
                "PatchRowcode": "                 url_name = self._find_url_name(Link(self.index_urls[0], trusted=True), url_name, req) or req.url_name"
            },
            "21": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": 199,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # Combine index URLs with mirror URLs here to allow"
            },
            "23": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # adding more index URLs from requirements files"
            },
            "24": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        all_index_urls = self.index_urls + self.mirror_urls"
            },
            "25": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "26": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 200,
                "PatchRowcode": "         if url_name is not None:"
            },
            "27": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 201,
                "PatchRowcode": "             locations = ["
            },
            "28": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": 202,
                "PatchRowcode": "                 mkurl_pypi_url(url)"
            },
            "29": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                for url in all_index_urls] + self.find_links"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 203,
                "PatchRowcode": "+                for url in self.index_urls] + self.find_links"
            },
            "31": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 204,
                "PatchRowcode": "         else:"
            },
            "32": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "             locations = list(self.find_links)"
            },
            "33": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "         for version in req.absolute_versions:"
            },
            "34": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "         file_locations.extend(_flocations)"
            },
            "35": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 214,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "         # We trust every url that the user has given us whether it was given"
            },
            "37": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        #   via --index-url, --user-mirrors/--mirror, or --find-links or a"
            },
            "38": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        #   default option thereof"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+        #   via --index-url or --find-links"
            },
            "40": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "         locations = [Link(url, trusted=True) for url in url_locations]"
            },
            "41": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 218,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "         # We explicitly do not trust links that came from dependency_links"
            },
            "43": {
                "beforePatchRowNumber": 474,
                "afterPatchRowNumber": 464,
                "PatchRowcode": "                     return []"
            },
            "44": {
                "beforePatchRowNumber": 475,
                "afterPatchRowNumber": 465,
                "PatchRowcode": " "
            },
            "45": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 466,
                "PatchRowcode": "                 # This is a dirty hack to prevent installing Binary Wheels from"
            },
            "46": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                #   PyPI or one of its mirrors unless it is a Windows Binary"
            },
            "47": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                #   Wheel. This is paired with a change to PyPI disabling"
            },
            "48": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                #   uploads for the same. Once we have a mechanism for enabling"
            },
            "49": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                #   support for binary wheels on linux that deals with the"
            },
            "50": {
                "beforePatchRowNumber": 481,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                #   inherent problems of binary distribution this can be"
            },
            "51": {
                "beforePatchRowNumber": 482,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                #   removed."
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 467,
                "PatchRowcode": "+                #   PyPI unless it is a Windows Binary Wheel. This is paired"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 468,
                "PatchRowcode": "+                #   with a change to PyPI disabling uploads for the same. Once"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 469,
                "PatchRowcode": "+                #   we have a mechanism for enabling support for binary wheels"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 470,
                "PatchRowcode": "+                #   on linux that deals with the inherent problems of binary"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 471,
                "PatchRowcode": "+                #   distribution this can be removed."
            },
            "57": {
                "beforePatchRowNumber": 483,
                "afterPatchRowNumber": 472,
                "PatchRowcode": "                 comes_from = getattr(link, \"comes_from\", None)"
            },
            "58": {
                "beforePatchRowNumber": 484,
                "afterPatchRowNumber": 473,
                "PatchRowcode": "                 if (not platform.startswith('win')"
            },
            "59": {
                "beforePatchRowNumber": 485,
                "afterPatchRowNumber": 474,
                "PatchRowcode": "                     and comes_from is not None"
            },
            "60": {
                "beforePatchRowNumber": 550,
                "afterPatchRowNumber": 539,
                "PatchRowcode": "     def _get_page(self, link, req):"
            },
            "61": {
                "beforePatchRowNumber": 551,
                "afterPatchRowNumber": 540,
                "PatchRowcode": "         return HTMLPage.get_page(link, req, cache=self.cache)"
            },
            "62": {
                "beforePatchRowNumber": 552,
                "afterPatchRowNumber": 541,
                "PatchRowcode": " "
            },
            "63": {
                "beforePatchRowNumber": 553,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def _get_mirror_urls(self, mirrors=None, main_mirror_url=None):"
            },
            "64": {
                "beforePatchRowNumber": 554,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\"Retrieves a list of URLs from the main mirror DNS entry"
            },
            "65": {
                "beforePatchRowNumber": 555,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        unless a list of mirror URLs are passed."
            },
            "66": {
                "beforePatchRowNumber": 556,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "67": {
                "beforePatchRowNumber": 557,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not mirrors:"
            },
            "68": {
                "beforePatchRowNumber": 558,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            mirrors = get_mirrors(main_mirror_url)"
            },
            "69": {
                "beforePatchRowNumber": 559,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Should this be made \"less random\"? E.g. netselect like?"
            },
            "70": {
                "beforePatchRowNumber": 560,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            random.shuffle(mirrors)"
            },
            "71": {
                "beforePatchRowNumber": 561,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "72": {
                "beforePatchRowNumber": 562,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        mirror_urls = set()"
            },
            "73": {
                "beforePatchRowNumber": 563,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        for mirror_url in mirrors:"
            },
            "74": {
                "beforePatchRowNumber": 564,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            mirror_url = mirror_url.rstrip('/')"
            },
            "75": {
                "beforePatchRowNumber": 565,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # Make sure we have a valid URL"
            },
            "76": {
                "beforePatchRowNumber": 566,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if not any([mirror_url.startswith(scheme) for scheme in [\"http://\", \"https://\", \"file://\"]]):"
            },
            "77": {
                "beforePatchRowNumber": 567,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                mirror_url = \"http://%s\" % mirror_url"
            },
            "78": {
                "beforePatchRowNumber": 568,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if not mirror_url.endswith(\"/simple\"):"
            },
            "79": {
                "beforePatchRowNumber": 569,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                mirror_url = \"%s/simple\" % mirror_url"
            },
            "80": {
                "beforePatchRowNumber": 570,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            mirror_urls.add(mirror_url + '/')"
            },
            "81": {
                "beforePatchRowNumber": 571,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "82": {
                "beforePatchRowNumber": 572,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return list(mirror_urls)"
            },
            "83": {
                "beforePatchRowNumber": 573,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "84": {
                "beforePatchRowNumber": 574,
                "afterPatchRowNumber": 542,
                "PatchRowcode": " "
            },
            "85": {
                "beforePatchRowNumber": 575,
                "afterPatchRowNumber": 543,
                "PatchRowcode": " class PageCache(object):"
            },
            "86": {
                "beforePatchRowNumber": 576,
                "afterPatchRowNumber": 544,
                "PatchRowcode": "     \"\"\"Cache of HTML pages\"\"\""
            },
            "87": {
                "beforePatchRowNumber": 994,
                "afterPatchRowNumber": 962,
                "PatchRowcode": "         return '%s==%s' % (name, version)"
            },
            "88": {
                "beforePatchRowNumber": 995,
                "afterPatchRowNumber": 963,
                "PatchRowcode": "     else:"
            },
            "89": {
                "beforePatchRowNumber": 996,
                "afterPatchRowNumber": 964,
                "PatchRowcode": "         return name"
            },
            "90": {
                "beforePatchRowNumber": 997,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "91": {
                "beforePatchRowNumber": 998,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "92": {
                "beforePatchRowNumber": 999,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def get_mirrors(hostname=None):"
            },
            "93": {
                "beforePatchRowNumber": 1000,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Return the list of mirrors from the last record found on the DNS"
            },
            "94": {
                "beforePatchRowNumber": 1001,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    entry::"
            },
            "95": {
                "beforePatchRowNumber": 1002,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "96": {
                "beforePatchRowNumber": 1003,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    >>> from pip.index import get_mirrors"
            },
            "97": {
                "beforePatchRowNumber": 1004,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    >>> get_mirrors()"
            },
            "98": {
                "beforePatchRowNumber": 1005,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ['a.pypi.python.org', 'b.pypi.python.org', 'c.pypi.python.org',"
            },
            "99": {
                "beforePatchRowNumber": 1006,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    'd.pypi.python.org']"
            },
            "100": {
                "beforePatchRowNumber": 1007,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "101": {
                "beforePatchRowNumber": 1008,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Originally written for the distutils2 project by Alexis Metaireau."
            },
            "102": {
                "beforePatchRowNumber": 1009,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\""
            },
            "103": {
                "beforePatchRowNumber": 1010,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if hostname is None:"
            },
            "104": {
                "beforePatchRowNumber": 1011,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        hostname = DEFAULT_MIRROR_HOSTNAME"
            },
            "105": {
                "beforePatchRowNumber": 1012,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "106": {
                "beforePatchRowNumber": 1013,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # return the last mirror registered on PyPI."
            },
            "107": {
                "beforePatchRowNumber": 1014,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    last_mirror_hostname = None"
            },
            "108": {
                "beforePatchRowNumber": 1015,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    try:"
            },
            "109": {
                "beforePatchRowNumber": 1016,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        last_mirror_hostname = socket.gethostbyname_ex(hostname)[0]"
            },
            "110": {
                "beforePatchRowNumber": 1017,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    except socket.gaierror:"
            },
            "111": {
                "beforePatchRowNumber": 1018,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return []"
            },
            "112": {
                "beforePatchRowNumber": 1019,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not last_mirror_hostname or last_mirror_hostname == DEFAULT_MIRROR_HOSTNAME:"
            },
            "113": {
                "beforePatchRowNumber": 1020,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        last_mirror_hostname = \"z.pypi.python.org\""
            },
            "114": {
                "beforePatchRowNumber": 1021,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    end_letter = last_mirror_hostname.split(\".\", 1)"
            },
            "115": {
                "beforePatchRowNumber": 1022,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "116": {
                "beforePatchRowNumber": 1023,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # determine the list from the last one."
            },
            "117": {
                "beforePatchRowNumber": 1024,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return [\"%s.%s\" % (s, end_letter[1]) for s in string_range(end_letter[0])]"
            },
            "118": {
                "beforePatchRowNumber": 1025,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "119": {
                "beforePatchRowNumber": 1026,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "120": {
                "beforePatchRowNumber": 1027,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def string_range(last):"
            },
            "121": {
                "beforePatchRowNumber": 1028,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Compute the range of string between \"a\" and last."
            },
            "122": {
                "beforePatchRowNumber": 1029,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "123": {
                "beforePatchRowNumber": 1030,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    This works for simple \"a to z\" lists, but also for \"a to zz\" lists."
            },
            "124": {
                "beforePatchRowNumber": 1031,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\""
            },
            "125": {
                "beforePatchRowNumber": 1032,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for k in range(len(last)):"
            },
            "126": {
                "beforePatchRowNumber": 1033,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        for x in product(string.ascii_lowercase, repeat=k+1):"
            },
            "127": {
                "beforePatchRowNumber": 1034,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            result = ''.join(x)"
            },
            "128": {
                "beforePatchRowNumber": 1035,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            yield result"
            },
            "129": {
                "beforePatchRowNumber": 1036,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if result == last:"
            },
            "130": {
                "beforePatchRowNumber": 1037,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                return"
            },
            "131": {
                "beforePatchRowNumber": 1038,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            }
        },
        "frontPatchFile": [
            "\"\"\"Routines related to PyPI, indexes\"\"\"",
            "",
            "import sys",
            "import os",
            "import re",
            "import gzip",
            "import mimetypes",
            "import posixpath",
            "import pkg_resources",
            "import random",
            "import socket",
            "import ssl",
            "import string",
            "import zlib",
            "",
            "try:",
            "    import threading",
            "except ImportError:",
            "    import dummy_threading as threading",
            "",
            "from pip.log import logger",
            "from pip.util import Inf, normalize_name, splitext, is_prerelease",
            "from pip.exceptions import DistributionNotFound, BestVersionAlreadyInstalled,\\",
            "    InstallationError",
            "from pip.backwardcompat import (WindowsError, BytesIO,",
            "                                Queue, urlparse,",
            "                                URLError, HTTPError, u,",
            "                                product, url2pathname,",
            "                                Empty as QueueEmpty)",
            "from pip.backwardcompat import CertificateError",
            "from pip.download import urlopen, path_to_url2, url_to_path, geturl, Urllib2HeadRequest",
            "from pip.wheel import Wheel, wheel_ext, wheel_setuptools_support, setuptools_requirement",
            "from pip.pep425tags import supported_tags, supported_tags_noarch, get_platform",
            "from pip.vendor import html5lib",
            "",
            "__all__ = ['PackageFinder']",
            "",
            "",
            "DEFAULT_MIRROR_HOSTNAME = \"last.pypi.python.org\"",
            "",
            "",
            "class PackageFinder(object):",
            "    \"\"\"This finds packages.",
            "",
            "    This is meant to match easy_install's technique for looking for",
            "    packages, by reading pages and looking for appropriate links",
            "    \"\"\"",
            "",
            "    def __init__(self, find_links, index_urls,",
            "            use_mirrors=False, mirrors=None, main_mirror_url=None,",
            "            use_wheel=False, allow_external=[], allow_insecure=[],",
            "            allow_all_external=False, allow_all_insecure=False,",
            "            allow_all_prereleases=False):",
            "        self.find_links = find_links",
            "        self.index_urls = index_urls",
            "        self.dependency_links = []",
            "        self.cache = PageCache()",
            "        # These are boring links that have already been logged somehow:",
            "        self.logged_links = set()",
            "        if use_mirrors:",
            "            self.mirror_urls = self._get_mirror_urls(mirrors, main_mirror_url)",
            "            logger.info('Using PyPI mirrors: %s' % ', '.join(self.mirror_urls))",
            "        else:",
            "            self.mirror_urls = []",
            "        self.use_wheel = use_wheel",
            "",
            "        # Do we allow (safe and verifiable) externally hosted files?",
            "        self.allow_external = set(normalize_name(n) for n in allow_external)",
            "",
            "        # Which names are allowed to install insecure and unverifiable files?",
            "        self.allow_insecure = set(normalize_name(n) for n in allow_insecure)",
            "",
            "        # Do we allow all (safe and verifiable) externally hosted files?",
            "        self.allow_all_external = allow_all_external",
            "",
            "        # Do we allow unsafe and unverifiable files?",
            "        self.allow_all_insecure = allow_all_insecure",
            "",
            "        # Stores if we ignored any external links so that we can instruct",
            "        #   end users how to install them if no distributions are available",
            "        self.need_warn_external = False",
            "",
            "        # Stores if we ignored any unsafe links so that we can instruct",
            "        #   end users how to install them if no distributions are available",
            "        self.need_warn_insecure = False",
            "",
            "        # Do we want to allow _all_ pre-releases?",
            "        self.allow_all_prereleases = allow_all_prereleases",
            "",
            "    @property",
            "    def use_wheel(self):",
            "        return self._use_wheel",
            "",
            "    @use_wheel.setter",
            "    def use_wheel(self, value):",
            "        self._use_wheel = value",
            "        if self._use_wheel and not wheel_setuptools_support():",
            "            raise InstallationError(\"pip's wheel support requires %s.\" % setuptools_requirement)",
            "",
            "    def add_dependency_links(self, links):",
            "        ## FIXME: this shouldn't be global list this, it should only",
            "        ## apply to requirements of the package that specifies the",
            "        ## dependency_links value",
            "        ## FIXME: also, we should track comes_from (i.e., use Link)",
            "        self.dependency_links.extend(links)",
            "",
            "    def _sort_locations(self, locations):",
            "        \"\"\"",
            "        Sort locations into \"files\" (archives) and \"urls\", and return",
            "        a pair of lists (files,urls)",
            "        \"\"\"",
            "        files = []",
            "        urls = []",
            "",
            "        # puts the url for the given file path into the appropriate list",
            "        def sort_path(path):",
            "            url = path_to_url2(path)",
            "            if mimetypes.guess_type(url, strict=False)[0] == 'text/html':",
            "                urls.append(url)",
            "            else:",
            "                files.append(url)",
            "",
            "        for url in locations:",
            "",
            "            is_local_path = os.path.exists(url)",
            "            is_file_url = url.startswith('file:')",
            "            is_find_link = url in self.find_links",
            "",
            "            if is_local_path or is_file_url:",
            "                if is_local_path:",
            "                    path = url",
            "                else:",
            "                    path = url_to_path(url)",
            "                if is_find_link and os.path.isdir(path):",
            "                    path = os.path.realpath(path)",
            "                    for item in os.listdir(path):",
            "                        sort_path(os.path.join(path, item))",
            "                elif is_file_url and os.path.isdir(path):",
            "                    urls.append(url)",
            "                elif os.path.isfile(path):",
            "                    sort_path(path)",
            "            else:",
            "                urls.append(url)",
            "",
            "        return files, urls",
            "",
            "    def _link_sort_key(self, link_tuple):",
            "        \"\"\"",
            "        Function used to generate link sort key for link tuples.",
            "        The greater the return value, the more preferred it is.",
            "        If not finding wheels, then sorted by version only.",
            "        If finding wheels, then the sort order is by version, then:",
            "          1. existing installs",
            "          2. wheels ordered via Wheel.support_index_min()",
            "          3. source archives",
            "        Note: it was considered to embed this logic into the Link",
            "              comparison operators, but then different sdist links",
            "              with the same version, would have to be considered equal",
            "        \"\"\"",
            "        parsed_version, link, _ = link_tuple",
            "        if self.use_wheel:",
            "            support_num = len(supported_tags)",
            "            if link == InfLink: # existing install",
            "                pri = 1",
            "            elif link.wheel:",
            "                # all wheel links are known to be supported at this stage",
            "                pri = -(link.wheel.support_index_min())",
            "            else: # sdist",
            "                pri = -(support_num)",
            "            return (parsed_version, pri)",
            "        else:",
            "            return parsed_version",
            "",
            "    def _sort_versions(self, applicable_versions):",
            "        \"\"\"",
            "        Bring the latest version (and wheels) to the front, but maintain the existing ordering as secondary.",
            "        See the docstring for `_link_sort_key` for details.",
            "        This function is isolated for easier unit testing.",
            "        \"\"\"",
            "        return sorted(applicable_versions, key=self._link_sort_key, reverse=True)",
            "",
            "    def find_requirement(self, req, upgrade):",
            "",
            "        def mkurl_pypi_url(url):",
            "            loc = posixpath.join(url, url_name)",
            "            # For maximum compatibility with easy_install, ensure the path",
            "            # ends in a trailing slash.  Although this isn't in the spec",
            "            # (and PyPI can handle it without the slash) some other index",
            "            # implementations might break if they relied on easy_install's behavior.",
            "            if not loc.endswith('/'):",
            "                loc = loc + '/'",
            "            return loc",
            "",
            "        url_name = req.url_name",
            "        # Only check main index if index URL is given:",
            "        main_index_url = None",
            "        if self.index_urls:",
            "            # Check that we have the url_name correctly spelled:",
            "            main_index_url = Link(mkurl_pypi_url(self.index_urls[0]), trusted=True)",
            "            # This will also cache the page, so it's okay that we get it again later:",
            "            page = self._get_page(main_index_url, req)",
            "            if page is None:",
            "                url_name = self._find_url_name(Link(self.index_urls[0], trusted=True), url_name, req) or req.url_name",
            "",
            "        # Combine index URLs with mirror URLs here to allow",
            "        # adding more index URLs from requirements files",
            "        all_index_urls = self.index_urls + self.mirror_urls",
            "",
            "        if url_name is not None:",
            "            locations = [",
            "                mkurl_pypi_url(url)",
            "                for url in all_index_urls] + self.find_links",
            "        else:",
            "            locations = list(self.find_links)",
            "        for version in req.absolute_versions:",
            "            if url_name is not None and main_index_url is not None:",
            "                locations = [",
            "                    posixpath.join(main_index_url.url, version)] + locations",
            "",
            "        file_locations, url_locations = self._sort_locations(locations)",
            "        _flocations, _ulocations = self._sort_locations(self.dependency_links)",
            "        file_locations.extend(_flocations)",
            "",
            "        # We trust every url that the user has given us whether it was given",
            "        #   via --index-url, --user-mirrors/--mirror, or --find-links or a",
            "        #   default option thereof",
            "        locations = [Link(url, trusted=True) for url in url_locations]",
            "",
            "        # We explicitly do not trust links that came from dependency_links",
            "        locations.extend([Link(url) for url in _ulocations])",
            "",
            "        logger.debug('URLs to search for versions for %s:' % req)",
            "        for location in locations:",
            "            logger.debug('* %s' % location)",
            "        found_versions = []",
            "        found_versions.extend(",
            "            self._package_versions(",
            "                # We trust every directly linked archive in find_links",
            "                [Link(url, '-f', trusted=True) for url in self.find_links], req.name.lower()))",
            "        page_versions = []",
            "        for page in self._get_pages(locations, req):",
            "            logger.debug('Analyzing links from page %s' % page.url)",
            "            logger.indent += 2",
            "            try:",
            "                page_versions.extend(self._package_versions(page.links, req.name.lower()))",
            "            finally:",
            "                logger.indent -= 2",
            "        dependency_versions = list(self._package_versions(",
            "            [Link(url) for url in self.dependency_links], req.name.lower()))",
            "        if dependency_versions:",
            "            logger.info('dependency_links found: %s' % ', '.join([link.url for parsed, link, version in dependency_versions]))",
            "        file_versions = list(self._package_versions(",
            "                [Link(url) for url in file_locations], req.name.lower()))",
            "        if not found_versions and not page_versions and not dependency_versions and not file_versions:",
            "            logger.fatal('Could not find any downloads that satisfy the requirement %s' % req)",
            "",
            "            if self.need_warn_external:",
            "                logger.warn(\"Some externally hosted files were ignored (use \"",
            "                            \"--allow-external %s to allow).\" % req.name)",
            "",
            "            if self.need_warn_insecure:",
            "                logger.warn(\"Some insecure and unverifiable files were ignored\"",
            "                            \" (use --allow-insecure %s to allow).\" % req.name)",
            "",
            "            raise DistributionNotFound('No distributions at all found for %s' % req)",
            "        installed_version = []",
            "        if req.satisfied_by is not None:",
            "            installed_version = [(req.satisfied_by.parsed_version, InfLink, req.satisfied_by.version)]",
            "        if file_versions:",
            "            file_versions.sort(reverse=True)",
            "            logger.info('Local files found: %s' % ', '.join([url_to_path(link.url) for parsed, link, version in file_versions]))",
            "        #this is an intentional priority ordering",
            "        all_versions = installed_version + file_versions + found_versions + page_versions + dependency_versions",
            "        applicable_versions = []",
            "        for (parsed_version, link, version) in all_versions:",
            "            if version not in req.req:",
            "                logger.info(\"Ignoring link %s, version %s doesn't match %s\"",
            "                            % (link, version, ','.join([''.join(s) for s in req.req.specs])))",
            "                continue",
            "            elif is_prerelease(version) and not (self.allow_all_prereleases or req.prereleases):",
            "                # If this version isn't the already installed one, then",
            "                #   ignore it if it's a pre-release.",
            "                if link is not InfLink:",
            "                    logger.info(\"Ignoring link %s, version %s is a pre-release (use --pre to allow).\" % (link, version))",
            "                    continue",
            "            applicable_versions.append((parsed_version, link, version))",
            "        applicable_versions = self._sort_versions(applicable_versions)",
            "        existing_applicable = bool([link for parsed_version, link, version in applicable_versions if link is InfLink])",
            "        if not upgrade and existing_applicable:",
            "            if applicable_versions[0][1] is InfLink:",
            "                logger.info('Existing installed version (%s) is most up-to-date and satisfies requirement'",
            "                            % req.satisfied_by.version)",
            "            else:",
            "                logger.info('Existing installed version (%s) satisfies requirement (most up-to-date version is %s)'",
            "                            % (req.satisfied_by.version, applicable_versions[0][2]))",
            "            return None",
            "        if not applicable_versions:",
            "            logger.fatal('Could not find a version that satisfies the requirement %s (from versions: %s)'",
            "                         % (req, ', '.join([version for parsed_version, link, version in all_versions])))",
            "",
            "            if self.need_warn_external:",
            "                logger.warn(\"Some externally hosted files were ignored (use \"",
            "                            \"--allow-external to allow).\")",
            "",
            "            if self.need_warn_insecure:",
            "                logger.warn(\"Some insecure and unverifiable files were ignored\"",
            "                            \" (use --allow-insecure %s to allow).\" % req.name)",
            "",
            "            raise DistributionNotFound('No distributions matching the version for %s' % req)",
            "        if applicable_versions[0][1] is InfLink:",
            "            # We have an existing version, and its the best version",
            "            logger.info('Installed version (%s) is most up-to-date (past versions: %s)'",
            "                        % (req.satisfied_by.version, ', '.join([version for parsed_version, link, version in applicable_versions[1:]]) or 'none'))",
            "            raise BestVersionAlreadyInstalled",
            "        if len(applicable_versions) > 1:",
            "            logger.info('Using version %s (newest of versions: %s)' %",
            "                        (applicable_versions[0][2], ', '.join([version for parsed_version, link, version in applicable_versions])))",
            "",
            "        selected_version = applicable_versions[0][1]",
            "",
            "        if (selected_version.internal is not None",
            "                and not selected_version.internal):",
            "            logger.warn(\"%s an externally hosted file and may be \"",
            "                        \"unreliable\" % req.name)",
            "",
            "        if (selected_version.verifiable is not None",
            "                and not selected_version.verifiable):",
            "            logger.warn(\"%s is potentially insecure and \"",
            "                        \"unverifiable.\" % req.name)",
            "",
            "        return selected_version",
            "",
            "",
            "    def _find_url_name(self, index_url, url_name, req):",
            "        \"\"\"Finds the true URL name of a package, when the given name isn't quite correct.",
            "        This is usually used to implement case-insensitivity.\"\"\"",
            "        if not index_url.url.endswith('/'):",
            "            # Vaguely part of the PyPI API... weird but true.",
            "            ## FIXME: bad to modify this?",
            "            index_url.url += '/'",
            "        page = self._get_page(index_url, req)",
            "        if page is None:",
            "            logger.fatal('Cannot fetch index base URL %s' % index_url)",
            "            return",
            "        norm_name = normalize_name(req.url_name)",
            "        for link in page.links:",
            "            base = posixpath.basename(link.path.rstrip('/'))",
            "            if norm_name == normalize_name(base):",
            "                logger.notify('Real name of requirement %s is %s' % (url_name, base))",
            "                return base",
            "        return None",
            "",
            "    def _get_pages(self, locations, req):",
            "        \"\"\"Yields (page, page_url) from the given locations, skipping",
            "        locations that have errors, and adding download/homepage links\"\"\"",
            "        pending_queue = Queue()",
            "        for location in locations:",
            "            pending_queue.put(location)",
            "        done = []",
            "        seen = set()",
            "        threads = []",
            "        for i in range(min(10, len(locations))):",
            "            t = threading.Thread(target=self._get_queued_page, args=(req, pending_queue, done, seen))",
            "            t.setDaemon(True)",
            "            threads.append(t)",
            "            t.start()",
            "        for t in threads:",
            "            t.join()",
            "        return done",
            "",
            "    _log_lock = threading.Lock()",
            "",
            "    def _get_queued_page(self, req, pending_queue, done, seen):",
            "        while 1:",
            "            try:",
            "                location = pending_queue.get(False)",
            "            except QueueEmpty:",
            "                return",
            "            if location in seen:",
            "                continue",
            "            seen.add(location)",
            "            page = self._get_page(location, req)",
            "            if page is None:",
            "                continue",
            "            done.append(page)",
            "            for link in page.rel_links():",
            "                normalized = normalize_name(req.name).lower()",
            "",
            "                if (not normalized in self.allow_external",
            "                        and not self.allow_all_external):",
            "                    self.need_warn_external = True",
            "                    logger.debug(\"Not searching %s for files because external \"",
            "                                 \"urls are disallowed.\" % link)",
            "                    continue",
            "",
            "                if (link.trusted is not None",
            "                        and not link.trusted",
            "                        and not normalized in self.allow_insecure",
            "                        and not self.allow_all_insecure):",
            "                    logger.debug(\"Not searching %s for urls, it is an \"",
            "                                \"untrusted link and cannot produce safe or \"",
            "                                \"verifiable files.\" % link)",
            "                    self.need_warn_insecure = True",
            "                    continue",
            "",
            "                pending_queue.put(link)",
            "",
            "    _egg_fragment_re = re.compile(r'#egg=([^&]*)')",
            "    _egg_info_re = re.compile(r'([a-z0-9_.]+)-([a-z0-9_.-]+)', re.I)",
            "    _py_version_re = re.compile(r'-py([123]\\.?[0-9]?)$')",
            "",
            "    def _sort_links(self, links):",
            "        \"Returns elements of links in order, non-egg links first, egg links second, while eliminating duplicates\"",
            "        eggs, no_eggs = [], []",
            "        seen = set()",
            "        for link in links:",
            "            if link not in seen:",
            "                seen.add(link)",
            "                if link.egg_fragment:",
            "                    eggs.append(link)",
            "                else:",
            "                    no_eggs.append(link)",
            "        return no_eggs + eggs",
            "",
            "    def _package_versions(self, links, search_name):",
            "        for link in self._sort_links(links):",
            "            for v in self._link_package_versions(link, search_name):",
            "                yield v",
            "",
            "    def _known_extensions(self):",
            "        extensions = ('.tar.gz', '.tar.bz2', '.tar', '.tgz', '.zip')",
            "        if self.use_wheel:",
            "            return extensions + (wheel_ext,)",
            "        return extensions",
            "",
            "    def _link_package_versions(self, link, search_name):",
            "        \"\"\"",
            "        Return an iterable of triples (pkg_resources_version_key,",
            "        link, python_version) that can be extracted from the given",
            "        link.",
            "",
            "        Meant to be overridden by subclasses, not called by clients.",
            "        \"\"\"",
            "        platform = get_platform()",
            "",
            "        version = None",
            "        if link.egg_fragment:",
            "            egg_info = link.egg_fragment",
            "        else:",
            "            egg_info, ext = link.splitext()",
            "            if not ext:",
            "                if link not in self.logged_links:",
            "                    logger.debug('Skipping link %s; not a file' % link)",
            "                    self.logged_links.add(link)",
            "                return []",
            "            if egg_info.endswith('.tar'):",
            "                # Special double-extension case:",
            "                egg_info = egg_info[:-4]",
            "                ext = '.tar' + ext",
            "            if ext not in self._known_extensions():",
            "                if link not in self.logged_links:",
            "                    logger.debug('Skipping link %s; unknown archive format: %s' % (link, ext))",
            "                    self.logged_links.add(link)",
            "                return []",
            "            if \"macosx10\" in link.path and ext == '.zip':",
            "                if link not in self.logged_links:",
            "                    logger.debug('Skipping link %s; macosx10 one' % (link))",
            "                    self.logged_links.add(link)",
            "                return []",
            "            if link.wheel and link.wheel.name.lower() == search_name.lower():",
            "                version = link.wheel.version",
            "                if not link.wheel.supported():",
            "                    logger.debug('Skipping %s because it is not compatible with this Python' % link)",
            "                    return []",
            "",
            "                # This is a dirty hack to prevent installing Binary Wheels from",
            "                #   PyPI or one of its mirrors unless it is a Windows Binary",
            "                #   Wheel. This is paired with a change to PyPI disabling",
            "                #   uploads for the same. Once we have a mechanism for enabling",
            "                #   support for binary wheels on linux that deals with the",
            "                #   inherent problems of binary distribution this can be",
            "                #   removed.",
            "                comes_from = getattr(link, \"comes_from\", None)",
            "                if (not platform.startswith('win')",
            "                    and comes_from is not None",
            "                    and urlparse.urlparse(comes_from.url).netloc.endswith(",
            "                                                        \"pypi.python.org\")):",
            "                    if not link.wheel.supported(tags=supported_tags_noarch):",
            "                        logger.debug(",
            "                            \"Skipping %s because it is a pypi-hosted binary \"",
            "                            \"Wheel on an unsupported platform\" % link",
            "                        )",
            "                        return []",
            "",
            "        if not version:",
            "            version = self._egg_info_matches(egg_info, search_name, link)",
            "        if version is None:",
            "            logger.debug('Skipping link %s; wrong project name (not %s)' % (link, search_name))",
            "            return []",
            "",
            "        if (link.internal is not None",
            "                and not link.internal",
            "                and not normalize_name(search_name).lower() in self.allow_external",
            "                and not self.allow_all_external):",
            "            # We have a link that we are sure is external, so we should skip",
            "            #   it unless we are allowing externals",
            "            logger.debug(\"Skipping %s because it is externally hosted.\" % link)",
            "            self.need_warn_external = True",
            "            return []",
            "",
            "        if (link.verifiable is not None",
            "                and not link.verifiable",
            "                and not normalize_name(search_name).lower() in self.allow_insecure",
            "                and not self.allow_all_insecure):",
            "            # We have a link that we are sure we cannot verify it's integrity,",
            "            #   so we should skip it unless we are allowing unsafe installs",
            "            #   for this requirement.",
            "            logger.debug(\"Skipping %s because it is an insecure and \"",
            "                         \"unverifiable file.\" % link)",
            "            self.need_warn_insecure = True",
            "            return []",
            "",
            "        match = self._py_version_re.search(version)",
            "        if match:",
            "            version = version[:match.start()]",
            "            py_version = match.group(1)",
            "            if py_version != sys.version[:3]:",
            "                logger.debug('Skipping %s because Python version is incorrect' % link)",
            "                return []",
            "        logger.debug('Found link %s, version: %s' % (link, version))",
            "        return [(pkg_resources.parse_version(version),",
            "               link,",
            "               version)]",
            "",
            "    def _egg_info_matches(self, egg_info, search_name, link):",
            "        match = self._egg_info_re.search(egg_info)",
            "        if not match:",
            "            logger.debug('Could not parse version from link: %s' % link)",
            "            return None",
            "        name = match.group(0).lower()",
            "        # To match the \"safe\" name that pkg_resources creates:",
            "        name = name.replace('_', '-')",
            "        # project name and version must be separated by a dash",
            "        look_for = search_name.lower() + \"-\"",
            "        if name.startswith(look_for):",
            "            return match.group(0)[len(look_for):]",
            "        else:",
            "            return None",
            "",
            "    def _get_page(self, link, req):",
            "        return HTMLPage.get_page(link, req, cache=self.cache)",
            "",
            "    def _get_mirror_urls(self, mirrors=None, main_mirror_url=None):",
            "        \"\"\"Retrieves a list of URLs from the main mirror DNS entry",
            "        unless a list of mirror URLs are passed.",
            "        \"\"\"",
            "        if not mirrors:",
            "            mirrors = get_mirrors(main_mirror_url)",
            "            # Should this be made \"less random\"? E.g. netselect like?",
            "            random.shuffle(mirrors)",
            "",
            "        mirror_urls = set()",
            "        for mirror_url in mirrors:",
            "            mirror_url = mirror_url.rstrip('/')",
            "            # Make sure we have a valid URL",
            "            if not any([mirror_url.startswith(scheme) for scheme in [\"http://\", \"https://\", \"file://\"]]):",
            "                mirror_url = \"http://%s\" % mirror_url",
            "            if not mirror_url.endswith(\"/simple\"):",
            "                mirror_url = \"%s/simple\" % mirror_url",
            "            mirror_urls.add(mirror_url + '/')",
            "",
            "        return list(mirror_urls)",
            "",
            "",
            "class PageCache(object):",
            "    \"\"\"Cache of HTML pages\"\"\"",
            "",
            "    failure_limit = 3",
            "",
            "    def __init__(self):",
            "        self._failures = {}",
            "        self._pages = {}",
            "        self._archives = {}",
            "",
            "    def too_many_failures(self, url):",
            "        return self._failures.get(url, 0) >= self.failure_limit",
            "",
            "    def get_page(self, url):",
            "        return self._pages.get(url)",
            "",
            "    def is_archive(self, url):",
            "        return self._archives.get(url, False)",
            "",
            "    def set_is_archive(self, url, value=True):",
            "        self._archives[url] = value",
            "",
            "    def add_page_failure(self, url, level):",
            "        self._failures[url] = self._failures.get(url, 0)+level",
            "",
            "    def add_page(self, urls, page):",
            "        for url in urls:",
            "            self._pages[url] = page",
            "",
            "",
            "class HTMLPage(object):",
            "    \"\"\"Represents one page, along with its URL\"\"\"",
            "",
            "    ## FIXME: these regexes are horrible hacks:",
            "    _homepage_re = re.compile(r'<th>\\s*home\\s*page', re.I)",
            "    _download_re = re.compile(r'<th>\\s*download\\s+url', re.I)",
            "    _href_re = re.compile('href=(?:\"([^\"]*)\"|\\'([^\\']*)\\'|([^>\\\\s\\\\n]*))', re.I|re.S)",
            "",
            "    def __init__(self, content, url, headers=None, trusted=None):",
            "        self.content = content",
            "        self.parsed = html5lib.parse(self.content, namespaceHTMLElements=False)",
            "        self.url = url",
            "        self.headers = headers",
            "        self.trusted = trusted",
            "",
            "    def __str__(self):",
            "        return self.url",
            "",
            "    @classmethod",
            "    def get_page(cls, link, req, cache=None, skip_archives=True):",
            "        url = link.url",
            "        url = url.split('#', 1)[0]",
            "        if cache.too_many_failures(url):",
            "            return None",
            "",
            "        # Check for VCS schemes that do not support lookup as web pages.",
            "        from pip.vcs import VcsSupport",
            "        for scheme in VcsSupport.schemes:",
            "            if url.lower().startswith(scheme) and url[len(scheme)] in '+:':",
            "                logger.debug('Cannot look at %(scheme)s URL %(link)s' % locals())",
            "                return None",
            "",
            "        if cache is not None:",
            "            inst = cache.get_page(url)",
            "            if inst is not None:",
            "                return inst",
            "        try:",
            "            if skip_archives:",
            "                if cache is not None:",
            "                    if cache.is_archive(url):",
            "                        return None",
            "                filename = link.filename",
            "                for bad_ext in ['.tar', '.tar.gz', '.tar.bz2', '.tgz', '.zip']:",
            "                    if filename.endswith(bad_ext):",
            "                        content_type = cls._get_content_type(url)",
            "                        if content_type.lower().startswith('text/html'):",
            "                            break",
            "                        else:",
            "                            logger.debug('Skipping page %s because of Content-Type: %s' % (link, content_type))",
            "                            if cache is not None:",
            "                                cache.set_is_archive(url)",
            "                            return None",
            "            logger.debug('Getting page %s' % url)",
            "",
            "            # Tack index.html onto file:// URLs that point to directories",
            "            (scheme, netloc, path, params, query, fragment) = urlparse.urlparse(url)",
            "            if scheme == 'file' and os.path.isdir(url2pathname(path)):",
            "                # add trailing slash if not present so urljoin doesn't trim final segment",
            "                if not url.endswith('/'):",
            "                    url += '/'",
            "                url = urlparse.urljoin(url, 'index.html')",
            "                logger.debug(' file: URL is directory, getting %s' % url)",
            "",
            "            resp = urlopen(url)",
            "",
            "            real_url = geturl(resp)",
            "            headers = resp.info()",
            "            contents = resp.read()",
            "            encoding = headers.get('Content-Encoding', None)",
            "            #XXX need to handle exceptions and add testing for this",
            "            if encoding is not None:",
            "                if encoding == 'gzip':",
            "                    contents = gzip.GzipFile(fileobj=BytesIO(contents)).read()",
            "                if encoding == 'deflate':",
            "                    contents = zlib.decompress(contents)",
            "",
            "            # The check for archives above only works if the url ends with",
            "            #   something that looks like an archive. However that is not a",
            "            #   requirement. For instance http://sourceforge.net/projects/docutils/files/docutils/0.8.1/docutils-0.8.1.tar.gz/download",
            "            #   redirects to http://superb-dca3.dl.sourceforge.net/project/docutils/docutils/0.8.1/docutils-0.8.1.tar.gz",
            "            #   Unless we issue a HEAD request on every url we cannot know",
            "            #   ahead of time for sure if something is HTML or not. However we",
            "            #   can check after we've downloaded it.",
            "            content_type = headers.get('Content-Type', 'unknown')",
            "            if not content_type.lower().startswith(\"text/html\"):",
            "                logger.debug('Skipping page %s because of Content-Type: %s' %",
            "                                            (link, content_type))",
            "                if cache is not None:",
            "                    cache.set_is_archive(url)",
            "                return None",
            "",
            "            inst = cls(u(contents), real_url, headers, trusted=link.trusted)",
            "        except (HTTPError, URLError, socket.timeout, socket.error, OSError, WindowsError):",
            "            e = sys.exc_info()[1]",
            "            desc = str(e)",
            "            if isinstance(e, socket.timeout):",
            "                log_meth = logger.info",
            "                level =1",
            "                desc = 'timed out'",
            "            elif isinstance(e, URLError):",
            "                #ssl/certificate error",
            "                if hasattr(e, 'reason') and (isinstance(e.reason, ssl.SSLError) or isinstance(e.reason, CertificateError)):",
            "                    desc = 'There was a problem confirming the ssl certificate: %s' % e",
            "                    log_meth = logger.notify",
            "                else:",
            "                    log_meth = logger.info",
            "                if hasattr(e, 'reason') and isinstance(e.reason, socket.timeout):",
            "                    desc = 'timed out'",
            "                    level = 1",
            "                else:",
            "                    level = 2",
            "            elif isinstance(e, HTTPError) and e.code == 404:",
            "                ## FIXME: notify?",
            "                log_meth = logger.info",
            "                level = 2",
            "            else:",
            "                log_meth = logger.info",
            "                level = 1",
            "            log_meth('Could not fetch URL %s: %s' % (link, desc))",
            "            log_meth('Will skip URL %s when looking for download links for %s' % (link.url, req))",
            "            if cache is not None:",
            "                cache.add_page_failure(url, level)",
            "            return None",
            "        if cache is not None:",
            "            cache.add_page([url, real_url], inst)",
            "        return inst",
            "",
            "    @staticmethod",
            "    def _get_content_type(url):",
            "        \"\"\"Get the Content-Type of the given url, using a HEAD request\"\"\"",
            "        scheme, netloc, path, query, fragment = urlparse.urlsplit(url)",
            "        if not scheme in ('http', 'https', 'ftp', 'ftps'):",
            "            ## FIXME: some warning or something?",
            "            ## assertion error?",
            "            return ''",
            "        req = Urllib2HeadRequest(url, headers={'Host': netloc})",
            "        resp = urlopen(req)",
            "        try:",
            "            if hasattr(resp, 'code') and resp.code != 200 and scheme not in ('ftp', 'ftps'):",
            "                ## FIXME: doesn't handle redirects",
            "                return ''",
            "            return resp.info().get('content-type', '')",
            "        finally:",
            "            resp.close()",
            "",
            "    @property",
            "    def api_version(self):",
            "        if not hasattr(self, \"_api_version\"):",
            "            _api_version = None",
            "",
            "            metas = [x for x in self.parsed.findall(\".//meta\")",
            "                        if x.get(\"name\", \"\").lower() == \"api-version\"]",
            "            if metas:",
            "                try:",
            "                    _api_version = int(metas[0].get(\"value\", None))",
            "                except (TypeError, ValueError):",
            "                    _api_version = None",
            "            self._api_version = _api_version",
            "        return self._api_version",
            "",
            "    @property",
            "    def base_url(self):",
            "        if not hasattr(self, \"_base_url\"):",
            "            base = self.parsed.find(\".//base\")",
            "            if base is not None and base.get(\"href\"):",
            "                self._base_url = base.get(\"href\")",
            "            else:",
            "                self._base_url = self.url",
            "        return self._base_url",
            "",
            "    @property",
            "    def links(self):",
            "        \"\"\"Yields all links in the page\"\"\"",
            "        for anchor in self.parsed.findall(\".//a\"):",
            "            if anchor.get(\"href\"):",
            "                href = anchor.get(\"href\")",
            "                url = self.clean_link(urlparse.urljoin(self.base_url, href))",
            "",
            "                # Determine if this link is internal. If that distinction",
            "                #   doesn't make sense in this context, then we don't make",
            "                #   any distinction.",
            "                internal = None",
            "                if self.api_version and self.api_version >= 2:",
            "                    # Only api_versions >= 2 have a distinction between",
            "                    #   external and internal links",
            "                    internal = bool(anchor.get(\"rel\")",
            "                                and \"internal\" in anchor.get(\"rel\").split())",
            "",
            "                yield Link(url, self, internal=internal)",
            "",
            "    def rel_links(self):",
            "        for url in self.explicit_rel_links():",
            "            yield url",
            "        for url in self.scraped_rel_links():",
            "            yield url",
            "",
            "    def explicit_rel_links(self, rels=('homepage', 'download')):",
            "        \"\"\"Yields all links with the given relations\"\"\"",
            "        rels = set(rels)",
            "",
            "        for anchor in self.parsed.findall(\".//a\"):",
            "            if anchor.get(\"rel\") and anchor.get(\"href\"):",
            "                found_rels = set(anchor.get(\"rel\").split())",
            "                # Determine the intersection between what rels were found and",
            "                #   what rels were being looked for",
            "                if found_rels & rels:",
            "                    href = anchor.get(\"href\")",
            "                    url = self.clean_link(urlparse.urljoin(self.base_url, href))",
            "                    yield Link(url, self, trusted=False)",
            "",
            "    def scraped_rel_links(self):",
            "        # Can we get rid of this horrible horrible method?",
            "        for regex in (self._homepage_re, self._download_re):",
            "            match = regex.search(self.content)",
            "            if not match:",
            "                continue",
            "            href_match = self._href_re.search(self.content, pos=match.end())",
            "            if not href_match:",
            "                continue",
            "            url = href_match.group(1) or href_match.group(2) or href_match.group(3)",
            "            if not url:",
            "                continue",
            "            url = self.clean_link(urlparse.urljoin(self.base_url, url))",
            "            yield Link(url, self, trusted=False)",
            "",
            "    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\\\|-]', re.I)",
            "",
            "    def clean_link(self, url):",
            "        \"\"\"Makes sure a link is fully encoded.  That is, if a ' ' shows up in",
            "        the link, it will be rewritten to %20 (while not over-quoting",
            "        % or other characters).\"\"\"",
            "        return self._clean_re.sub(",
            "            lambda match: '%%%2x' % ord(match.group(0)), url)",
            "",
            "",
            "class Link(object):",
            "",
            "    def __init__(self, url, comes_from=None, internal=None, trusted=None):",
            "        self.url = url",
            "        self.comes_from = comes_from",
            "        self.internal = internal",
            "        self.trusted = trusted",
            "",
            "        # Set whether it's a wheel",
            "        self.wheel = None",
            "        if url != Inf and self.splitext()[1] == wheel_ext:",
            "            self.wheel = Wheel(self.filename)",
            "",
            "    def __str__(self):",
            "        if self.comes_from:",
            "            return '%s (from %s)' % (self.url, self.comes_from)",
            "        else:",
            "            return str(self.url)",
            "",
            "    def __repr__(self):",
            "        return '<Link %s>' % self",
            "",
            "    def __eq__(self, other):",
            "        return self.url == other.url",
            "",
            "    def __ne__(self, other):",
            "        return self.url != other.url",
            "",
            "    def __lt__(self, other):",
            "        return self.url < other.url",
            "",
            "    def __le__(self, other):",
            "        return self.url <= other.url",
            "",
            "    def __gt__(self, other):",
            "        return self.url > other.url",
            "",
            "    def __ge__(self, other):",
            "        return self.url >= other.url",
            "",
            "    def __hash__(self):",
            "        return hash(self.url)",
            "",
            "    @property",
            "    def filename(self):",
            "        _, netloc, path, _, _ = urlparse.urlsplit(self.url)",
            "        name = posixpath.basename(path.rstrip('/')) or netloc",
            "        assert name, ('URL %r produced no filename' % self.url)",
            "        return name",
            "",
            "    @property",
            "    def scheme(self):",
            "        return urlparse.urlsplit(self.url)[0]",
            "",
            "    @property",
            "    def path(self):",
            "        return urlparse.urlsplit(self.url)[2]",
            "",
            "    def splitext(self):",
            "        return splitext(posixpath.basename(self.path.rstrip('/')))",
            "",
            "    @property",
            "    def url_without_fragment(self):",
            "        scheme, netloc, path, query, fragment = urlparse.urlsplit(self.url)",
            "        return urlparse.urlunsplit((scheme, netloc, path, query, None))",
            "",
            "    _egg_fragment_re = re.compile(r'#egg=([^&]*)')",
            "",
            "    @property",
            "    def egg_fragment(self):",
            "        match = self._egg_fragment_re.search(self.url)",
            "        if not match:",
            "            return None",
            "        return match.group(1)",
            "",
            "    _hash_re = re.compile(r'(sha1|sha224|sha384|sha256|sha512|md5)=([a-f0-9]+)')",
            "",
            "    @property",
            "    def hash(self):",
            "        match = self._hash_re.search(self.url)",
            "        if match:",
            "            return match.group(2)",
            "        return None",
            "",
            "    @property",
            "    def hash_name(self):",
            "        match = self._hash_re.search(self.url)",
            "        if match:",
            "            return match.group(1)",
            "        return None",
            "",
            "    @property",
            "    def show_url(self):",
            "        return posixpath.basename(self.url.split('#', 1)[0].split('?', 1)[0])",
            "",
            "    @property",
            "    def verifiable(self):",
            "        \"\"\"",
            "        Returns True if this link can be verified after download, False if it",
            "        cannot, and None if we cannot determine.",
            "        \"\"\"",
            "        trusted = self.trusted or getattr(self.comes_from, \"trusted\", None)",
            "        if trusted is not None and trusted:",
            "            # This link came from a trusted source. It *may* be verifiable but",
            "            #   first we need to see if this page is operating under the new",
            "            #   API version.",
            "            try:",
            "                api_version = getattr(self.comes_from, \"api_version\", None)",
            "                api_version = int(api_version)",
            "            except (ValueError, TypeError):",
            "                api_version = None",
            "",
            "            if api_version is None or api_version <= 1:",
            "                # This link is either trusted, or it came from a trusted,",
            "                #   however it is not operating under the API version 2 so",
            "                #   we can't make any claims about if it's safe or not",
            "                return",
            "",
            "            if self.hash:",
            "                # This link came from a trusted source and it has a hash, so we",
            "                #   can consider it safe.",
            "                return True",
            "            else:",
            "                # This link came from a trusted source, using the new API",
            "                #   version, and it does not have a hash. It is NOT verifiable",
            "                return False",
            "        elif trusted is not None:",
            "            # This link came from an untrusted source and we cannot trust it",
            "            return False",
            "",
            "#An \"Infinite Link\" that compares greater than other links",
            "InfLink = Link(Inf) #this object is not currently used as a sortable",
            "",
            "",
            "def get_requirement_from_url(url):",
            "    \"\"\"Get a requirement from the URL, if possible.  This looks for #egg",
            "    in the URL\"\"\"",
            "    link = Link(url)",
            "    egg_info = link.egg_fragment",
            "    if not egg_info:",
            "        egg_info = splitext(link.filename)[0]",
            "    return package_to_requirement(egg_info)",
            "",
            "",
            "def package_to_requirement(package_name):",
            "    \"\"\"Translate a name like Foo-1.2 to Foo==1.3\"\"\"",
            "    match = re.search(r'^(.*?)-(dev|\\d.*)', package_name)",
            "    if match:",
            "        name = match.group(1)",
            "        version = match.group(2)",
            "    else:",
            "        name = package_name",
            "        version = ''",
            "    if version:",
            "        return '%s==%s' % (name, version)",
            "    else:",
            "        return name",
            "",
            "",
            "def get_mirrors(hostname=None):",
            "    \"\"\"Return the list of mirrors from the last record found on the DNS",
            "    entry::",
            "",
            "    >>> from pip.index import get_mirrors",
            "    >>> get_mirrors()",
            "    ['a.pypi.python.org', 'b.pypi.python.org', 'c.pypi.python.org',",
            "    'd.pypi.python.org']",
            "",
            "    Originally written for the distutils2 project by Alexis Metaireau.",
            "    \"\"\"",
            "    if hostname is None:",
            "        hostname = DEFAULT_MIRROR_HOSTNAME",
            "",
            "    # return the last mirror registered on PyPI.",
            "    last_mirror_hostname = None",
            "    try:",
            "        last_mirror_hostname = socket.gethostbyname_ex(hostname)[0]",
            "    except socket.gaierror:",
            "        return []",
            "    if not last_mirror_hostname or last_mirror_hostname == DEFAULT_MIRROR_HOSTNAME:",
            "        last_mirror_hostname = \"z.pypi.python.org\"",
            "    end_letter = last_mirror_hostname.split(\".\", 1)",
            "",
            "    # determine the list from the last one.",
            "    return [\"%s.%s\" % (s, end_letter[1]) for s in string_range(end_letter[0])]",
            "",
            "",
            "def string_range(last):",
            "    \"\"\"Compute the range of string between \"a\" and last.",
            "",
            "    This works for simple \"a to z\" lists, but also for \"a to zz\" lists.",
            "    \"\"\"",
            "    for k in range(len(last)):",
            "        for x in product(string.ascii_lowercase, repeat=k+1):",
            "            result = ''.join(x)",
            "            yield result",
            "            if result == last:",
            "                return"
        ],
        "afterPatchFile": [
            "\"\"\"Routines related to PyPI, indexes\"\"\"",
            "",
            "import sys",
            "import os",
            "import re",
            "import gzip",
            "import mimetypes",
            "import posixpath",
            "import pkg_resources",
            "import random",
            "import socket",
            "import ssl",
            "import string",
            "import zlib",
            "",
            "try:",
            "    import threading",
            "except ImportError:",
            "    import dummy_threading as threading",
            "",
            "from pip.log import logger",
            "from pip.util import Inf, normalize_name, splitext, is_prerelease",
            "from pip.exceptions import DistributionNotFound, BestVersionAlreadyInstalled,\\",
            "    InstallationError",
            "from pip.backwardcompat import (WindowsError, BytesIO,",
            "                                Queue, urlparse,",
            "                                URLError, HTTPError, u,",
            "                                product, url2pathname,",
            "                                Empty as QueueEmpty)",
            "from pip.backwardcompat import CertificateError",
            "from pip.download import urlopen, path_to_url2, url_to_path, geturl, Urllib2HeadRequest",
            "from pip.wheel import Wheel, wheel_ext, wheel_setuptools_support, setuptools_requirement",
            "from pip.pep425tags import supported_tags, supported_tags_noarch, get_platform",
            "from pip.vendor import html5lib",
            "",
            "__all__ = ['PackageFinder']",
            "",
            "",
            "DEFAULT_MIRROR_HOSTNAME = \"last.pypi.python.org\"",
            "",
            "",
            "class PackageFinder(object):",
            "    \"\"\"This finds packages.",
            "",
            "    This is meant to match easy_install's technique for looking for",
            "    packages, by reading pages and looking for appropriate links",
            "    \"\"\"",
            "",
            "    def __init__(self, find_links, index_urls,",
            "            use_wheel=False, allow_external=[], allow_insecure=[],",
            "            allow_all_external=False, allow_all_insecure=False,",
            "            allow_all_prereleases=False):",
            "        self.find_links = find_links",
            "        self.index_urls = index_urls",
            "        self.dependency_links = []",
            "        self.cache = PageCache()",
            "        # These are boring links that have already been logged somehow:",
            "        self.logged_links = set()",
            "",
            "        self.use_wheel = use_wheel",
            "",
            "        # Do we allow (safe and verifiable) externally hosted files?",
            "        self.allow_external = set(normalize_name(n) for n in allow_external)",
            "",
            "        # Which names are allowed to install insecure and unverifiable files?",
            "        self.allow_insecure = set(normalize_name(n) for n in allow_insecure)",
            "",
            "        # Do we allow all (safe and verifiable) externally hosted files?",
            "        self.allow_all_external = allow_all_external",
            "",
            "        # Do we allow unsafe and unverifiable files?",
            "        self.allow_all_insecure = allow_all_insecure",
            "",
            "        # Stores if we ignored any external links so that we can instruct",
            "        #   end users how to install them if no distributions are available",
            "        self.need_warn_external = False",
            "",
            "        # Stores if we ignored any unsafe links so that we can instruct",
            "        #   end users how to install them if no distributions are available",
            "        self.need_warn_insecure = False",
            "",
            "        # Do we want to allow _all_ pre-releases?",
            "        self.allow_all_prereleases = allow_all_prereleases",
            "",
            "    @property",
            "    def use_wheel(self):",
            "        return self._use_wheel",
            "",
            "    @use_wheel.setter",
            "    def use_wheel(self, value):",
            "        self._use_wheel = value",
            "        if self._use_wheel and not wheel_setuptools_support():",
            "            raise InstallationError(\"pip's wheel support requires %s.\" % setuptools_requirement)",
            "",
            "    def add_dependency_links(self, links):",
            "        ## FIXME: this shouldn't be global list this, it should only",
            "        ## apply to requirements of the package that specifies the",
            "        ## dependency_links value",
            "        ## FIXME: also, we should track comes_from (i.e., use Link)",
            "        self.dependency_links.extend(links)",
            "",
            "    def _sort_locations(self, locations):",
            "        \"\"\"",
            "        Sort locations into \"files\" (archives) and \"urls\", and return",
            "        a pair of lists (files,urls)",
            "        \"\"\"",
            "        files = []",
            "        urls = []",
            "",
            "        # puts the url for the given file path into the appropriate list",
            "        def sort_path(path):",
            "            url = path_to_url2(path)",
            "            if mimetypes.guess_type(url, strict=False)[0] == 'text/html':",
            "                urls.append(url)",
            "            else:",
            "                files.append(url)",
            "",
            "        for url in locations:",
            "",
            "            is_local_path = os.path.exists(url)",
            "            is_file_url = url.startswith('file:')",
            "            is_find_link = url in self.find_links",
            "",
            "            if is_local_path or is_file_url:",
            "                if is_local_path:",
            "                    path = url",
            "                else:",
            "                    path = url_to_path(url)",
            "                if is_find_link and os.path.isdir(path):",
            "                    path = os.path.realpath(path)",
            "                    for item in os.listdir(path):",
            "                        sort_path(os.path.join(path, item))",
            "                elif is_file_url and os.path.isdir(path):",
            "                    urls.append(url)",
            "                elif os.path.isfile(path):",
            "                    sort_path(path)",
            "            else:",
            "                urls.append(url)",
            "",
            "        return files, urls",
            "",
            "    def _link_sort_key(self, link_tuple):",
            "        \"\"\"",
            "        Function used to generate link sort key for link tuples.",
            "        The greater the return value, the more preferred it is.",
            "        If not finding wheels, then sorted by version only.",
            "        If finding wheels, then the sort order is by version, then:",
            "          1. existing installs",
            "          2. wheels ordered via Wheel.support_index_min()",
            "          3. source archives",
            "        Note: it was considered to embed this logic into the Link",
            "              comparison operators, but then different sdist links",
            "              with the same version, would have to be considered equal",
            "        \"\"\"",
            "        parsed_version, link, _ = link_tuple",
            "        if self.use_wheel:",
            "            support_num = len(supported_tags)",
            "            if link == InfLink: # existing install",
            "                pri = 1",
            "            elif link.wheel:",
            "                # all wheel links are known to be supported at this stage",
            "                pri = -(link.wheel.support_index_min())",
            "            else: # sdist",
            "                pri = -(support_num)",
            "            return (parsed_version, pri)",
            "        else:",
            "            return parsed_version",
            "",
            "    def _sort_versions(self, applicable_versions):",
            "        \"\"\"",
            "        Bring the latest version (and wheels) to the front, but maintain the existing ordering as secondary.",
            "        See the docstring for `_link_sort_key` for details.",
            "        This function is isolated for easier unit testing.",
            "        \"\"\"",
            "        return sorted(applicable_versions, key=self._link_sort_key, reverse=True)",
            "",
            "    def find_requirement(self, req, upgrade):",
            "",
            "        def mkurl_pypi_url(url):",
            "            loc = posixpath.join(url, url_name)",
            "            # For maximum compatibility with easy_install, ensure the path",
            "            # ends in a trailing slash.  Although this isn't in the spec",
            "            # (and PyPI can handle it without the slash) some other index",
            "            # implementations might break if they relied on easy_install's behavior.",
            "            if not loc.endswith('/'):",
            "                loc = loc + '/'",
            "            return loc",
            "",
            "        url_name = req.url_name",
            "        # Only check main index if index URL is given:",
            "        main_index_url = None",
            "        if self.index_urls:",
            "            # Check that we have the url_name correctly spelled:",
            "            main_index_url = Link(mkurl_pypi_url(self.index_urls[0]), trusted=True)",
            "            # This will also cache the page, so it's okay that we get it again later:",
            "            page = self._get_page(main_index_url, req)",
            "            if page is None:",
            "                url_name = self._find_url_name(Link(self.index_urls[0], trusted=True), url_name, req) or req.url_name",
            "",
            "        if url_name is not None:",
            "            locations = [",
            "                mkurl_pypi_url(url)",
            "                for url in self.index_urls] + self.find_links",
            "        else:",
            "            locations = list(self.find_links)",
            "        for version in req.absolute_versions:",
            "            if url_name is not None and main_index_url is not None:",
            "                locations = [",
            "                    posixpath.join(main_index_url.url, version)] + locations",
            "",
            "        file_locations, url_locations = self._sort_locations(locations)",
            "        _flocations, _ulocations = self._sort_locations(self.dependency_links)",
            "        file_locations.extend(_flocations)",
            "",
            "        # We trust every url that the user has given us whether it was given",
            "        #   via --index-url or --find-links",
            "        locations = [Link(url, trusted=True) for url in url_locations]",
            "",
            "        # We explicitly do not trust links that came from dependency_links",
            "        locations.extend([Link(url) for url in _ulocations])",
            "",
            "        logger.debug('URLs to search for versions for %s:' % req)",
            "        for location in locations:",
            "            logger.debug('* %s' % location)",
            "        found_versions = []",
            "        found_versions.extend(",
            "            self._package_versions(",
            "                # We trust every directly linked archive in find_links",
            "                [Link(url, '-f', trusted=True) for url in self.find_links], req.name.lower()))",
            "        page_versions = []",
            "        for page in self._get_pages(locations, req):",
            "            logger.debug('Analyzing links from page %s' % page.url)",
            "            logger.indent += 2",
            "            try:",
            "                page_versions.extend(self._package_versions(page.links, req.name.lower()))",
            "            finally:",
            "                logger.indent -= 2",
            "        dependency_versions = list(self._package_versions(",
            "            [Link(url) for url in self.dependency_links], req.name.lower()))",
            "        if dependency_versions:",
            "            logger.info('dependency_links found: %s' % ', '.join([link.url for parsed, link, version in dependency_versions]))",
            "        file_versions = list(self._package_versions(",
            "                [Link(url) for url in file_locations], req.name.lower()))",
            "        if not found_versions and not page_versions and not dependency_versions and not file_versions:",
            "            logger.fatal('Could not find any downloads that satisfy the requirement %s' % req)",
            "",
            "            if self.need_warn_external:",
            "                logger.warn(\"Some externally hosted files were ignored (use \"",
            "                            \"--allow-external %s to allow).\" % req.name)",
            "",
            "            if self.need_warn_insecure:",
            "                logger.warn(\"Some insecure and unverifiable files were ignored\"",
            "                            \" (use --allow-insecure %s to allow).\" % req.name)",
            "",
            "            raise DistributionNotFound('No distributions at all found for %s' % req)",
            "        installed_version = []",
            "        if req.satisfied_by is not None:",
            "            installed_version = [(req.satisfied_by.parsed_version, InfLink, req.satisfied_by.version)]",
            "        if file_versions:",
            "            file_versions.sort(reverse=True)",
            "            logger.info('Local files found: %s' % ', '.join([url_to_path(link.url) for parsed, link, version in file_versions]))",
            "        #this is an intentional priority ordering",
            "        all_versions = installed_version + file_versions + found_versions + page_versions + dependency_versions",
            "        applicable_versions = []",
            "        for (parsed_version, link, version) in all_versions:",
            "            if version not in req.req:",
            "                logger.info(\"Ignoring link %s, version %s doesn't match %s\"",
            "                            % (link, version, ','.join([''.join(s) for s in req.req.specs])))",
            "                continue",
            "            elif is_prerelease(version) and not (self.allow_all_prereleases or req.prereleases):",
            "                # If this version isn't the already installed one, then",
            "                #   ignore it if it's a pre-release.",
            "                if link is not InfLink:",
            "                    logger.info(\"Ignoring link %s, version %s is a pre-release (use --pre to allow).\" % (link, version))",
            "                    continue",
            "            applicable_versions.append((parsed_version, link, version))",
            "        applicable_versions = self._sort_versions(applicable_versions)",
            "        existing_applicable = bool([link for parsed_version, link, version in applicable_versions if link is InfLink])",
            "        if not upgrade and existing_applicable:",
            "            if applicable_versions[0][1] is InfLink:",
            "                logger.info('Existing installed version (%s) is most up-to-date and satisfies requirement'",
            "                            % req.satisfied_by.version)",
            "            else:",
            "                logger.info('Existing installed version (%s) satisfies requirement (most up-to-date version is %s)'",
            "                            % (req.satisfied_by.version, applicable_versions[0][2]))",
            "            return None",
            "        if not applicable_versions:",
            "            logger.fatal('Could not find a version that satisfies the requirement %s (from versions: %s)'",
            "                         % (req, ', '.join([version for parsed_version, link, version in all_versions])))",
            "",
            "            if self.need_warn_external:",
            "                logger.warn(\"Some externally hosted files were ignored (use \"",
            "                            \"--allow-external to allow).\")",
            "",
            "            if self.need_warn_insecure:",
            "                logger.warn(\"Some insecure and unverifiable files were ignored\"",
            "                            \" (use --allow-insecure %s to allow).\" % req.name)",
            "",
            "            raise DistributionNotFound('No distributions matching the version for %s' % req)",
            "        if applicable_versions[0][1] is InfLink:",
            "            # We have an existing version, and its the best version",
            "            logger.info('Installed version (%s) is most up-to-date (past versions: %s)'",
            "                        % (req.satisfied_by.version, ', '.join([version for parsed_version, link, version in applicable_versions[1:]]) or 'none'))",
            "            raise BestVersionAlreadyInstalled",
            "        if len(applicable_versions) > 1:",
            "            logger.info('Using version %s (newest of versions: %s)' %",
            "                        (applicable_versions[0][2], ', '.join([version for parsed_version, link, version in applicable_versions])))",
            "",
            "        selected_version = applicable_versions[0][1]",
            "",
            "        if (selected_version.internal is not None",
            "                and not selected_version.internal):",
            "            logger.warn(\"%s an externally hosted file and may be \"",
            "                        \"unreliable\" % req.name)",
            "",
            "        if (selected_version.verifiable is not None",
            "                and not selected_version.verifiable):",
            "            logger.warn(\"%s is potentially insecure and \"",
            "                        \"unverifiable.\" % req.name)",
            "",
            "        return selected_version",
            "",
            "",
            "    def _find_url_name(self, index_url, url_name, req):",
            "        \"\"\"Finds the true URL name of a package, when the given name isn't quite correct.",
            "        This is usually used to implement case-insensitivity.\"\"\"",
            "        if not index_url.url.endswith('/'):",
            "            # Vaguely part of the PyPI API... weird but true.",
            "            ## FIXME: bad to modify this?",
            "            index_url.url += '/'",
            "        page = self._get_page(index_url, req)",
            "        if page is None:",
            "            logger.fatal('Cannot fetch index base URL %s' % index_url)",
            "            return",
            "        norm_name = normalize_name(req.url_name)",
            "        for link in page.links:",
            "            base = posixpath.basename(link.path.rstrip('/'))",
            "            if norm_name == normalize_name(base):",
            "                logger.notify('Real name of requirement %s is %s' % (url_name, base))",
            "                return base",
            "        return None",
            "",
            "    def _get_pages(self, locations, req):",
            "        \"\"\"Yields (page, page_url) from the given locations, skipping",
            "        locations that have errors, and adding download/homepage links\"\"\"",
            "        pending_queue = Queue()",
            "        for location in locations:",
            "            pending_queue.put(location)",
            "        done = []",
            "        seen = set()",
            "        threads = []",
            "        for i in range(min(10, len(locations))):",
            "            t = threading.Thread(target=self._get_queued_page, args=(req, pending_queue, done, seen))",
            "            t.setDaemon(True)",
            "            threads.append(t)",
            "            t.start()",
            "        for t in threads:",
            "            t.join()",
            "        return done",
            "",
            "    _log_lock = threading.Lock()",
            "",
            "    def _get_queued_page(self, req, pending_queue, done, seen):",
            "        while 1:",
            "            try:",
            "                location = pending_queue.get(False)",
            "            except QueueEmpty:",
            "                return",
            "            if location in seen:",
            "                continue",
            "            seen.add(location)",
            "            page = self._get_page(location, req)",
            "            if page is None:",
            "                continue",
            "            done.append(page)",
            "            for link in page.rel_links():",
            "                normalized = normalize_name(req.name).lower()",
            "",
            "                if (not normalized in self.allow_external",
            "                        and not self.allow_all_external):",
            "                    self.need_warn_external = True",
            "                    logger.debug(\"Not searching %s for files because external \"",
            "                                 \"urls are disallowed.\" % link)",
            "                    continue",
            "",
            "                if (link.trusted is not None",
            "                        and not link.trusted",
            "                        and not normalized in self.allow_insecure",
            "                        and not self.allow_all_insecure):",
            "                    logger.debug(\"Not searching %s for urls, it is an \"",
            "                                \"untrusted link and cannot produce safe or \"",
            "                                \"verifiable files.\" % link)",
            "                    self.need_warn_insecure = True",
            "                    continue",
            "",
            "                pending_queue.put(link)",
            "",
            "    _egg_fragment_re = re.compile(r'#egg=([^&]*)')",
            "    _egg_info_re = re.compile(r'([a-z0-9_.]+)-([a-z0-9_.-]+)', re.I)",
            "    _py_version_re = re.compile(r'-py([123]\\.?[0-9]?)$')",
            "",
            "    def _sort_links(self, links):",
            "        \"Returns elements of links in order, non-egg links first, egg links second, while eliminating duplicates\"",
            "        eggs, no_eggs = [], []",
            "        seen = set()",
            "        for link in links:",
            "            if link not in seen:",
            "                seen.add(link)",
            "                if link.egg_fragment:",
            "                    eggs.append(link)",
            "                else:",
            "                    no_eggs.append(link)",
            "        return no_eggs + eggs",
            "",
            "    def _package_versions(self, links, search_name):",
            "        for link in self._sort_links(links):",
            "            for v in self._link_package_versions(link, search_name):",
            "                yield v",
            "",
            "    def _known_extensions(self):",
            "        extensions = ('.tar.gz', '.tar.bz2', '.tar', '.tgz', '.zip')",
            "        if self.use_wheel:",
            "            return extensions + (wheel_ext,)",
            "        return extensions",
            "",
            "    def _link_package_versions(self, link, search_name):",
            "        \"\"\"",
            "        Return an iterable of triples (pkg_resources_version_key,",
            "        link, python_version) that can be extracted from the given",
            "        link.",
            "",
            "        Meant to be overridden by subclasses, not called by clients.",
            "        \"\"\"",
            "        platform = get_platform()",
            "",
            "        version = None",
            "        if link.egg_fragment:",
            "            egg_info = link.egg_fragment",
            "        else:",
            "            egg_info, ext = link.splitext()",
            "            if not ext:",
            "                if link not in self.logged_links:",
            "                    logger.debug('Skipping link %s; not a file' % link)",
            "                    self.logged_links.add(link)",
            "                return []",
            "            if egg_info.endswith('.tar'):",
            "                # Special double-extension case:",
            "                egg_info = egg_info[:-4]",
            "                ext = '.tar' + ext",
            "            if ext not in self._known_extensions():",
            "                if link not in self.logged_links:",
            "                    logger.debug('Skipping link %s; unknown archive format: %s' % (link, ext))",
            "                    self.logged_links.add(link)",
            "                return []",
            "            if \"macosx10\" in link.path and ext == '.zip':",
            "                if link not in self.logged_links:",
            "                    logger.debug('Skipping link %s; macosx10 one' % (link))",
            "                    self.logged_links.add(link)",
            "                return []",
            "            if link.wheel and link.wheel.name.lower() == search_name.lower():",
            "                version = link.wheel.version",
            "                if not link.wheel.supported():",
            "                    logger.debug('Skipping %s because it is not compatible with this Python' % link)",
            "                    return []",
            "",
            "                # This is a dirty hack to prevent installing Binary Wheels from",
            "                #   PyPI unless it is a Windows Binary Wheel. This is paired",
            "                #   with a change to PyPI disabling uploads for the same. Once",
            "                #   we have a mechanism for enabling support for binary wheels",
            "                #   on linux that deals with the inherent problems of binary",
            "                #   distribution this can be removed.",
            "                comes_from = getattr(link, \"comes_from\", None)",
            "                if (not platform.startswith('win')",
            "                    and comes_from is not None",
            "                    and urlparse.urlparse(comes_from.url).netloc.endswith(",
            "                                                        \"pypi.python.org\")):",
            "                    if not link.wheel.supported(tags=supported_tags_noarch):",
            "                        logger.debug(",
            "                            \"Skipping %s because it is a pypi-hosted binary \"",
            "                            \"Wheel on an unsupported platform\" % link",
            "                        )",
            "                        return []",
            "",
            "        if not version:",
            "            version = self._egg_info_matches(egg_info, search_name, link)",
            "        if version is None:",
            "            logger.debug('Skipping link %s; wrong project name (not %s)' % (link, search_name))",
            "            return []",
            "",
            "        if (link.internal is not None",
            "                and not link.internal",
            "                and not normalize_name(search_name).lower() in self.allow_external",
            "                and not self.allow_all_external):",
            "            # We have a link that we are sure is external, so we should skip",
            "            #   it unless we are allowing externals",
            "            logger.debug(\"Skipping %s because it is externally hosted.\" % link)",
            "            self.need_warn_external = True",
            "            return []",
            "",
            "        if (link.verifiable is not None",
            "                and not link.verifiable",
            "                and not normalize_name(search_name).lower() in self.allow_insecure",
            "                and not self.allow_all_insecure):",
            "            # We have a link that we are sure we cannot verify it's integrity,",
            "            #   so we should skip it unless we are allowing unsafe installs",
            "            #   for this requirement.",
            "            logger.debug(\"Skipping %s because it is an insecure and \"",
            "                         \"unverifiable file.\" % link)",
            "            self.need_warn_insecure = True",
            "            return []",
            "",
            "        match = self._py_version_re.search(version)",
            "        if match:",
            "            version = version[:match.start()]",
            "            py_version = match.group(1)",
            "            if py_version != sys.version[:3]:",
            "                logger.debug('Skipping %s because Python version is incorrect' % link)",
            "                return []",
            "        logger.debug('Found link %s, version: %s' % (link, version))",
            "        return [(pkg_resources.parse_version(version),",
            "               link,",
            "               version)]",
            "",
            "    def _egg_info_matches(self, egg_info, search_name, link):",
            "        match = self._egg_info_re.search(egg_info)",
            "        if not match:",
            "            logger.debug('Could not parse version from link: %s' % link)",
            "            return None",
            "        name = match.group(0).lower()",
            "        # To match the \"safe\" name that pkg_resources creates:",
            "        name = name.replace('_', '-')",
            "        # project name and version must be separated by a dash",
            "        look_for = search_name.lower() + \"-\"",
            "        if name.startswith(look_for):",
            "            return match.group(0)[len(look_for):]",
            "        else:",
            "            return None",
            "",
            "    def _get_page(self, link, req):",
            "        return HTMLPage.get_page(link, req, cache=self.cache)",
            "",
            "",
            "class PageCache(object):",
            "    \"\"\"Cache of HTML pages\"\"\"",
            "",
            "    failure_limit = 3",
            "",
            "    def __init__(self):",
            "        self._failures = {}",
            "        self._pages = {}",
            "        self._archives = {}",
            "",
            "    def too_many_failures(self, url):",
            "        return self._failures.get(url, 0) >= self.failure_limit",
            "",
            "    def get_page(self, url):",
            "        return self._pages.get(url)",
            "",
            "    def is_archive(self, url):",
            "        return self._archives.get(url, False)",
            "",
            "    def set_is_archive(self, url, value=True):",
            "        self._archives[url] = value",
            "",
            "    def add_page_failure(self, url, level):",
            "        self._failures[url] = self._failures.get(url, 0)+level",
            "",
            "    def add_page(self, urls, page):",
            "        for url in urls:",
            "            self._pages[url] = page",
            "",
            "",
            "class HTMLPage(object):",
            "    \"\"\"Represents one page, along with its URL\"\"\"",
            "",
            "    ## FIXME: these regexes are horrible hacks:",
            "    _homepage_re = re.compile(r'<th>\\s*home\\s*page', re.I)",
            "    _download_re = re.compile(r'<th>\\s*download\\s+url', re.I)",
            "    _href_re = re.compile('href=(?:\"([^\"]*)\"|\\'([^\\']*)\\'|([^>\\\\s\\\\n]*))', re.I|re.S)",
            "",
            "    def __init__(self, content, url, headers=None, trusted=None):",
            "        self.content = content",
            "        self.parsed = html5lib.parse(self.content, namespaceHTMLElements=False)",
            "        self.url = url",
            "        self.headers = headers",
            "        self.trusted = trusted",
            "",
            "    def __str__(self):",
            "        return self.url",
            "",
            "    @classmethod",
            "    def get_page(cls, link, req, cache=None, skip_archives=True):",
            "        url = link.url",
            "        url = url.split('#', 1)[0]",
            "        if cache.too_many_failures(url):",
            "            return None",
            "",
            "        # Check for VCS schemes that do not support lookup as web pages.",
            "        from pip.vcs import VcsSupport",
            "        for scheme in VcsSupport.schemes:",
            "            if url.lower().startswith(scheme) and url[len(scheme)] in '+:':",
            "                logger.debug('Cannot look at %(scheme)s URL %(link)s' % locals())",
            "                return None",
            "",
            "        if cache is not None:",
            "            inst = cache.get_page(url)",
            "            if inst is not None:",
            "                return inst",
            "        try:",
            "            if skip_archives:",
            "                if cache is not None:",
            "                    if cache.is_archive(url):",
            "                        return None",
            "                filename = link.filename",
            "                for bad_ext in ['.tar', '.tar.gz', '.tar.bz2', '.tgz', '.zip']:",
            "                    if filename.endswith(bad_ext):",
            "                        content_type = cls._get_content_type(url)",
            "                        if content_type.lower().startswith('text/html'):",
            "                            break",
            "                        else:",
            "                            logger.debug('Skipping page %s because of Content-Type: %s' % (link, content_type))",
            "                            if cache is not None:",
            "                                cache.set_is_archive(url)",
            "                            return None",
            "            logger.debug('Getting page %s' % url)",
            "",
            "            # Tack index.html onto file:// URLs that point to directories",
            "            (scheme, netloc, path, params, query, fragment) = urlparse.urlparse(url)",
            "            if scheme == 'file' and os.path.isdir(url2pathname(path)):",
            "                # add trailing slash if not present so urljoin doesn't trim final segment",
            "                if not url.endswith('/'):",
            "                    url += '/'",
            "                url = urlparse.urljoin(url, 'index.html')",
            "                logger.debug(' file: URL is directory, getting %s' % url)",
            "",
            "            resp = urlopen(url)",
            "",
            "            real_url = geturl(resp)",
            "            headers = resp.info()",
            "            contents = resp.read()",
            "            encoding = headers.get('Content-Encoding', None)",
            "            #XXX need to handle exceptions and add testing for this",
            "            if encoding is not None:",
            "                if encoding == 'gzip':",
            "                    contents = gzip.GzipFile(fileobj=BytesIO(contents)).read()",
            "                if encoding == 'deflate':",
            "                    contents = zlib.decompress(contents)",
            "",
            "            # The check for archives above only works if the url ends with",
            "            #   something that looks like an archive. However that is not a",
            "            #   requirement. For instance http://sourceforge.net/projects/docutils/files/docutils/0.8.1/docutils-0.8.1.tar.gz/download",
            "            #   redirects to http://superb-dca3.dl.sourceforge.net/project/docutils/docutils/0.8.1/docutils-0.8.1.tar.gz",
            "            #   Unless we issue a HEAD request on every url we cannot know",
            "            #   ahead of time for sure if something is HTML or not. However we",
            "            #   can check after we've downloaded it.",
            "            content_type = headers.get('Content-Type', 'unknown')",
            "            if not content_type.lower().startswith(\"text/html\"):",
            "                logger.debug('Skipping page %s because of Content-Type: %s' %",
            "                                            (link, content_type))",
            "                if cache is not None:",
            "                    cache.set_is_archive(url)",
            "                return None",
            "",
            "            inst = cls(u(contents), real_url, headers, trusted=link.trusted)",
            "        except (HTTPError, URLError, socket.timeout, socket.error, OSError, WindowsError):",
            "            e = sys.exc_info()[1]",
            "            desc = str(e)",
            "            if isinstance(e, socket.timeout):",
            "                log_meth = logger.info",
            "                level =1",
            "                desc = 'timed out'",
            "            elif isinstance(e, URLError):",
            "                #ssl/certificate error",
            "                if hasattr(e, 'reason') and (isinstance(e.reason, ssl.SSLError) or isinstance(e.reason, CertificateError)):",
            "                    desc = 'There was a problem confirming the ssl certificate: %s' % e",
            "                    log_meth = logger.notify",
            "                else:",
            "                    log_meth = logger.info",
            "                if hasattr(e, 'reason') and isinstance(e.reason, socket.timeout):",
            "                    desc = 'timed out'",
            "                    level = 1",
            "                else:",
            "                    level = 2",
            "            elif isinstance(e, HTTPError) and e.code == 404:",
            "                ## FIXME: notify?",
            "                log_meth = logger.info",
            "                level = 2",
            "            else:",
            "                log_meth = logger.info",
            "                level = 1",
            "            log_meth('Could not fetch URL %s: %s' % (link, desc))",
            "            log_meth('Will skip URL %s when looking for download links for %s' % (link.url, req))",
            "            if cache is not None:",
            "                cache.add_page_failure(url, level)",
            "            return None",
            "        if cache is not None:",
            "            cache.add_page([url, real_url], inst)",
            "        return inst",
            "",
            "    @staticmethod",
            "    def _get_content_type(url):",
            "        \"\"\"Get the Content-Type of the given url, using a HEAD request\"\"\"",
            "        scheme, netloc, path, query, fragment = urlparse.urlsplit(url)",
            "        if not scheme in ('http', 'https', 'ftp', 'ftps'):",
            "            ## FIXME: some warning or something?",
            "            ## assertion error?",
            "            return ''",
            "        req = Urllib2HeadRequest(url, headers={'Host': netloc})",
            "        resp = urlopen(req)",
            "        try:",
            "            if hasattr(resp, 'code') and resp.code != 200 and scheme not in ('ftp', 'ftps'):",
            "                ## FIXME: doesn't handle redirects",
            "                return ''",
            "            return resp.info().get('content-type', '')",
            "        finally:",
            "            resp.close()",
            "",
            "    @property",
            "    def api_version(self):",
            "        if not hasattr(self, \"_api_version\"):",
            "            _api_version = None",
            "",
            "            metas = [x for x in self.parsed.findall(\".//meta\")",
            "                        if x.get(\"name\", \"\").lower() == \"api-version\"]",
            "            if metas:",
            "                try:",
            "                    _api_version = int(metas[0].get(\"value\", None))",
            "                except (TypeError, ValueError):",
            "                    _api_version = None",
            "            self._api_version = _api_version",
            "        return self._api_version",
            "",
            "    @property",
            "    def base_url(self):",
            "        if not hasattr(self, \"_base_url\"):",
            "            base = self.parsed.find(\".//base\")",
            "            if base is not None and base.get(\"href\"):",
            "                self._base_url = base.get(\"href\")",
            "            else:",
            "                self._base_url = self.url",
            "        return self._base_url",
            "",
            "    @property",
            "    def links(self):",
            "        \"\"\"Yields all links in the page\"\"\"",
            "        for anchor in self.parsed.findall(\".//a\"):",
            "            if anchor.get(\"href\"):",
            "                href = anchor.get(\"href\")",
            "                url = self.clean_link(urlparse.urljoin(self.base_url, href))",
            "",
            "                # Determine if this link is internal. If that distinction",
            "                #   doesn't make sense in this context, then we don't make",
            "                #   any distinction.",
            "                internal = None",
            "                if self.api_version and self.api_version >= 2:",
            "                    # Only api_versions >= 2 have a distinction between",
            "                    #   external and internal links",
            "                    internal = bool(anchor.get(\"rel\")",
            "                                and \"internal\" in anchor.get(\"rel\").split())",
            "",
            "                yield Link(url, self, internal=internal)",
            "",
            "    def rel_links(self):",
            "        for url in self.explicit_rel_links():",
            "            yield url",
            "        for url in self.scraped_rel_links():",
            "            yield url",
            "",
            "    def explicit_rel_links(self, rels=('homepage', 'download')):",
            "        \"\"\"Yields all links with the given relations\"\"\"",
            "        rels = set(rels)",
            "",
            "        for anchor in self.parsed.findall(\".//a\"):",
            "            if anchor.get(\"rel\") and anchor.get(\"href\"):",
            "                found_rels = set(anchor.get(\"rel\").split())",
            "                # Determine the intersection between what rels were found and",
            "                #   what rels were being looked for",
            "                if found_rels & rels:",
            "                    href = anchor.get(\"href\")",
            "                    url = self.clean_link(urlparse.urljoin(self.base_url, href))",
            "                    yield Link(url, self, trusted=False)",
            "",
            "    def scraped_rel_links(self):",
            "        # Can we get rid of this horrible horrible method?",
            "        for regex in (self._homepage_re, self._download_re):",
            "            match = regex.search(self.content)",
            "            if not match:",
            "                continue",
            "            href_match = self._href_re.search(self.content, pos=match.end())",
            "            if not href_match:",
            "                continue",
            "            url = href_match.group(1) or href_match.group(2) or href_match.group(3)",
            "            if not url:",
            "                continue",
            "            url = self.clean_link(urlparse.urljoin(self.base_url, url))",
            "            yield Link(url, self, trusted=False)",
            "",
            "    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\\\|-]', re.I)",
            "",
            "    def clean_link(self, url):",
            "        \"\"\"Makes sure a link is fully encoded.  That is, if a ' ' shows up in",
            "        the link, it will be rewritten to %20 (while not over-quoting",
            "        % or other characters).\"\"\"",
            "        return self._clean_re.sub(",
            "            lambda match: '%%%2x' % ord(match.group(0)), url)",
            "",
            "",
            "class Link(object):",
            "",
            "    def __init__(self, url, comes_from=None, internal=None, trusted=None):",
            "        self.url = url",
            "        self.comes_from = comes_from",
            "        self.internal = internal",
            "        self.trusted = trusted",
            "",
            "        # Set whether it's a wheel",
            "        self.wheel = None",
            "        if url != Inf and self.splitext()[1] == wheel_ext:",
            "            self.wheel = Wheel(self.filename)",
            "",
            "    def __str__(self):",
            "        if self.comes_from:",
            "            return '%s (from %s)' % (self.url, self.comes_from)",
            "        else:",
            "            return str(self.url)",
            "",
            "    def __repr__(self):",
            "        return '<Link %s>' % self",
            "",
            "    def __eq__(self, other):",
            "        return self.url == other.url",
            "",
            "    def __ne__(self, other):",
            "        return self.url != other.url",
            "",
            "    def __lt__(self, other):",
            "        return self.url < other.url",
            "",
            "    def __le__(self, other):",
            "        return self.url <= other.url",
            "",
            "    def __gt__(self, other):",
            "        return self.url > other.url",
            "",
            "    def __ge__(self, other):",
            "        return self.url >= other.url",
            "",
            "    def __hash__(self):",
            "        return hash(self.url)",
            "",
            "    @property",
            "    def filename(self):",
            "        _, netloc, path, _, _ = urlparse.urlsplit(self.url)",
            "        name = posixpath.basename(path.rstrip('/')) or netloc",
            "        assert name, ('URL %r produced no filename' % self.url)",
            "        return name",
            "",
            "    @property",
            "    def scheme(self):",
            "        return urlparse.urlsplit(self.url)[0]",
            "",
            "    @property",
            "    def path(self):",
            "        return urlparse.urlsplit(self.url)[2]",
            "",
            "    def splitext(self):",
            "        return splitext(posixpath.basename(self.path.rstrip('/')))",
            "",
            "    @property",
            "    def url_without_fragment(self):",
            "        scheme, netloc, path, query, fragment = urlparse.urlsplit(self.url)",
            "        return urlparse.urlunsplit((scheme, netloc, path, query, None))",
            "",
            "    _egg_fragment_re = re.compile(r'#egg=([^&]*)')",
            "",
            "    @property",
            "    def egg_fragment(self):",
            "        match = self._egg_fragment_re.search(self.url)",
            "        if not match:",
            "            return None",
            "        return match.group(1)",
            "",
            "    _hash_re = re.compile(r'(sha1|sha224|sha384|sha256|sha512|md5)=([a-f0-9]+)')",
            "",
            "    @property",
            "    def hash(self):",
            "        match = self._hash_re.search(self.url)",
            "        if match:",
            "            return match.group(2)",
            "        return None",
            "",
            "    @property",
            "    def hash_name(self):",
            "        match = self._hash_re.search(self.url)",
            "        if match:",
            "            return match.group(1)",
            "        return None",
            "",
            "    @property",
            "    def show_url(self):",
            "        return posixpath.basename(self.url.split('#', 1)[0].split('?', 1)[0])",
            "",
            "    @property",
            "    def verifiable(self):",
            "        \"\"\"",
            "        Returns True if this link can be verified after download, False if it",
            "        cannot, and None if we cannot determine.",
            "        \"\"\"",
            "        trusted = self.trusted or getattr(self.comes_from, \"trusted\", None)",
            "        if trusted is not None and trusted:",
            "            # This link came from a trusted source. It *may* be verifiable but",
            "            #   first we need to see if this page is operating under the new",
            "            #   API version.",
            "            try:",
            "                api_version = getattr(self.comes_from, \"api_version\", None)",
            "                api_version = int(api_version)",
            "            except (ValueError, TypeError):",
            "                api_version = None",
            "",
            "            if api_version is None or api_version <= 1:",
            "                # This link is either trusted, or it came from a trusted,",
            "                #   however it is not operating under the API version 2 so",
            "                #   we can't make any claims about if it's safe or not",
            "                return",
            "",
            "            if self.hash:",
            "                # This link came from a trusted source and it has a hash, so we",
            "                #   can consider it safe.",
            "                return True",
            "            else:",
            "                # This link came from a trusted source, using the new API",
            "                #   version, and it does not have a hash. It is NOT verifiable",
            "                return False",
            "        elif trusted is not None:",
            "            # This link came from an untrusted source and we cannot trust it",
            "            return False",
            "",
            "#An \"Infinite Link\" that compares greater than other links",
            "InfLink = Link(Inf) #this object is not currently used as a sortable",
            "",
            "",
            "def get_requirement_from_url(url):",
            "    \"\"\"Get a requirement from the URL, if possible.  This looks for #egg",
            "    in the URL\"\"\"",
            "    link = Link(url)",
            "    egg_info = link.egg_fragment",
            "    if not egg_info:",
            "        egg_info = splitext(link.filename)[0]",
            "    return package_to_requirement(egg_info)",
            "",
            "",
            "def package_to_requirement(package_name):",
            "    \"\"\"Translate a name like Foo-1.2 to Foo==1.3\"\"\"",
            "    match = re.search(r'^(.*?)-(dev|\\d.*)', package_name)",
            "    if match:",
            "        name = match.group(1)",
            "        version = match.group(2)",
            "    else:",
            "        name = package_name",
            "        version = ''",
            "    if version:",
            "        return '%s==%s' % (name, version)",
            "    else:",
            "        return name"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "50": [
                "PackageFinder",
                "__init__"
            ],
            "60": [
                "PackageFinder",
                "__init__"
            ],
            "61": [
                "PackageFinder",
                "__init__"
            ],
            "62": [
                "PackageFinder",
                "__init__"
            ],
            "63": [
                "PackageFinder",
                "__init__"
            ],
            "64": [
                "PackageFinder",
                "__init__"
            ],
            "205": [
                "PackageFinder",
                "find_requirement"
            ],
            "206": [
                "PackageFinder",
                "find_requirement"
            ],
            "207": [
                "PackageFinder",
                "find_requirement"
            ],
            "208": [
                "PackageFinder",
                "find_requirement"
            ],
            "212": [
                "PackageFinder",
                "find_requirement"
            ],
            "225": [
                "PackageFinder",
                "find_requirement"
            ],
            "226": [
                "PackageFinder",
                "find_requirement"
            ],
            "477": [
                "PackageFinder",
                "_link_package_versions"
            ],
            "478": [
                "PackageFinder",
                "_link_package_versions"
            ],
            "479": [
                "PackageFinder",
                "_link_package_versions"
            ],
            "480": [
                "PackageFinder",
                "_link_package_versions"
            ],
            "481": [
                "PackageFinder",
                "_link_package_versions"
            ],
            "482": [
                "PackageFinder",
                "_link_package_versions"
            ],
            "553": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "554": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "555": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "556": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "557": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "558": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "559": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "560": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "561": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "562": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "563": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "564": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "565": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "566": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "567": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "568": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "569": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "570": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "571": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "572": [
                "PackageFinder",
                "_get_mirror_urls"
            ],
            "573": [],
            "997": [],
            "998": [],
            "999": [
                "get_mirrors"
            ],
            "1000": [
                "get_mirrors"
            ],
            "1001": [
                "get_mirrors"
            ],
            "1002": [
                "get_mirrors"
            ],
            "1003": [
                "get_mirrors"
            ],
            "1004": [
                "get_mirrors"
            ],
            "1005": [
                "get_mirrors"
            ],
            "1006": [
                "get_mirrors"
            ],
            "1007": [
                "get_mirrors"
            ],
            "1008": [
                "get_mirrors"
            ],
            "1009": [
                "get_mirrors"
            ],
            "1010": [
                "get_mirrors"
            ],
            "1011": [
                "get_mirrors"
            ],
            "1012": [
                "get_mirrors"
            ],
            "1013": [
                "get_mirrors"
            ],
            "1014": [
                "get_mirrors"
            ],
            "1015": [
                "get_mirrors"
            ],
            "1016": [
                "get_mirrors"
            ],
            "1017": [
                "get_mirrors"
            ],
            "1018": [
                "get_mirrors"
            ],
            "1019": [
                "get_mirrors"
            ],
            "1020": [
                "get_mirrors"
            ],
            "1021": [
                "get_mirrors"
            ],
            "1022": [
                "get_mirrors"
            ],
            "1023": [
                "get_mirrors"
            ],
            "1024": [
                "get_mirrors"
            ],
            "1025": [],
            "1026": [],
            "1027": [
                "string_range"
            ],
            "1028": [
                "string_range"
            ],
            "1029": [
                "string_range"
            ],
            "1030": [
                "string_range"
            ],
            "1031": [
                "string_range"
            ],
            "1032": [
                "string_range"
            ],
            "1033": [
                "string_range"
            ],
            "1034": [
                "string_range"
            ],
            "1035": [
                "string_range"
            ],
            "1036": [
                "string_range"
            ],
            "1037": [
                "string_range"
            ],
            "1038": []
        },
        "addLocation": []
    }
}