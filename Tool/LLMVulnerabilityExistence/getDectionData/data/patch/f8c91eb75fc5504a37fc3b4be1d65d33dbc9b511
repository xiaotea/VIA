{
    "nova/tests/fixtures/libvirt.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2234,
                "afterPatchRowNumber": 2234,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 2235,
                "afterPatchRowNumber": 2235,
                "PatchRowcode": "         # libvirt driver needs to call out to the filesystem to get the"
            },
            "2": {
                "beforePatchRowNumber": 2236,
                "afterPatchRowNumber": 2236,
                "PatchRowcode": "         # parent_ifname for the SRIOV VFs."
            },
            "3": {
                "beforePatchRowNumber": 2237,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.useFixture(fixtures.MockPatch("
            },
            "4": {
                "beforePatchRowNumber": 2238,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            'nova.pci.utils.get_ifname_by_pci_address',"
            },
            "5": {
                "beforePatchRowNumber": 2239,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return_value='fake_pf_interface_name'))"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2237,
                "PatchRowcode": "+        self.mock_get_ifname_by_pci_address = self.useFixture("
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2238,
                "PatchRowcode": "+            fixtures.MockPatch("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2239,
                "PatchRowcode": "+                \"nova.pci.utils.get_ifname_by_pci_address\","
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2240,
                "PatchRowcode": "+                return_value=\"fake_pf_interface_name\","
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2241,
                "PatchRowcode": "+            )"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2242,
                "PatchRowcode": "+        ).mock"
            },
            "12": {
                "beforePatchRowNumber": 2240,
                "afterPatchRowNumber": 2243,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 2241,
                "afterPatchRowNumber": 2244,
                "PatchRowcode": "         self.useFixture(fixtures.MockPatch("
            },
            "14": {
                "beforePatchRowNumber": 2242,
                "afterPatchRowNumber": 2245,
                "PatchRowcode": "             'nova.pci.utils.get_mac_by_pci_address',"
            }
        },
        "frontPatchFile": [
            "#    Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import collections",
            "import os",
            "import sys",
            "import textwrap",
            "import time",
            "import typing as ty",
            "from unittest import mock",
            "",
            "import fixtures",
            "from lxml import etree",
            "from oslo_log import log as logging",
            "from oslo_utils.fixture import uuidsentinel as uuids",
            "from oslo_utils import versionutils",
            "",
            "from nova import conf",
            "from nova.objects import fields as obj_fields",
            "from nova.tests.fixtures import libvirt_data as fake_libvirt_data",
            "from nova.virt.libvirt import config as vconfig",
            "from nova.virt.libvirt import driver as libvirt_driver",
            "from nova.virt.libvirt import host",
            "",
            "",
            "# Allow passing None to the various connect methods",
            "# (i.e. allow the client to rely on default URLs)",
            "allow_default_uri_connection = True",
            "",
            "# Has libvirt connection been used at least once",
            "connection_used = False",
            "",
            "",
            "def _reset():",
            "    global allow_default_uri_connection",
            "    allow_default_uri_connection = True",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "CONF = conf.CONF",
            "",
            "# virDomainState",
            "VIR_DOMAIN_NOSTATE = 0",
            "VIR_DOMAIN_RUNNING = 1",
            "VIR_DOMAIN_BLOCKED = 2",
            "VIR_DOMAIN_PAUSED = 3",
            "VIR_DOMAIN_SHUTDOWN = 4",
            "VIR_DOMAIN_SHUTOFF = 5",
            "VIR_DOMAIN_CRASHED = 6",
            "",
            "# NOTE(mriedem): These values come from include/libvirt/libvirt-domain.h",
            "VIR_DOMAIN_XML_SECURE = 1",
            "VIR_DOMAIN_XML_INACTIVE = 2",
            "VIR_DOMAIN_XML_UPDATE_CPU = 4",
            "VIR_DOMAIN_XML_MIGRATABLE = 8",
            "",
            "VIR_DOMAIN_BLOCK_COPY_SHALLOW = 1",
            "VIR_DOMAIN_BLOCK_COPY_REUSE_EXT = 2",
            "VIR_DOMAIN_BLOCK_COPY_TRANSIENT_JOB = 4",
            "",
            "VIR_DOMAIN_BLOCK_REBASE_SHALLOW = 1",
            "VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT = 2",
            "VIR_DOMAIN_BLOCK_REBASE_COPY = 8",
            "VIR_DOMAIN_BLOCK_REBASE_RELATIVE = 16",
            "VIR_DOMAIN_BLOCK_REBASE_COPY_DEV = 32",
            "",
            "# virDomainBlockResize",
            "VIR_DOMAIN_BLOCK_RESIZE_BYTES = 1",
            "",
            "VIR_DOMAIN_BLOCK_JOB_ABORT_ASYNC = 1",
            "VIR_DOMAIN_BLOCK_JOB_ABORT_PIVOT = 2",
            "",
            "VIR_DOMAIN_EVENT_ID_LIFECYCLE = 0",
            "",
            "VIR_DOMAIN_EVENT_DEFINED = 0",
            "VIR_DOMAIN_EVENT_UNDEFINED = 1",
            "VIR_DOMAIN_EVENT_STARTED = 2",
            "VIR_DOMAIN_EVENT_SUSPENDED = 3",
            "VIR_DOMAIN_EVENT_RESUMED = 4",
            "VIR_DOMAIN_EVENT_STOPPED = 5",
            "VIR_DOMAIN_EVENT_SHUTDOWN = 6",
            "VIR_DOMAIN_EVENT_PMSUSPENDED = 7",
            "",
            "VIR_DOMAIN_EVENT_ID_DEVICE_REMOVED = 15",
            "VIR_DOMAIN_EVENT_ID_DEVICE_REMOVAL_FAILED = 22",
            "",
            "VIR_DOMAIN_EVENT_SUSPENDED_MIGRATED = 1",
            "VIR_DOMAIN_EVENT_SUSPENDED_POSTCOPY = 7",
            "",
            "VIR_DOMAIN_UNDEFINE_MANAGED_SAVE = 1",
            "VIR_DOMAIN_UNDEFINE_NVRAM = 4",
            "",
            "VIR_DOMAIN_AFFECT_CURRENT = 0",
            "VIR_DOMAIN_AFFECT_LIVE = 1",
            "VIR_DOMAIN_AFFECT_CONFIG = 2",
            "",
            "VIR_CPU_COMPARE_ERROR = -1",
            "VIR_CPU_COMPARE_INCOMPATIBLE = 0",
            "VIR_CPU_COMPARE_IDENTICAL = 1",
            "VIR_CPU_COMPARE_SUPERSET = 2",
            "",
            "VIR_CRED_USERNAME = 1",
            "VIR_CRED_AUTHNAME = 2",
            "VIR_CRED_LANGUAGE = 3",
            "VIR_CRED_CNONCE = 4",
            "VIR_CRED_PASSPHRASE = 5",
            "VIR_CRED_ECHOPROMPT = 6",
            "VIR_CRED_NOECHOPROMPT = 7",
            "VIR_CRED_REALM = 8",
            "VIR_CRED_EXTERNAL = 9",
            "",
            "VIR_MIGRATE_LIVE = 1",
            "VIR_MIGRATE_PEER2PEER = 2",
            "VIR_MIGRATE_TUNNELLED = 4",
            "VIR_MIGRATE_PERSIST_DEST = 8",
            "VIR_MIGRATE_UNDEFINE_SOURCE = 16",
            "VIR_MIGRATE_NON_SHARED_INC = 128",
            "VIR_MIGRATE_AUTO_CONVERGE = 8192",
            "VIR_MIGRATE_POSTCOPY = 32768",
            "VIR_MIGRATE_TLS = 65536",
            "",
            "VIR_NODE_CPU_STATS_ALL_CPUS = -1",
            "",
            "VIR_DOMAIN_START_PAUSED = 1",
            "",
            "# libvirtError enums",
            "# (Intentionally different from what's in libvirt. We do this to check,",
            "#  that consumers of the library are using the symbolic names rather than",
            "#  hardcoding the numerical values)",
            "VIR_FROM_QEMU = 100",
            "VIR_FROM_DOMAIN = 200",
            "VIR_FROM_SECRET = 300",
            "VIR_FROM_NWFILTER = 330",
            "VIR_FROM_REMOTE = 340",
            "VIR_FROM_RPC = 345",
            "VIR_FROM_NODEDEV = 666",
            "",
            "VIR_ERR_INVALID_ARG = 8",
            "VIR_ERR_NO_SUPPORT = 3",
            "VIR_ERR_XML_ERROR = 27",
            "VIR_ERR_XML_DETAIL = 350",
            "VIR_ERR_NO_DOMAIN = 420",
            "VIR_ERR_OPERATION_FAILED = 510",
            "VIR_ERR_OPERATION_INVALID = 55",
            "VIR_ERR_OPERATION_TIMEOUT = 68",
            "VIR_ERR_NO_NWFILTER = 620",
            "VIR_ERR_SYSTEM_ERROR = 900",
            "VIR_ERR_INTERNAL_ERROR = 950",
            "VIR_ERR_CONFIG_UNSUPPORTED = 951",
            "VIR_ERR_NO_NODE_DEVICE = 667",
            "VIR_ERR_INVALID_SECRET = 65",
            "VIR_ERR_NO_SECRET = 66",
            "VIR_ERR_AGENT_UNRESPONSIVE = 86",
            "VIR_ERR_ARGUMENT_UNSUPPORTED = 74",
            "VIR_ERR_OPERATION_UNSUPPORTED = 84",
            "VIR_ERR_DEVICE_MISSING = 99",
            "# Readonly",
            "VIR_CONNECT_RO = 1",
            "",
            "# virConnectBaselineCPU flags",
            "VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES = 1",
            "",
            "# snapshotCreateXML flags",
            "VIR_DOMAIN_SNAPSHOT_CREATE_NO_METADATA = 4",
            "VIR_DOMAIN_SNAPSHOT_CREATE_DISK_ONLY = 16",
            "VIR_DOMAIN_SNAPSHOT_CREATE_REUSE_EXT = 32",
            "VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE = 64",
            "",
            "# blockCommit flags",
            "VIR_DOMAIN_BLOCK_COMMIT_RELATIVE = 4",
            "",
            "",
            "VIR_CONNECT_LIST_DOMAINS_ACTIVE = 1",
            "VIR_CONNECT_LIST_DOMAINS_INACTIVE = 2",
            "",
            "# virConnectListAllNodeDevices flags",
            "VIR_CONNECT_LIST_NODE_DEVICES_CAP_PCI_DEV = 2",
            "VIR_CONNECT_LIST_NODE_DEVICES_CAP_NET = 1 << 4",
            "VIR_CONNECT_LIST_NODE_DEVICES_CAP_VDPA = 1 << 17",
            "",
            "# secret type",
            "VIR_SECRET_USAGE_TYPE_NONE = 0",
            "VIR_SECRET_USAGE_TYPE_VOLUME = 1",
            "VIR_SECRET_USAGE_TYPE_CEPH = 2",
            "VIR_SECRET_USAGE_TYPE_ISCSI = 3",
            "",
            "# metadata types",
            "VIR_DOMAIN_METADATA_DESCRIPTION = 0",
            "VIR_DOMAIN_METADATA_TITLE = 1",
            "VIR_DOMAIN_METADATA_ELEMENT = 2",
            "",
            "# Libvirt version to match MIN_LIBVIRT_VERSION in driver.py",
            "FAKE_LIBVIRT_VERSION = versionutils.convert_version_to_int(",
            "    libvirt_driver.MIN_LIBVIRT_VERSION)",
            "# Libvirt version to match MIN_QEMU_VERSION in driver.py",
            "FAKE_QEMU_VERSION = versionutils.convert_version_to_int(",
            "    libvirt_driver.MIN_QEMU_VERSION)",
            "",
            "PCI_VEND_ID = '8086'",
            "PCI_VEND_NAME = 'Intel Corporation'",
            "",
            "PCI_PROD_ID = '1533'",
            "PCI_PROD_NAME = 'I210 Gigabit Network Connection'",
            "PCI_DRIVER_NAME = 'igb'",
            "",
            "PF_PROD_ID = '1528'",
            "PF_PROD_NAME = 'Ethernet Controller 10-Gigabit X540-AT2'",
            "PF_DRIVER_NAME = 'ixgbe'",
            "PF_CAP_TYPE = 'virt_functions'",
            "",
            "VF_PROD_ID = '1515'",
            "VF_PROD_NAME = 'X540 Ethernet Controller Virtual Function'",
            "VF_DRIVER_NAME = 'ixgbevf'",
            "VF_CAP_TYPE = 'phys_function'",
            "",
            "MDEV_CAPABLE_VEND_ID = '10DE'",
            "MDEV_CAPABLE_VEND_NAME = 'Nvidia'",
            "MDEV_CAPABLE_PROD_ID = '0FFE'",
            "MDEV_CAPABLE_PROD_NAME = 'GRID M60-0B'",
            "MDEV_CAPABLE_DRIVER_NAME = 'nvidia'",
            "MDEV_CAPABLE_CAP_TYPE = 'mdev_types'",
            "",
            "NVIDIA_11_VGPU_TYPE = 'nvidia-11'",
            "NVIDIA_12_VGPU_TYPE = 'nvidia-12'",
            "MLX5_CORE_TYPE = 'mlx5_core'",
            "MDEVCAP_DEV1_PCI_ADDR = 'pci_0000_81_00_0'",
            "MDEVCAP_DEV2_PCI_ADDR = 'pci_0000_81_01_0'",
            "MDEVCAP_DEV3_PCI_ADDR = 'pci_0000_81_02_0'",
            "",
            "os_uname = collections.namedtuple(",
            "    'uname_result', ['sysname', 'nodename', 'release', 'version', 'machine'],",
            ")",
            "",
            "",
            "def _get_libvirt_nodedev_name(bus, slot, function):",
            "    \"\"\"Convert an address to a libvirt device name string.\"\"\"",
            "    return f'pci_0000_{bus:02x}_{slot:02x}_{function:d}'",
            "",
            "",
            "class FakePCIDevice(object):",
            "    \"\"\"Generate a fake PCI device.",
            "",
            "    Generate a fake PCI devices corresponding to one of the following",
            "    real-world PCI devices.",
            "",
            "    - I210 Gigabit Network Connection (8086:1533)",
            "    - Ethernet Controller 10-Gigabit X540-AT2 (8086:1528)",
            "    - X540 Ethernet Controller Virtual Function (8086:1515)",
            "    \"\"\"",
            "",
            "    pci_default_parent = \"pci_0000_80_01_0\"",
            "    pci_device_template = textwrap.dedent(\"\"\"",
            "        <device>",
            "          <name>pci_0000_%(bus)02x_%(slot)02x_%(function)d</name>",
            "          <path>/sys/devices/pci0000:80/0000:80:01.0/0000:%(bus)02x:%(slot)02x.%(function)d</path>",
            "          <parent>%(parent)s</parent>",
            "          <driver>",
            "            <name>%(driver)s</name>",
            "          </driver>",
            "          <capability type='pci'>",
            "            <domain>0</domain>",
            "            <bus>%(bus)d</bus>",
            "            <slot>%(slot)d</slot>",
            "            <function>%(function)d</function>",
            "            <product id='0x%(prod_id)s'>%(prod_name)s</product>",
            "            <vendor id='0x%(vend_id)s'>%(vend_name)s</vendor>",
            "        %(capability)s",
            "        %(vpd_capability)s",
            "            <iommuGroup number='%(iommu_group)d'>",
            "              <address domain='0x0000' bus='%(bus)#02x' slot='%(slot)#02x' function='0x%(function)d'/>",
            "            </iommuGroup>",
            "            <numa node='%(numa_node)s'/>",
            "            <pci-express>",
            "              <link validity='cap' port='0' speed='5' width='8'/>",
            "              <link validity='sta' speed='5' width='8'/>",
            "            </pci-express>",
            "          </capability>",
            "        </device>\"\"\".strip())  # noqa",
            "    cap_templ = \"<capability type='%(cap_type)s'>%(addresses)s</capability>\"",
            "    addr_templ = \"<address domain='0x0000' bus='%(bus)#02x' slot='%(slot)#02x' function='%(function)#02x'/>\"  # noqa",
            "    mdevtypes_templ = textwrap.dedent(\"\"\"",
            "        <type id='%(type_id)s'>",
            "        <name>GRID M60-0B</name><deviceAPI>vfio-pci</deviceAPI>",
            "        <availableInstances>%(instances)s</availableInstances>",
            "        </type>\"\"\".strip())  # noqa",
            "",
            "    vpd_cap_templ = textwrap.dedent(\"\"\"",
            "        <capability type='vpd'>",
            "        <name>%(name)s</name>",
            "        %(fields)s",
            "        </capability>\"\"\".strip())",
            "    vpd_fields_templ = textwrap.dedent(\"\"\"",
            "        <fields access='%(access)s'>%(section_fields)s</fields>\"\"\".strip())",
            "    vpd_field_templ = \"\"\"<%(field_name)s>%(field_value)s</%(field_name)s>\"\"\"",
            "",
            "    is_capable_of_mdevs = False",
            "",
            "    def __init__(",
            "        self, dev_type, bus, slot, function, iommu_group, numa_node, *,",
            "        vf_ratio=None, multiple_gpu_types=False, generic_types=False,",
            "        parent=None, vend_id=None, vend_name=None, prod_id=None,",
            "        prod_name=None, driver_name=None, vpd_fields=None, mac_address=None,",
            "    ):",
            "        \"\"\"Populate pci devices",
            "",
            "        :param dev_type: (str) Indicates the type of the device (PCI, PF, VF,",
            "            MDEV_TYPES).",
            "        :param bus: (int) Bus number of the device.",
            "        :param slot: (int) Slot number of the device.",
            "        :param function: (int) Function number of the device.",
            "        :param iommu_group: (int) IOMMU group ID.",
            "        :param numa_node: (int) NUMA node of the device.",
            "        :param vf_ratio: (int) Ratio of Virtual Functions on Physical. Only",
            "            applicable if ``dev_type`` is one of: ``PF``, ``VF``.",
            "        :param multiple_gpu_types: (bool) Supports different vGPU types.",
            "        :param generic_types: (bool) Support both mlx5 and nvidia-12 types.",
            "        :param parent: (int, int, int) A tuple of bus, slot and function",
            "            corresponding to the parent.",
            "        :param vend_id: (str) The vendor ID.",
            "        :param vend_name: (str) The vendor name.",
            "        :param prod_id: (str) The product ID.",
            "        :param prod_name: (str) The product name.",
            "        :param driver_name: (str) The driver name.",
            "        :param mac_address: (str) The MAC of the device.",
            "            Used in case of SRIOV PFs",
            "        \"\"\"",
            "",
            "        self.dev_type = dev_type",
            "        self.bus = bus",
            "        self.slot = slot",
            "        self.function = function",
            "        self.iommu_group = iommu_group",
            "        self.numa_node = numa_node",
            "        self.vf_ratio = vf_ratio",
            "        self.multiple_gpu_types = multiple_gpu_types",
            "        self.generic_types = generic_types",
            "        self.parent = parent",
            "",
            "        self.vend_id = vend_id",
            "        self.vend_name = vend_name",
            "        self.prod_id = prod_id",
            "        self.prod_name = prod_name",
            "        self.driver_name = driver_name",
            "        self.mac_address = mac_address",
            "",
            "        self.vpd_fields = vpd_fields",
            "",
            "        self.generate_xml()",
            "",
            "    def generate_xml(self, skip_capability=False):",
            "",
            "        # initial validation",
            "        assert self.dev_type in ('PCI', 'VF', 'PF', 'MDEV_TYPES'), (",
            "            f'got invalid dev_type {self.dev_type}')",
            "",
            "        if self.dev_type == 'PCI':",
            "            assert not self.vf_ratio, 'vf_ratio does not apply for PCI devices'",
            "",
            "        if self.dev_type in ('PF', 'VF'):",
            "            assert (",
            "                self.vf_ratio is not None",
            "            ), 'require vf_ratio for PFs and VFs'",
            "",
            "        if self.dev_type == 'VF':",
            "            assert self.parent, 'require parent for VFs'",
            "            assert isinstance(self.parent, tuple), 'parent must be an address'",
            "            assert len(self.parent) == 3, 'parent must be an address'",
            "",
            "        vend_id = self.vend_id or PCI_VEND_ID",
            "        vend_name = self.vend_name or PCI_VEND_NAME",
            "        capability = ''",
            "        if self.dev_type == 'PCI':",
            "            prod_id = self.prod_id or PCI_PROD_ID",
            "            prod_name = self.prod_name or PCI_PROD_NAME",
            "            driver = self.driver_name or PCI_DRIVER_NAME",
            "        elif self.dev_type == 'PF':",
            "            prod_id = self.prod_id or PF_PROD_ID",
            "            prod_name = self.prod_name or PF_PROD_NAME",
            "            driver = self.driver_name or PF_DRIVER_NAME",
            "            if not skip_capability:",
            "                capability = self.cap_templ % {",
            "                    'cap_type': PF_CAP_TYPE,",
            "                    'addresses': '\\n'.join([",
            "                        self.addr_templ % {",
            "                            'bus': self.bus,",
            "                            # these are the slot, function values of the child",
            "                            # VFs, we can only assign 8 functions to a slot",
            "                            # (0-7) so bump the slot each time we exceed this",
            "                            'slot': self.slot + (x // 8),",
            "                            # ...and wrap the function value",
            "                            'function': x % 8,",
            "                        # the offset is because the PF is occupying function 0",
            "                        } for x in range(1, self.vf_ratio + 1)])",
            "                }",
            "        elif self.dev_type == 'VF':",
            "            prod_id = self.prod_id or VF_PROD_ID",
            "            prod_name = self.prod_name or VF_PROD_NAME",
            "            driver = self.driver_name or VF_DRIVER_NAME",
            "            if not skip_capability:",
            "                capability = self.cap_templ % {",
            "                    'cap_type': VF_CAP_TYPE,",
            "                    'addresses': self.addr_templ % {",
            "                        'bus': self.bus,",
            "                        # this is the slot, function value of the parent PF",
            "                        # if we're e.g. device 8, we'll have a different slot",
            "                        # to our parent so reverse this",
            "                        'slot': self.slot - ((self.vf_ratio + 1) // 8),",
            "                        # the parent PF is always function 0",
            "                        'function': 0,",
            "                    }",
            "                }",
            "        elif self.dev_type == 'MDEV_TYPES':",
            "            prod_id = self.prod_id or MDEV_CAPABLE_PROD_ID",
            "            prod_name = self.prod_name or MDEV_CAPABLE_PROD_NAME",
            "            driver = self.driver_name or MDEV_CAPABLE_DRIVER_NAME",
            "            vend_id = self.vend_id or MDEV_CAPABLE_VEND_ID",
            "            vend_name = self.vend_name or MDEV_CAPABLE_VEND_NAME",
            "            types = [self.mdevtypes_templ % {",
            "                'type_id': NVIDIA_11_VGPU_TYPE,",
            "                'instances': 16,",
            "            }]",
            "            if self.multiple_gpu_types:",
            "                types.append(self.mdevtypes_templ % {",
            "                    'type_id': NVIDIA_12_VGPU_TYPE,",
            "                    'instances': 8,",
            "                })",
            "            if self.generic_types:",
            "                types = [self.mdevtypes_templ % {",
            "                    'type_id': MLX5_CORE_TYPE,",
            "                    'instances': 16,",
            "                }]",
            "                types.append(self.mdevtypes_templ % {",
            "                    'type_id': NVIDIA_12_VGPU_TYPE,",
            "                    'instances': 8,",
            "                })",
            "            if not skip_capability:",
            "                capability = self.cap_templ % {",
            "                    'cap_type': MDEV_CAPABLE_CAP_TYPE,",
            "                    'addresses': '\\n'.join(types)",
            "                }",
            "            self.is_capable_of_mdevs = True",
            "",
            "        parent = self.pci_default_parent",
            "        if self.parent:",
            "            parent = _get_libvirt_nodedev_name(*self.parent)",
            "",
            "        self.pci_device = self.pci_device_template % {",
            "            'bus': self.bus,",
            "            'slot': self.slot,",
            "            'function': self.function,",
            "            'vend_id': vend_id,",
            "            'vend_name': vend_name,",
            "            'prod_id': prod_id,",
            "            'prod_name': prod_name,",
            "            'driver': driver,",
            "            'capability': capability,",
            "            'vpd_capability': self.format_vpd_cap(),",
            "            'iommu_group': self.iommu_group,",
            "            'numa_node': self.numa_node,",
            "            'parent': parent,",
            "        }",
            "        # -1 is the sentinel set in /sys/bus/pci/devices/*/numa_node",
            "        # for no NUMA affinity. When the numa_node is set to -1 on a device",
            "        # Libvirt omits the NUMA element so we remove it.",
            "        if self.numa_node == -1:",
            "            self.pci_device = self.pci_device.replace(\"<numa node='-1'/>\", \"\")",
            "",
            "    def format_vpd_cap(self):",
            "        if not self.vpd_fields:",
            "            return ''",
            "        fields = []",
            "        for access_type in ('readonly', 'readwrite'):",
            "            section_fields = []",
            "            for field_name, field_value in self.vpd_fields.get(",
            "                    access_type, {}).items():",
            "                section_fields.append(self.vpd_field_templ % {",
            "                    'field_name': field_name,",
            "                    'field_value': field_value,",
            "                })",
            "            if section_fields:",
            "                fields.append(",
            "                    self.vpd_fields_templ % {",
            "                        'access': access_type,",
            "                        'section_fields': '\\n'.join(section_fields),",
            "                    }",
            "                )",
            "        return self.vpd_cap_templ % {",
            "            'name': self.vpd_fields.get('name', ''),",
            "            'fields': '\\n'.join(fields)",
            "        }",
            "",
            "    def XMLDesc(self, flags):",
            "        return self.pci_device",
            "",
            "    @property",
            "    def address(self):",
            "        return \"0000:%02x:%02x.%1x\" % (self.bus, self.slot, self.function)",
            "",
            "",
            "# TODO(stephenfin): Remove all of these HostFooDevicesInfo objects in favour of",
            "# a unified devices object",
            "class HostPCIDevicesInfo(object):",
            "    \"\"\"Represent a pool of host PCI devices.\"\"\"",
            "",
            "    TOTAL_NUMA_NODES = 2",
            "",
            "    def __init__(self, num_pci=0, num_pfs=2, num_vfs=8, num_mdevcap=0,",
            "                 numa_node=None, multiple_gpu_types=False,",
            "                 generic_types=False):",
            "        \"\"\"Create a new HostPCIDevicesInfo object.",
            "",
            "        :param num_pci: (int) The number of (non-SR-IOV) and (non-MDEV capable)",
            "            PCI devices.",
            "        :param num_pfs: (int) The number of PCI SR-IOV Physical Functions.",
            "        :param num_vfs: (int) The number of PCI SR-IOV Virtual Functions.",
            "        :param num_mdevcap: (int) The number of PCI devices capable of creating",
            "            mediated devices.",
            "        :param numa_node: (int) NUMA node of the device; if set all of the",
            "            devices will be assigned to the specified node else they will be",
            "            split between ``$TOTAL_NUMA_NODES`` nodes.",
            "        :param multiple_gpu_types: (bool) Supports different vGPU types",
            "        :param generic_types: (bool) Supports both nvidia-12 and mlx5 types",
            "        \"\"\"",
            "        self.devices = {}",
            "",
            "        if not (num_vfs or num_pfs or num_pci) and not num_mdevcap:",
            "            return",
            "",
            "        if num_vfs and not num_pfs:",
            "            raise ValueError('Cannot create VFs without PFs')",
            "",
            "        if num_pfs and num_vfs % num_pfs:",
            "            raise ValueError('num_vfs must be a factor of num_pfs')",
            "",
            "        bus = 0x81",
            "        slot = 0x0",
            "        function = 0",
            "        iommu_group = 40  # totally arbitrary number",
            "",
            "        # Generate PCI devs",
            "        for dev in range(num_pci):",
            "            self.add_device(",
            "                dev_type='PCI',",
            "                bus=bus,",
            "                slot=slot,",
            "                function=function,",
            "                iommu_group=iommu_group,",
            "                numa_node=self._calc_numa_node(dev, numa_node))",
            "",
            "            slot += 1",
            "            iommu_group += 1",
            "",
            "        # Generate MDEV capable devs",
            "        for dev in range(num_mdevcap):",
            "            self.add_device(",
            "                dev_type='MDEV_TYPES',",
            "                bus=bus,",
            "                slot=slot,",
            "                function=function,",
            "                iommu_group=iommu_group,",
            "                numa_node=self._calc_numa_node(dev, numa_node),",
            "                multiple_gpu_types=multiple_gpu_types,",
            "                generic_types=generic_types)",
            "",
            "            slot += 1",
            "            iommu_group += 1",
            "",
            "        vf_ratio = num_vfs // num_pfs if num_pfs else 0",
            "",
            "        # Generate PFs",
            "        for dev in range(num_pfs):",
            "            function = 0",
            "            numa_node_pf = self._calc_numa_node(dev, numa_node)",
            "",
            "            self.add_device(",
            "                dev_type='PF',",
            "                bus=bus,",
            "                slot=slot,",
            "                function=function,",
            "                iommu_group=iommu_group,",
            "                numa_node=numa_node_pf,",
            "                vf_ratio=vf_ratio)",
            "",
            "            parent = (bus, slot, function)",
            "            # Generate VFs",
            "            for _ in range(vf_ratio):",
            "                function += 1",
            "                iommu_group += 1",
            "",
            "                if function % 8 == 0:",
            "                    # functions must be 0-7",
            "                    slot += 1",
            "                    function = 0",
            "",
            "                self.add_device(",
            "                    dev_type='VF',",
            "                    bus=bus,",
            "                    slot=slot,",
            "                    function=function,",
            "                    iommu_group=iommu_group,",
            "                    numa_node=numa_node_pf,",
            "                    vf_ratio=vf_ratio,",
            "                    parent=parent)",
            "",
            "            slot += 1",
            "",
            "    def add_device(",
            "        self, dev_type, bus, slot, function, iommu_group, numa_node,",
            "        vf_ratio=None, multiple_gpu_types=False, generic_types=False,",
            "        parent=None, vend_id=None, vend_name=None, prod_id=None,",
            "        prod_name=None, driver_name=None, vpd_fields=None, mac_address=None,",
            "    ):",
            "        pci_dev_name = _get_libvirt_nodedev_name(bus, slot, function)",
            "",
            "        LOG.info('Generating %s device %r', dev_type, pci_dev_name)",
            "",
            "        dev = FakePCIDevice(",
            "            dev_type=dev_type,",
            "            bus=bus,",
            "            slot=slot,",
            "            function=function,",
            "            iommu_group=iommu_group,",
            "            numa_node=numa_node,",
            "            vf_ratio=vf_ratio,",
            "            multiple_gpu_types=multiple_gpu_types,",
            "            generic_types=generic_types,",
            "            parent=parent,",
            "            vend_id=vend_id,",
            "            vend_name=vend_name,",
            "            prod_id=prod_id,",
            "            prod_name=prod_name,",
            "            driver_name=driver_name,",
            "            vpd_fields=vpd_fields,",
            "            mac_address=mac_address,",
            "        )",
            "        self.devices[pci_dev_name] = dev",
            "        return dev",
            "",
            "    @classmethod",
            "    def _calc_numa_node(cls, dev, numa_node):",
            "        return dev % cls.TOTAL_NUMA_NODES if numa_node is None else numa_node",
            "",
            "    def get_all_devices(self):",
            "        return self.devices.keys()",
            "",
            "    def get_device_by_name(self, device_name):",
            "        pci_dev = self.devices.get(device_name)",
            "        return pci_dev",
            "",
            "    def get_all_mdev_capable_devices(self):",
            "        return [dev for dev in self.devices",
            "                if self.devices[dev].is_capable_of_mdevs]",
            "",
            "    def get_pci_address_mac_mapping(self):",
            "        return {",
            "            device.address: device.mac_address",
            "            for dev_addr, device in self.devices.items()",
            "            if device.mac_address",
            "        }",
            "",
            "",
            "class FakeMdevDevice(object):",
            "    template = \"\"\"",
            "    <device>",
            "      <name>%(dev_name)s</name>",
            "      <path>/sys/devices/pci0000:00/0000:00:02.0/%(path)s</path>",
            "      <parent>%(parent)s</parent>",
            "      <driver>",
            "        <name>vfio_mdev</name>",
            "      </driver>",
            "      <capability type='mdev'>",
            "        <type id='%(type_id)s'/>",
            "        <iommuGroup number='12'/>",
            "      </capability>",
            "    </device>",
            "    \"\"\"",
            "",
            "    def __init__(self, dev_name, type_id, parent):",
            "        self.xml = self.template % {",
            "            'dev_name': dev_name, 'type_id': type_id,",
            "            'path': dev_name[len('mdev_'):],",
            "            'parent': parent}",
            "",
            "    def XMLDesc(self, flags):",
            "        return self.xml",
            "",
            "",
            "class HostMdevDevicesInfo(object):",
            "    def __init__(self, devices=None):",
            "        if devices is not None:",
            "            self.devices = devices",
            "        else:",
            "            self.devices = {}",
            "",
            "    def get_all_devices(self):",
            "        return self.devices.keys()",
            "",
            "    def get_device_by_name(self, device_name):",
            "        dev = self.devices[device_name]",
            "        return dev",
            "",
            "",
            "class FakeVDPADevice:",
            "",
            "    template = textwrap.dedent(\"\"\"",
            "        <device>",
            "          <name>%(name)s</name>",
            "          <path>%(path)s</path>",
            "          <parent>%(parent)s</parent>",
            "          <driver>",
            "            <name>vhost_vdpa</name>",
            "          </driver>",
            "          <capability type='vdpa'>",
            "            <chardev>/dev/vhost-vdpa-%(idx)d</chardev>",
            "          </capability>",
            "        </device>\"\"\".strip())",
            "",
            "    def __init__(self, name, idx, parent):",
            "        assert isinstance(parent, FakePCIDevice)",
            "        assert parent.dev_type == 'VF'",
            "",
            "        self.name = name",
            "        self.idx = idx",
            "        self.parent = parent",
            "        self.generate_xml()",
            "",
            "    def generate_xml(self):",
            "        pf_pci = self.parent.parent",
            "        vf_pci = (self.parent.bus, self.parent.slot, self.parent.function)",
            "        pf_addr = '0000:%02x:%02x.%d' % pf_pci",
            "        vf_addr = '0000:%02x:%02x.%d' % vf_pci",
            "        parent = _get_libvirt_nodedev_name(*vf_pci)",
            "        path = f'/sys/devices/pci0000:00/{pf_addr}/{vf_addr}/vdpa{self.idx}'",
            "        self.xml = self.template % {",
            "            'name': self.name,",
            "            'idx': self.idx,",
            "            'path': path,",
            "            'parent': parent,",
            "        }",
            "",
            "    def XMLDesc(self, flags):",
            "        return self.xml",
            "",
            "",
            "class HostVDPADevicesInfo:",
            "",
            "    def __init__(self):",
            "        self.devices = {}",
            "",
            "    def get_all_devices(self):",
            "        return self.devices.keys()",
            "",
            "    def get_device_by_name(self, device_name):",
            "        dev = self.devices[device_name]",
            "        return dev",
            "",
            "    def add_device(self, name, idx, parent):",
            "        LOG.info('Generating vDPA device %r', name)",
            "",
            "        dev = FakeVDPADevice(name=name, idx=idx, parent=parent)",
            "        self.devices[name] = dev",
            "        return dev",
            "",
            "",
            "class HostInfo(object):",
            "",
            "    def __init__(self, cpu_nodes=1, cpu_sockets=1, cpu_cores=2, cpu_threads=1,",
            "                 kB_mem=16780000, mempages=None):",
            "        \"\"\"Create a new Host Info object",
            "",
            "        :param cpu_nodes: (int) the number of NUMA cell, 1 for unusual",
            "                          NUMA topologies or uniform",
            "        :param cpu_sockets: (int) number of CPU sockets per node if nodes > 1,",
            "                            total number of CPU sockets otherwise",
            "        :param cpu_cores: (int) number of cores per socket",
            "        :param cpu_threads: (int) number of threads per core",
            "        :param kB_mem: (int) memory size in KBytes",
            "        \"\"\"",
            "",
            "        self.arch = obj_fields.Architecture.X86_64",
            "        self.kB_mem = kB_mem",
            "        self.cpus = cpu_nodes * cpu_sockets * cpu_cores * cpu_threads",
            "        self.cpu_mhz = 800",
            "        self.cpu_nodes = cpu_nodes",
            "        self.cpu_cores = cpu_cores",
            "        self.cpu_threads = cpu_threads",
            "        self.cpu_sockets = cpu_sockets",
            "        self.cpu_model = \"Penryn\"",
            "        self.cpu_vendor = \"Intel\"",
            "        self.numa_topology = NUMATopology(self.cpu_nodes, self.cpu_sockets,",
            "                                          self.cpu_cores, self.cpu_threads,",
            "                                          self.kB_mem, mempages)",
            "",
            "",
            "class NUMATopology(vconfig.LibvirtConfigCapsNUMATopology):",
            "    \"\"\"A batteries-included variant of LibvirtConfigCapsNUMATopology.",
            "",
            "    Provides sane defaults for LibvirtConfigCapsNUMATopology that can be used",
            "    in tests as is, or overridden where necessary.",
            "    \"\"\"",
            "",
            "    def __init__(self, cpu_nodes=4, cpu_sockets=1, cpu_cores=1, cpu_threads=2,",
            "                 kb_mem=1048576, mempages=None, **kwargs):",
            "",
            "        super(NUMATopology, self).__init__(**kwargs)",
            "",
            "        cpu_count = 0",
            "        cell_count = 0",
            "        for socket_count in range(cpu_sockets):",
            "            for cell_num in range(cpu_nodes):",
            "                cell = vconfig.LibvirtConfigCapsNUMACell()",
            "                cell.id = cell_count",
            "                cell.memory = kb_mem // (cpu_nodes * cpu_sockets)",
            "                for cpu_num in range(cpu_cores * cpu_threads):",
            "                    cpu = vconfig.LibvirtConfigCapsNUMACPU()",
            "                    cpu.id = cpu_count",
            "                    cpu.socket_id = socket_count",
            "                    cpu.core_id = cpu_num // cpu_threads",
            "                    cpu.siblings = set([cpu_threads *",
            "                                       (cpu_count // cpu_threads) + thread",
            "                                        for thread in range(cpu_threads)])",
            "                    cell.cpus.append(cpu)",
            "",
            "                    cpu_count += 1",
            "",
            "                # If no mempages are provided, use only the default 4K pages",
            "                if mempages:",
            "                    cell.mempages = mempages[cell_count]",
            "                else:",
            "                    cell.mempages = create_mempages([(4, cell.memory // 4)])",
            "",
            "                self.cells.append(cell)",
            "",
            "                cell_count += 1",
            "",
            "",
            "def create_mempages(mappings):",
            "    \"\"\"Generate a list of LibvirtConfigCapsNUMAPages objects.",
            "",
            "    :param mappings: (dict) A mapping of page size to quantity of",
            "        said pages.",
            "    :returns: [LibvirtConfigCapsNUMAPages, ...]",
            "    \"\"\"",
            "    mempages = []",
            "",
            "    for page_size, page_qty in mappings:",
            "        mempage = vconfig.LibvirtConfigCapsNUMAPages()",
            "        mempage.size = page_size",
            "        mempage.total = page_qty",
            "        mempages.append(mempage)",
            "",
            "    return mempages",
            "",
            "",
            "VIR_DOMAIN_JOB_NONE = 0",
            "VIR_DOMAIN_JOB_BOUNDED = 1",
            "VIR_DOMAIN_JOB_UNBOUNDED = 2",
            "VIR_DOMAIN_JOB_COMPLETED = 3",
            "VIR_DOMAIN_JOB_FAILED = 4",
            "VIR_DOMAIN_JOB_CANCELLED = 5",
            "",
            "",
            "def _parse_disk_info(element):",
            "    disk_info = {}",
            "    disk_info['type'] = element.get('type', 'file')",
            "    disk_info['device'] = element.get('device', 'disk')",
            "",
            "    driver = element.find('./driver')",
            "    if driver is not None:",
            "        disk_info['driver_name'] = driver.get('name')",
            "        disk_info['driver_type'] = driver.get('type')",
            "",
            "    source = element.find('./source')",
            "    if source is not None:",
            "        disk_info['source'] = source.get('file')",
            "        if not disk_info['source']:",
            "            disk_info['source'] = source.get('dev')",
            "",
            "        if not disk_info['source']:",
            "            disk_info['source'] = source.get('path')",
            "",
            "    target = element.find('./target')",
            "    if target is not None:",
            "        disk_info['target_dev'] = target.get('dev')",
            "        disk_info['target_bus'] = target.get('bus')",
            "",
            "    return disk_info",
            "",
            "",
            "def _parse_nic_info(element):",
            "    nic_info = {}",
            "    nic_info['type'] = element.get('type', 'bridge')",
            "",
            "    driver = element.find('./mac')",
            "    if driver is not None:",
            "        nic_info['mac'] = driver.get('address')",
            "",
            "    source = element.find('./source')",
            "    if source is not None:",
            "        nic_info['source'] = source.get('bridge')",
            "",
            "    target = element.find('./target')",
            "    if target is not None:",
            "        nic_info['target_dev'] = target.get('dev')",
            "",
            "    return nic_info",
            "",
            "",
            "def disable_event_thread(self):",
            "    \"\"\"Disable nova libvirt driver event thread.",
            "",
            "    The Nova libvirt driver includes a native thread which monitors",
            "    the libvirt event channel. In a testing environment this becomes",
            "    problematic because it means we've got a floating thread calling",
            "    sleep(1) over the life of the unit test. Seems harmless? It's not,",
            "    because we sometimes want to test things like retry loops that",
            "    should have specific sleep paterns. An unlucky firing of the",
            "    libvirt thread will cause a test failure.",
            "",
            "    \"\"\"",
            "    # because we are patching a method in a class MonkeyPatch doesn't",
            "    # auto import correctly. Import explicitly otherwise the patching",
            "    # may silently fail.",
            "    import nova.virt.libvirt.host  # noqa",
            "",
            "    def evloop(*args, **kwargs):",
            "        pass",
            "",
            "    self.useFixture(fixtures.MockPatch(",
            "        'nova.virt.libvirt.host.Host._init_events',",
            "        side_effect=evloop))",
            "",
            "",
            "class libvirtError(Exception):",
            "    \"\"\"This class was copied and slightly modified from",
            "    `libvirt-python:libvirt-override.py`.",
            "",
            "    Since a test environment will use the real `libvirt-python` version of",
            "    `libvirtError` if it's installed and not this fake, we need to maintain",
            "    strict compatibility with the original class, including `__init__` args",
            "    and instance-attributes.",
            "",
            "    To create a libvirtError instance you should:",
            "",
            "        # Create an unsupported error exception",
            "        exc = libvirtError('my message')",
            "        exc.err = (libvirt.VIR_ERR_NO_SUPPORT,)",
            "",
            "    self.err is a tuple of form:",
            "        (error_code, error_domain, error_message, error_level, str1, str2,",
            "         str3, int1, int2)",
            "",
            "    Alternatively, you can use the `make_libvirtError` convenience function to",
            "    allow you to specify these attributes in one shot.",
            "    \"\"\"",
            "",
            "    def __init__(self, defmsg, conn=None, dom=None, net=None, pool=None,",
            "                 vol=None):",
            "        Exception.__init__(self, defmsg)",
            "        self.err = None",
            "",
            "    def get_error_code(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[0]",
            "",
            "    def get_error_domain(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[1]",
            "",
            "    def get_error_message(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[2]",
            "",
            "    def get_error_level(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[3]",
            "",
            "    def get_str1(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[4]",
            "",
            "    def get_str2(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[5]",
            "",
            "    def get_str3(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[6]",
            "",
            "    def get_int1(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[7]",
            "",
            "    def get_int2(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[8]",
            "",
            "",
            "class NodeDevice(object):",
            "",
            "    def __init__(self, connection, xml=None):",
            "        self._connection = connection",
            "",
            "        self._xml = xml",
            "        if xml is not None:",
            "            self._parse_xml(xml)",
            "",
            "    def _parse_xml(self, xml):",
            "        tree = etree.fromstring(xml)",
            "        root = tree.find('.')",
            "        self._name = root.find('name').text",
            "        self._parent = root.find('parent').text",
            "",
            "    def attach(self):",
            "        pass",
            "",
            "    def dettach(self):",
            "        pass",
            "",
            "    def reset(self):",
            "        pass",
            "",
            "    def XMLDesc(self, flags: int) -> str:",
            "        return self._xml",
            "",
            "    def parent(self) -> str:",
            "        return self._parent",
            "",
            "    def name(self) -> str:",
            "        return self._name",
            "",
            "    def listCaps(self) -> ty.List[str]:",
            "        return [self.name().split('_')[0]]",
            "",
            "",
            "class Domain(object):",
            "    def __init__(self, connection, xml, running=False, transient=False):",
            "        self._connection = connection",
            "        if running:",
            "            connection._mark_running(self)",
            "",
            "        self._state = running and VIR_DOMAIN_RUNNING or VIR_DOMAIN_SHUTOFF",
            "        self._transient = transient",
            "        self._def = self._parse_definition(xml)",
            "        self._has_saved_state = False",
            "        self._snapshots = {}",
            "        self._id = self._connection._id_counter",
            "        self._job_type = VIR_DOMAIN_JOB_UNBOUNDED",
            "",
            "    def _parse_definition(self, xml):",
            "        try:",
            "            tree = etree.fromstring(xml)",
            "        except etree.ParseError:",
            "            raise make_libvirtError(",
            "                    libvirtError, \"Invalid XML.\",",
            "                    error_code=VIR_ERR_XML_DETAIL,",
            "                    error_domain=VIR_FROM_DOMAIN)",
            "",
            "        definition = {}",
            "",
            "        name = tree.find('./name')",
            "        if name is not None:",
            "            definition['name'] = name.text",
            "",
            "        uuid_elem = tree.find('./uuid')",
            "        if uuid_elem is not None:",
            "            definition['uuid'] = uuid_elem.text",
            "        else:",
            "            definition['uuid'] = uuids.fake",
            "",
            "        vcpu = tree.find('./vcpu')",
            "        if vcpu is not None:",
            "            definition['vcpu'] = int(vcpu.text)",
            "",
            "        memory = tree.find('./memory')",
            "        if memory is not None:",
            "            definition['memory'] = int(memory.text)",
            "",
            "        os = {}",
            "        os_type = tree.find('./os/type')",
            "        if os_type is not None:",
            "            os['type'] = os_type.text",
            "            os['arch'] = os_type.get('arch', self._connection.host_info.arch)",
            "",
            "        os_kernel = tree.find('./os/kernel')",
            "        if os_kernel is not None:",
            "            os['kernel'] = os_kernel.text",
            "",
            "        os_initrd = tree.find('./os/initrd')",
            "        if os_initrd is not None:",
            "            os['initrd'] = os_initrd.text",
            "",
            "        os_cmdline = tree.find('./os/cmdline')",
            "        if os_cmdline is not None:",
            "            os['cmdline'] = os_cmdline.text",
            "",
            "        os_boot = tree.find('./os/boot')",
            "        if os_boot is not None:",
            "            os['boot_dev'] = os_boot.get('dev')",
            "",
            "        definition['os'] = os",
            "",
            "        features = {}",
            "",
            "        acpi = tree.find('./features/acpi')",
            "        if acpi is not None:",
            "            features['acpi'] = True",
            "",
            "        definition['features'] = features",
            "",
            "        cpu_pins = {}",
            "",
            "        pins = tree.findall('./cputune/vcpupin')",
            "        for pin in pins:",
            "            cpu_pins[pin.get('vcpu')] = pin.get('cpuset')",
            "",
            "        definition['cpu_pins'] = cpu_pins",
            "",
            "        emulator_pin = tree.find('./cputune/emulatorpin')",
            "        if emulator_pin is not None:",
            "            definition['emulator_pin'] = emulator_pin.get('cpuset')",
            "",
            "        memnodes = {}",
            "",
            "        for node in tree.findall('./numatune/memnode'):",
            "            memnodes[node.get('cellid')] = node.get('nodeset')",
            "",
            "        definition['memnodes'] = memnodes",
            "",
            "        devices = {}",
            "",
            "        device_nodes = tree.find('./devices')",
            "        if device_nodes is not None:",
            "            disks_info = []",
            "            disks = device_nodes.findall('./disk')",
            "            for disk in disks:",
            "                disks_info += [_parse_disk_info(disk)]",
            "            devices['disks'] = disks_info",
            "",
            "            nics_info = []",
            "            nics = device_nodes.findall('./interface')",
            "            for nic in nics:",
            "                nic_info = {}",
            "                nic_info['type'] = nic.get('type')",
            "",
            "                mac = nic.find('./mac')",
            "                if mac is not None:",
            "                    nic_info['mac'] = mac.get('address')",
            "",
            "                source = nic.find('./source')",
            "                if source is not None:",
            "                    if nic_info['type'] == 'network':",
            "                        nic_info['source'] = source.get('network')",
            "                    elif nic_info['type'] == 'bridge':",
            "                        nic_info['source'] = source.get('bridge')",
            "                    elif nic_info['type'] == 'hostdev':",
            "                        # <interface type='hostdev'> is for VF when vnic_type",
            "                        # is direct. Add sriov vf pci information in nic_info",
            "                        address = source.find('./address')",
            "                        pci_type = address.get('type')",
            "                        pci_domain = address.get('domain').replace('0x', '')",
            "                        pci_bus = address.get('bus').replace('0x', '')",
            "                        pci_slot = address.get('slot').replace('0x', '')",
            "                        pci_function = address.get('function').replace(",
            "                            '0x', '')",
            "                        pci_device = \"%s_%s_%s_%s_%s\" % (pci_type, pci_domain,",
            "                                                         pci_bus, pci_slot,",
            "                                                         pci_function)",
            "                        nic_info['source'] = pci_device",
            "                    elif nic_info['type'] == 'vdpa':",
            "                        nic_info['source'] = source.get('dev')",
            "",
            "                nics_info += [nic_info]",
            "",
            "            devices['nics'] = nics_info",
            "",
            "            hostdev_info = []",
            "            hostdevs = device_nodes.findall('./hostdev')",
            "            for hostdev in hostdevs:",
            "                address = hostdev.find('./source/address')",
            "                # NOTE(gibi): only handle mdevs as pci is complicated",
            "                dev_type = hostdev.get('type')",
            "                if dev_type == 'mdev':",
            "                    hostdev_info.append({",
            "                        'type': dev_type,",
            "                        'model': hostdev.get('model'),",
            "                        'address_uuid': address.get('uuid')",
            "                    })",
            "            devices['hostdevs'] = hostdev_info",
            "",
            "            vpmem_info = []",
            "            vpmems = device_nodes.findall('./memory')",
            "            for vpmem in vpmems:",
            "                model = vpmem.get('model')",
            "                if model == 'nvdimm':",
            "                    source = vpmem.find('./source')",
            "                    target = vpmem.find('./target')",
            "                    path = source.find('./path').text",
            "                    alignsize = source.find('./alignsize').text",
            "                    size = target.find('./size').text",
            "                    node = target.find('./node').text",
            "                    vpmem_info.append({",
            "                        'path': path,",
            "                        'size': size,",
            "                        'alignsize': alignsize,",
            "                        'node': node})",
            "            devices['vpmems'] = vpmem_info",
            "",
            "        definition['devices'] = devices",
            "",
            "        return definition",
            "",
            "    def verify_hostdevs_interface_are_vfs(self):",
            "        \"\"\"Verify for interface type hostdev if the pci device is VF or not.",
            "        \"\"\"",
            "",
            "        error_message = (\"Interface type hostdev is currently supported on \"",
            "                         \"SR-IOV Virtual Functions only\")",
            "",
            "        nics = self._def['devices'].get('nics', [])",
            "        for nic in nics:",
            "            if nic['type'] == 'hostdev':",
            "                pci_device = nic['source']",
            "                pci_info_from_connection = self._connection.pci_info.devices[",
            "                    pci_device]",
            "                if 'phys_function' not in pci_info_from_connection.pci_device:",
            "                    raise make_libvirtError(",
            "                        libvirtError,",
            "                        error_message,",
            "                        error_code=VIR_ERR_CONFIG_UNSUPPORTED,",
            "                        error_domain=VIR_FROM_DOMAIN)",
            "",
            "    def create(self):",
            "        self.createWithFlags(0)",
            "",
            "    def createWithFlags(self, flags):",
            "        # FIXME: Not handling flags at the moment",
            "        self.verify_hostdevs_interface_are_vfs()",
            "        self._state = VIR_DOMAIN_RUNNING",
            "        self._connection._mark_running(self)",
            "        self._has_saved_state = False",
            "",
            "    def isActive(self):",
            "        return int(self._state == VIR_DOMAIN_RUNNING)",
            "",
            "    def undefine(self):",
            "        self._connection._undefine(self)",
            "",
            "    def isPersistent(self):",
            "        return True",
            "",
            "    def undefineFlags(self, flags):",
            "        self.undefine()",
            "        if flags & VIR_DOMAIN_UNDEFINE_MANAGED_SAVE:",
            "            if self.hasManagedSaveImage(0):",
            "                self.managedSaveRemove()",
            "",
            "    def destroy(self):",
            "        self._state = VIR_DOMAIN_SHUTOFF",
            "        self._connection._mark_not_running(self)",
            "",
            "    def ID(self):",
            "        return self._id",
            "",
            "    def name(self):",
            "        return self._def['name']",
            "",
            "    def UUIDString(self):",
            "        return self._def['uuid']",
            "",
            "    def interfaceStats(self, device):",
            "        return [10000242400, 1234, 0, 2, 213412343233, 34214234, 23, 3]",
            "",
            "    def blockStats(self, device):",
            "        return [2, 10000242400, 234, 2343424234, 34]",
            "",
            "    def setTime(self, time=None, flags=0):",
            "        pass",
            "",
            "    def suspend(self):",
            "        self._state = VIR_DOMAIN_PAUSED",
            "",
            "    def shutdown(self):",
            "        self._state = VIR_DOMAIN_SHUTDOWN",
            "        self._connection._mark_not_running(self)",
            "",
            "    def reset(self, flags):",
            "        # FIXME: Not handling flags at the moment",
            "        self._state = VIR_DOMAIN_RUNNING",
            "        self._connection._mark_running(self)",
            "",
            "    def info(self):",
            "        return [self._state,",
            "                int(self._def['memory']),",
            "                int(self._def['memory']),",
            "                self._def['vcpu'],",
            "                123456789]",
            "",
            "    def migrateToURI3(self, dconnuri, params, flags):",
            "        raise make_libvirtError(",
            "                libvirtError,",
            "                \"Migration always fails for fake libvirt!\",",
            "                error_code=VIR_ERR_INTERNAL_ERROR,",
            "                error_domain=VIR_FROM_QEMU)",
            "",
            "    def migrateSetMaxDowntime(self, downtime):",
            "        pass",
            "",
            "    def attachDevice(self, xml):",
            "        result = False",
            "        if xml.startswith(\"<disk\"):",
            "            disk_info = _parse_disk_info(etree.fromstring(xml))",
            "            disk_info['_attached'] = True",
            "            self._def['devices']['disks'] += [disk_info]",
            "            result = True",
            "        elif xml.startswith(\"<interface\"):",
            "            nic_info = _parse_nic_info(etree.fromstring(xml))",
            "            nic_info['_attached'] = True",
            "            self._def['devices']['nics'] += [nic_info]",
            "            result = True",
            "        else:",
            "            # FIXME(sean-k-mooney): We don't currently handle attaching",
            "            # or detaching hostdevs but we have tests that assume we do so",
            "            # this is an error not an exception. This affects PCI passthough,",
            "            # vGPUs and PF neutron ports.",
            "            LOG.error(",
            "                \"Trying to attach an unsupported device type.\"",
            "                \"The fakelibvirt implementation is incomplete \"",
            "                \"and should be extended to support %s: %s\",",
            "                xml, self._def['devices'])",
            "",
            "        return result",
            "",
            "    def attachDeviceFlags(self, xml, flags):",
            "        if (flags & VIR_DOMAIN_AFFECT_LIVE and",
            "                self._state != VIR_DOMAIN_RUNNING):",
            "            raise make_libvirtError(",
            "                libvirtError,",
            "                \"AFFECT_LIVE only allowed for running domains!\",",
            "                error_code=VIR_ERR_INTERNAL_ERROR,",
            "                error_domain=VIR_FROM_QEMU)",
            "        self.attachDevice(xml)",
            "",
            "    def detachDevice(self, xml):",
            "        # detachDevice is a common function used for all devices types",
            "        # so we need to handle each separately",
            "        if xml.startswith(\"<disk\"):",
            "            disk_info = _parse_disk_info(etree.fromstring(xml))",
            "            attached_disk_info = None",
            "            for attached_disk in self._def['devices']['disks']:",
            "                if attached_disk['target_dev'] == disk_info.get('target_dev'):",
            "                    attached_disk_info = attached_disk",
            "                    break",
            "",
            "            if attached_disk_info:",
            "                self._def['devices']['disks'].remove(attached_disk_info)",
            "",
            "            return attached_disk_info is not None",
            "",
            "        if xml.startswith(\"<interface\"):",
            "            nic_info = _parse_nic_info(etree.fromstring(xml))",
            "            attached_nic_info = None",
            "            for attached_nic in self._def['devices']['nics']:",
            "                if attached_nic['mac'] == nic_info['mac']:",
            "                    attached_nic_info = attached_nic",
            "                    break",
            "",
            "            if attached_nic_info:",
            "                self._def['devices']['nics'].remove(attached_nic_info)",
            "",
            "            return attached_nic_info is not None",
            "",
            "        # FIXME(sean-k-mooney): We don't currently handle attaching or",
            "        # detaching hostdevs but we have tests that assume we do so this is",
            "        # an error not an exception. This affects PCI passthough, vGPUs and",
            "        # PF neutron ports",
            "        LOG.error(",
            "            \"Trying to detach an unsupported device type.\"",
            "            \"The fakelibvirt implementation is incomplete \"",
            "            \"and should be extended to support %s: %s\",",
            "            xml, self._def['devices'])",
            "",
            "        return False",
            "",
            "    def detachDeviceFlags(self, xml, flags):",
            "        self.detachDevice(xml)",
            "",
            "    def setUserPassword(self, user, password, flags=0):",
            "        pass",
            "",
            "    def XMLDesc(self, flags):",
            "        disks = ''",
            "        for disk in self._def['devices']['disks']:",
            "            if disk['type'] == 'file':",
            "                source_attr = 'file'",
            "            else:",
            "                source_attr = 'dev'",
            "",
            "            disks += '''<disk type='%(type)s' device='%(device)s'>",
            "      <driver name='%(driver_name)s' type='%(driver_type)s'/>",
            "      <source %(source_attr)s='%(source)s'/>",
            "      <target dev='%(target_dev)s' bus='%(target_bus)s'/>",
            "      <address type='drive' controller='0' bus='0' unit='0'/>",
            "    </disk>''' % dict(source_attr=source_attr, **disk)",
            "        nics = ''",
            "        for func, nic in enumerate(self._def['devices']['nics']):",
            "            if func > 7:",
            "                # this should never be raised but is just present to highlight",
            "                # the limitations of the current fake when writing new tests.",
            "                # if you see this raised when add a new test you will need",
            "                # to extend this fake to use both functions and slots.",
            "                # the pci function is limited to 3 bits or 0-7.",
            "                raise RuntimeError(",
            "                    'Test attempts to add more than 8 PCI devices. This is '",
            "                    'not supported by the fake libvirt implementation.')",
            "            nic['func'] = func",
            "            if nic['type'] in ('ethernet',):",
            "                # this branch covers kernel ovs interfaces",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <target dev='tap274487d1-6%(func)s'/>",
            "          <address type='pci' domain='0x0000' bus='0x00' slot='0x03'",
            "                   function='0x%(func)s'/>",
            "        </interface>''' % nic",
            "            elif nic['type'] in ('vdpa',):",
            "                # this branch covers hardware offloaded ovs with vdpa",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <source dev='%(source)s'/>",
            "          <address type='pci' domain='0x0000' bus='0x00' slot='0x03'",
            "                   function='0x%(func)s'/>",
            "        </interface>''' % nic",
            "            # this branch covers most interface types with a source",
            "            # such as linux bridge interfaces.",
            "            elif 'source' in nic:",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <source %(type)s='%(source)s'/>",
            "          <target dev='tap274487d1-6%(func)s'/>",
            "          <address type='pci' domain='0x0000' bus='0x00' slot='0x03'",
            "                   function='0x%(func)s'/>",
            "        </interface>''' % nic",
            "            else:",
            "                # This branch covers the macvtap vnic-type.",
            "                # This is incomplete as the source dev should be unique",
            "                # and map to the VF netdev name but due to the mocking in",
            "                # the fixture we hard code it.",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <source dev='fake_pf_interface_name' mode='passthrough'>",
            "              <address type='pci' domain='0x0000' bus='0x81' slot='0x00'",
            "                   function='0x%(func)s'/>",
            "          </source>",
            "        </interface>''' % nic",
            "",
            "        hostdevs = ''",
            "        for hostdev in self._def['devices']['hostdevs']:",
            "            hostdevs += '''<hostdev mode='subsystem' type='%(type)s' model='%(model)s'>",
            "    <source>",
            "      <address uuid='%(address_uuid)s'/>",
            "    </source>",
            "    </hostdev>",
            "            ''' % hostdev  # noqa",
            "",
            "        vpmems = ''",
            "        for vpmem in self._def['devices']['vpmems']:",
            "            vpmems += '''",
            "    <memory model='nvdimm' access='shared'>",
            "      <source>",
            "        <path>%(path)s</path>",
            "        <alignsize>%(alignsize)s</alignsize>",
            "        <pmem/>",
            "      </source>",
            "      <target>",
            "        <size>%(size)s</size>",
            "        <node>%(node)s</node>",
            "        <label>",
            "          <size>2097152</size>",
            "        </label>",
            "      </target>",
            "    </memory>",
            "            ''' % vpmem",
            "        cputune = ''",
            "        for vcpu, cpuset in self._def['cpu_pins'].items():",
            "            cputune += '<vcpupin vcpu=\"%d\" cpuset=\"%s\"/>' % (int(vcpu), cpuset)",
            "        emulatorpin = None",
            "        if 'emulator_pin' in self._def:",
            "            emulatorpin = ('<emulatorpin cpuset=\"%s\"/>' %",
            "                           self._def['emulator_pin'])",
            "        if cputune or emulatorpin:",
            "            cputune = '<cputune>%s%s</cputune>' % (emulatorpin, cputune)",
            "",
            "        numatune = ''",
            "        for cellid, nodeset in self._def['memnodes'].items():",
            "            numatune += '<memnode cellid=\"%d\" nodeset=\"%s\"/>' % (int(cellid),",
            "                                                                 nodeset)",
            "            numatune += '<memory nodeset=\"%s\"/>' % ','.join(",
            "                self._def['memnodes'].values())",
            "        if numatune:",
            "            numatune = '<numatune>%s</numatune>' % numatune",
            "",
            "        serial_console = ''",
            "        if CONF.serial_console.enabled:",
            "            serial_console = \"\"\"<serial type=\"tcp\">",
            "                <source host=\"-1\" service=\"-1\" mode=\"bind\"/>",
            "                </serial>\"\"\"",
            "",
            "        return '''<domain type='kvm'>",
            "  <name>%(name)s</name>",
            "  <uuid>%(uuid)s</uuid>",
            "  <memory>%(memory)s</memory>",
            "  <currentMemory>%(memory)s</currentMemory>",
            "  <vcpu>%(vcpu)s</vcpu>",
            "  <os>",
            "    <type arch='%(arch)s' machine='pc-0.12'>hvm</type>",
            "    <boot dev='hd'/>",
            "  </os>",
            "  <features>",
            "    <acpi/>",
            "    <apic/>",
            "    <pae/>",
            "  </features>",
            "  <clock offset='localtime'/>",
            "  <on_poweroff>destroy</on_poweroff>",
            "  <on_reboot>restart</on_reboot>",
            "  <on_crash>restart</on_crash>",
            "  %(cputune)s",
            "  %(numatune)s",
            "  <devices>",
            "    <emulator>/usr/bin/kvm</emulator>",
            "    %(disks)s",
            "    <controller type='ide' index='0'>",
            "      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'",
            "               function='0x1'/>",
            "    </controller>",
            "    %(nics)s",
            "    %(serial_console)s",
            "    <console type='file'>",
            "      <source path='dummy.log'/>",
            "      <target port='0'/>",
            "    </console>",
            "    <input type='tablet' bus='usb'/>",
            "    <input type='mouse' bus='ps2'/>",
            "    <graphics type='vnc' port='-1' autoport='yes'/>",
            "    <graphics type='spice' port='-1' autoport='yes'/>",
            "    <video>",
            "      <model type='cirrus' vram='9216' heads='1'/>",
            "      <address type='pci' domain='0x0000' bus='0x00' slot='0x02'",
            "               function='0x0'/>",
            "    </video>",
            "    <memballoon model='virtio'>",
            "      <address type='pci' domain='0x0000' bus='0x00' slot='0x04'",
            "               function='0x0'/>",
            "    </memballoon>",
            "    %(hostdevs)s",
            "    %(vpmems)s",
            "  </devices>",
            "</domain>''' % {'name': self._def['name'],",
            "                'uuid': self._def['uuid'],",
            "                'memory': self._def['memory'],",
            "                'vcpu': self._def['vcpu'],",
            "                'arch': self._def['os']['arch'],",
            "                'disks': disks,",
            "                'nics': nics,",
            "                'hostdevs': hostdevs,",
            "                'vpmems': vpmems,",
            "                'serial_console': serial_console,",
            "                'cputune': cputune,",
            "                'numatune': numatune}",
            "",
            "    def managedSave(self, flags):",
            "        self._connection._mark_not_running(self)",
            "        self._has_saved_state = True",
            "",
            "    def managedSaveRemove(self, flags):",
            "        self._has_saved_state = False",
            "",
            "    def hasManagedSaveImage(self, flags):",
            "        return int(self._has_saved_state)",
            "",
            "    def resume(self):",
            "        self._state = VIR_DOMAIN_RUNNING",
            "",
            "    def snapshotCreateXML(self, xml, flags):",
            "        tree = etree.fromstring(xml)",
            "        name = tree.find('./name').text",
            "        snapshot = DomainSnapshot(name, self)",
            "        self._snapshots[name] = snapshot",
            "        return snapshot",
            "",
            "    def vcpus(self):",
            "        vcpus = ([], [])",
            "        for i in range(0, self._def['vcpu']):",
            "            vcpus[0].append((i, 1, 120405, i))",
            "            vcpus[1].append((True, True, True, True))",
            "        return vcpus",
            "",
            "    def memoryStats(self):",
            "        return {}",
            "",
            "    def maxMemory(self):",
            "        return self._def['memory']",
            "",
            "    def blockJobInfo(self, disk, flags):",
            "        return {}",
            "",
            "    def blockJobAbort(self, disk, flags):",
            "        pass",
            "",
            "    def blockResize(self, disk, size, flags):",
            "        pass",
            "",
            "    def blockRebase(self, disk, base, bandwidth=0, flags=0):",
            "        if (not base) and (flags and VIR_DOMAIN_BLOCK_REBASE_RELATIVE):",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                    'flag VIR_DOMAIN_BLOCK_REBASE_RELATIVE is '",
            "                    'valid only with non-null base',",
            "                    error_code=VIR_ERR_INVALID_ARG,",
            "                    error_domain=VIR_FROM_QEMU)",
            "        return 0",
            "",
            "    def blockCopy(self, disk, base, flags=0):",
            "        return 0",
            "",
            "    def blockCommit(self, disk, base, top, flags):",
            "        return 0",
            "",
            "    def jobInfo(self):",
            "        # NOTE(danms): This is an array of 12 integers, so just report",
            "        # something to avoid an IndexError if we look at this",
            "        return [0] * 12",
            "",
            "    def jobStats(self, flags=0):",
            "        # NOTE(artom) By returning VIR_DOMAIN_JOB_UNBOUNDED, we're pretending a",
            "        # job is constantly running. Tests are expected to call the",
            "        # complete_job or fail_job methods when they're ready for jobs (read:",
            "        # live migrations) to \"complete\".",
            "        return {'type': self._job_type}",
            "",
            "    def complete_job(self):",
            "        self._job_type = VIR_DOMAIN_JOB_COMPLETED",
            "",
            "    def fail_job(self):",
            "        self._job_type = VIR_DOMAIN_JOB_FAILED",
            "",
            "    def injectNMI(self, flags=0):",
            "        return 0",
            "",
            "    def abortJob(self):",
            "        pass",
            "",
            "    def fsFreeze(self):",
            "        pass",
            "",
            "    def fsThaw(self):",
            "        pass",
            "",
            "    def setMetadata(self, metadata_type, metadata, key, uri, flags=0):",
            "        pass",
            "",
            "",
            "class DomainSnapshot(object):",
            "    def __init__(self, name, domain):",
            "        self._name = name",
            "        self._domain = domain",
            "",
            "    def delete(self, flags):",
            "        del self._domain._snapshots[self._name]",
            "",
            "",
            "class Secret(object):",
            "    def __init__(self, connection, xml):",
            "        self._connection = connection",
            "        self._xml = xml",
            "        self._parse_xml(xml)",
            "        self._value = None",
            "",
            "    def _parse_xml(self, xml):",
            "        tree = etree.fromstring(xml)",
            "        self._uuid = tree.find('./uuid').text",
            "        self._private = tree.get('private') == 'yes'",
            "",
            "    def setValue(self, value, flags=0):",
            "        self._value = value",
            "        return 0",
            "",
            "    def value(self, flags=0):",
            "        if self._value is None:",
            "            raise make_libvirtError(",
            "                libvirtError,",
            "                \"secret '%s' does not have a value\" % self._uuid,",
            "                error_code=VIR_ERR_NO_SECRET,",
            "                error_domain=VIR_FROM_SECRET)",
            "            pass",
            "",
            "        if self._private:",
            "            raise make_libvirtError(",
            "                libvirtError,",
            "                'secret is private',",
            "                error_code=VIR_ERR_INVALID_SECRET,",
            "                error_domain=VIR_FROM_SECRET)",
            "",
            "        return self._value",
            "",
            "    def undefine(self):",
            "        self._connection._remove_secret(self)",
            "",
            "",
            "class Connection(object):",
            "    def __init__(",
            "        self, uri=None, readonly=False, version=FAKE_LIBVIRT_VERSION,",
            "        hv_version=FAKE_QEMU_VERSION, hostname=None,",
            "        host_info=None, pci_info=None, mdev_info=None, vdpa_info=None,",
            "    ):",
            "        if not uri or uri == '':",
            "            if allow_default_uri_connection:",
            "                uri = 'qemu:///session'",
            "            else:",
            "                raise ValueError(\"URI was None, but fake libvirt is \"",
            "                                 \"configured to not accept this.\")",
            "",
            "        uri_whitelist = [",
            "            'qemu:///system',",
            "            'qemu:///session',",
            "            'lxc:///',     # from LibvirtDriver._uri()",
            "            'test:///default',",
            "            'parallels:///system',",
            "        ]",
            "",
            "        if uri not in uri_whitelist:",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                   \"libvirt error: no connection driver \"",
            "                   \"available for No connection for URI %s\" % uri,",
            "                   error_code=5, error_domain=0)",
            "",
            "        self.readonly = readonly",
            "        self._uri = uri",
            "        self._vms = {}",
            "        self._running_vms = {}",
            "        self._id_counter = 1  # libvirt reserves 0 for the hypervisor.",
            "        self._nodedevs = {}",
            "        self._secrets = {}",
            "        self._event_callbacks = {}",
            "        self.fakeLibVersion = version",
            "        self.fakeVersion = hv_version",
            "        self.host_info = host_info or HostInfo()",
            "        self.pci_info = pci_info or HostPCIDevicesInfo(num_pci=0,",
            "                                                       num_pfs=0,",
            "                                                       num_vfs=0)",
            "        self.mdev_info = mdev_info or HostMdevDevicesInfo(devices={})",
            "        self.vdpa_info = vdpa_info or HostVDPADevicesInfo()",
            "        self.hostname = hostname or 'compute1'",
            "",
            "    def _add_nodedev(self, nodedev):",
            "        self._nodedevs[nodedev._name] = nodedev",
            "",
            "    def _remove_nodedev(self, nodedev):",
            "        del self._nodedevs[nodedev._name]",
            "",
            "    def _add_secret(self, secret):",
            "        self._secrets[secret._uuid] = secret",
            "",
            "    def _remove_secret(self, secret):",
            "        del self._secrets[secret._uuid]",
            "",
            "    def _mark_running(self, dom):",
            "        self._running_vms[self._id_counter] = dom",
            "        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STARTED, 0)",
            "        self._id_counter += 1",
            "",
            "    def _mark_not_running(self, dom):",
            "        if dom._transient:",
            "            self._undefine(dom)",
            "",
            "        dom._id = -1",
            "",
            "        for (k, v) in self._running_vms.items():",
            "            if v == dom:",
            "                del self._running_vms[k]",
            "                self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STOPPED, 0)",
            "                return",
            "",
            "    def _undefine(self, dom):",
            "        del self._vms[dom.name()]",
            "        if not dom._transient:",
            "            self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_UNDEFINED, 0)",
            "",
            "    def getInfo(self):",
            "        return [self.host_info.arch,",
            "                self.host_info.kB_mem,",
            "                self.host_info.cpus,",
            "                self.host_info.cpu_mhz,",
            "                self.host_info.cpu_nodes,",
            "                self.host_info.cpu_sockets,",
            "                self.host_info.cpu_cores,",
            "                self.host_info.cpu_threads]",
            "",
            "    def lookupByUUIDString(self, uuid):",
            "        for vm in self._vms.values():",
            "            if vm.UUIDString() == uuid:",
            "                return vm",
            "        raise make_libvirtError(",
            "                libvirtError,",
            "                'Domain not found: no domain with matching uuid \"%s\"' % uuid,",
            "                error_code=VIR_ERR_NO_DOMAIN,",
            "                error_domain=VIR_FROM_QEMU)",
            "",
            "    def listAllDomains(self, flags=None):",
            "        vms = []",
            "        for vm in self._vms.values():",
            "            if flags & VIR_CONNECT_LIST_DOMAINS_ACTIVE:",
            "                if vm._state != VIR_DOMAIN_SHUTOFF:",
            "                    vms.append(vm)",
            "            if flags & VIR_CONNECT_LIST_DOMAINS_INACTIVE:",
            "                if vm._state == VIR_DOMAIN_SHUTOFF:",
            "                    vms.append(vm)",
            "        return vms",
            "",
            "    def _emit_lifecycle(self, dom, event, detail):",
            "        if VIR_DOMAIN_EVENT_ID_LIFECYCLE not in self._event_callbacks:",
            "            return",
            "",
            "        cbinfo = self._event_callbacks[VIR_DOMAIN_EVENT_ID_LIFECYCLE]",
            "        callback = cbinfo[0]",
            "        opaque = cbinfo[1]",
            "        callback(self, dom, event, detail, opaque)",
            "",
            "    def defineXML(self, xml):",
            "        dom = Domain(connection=self, running=False, transient=False, xml=xml)",
            "        self._vms[dom.name()] = dom",
            "        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_DEFINED, 0)",
            "        return dom",
            "",
            "    def createXML(self, xml, flags):",
            "        dom = Domain(connection=self, running=True, transient=True, xml=xml)",
            "        self._vms[dom.name()] = dom",
            "        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STARTED, 0)",
            "        return dom",
            "",
            "    def getType(self):",
            "        if self._uri == 'qemu:///system':",
            "            return 'QEMU'",
            "",
            "    def getLibVersion(self):",
            "        return self.fakeLibVersion",
            "",
            "    def getVersion(self):",
            "        return self.fakeVersion",
            "",
            "    def getHostname(self):",
            "        return self.hostname",
            "",
            "    def domainEventRegisterAny(self, dom, eventid, callback, opaque):",
            "        self._event_callbacks[eventid] = [callback, opaque]",
            "",
            "    def registerCloseCallback(self, cb, opaque):",
            "        pass",
            "",
            "    def getCPUMap(self):",
            "        \"\"\"Return calculated CPU map from HostInfo, by default showing 2",
            "           online CPUs.",
            "        \"\"\"",
            "        total_cpus = self.host_info.cpus",
            "        cpu_map = [True for cpu_num in range(total_cpus)]",
            "        return (total_cpus, cpu_map, total_cpus)",
            "",
            "    def getDomainCapabilities(",
            "        self, emulatorbin, arch, machine_type, virt_type, flags,",
            "    ):",
            "        \"\"\"Return spoofed domain capabilities.\"\"\"",
            "        if arch in fake_libvirt_data.STATIC_DOMCAPABILITIES:",
            "            xml = fake_libvirt_data.STATIC_DOMCAPABILITIES[arch]",
            "            if machine_type:",
            "                # if we request a specific machine type, we should get",
            "                # information on that and not something else",
            "                tree = etree.fromstring(xml)",
            "                if machine_type not in tree.find('./machine').text:",
            "                    raise Exception(",
            "                        'Expected machine type of ~%s but got %s' % (",
            "                            machine_type, tree.find('./machine').text,",
            "                        ))",
            "            return xml",
            "",
            "        if arch == 'x86_64':",
            "            aliases = {'pc': 'pc-i440fx-2.11', 'q35': 'pc-q35-2.11'}",
            "            return fake_libvirt_data.DOMCAPABILITIES_X86_64_TEMPLATE % \\",
            "                {'features': self._domain_capability_features,",
            "                 'mtype': aliases.get(machine_type, machine_type)}",
            "",
            "        raise Exception(\"fakelibvirt doesn't support getDomainCapabilities \"",
            "                        \"for %s architecture\" % arch)",
            "",
            "    def getCPUModelNames(self, arch):",
            "        mapping = {",
            "            'x86_64': [",
            "                '486',",
            "                'pentium',",
            "                'pentium2',",
            "                'pentium3',",
            "                'pentiumpro',",
            "                'coreduo',",
            "                'n270',",
            "                'core2duo',",
            "                'qemu32',",
            "                'kvm32',",
            "                'cpu64-rhel5',",
            "                'cpu64-rhel6',",
            "                'qemu64',",
            "                'kvm64',",
            "                'Conroe',",
            "                'Penryn',",
            "                'Nehalem',",
            "                'Nehalem-IBRS',",
            "                'Westmere',",
            "                'Westmere-IBRS',",
            "                'SandyBridge',",
            "                'SandyBridge-IBRS',",
            "                'IvyBridge',",
            "                'IvyBridge-IBRS',",
            "                'Haswell-noTSX',",
            "                'Haswell-noTSX-IBRS',",
            "                'Haswell',",
            "                'Haswell-IBRS',",
            "                'Broadwell-noTSX',",
            "                'Broadwell-noTSX-IBRS',",
            "                'Broadwell',",
            "                'Broadwell-IBRS',",
            "                'Skylake-Client',",
            "                'Skylake-Client-IBRS',",
            "                'Skylake-Server',",
            "                'Skylake-Server-IBRS',",
            "                'Cascadelake-Server',",
            "                'Icelake-Client',",
            "                'Icelake-Server',",
            "                'athlon',",
            "                'phenom',",
            "                'Opteron_G1',",
            "                'Opteron_G2',",
            "                'Opteron_G3',",
            "                'Opteron_G4',",
            "                'Opteron_G5',",
            "                'EPYC',",
            "                'EPYC-IBPB'],",
            "            'ppc64': [",
            "                'POWER6',",
            "                'POWER7',",
            "                'POWER8',",
            "                'POWER9',",
            "                'POWERPC_e5500',",
            "                'POWERPC_e6500']",
            "        }",
            "        return mapping.get(arch, [])",
            "",
            "    # Features are kept separately so that the tests can patch this",
            "    # class variable with alternate values.",
            "    _domain_capability_features = '''  <features>",
            "    <gic supported='no'/>",
            "  </features>'''",
            "",
            "    _domain_capability_features_with_SEV = '''  <features>",
            "    <gic supported='no'/>",
            "    <sev supported='yes'>",
            "      <cbitpos>47</cbitpos>",
            "      <reducedPhysBits>1</reducedPhysBits>",
            "    </sev>",
            "  </features>'''",
            "",
            "    _domain_capability_features_with_SEV_unsupported = \\",
            "        _domain_capability_features_with_SEV.replace('yes', 'no')",
            "",
            "    def getCapabilities(self):",
            "        \"\"\"Return spoofed capabilities.\"\"\"",
            "        numa_topology = self.host_info.numa_topology",
            "        if isinstance(numa_topology, vconfig.LibvirtConfigCapsNUMATopology):",
            "            numa_topology = numa_topology.to_xml()",
            "",
            "        # we rely on os.uname() having been mocked already to ensure we have",
            "        # the correct \"host\" architecture",
            "        _capabilities = [",
            "            '<capabilities>\\n',",
            "            fake_libvirt_data.CAPABILITIES_HOST_TEMPLATES[os.uname().machine],",
            "        ] + list(fake_libvirt_data.CAPABILITIES_GUEST.values()) + [",
            "            '</capabilities>',",
            "        ]",
            "",
            "        return ''.join(_capabilities) % {",
            "            'sockets': self.host_info.cpu_sockets,",
            "            'cores': self.host_info.cpu_cores,",
            "            'threads': self.host_info.cpu_threads,",
            "            'topology': numa_topology,",
            "        }",
            "",
            "    def compareCPU(self, xml, flags):",
            "        tree = etree.fromstring(xml)",
            "",
            "        arch_node = tree.find('./arch')",
            "        if arch_node is not None:",
            "            if arch_node.text not in [obj_fields.Architecture.X86_64,",
            "                                      obj_fields.Architecture.I686]:",
            "                return VIR_CPU_COMPARE_INCOMPATIBLE",
            "",
            "        model_node = tree.find('./model')",
            "        if model_node is not None:",
            "            # arch_node may not present, therefore query all cpu models.",
            "            if model_node.text not in self.getCPUModelNames('x86_64') and \\",
            "                model_node.text not in self.getCPUModelNames('ppc64'):",
            "                raise make_libvirtError(",
            "                    libvirtError,",
            "                    \"internal error: Unknown CPU model %s\" % model_node.text,",
            "                    error_code = VIR_ERR_INTERNAL_ERROR,",
            "                    error_domain=VIR_FROM_QEMU)",
            "            if model_node.text != self.host_info.cpu_model:",
            "                return VIR_CPU_COMPARE_INCOMPATIBLE",
            "",
            "        vendor_node = tree.find('./vendor')",
            "        if vendor_node is not None:",
            "            if vendor_node.text != self.host_info.cpu_vendor:",
            "                return VIR_CPU_COMPARE_INCOMPATIBLE",
            "",
            "        # The rest of the stuff libvirt implements is rather complicated",
            "        # and I don't think it adds much value to replicate it here.",
            "",
            "        return VIR_CPU_COMPARE_IDENTICAL",
            "",
            "    def getCPUStats(self, cpuNum, flag):",
            "        if cpuNum < 2:",
            "            return {'kernel': 5664160000000,",
            "                    'idle': 1592705190000000,",
            "                    'user': 26728850000000,",
            "                    'iowait': 6121490000000}",
            "        else:",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                    \"invalid argument: Invalid cpu number\",",
            "                    error_code=VIR_ERR_INTERNAL_ERROR,",
            "                    error_domain=VIR_FROM_QEMU)",
            "",
            "    def device_lookup_by_name(self, dev_name):",
            "        return self.pci_info.get_device_by_name(dev_name)",
            "",
            "    def nodeDeviceLookupByName(self, name):",
            "        if name.startswith('mdev'):",
            "            return self.mdev_info.get_device_by_name(name)",
            "",
            "        if name.startswith('vdpa'):",
            "            return self.vdpa_info.get_device_by_name(name)",
            "",
            "        pci_dev = self.pci_info.get_device_by_name(name)",
            "        if pci_dev:",
            "            return pci_dev",
            "        try:",
            "            return self._nodedevs[name]",
            "        except KeyError:",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                    \"no nodedev with matching name %s\" % name,",
            "                    error_code=VIR_ERR_NO_NODE_DEVICE,",
            "                    error_domain=VIR_FROM_NODEDEV)",
            "",
            "    def listDevices(self, cap, flags):",
            "        if cap == 'pci':",
            "            return self.pci_info.get_all_devices()",
            "        if cap == 'mdev':",
            "            return self.mdev_info.get_all_devices()",
            "        if cap == 'mdev_types':",
            "            return self.pci_info.get_all_mdev_capable_devices()",
            "        if cap == 'vdpa':",
            "            return self.vdpa_info.get_all_devices()",
            "        else:",
            "            raise ValueError('Capability \"%s\" is not supported' % cap)",
            "",
            "    def baselineCPU(self, cpu, flag):",
            "        \"\"\"Add new libvirt API.\"\"\"",
            "        return \"\"\"<cpu mode='custom' match='exact'>",
            "                    <model>Penryn</model>",
            "                    <vendor>Intel</vendor>",
            "                    <feature name='xtpr'/>",
            "                    <feature name='tm2'/>",
            "                    <feature name='est'/>",
            "                    <feature name='vmx'/>",
            "                    <feature name='ds_cpl'/>",
            "                    <feature name='monitor'/>",
            "                    <feature name='pbe'/>",
            "                    <feature name='tm'/>",
            "                    <feature name='ht'/>",
            "                    <feature name='ss'/>",
            "                    <feature name='acpi'/>",
            "                    <feature name='ds'/>",
            "                    <feature name='vme'/>",
            "                    <feature policy='require' name='aes'/>",
            "                  </cpu>\"\"\"",
            "",
            "    def secretLookupByUsage(self, usage_type_obj, usage_id):",
            "        pass",
            "",
            "    def secretDefineXML(self, xml):",
            "        secret = Secret(self, xml)",
            "        self._add_secret(secret)",
            "        return secret",
            "",
            "    def listAllDevices(self, flags):",
            "        devices = []",
            "        if flags & VIR_CONNECT_LIST_NODE_DEVICES_CAP_PCI_DEV:",
            "            devices.extend(",
            "                NodeDevice(self, xml=dev.XMLDesc(0))",
            "                for dev in self.pci_info.devices.values()",
            "            )",
            "        if flags & VIR_CONNECT_LIST_NODE_DEVICES_CAP_NET:",
            "            # TODO(stephenfin): Implement fake netdevs so we can test the",
            "            # capability reporting",
            "            pass",
            "        if flags & VIR_CONNECT_LIST_NODE_DEVICES_CAP_VDPA:",
            "            devices.extend(",
            "                NodeDevice(self, xml=dev.XMLDesc(0))",
            "                for dev in self.vdpa_info.devices.values()",
            "            )",
            "        return devices",
            "",
            "",
            "def openAuth(uri, auth, flags=0):",
            "",
            "    if type(auth) != list:",
            "        raise Exception(\"Expected a list for 'auth' parameter\")",
            "",
            "    if type(auth[0]) != list:",
            "        raise Exception(\"Expected a function in 'auth[0]' parameter\")",
            "",
            "    if not callable(auth[1]):",
            "        raise Exception(\"Expected a function in 'auth[1]' parameter\")",
            "",
            "    return Connection(uri, (flags == VIR_CONNECT_RO))",
            "",
            "",
            "def virEventRunDefaultImpl():",
            "    time.sleep(1)",
            "",
            "",
            "def virEventRegisterDefaultImpl():",
            "    if connection_used:",
            "        raise Exception(\"virEventRegisterDefaultImpl() must be \"",
            "                        \"called before connection is used.\")",
            "",
            "",
            "def registerErrorHandler(handler, ctxt):",
            "    pass",
            "",
            "",
            "def make_libvirtError(error_class, msg, error_code=None,",
            "                       error_domain=None, error_message=None,",
            "                       error_level=None, str1=None, str2=None, str3=None,",
            "                       int1=None, int2=None):",
            "    \"\"\"Convenience function for creating `libvirtError` exceptions which",
            "    allow you to specify arguments in constructor without having to manipulate",
            "    the `err` tuple directly.",
            "",
            "    We need to pass in `error_class` to this function because it may be",
            "    `libvirt.libvirtError` or `fakelibvirt.libvirtError` depending on whether",
            "    `libvirt-python` is installed.",
            "    \"\"\"",
            "    exc = error_class(msg)",
            "    exc.err = (error_code, error_domain, error_message, error_level,",
            "               str1, str2, str3, int1, int2)",
            "    return exc",
            "",
            "",
            "virDomain = Domain",
            "virNodeDevice = NodeDevice",
            "",
            "virConnect = Connection",
            "virSecret = Secret",
            "",
            "",
            "# A private libvirt-python class and global only provided here for testing to",
            "# ensure it's not returned by libvirt.host.Host.get_libvirt_proxy_classes.",
            "class FakeHandler(object):",
            "    def __init__(self):",
            "        pass",
            "",
            "",
            "_EventAddHandleFunc = FakeHandler",
            "",
            "",
            "class LibvirtFixture(fixtures.Fixture):",
            "    \"\"\"Performs global setup/stubbing for all libvirt tests.",
            "    \"\"\"",
            "",
            "    def __init__(self, stub_os_vif=True):",
            "        self.stub_os_vif = stub_os_vif",
            "        self.pci_address_to_mac_map = collections.defaultdict(",
            "            lambda: '52:54:00:1e:59:c6')",
            "",
            "    def update_sriov_mac_address_mapping(self, pci_address_to_mac_map):",
            "        self.pci_address_to_mac_map.update(pci_address_to_mac_map)",
            "",
            "    def fake_get_mac_by_pci_address(self, pci_addr, pf_interface=False):",
            "        res = self.pci_address_to_mac_map[pci_addr]",
            "        return res",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        # Some modules load the libvirt library in a strange way",
            "        for module in ('driver', 'host', 'guest', 'migration'):",
            "            i = 'nova.virt.libvirt.{module}.libvirt'.format(module=module)",
            "            # NOTE(mdbooth): The strange incantation below means 'this module'",
            "            self.useFixture(fixtures.MonkeyPatch(i, sys.modules[__name__]))",
            "",
            "        self.useFixture(",
            "            fixtures.MockPatch('nova.virt.libvirt.utils.get_fs_info'))",
            "        self.mock_get_machine_ips = self.useFixture(",
            "            fixtures.MockPatch('nova.compute.utils.get_machine_ips')).mock",
            "",
            "        # libvirt driver needs to call out to the filesystem to get the",
            "        # parent_ifname for the SRIOV VFs.",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.pci.utils.get_ifname_by_pci_address',",
            "            return_value='fake_pf_interface_name'))",
            "",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.pci.utils.get_mac_by_pci_address',",
            "            side_effect=self.fake_get_mac_by_pci_address))",
            "",
            "        # libvirt calls out to sysfs to get the vfs ID during macvtap plug",
            "        self.mock_get_vf_num_by_pci_address = self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.pci.utils.get_vf_num_by_pci_address', return_value=1",
            "            )",
            "        ).mock",
            "",
            "        # libvirt calls out to privsep to set the mac and vlan of a macvtap",
            "        self.mock_set_device_macaddr_and_vlan = self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.privsep.linux_net.set_device_macaddr_and_vlan')).mock",
            "",
            "        # libvirt calls out to privsep to set the port state during macvtap",
            "        # plug",
            "        self.mock_set_device_macaddr = self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.privsep.linux_net.set_device_macaddr')).mock",
            "",
            "        # Don't assume that the system running tests has a valid machine-id",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '._get_host_sysinfo_serial_os', return_value=uuids.machine_id))",
            "",
            "        # Stub out _log_host_capabilities since it logs a giant string at INFO",
            "        # and we don't want that to blow up the subunit parser in test runs.",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.virt.libvirt.host.Host._log_host_capabilities'))",
            "",
            "        # Ensure tests perform the same on all host architectures",
            "        fake_uname = os_uname(",
            "            'Linux', '', '5.4.0-0-generic', '', obj_fields.Architecture.X86_64)",
            "        self.mock_uname = self.useFixture(",
            "            fixtures.MockPatch('os.uname', return_value=fake_uname)).mock",
            "",
            "        real_exists = os.path.exists",
            "",
            "        def fake_exists(path):",
            "            if path == host.SEV_KERNEL_PARAM_FILE:",
            "                return False",
            "            return real_exists(path)",
            "",
            "        self.useFixture(fixtures.MonkeyPatch('os.path.exists', fake_exists))",
            "",
            "        # ...and on all machine types",
            "        fake_loaders = [",
            "            {",
            "                'description': 'UEFI firmware for x86_64',",
            "                'interface-types': ['uefi'],",
            "                'mapping': {",
            "                    'device': 'flash',",
            "                    'executable': {",
            "                        'filename': '/usr/share/OVMF/OVMF_CODE.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                    'nvram-template': {",
            "                        'filename': '/usr/share/OVMF/OVMF_VARS.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                },",
            "                'targets': [",
            "                    {",
            "                        'architecture': 'x86_64',",
            "                        'machines': ['pc-i440fx-*', 'pc-q35-*'],",
            "                    },",
            "                ],",
            "                'features': ['acpi-s3', 'amd-sev', 'verbose-dynamic'],",
            "                'tags': [],",
            "            },",
            "            {",
            "                'description': 'UEFI firmware for x86_64, with SB+SMM',",
            "                'interface-types': ['uefi'],",
            "                'mapping': {",
            "                    'device': 'flash',",
            "                    'executable': {",
            "                        'filename': '/usr/share/OVMF/OVMF_CODE.secboot.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                    'nvram-template': {",
            "                        'filename': '/usr/share/OVMF/OVMF_VARS.secboot.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                },",
            "                'targets': [",
            "                    {",
            "                        'architecture': 'x86_64',",
            "                        'machines': ['pc-q35-*'],",
            "                    },",
            "                ],",
            "                'features': [",
            "                    'acpi-s3',",
            "                    'amd-sev',",
            "                    'enrolled-keys',",
            "                    'requires-smm',",
            "                    'secure-boot',",
            "                    'verbose-dynamic',",
            "                ],",
            "                'tags': [],",
            "            },",
            "            {",
            "                'description': 'UEFI firmware for aarch64',",
            "                'interface-types': ['uefi'],",
            "                'mapping': {",
            "                    'device': 'flash',",
            "                    'executable': {",
            "                        'filename': '/usr/share/AAVMF/AAVMF_CODE.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                    'nvram-template': {",
            "                        'filename': '/usr/share/AAVMF/AAVMF_VARS.fd',",
            "                        'format': 'raw',",
            "                    }",
            "                },",
            "                'targets': [",
            "                    {",
            "                        'architecture': 'aarch64',",
            "                        'machines': ['virt-*'],",
            "                    }",
            "                ],",
            "                'features': ['verbose-static'],",
            "                \"tags\": [],",
            "            },",
            "        ]",
            "        self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.virt.libvirt.host.Host.loaders',",
            "                new_callable=mock.PropertyMock,",
            "                return_value=fake_loaders))",
            "",
            "        disable_event_thread(self)",
            "",
            "        if self.stub_os_vif:",
            "            # Make sure to never try and actually plug/unplug VIFs in os-vif",
            "            # unless we're explicitly testing that code and the test itself",
            "            # will handle the appropriate mocking.",
            "            self.useFixture(fixtures.MonkeyPatch(",
            "                'nova.virt.libvirt.vif.LibvirtGenericVIFDriver._plug_os_vif',",
            "                lambda *a, **kw: None))",
            "            self.useFixture(fixtures.MonkeyPatch(",
            "                'nova.virt.libvirt.vif.LibvirtGenericVIFDriver._unplug_os_vif',",
            "                lambda *a, **kw: None))",
            "",
            "        # os_vif.initialize is typically done in nova-compute startup",
            "        # even if we are not planning to plug anything with os_vif in the test",
            "        # we still need the object model initialized to be able to generate",
            "        # guest config xml properly",
            "        import os_vif",
            "        os_vif.initialize()"
        ],
        "afterPatchFile": [
            "#    Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import collections",
            "import os",
            "import sys",
            "import textwrap",
            "import time",
            "import typing as ty",
            "from unittest import mock",
            "",
            "import fixtures",
            "from lxml import etree",
            "from oslo_log import log as logging",
            "from oslo_utils.fixture import uuidsentinel as uuids",
            "from oslo_utils import versionutils",
            "",
            "from nova import conf",
            "from nova.objects import fields as obj_fields",
            "from nova.tests.fixtures import libvirt_data as fake_libvirt_data",
            "from nova.virt.libvirt import config as vconfig",
            "from nova.virt.libvirt import driver as libvirt_driver",
            "from nova.virt.libvirt import host",
            "",
            "",
            "# Allow passing None to the various connect methods",
            "# (i.e. allow the client to rely on default URLs)",
            "allow_default_uri_connection = True",
            "",
            "# Has libvirt connection been used at least once",
            "connection_used = False",
            "",
            "",
            "def _reset():",
            "    global allow_default_uri_connection",
            "    allow_default_uri_connection = True",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "CONF = conf.CONF",
            "",
            "# virDomainState",
            "VIR_DOMAIN_NOSTATE = 0",
            "VIR_DOMAIN_RUNNING = 1",
            "VIR_DOMAIN_BLOCKED = 2",
            "VIR_DOMAIN_PAUSED = 3",
            "VIR_DOMAIN_SHUTDOWN = 4",
            "VIR_DOMAIN_SHUTOFF = 5",
            "VIR_DOMAIN_CRASHED = 6",
            "",
            "# NOTE(mriedem): These values come from include/libvirt/libvirt-domain.h",
            "VIR_DOMAIN_XML_SECURE = 1",
            "VIR_DOMAIN_XML_INACTIVE = 2",
            "VIR_DOMAIN_XML_UPDATE_CPU = 4",
            "VIR_DOMAIN_XML_MIGRATABLE = 8",
            "",
            "VIR_DOMAIN_BLOCK_COPY_SHALLOW = 1",
            "VIR_DOMAIN_BLOCK_COPY_REUSE_EXT = 2",
            "VIR_DOMAIN_BLOCK_COPY_TRANSIENT_JOB = 4",
            "",
            "VIR_DOMAIN_BLOCK_REBASE_SHALLOW = 1",
            "VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT = 2",
            "VIR_DOMAIN_BLOCK_REBASE_COPY = 8",
            "VIR_DOMAIN_BLOCK_REBASE_RELATIVE = 16",
            "VIR_DOMAIN_BLOCK_REBASE_COPY_DEV = 32",
            "",
            "# virDomainBlockResize",
            "VIR_DOMAIN_BLOCK_RESIZE_BYTES = 1",
            "",
            "VIR_DOMAIN_BLOCK_JOB_ABORT_ASYNC = 1",
            "VIR_DOMAIN_BLOCK_JOB_ABORT_PIVOT = 2",
            "",
            "VIR_DOMAIN_EVENT_ID_LIFECYCLE = 0",
            "",
            "VIR_DOMAIN_EVENT_DEFINED = 0",
            "VIR_DOMAIN_EVENT_UNDEFINED = 1",
            "VIR_DOMAIN_EVENT_STARTED = 2",
            "VIR_DOMAIN_EVENT_SUSPENDED = 3",
            "VIR_DOMAIN_EVENT_RESUMED = 4",
            "VIR_DOMAIN_EVENT_STOPPED = 5",
            "VIR_DOMAIN_EVENT_SHUTDOWN = 6",
            "VIR_DOMAIN_EVENT_PMSUSPENDED = 7",
            "",
            "VIR_DOMAIN_EVENT_ID_DEVICE_REMOVED = 15",
            "VIR_DOMAIN_EVENT_ID_DEVICE_REMOVAL_FAILED = 22",
            "",
            "VIR_DOMAIN_EVENT_SUSPENDED_MIGRATED = 1",
            "VIR_DOMAIN_EVENT_SUSPENDED_POSTCOPY = 7",
            "",
            "VIR_DOMAIN_UNDEFINE_MANAGED_SAVE = 1",
            "VIR_DOMAIN_UNDEFINE_NVRAM = 4",
            "",
            "VIR_DOMAIN_AFFECT_CURRENT = 0",
            "VIR_DOMAIN_AFFECT_LIVE = 1",
            "VIR_DOMAIN_AFFECT_CONFIG = 2",
            "",
            "VIR_CPU_COMPARE_ERROR = -1",
            "VIR_CPU_COMPARE_INCOMPATIBLE = 0",
            "VIR_CPU_COMPARE_IDENTICAL = 1",
            "VIR_CPU_COMPARE_SUPERSET = 2",
            "",
            "VIR_CRED_USERNAME = 1",
            "VIR_CRED_AUTHNAME = 2",
            "VIR_CRED_LANGUAGE = 3",
            "VIR_CRED_CNONCE = 4",
            "VIR_CRED_PASSPHRASE = 5",
            "VIR_CRED_ECHOPROMPT = 6",
            "VIR_CRED_NOECHOPROMPT = 7",
            "VIR_CRED_REALM = 8",
            "VIR_CRED_EXTERNAL = 9",
            "",
            "VIR_MIGRATE_LIVE = 1",
            "VIR_MIGRATE_PEER2PEER = 2",
            "VIR_MIGRATE_TUNNELLED = 4",
            "VIR_MIGRATE_PERSIST_DEST = 8",
            "VIR_MIGRATE_UNDEFINE_SOURCE = 16",
            "VIR_MIGRATE_NON_SHARED_INC = 128",
            "VIR_MIGRATE_AUTO_CONVERGE = 8192",
            "VIR_MIGRATE_POSTCOPY = 32768",
            "VIR_MIGRATE_TLS = 65536",
            "",
            "VIR_NODE_CPU_STATS_ALL_CPUS = -1",
            "",
            "VIR_DOMAIN_START_PAUSED = 1",
            "",
            "# libvirtError enums",
            "# (Intentionally different from what's in libvirt. We do this to check,",
            "#  that consumers of the library are using the symbolic names rather than",
            "#  hardcoding the numerical values)",
            "VIR_FROM_QEMU = 100",
            "VIR_FROM_DOMAIN = 200",
            "VIR_FROM_SECRET = 300",
            "VIR_FROM_NWFILTER = 330",
            "VIR_FROM_REMOTE = 340",
            "VIR_FROM_RPC = 345",
            "VIR_FROM_NODEDEV = 666",
            "",
            "VIR_ERR_INVALID_ARG = 8",
            "VIR_ERR_NO_SUPPORT = 3",
            "VIR_ERR_XML_ERROR = 27",
            "VIR_ERR_XML_DETAIL = 350",
            "VIR_ERR_NO_DOMAIN = 420",
            "VIR_ERR_OPERATION_FAILED = 510",
            "VIR_ERR_OPERATION_INVALID = 55",
            "VIR_ERR_OPERATION_TIMEOUT = 68",
            "VIR_ERR_NO_NWFILTER = 620",
            "VIR_ERR_SYSTEM_ERROR = 900",
            "VIR_ERR_INTERNAL_ERROR = 950",
            "VIR_ERR_CONFIG_UNSUPPORTED = 951",
            "VIR_ERR_NO_NODE_DEVICE = 667",
            "VIR_ERR_INVALID_SECRET = 65",
            "VIR_ERR_NO_SECRET = 66",
            "VIR_ERR_AGENT_UNRESPONSIVE = 86",
            "VIR_ERR_ARGUMENT_UNSUPPORTED = 74",
            "VIR_ERR_OPERATION_UNSUPPORTED = 84",
            "VIR_ERR_DEVICE_MISSING = 99",
            "# Readonly",
            "VIR_CONNECT_RO = 1",
            "",
            "# virConnectBaselineCPU flags",
            "VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES = 1",
            "",
            "# snapshotCreateXML flags",
            "VIR_DOMAIN_SNAPSHOT_CREATE_NO_METADATA = 4",
            "VIR_DOMAIN_SNAPSHOT_CREATE_DISK_ONLY = 16",
            "VIR_DOMAIN_SNAPSHOT_CREATE_REUSE_EXT = 32",
            "VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE = 64",
            "",
            "# blockCommit flags",
            "VIR_DOMAIN_BLOCK_COMMIT_RELATIVE = 4",
            "",
            "",
            "VIR_CONNECT_LIST_DOMAINS_ACTIVE = 1",
            "VIR_CONNECT_LIST_DOMAINS_INACTIVE = 2",
            "",
            "# virConnectListAllNodeDevices flags",
            "VIR_CONNECT_LIST_NODE_DEVICES_CAP_PCI_DEV = 2",
            "VIR_CONNECT_LIST_NODE_DEVICES_CAP_NET = 1 << 4",
            "VIR_CONNECT_LIST_NODE_DEVICES_CAP_VDPA = 1 << 17",
            "",
            "# secret type",
            "VIR_SECRET_USAGE_TYPE_NONE = 0",
            "VIR_SECRET_USAGE_TYPE_VOLUME = 1",
            "VIR_SECRET_USAGE_TYPE_CEPH = 2",
            "VIR_SECRET_USAGE_TYPE_ISCSI = 3",
            "",
            "# metadata types",
            "VIR_DOMAIN_METADATA_DESCRIPTION = 0",
            "VIR_DOMAIN_METADATA_TITLE = 1",
            "VIR_DOMAIN_METADATA_ELEMENT = 2",
            "",
            "# Libvirt version to match MIN_LIBVIRT_VERSION in driver.py",
            "FAKE_LIBVIRT_VERSION = versionutils.convert_version_to_int(",
            "    libvirt_driver.MIN_LIBVIRT_VERSION)",
            "# Libvirt version to match MIN_QEMU_VERSION in driver.py",
            "FAKE_QEMU_VERSION = versionutils.convert_version_to_int(",
            "    libvirt_driver.MIN_QEMU_VERSION)",
            "",
            "PCI_VEND_ID = '8086'",
            "PCI_VEND_NAME = 'Intel Corporation'",
            "",
            "PCI_PROD_ID = '1533'",
            "PCI_PROD_NAME = 'I210 Gigabit Network Connection'",
            "PCI_DRIVER_NAME = 'igb'",
            "",
            "PF_PROD_ID = '1528'",
            "PF_PROD_NAME = 'Ethernet Controller 10-Gigabit X540-AT2'",
            "PF_DRIVER_NAME = 'ixgbe'",
            "PF_CAP_TYPE = 'virt_functions'",
            "",
            "VF_PROD_ID = '1515'",
            "VF_PROD_NAME = 'X540 Ethernet Controller Virtual Function'",
            "VF_DRIVER_NAME = 'ixgbevf'",
            "VF_CAP_TYPE = 'phys_function'",
            "",
            "MDEV_CAPABLE_VEND_ID = '10DE'",
            "MDEV_CAPABLE_VEND_NAME = 'Nvidia'",
            "MDEV_CAPABLE_PROD_ID = '0FFE'",
            "MDEV_CAPABLE_PROD_NAME = 'GRID M60-0B'",
            "MDEV_CAPABLE_DRIVER_NAME = 'nvidia'",
            "MDEV_CAPABLE_CAP_TYPE = 'mdev_types'",
            "",
            "NVIDIA_11_VGPU_TYPE = 'nvidia-11'",
            "NVIDIA_12_VGPU_TYPE = 'nvidia-12'",
            "MLX5_CORE_TYPE = 'mlx5_core'",
            "MDEVCAP_DEV1_PCI_ADDR = 'pci_0000_81_00_0'",
            "MDEVCAP_DEV2_PCI_ADDR = 'pci_0000_81_01_0'",
            "MDEVCAP_DEV3_PCI_ADDR = 'pci_0000_81_02_0'",
            "",
            "os_uname = collections.namedtuple(",
            "    'uname_result', ['sysname', 'nodename', 'release', 'version', 'machine'],",
            ")",
            "",
            "",
            "def _get_libvirt_nodedev_name(bus, slot, function):",
            "    \"\"\"Convert an address to a libvirt device name string.\"\"\"",
            "    return f'pci_0000_{bus:02x}_{slot:02x}_{function:d}'",
            "",
            "",
            "class FakePCIDevice(object):",
            "    \"\"\"Generate a fake PCI device.",
            "",
            "    Generate a fake PCI devices corresponding to one of the following",
            "    real-world PCI devices.",
            "",
            "    - I210 Gigabit Network Connection (8086:1533)",
            "    - Ethernet Controller 10-Gigabit X540-AT2 (8086:1528)",
            "    - X540 Ethernet Controller Virtual Function (8086:1515)",
            "    \"\"\"",
            "",
            "    pci_default_parent = \"pci_0000_80_01_0\"",
            "    pci_device_template = textwrap.dedent(\"\"\"",
            "        <device>",
            "          <name>pci_0000_%(bus)02x_%(slot)02x_%(function)d</name>",
            "          <path>/sys/devices/pci0000:80/0000:80:01.0/0000:%(bus)02x:%(slot)02x.%(function)d</path>",
            "          <parent>%(parent)s</parent>",
            "          <driver>",
            "            <name>%(driver)s</name>",
            "          </driver>",
            "          <capability type='pci'>",
            "            <domain>0</domain>",
            "            <bus>%(bus)d</bus>",
            "            <slot>%(slot)d</slot>",
            "            <function>%(function)d</function>",
            "            <product id='0x%(prod_id)s'>%(prod_name)s</product>",
            "            <vendor id='0x%(vend_id)s'>%(vend_name)s</vendor>",
            "        %(capability)s",
            "        %(vpd_capability)s",
            "            <iommuGroup number='%(iommu_group)d'>",
            "              <address domain='0x0000' bus='%(bus)#02x' slot='%(slot)#02x' function='0x%(function)d'/>",
            "            </iommuGroup>",
            "            <numa node='%(numa_node)s'/>",
            "            <pci-express>",
            "              <link validity='cap' port='0' speed='5' width='8'/>",
            "              <link validity='sta' speed='5' width='8'/>",
            "            </pci-express>",
            "          </capability>",
            "        </device>\"\"\".strip())  # noqa",
            "    cap_templ = \"<capability type='%(cap_type)s'>%(addresses)s</capability>\"",
            "    addr_templ = \"<address domain='0x0000' bus='%(bus)#02x' slot='%(slot)#02x' function='%(function)#02x'/>\"  # noqa",
            "    mdevtypes_templ = textwrap.dedent(\"\"\"",
            "        <type id='%(type_id)s'>",
            "        <name>GRID M60-0B</name><deviceAPI>vfio-pci</deviceAPI>",
            "        <availableInstances>%(instances)s</availableInstances>",
            "        </type>\"\"\".strip())  # noqa",
            "",
            "    vpd_cap_templ = textwrap.dedent(\"\"\"",
            "        <capability type='vpd'>",
            "        <name>%(name)s</name>",
            "        %(fields)s",
            "        </capability>\"\"\".strip())",
            "    vpd_fields_templ = textwrap.dedent(\"\"\"",
            "        <fields access='%(access)s'>%(section_fields)s</fields>\"\"\".strip())",
            "    vpd_field_templ = \"\"\"<%(field_name)s>%(field_value)s</%(field_name)s>\"\"\"",
            "",
            "    is_capable_of_mdevs = False",
            "",
            "    def __init__(",
            "        self, dev_type, bus, slot, function, iommu_group, numa_node, *,",
            "        vf_ratio=None, multiple_gpu_types=False, generic_types=False,",
            "        parent=None, vend_id=None, vend_name=None, prod_id=None,",
            "        prod_name=None, driver_name=None, vpd_fields=None, mac_address=None,",
            "    ):",
            "        \"\"\"Populate pci devices",
            "",
            "        :param dev_type: (str) Indicates the type of the device (PCI, PF, VF,",
            "            MDEV_TYPES).",
            "        :param bus: (int) Bus number of the device.",
            "        :param slot: (int) Slot number of the device.",
            "        :param function: (int) Function number of the device.",
            "        :param iommu_group: (int) IOMMU group ID.",
            "        :param numa_node: (int) NUMA node of the device.",
            "        :param vf_ratio: (int) Ratio of Virtual Functions on Physical. Only",
            "            applicable if ``dev_type`` is one of: ``PF``, ``VF``.",
            "        :param multiple_gpu_types: (bool) Supports different vGPU types.",
            "        :param generic_types: (bool) Support both mlx5 and nvidia-12 types.",
            "        :param parent: (int, int, int) A tuple of bus, slot and function",
            "            corresponding to the parent.",
            "        :param vend_id: (str) The vendor ID.",
            "        :param vend_name: (str) The vendor name.",
            "        :param prod_id: (str) The product ID.",
            "        :param prod_name: (str) The product name.",
            "        :param driver_name: (str) The driver name.",
            "        :param mac_address: (str) The MAC of the device.",
            "            Used in case of SRIOV PFs",
            "        \"\"\"",
            "",
            "        self.dev_type = dev_type",
            "        self.bus = bus",
            "        self.slot = slot",
            "        self.function = function",
            "        self.iommu_group = iommu_group",
            "        self.numa_node = numa_node",
            "        self.vf_ratio = vf_ratio",
            "        self.multiple_gpu_types = multiple_gpu_types",
            "        self.generic_types = generic_types",
            "        self.parent = parent",
            "",
            "        self.vend_id = vend_id",
            "        self.vend_name = vend_name",
            "        self.prod_id = prod_id",
            "        self.prod_name = prod_name",
            "        self.driver_name = driver_name",
            "        self.mac_address = mac_address",
            "",
            "        self.vpd_fields = vpd_fields",
            "",
            "        self.generate_xml()",
            "",
            "    def generate_xml(self, skip_capability=False):",
            "",
            "        # initial validation",
            "        assert self.dev_type in ('PCI', 'VF', 'PF', 'MDEV_TYPES'), (",
            "            f'got invalid dev_type {self.dev_type}')",
            "",
            "        if self.dev_type == 'PCI':",
            "            assert not self.vf_ratio, 'vf_ratio does not apply for PCI devices'",
            "",
            "        if self.dev_type in ('PF', 'VF'):",
            "            assert (",
            "                self.vf_ratio is not None",
            "            ), 'require vf_ratio for PFs and VFs'",
            "",
            "        if self.dev_type == 'VF':",
            "            assert self.parent, 'require parent for VFs'",
            "            assert isinstance(self.parent, tuple), 'parent must be an address'",
            "            assert len(self.parent) == 3, 'parent must be an address'",
            "",
            "        vend_id = self.vend_id or PCI_VEND_ID",
            "        vend_name = self.vend_name or PCI_VEND_NAME",
            "        capability = ''",
            "        if self.dev_type == 'PCI':",
            "            prod_id = self.prod_id or PCI_PROD_ID",
            "            prod_name = self.prod_name or PCI_PROD_NAME",
            "            driver = self.driver_name or PCI_DRIVER_NAME",
            "        elif self.dev_type == 'PF':",
            "            prod_id = self.prod_id or PF_PROD_ID",
            "            prod_name = self.prod_name or PF_PROD_NAME",
            "            driver = self.driver_name or PF_DRIVER_NAME",
            "            if not skip_capability:",
            "                capability = self.cap_templ % {",
            "                    'cap_type': PF_CAP_TYPE,",
            "                    'addresses': '\\n'.join([",
            "                        self.addr_templ % {",
            "                            'bus': self.bus,",
            "                            # these are the slot, function values of the child",
            "                            # VFs, we can only assign 8 functions to a slot",
            "                            # (0-7) so bump the slot each time we exceed this",
            "                            'slot': self.slot + (x // 8),",
            "                            # ...and wrap the function value",
            "                            'function': x % 8,",
            "                        # the offset is because the PF is occupying function 0",
            "                        } for x in range(1, self.vf_ratio + 1)])",
            "                }",
            "        elif self.dev_type == 'VF':",
            "            prod_id = self.prod_id or VF_PROD_ID",
            "            prod_name = self.prod_name or VF_PROD_NAME",
            "            driver = self.driver_name or VF_DRIVER_NAME",
            "            if not skip_capability:",
            "                capability = self.cap_templ % {",
            "                    'cap_type': VF_CAP_TYPE,",
            "                    'addresses': self.addr_templ % {",
            "                        'bus': self.bus,",
            "                        # this is the slot, function value of the parent PF",
            "                        # if we're e.g. device 8, we'll have a different slot",
            "                        # to our parent so reverse this",
            "                        'slot': self.slot - ((self.vf_ratio + 1) // 8),",
            "                        # the parent PF is always function 0",
            "                        'function': 0,",
            "                    }",
            "                }",
            "        elif self.dev_type == 'MDEV_TYPES':",
            "            prod_id = self.prod_id or MDEV_CAPABLE_PROD_ID",
            "            prod_name = self.prod_name or MDEV_CAPABLE_PROD_NAME",
            "            driver = self.driver_name or MDEV_CAPABLE_DRIVER_NAME",
            "            vend_id = self.vend_id or MDEV_CAPABLE_VEND_ID",
            "            vend_name = self.vend_name or MDEV_CAPABLE_VEND_NAME",
            "            types = [self.mdevtypes_templ % {",
            "                'type_id': NVIDIA_11_VGPU_TYPE,",
            "                'instances': 16,",
            "            }]",
            "            if self.multiple_gpu_types:",
            "                types.append(self.mdevtypes_templ % {",
            "                    'type_id': NVIDIA_12_VGPU_TYPE,",
            "                    'instances': 8,",
            "                })",
            "            if self.generic_types:",
            "                types = [self.mdevtypes_templ % {",
            "                    'type_id': MLX5_CORE_TYPE,",
            "                    'instances': 16,",
            "                }]",
            "                types.append(self.mdevtypes_templ % {",
            "                    'type_id': NVIDIA_12_VGPU_TYPE,",
            "                    'instances': 8,",
            "                })",
            "            if not skip_capability:",
            "                capability = self.cap_templ % {",
            "                    'cap_type': MDEV_CAPABLE_CAP_TYPE,",
            "                    'addresses': '\\n'.join(types)",
            "                }",
            "            self.is_capable_of_mdevs = True",
            "",
            "        parent = self.pci_default_parent",
            "        if self.parent:",
            "            parent = _get_libvirt_nodedev_name(*self.parent)",
            "",
            "        self.pci_device = self.pci_device_template % {",
            "            'bus': self.bus,",
            "            'slot': self.slot,",
            "            'function': self.function,",
            "            'vend_id': vend_id,",
            "            'vend_name': vend_name,",
            "            'prod_id': prod_id,",
            "            'prod_name': prod_name,",
            "            'driver': driver,",
            "            'capability': capability,",
            "            'vpd_capability': self.format_vpd_cap(),",
            "            'iommu_group': self.iommu_group,",
            "            'numa_node': self.numa_node,",
            "            'parent': parent,",
            "        }",
            "        # -1 is the sentinel set in /sys/bus/pci/devices/*/numa_node",
            "        # for no NUMA affinity. When the numa_node is set to -1 on a device",
            "        # Libvirt omits the NUMA element so we remove it.",
            "        if self.numa_node == -1:",
            "            self.pci_device = self.pci_device.replace(\"<numa node='-1'/>\", \"\")",
            "",
            "    def format_vpd_cap(self):",
            "        if not self.vpd_fields:",
            "            return ''",
            "        fields = []",
            "        for access_type in ('readonly', 'readwrite'):",
            "            section_fields = []",
            "            for field_name, field_value in self.vpd_fields.get(",
            "                    access_type, {}).items():",
            "                section_fields.append(self.vpd_field_templ % {",
            "                    'field_name': field_name,",
            "                    'field_value': field_value,",
            "                })",
            "            if section_fields:",
            "                fields.append(",
            "                    self.vpd_fields_templ % {",
            "                        'access': access_type,",
            "                        'section_fields': '\\n'.join(section_fields),",
            "                    }",
            "                )",
            "        return self.vpd_cap_templ % {",
            "            'name': self.vpd_fields.get('name', ''),",
            "            'fields': '\\n'.join(fields)",
            "        }",
            "",
            "    def XMLDesc(self, flags):",
            "        return self.pci_device",
            "",
            "    @property",
            "    def address(self):",
            "        return \"0000:%02x:%02x.%1x\" % (self.bus, self.slot, self.function)",
            "",
            "",
            "# TODO(stephenfin): Remove all of these HostFooDevicesInfo objects in favour of",
            "# a unified devices object",
            "class HostPCIDevicesInfo(object):",
            "    \"\"\"Represent a pool of host PCI devices.\"\"\"",
            "",
            "    TOTAL_NUMA_NODES = 2",
            "",
            "    def __init__(self, num_pci=0, num_pfs=2, num_vfs=8, num_mdevcap=0,",
            "                 numa_node=None, multiple_gpu_types=False,",
            "                 generic_types=False):",
            "        \"\"\"Create a new HostPCIDevicesInfo object.",
            "",
            "        :param num_pci: (int) The number of (non-SR-IOV) and (non-MDEV capable)",
            "            PCI devices.",
            "        :param num_pfs: (int) The number of PCI SR-IOV Physical Functions.",
            "        :param num_vfs: (int) The number of PCI SR-IOV Virtual Functions.",
            "        :param num_mdevcap: (int) The number of PCI devices capable of creating",
            "            mediated devices.",
            "        :param numa_node: (int) NUMA node of the device; if set all of the",
            "            devices will be assigned to the specified node else they will be",
            "            split between ``$TOTAL_NUMA_NODES`` nodes.",
            "        :param multiple_gpu_types: (bool) Supports different vGPU types",
            "        :param generic_types: (bool) Supports both nvidia-12 and mlx5 types",
            "        \"\"\"",
            "        self.devices = {}",
            "",
            "        if not (num_vfs or num_pfs or num_pci) and not num_mdevcap:",
            "            return",
            "",
            "        if num_vfs and not num_pfs:",
            "            raise ValueError('Cannot create VFs without PFs')",
            "",
            "        if num_pfs and num_vfs % num_pfs:",
            "            raise ValueError('num_vfs must be a factor of num_pfs')",
            "",
            "        bus = 0x81",
            "        slot = 0x0",
            "        function = 0",
            "        iommu_group = 40  # totally arbitrary number",
            "",
            "        # Generate PCI devs",
            "        for dev in range(num_pci):",
            "            self.add_device(",
            "                dev_type='PCI',",
            "                bus=bus,",
            "                slot=slot,",
            "                function=function,",
            "                iommu_group=iommu_group,",
            "                numa_node=self._calc_numa_node(dev, numa_node))",
            "",
            "            slot += 1",
            "            iommu_group += 1",
            "",
            "        # Generate MDEV capable devs",
            "        for dev in range(num_mdevcap):",
            "            self.add_device(",
            "                dev_type='MDEV_TYPES',",
            "                bus=bus,",
            "                slot=slot,",
            "                function=function,",
            "                iommu_group=iommu_group,",
            "                numa_node=self._calc_numa_node(dev, numa_node),",
            "                multiple_gpu_types=multiple_gpu_types,",
            "                generic_types=generic_types)",
            "",
            "            slot += 1",
            "            iommu_group += 1",
            "",
            "        vf_ratio = num_vfs // num_pfs if num_pfs else 0",
            "",
            "        # Generate PFs",
            "        for dev in range(num_pfs):",
            "            function = 0",
            "            numa_node_pf = self._calc_numa_node(dev, numa_node)",
            "",
            "            self.add_device(",
            "                dev_type='PF',",
            "                bus=bus,",
            "                slot=slot,",
            "                function=function,",
            "                iommu_group=iommu_group,",
            "                numa_node=numa_node_pf,",
            "                vf_ratio=vf_ratio)",
            "",
            "            parent = (bus, slot, function)",
            "            # Generate VFs",
            "            for _ in range(vf_ratio):",
            "                function += 1",
            "                iommu_group += 1",
            "",
            "                if function % 8 == 0:",
            "                    # functions must be 0-7",
            "                    slot += 1",
            "                    function = 0",
            "",
            "                self.add_device(",
            "                    dev_type='VF',",
            "                    bus=bus,",
            "                    slot=slot,",
            "                    function=function,",
            "                    iommu_group=iommu_group,",
            "                    numa_node=numa_node_pf,",
            "                    vf_ratio=vf_ratio,",
            "                    parent=parent)",
            "",
            "            slot += 1",
            "",
            "    def add_device(",
            "        self, dev_type, bus, slot, function, iommu_group, numa_node,",
            "        vf_ratio=None, multiple_gpu_types=False, generic_types=False,",
            "        parent=None, vend_id=None, vend_name=None, prod_id=None,",
            "        prod_name=None, driver_name=None, vpd_fields=None, mac_address=None,",
            "    ):",
            "        pci_dev_name = _get_libvirt_nodedev_name(bus, slot, function)",
            "",
            "        LOG.info('Generating %s device %r', dev_type, pci_dev_name)",
            "",
            "        dev = FakePCIDevice(",
            "            dev_type=dev_type,",
            "            bus=bus,",
            "            slot=slot,",
            "            function=function,",
            "            iommu_group=iommu_group,",
            "            numa_node=numa_node,",
            "            vf_ratio=vf_ratio,",
            "            multiple_gpu_types=multiple_gpu_types,",
            "            generic_types=generic_types,",
            "            parent=parent,",
            "            vend_id=vend_id,",
            "            vend_name=vend_name,",
            "            prod_id=prod_id,",
            "            prod_name=prod_name,",
            "            driver_name=driver_name,",
            "            vpd_fields=vpd_fields,",
            "            mac_address=mac_address,",
            "        )",
            "        self.devices[pci_dev_name] = dev",
            "        return dev",
            "",
            "    @classmethod",
            "    def _calc_numa_node(cls, dev, numa_node):",
            "        return dev % cls.TOTAL_NUMA_NODES if numa_node is None else numa_node",
            "",
            "    def get_all_devices(self):",
            "        return self.devices.keys()",
            "",
            "    def get_device_by_name(self, device_name):",
            "        pci_dev = self.devices.get(device_name)",
            "        return pci_dev",
            "",
            "    def get_all_mdev_capable_devices(self):",
            "        return [dev for dev in self.devices",
            "                if self.devices[dev].is_capable_of_mdevs]",
            "",
            "    def get_pci_address_mac_mapping(self):",
            "        return {",
            "            device.address: device.mac_address",
            "            for dev_addr, device in self.devices.items()",
            "            if device.mac_address",
            "        }",
            "",
            "",
            "class FakeMdevDevice(object):",
            "    template = \"\"\"",
            "    <device>",
            "      <name>%(dev_name)s</name>",
            "      <path>/sys/devices/pci0000:00/0000:00:02.0/%(path)s</path>",
            "      <parent>%(parent)s</parent>",
            "      <driver>",
            "        <name>vfio_mdev</name>",
            "      </driver>",
            "      <capability type='mdev'>",
            "        <type id='%(type_id)s'/>",
            "        <iommuGroup number='12'/>",
            "      </capability>",
            "    </device>",
            "    \"\"\"",
            "",
            "    def __init__(self, dev_name, type_id, parent):",
            "        self.xml = self.template % {",
            "            'dev_name': dev_name, 'type_id': type_id,",
            "            'path': dev_name[len('mdev_'):],",
            "            'parent': parent}",
            "",
            "    def XMLDesc(self, flags):",
            "        return self.xml",
            "",
            "",
            "class HostMdevDevicesInfo(object):",
            "    def __init__(self, devices=None):",
            "        if devices is not None:",
            "            self.devices = devices",
            "        else:",
            "            self.devices = {}",
            "",
            "    def get_all_devices(self):",
            "        return self.devices.keys()",
            "",
            "    def get_device_by_name(self, device_name):",
            "        dev = self.devices[device_name]",
            "        return dev",
            "",
            "",
            "class FakeVDPADevice:",
            "",
            "    template = textwrap.dedent(\"\"\"",
            "        <device>",
            "          <name>%(name)s</name>",
            "          <path>%(path)s</path>",
            "          <parent>%(parent)s</parent>",
            "          <driver>",
            "            <name>vhost_vdpa</name>",
            "          </driver>",
            "          <capability type='vdpa'>",
            "            <chardev>/dev/vhost-vdpa-%(idx)d</chardev>",
            "          </capability>",
            "        </device>\"\"\".strip())",
            "",
            "    def __init__(self, name, idx, parent):",
            "        assert isinstance(parent, FakePCIDevice)",
            "        assert parent.dev_type == 'VF'",
            "",
            "        self.name = name",
            "        self.idx = idx",
            "        self.parent = parent",
            "        self.generate_xml()",
            "",
            "    def generate_xml(self):",
            "        pf_pci = self.parent.parent",
            "        vf_pci = (self.parent.bus, self.parent.slot, self.parent.function)",
            "        pf_addr = '0000:%02x:%02x.%d' % pf_pci",
            "        vf_addr = '0000:%02x:%02x.%d' % vf_pci",
            "        parent = _get_libvirt_nodedev_name(*vf_pci)",
            "        path = f'/sys/devices/pci0000:00/{pf_addr}/{vf_addr}/vdpa{self.idx}'",
            "        self.xml = self.template % {",
            "            'name': self.name,",
            "            'idx': self.idx,",
            "            'path': path,",
            "            'parent': parent,",
            "        }",
            "",
            "    def XMLDesc(self, flags):",
            "        return self.xml",
            "",
            "",
            "class HostVDPADevicesInfo:",
            "",
            "    def __init__(self):",
            "        self.devices = {}",
            "",
            "    def get_all_devices(self):",
            "        return self.devices.keys()",
            "",
            "    def get_device_by_name(self, device_name):",
            "        dev = self.devices[device_name]",
            "        return dev",
            "",
            "    def add_device(self, name, idx, parent):",
            "        LOG.info('Generating vDPA device %r', name)",
            "",
            "        dev = FakeVDPADevice(name=name, idx=idx, parent=parent)",
            "        self.devices[name] = dev",
            "        return dev",
            "",
            "",
            "class HostInfo(object):",
            "",
            "    def __init__(self, cpu_nodes=1, cpu_sockets=1, cpu_cores=2, cpu_threads=1,",
            "                 kB_mem=16780000, mempages=None):",
            "        \"\"\"Create a new Host Info object",
            "",
            "        :param cpu_nodes: (int) the number of NUMA cell, 1 for unusual",
            "                          NUMA topologies or uniform",
            "        :param cpu_sockets: (int) number of CPU sockets per node if nodes > 1,",
            "                            total number of CPU sockets otherwise",
            "        :param cpu_cores: (int) number of cores per socket",
            "        :param cpu_threads: (int) number of threads per core",
            "        :param kB_mem: (int) memory size in KBytes",
            "        \"\"\"",
            "",
            "        self.arch = obj_fields.Architecture.X86_64",
            "        self.kB_mem = kB_mem",
            "        self.cpus = cpu_nodes * cpu_sockets * cpu_cores * cpu_threads",
            "        self.cpu_mhz = 800",
            "        self.cpu_nodes = cpu_nodes",
            "        self.cpu_cores = cpu_cores",
            "        self.cpu_threads = cpu_threads",
            "        self.cpu_sockets = cpu_sockets",
            "        self.cpu_model = \"Penryn\"",
            "        self.cpu_vendor = \"Intel\"",
            "        self.numa_topology = NUMATopology(self.cpu_nodes, self.cpu_sockets,",
            "                                          self.cpu_cores, self.cpu_threads,",
            "                                          self.kB_mem, mempages)",
            "",
            "",
            "class NUMATopology(vconfig.LibvirtConfigCapsNUMATopology):",
            "    \"\"\"A batteries-included variant of LibvirtConfigCapsNUMATopology.",
            "",
            "    Provides sane defaults for LibvirtConfigCapsNUMATopology that can be used",
            "    in tests as is, or overridden where necessary.",
            "    \"\"\"",
            "",
            "    def __init__(self, cpu_nodes=4, cpu_sockets=1, cpu_cores=1, cpu_threads=2,",
            "                 kb_mem=1048576, mempages=None, **kwargs):",
            "",
            "        super(NUMATopology, self).__init__(**kwargs)",
            "",
            "        cpu_count = 0",
            "        cell_count = 0",
            "        for socket_count in range(cpu_sockets):",
            "            for cell_num in range(cpu_nodes):",
            "                cell = vconfig.LibvirtConfigCapsNUMACell()",
            "                cell.id = cell_count",
            "                cell.memory = kb_mem // (cpu_nodes * cpu_sockets)",
            "                for cpu_num in range(cpu_cores * cpu_threads):",
            "                    cpu = vconfig.LibvirtConfigCapsNUMACPU()",
            "                    cpu.id = cpu_count",
            "                    cpu.socket_id = socket_count",
            "                    cpu.core_id = cpu_num // cpu_threads",
            "                    cpu.siblings = set([cpu_threads *",
            "                                       (cpu_count // cpu_threads) + thread",
            "                                        for thread in range(cpu_threads)])",
            "                    cell.cpus.append(cpu)",
            "",
            "                    cpu_count += 1",
            "",
            "                # If no mempages are provided, use only the default 4K pages",
            "                if mempages:",
            "                    cell.mempages = mempages[cell_count]",
            "                else:",
            "                    cell.mempages = create_mempages([(4, cell.memory // 4)])",
            "",
            "                self.cells.append(cell)",
            "",
            "                cell_count += 1",
            "",
            "",
            "def create_mempages(mappings):",
            "    \"\"\"Generate a list of LibvirtConfigCapsNUMAPages objects.",
            "",
            "    :param mappings: (dict) A mapping of page size to quantity of",
            "        said pages.",
            "    :returns: [LibvirtConfigCapsNUMAPages, ...]",
            "    \"\"\"",
            "    mempages = []",
            "",
            "    for page_size, page_qty in mappings:",
            "        mempage = vconfig.LibvirtConfigCapsNUMAPages()",
            "        mempage.size = page_size",
            "        mempage.total = page_qty",
            "        mempages.append(mempage)",
            "",
            "    return mempages",
            "",
            "",
            "VIR_DOMAIN_JOB_NONE = 0",
            "VIR_DOMAIN_JOB_BOUNDED = 1",
            "VIR_DOMAIN_JOB_UNBOUNDED = 2",
            "VIR_DOMAIN_JOB_COMPLETED = 3",
            "VIR_DOMAIN_JOB_FAILED = 4",
            "VIR_DOMAIN_JOB_CANCELLED = 5",
            "",
            "",
            "def _parse_disk_info(element):",
            "    disk_info = {}",
            "    disk_info['type'] = element.get('type', 'file')",
            "    disk_info['device'] = element.get('device', 'disk')",
            "",
            "    driver = element.find('./driver')",
            "    if driver is not None:",
            "        disk_info['driver_name'] = driver.get('name')",
            "        disk_info['driver_type'] = driver.get('type')",
            "",
            "    source = element.find('./source')",
            "    if source is not None:",
            "        disk_info['source'] = source.get('file')",
            "        if not disk_info['source']:",
            "            disk_info['source'] = source.get('dev')",
            "",
            "        if not disk_info['source']:",
            "            disk_info['source'] = source.get('path')",
            "",
            "    target = element.find('./target')",
            "    if target is not None:",
            "        disk_info['target_dev'] = target.get('dev')",
            "        disk_info['target_bus'] = target.get('bus')",
            "",
            "    return disk_info",
            "",
            "",
            "def _parse_nic_info(element):",
            "    nic_info = {}",
            "    nic_info['type'] = element.get('type', 'bridge')",
            "",
            "    driver = element.find('./mac')",
            "    if driver is not None:",
            "        nic_info['mac'] = driver.get('address')",
            "",
            "    source = element.find('./source')",
            "    if source is not None:",
            "        nic_info['source'] = source.get('bridge')",
            "",
            "    target = element.find('./target')",
            "    if target is not None:",
            "        nic_info['target_dev'] = target.get('dev')",
            "",
            "    return nic_info",
            "",
            "",
            "def disable_event_thread(self):",
            "    \"\"\"Disable nova libvirt driver event thread.",
            "",
            "    The Nova libvirt driver includes a native thread which monitors",
            "    the libvirt event channel. In a testing environment this becomes",
            "    problematic because it means we've got a floating thread calling",
            "    sleep(1) over the life of the unit test. Seems harmless? It's not,",
            "    because we sometimes want to test things like retry loops that",
            "    should have specific sleep paterns. An unlucky firing of the",
            "    libvirt thread will cause a test failure.",
            "",
            "    \"\"\"",
            "    # because we are patching a method in a class MonkeyPatch doesn't",
            "    # auto import correctly. Import explicitly otherwise the patching",
            "    # may silently fail.",
            "    import nova.virt.libvirt.host  # noqa",
            "",
            "    def evloop(*args, **kwargs):",
            "        pass",
            "",
            "    self.useFixture(fixtures.MockPatch(",
            "        'nova.virt.libvirt.host.Host._init_events',",
            "        side_effect=evloop))",
            "",
            "",
            "class libvirtError(Exception):",
            "    \"\"\"This class was copied and slightly modified from",
            "    `libvirt-python:libvirt-override.py`.",
            "",
            "    Since a test environment will use the real `libvirt-python` version of",
            "    `libvirtError` if it's installed and not this fake, we need to maintain",
            "    strict compatibility with the original class, including `__init__` args",
            "    and instance-attributes.",
            "",
            "    To create a libvirtError instance you should:",
            "",
            "        # Create an unsupported error exception",
            "        exc = libvirtError('my message')",
            "        exc.err = (libvirt.VIR_ERR_NO_SUPPORT,)",
            "",
            "    self.err is a tuple of form:",
            "        (error_code, error_domain, error_message, error_level, str1, str2,",
            "         str3, int1, int2)",
            "",
            "    Alternatively, you can use the `make_libvirtError` convenience function to",
            "    allow you to specify these attributes in one shot.",
            "    \"\"\"",
            "",
            "    def __init__(self, defmsg, conn=None, dom=None, net=None, pool=None,",
            "                 vol=None):",
            "        Exception.__init__(self, defmsg)",
            "        self.err = None",
            "",
            "    def get_error_code(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[0]",
            "",
            "    def get_error_domain(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[1]",
            "",
            "    def get_error_message(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[2]",
            "",
            "    def get_error_level(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[3]",
            "",
            "    def get_str1(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[4]",
            "",
            "    def get_str2(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[5]",
            "",
            "    def get_str3(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[6]",
            "",
            "    def get_int1(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[7]",
            "",
            "    def get_int2(self):",
            "        if self.err is None:",
            "            return None",
            "        return self.err[8]",
            "",
            "",
            "class NodeDevice(object):",
            "",
            "    def __init__(self, connection, xml=None):",
            "        self._connection = connection",
            "",
            "        self._xml = xml",
            "        if xml is not None:",
            "            self._parse_xml(xml)",
            "",
            "    def _parse_xml(self, xml):",
            "        tree = etree.fromstring(xml)",
            "        root = tree.find('.')",
            "        self._name = root.find('name').text",
            "        self._parent = root.find('parent').text",
            "",
            "    def attach(self):",
            "        pass",
            "",
            "    def dettach(self):",
            "        pass",
            "",
            "    def reset(self):",
            "        pass",
            "",
            "    def XMLDesc(self, flags: int) -> str:",
            "        return self._xml",
            "",
            "    def parent(self) -> str:",
            "        return self._parent",
            "",
            "    def name(self) -> str:",
            "        return self._name",
            "",
            "    def listCaps(self) -> ty.List[str]:",
            "        return [self.name().split('_')[0]]",
            "",
            "",
            "class Domain(object):",
            "    def __init__(self, connection, xml, running=False, transient=False):",
            "        self._connection = connection",
            "        if running:",
            "            connection._mark_running(self)",
            "",
            "        self._state = running and VIR_DOMAIN_RUNNING or VIR_DOMAIN_SHUTOFF",
            "        self._transient = transient",
            "        self._def = self._parse_definition(xml)",
            "        self._has_saved_state = False",
            "        self._snapshots = {}",
            "        self._id = self._connection._id_counter",
            "        self._job_type = VIR_DOMAIN_JOB_UNBOUNDED",
            "",
            "    def _parse_definition(self, xml):",
            "        try:",
            "            tree = etree.fromstring(xml)",
            "        except etree.ParseError:",
            "            raise make_libvirtError(",
            "                    libvirtError, \"Invalid XML.\",",
            "                    error_code=VIR_ERR_XML_DETAIL,",
            "                    error_domain=VIR_FROM_DOMAIN)",
            "",
            "        definition = {}",
            "",
            "        name = tree.find('./name')",
            "        if name is not None:",
            "            definition['name'] = name.text",
            "",
            "        uuid_elem = tree.find('./uuid')",
            "        if uuid_elem is not None:",
            "            definition['uuid'] = uuid_elem.text",
            "        else:",
            "            definition['uuid'] = uuids.fake",
            "",
            "        vcpu = tree.find('./vcpu')",
            "        if vcpu is not None:",
            "            definition['vcpu'] = int(vcpu.text)",
            "",
            "        memory = tree.find('./memory')",
            "        if memory is not None:",
            "            definition['memory'] = int(memory.text)",
            "",
            "        os = {}",
            "        os_type = tree.find('./os/type')",
            "        if os_type is not None:",
            "            os['type'] = os_type.text",
            "            os['arch'] = os_type.get('arch', self._connection.host_info.arch)",
            "",
            "        os_kernel = tree.find('./os/kernel')",
            "        if os_kernel is not None:",
            "            os['kernel'] = os_kernel.text",
            "",
            "        os_initrd = tree.find('./os/initrd')",
            "        if os_initrd is not None:",
            "            os['initrd'] = os_initrd.text",
            "",
            "        os_cmdline = tree.find('./os/cmdline')",
            "        if os_cmdline is not None:",
            "            os['cmdline'] = os_cmdline.text",
            "",
            "        os_boot = tree.find('./os/boot')",
            "        if os_boot is not None:",
            "            os['boot_dev'] = os_boot.get('dev')",
            "",
            "        definition['os'] = os",
            "",
            "        features = {}",
            "",
            "        acpi = tree.find('./features/acpi')",
            "        if acpi is not None:",
            "            features['acpi'] = True",
            "",
            "        definition['features'] = features",
            "",
            "        cpu_pins = {}",
            "",
            "        pins = tree.findall('./cputune/vcpupin')",
            "        for pin in pins:",
            "            cpu_pins[pin.get('vcpu')] = pin.get('cpuset')",
            "",
            "        definition['cpu_pins'] = cpu_pins",
            "",
            "        emulator_pin = tree.find('./cputune/emulatorpin')",
            "        if emulator_pin is not None:",
            "            definition['emulator_pin'] = emulator_pin.get('cpuset')",
            "",
            "        memnodes = {}",
            "",
            "        for node in tree.findall('./numatune/memnode'):",
            "            memnodes[node.get('cellid')] = node.get('nodeset')",
            "",
            "        definition['memnodes'] = memnodes",
            "",
            "        devices = {}",
            "",
            "        device_nodes = tree.find('./devices')",
            "        if device_nodes is not None:",
            "            disks_info = []",
            "            disks = device_nodes.findall('./disk')",
            "            for disk in disks:",
            "                disks_info += [_parse_disk_info(disk)]",
            "            devices['disks'] = disks_info",
            "",
            "            nics_info = []",
            "            nics = device_nodes.findall('./interface')",
            "            for nic in nics:",
            "                nic_info = {}",
            "                nic_info['type'] = nic.get('type')",
            "",
            "                mac = nic.find('./mac')",
            "                if mac is not None:",
            "                    nic_info['mac'] = mac.get('address')",
            "",
            "                source = nic.find('./source')",
            "                if source is not None:",
            "                    if nic_info['type'] == 'network':",
            "                        nic_info['source'] = source.get('network')",
            "                    elif nic_info['type'] == 'bridge':",
            "                        nic_info['source'] = source.get('bridge')",
            "                    elif nic_info['type'] == 'hostdev':",
            "                        # <interface type='hostdev'> is for VF when vnic_type",
            "                        # is direct. Add sriov vf pci information in nic_info",
            "                        address = source.find('./address')",
            "                        pci_type = address.get('type')",
            "                        pci_domain = address.get('domain').replace('0x', '')",
            "                        pci_bus = address.get('bus').replace('0x', '')",
            "                        pci_slot = address.get('slot').replace('0x', '')",
            "                        pci_function = address.get('function').replace(",
            "                            '0x', '')",
            "                        pci_device = \"%s_%s_%s_%s_%s\" % (pci_type, pci_domain,",
            "                                                         pci_bus, pci_slot,",
            "                                                         pci_function)",
            "                        nic_info['source'] = pci_device",
            "                    elif nic_info['type'] == 'vdpa':",
            "                        nic_info['source'] = source.get('dev')",
            "",
            "                nics_info += [nic_info]",
            "",
            "            devices['nics'] = nics_info",
            "",
            "            hostdev_info = []",
            "            hostdevs = device_nodes.findall('./hostdev')",
            "            for hostdev in hostdevs:",
            "                address = hostdev.find('./source/address')",
            "                # NOTE(gibi): only handle mdevs as pci is complicated",
            "                dev_type = hostdev.get('type')",
            "                if dev_type == 'mdev':",
            "                    hostdev_info.append({",
            "                        'type': dev_type,",
            "                        'model': hostdev.get('model'),",
            "                        'address_uuid': address.get('uuid')",
            "                    })",
            "            devices['hostdevs'] = hostdev_info",
            "",
            "            vpmem_info = []",
            "            vpmems = device_nodes.findall('./memory')",
            "            for vpmem in vpmems:",
            "                model = vpmem.get('model')",
            "                if model == 'nvdimm':",
            "                    source = vpmem.find('./source')",
            "                    target = vpmem.find('./target')",
            "                    path = source.find('./path').text",
            "                    alignsize = source.find('./alignsize').text",
            "                    size = target.find('./size').text",
            "                    node = target.find('./node').text",
            "                    vpmem_info.append({",
            "                        'path': path,",
            "                        'size': size,",
            "                        'alignsize': alignsize,",
            "                        'node': node})",
            "            devices['vpmems'] = vpmem_info",
            "",
            "        definition['devices'] = devices",
            "",
            "        return definition",
            "",
            "    def verify_hostdevs_interface_are_vfs(self):",
            "        \"\"\"Verify for interface type hostdev if the pci device is VF or not.",
            "        \"\"\"",
            "",
            "        error_message = (\"Interface type hostdev is currently supported on \"",
            "                         \"SR-IOV Virtual Functions only\")",
            "",
            "        nics = self._def['devices'].get('nics', [])",
            "        for nic in nics:",
            "            if nic['type'] == 'hostdev':",
            "                pci_device = nic['source']",
            "                pci_info_from_connection = self._connection.pci_info.devices[",
            "                    pci_device]",
            "                if 'phys_function' not in pci_info_from_connection.pci_device:",
            "                    raise make_libvirtError(",
            "                        libvirtError,",
            "                        error_message,",
            "                        error_code=VIR_ERR_CONFIG_UNSUPPORTED,",
            "                        error_domain=VIR_FROM_DOMAIN)",
            "",
            "    def create(self):",
            "        self.createWithFlags(0)",
            "",
            "    def createWithFlags(self, flags):",
            "        # FIXME: Not handling flags at the moment",
            "        self.verify_hostdevs_interface_are_vfs()",
            "        self._state = VIR_DOMAIN_RUNNING",
            "        self._connection._mark_running(self)",
            "        self._has_saved_state = False",
            "",
            "    def isActive(self):",
            "        return int(self._state == VIR_DOMAIN_RUNNING)",
            "",
            "    def undefine(self):",
            "        self._connection._undefine(self)",
            "",
            "    def isPersistent(self):",
            "        return True",
            "",
            "    def undefineFlags(self, flags):",
            "        self.undefine()",
            "        if flags & VIR_DOMAIN_UNDEFINE_MANAGED_SAVE:",
            "            if self.hasManagedSaveImage(0):",
            "                self.managedSaveRemove()",
            "",
            "    def destroy(self):",
            "        self._state = VIR_DOMAIN_SHUTOFF",
            "        self._connection._mark_not_running(self)",
            "",
            "    def ID(self):",
            "        return self._id",
            "",
            "    def name(self):",
            "        return self._def['name']",
            "",
            "    def UUIDString(self):",
            "        return self._def['uuid']",
            "",
            "    def interfaceStats(self, device):",
            "        return [10000242400, 1234, 0, 2, 213412343233, 34214234, 23, 3]",
            "",
            "    def blockStats(self, device):",
            "        return [2, 10000242400, 234, 2343424234, 34]",
            "",
            "    def setTime(self, time=None, flags=0):",
            "        pass",
            "",
            "    def suspend(self):",
            "        self._state = VIR_DOMAIN_PAUSED",
            "",
            "    def shutdown(self):",
            "        self._state = VIR_DOMAIN_SHUTDOWN",
            "        self._connection._mark_not_running(self)",
            "",
            "    def reset(self, flags):",
            "        # FIXME: Not handling flags at the moment",
            "        self._state = VIR_DOMAIN_RUNNING",
            "        self._connection._mark_running(self)",
            "",
            "    def info(self):",
            "        return [self._state,",
            "                int(self._def['memory']),",
            "                int(self._def['memory']),",
            "                self._def['vcpu'],",
            "                123456789]",
            "",
            "    def migrateToURI3(self, dconnuri, params, flags):",
            "        raise make_libvirtError(",
            "                libvirtError,",
            "                \"Migration always fails for fake libvirt!\",",
            "                error_code=VIR_ERR_INTERNAL_ERROR,",
            "                error_domain=VIR_FROM_QEMU)",
            "",
            "    def migrateSetMaxDowntime(self, downtime):",
            "        pass",
            "",
            "    def attachDevice(self, xml):",
            "        result = False",
            "        if xml.startswith(\"<disk\"):",
            "            disk_info = _parse_disk_info(etree.fromstring(xml))",
            "            disk_info['_attached'] = True",
            "            self._def['devices']['disks'] += [disk_info]",
            "            result = True",
            "        elif xml.startswith(\"<interface\"):",
            "            nic_info = _parse_nic_info(etree.fromstring(xml))",
            "            nic_info['_attached'] = True",
            "            self._def['devices']['nics'] += [nic_info]",
            "            result = True",
            "        else:",
            "            # FIXME(sean-k-mooney): We don't currently handle attaching",
            "            # or detaching hostdevs but we have tests that assume we do so",
            "            # this is an error not an exception. This affects PCI passthough,",
            "            # vGPUs and PF neutron ports.",
            "            LOG.error(",
            "                \"Trying to attach an unsupported device type.\"",
            "                \"The fakelibvirt implementation is incomplete \"",
            "                \"and should be extended to support %s: %s\",",
            "                xml, self._def['devices'])",
            "",
            "        return result",
            "",
            "    def attachDeviceFlags(self, xml, flags):",
            "        if (flags & VIR_DOMAIN_AFFECT_LIVE and",
            "                self._state != VIR_DOMAIN_RUNNING):",
            "            raise make_libvirtError(",
            "                libvirtError,",
            "                \"AFFECT_LIVE only allowed for running domains!\",",
            "                error_code=VIR_ERR_INTERNAL_ERROR,",
            "                error_domain=VIR_FROM_QEMU)",
            "        self.attachDevice(xml)",
            "",
            "    def detachDevice(self, xml):",
            "        # detachDevice is a common function used for all devices types",
            "        # so we need to handle each separately",
            "        if xml.startswith(\"<disk\"):",
            "            disk_info = _parse_disk_info(etree.fromstring(xml))",
            "            attached_disk_info = None",
            "            for attached_disk in self._def['devices']['disks']:",
            "                if attached_disk['target_dev'] == disk_info.get('target_dev'):",
            "                    attached_disk_info = attached_disk",
            "                    break",
            "",
            "            if attached_disk_info:",
            "                self._def['devices']['disks'].remove(attached_disk_info)",
            "",
            "            return attached_disk_info is not None",
            "",
            "        if xml.startswith(\"<interface\"):",
            "            nic_info = _parse_nic_info(etree.fromstring(xml))",
            "            attached_nic_info = None",
            "            for attached_nic in self._def['devices']['nics']:",
            "                if attached_nic['mac'] == nic_info['mac']:",
            "                    attached_nic_info = attached_nic",
            "                    break",
            "",
            "            if attached_nic_info:",
            "                self._def['devices']['nics'].remove(attached_nic_info)",
            "",
            "            return attached_nic_info is not None",
            "",
            "        # FIXME(sean-k-mooney): We don't currently handle attaching or",
            "        # detaching hostdevs but we have tests that assume we do so this is",
            "        # an error not an exception. This affects PCI passthough, vGPUs and",
            "        # PF neutron ports",
            "        LOG.error(",
            "            \"Trying to detach an unsupported device type.\"",
            "            \"The fakelibvirt implementation is incomplete \"",
            "            \"and should be extended to support %s: %s\",",
            "            xml, self._def['devices'])",
            "",
            "        return False",
            "",
            "    def detachDeviceFlags(self, xml, flags):",
            "        self.detachDevice(xml)",
            "",
            "    def setUserPassword(self, user, password, flags=0):",
            "        pass",
            "",
            "    def XMLDesc(self, flags):",
            "        disks = ''",
            "        for disk in self._def['devices']['disks']:",
            "            if disk['type'] == 'file':",
            "                source_attr = 'file'",
            "            else:",
            "                source_attr = 'dev'",
            "",
            "            disks += '''<disk type='%(type)s' device='%(device)s'>",
            "      <driver name='%(driver_name)s' type='%(driver_type)s'/>",
            "      <source %(source_attr)s='%(source)s'/>",
            "      <target dev='%(target_dev)s' bus='%(target_bus)s'/>",
            "      <address type='drive' controller='0' bus='0' unit='0'/>",
            "    </disk>''' % dict(source_attr=source_attr, **disk)",
            "        nics = ''",
            "        for func, nic in enumerate(self._def['devices']['nics']):",
            "            if func > 7:",
            "                # this should never be raised but is just present to highlight",
            "                # the limitations of the current fake when writing new tests.",
            "                # if you see this raised when add a new test you will need",
            "                # to extend this fake to use both functions and slots.",
            "                # the pci function is limited to 3 bits or 0-7.",
            "                raise RuntimeError(",
            "                    'Test attempts to add more than 8 PCI devices. This is '",
            "                    'not supported by the fake libvirt implementation.')",
            "            nic['func'] = func",
            "            if nic['type'] in ('ethernet',):",
            "                # this branch covers kernel ovs interfaces",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <target dev='tap274487d1-6%(func)s'/>",
            "          <address type='pci' domain='0x0000' bus='0x00' slot='0x03'",
            "                   function='0x%(func)s'/>",
            "        </interface>''' % nic",
            "            elif nic['type'] in ('vdpa',):",
            "                # this branch covers hardware offloaded ovs with vdpa",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <source dev='%(source)s'/>",
            "          <address type='pci' domain='0x0000' bus='0x00' slot='0x03'",
            "                   function='0x%(func)s'/>",
            "        </interface>''' % nic",
            "            # this branch covers most interface types with a source",
            "            # such as linux bridge interfaces.",
            "            elif 'source' in nic:",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <source %(type)s='%(source)s'/>",
            "          <target dev='tap274487d1-6%(func)s'/>",
            "          <address type='pci' domain='0x0000' bus='0x00' slot='0x03'",
            "                   function='0x%(func)s'/>",
            "        </interface>''' % nic",
            "            else:",
            "                # This branch covers the macvtap vnic-type.",
            "                # This is incomplete as the source dev should be unique",
            "                # and map to the VF netdev name but due to the mocking in",
            "                # the fixture we hard code it.",
            "                nics += '''<interface type='%(type)s'>",
            "          <mac address='%(mac)s'/>",
            "          <source dev='fake_pf_interface_name' mode='passthrough'>",
            "              <address type='pci' domain='0x0000' bus='0x81' slot='0x00'",
            "                   function='0x%(func)s'/>",
            "          </source>",
            "        </interface>''' % nic",
            "",
            "        hostdevs = ''",
            "        for hostdev in self._def['devices']['hostdevs']:",
            "            hostdevs += '''<hostdev mode='subsystem' type='%(type)s' model='%(model)s'>",
            "    <source>",
            "      <address uuid='%(address_uuid)s'/>",
            "    </source>",
            "    </hostdev>",
            "            ''' % hostdev  # noqa",
            "",
            "        vpmems = ''",
            "        for vpmem in self._def['devices']['vpmems']:",
            "            vpmems += '''",
            "    <memory model='nvdimm' access='shared'>",
            "      <source>",
            "        <path>%(path)s</path>",
            "        <alignsize>%(alignsize)s</alignsize>",
            "        <pmem/>",
            "      </source>",
            "      <target>",
            "        <size>%(size)s</size>",
            "        <node>%(node)s</node>",
            "        <label>",
            "          <size>2097152</size>",
            "        </label>",
            "      </target>",
            "    </memory>",
            "            ''' % vpmem",
            "        cputune = ''",
            "        for vcpu, cpuset in self._def['cpu_pins'].items():",
            "            cputune += '<vcpupin vcpu=\"%d\" cpuset=\"%s\"/>' % (int(vcpu), cpuset)",
            "        emulatorpin = None",
            "        if 'emulator_pin' in self._def:",
            "            emulatorpin = ('<emulatorpin cpuset=\"%s\"/>' %",
            "                           self._def['emulator_pin'])",
            "        if cputune or emulatorpin:",
            "            cputune = '<cputune>%s%s</cputune>' % (emulatorpin, cputune)",
            "",
            "        numatune = ''",
            "        for cellid, nodeset in self._def['memnodes'].items():",
            "            numatune += '<memnode cellid=\"%d\" nodeset=\"%s\"/>' % (int(cellid),",
            "                                                                 nodeset)",
            "            numatune += '<memory nodeset=\"%s\"/>' % ','.join(",
            "                self._def['memnodes'].values())",
            "        if numatune:",
            "            numatune = '<numatune>%s</numatune>' % numatune",
            "",
            "        serial_console = ''",
            "        if CONF.serial_console.enabled:",
            "            serial_console = \"\"\"<serial type=\"tcp\">",
            "                <source host=\"-1\" service=\"-1\" mode=\"bind\"/>",
            "                </serial>\"\"\"",
            "",
            "        return '''<domain type='kvm'>",
            "  <name>%(name)s</name>",
            "  <uuid>%(uuid)s</uuid>",
            "  <memory>%(memory)s</memory>",
            "  <currentMemory>%(memory)s</currentMemory>",
            "  <vcpu>%(vcpu)s</vcpu>",
            "  <os>",
            "    <type arch='%(arch)s' machine='pc-0.12'>hvm</type>",
            "    <boot dev='hd'/>",
            "  </os>",
            "  <features>",
            "    <acpi/>",
            "    <apic/>",
            "    <pae/>",
            "  </features>",
            "  <clock offset='localtime'/>",
            "  <on_poweroff>destroy</on_poweroff>",
            "  <on_reboot>restart</on_reboot>",
            "  <on_crash>restart</on_crash>",
            "  %(cputune)s",
            "  %(numatune)s",
            "  <devices>",
            "    <emulator>/usr/bin/kvm</emulator>",
            "    %(disks)s",
            "    <controller type='ide' index='0'>",
            "      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'",
            "               function='0x1'/>",
            "    </controller>",
            "    %(nics)s",
            "    %(serial_console)s",
            "    <console type='file'>",
            "      <source path='dummy.log'/>",
            "      <target port='0'/>",
            "    </console>",
            "    <input type='tablet' bus='usb'/>",
            "    <input type='mouse' bus='ps2'/>",
            "    <graphics type='vnc' port='-1' autoport='yes'/>",
            "    <graphics type='spice' port='-1' autoport='yes'/>",
            "    <video>",
            "      <model type='cirrus' vram='9216' heads='1'/>",
            "      <address type='pci' domain='0x0000' bus='0x00' slot='0x02'",
            "               function='0x0'/>",
            "    </video>",
            "    <memballoon model='virtio'>",
            "      <address type='pci' domain='0x0000' bus='0x00' slot='0x04'",
            "               function='0x0'/>",
            "    </memballoon>",
            "    %(hostdevs)s",
            "    %(vpmems)s",
            "  </devices>",
            "</domain>''' % {'name': self._def['name'],",
            "                'uuid': self._def['uuid'],",
            "                'memory': self._def['memory'],",
            "                'vcpu': self._def['vcpu'],",
            "                'arch': self._def['os']['arch'],",
            "                'disks': disks,",
            "                'nics': nics,",
            "                'hostdevs': hostdevs,",
            "                'vpmems': vpmems,",
            "                'serial_console': serial_console,",
            "                'cputune': cputune,",
            "                'numatune': numatune}",
            "",
            "    def managedSave(self, flags):",
            "        self._connection._mark_not_running(self)",
            "        self._has_saved_state = True",
            "",
            "    def managedSaveRemove(self, flags):",
            "        self._has_saved_state = False",
            "",
            "    def hasManagedSaveImage(self, flags):",
            "        return int(self._has_saved_state)",
            "",
            "    def resume(self):",
            "        self._state = VIR_DOMAIN_RUNNING",
            "",
            "    def snapshotCreateXML(self, xml, flags):",
            "        tree = etree.fromstring(xml)",
            "        name = tree.find('./name').text",
            "        snapshot = DomainSnapshot(name, self)",
            "        self._snapshots[name] = snapshot",
            "        return snapshot",
            "",
            "    def vcpus(self):",
            "        vcpus = ([], [])",
            "        for i in range(0, self._def['vcpu']):",
            "            vcpus[0].append((i, 1, 120405, i))",
            "            vcpus[1].append((True, True, True, True))",
            "        return vcpus",
            "",
            "    def memoryStats(self):",
            "        return {}",
            "",
            "    def maxMemory(self):",
            "        return self._def['memory']",
            "",
            "    def blockJobInfo(self, disk, flags):",
            "        return {}",
            "",
            "    def blockJobAbort(self, disk, flags):",
            "        pass",
            "",
            "    def blockResize(self, disk, size, flags):",
            "        pass",
            "",
            "    def blockRebase(self, disk, base, bandwidth=0, flags=0):",
            "        if (not base) and (flags and VIR_DOMAIN_BLOCK_REBASE_RELATIVE):",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                    'flag VIR_DOMAIN_BLOCK_REBASE_RELATIVE is '",
            "                    'valid only with non-null base',",
            "                    error_code=VIR_ERR_INVALID_ARG,",
            "                    error_domain=VIR_FROM_QEMU)",
            "        return 0",
            "",
            "    def blockCopy(self, disk, base, flags=0):",
            "        return 0",
            "",
            "    def blockCommit(self, disk, base, top, flags):",
            "        return 0",
            "",
            "    def jobInfo(self):",
            "        # NOTE(danms): This is an array of 12 integers, so just report",
            "        # something to avoid an IndexError if we look at this",
            "        return [0] * 12",
            "",
            "    def jobStats(self, flags=0):",
            "        # NOTE(artom) By returning VIR_DOMAIN_JOB_UNBOUNDED, we're pretending a",
            "        # job is constantly running. Tests are expected to call the",
            "        # complete_job or fail_job methods when they're ready for jobs (read:",
            "        # live migrations) to \"complete\".",
            "        return {'type': self._job_type}",
            "",
            "    def complete_job(self):",
            "        self._job_type = VIR_DOMAIN_JOB_COMPLETED",
            "",
            "    def fail_job(self):",
            "        self._job_type = VIR_DOMAIN_JOB_FAILED",
            "",
            "    def injectNMI(self, flags=0):",
            "        return 0",
            "",
            "    def abortJob(self):",
            "        pass",
            "",
            "    def fsFreeze(self):",
            "        pass",
            "",
            "    def fsThaw(self):",
            "        pass",
            "",
            "    def setMetadata(self, metadata_type, metadata, key, uri, flags=0):",
            "        pass",
            "",
            "",
            "class DomainSnapshot(object):",
            "    def __init__(self, name, domain):",
            "        self._name = name",
            "        self._domain = domain",
            "",
            "    def delete(self, flags):",
            "        del self._domain._snapshots[self._name]",
            "",
            "",
            "class Secret(object):",
            "    def __init__(self, connection, xml):",
            "        self._connection = connection",
            "        self._xml = xml",
            "        self._parse_xml(xml)",
            "        self._value = None",
            "",
            "    def _parse_xml(self, xml):",
            "        tree = etree.fromstring(xml)",
            "        self._uuid = tree.find('./uuid').text",
            "        self._private = tree.get('private') == 'yes'",
            "",
            "    def setValue(self, value, flags=0):",
            "        self._value = value",
            "        return 0",
            "",
            "    def value(self, flags=0):",
            "        if self._value is None:",
            "            raise make_libvirtError(",
            "                libvirtError,",
            "                \"secret '%s' does not have a value\" % self._uuid,",
            "                error_code=VIR_ERR_NO_SECRET,",
            "                error_domain=VIR_FROM_SECRET)",
            "            pass",
            "",
            "        if self._private:",
            "            raise make_libvirtError(",
            "                libvirtError,",
            "                'secret is private',",
            "                error_code=VIR_ERR_INVALID_SECRET,",
            "                error_domain=VIR_FROM_SECRET)",
            "",
            "        return self._value",
            "",
            "    def undefine(self):",
            "        self._connection._remove_secret(self)",
            "",
            "",
            "class Connection(object):",
            "    def __init__(",
            "        self, uri=None, readonly=False, version=FAKE_LIBVIRT_VERSION,",
            "        hv_version=FAKE_QEMU_VERSION, hostname=None,",
            "        host_info=None, pci_info=None, mdev_info=None, vdpa_info=None,",
            "    ):",
            "        if not uri or uri == '':",
            "            if allow_default_uri_connection:",
            "                uri = 'qemu:///session'",
            "            else:",
            "                raise ValueError(\"URI was None, but fake libvirt is \"",
            "                                 \"configured to not accept this.\")",
            "",
            "        uri_whitelist = [",
            "            'qemu:///system',",
            "            'qemu:///session',",
            "            'lxc:///',     # from LibvirtDriver._uri()",
            "            'test:///default',",
            "            'parallels:///system',",
            "        ]",
            "",
            "        if uri not in uri_whitelist:",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                   \"libvirt error: no connection driver \"",
            "                   \"available for No connection for URI %s\" % uri,",
            "                   error_code=5, error_domain=0)",
            "",
            "        self.readonly = readonly",
            "        self._uri = uri",
            "        self._vms = {}",
            "        self._running_vms = {}",
            "        self._id_counter = 1  # libvirt reserves 0 for the hypervisor.",
            "        self._nodedevs = {}",
            "        self._secrets = {}",
            "        self._event_callbacks = {}",
            "        self.fakeLibVersion = version",
            "        self.fakeVersion = hv_version",
            "        self.host_info = host_info or HostInfo()",
            "        self.pci_info = pci_info or HostPCIDevicesInfo(num_pci=0,",
            "                                                       num_pfs=0,",
            "                                                       num_vfs=0)",
            "        self.mdev_info = mdev_info or HostMdevDevicesInfo(devices={})",
            "        self.vdpa_info = vdpa_info or HostVDPADevicesInfo()",
            "        self.hostname = hostname or 'compute1'",
            "",
            "    def _add_nodedev(self, nodedev):",
            "        self._nodedevs[nodedev._name] = nodedev",
            "",
            "    def _remove_nodedev(self, nodedev):",
            "        del self._nodedevs[nodedev._name]",
            "",
            "    def _add_secret(self, secret):",
            "        self._secrets[secret._uuid] = secret",
            "",
            "    def _remove_secret(self, secret):",
            "        del self._secrets[secret._uuid]",
            "",
            "    def _mark_running(self, dom):",
            "        self._running_vms[self._id_counter] = dom",
            "        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STARTED, 0)",
            "        self._id_counter += 1",
            "",
            "    def _mark_not_running(self, dom):",
            "        if dom._transient:",
            "            self._undefine(dom)",
            "",
            "        dom._id = -1",
            "",
            "        for (k, v) in self._running_vms.items():",
            "            if v == dom:",
            "                del self._running_vms[k]",
            "                self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STOPPED, 0)",
            "                return",
            "",
            "    def _undefine(self, dom):",
            "        del self._vms[dom.name()]",
            "        if not dom._transient:",
            "            self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_UNDEFINED, 0)",
            "",
            "    def getInfo(self):",
            "        return [self.host_info.arch,",
            "                self.host_info.kB_mem,",
            "                self.host_info.cpus,",
            "                self.host_info.cpu_mhz,",
            "                self.host_info.cpu_nodes,",
            "                self.host_info.cpu_sockets,",
            "                self.host_info.cpu_cores,",
            "                self.host_info.cpu_threads]",
            "",
            "    def lookupByUUIDString(self, uuid):",
            "        for vm in self._vms.values():",
            "            if vm.UUIDString() == uuid:",
            "                return vm",
            "        raise make_libvirtError(",
            "                libvirtError,",
            "                'Domain not found: no domain with matching uuid \"%s\"' % uuid,",
            "                error_code=VIR_ERR_NO_DOMAIN,",
            "                error_domain=VIR_FROM_QEMU)",
            "",
            "    def listAllDomains(self, flags=None):",
            "        vms = []",
            "        for vm in self._vms.values():",
            "            if flags & VIR_CONNECT_LIST_DOMAINS_ACTIVE:",
            "                if vm._state != VIR_DOMAIN_SHUTOFF:",
            "                    vms.append(vm)",
            "            if flags & VIR_CONNECT_LIST_DOMAINS_INACTIVE:",
            "                if vm._state == VIR_DOMAIN_SHUTOFF:",
            "                    vms.append(vm)",
            "        return vms",
            "",
            "    def _emit_lifecycle(self, dom, event, detail):",
            "        if VIR_DOMAIN_EVENT_ID_LIFECYCLE not in self._event_callbacks:",
            "            return",
            "",
            "        cbinfo = self._event_callbacks[VIR_DOMAIN_EVENT_ID_LIFECYCLE]",
            "        callback = cbinfo[0]",
            "        opaque = cbinfo[1]",
            "        callback(self, dom, event, detail, opaque)",
            "",
            "    def defineXML(self, xml):",
            "        dom = Domain(connection=self, running=False, transient=False, xml=xml)",
            "        self._vms[dom.name()] = dom",
            "        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_DEFINED, 0)",
            "        return dom",
            "",
            "    def createXML(self, xml, flags):",
            "        dom = Domain(connection=self, running=True, transient=True, xml=xml)",
            "        self._vms[dom.name()] = dom",
            "        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STARTED, 0)",
            "        return dom",
            "",
            "    def getType(self):",
            "        if self._uri == 'qemu:///system':",
            "            return 'QEMU'",
            "",
            "    def getLibVersion(self):",
            "        return self.fakeLibVersion",
            "",
            "    def getVersion(self):",
            "        return self.fakeVersion",
            "",
            "    def getHostname(self):",
            "        return self.hostname",
            "",
            "    def domainEventRegisterAny(self, dom, eventid, callback, opaque):",
            "        self._event_callbacks[eventid] = [callback, opaque]",
            "",
            "    def registerCloseCallback(self, cb, opaque):",
            "        pass",
            "",
            "    def getCPUMap(self):",
            "        \"\"\"Return calculated CPU map from HostInfo, by default showing 2",
            "           online CPUs.",
            "        \"\"\"",
            "        total_cpus = self.host_info.cpus",
            "        cpu_map = [True for cpu_num in range(total_cpus)]",
            "        return (total_cpus, cpu_map, total_cpus)",
            "",
            "    def getDomainCapabilities(",
            "        self, emulatorbin, arch, machine_type, virt_type, flags,",
            "    ):",
            "        \"\"\"Return spoofed domain capabilities.\"\"\"",
            "        if arch in fake_libvirt_data.STATIC_DOMCAPABILITIES:",
            "            xml = fake_libvirt_data.STATIC_DOMCAPABILITIES[arch]",
            "            if machine_type:",
            "                # if we request a specific machine type, we should get",
            "                # information on that and not something else",
            "                tree = etree.fromstring(xml)",
            "                if machine_type not in tree.find('./machine').text:",
            "                    raise Exception(",
            "                        'Expected machine type of ~%s but got %s' % (",
            "                            machine_type, tree.find('./machine').text,",
            "                        ))",
            "            return xml",
            "",
            "        if arch == 'x86_64':",
            "            aliases = {'pc': 'pc-i440fx-2.11', 'q35': 'pc-q35-2.11'}",
            "            return fake_libvirt_data.DOMCAPABILITIES_X86_64_TEMPLATE % \\",
            "                {'features': self._domain_capability_features,",
            "                 'mtype': aliases.get(machine_type, machine_type)}",
            "",
            "        raise Exception(\"fakelibvirt doesn't support getDomainCapabilities \"",
            "                        \"for %s architecture\" % arch)",
            "",
            "    def getCPUModelNames(self, arch):",
            "        mapping = {",
            "            'x86_64': [",
            "                '486',",
            "                'pentium',",
            "                'pentium2',",
            "                'pentium3',",
            "                'pentiumpro',",
            "                'coreduo',",
            "                'n270',",
            "                'core2duo',",
            "                'qemu32',",
            "                'kvm32',",
            "                'cpu64-rhel5',",
            "                'cpu64-rhel6',",
            "                'qemu64',",
            "                'kvm64',",
            "                'Conroe',",
            "                'Penryn',",
            "                'Nehalem',",
            "                'Nehalem-IBRS',",
            "                'Westmere',",
            "                'Westmere-IBRS',",
            "                'SandyBridge',",
            "                'SandyBridge-IBRS',",
            "                'IvyBridge',",
            "                'IvyBridge-IBRS',",
            "                'Haswell-noTSX',",
            "                'Haswell-noTSX-IBRS',",
            "                'Haswell',",
            "                'Haswell-IBRS',",
            "                'Broadwell-noTSX',",
            "                'Broadwell-noTSX-IBRS',",
            "                'Broadwell',",
            "                'Broadwell-IBRS',",
            "                'Skylake-Client',",
            "                'Skylake-Client-IBRS',",
            "                'Skylake-Server',",
            "                'Skylake-Server-IBRS',",
            "                'Cascadelake-Server',",
            "                'Icelake-Client',",
            "                'Icelake-Server',",
            "                'athlon',",
            "                'phenom',",
            "                'Opteron_G1',",
            "                'Opteron_G2',",
            "                'Opteron_G3',",
            "                'Opteron_G4',",
            "                'Opteron_G5',",
            "                'EPYC',",
            "                'EPYC-IBPB'],",
            "            'ppc64': [",
            "                'POWER6',",
            "                'POWER7',",
            "                'POWER8',",
            "                'POWER9',",
            "                'POWERPC_e5500',",
            "                'POWERPC_e6500']",
            "        }",
            "        return mapping.get(arch, [])",
            "",
            "    # Features are kept separately so that the tests can patch this",
            "    # class variable with alternate values.",
            "    _domain_capability_features = '''  <features>",
            "    <gic supported='no'/>",
            "  </features>'''",
            "",
            "    _domain_capability_features_with_SEV = '''  <features>",
            "    <gic supported='no'/>",
            "    <sev supported='yes'>",
            "      <cbitpos>47</cbitpos>",
            "      <reducedPhysBits>1</reducedPhysBits>",
            "    </sev>",
            "  </features>'''",
            "",
            "    _domain_capability_features_with_SEV_unsupported = \\",
            "        _domain_capability_features_with_SEV.replace('yes', 'no')",
            "",
            "    def getCapabilities(self):",
            "        \"\"\"Return spoofed capabilities.\"\"\"",
            "        numa_topology = self.host_info.numa_topology",
            "        if isinstance(numa_topology, vconfig.LibvirtConfigCapsNUMATopology):",
            "            numa_topology = numa_topology.to_xml()",
            "",
            "        # we rely on os.uname() having been mocked already to ensure we have",
            "        # the correct \"host\" architecture",
            "        _capabilities = [",
            "            '<capabilities>\\n',",
            "            fake_libvirt_data.CAPABILITIES_HOST_TEMPLATES[os.uname().machine],",
            "        ] + list(fake_libvirt_data.CAPABILITIES_GUEST.values()) + [",
            "            '</capabilities>',",
            "        ]",
            "",
            "        return ''.join(_capabilities) % {",
            "            'sockets': self.host_info.cpu_sockets,",
            "            'cores': self.host_info.cpu_cores,",
            "            'threads': self.host_info.cpu_threads,",
            "            'topology': numa_topology,",
            "        }",
            "",
            "    def compareCPU(self, xml, flags):",
            "        tree = etree.fromstring(xml)",
            "",
            "        arch_node = tree.find('./arch')",
            "        if arch_node is not None:",
            "            if arch_node.text not in [obj_fields.Architecture.X86_64,",
            "                                      obj_fields.Architecture.I686]:",
            "                return VIR_CPU_COMPARE_INCOMPATIBLE",
            "",
            "        model_node = tree.find('./model')",
            "        if model_node is not None:",
            "            # arch_node may not present, therefore query all cpu models.",
            "            if model_node.text not in self.getCPUModelNames('x86_64') and \\",
            "                model_node.text not in self.getCPUModelNames('ppc64'):",
            "                raise make_libvirtError(",
            "                    libvirtError,",
            "                    \"internal error: Unknown CPU model %s\" % model_node.text,",
            "                    error_code = VIR_ERR_INTERNAL_ERROR,",
            "                    error_domain=VIR_FROM_QEMU)",
            "            if model_node.text != self.host_info.cpu_model:",
            "                return VIR_CPU_COMPARE_INCOMPATIBLE",
            "",
            "        vendor_node = tree.find('./vendor')",
            "        if vendor_node is not None:",
            "            if vendor_node.text != self.host_info.cpu_vendor:",
            "                return VIR_CPU_COMPARE_INCOMPATIBLE",
            "",
            "        # The rest of the stuff libvirt implements is rather complicated",
            "        # and I don't think it adds much value to replicate it here.",
            "",
            "        return VIR_CPU_COMPARE_IDENTICAL",
            "",
            "    def getCPUStats(self, cpuNum, flag):",
            "        if cpuNum < 2:",
            "            return {'kernel': 5664160000000,",
            "                    'idle': 1592705190000000,",
            "                    'user': 26728850000000,",
            "                    'iowait': 6121490000000}",
            "        else:",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                    \"invalid argument: Invalid cpu number\",",
            "                    error_code=VIR_ERR_INTERNAL_ERROR,",
            "                    error_domain=VIR_FROM_QEMU)",
            "",
            "    def device_lookup_by_name(self, dev_name):",
            "        return self.pci_info.get_device_by_name(dev_name)",
            "",
            "    def nodeDeviceLookupByName(self, name):",
            "        if name.startswith('mdev'):",
            "            return self.mdev_info.get_device_by_name(name)",
            "",
            "        if name.startswith('vdpa'):",
            "            return self.vdpa_info.get_device_by_name(name)",
            "",
            "        pci_dev = self.pci_info.get_device_by_name(name)",
            "        if pci_dev:",
            "            return pci_dev",
            "        try:",
            "            return self._nodedevs[name]",
            "        except KeyError:",
            "            raise make_libvirtError(",
            "                    libvirtError,",
            "                    \"no nodedev with matching name %s\" % name,",
            "                    error_code=VIR_ERR_NO_NODE_DEVICE,",
            "                    error_domain=VIR_FROM_NODEDEV)",
            "",
            "    def listDevices(self, cap, flags):",
            "        if cap == 'pci':",
            "            return self.pci_info.get_all_devices()",
            "        if cap == 'mdev':",
            "            return self.mdev_info.get_all_devices()",
            "        if cap == 'mdev_types':",
            "            return self.pci_info.get_all_mdev_capable_devices()",
            "        if cap == 'vdpa':",
            "            return self.vdpa_info.get_all_devices()",
            "        else:",
            "            raise ValueError('Capability \"%s\" is not supported' % cap)",
            "",
            "    def baselineCPU(self, cpu, flag):",
            "        \"\"\"Add new libvirt API.\"\"\"",
            "        return \"\"\"<cpu mode='custom' match='exact'>",
            "                    <model>Penryn</model>",
            "                    <vendor>Intel</vendor>",
            "                    <feature name='xtpr'/>",
            "                    <feature name='tm2'/>",
            "                    <feature name='est'/>",
            "                    <feature name='vmx'/>",
            "                    <feature name='ds_cpl'/>",
            "                    <feature name='monitor'/>",
            "                    <feature name='pbe'/>",
            "                    <feature name='tm'/>",
            "                    <feature name='ht'/>",
            "                    <feature name='ss'/>",
            "                    <feature name='acpi'/>",
            "                    <feature name='ds'/>",
            "                    <feature name='vme'/>",
            "                    <feature policy='require' name='aes'/>",
            "                  </cpu>\"\"\"",
            "",
            "    def secretLookupByUsage(self, usage_type_obj, usage_id):",
            "        pass",
            "",
            "    def secretDefineXML(self, xml):",
            "        secret = Secret(self, xml)",
            "        self._add_secret(secret)",
            "        return secret",
            "",
            "    def listAllDevices(self, flags):",
            "        devices = []",
            "        if flags & VIR_CONNECT_LIST_NODE_DEVICES_CAP_PCI_DEV:",
            "            devices.extend(",
            "                NodeDevice(self, xml=dev.XMLDesc(0))",
            "                for dev in self.pci_info.devices.values()",
            "            )",
            "        if flags & VIR_CONNECT_LIST_NODE_DEVICES_CAP_NET:",
            "            # TODO(stephenfin): Implement fake netdevs so we can test the",
            "            # capability reporting",
            "            pass",
            "        if flags & VIR_CONNECT_LIST_NODE_DEVICES_CAP_VDPA:",
            "            devices.extend(",
            "                NodeDevice(self, xml=dev.XMLDesc(0))",
            "                for dev in self.vdpa_info.devices.values()",
            "            )",
            "        return devices",
            "",
            "",
            "def openAuth(uri, auth, flags=0):",
            "",
            "    if type(auth) != list:",
            "        raise Exception(\"Expected a list for 'auth' parameter\")",
            "",
            "    if type(auth[0]) != list:",
            "        raise Exception(\"Expected a function in 'auth[0]' parameter\")",
            "",
            "    if not callable(auth[1]):",
            "        raise Exception(\"Expected a function in 'auth[1]' parameter\")",
            "",
            "    return Connection(uri, (flags == VIR_CONNECT_RO))",
            "",
            "",
            "def virEventRunDefaultImpl():",
            "    time.sleep(1)",
            "",
            "",
            "def virEventRegisterDefaultImpl():",
            "    if connection_used:",
            "        raise Exception(\"virEventRegisterDefaultImpl() must be \"",
            "                        \"called before connection is used.\")",
            "",
            "",
            "def registerErrorHandler(handler, ctxt):",
            "    pass",
            "",
            "",
            "def make_libvirtError(error_class, msg, error_code=None,",
            "                       error_domain=None, error_message=None,",
            "                       error_level=None, str1=None, str2=None, str3=None,",
            "                       int1=None, int2=None):",
            "    \"\"\"Convenience function for creating `libvirtError` exceptions which",
            "    allow you to specify arguments in constructor without having to manipulate",
            "    the `err` tuple directly.",
            "",
            "    We need to pass in `error_class` to this function because it may be",
            "    `libvirt.libvirtError` or `fakelibvirt.libvirtError` depending on whether",
            "    `libvirt-python` is installed.",
            "    \"\"\"",
            "    exc = error_class(msg)",
            "    exc.err = (error_code, error_domain, error_message, error_level,",
            "               str1, str2, str3, int1, int2)",
            "    return exc",
            "",
            "",
            "virDomain = Domain",
            "virNodeDevice = NodeDevice",
            "",
            "virConnect = Connection",
            "virSecret = Secret",
            "",
            "",
            "# A private libvirt-python class and global only provided here for testing to",
            "# ensure it's not returned by libvirt.host.Host.get_libvirt_proxy_classes.",
            "class FakeHandler(object):",
            "    def __init__(self):",
            "        pass",
            "",
            "",
            "_EventAddHandleFunc = FakeHandler",
            "",
            "",
            "class LibvirtFixture(fixtures.Fixture):",
            "    \"\"\"Performs global setup/stubbing for all libvirt tests.",
            "    \"\"\"",
            "",
            "    def __init__(self, stub_os_vif=True):",
            "        self.stub_os_vif = stub_os_vif",
            "        self.pci_address_to_mac_map = collections.defaultdict(",
            "            lambda: '52:54:00:1e:59:c6')",
            "",
            "    def update_sriov_mac_address_mapping(self, pci_address_to_mac_map):",
            "        self.pci_address_to_mac_map.update(pci_address_to_mac_map)",
            "",
            "    def fake_get_mac_by_pci_address(self, pci_addr, pf_interface=False):",
            "        res = self.pci_address_to_mac_map[pci_addr]",
            "        return res",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        # Some modules load the libvirt library in a strange way",
            "        for module in ('driver', 'host', 'guest', 'migration'):",
            "            i = 'nova.virt.libvirt.{module}.libvirt'.format(module=module)",
            "            # NOTE(mdbooth): The strange incantation below means 'this module'",
            "            self.useFixture(fixtures.MonkeyPatch(i, sys.modules[__name__]))",
            "",
            "        self.useFixture(",
            "            fixtures.MockPatch('nova.virt.libvirt.utils.get_fs_info'))",
            "        self.mock_get_machine_ips = self.useFixture(",
            "            fixtures.MockPatch('nova.compute.utils.get_machine_ips')).mock",
            "",
            "        # libvirt driver needs to call out to the filesystem to get the",
            "        # parent_ifname for the SRIOV VFs.",
            "        self.mock_get_ifname_by_pci_address = self.useFixture(",
            "            fixtures.MockPatch(",
            "                \"nova.pci.utils.get_ifname_by_pci_address\",",
            "                return_value=\"fake_pf_interface_name\",",
            "            )",
            "        ).mock",
            "",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.pci.utils.get_mac_by_pci_address',",
            "            side_effect=self.fake_get_mac_by_pci_address))",
            "",
            "        # libvirt calls out to sysfs to get the vfs ID during macvtap plug",
            "        self.mock_get_vf_num_by_pci_address = self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.pci.utils.get_vf_num_by_pci_address', return_value=1",
            "            )",
            "        ).mock",
            "",
            "        # libvirt calls out to privsep to set the mac and vlan of a macvtap",
            "        self.mock_set_device_macaddr_and_vlan = self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.privsep.linux_net.set_device_macaddr_and_vlan')).mock",
            "",
            "        # libvirt calls out to privsep to set the port state during macvtap",
            "        # plug",
            "        self.mock_set_device_macaddr = self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.privsep.linux_net.set_device_macaddr')).mock",
            "",
            "        # Don't assume that the system running tests has a valid machine-id",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '._get_host_sysinfo_serial_os', return_value=uuids.machine_id))",
            "",
            "        # Stub out _log_host_capabilities since it logs a giant string at INFO",
            "        # and we don't want that to blow up the subunit parser in test runs.",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.virt.libvirt.host.Host._log_host_capabilities'))",
            "",
            "        # Ensure tests perform the same on all host architectures",
            "        fake_uname = os_uname(",
            "            'Linux', '', '5.4.0-0-generic', '', obj_fields.Architecture.X86_64)",
            "        self.mock_uname = self.useFixture(",
            "            fixtures.MockPatch('os.uname', return_value=fake_uname)).mock",
            "",
            "        real_exists = os.path.exists",
            "",
            "        def fake_exists(path):",
            "            if path == host.SEV_KERNEL_PARAM_FILE:",
            "                return False",
            "            return real_exists(path)",
            "",
            "        self.useFixture(fixtures.MonkeyPatch('os.path.exists', fake_exists))",
            "",
            "        # ...and on all machine types",
            "        fake_loaders = [",
            "            {",
            "                'description': 'UEFI firmware for x86_64',",
            "                'interface-types': ['uefi'],",
            "                'mapping': {",
            "                    'device': 'flash',",
            "                    'executable': {",
            "                        'filename': '/usr/share/OVMF/OVMF_CODE.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                    'nvram-template': {",
            "                        'filename': '/usr/share/OVMF/OVMF_VARS.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                },",
            "                'targets': [",
            "                    {",
            "                        'architecture': 'x86_64',",
            "                        'machines': ['pc-i440fx-*', 'pc-q35-*'],",
            "                    },",
            "                ],",
            "                'features': ['acpi-s3', 'amd-sev', 'verbose-dynamic'],",
            "                'tags': [],",
            "            },",
            "            {",
            "                'description': 'UEFI firmware for x86_64, with SB+SMM',",
            "                'interface-types': ['uefi'],",
            "                'mapping': {",
            "                    'device': 'flash',",
            "                    'executable': {",
            "                        'filename': '/usr/share/OVMF/OVMF_CODE.secboot.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                    'nvram-template': {",
            "                        'filename': '/usr/share/OVMF/OVMF_VARS.secboot.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                },",
            "                'targets': [",
            "                    {",
            "                        'architecture': 'x86_64',",
            "                        'machines': ['pc-q35-*'],",
            "                    },",
            "                ],",
            "                'features': [",
            "                    'acpi-s3',",
            "                    'amd-sev',",
            "                    'enrolled-keys',",
            "                    'requires-smm',",
            "                    'secure-boot',",
            "                    'verbose-dynamic',",
            "                ],",
            "                'tags': [],",
            "            },",
            "            {",
            "                'description': 'UEFI firmware for aarch64',",
            "                'interface-types': ['uefi'],",
            "                'mapping': {",
            "                    'device': 'flash',",
            "                    'executable': {",
            "                        'filename': '/usr/share/AAVMF/AAVMF_CODE.fd',",
            "                        'format': 'raw',",
            "                    },",
            "                    'nvram-template': {",
            "                        'filename': '/usr/share/AAVMF/AAVMF_VARS.fd',",
            "                        'format': 'raw',",
            "                    }",
            "                },",
            "                'targets': [",
            "                    {",
            "                        'architecture': 'aarch64',",
            "                        'machines': ['virt-*'],",
            "                    }",
            "                ],",
            "                'features': ['verbose-static'],",
            "                \"tags\": [],",
            "            },",
            "        ]",
            "        self.useFixture(",
            "            fixtures.MockPatch(",
            "                'nova.virt.libvirt.host.Host.loaders',",
            "                new_callable=mock.PropertyMock,",
            "                return_value=fake_loaders))",
            "",
            "        disable_event_thread(self)",
            "",
            "        if self.stub_os_vif:",
            "            # Make sure to never try and actually plug/unplug VIFs in os-vif",
            "            # unless we're explicitly testing that code and the test itself",
            "            # will handle the appropriate mocking.",
            "            self.useFixture(fixtures.MonkeyPatch(",
            "                'nova.virt.libvirt.vif.LibvirtGenericVIFDriver._plug_os_vif',",
            "                lambda *a, **kw: None))",
            "            self.useFixture(fixtures.MonkeyPatch(",
            "                'nova.virt.libvirt.vif.LibvirtGenericVIFDriver._unplug_os_vif',",
            "                lambda *a, **kw: None))",
            "",
            "        # os_vif.initialize is typically done in nova-compute startup",
            "        # even if we are not planning to plug anything with os_vif in the test",
            "        # we still need the object model initialized to be able to generate",
            "        # guest config xml properly",
            "        import os_vif",
            "        os_vif.initialize()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "2237": [
                "LibvirtFixture",
                "setUp"
            ],
            "2238": [
                "LibvirtFixture",
                "setUp"
            ],
            "2239": [
                "LibvirtFixture",
                "setUp"
            ]
        },
        "addLocation": []
    },
    "nova/tests/functional/libvirt/test_pci_sriov_servers.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " import nova"
            },
            "2": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from nova import context"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+from nova import exception"
            },
            "4": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from nova.network import constants"
            },
            "5": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from nova import objects"
            },
            "6": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from nova.objects import fields"
            },
            "7": {
                "beforePatchRowNumber": 1042,
                "afterPatchRowNumber": 1043,
                "PatchRowcode": "             ],"
            },
            "8": {
                "beforePatchRowNumber": 1043,
                "afterPatchRowNumber": 1044,
                "PatchRowcode": "         )"
            },
            "9": {
                "beforePatchRowNumber": 1044,
                "afterPatchRowNumber": 1045,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1046,
                "PatchRowcode": "+    def test_change_bound_port_vnic_type_kills_compute_at_restart(self):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1047,
                "PatchRowcode": "+        \"\"\"Create a server with a direct port and change the vnic_type of the"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1048,
                "PatchRowcode": "+        bound port to macvtap. Then restart the compute service."
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1049,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1050,
                "PatchRowcode": "+        As the vnic_type is changed on the port but the vif_type is hwveb"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1051,
                "PatchRowcode": "+        instead of macvtap the vif plug logic will try to look up the netdev"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1052,
                "PatchRowcode": "+        of the parent VF. Howvere that VF consumed by the instance so the"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1053,
                "PatchRowcode": "+        netdev does not exists. This causes that the compute service will fail"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1054,
                "PatchRowcode": "+        with an exception during startup"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1055,
                "PatchRowcode": "+        \"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1056,
                "PatchRowcode": "+        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1057,
                "PatchRowcode": "+        self.start_compute(pci_info=pci_info)"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1058,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1059,
                "PatchRowcode": "+        # create a direct port"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1060,
                "PatchRowcode": "+        port = self.neutron.network_4_port_1"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1061,
                "PatchRowcode": "+        self.neutron.create_port({'port': port})"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1062,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1063,
                "PatchRowcode": "+        # create a server using the VF via neutron"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1064,
                "PatchRowcode": "+        server = self._create_server(networks=[{'port': port['id']}])"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1065,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1066,
                "PatchRowcode": "+        # update the vnic_type of the port in neutron"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1067,
                "PatchRowcode": "+        port = copy.deepcopy(port)"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1068,
                "PatchRowcode": "+        port['binding:vnic_type'] = 'macvtap'"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1069,
                "PatchRowcode": "+        self.neutron.update_port(port['id'], {\"port\": port})"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1070,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1071,
                "PatchRowcode": "+        compute = self.computes['compute1']"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1072,
                "PatchRowcode": "+"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1073,
                "PatchRowcode": "+        # Force an update on the instance info cache to ensure nova gets the"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1074,
                "PatchRowcode": "+        # information about the updated port"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1075,
                "PatchRowcode": "+        with context.target_cell("
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1076,
                "PatchRowcode": "+            context.get_admin_context(),"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1077,
                "PatchRowcode": "+            self.host_mappings['compute1'].cell_mapping"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1078,
                "PatchRowcode": "+        ) as cctxt:"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1079,
                "PatchRowcode": "+            compute.manager._heal_instance_info_cache(cctxt)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1080,
                "PatchRowcode": "+"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1081,
                "PatchRowcode": "+        def fake_get_ifname_by_pci_address(pci_addr: str, pf_interface=False):"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1082,
                "PatchRowcode": "+            # we want to fail the netdev lookup only if the pci_address is"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1083,
                "PatchRowcode": "+            # already consumed by our instance. So we look into the instance"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1084,
                "PatchRowcode": "+            # definition to see if the device is attached to the instance as VF"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1085,
                "PatchRowcode": "+            conn = compute.manager.driver._host.get_connection()"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1086,
                "PatchRowcode": "+            dom = conn.lookupByUUIDString(server['id'])"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1087,
                "PatchRowcode": "+            dev = dom._def['devices']['nics'][0]"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1088,
                "PatchRowcode": "+            lookup_addr = pci_addr.replace(':', '_').replace('.', '_')"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1089,
                "PatchRowcode": "+            if ("
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1090,
                "PatchRowcode": "+                dev['type'] == 'hostdev' and"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1091,
                "PatchRowcode": "+                dev['source'] == 'pci_' + lookup_addr"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1092,
                "PatchRowcode": "+            ):"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1093,
                "PatchRowcode": "+                # nova tried to look up the netdev of an already consumed VF."
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1094,
                "PatchRowcode": "+                # So we have to fail"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1095,
                "PatchRowcode": "+                raise exception.PciDeviceNotFoundById(id=pci_addr)"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1096,
                "PatchRowcode": "+"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1097,
                "PatchRowcode": "+        # We need to simulate the actual failure manually as in our functional"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1098,
                "PatchRowcode": "+        # environment all the PCI lookup is mocked. In reality nova tries to"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1099,
                "PatchRowcode": "+        # look up the netdev of the pci device on the host used by the port as"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1100,
                "PatchRowcode": "+        # the parent of the macvtap. However, as the originally direct port is"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1101,
                "PatchRowcode": "+        # bound to the instance, the VF pci device is already consumed by the"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1102,
                "PatchRowcode": "+        # instance and therefore there is no netdev for the VF."
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1103,
                "PatchRowcode": "+        self.libvirt.mock_get_ifname_by_pci_address.side_effect = ("
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1104,
                "PatchRowcode": "+            fake_get_ifname_by_pci_address"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1105,
                "PatchRowcode": "+        )"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1106,
                "PatchRowcode": "+        # This is bug 1981813 as the compute service fails to start with an"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1107,
                "PatchRowcode": "+        # exception."
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1108,
                "PatchRowcode": "+        # Nova cannot prevent the vnic_type change on a bound port. Neutron"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1109,
                "PatchRowcode": "+        # should prevent that instead. But the nova-compute should still"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1110,
                "PatchRowcode": "+        # be able to start up and only log an ERROR for this instance in"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1111,
                "PatchRowcode": "+        # inconsistent state."
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1112,
                "PatchRowcode": "+        self.assertRaises("
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1113,
                "PatchRowcode": "+            exception.PciDeviceNotFoundById,"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1114,
                "PatchRowcode": "+            self.restart_compute_service, 'compute1'"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1115,
                "PatchRowcode": "+        )"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1116,
                "PatchRowcode": "+"
            },
            "81": {
                "beforePatchRowNumber": 1045,
                "afterPatchRowNumber": 1117,
                "PatchRowcode": " "
            },
            "82": {
                "beforePatchRowNumber": 1046,
                "afterPatchRowNumber": 1118,
                "PatchRowcode": " class SRIOVAttachDetachTest(_PCIServersTestBase):"
            },
            "83": {
                "beforePatchRowNumber": 1047,
                "afterPatchRowNumber": 1119,
                "PatchRowcode": "     # no need for aliases as these test will request SRIOV via neutron"
            }
        },
        "frontPatchFile": [
            "# Copyright (C) 2016 Red Hat, Inc",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import copy",
            "from unittest import mock",
            "from urllib import parse as urlparse",
            "",
            "import ddt",
            "import fixtures",
            "from lxml import etree",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_serialization import jsonutils",
            "from oslo_utils.fixture import uuidsentinel as uuids",
            "from oslo_utils import units",
            "",
            "import nova",
            "from nova import context",
            "from nova.network import constants",
            "from nova import objects",
            "from nova.objects import fields",
            "from nova.pci.utils import parse_address",
            "from nova.tests import fixtures as nova_fixtures",
            "from nova.tests.fixtures import libvirt as fakelibvirt",
            "from nova.tests.functional.api import client",
            "from nova.tests.functional.libvirt import base",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class _PCIServersTestBase(base.ServersTestBase):",
            "",
            "    ADDITIONAL_FILTERS = ['NUMATopologyFilter', 'PciPassthroughFilter']",
            "",
            "    def setUp(self):",
            "        self.ctxt = context.get_admin_context()",
            "        self.flags(",
            "            device_spec=self.PCI_DEVICE_SPEC,",
            "            alias=self.PCI_ALIAS,",
            "            group='pci'",
            "        )",
            "",
            "        super(_PCIServersTestBase, self).setUp()",
            "",
            "        # Mock the 'PciPassthroughFilter' filter, as most tests need to inspect",
            "        # this",
            "        host_manager = self.scheduler.manager.host_manager",
            "        pci_filter_class = host_manager.filter_cls_map['PciPassthroughFilter']",
            "        host_pass_mock = mock.Mock(wraps=pci_filter_class().host_passes)",
            "        self.mock_filter = self.useFixture(fixtures.MockPatch(",
            "            'nova.scheduler.filters.pci_passthrough_filter'",
            "            '.PciPassthroughFilter.host_passes',",
            "            side_effect=host_pass_mock)).mock",
            "",
            "    def assertPCIDeviceCounts(self, hostname, total, free):",
            "        \"\"\"Ensure $hostname has $total devices, $free of which are free.\"\"\"",
            "        devices = objects.PciDeviceList.get_by_compute_node(",
            "            self.ctxt,",
            "            objects.ComputeNode.get_by_nodename(self.ctxt, hostname).id,",
            "        )",
            "        self.assertEqual(total, len(devices))",
            "        self.assertEqual(free, len([d for d in devices if d.is_available()]))",
            "",
            "    def _get_rp_by_name(self, name, rps):",
            "        for rp in rps:",
            "            if rp[\"name\"] == name:",
            "                return rp",
            "        self.fail(f'RP {name} is not found in Placement {rps}')",
            "",
            "    def assert_placement_pci_view(",
            "        self, hostname, inventories, traits, usages=None, allocations=None",
            "    ):",
            "        if not usages:",
            "            usages = {}",
            "",
            "        if not allocations:",
            "            allocations = {}",
            "",
            "        compute_rp_uuid = self.compute_rp_uuids[hostname]",
            "        rps = self._get_all_rps_in_a_tree(compute_rp_uuid)",
            "",
            "        # rps also contains the root provider so we subtract 1",
            "        self.assertEqual(",
            "            len(inventories),",
            "            len(rps) - 1,",
            "            f\"Number of RPs on {hostname} doesn't match. \"",
            "            f\"Expected {list(inventories)} actual {[rp['name'] for rp in rps]}\"",
            "        )",
            "",
            "        for rp_name, inv in inventories.items():",
            "            real_rp_name = f'{hostname}_{rp_name}'",
            "            rp = self._get_rp_by_name(real_rp_name, rps)",
            "            rp_inv = self._get_provider_inventory(rp['uuid'])",
            "",
            "            self.assertEqual(",
            "                len(inv),",
            "                len(rp_inv),",
            "                f\"Number of inventories on {real_rp_name} are not as \"",
            "                f\"expected. Expected {inv}, actual {rp_inv}\"",
            "            )",
            "            for rc, total in inv.items():",
            "                self.assertEqual(",
            "                    total,",
            "                    rp_inv[rc][\"total\"])",
            "                self.assertEqual(",
            "                    total,",
            "                    rp_inv[rc][\"max_unit\"])",
            "",
            "            rp_traits = self._get_provider_traits(rp['uuid'])",
            "            self.assertEqual(",
            "                # COMPUTE_MANAGED_PCI_DEVICE is automatically reported on",
            "                # PCI device RPs by nova",
            "                set(traits[rp_name]) | {\"COMPUTE_MANAGED_PCI_DEVICE\"},",
            "                set(rp_traits),",
            "                f\"Traits on RP {real_rp_name} does not match with expectation\"",
            "            )",
            "",
            "        for rp_name, usage in usages.items():",
            "            real_rp_name = f'{hostname}_{rp_name}'",
            "            rp = self._get_rp_by_name(real_rp_name, rps)",
            "            rp_usage = self._get_provider_usages(rp['uuid'])",
            "            self.assertEqual(",
            "                usage,",
            "                rp_usage,",
            "                f\"Usage on RP {real_rp_name} does not match with expectation\"",
            "            )",
            "",
            "        for consumer, expected_allocations in allocations.items():",
            "            actual_allocations = self._get_allocations_by_server_uuid(consumer)",
            "            self.assertEqual(",
            "                len(expected_allocations),",
            "                # actual_allocations also contains allocations against the",
            "                # root provider for VCPU, MEMORY_MB, and DISK_GB so subtract",
            "                # one",
            "                len(actual_allocations) - 1,",
            "                f\"The consumer {consumer} allocates from different number of \"",
            "                f\"RPs than expected. Expected: {expected_allocations}, \"",
            "                f\"Actual: {actual_allocations}\"",
            "            )",
            "            for rp_name, expected_rp_allocs in expected_allocations.items():",
            "                real_rp_name = f'{hostname}_{rp_name}'",
            "                rp = self._get_rp_by_name(real_rp_name, rps)",
            "                self.assertIn(",
            "                    rp['uuid'],",
            "                    actual_allocations,",
            "                    f\"The consumer {consumer} expected to allocate from \"",
            "                    f\"{rp['uuid']}. Expected: {expected_allocations}, \"",
            "                    f\"Actual: {actual_allocations}\"",
            "                )",
            "                actual_rp_allocs = actual_allocations[rp['uuid']]['resources']",
            "                self.assertEqual(",
            "                    expected_rp_allocs,",
            "                    actual_rp_allocs,",
            "                    f\"The consumer {consumer} expected to have allocation \"",
            "                    f\"{expected_rp_allocs} on {rp_name} but it has \"",
            "                    f\"{actual_rp_allocs} instead.\"",
            "                )",
            "",
            "",
            "class _PCIServersWithMigrationTestBase(_PCIServersTestBase):",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        self.useFixture(fixtures.MonkeyPatch(",
            "            'nova.tests.fixtures.libvirt.Domain.migrateToURI3',",
            "            self._migrate_stub))",
            "",
            "    def _migrate_stub(self, domain, destination, params, flags):",
            "        \"\"\"Stub out migrateToURI3.\"\"\"",
            "",
            "        src_hostname = domain._connection.hostname",
            "        dst_hostname = urlparse.urlparse(destination).netloc",
            "",
            "        # In a real live migration, libvirt and QEMU on the source and",
            "        # destination talk it out, resulting in the instance starting to exist",
            "        # on the destination. Fakelibvirt cannot do that, so we have to",
            "        # manually create the \"incoming\" instance on the destination",
            "        # fakelibvirt.",
            "        dst = self.computes[dst_hostname]",
            "        dst.driver._host.get_connection().createXML(",
            "            params['destination_xml'],",
            "            'fake-createXML-doesnt-care-about-flags')",
            "",
            "        src = self.computes[src_hostname]",
            "        conn = src.driver._host.get_connection()",
            "",
            "        # because migrateToURI3 is spawned in a background thread, this method",
            "        # does not block the upper nova layers. Because we don't want nova to",
            "        # think the live migration has finished until this method is done, the",
            "        # last thing we do is make fakelibvirt's Domain.jobStats() return",
            "        # VIR_DOMAIN_JOB_COMPLETED.",
            "        server = etree.fromstring(",
            "            params['destination_xml']",
            "        ).find('./uuid').text",
            "        dom = conn.lookupByUUIDString(server)",
            "        dom.complete_job()",
            "",
            "",
            "class SRIOVServersTest(_PCIServersWithMigrationTestBase):",
            "",
            "    # TODO(stephenfin): We're using this because we want to be able to force",
            "    # the host during scheduling. We should instead look at overriding policy",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    VFS_ALIAS_NAME = 'vfs'",
            "    PFS_ALIAS_NAME = 'pfs'",
            "",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "    )]",
            "    # PFs will be removed from pools unless they are specifically",
            "    # requested, so we explicitly request them with the 'device_type'",
            "    # attribute",
            "    PCI_ALIAS = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            'device_type': fields.PciDeviceType.SRIOV_PF,",
            "            'name': PFS_ALIAS_NAME,",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            'name': VFS_ALIAS_NAME,",
            "        },",
            "    )]",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        # The ultimate base class _IntegratedTestBase uses NeutronFixture but",
            "        # we need a bit more intelligent neutron for these tests. Applying the",
            "        # new fixture here means that we re-stub what the previous neutron",
            "        # fixture already stubbed.",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "",
            "    def _disable_sriov_in_pf(self, pci_info):",
            "        # Check for PF and change the capability from virt_functions",
            "        # Delete all the VFs",
            "        vfs_to_delete = []",
            "",
            "        for device_name, device in pci_info.devices.items():",
            "            if 'virt_functions' in device.pci_device:",
            "                device.generate_xml(skip_capability=True)",
            "            elif 'phys_function' in device.pci_device:",
            "                vfs_to_delete.append(device_name)",
            "",
            "        for device in vfs_to_delete:",
            "            del pci_info.devices[device]",
            "",
            "    def test_create_server_with_VF(self):",
            "        \"\"\"Create a server with an SR-IOV VF-type PCI device.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo()",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server",
            "        extra_spec = {\"pci_passthrough:alias\": \"%s:1\" % self.VFS_ALIAS_NAME}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # ensure the filter was called",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_create_server_with_PF(self):",
            "        \"\"\"Create a server with an SR-IOV PF-type PCI device.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo()",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server",
            "        extra_spec = {\"pci_passthrough:alias\": \"%s:1\" % self.PFS_ALIAS_NAME}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # ensure the filter was called",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_create_server_with_PF_no_VF(self):",
            "        \"\"\"Create a server with a PF and ensure the VFs are then reserved.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=4)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server using the PF",
            "        extra_spec_pfs = {\"pci_passthrough:alias\": f\"{self.PFS_ALIAS_NAME}:1\"}",
            "        flavor_id_pfs = self._create_flavor(extra_spec=extra_spec_pfs)",
            "        self._create_server(flavor_id=flavor_id_pfs, networks='none')",
            "",
            "        # now attempt to build another server, this time using the VF; this",
            "        # should fail because the VF is used by an instance",
            "        extra_spec_vfs = {\"pci_passthrough:alias\": f\"{self.VFS_ALIAS_NAME}:1\"}",
            "        flavor_id_vfs = self._create_flavor(extra_spec=extra_spec_vfs)",
            "        self._create_server(",
            "            flavor_id=flavor_id_vfs, networks='none', expected_state='ERROR',",
            "        )",
            "",
            "    def test_create_server_with_VF_no_PF(self):",
            "        \"\"\"Create a server with a VF and ensure the PF is then reserved.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=4)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server using the VF",
            "        extra_spec_vfs = {'pci_passthrough:alias': f'{self.VFS_ALIAS_NAME}:1'}",
            "        flavor_id_vfs = self._create_flavor(extra_spec=extra_spec_vfs)",
            "        self._create_server(flavor_id=flavor_id_vfs, networks='none')",
            "",
            "        # now attempt to build another server, this time using the PF; this",
            "        # should fail because the PF is used by an instance",
            "        extra_spec_pfs = {'pci_passthrough:alias': f'{self.PFS_ALIAS_NAME}:1'}",
            "        flavor_id_pfs = self._create_flavor(extra_spec=extra_spec_pfs)",
            "        self._create_server(",
            "            flavor_id=flavor_id_pfs, networks='none', expected_state='ERROR',",
            "        )",
            "",
            "    def test_create_server_with_neutron(self):",
            "        \"\"\"Create an instance using a neutron-provisioned SR-IOV VIF.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)",
            "",
            "        orig_create = nova.virt.libvirt.guest.Guest.create",
            "",
            "        def fake_create(cls, xml, host):",
            "            tree = etree.fromstring(xml)",
            "            elem = tree.find('./devices/interface/source/address')",
            "",
            "            # compare address",
            "            expected = ('0x81', '0x00', '0x2')",
            "            actual = (",
            "                elem.get('bus'), elem.get('slot'), elem.get('function'),",
            "            )",
            "            self.assertEqual(expected, actual)",
            "",
            "            return orig_create(xml, host)",
            "",
            "        self.stub_out(",
            "            'nova.virt.libvirt.guest.Guest.create',",
            "            fake_create,",
            "        )",
            "",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create the port",
            "        self.neutron.create_port({'port': self.neutron.network_4_port_1})",
            "",
            "        # ensure the binding details are currently unset",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            networks=[",
            "                {'port': base.LibvirtNeutronFixture.network_4_port_1['id']},",
            "            ],",
            "        )",
            "",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1515',",
            "                'pci_slot': '0000:81:00.2',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_live_migrate_server_with_PF(self):",
            "        \"\"\"Live migrate an instance with a PCI PF.",
            "",
            "        This should fail because it's not possible to live migrate an instance",
            "        with a PCI passthrough device, even if it's a SR-IOV PF.",
            "        \"\"\"",
            "",
            "        # start two compute services",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "        self.start_compute(",
            "            hostname='test_compute1',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "",
            "        # create a server",
            "        extra_spec = {'pci_passthrough:alias': f'{self.PFS_ALIAS_NAME}:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now live migrate that server",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            self._live_migrate,",
            "            server, 'completed')",
            "        # NOTE(stephenfin): this wouldn't happen in a real deployment since",
            "        # live migration is a cast, but since we are using CastAsCallFixture",
            "        # this will bubble to the API",
            "        self.assertEqual(500, ex.response.status_code)",
            "        self.assertIn('NoValidHost', str(ex))",
            "",
            "    def test_live_migrate_server_with_VF(self):",
            "        \"\"\"Live migrate an instance with a PCI VF.",
            "",
            "        This should fail because it's not possible to live migrate an instance",
            "        with a PCI passthrough device, even if it's a SR-IOV VF.",
            "        \"\"\"",
            "",
            "        # start two compute services",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "        self.start_compute(",
            "            hostname='test_compute1',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "",
            "        # create a server",
            "        extra_spec = {'pci_passthrough:alias': f'{self.VFS_ALIAS_NAME}:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now live migrate that server",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            self._live_migrate,",
            "            server, 'completed')",
            "        # NOTE(stephenfin): this wouldn't happen in a real deployment since",
            "        # live migration is a cast, but since we are using CastAsCallFixture",
            "        # this will bubble to the API",
            "        self.assertEqual(500, ex.response.status_code)",
            "        self.assertIn('NoValidHost', str(ex))",
            "",
            "    def _test_move_operation_with_neutron(self, move_operation,",
            "                                          expect_fail=False):",
            "        # The purpose here is to force an observable PCI slot update when",
            "        # moving from source to dest. This is accomplished by having a single",
            "        # PCI VF device on the source, 2 PCI VF devices on the dest, and",
            "        # relying on the fact that our fake HostPCIDevicesInfo creates",
            "        # predictable PCI addresses. The PCI VF device on source and the first",
            "        # PCI VF device on dest will have identical PCI addresses. By sticking",
            "        # a \"placeholder\" instance on that first PCI VF device on the dest, the",
            "        # incoming instance from source will be forced to consume the second",
            "        # dest PCI VF device, with a different PCI address.",
            "        # We want to test server operations with SRIOV VFs and SRIOV PFs so",
            "        # the config of the compute hosts also have one extra PCI PF devices",
            "        # without any VF children. But the two compute has different PCI PF",
            "        # addresses and MAC so that the test can observe the slot update as",
            "        # well as the MAC updated during migration and after revert.",
            "        source_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        source_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:aa',",
            "        )",
            "        self.start_compute(",
            "            hostname='source',",
            "            pci_info=source_pci_info)",
            "",
            "        dest_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        dest_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x6,  # make it different from the source host",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:bb',",
            "        )",
            "        self.start_compute(",
            "            hostname='dest',",
            "            pci_info=dest_pci_info)",
            "",
            "        source_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})",
            "        source_pf_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_pf})",
            "        dest_port1 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_2})",
            "        dest_port2 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_3})",
            "",
            "        source_server = self._create_server(",
            "            networks=[",
            "                {'port': source_port['port']['id']},",
            "                {'port': source_pf_port['port']['id']}",
            "            ],",
            "            host='source',",
            "        )",
            "        dest_server1 = self._create_server(",
            "            networks=[{'port': dest_port1['port']['id']}], host='dest')",
            "        dest_server2 = self._create_server(",
            "            networks=[{'port': dest_port2['port']['id']}], host='dest')",
            "",
            "        # Refresh the ports.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "        dest_port1 = self.neutron.show_port(dest_port1['port']['id'])",
            "        dest_port2 = self.neutron.show_port(dest_port2['port']['id'])",
            "",
            "        # Find the server on the dest compute that's using the same pci_slot as",
            "        # the server on the source compute, and delete the other one to make",
            "        # room for the incoming server from the source.",
            "        source_pci_slot = source_port['port']['binding:profile']['pci_slot']",
            "        dest_pci_slot1 = dest_port1['port']['binding:profile']['pci_slot']",
            "        if dest_pci_slot1 == source_pci_slot:",
            "            same_slot_port = dest_port1",
            "            self._delete_server(dest_server2)",
            "        else:",
            "            same_slot_port = dest_port2",
            "            self._delete_server(dest_server1)",
            "",
            "        # Before moving, explicitly assert that the servers on source and dest",
            "        # have the same pci_slot in their port's binding profile",
            "        self.assertEqual(source_port['port']['binding:profile']['pci_slot'],",
            "                         same_slot_port['port']['binding:profile']['pci_slot'])",
            "",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the source host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:00.0',  # which is in sync with the source host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the source host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:aa',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "",
            "        # Before moving, assert that the servers on source and dest have the",
            "        # same PCI source address in their XML for their SRIOV nic.",
            "        source_conn = self.computes['source'].driver._host.get_connection()",
            "        dest_conn = self.computes['source'].driver._host.get_connection()",
            "        source_vms = [vm._def for vm in source_conn._vms.values()]",
            "        dest_vms = [vm._def for vm in dest_conn._vms.values()]",
            "        self.assertEqual(1, len(source_vms))",
            "        self.assertEqual(1, len(dest_vms))",
            "        self.assertEqual(1, len(source_vms[0]['devices']['nics']))",
            "        self.assertEqual(1, len(dest_vms[0]['devices']['nics']))",
            "        self.assertEqual(source_vms[0]['devices']['nics'][0]['source'],",
            "                         dest_vms[0]['devices']['nics'][0]['source'])",
            "",
            "        move_operation(source_server)",
            "",
            "        # Refresh the ports again, keeping in mind the source_port is now bound",
            "        # on the dest after the move.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        same_slot_port = self.neutron.show_port(same_slot_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "",
            "        self.assertNotEqual(",
            "            source_port['port']['binding:profile']['pci_slot'],",
            "            same_slot_port['port']['binding:profile']['pci_slot'])",
            "",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the dest host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:06.0',  # which is in sync with the dest host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the dest host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:bb',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "",
            "        conn = self.computes['dest'].driver._host.get_connection()",
            "        vms = [vm._def for vm in conn._vms.values()]",
            "        self.assertEqual(2, len(vms))",
            "        for vm in vms:",
            "            self.assertEqual(1, len(vm['devices']['nics']))",
            "        self.assertNotEqual(vms[0]['devices']['nics'][0]['source'],",
            "                            vms[1]['devices']['nics'][0]['source'])",
            "",
            "    def test_unshelve_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            self._shelve_server(source_server)",
            "            # Disable the source compute, to force unshelving on the dest.",
            "            self.api.put_service(self.computes['source'].service_ref.uuid,",
            "                                 {'status': 'disabled'})",
            "            self._unshelve_server(source_server)",
            "        self._test_move_operation_with_neutron(move_operation)",
            "",
            "    def test_cold_migrate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            # TODO(stephenfin): The mock of 'migrate_disk_and_power_off' should",
            "            # probably be less...dumb",
            "            with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                            '.migrate_disk_and_power_off', return_value='{}'):",
            "                self._migrate_server(source_server)",
            "            self._confirm_resize(source_server)",
            "        self._test_move_operation_with_neutron(move_operation)",
            "",
            "    def test_cold_migrate_and_rever_server_with_neutron(self):",
            "        # The purpose here is to force an observable PCI slot update when",
            "        # moving from source to dest and the from dest to source after the",
            "        # revert. This is accomplished by having a single",
            "        # PCI VF device on the source, 2 PCI VF devices on the dest, and",
            "        # relying on the fact that our fake HostPCIDevicesInfo creates",
            "        # predictable PCI addresses. The PCI VF device on source and the first",
            "        # PCI VF device on dest will have identical PCI addresses. By sticking",
            "        # a \"placeholder\" instance on that first PCI VF device on the dest, the",
            "        # incoming instance from source will be forced to consume the second",
            "        # dest PCI VF device, with a different PCI address.",
            "        # We want to test server operations with SRIOV VFs and SRIOV PFs so",
            "        # the config of the compute hosts also have one extra PCI PF devices",
            "        # without any VF children. But the two compute has different PCI PF",
            "        # addresses and MAC so that the test can observe the slot update as",
            "        # well as the MAC updated during migration and after revert.",
            "        source_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        source_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:aa',",
            "        )",
            "        self.start_compute(",
            "            hostname='source',",
            "            pci_info=source_pci_info)",
            "        dest_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        dest_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x6,  # make it different from the source host",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:bb',",
            "        )",
            "        self.start_compute(",
            "            hostname='dest',",
            "            pci_info=dest_pci_info)",
            "        source_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})",
            "        source_pf_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_pf})",
            "        dest_port1 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_2})",
            "        dest_port2 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_3})",
            "        source_server = self._create_server(",
            "            networks=[",
            "                {'port': source_port['port']['id']},",
            "                {'port': source_pf_port['port']['id']}",
            "            ],",
            "            host='source',",
            "        )",
            "        dest_server1 = self._create_server(",
            "            networks=[{'port': dest_port1['port']['id']}], host='dest')",
            "        dest_server2 = self._create_server(",
            "            networks=[{'port': dest_port2['port']['id']}], host='dest')",
            "        # Refresh the ports.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "        dest_port1 = self.neutron.show_port(dest_port1['port']['id'])",
            "        dest_port2 = self.neutron.show_port(dest_port2['port']['id'])",
            "        # Find the server on the dest compute that's using the same pci_slot as",
            "        # the server on the source compute, and delete the other one to make",
            "        # room for the incoming server from the source.",
            "        source_pci_slot = source_port['port']['binding:profile']['pci_slot']",
            "        dest_pci_slot1 = dest_port1['port']['binding:profile']['pci_slot']",
            "        if dest_pci_slot1 == source_pci_slot:",
            "            same_slot_port = dest_port1",
            "            self._delete_server(dest_server2)",
            "        else:",
            "            same_slot_port = dest_port2",
            "            self._delete_server(dest_server1)",
            "        # Before moving, explicitly assert that the servers on source and dest",
            "        # have the same pci_slot in their port's binding profile",
            "        self.assertEqual(source_port['port']['binding:profile']['pci_slot'],",
            "                         same_slot_port['port']['binding:profile']['pci_slot'])",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the source host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:00.0',  # which is in sync with the source host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the source host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:aa',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "        # Before moving, assert that the servers on source and dest have the",
            "        # same PCI source address in their XML for their SRIOV nic.",
            "        source_conn = self.computes['source'].driver._host.get_connection()",
            "        dest_conn = self.computes['source'].driver._host.get_connection()",
            "        source_vms = [vm._def for vm in source_conn._vms.values()]",
            "        dest_vms = [vm._def for vm in dest_conn._vms.values()]",
            "        self.assertEqual(1, len(source_vms))",
            "        self.assertEqual(1, len(dest_vms))",
            "        self.assertEqual(1, len(source_vms[0]['devices']['nics']))",
            "        self.assertEqual(1, len(dest_vms[0]['devices']['nics']))",
            "        self.assertEqual(source_vms[0]['devices']['nics'][0]['source'],",
            "                         dest_vms[0]['devices']['nics'][0]['source'])",
            "",
            "        # TODO(stephenfin): The mock of 'migrate_disk_and_power_off' should",
            "        # probably be less...dumb",
            "        with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                        '.migrate_disk_and_power_off', return_value='{}'):",
            "            self._migrate_server(source_server)",
            "",
            "        # Refresh the ports again, keeping in mind the ports are now bound",
            "        # on the dest after migrating.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        same_slot_port = self.neutron.show_port(same_slot_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "        self.assertNotEqual(",
            "            source_port['port']['binding:profile']['pci_slot'],",
            "            same_slot_port['port']['binding:profile']['pci_slot'])",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the dest host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:06.0',  # which is in sync with the dest host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the dest host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:bb',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "        conn = self.computes['dest'].driver._host.get_connection()",
            "        vms = [vm._def for vm in conn._vms.values()]",
            "        self.assertEqual(2, len(vms))",
            "        for vm in vms:",
            "            self.assertEqual(1, len(vm['devices']['nics']))",
            "        self.assertNotEqual(vms[0]['devices']['nics'][0]['source'],",
            "                            vms[1]['devices']['nics'][0]['source'])",
            "",
            "        self._revert_resize(source_server)",
            "",
            "        # Refresh the ports again, keeping in mind the ports are now bound",
            "        # on the source as the migration is reverted",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the source host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:00.0',  # which is in sync with the source host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the source host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:aa',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "",
            "    def test_evacuate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            # Down the source compute to enable the evacuation",
            "            self.api.put_service(self.computes['source'].service_ref.uuid,",
            "                                 {'forced_down': True})",
            "            self.computes['source'].stop()",
            "            self._evacuate_server(source_server)",
            "        self._test_move_operation_with_neutron(move_operation)",
            "",
            "    def test_live_migrate_server_with_neutron(self):",
            "        \"\"\"Live migrate an instance using a neutron-provisioned SR-IOV VIF.",
            "",
            "        This should succeed since we support this, via detach and attach of the",
            "        PCI device.",
            "        \"\"\"",
            "",
            "        # start two compute services with differing PCI device inventory",
            "        source_pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=2, num_vfs=8, numa_node=0)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        source_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:aa',",
            "        )",
            "        self.start_compute(hostname='test_compute0', pci_info=source_pci_info)",
            "",
            "        dest_pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=2, numa_node=1)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        dest_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x6,  # make it different from the source host",
            "            function=0,",
            "            iommu_group=42,",
            "            # numa node needs to be aligned with the other pci devices in this",
            "            # host as the instance needs to fit into a single host numa node",
            "            numa_node=1,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:bb',",
            "        )",
            "",
            "        self.start_compute(hostname='test_compute1', pci_info=dest_pci_info)",
            "",
            "        # create the ports",
            "        port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})['port']",
            "        pf_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_pf})['port']",
            "",
            "        # create a server using the VF via neutron",
            "        extra_spec = {'hw:cpu_policy': 'dedicated'}",
            "        flavor_id = self._create_flavor(vcpu=4, extra_spec=extra_spec)",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port['id']},",
            "                {'port': pf_port['id']},",
            "            ],",
            "            host='test_compute0',",
            "        )",
            "",
            "        # our source host should have marked two PCI devices as used, the VF",
            "        # and the parent PF, while the future destination is currently unused",
            "        self.assertEqual('test_compute0', server['OS-EXT-SRV-ATTR:host'])",
            "        self.assertPCIDeviceCounts('test_compute0', total=11, free=8)",
            "        self.assertPCIDeviceCounts('test_compute1', total=4, free=4)",
            "",
            "        # the instance should be on host NUMA node 0, since that's where our",
            "        # PCI devices are",
            "        host_numa = objects.NUMATopology.obj_from_db_obj(",
            "            objects.ComputeNode.get_by_nodename(",
            "                self.ctxt, 'test_compute0',",
            "            ).numa_topology",
            "        )",
            "        self.assertEqual({0, 1, 2, 3}, host_numa.cells[0].pinned_cpus)",
            "        self.assertEqual(set(), host_numa.cells[1].pinned_cpus)",
            "",
            "        # ensure the binding details sent to \"neutron\" are correct",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1515',",
            "                # TODO(stephenfin): Stop relying on a side-effect of how nova",
            "                # chooses from multiple PCI devices (apparently the last",
            "                # matching one)",
            "                'pci_slot': '0000:81:01.4',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "        # ensure the binding details sent to \"neutron\" are correct",
            "        pf_port = self.neutron.show_port(pf_port['id'],)['port']",
            "        self.assertIn('binding:profile', pf_port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1528',",
            "                'pci_slot': '0000:82:00.0',",
            "                'physical_network': 'physnet4',",
            "                'device_mac_address': 'b4:96:91:34:f4:aa',",
            "            },",
            "            pf_port['binding:profile'],",
            "        )",
            "",
            "        # now live migrate that server",
            "        self._live_migrate(server, 'completed')",
            "",
            "        # we should now have transitioned our usage to the destination, freeing",
            "        # up the source in the process",
            "        self.assertPCIDeviceCounts('test_compute0', total=11, free=11)",
            "        self.assertPCIDeviceCounts('test_compute1', total=4, free=1)",
            "",
            "        # the instance should now be on host NUMA node 1, since that's where",
            "        # our PCI devices are for this second host",
            "        host_numa = objects.NUMATopology.obj_from_db_obj(",
            "            objects.ComputeNode.get_by_nodename(",
            "                self.ctxt, 'test_compute1',",
            "            ).numa_topology",
            "        )",
            "        self.assertEqual(set(), host_numa.cells[0].pinned_cpus)",
            "        self.assertEqual({4, 5, 6, 7}, host_numa.cells[1].pinned_cpus)",
            "",
            "        # ensure the binding details sent to \"neutron\" have been updated",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1515',",
            "                'pci_slot': '0000:81:00.2',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "        # ensure the binding details sent to \"neutron\" are correct",
            "        pf_port = self.neutron.show_port(pf_port['id'],)['port']",
            "        self.assertIn('binding:profile', pf_port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1528',",
            "                'pci_slot': '0000:82:06.0',",
            "                'physical_network': 'physnet4',",
            "                'device_mac_address': 'b4:96:91:34:f4:bb',",
            "            },",
            "            pf_port['binding:profile'],",
            "        )",
            "",
            "    def test_get_server_diagnostics_server_with_VF(self):",
            "        \"\"\"Ensure server disagnostics include info on VF-type PCI devices.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo()",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create the SR-IOV port",
            "        port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})",
            "",
            "        flavor_id = self._create_flavor()",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'uuid': base.LibvirtNeutronFixture.network_1['id']},",
            "                {'port': port['port']['id']},",
            "            ],",
            "        )",
            "",
            "        # now check the server diagnostics to ensure the VF-type PCI device is",
            "        # attached",
            "        diagnostics = self.api.get_server_diagnostics(",
            "            server['id']",
            "        )",
            "",
            "        self.assertEqual(",
            "            base.LibvirtNeutronFixture.network_1_port_2['mac_address'],",
            "            diagnostics['nic_details'][0]['mac_address'],",
            "        )",
            "",
            "        for key in ('rx_packets', 'tx_packets'):",
            "            self.assertIn(key, diagnostics['nic_details'][0])",
            "",
            "        self.assertEqual(",
            "            base.LibvirtNeutronFixture.network_4_port_1['mac_address'],",
            "            diagnostics['nic_details'][1]['mac_address'],",
            "        )",
            "        for key in ('rx_packets', 'tx_packets'):",
            "            self.assertIn(key, diagnostics['nic_details'][1])",
            "",
            "    def test_create_server_after_change_in_nonsriov_pf_to_sriov_pf(self):",
            "        # Starts a compute with PF not configured with SRIOV capabilities",
            "        # Updates the PF with SRIOV capability and restart the compute service",
            "        # Then starts a VM with the sriov port. The VM should be in active",
            "        # state with sriov port attached.",
            "",
            "        # To emulate the device type changing, we first create a",
            "        # HostPCIDevicesInfo object with PFs and VFs. Then we make a copy",
            "        # and remove the VFs and the virt_function capability. This is",
            "        # done to ensure the physical function product id is same in both",
            "        # the versions.",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        pci_info_no_sriov = copy.deepcopy(pci_info)",
            "",
            "        # Disable SRIOV capabilties in PF and delete the VFs",
            "        self._disable_sriov_in_pf(pci_info_no_sriov)",
            "",
            "        self.start_compute('test_compute0', pci_info=pci_info_no_sriov)",
            "        self.compute = self.computes['test_compute0']",
            "",
            "        ctxt = context.get_admin_context()",
            "        pci_devices = objects.PciDeviceList.get_by_compute_node(",
            "            ctxt,",
            "            objects.ComputeNode.get_by_nodename(",
            "                ctxt, 'test_compute0',",
            "            ).id,",
            "        )",
            "        self.assertEqual(1, len(pci_devices))",
            "        self.assertEqual('type-PCI', pci_devices[0].dev_type)",
            "",
            "        # Restart the compute service with sriov PFs",
            "        self.restart_compute_service(",
            "            self.compute.host, pci_info=pci_info, keep_hypervisor_state=False)",
            "",
            "        # Verify if PCI devices are of type type-PF or type-VF",
            "        pci_devices = objects.PciDeviceList.get_by_compute_node(",
            "            ctxt,",
            "            objects.ComputeNode.get_by_nodename(",
            "                ctxt, 'test_compute0',",
            "            ).id,",
            "        )",
            "        for pci_device in pci_devices:",
            "            self.assertIn(pci_device.dev_type, ['type-PF', 'type-VF'])",
            "",
            "        # create the port",
            "        self.neutron.create_port({'port': self.neutron.network_4_port_1})",
            "",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            networks=[",
            "                {'port': base.LibvirtNeutronFixture.network_4_port_1['id']},",
            "            ],",
            "        )",
            "",
            "",
            "class SRIOVAttachDetachTest(_PCIServersTestBase):",
            "    # no need for aliases as these test will request SRIOV via neutron",
            "    PCI_ALIAS = []",
            "",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            \"physical_network\": \"physnet2\",",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            \"physical_network\": \"physnet2\",",
            "        },",
            "    )]",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        self.neutron = self.useFixture(nova_fixtures.NeutronFixture(self))",
            "",
            "        # add extra ports and the related network to the neutron fixture",
            "        # specifically for these tests. It cannot be added globally in the",
            "        # fixture init as it adds a second network that makes auto allocation",
            "        # based test to fail due to ambiguous networks.",
            "        self.neutron._networks[",
            "            self.neutron.network_2['id']] = self.neutron.network_2",
            "        self.neutron._subnets[",
            "            self.neutron.subnet_2['id']] = self.neutron.subnet_2",
            "        for port in [self.neutron.sriov_port, self.neutron.sriov_port2,",
            "                     self.neutron.sriov_pf_port, self.neutron.sriov_pf_port2,",
            "                     self.neutron.macvtap_port, self.neutron.macvtap_port2]:",
            "            self.neutron._ports[port['id']] = copy.deepcopy(port)",
            "",
            "    def _get_attached_port_ids(self, instance_uuid):",
            "        return [",
            "            attachment['port_id']",
            "            for attachment in self.api.get_port_interfaces(instance_uuid)]",
            "",
            "    def _detach_port(self, instance_uuid, port_id):",
            "        self.api.detach_interface(instance_uuid, port_id)",
            "        self.notifier.wait_for_versioned_notifications(",
            "            'instance.interface_detach.end')",
            "",
            "    def _attach_port(self, instance_uuid, port_id):",
            "        self.api.attach_interface(",
            "            instance_uuid,",
            "            {'interfaceAttachment': {'port_id': port_id}})",
            "        self.notifier.wait_for_versioned_notifications(",
            "            'instance.interface_attach.end')",
            "",
            "    def _test_detach_attach(self, first_port_id, second_port_id):",
            "        # This test takes two ports that requires PCI claim.",
            "        # Starts a compute with one PF and one connected VF.",
            "        # Then starts a VM with the first port. Then detach it, then",
            "        # re-attach it. These expected to be successful. Then try to attach the",
            "        # second port and asserts that it fails as no free PCI device left on",
            "        # the host.",
            "        host_info = fakelibvirt.HostInfo(cpu_nodes=2, cpu_sockets=1,",
            "                                         cpu_cores=2, cpu_threads=2)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        self.start_compute(",
            "            'test_compute0', host_info=host_info, pci_info=pci_info)",
            "        self.compute = self.computes['test_compute0']",
            "",
            "        # Create server with a port",
            "        server = self._create_server(networks=[{'port': first_port_id}])",
            "",
            "        updated_port = self.neutron.show_port(first_port_id)['port']",
            "        self.assertEqual('test_compute0', updated_port['binding:host_id'])",
            "        self.assertIn(first_port_id, self._get_attached_port_ids(server['id']))",
            "",
            "        self._detach_port(server['id'], first_port_id)",
            "",
            "        updated_port = self.neutron.show_port(first_port_id)['port']",
            "        self.assertIsNone(updated_port['binding:host_id'])",
            "        self.assertNotIn(",
            "            first_port_id,",
            "            self._get_attached_port_ids(server['id']))",
            "",
            "        # Attach back the port",
            "        self._attach_port(server['id'], first_port_id)",
            "",
            "        updated_port = self.neutron.show_port(first_port_id)['port']",
            "        self.assertEqual('test_compute0', updated_port['binding:host_id'])",
            "        self.assertIn(first_port_id, self._get_attached_port_ids(server['id']))",
            "",
            "        # Try to attach the second port but no free PCI device left",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException, self._attach_port, server['id'],",
            "            second_port_id)",
            "",
            "        self.assertEqual(400, ex.response.status_code)",
            "        self.assertIn('Failed to claim PCI device', str(ex))",
            "        attached_ports = self._get_attached_port_ids(server['id'])",
            "        self.assertIn(first_port_id, attached_ports)",
            "        self.assertNotIn(second_port_id, attached_ports)",
            "",
            "    def test_detach_attach_direct(self):",
            "        self._test_detach_attach(",
            "            self.neutron.sriov_port['id'], self.neutron.sriov_port2['id'])",
            "",
            "    def test_detach_macvtap(self):",
            "        self._test_detach_attach(",
            "            self.neutron.macvtap_port['id'],",
            "            self.neutron.macvtap_port2['id'])",
            "",
            "    def test_detach_direct_physical(self):",
            "        self._test_detach_attach(",
            "            self.neutron.sriov_pf_port['id'],",
            "            self.neutron.sriov_pf_port2['id'])",
            "",
            "",
            "class VDPAServersTest(_PCIServersWithMigrationTestBase):",
            "",
            "    # this is needed for os_compute_api:os-migrate-server:migrate policy",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    # Whitelist both the PF and VF; in reality, you probably wouldn't do this",
            "    # but we want to make sure that the PF is correctly taken off the table",
            "    # once any VF is used",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '101d',",
            "            'physical_network': 'physnet4',",
            "        },",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '101e',",
            "            'physical_network': 'physnet4',",
            "        },",
            "    )]",
            "    # No need for aliases as these test will request SRIOV via neutron",
            "    PCI_ALIAS = []",
            "",
            "    NUM_PFS = 1",
            "    NUM_VFS = 4",
            "",
            "    FAKE_LIBVIRT_VERSION = 6_009_000  # 6.9.0",
            "    FAKE_QEMU_VERSION = 5_001_000  # 5.1.0",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        # The ultimate base class _IntegratedTestBase uses NeutronFixture but",
            "        # we need a bit more intelligent neutron for these tests. Applying the",
            "        # new fixture here means that we re-stub what the previous neutron",
            "        # fixture already stubbed.",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "",
            "    def start_vdpa_compute(self, hostname='compute-0'):",
            "        vf_ratio = self.NUM_VFS // self.NUM_PFS",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pci=0, num_pfs=0, num_vfs=0)",
            "        vdpa_info = fakelibvirt.HostVDPADevicesInfo()",
            "",
            "        pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x6,",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=40,  # totally arbitrary number",
            "            numa_node=0,",
            "            vf_ratio=vf_ratio,",
            "            vend_id='15b3',",
            "            vend_name='Mellanox Technologies',",
            "            prod_id='101d',",
            "            prod_name='MT2892 Family [ConnectX-6 Dx]',",
            "            driver_name='mlx5_core')",
            "",
            "        for idx in range(self.NUM_VFS):",
            "            vf = pci_info.add_device(",
            "                dev_type='VF',",
            "                bus=0x6,",
            "                slot=0x0,",
            "                function=idx + 1,",
            "                iommu_group=idx + 41,  # totally arbitrary number + offset",
            "                numa_node=0,",
            "                vf_ratio=vf_ratio,",
            "                parent=(0x6, 0x0, 0),",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='101e',",
            "                prod_name='ConnectX Family mlx5Gen Virtual Function',",
            "                driver_name='mlx5_core')",
            "            vdpa_info.add_device(f'vdpa_vdpa{idx}', idx, vf)",
            "",
            "        return super().start_compute(hostname=hostname,",
            "            pci_info=pci_info, vdpa_info=vdpa_info,",
            "            libvirt_version=self.FAKE_LIBVIRT_VERSION,",
            "            qemu_version=self.FAKE_QEMU_VERSION)",
            "",
            "    def create_vdpa_port(self):",
            "        vdpa_port = {",
            "            'id': uuids.vdpa_port,",
            "            'network_id': self.neutron.network_4['id'],",
            "            'status': 'ACTIVE',",
            "            'mac_address': 'b5:bc:2e:e7:51:ee',",
            "            'fixed_ips': [",
            "                {",
            "                    'ip_address': '192.168.4.6',",
            "                    'subnet_id': self.neutron.subnet_4['id']",
            "                }",
            "            ],",
            "            'binding:vif_details': {},",
            "            'binding:vif_type': 'ovs',",
            "            'binding:vnic_type': 'vdpa',",
            "        }",
            "",
            "        # create the port",
            "        self.neutron.create_port({'port': vdpa_port})",
            "        return vdpa_port",
            "",
            "    def test_create_server(self):",
            "        \"\"\"Create an instance using a neutron-provisioned vDPA VIF.\"\"\"",
            "",
            "        orig_create = nova.virt.libvirt.guest.Guest.create",
            "",
            "        def fake_create(cls, xml, host):",
            "            tree = etree.fromstring(xml)",
            "            elem = tree.find('./devices/interface/[@type=\"vdpa\"]')",
            "",
            "            # compare source device",
            "            # the MAC address is derived from the neutron port, while the",
            "            # source dev path assumes we attach vDPA devs in order",
            "            expected = \"\"\"",
            "                <interface type=\"vdpa\">",
            "                  <mac address=\"b5:bc:2e:e7:51:ee\"/>",
            "                  <source dev=\"/dev/vhost-vdpa-3\"/>",
            "                </interface>\"\"\"",
            "            actual = etree.tostring(elem, encoding='unicode')",
            "",
            "            self.assertXmlEqual(expected, actual)",
            "",
            "            return orig_create(xml, host)",
            "",
            "        self.stub_out(",
            "            'nova.virt.libvirt.guest.Guest.create',",
            "            fake_create,",
            "        )",
            "",
            "        hostname = self.start_vdpa_compute()",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "",
            "        # both the PF and VF with vDPA capabilities (dev_type=vdpa) should have",
            "        # been counted",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        # create the port",
            "        vdpa_port = self.create_vdpa_port()",
            "",
            "        # ensure the binding details are currently unset",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        # create a server using the vDPA device via neutron",
            "        self._create_server(networks=[{'port': vdpa_port['id']}])",
            "",
            "        # ensure there is one less VF available and that the PF is no longer",
            "        # usable",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:06:00.4',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def _create_port_and_server(self):",
            "        # create the port and a server, with the port attached to the server",
            "        vdpa_port = self.create_vdpa_port()",
            "        server = self._create_server(networks=[{'port': vdpa_port['id']}])",
            "        return vdpa_port, server",
            "",
            "    def _test_common(self, op, *args, **kwargs):",
            "        self.start_vdpa_compute()",
            "",
            "        vdpa_port, server = self._create_port_and_server()",
            "",
            "        # attempt the unsupported action and ensure it fails",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            op, server, *args, **kwargs)",
            "        self.assertIn(",
            "            'not supported for instance with vDPA ports',",
            "            ex.response.text)",
            "",
            "    def test_attach_interface_service_version_61(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=61",
            "        ):",
            "            self._test_common(self._attach_interface, uuids.vdpa_port)",
            "",
            "    def test_attach_interface(self):",
            "        hostname = self.start_vdpa_compute()",
            "        # create the port and a server, but don't attach the port to the server",
            "        # yet",
            "        server = self._create_server(networks='none')",
            "        vdpa_port = self.create_vdpa_port()",
            "        # attempt to attach the port to the server",
            "        self._attach_interface(server, vdpa_port['id'])",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:06:00.4',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "        self.assertEqual(server['id'], port['device_id'])",
            "",
            "    def test_detach_interface_service_version_61(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=61",
            "        ):",
            "            self._test_common(self._detach_interface, uuids.vdpa_port)",
            "",
            "    def test_detach_interface(self):",
            "        self.start_vdpa_compute()",
            "        vdpa_port, server = self._create_port_and_server()",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(server['id'], port['device_id'])",
            "        self._detach_interface(server, vdpa_port['id'])",
            "        # ensure the port is no longer owned by the vm",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual('', port['device_id'])",
            "        self.assertEqual({}, port['binding:profile'])",
            "",
            "    def test_shelve_offload(self):",
            "        hostname = self.start_vdpa_compute()",
            "        vdpa_port, server = self._create_port_and_server()",
            "        # assert the port is bound to the vm and the compute host",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(server['id'], port['device_id'])",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        # -2 we claim the vdpa device which make the parent PF unavailable",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "        server = self._shelve_server(server)",
            "        # now that the vm is shelve offloaded it should not be bound",
            "        # to any host but should still be owned by the vm",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(server['id'], port['device_id'])",
            "        # FIXME(sean-k-mooney): we should be unbinding the port from",
            "        # the host when we shelve offload but we don't today.",
            "        # This is unrelated to vdpa port and is a general issue.",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "        self.assertIn('binding:profile', port)",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "    def test_unshelve_to_same_host(self):",
            "        hostname = self.start_vdpa_compute()",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            hostname, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "",
            "        server = self._shelve_server(server)",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        # FIXME(sean-k-mooney): shelve  offload should unbind the port",
            "        # self.assertEqual('', port['binding:host_id'])",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "",
            "        server = self._unshelve_server(server)",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            hostname, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "",
            "    def test_unshelve_to_different_host(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(source, port['binding:host_id'])",
            "",
            "        server = self._shelve_server(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        # FIXME(sean-k-mooney): shelve should unbind the port",
            "        # self.assertEqual('', port['binding:host_id'])",
            "        self.assertEqual(source, port['binding:host_id'])",
            "",
            "        # force the unshelve to the other host",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'status': 'disabled'})",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "        server = self._unshelve_server(server)",
            "        # the dest devices should be claimed",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "        # and the source host devices should still be free",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertEqual(",
            "            dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(dest, port['binding:host_id'])",
            "",
            "    def test_evacute(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(source, port['binding:host_id'])",
            "",
            "        # stop the source compute and enable the dest",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        self.computes['source'].stop()",
            "        # Down the source compute to enable the evacuation",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'forced_down': True})",
            "",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "        server = self._evacuate_server(server)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(dest, port['binding:host_id'])",
            "",
            "        # as the source compute is offline the pci claims will not be cleaned",
            "        # up on the source compute.",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        # but if you fix/restart the source node the allocations for evacuated",
            "        # instances should be released.",
            "        self.restart_compute_service(source)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "",
            "    def test_resize_same_host(self):",
            "        self.flags(allow_resize_to_same_host=True)",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        source = self.start_vdpa_compute()",
            "        vdpa_port, server = self._create_port_and_server()",
            "        # before we resize the vm should be using 1 VF but that will mark",
            "        # the PF as unavailable so we assert 2 devices are in use.",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        flavor_id = self._create_flavor(name='new-flavor')",
            "        self.assertNotEqual(server['flavor']['original_name'], 'new-flavor')",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._resize_server(server, flavor_id)",
            "            self.assertEqual(",
            "                server['flavor']['original_name'], 'new-flavor')",
            "            # in resize verify the VF claims should be doubled even",
            "            # for same host resize so assert that 3 are in devices in use",
            "            # 1 PF and 2 VFs .",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 3)",
            "            server = self._confirm_resize(server)",
            "            # but once we confrim it should be reduced back to 1 PF and 1 VF",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            # assert the hostname has not have changed as part",
            "            # of the resize.",
            "            self.assertEqual(",
            "                source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_resize_different_host(self):",
            "        self.flags(allow_resize_to_same_host=False)",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        flavor_id = self._create_flavor(name='new-flavor')",
            "        self.assertNotEqual(server['flavor']['original_name'], 'new-flavor')",
            "        # disable the source compute and enable the dest",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'status': 'disabled'})",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._resize_server(server, flavor_id)",
            "            self.assertEqual(",
            "                server['flavor']['original_name'], 'new-flavor')",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            server = self._confirm_resize(server)",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            self.assertEqual(",
            "                dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_resize_revert(self):",
            "        self.flags(allow_resize_to_same_host=False)",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        flavor_id = self._create_flavor(name='new-flavor')",
            "        self.assertNotEqual(server['flavor']['original_name'], 'new-flavor')",
            "        # disable the source compute and enable the dest",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'status': 'disabled'})",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._resize_server(server, flavor_id)",
            "            self.assertEqual(",
            "                server['flavor']['original_name'], 'new-flavor')",
            "            # in resize verify both the dest and source pci claims should be",
            "            # present.",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            server = self._revert_resize(server)",
            "            # but once we revert the dest claims should be freed.",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertEqual(",
            "                source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_cold_migrate(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        # enable the dest we do not need to disable the source since cold",
            "        # migrate wont happen to the same host in the libvirt driver",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._migrate_server(server)",
            "            self.assertEqual(",
            "                dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            server = self._confirm_resize(server)",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            self.assertEqual(",
            "                dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_suspend_and_resume_service_version_62(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=62",
            "        ):",
            "            self._test_common(self._suspend_server)",
            "",
            "    def test_suspend_and_resume(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        vdpa_port, server = self._create_port_and_server()",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        server = self._suspend_server(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual('SUSPENDED', server['status'])",
            "        server = self._resume_server(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual('ACTIVE', server['status'])",
            "",
            "    def test_live_migrate_service_version_62(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=62",
            "        ):",
            "            self._test_common(self._live_migrate)",
            "",
            "    def test_live_migrate(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        # enable the dest we do not need to disable the source since cold",
            "        # migrate wont happen to the same host in the libvirt driver",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "",
            "        with mock.patch(",
            "                'nova.virt.libvirt.LibvirtDriver.'",
            "                '_detach_direct_passthrough_vifs'",
            "        ):",
            "            server = self._live_migrate(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "",
            "class PCIServersTest(_PCIServersTestBase):",
            "",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "        }",
            "    )]",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "        }",
            "    )]",
            "    PCI_RC = f\"CUSTOM_PCI_{fakelibvirt.PCI_VEND_ID}_{fakelibvirt.PCI_PROD_ID}\"",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.flags(group=\"pci\", report_in_placement=True)",
            "",
            "    def test_create_server_with_pci_dev_and_numa(self):",
            "        \"\"\"Verifies that an instance can be booted with cpu pinning and with an",
            "           assigned pci device.",
            "        \"\"\"",
            "",
            "        self.flags(cpu_dedicated_set='0-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=1)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        self.assert_placement_pci_view(",
            "            \"compute1\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        # create a flavor",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "    def test_create_server_with_pci_dev_and_numa_fails(self):",
            "        \"\"\"This test ensures that it is not possible to allocated CPU and",
            "           memory resources from one NUMA node and a PCI device from another.",
            "        \"\"\"",
            "",
            "        self.flags(cpu_dedicated_set='0-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=0)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        self.assert_placement_pci_view(",
            "            \"compute1\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        # boot one instance with no PCI device to \"fill up\" NUMA node 0",
            "        extra_spec = {'hw:cpu_policy': 'dedicated'}",
            "        flavor_id = self._create_flavor(vcpu=4, extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now boot one with a PCI device, which should fail to boot",
            "        extra_spec['pci_passthrough:alias'] = '%s:1' % self.ALIAS_NAME",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(",
            "            flavor_id=flavor_id, networks='none', expected_state='ERROR')",
            "",
            "    def test_live_migrate_server_with_pci(self):",
            "        \"\"\"Live migrate an instance with a PCI passthrough device.",
            "",
            "        This should fail because it's not possible to live migrate an instance",
            "        with a PCI passthrough device, even if it's a SR-IOV VF.",
            "        \"\"\"",
            "",
            "        # start two compute services",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "",
            "        self.assert_placement_pci_view(",
            "            \"test_compute0\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        self.start_compute(",
            "            hostname='test_compute1',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "",
            "        self.assert_placement_pci_view(",
            "            \"test_compute1\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        # create a server",
            "        extra_spec = {'pci_passthrough:alias': f'{self.ALIAS_NAME}:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now live migrate that server",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            self._live_migrate,",
            "            server, 'completed')",
            "        # NOTE(stephenfin): this wouldn't happen in a real deployment since",
            "        # live migration is a cast, but since we are using CastAsCallFixture",
            "        # this will bubble to the API",
            "        self.assertEqual(500, ex.response.status_code)",
            "        self.assertIn('NoValidHost', str(ex))",
            "",
            "    def test_resize_pci_to_vanilla(self):",
            "        # Start two computes, one with PCI and one without.",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "        self.assert_placement_pci_view(",
            "            \"test_compute0\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "        self.start_compute(hostname='test_compute1')",
            "        self.assert_placement_pci_view(",
            "            \"test_compute1\",",
            "            inventories={},",
            "            traits={},",
            "        )",
            "",
            "        # Boot a server with a single PCI device.",
            "        extra_spec = {'pci_passthrough:alias': f'{self.ALIAS_NAME}:1'}",
            "        pci_flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=pci_flavor_id, networks='none')",
            "",
            "        # Resize it to a flavor without PCI devices. We expect this to work, as",
            "        # test_compute1 is available.",
            "        flavor_id = self._create_flavor()",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off',",
            "            return_value='{}',",
            "        ):",
            "            self._resize_server(server, flavor_id)",
            "        self._confirm_resize(server)",
            "        self.assertPCIDeviceCounts('test_compute0', total=1, free=1)",
            "        self.assertPCIDeviceCounts('test_compute1', total=0, free=0)",
            "",
            "    def _confirm_resize(self, server, host='host1'):",
            "        # NOTE(sbauza): Unfortunately, _cleanup_resize() in libvirt checks the",
            "        # host option to know the source hostname but given we have a global",
            "        # CONF, the value will be the hostname of the last compute service that",
            "        # was created, so we need to change it here.",
            "        # TODO(sbauza): Remove the below once we stop using CONF.host in",
            "        # libvirt and rather looking at the compute host value.",
            "        orig_host = CONF.host",
            "        self.flags(host=host)",
            "        super()._confirm_resize(server)",
            "        self.flags(host=orig_host)",
            "",
            "    def test_cold_migrate_server_with_pci(self):",
            "",
            "        host_devices = {}",
            "        orig_create = nova.virt.libvirt.guest.Guest.create",
            "",
            "        def fake_create(cls, xml, host):",
            "            tree = etree.fromstring(xml)",
            "            elem = tree.find('./devices/hostdev/source/address')",
            "",
            "            hostname = host.get_hostname()",
            "            address = (",
            "                elem.get('bus'), elem.get('slot'), elem.get('function'),",
            "            )",
            "            if hostname in host_devices:",
            "                self.assertNotIn(address, host_devices[hostname])",
            "            else:",
            "                host_devices[hostname] = []",
            "            host_devices[host.get_hostname()].append(address)",
            "",
            "            return orig_create(xml, host)",
            "",
            "        self.stub_out(",
            "            'nova.virt.libvirt.guest.Guest.create',",
            "            fake_create,",
            "        )",
            "",
            "        # start two compute services",
            "        for hostname in ('test_compute0', 'test_compute1'):",
            "            pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=2)",
            "            self.start_compute(hostname=hostname, pci_info=pci_info)",
            "            self.assert_placement_pci_view(",
            "                hostname,",
            "                inventories={",
            "                    \"0000:81:00.0\": {self.PCI_RC: 1},",
            "                    \"0000:81:01.0\": {self.PCI_RC: 1},",
            "                },",
            "                traits={",
            "                    \"0000:81:00.0\": [],",
            "                    \"0000:81:01.0\": [],",
            "                },",
            "            )",
            "",
            "        # boot an instance with a PCI device on each host",
            "        extra_spec = {",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "",
            "        server_a = self._create_server(",
            "            flavor_id=flavor_id, networks='none', host='test_compute0')",
            "        server_b = self._create_server(",
            "            flavor_id=flavor_id, networks='none', host='test_compute1')",
            "",
            "        # the instances should have landed on separate hosts; ensure both hosts",
            "        # have one used PCI device and one free PCI device",
            "        self.assertNotEqual(",
            "            server_a['OS-EXT-SRV-ATTR:host'], server_b['OS-EXT-SRV-ATTR:host'],",
            "        )",
            "        for hostname in ('test_compute0', 'test_compute1'):",
            "            self.assertPCIDeviceCounts(hostname, total=2, free=1)",
            "",
            "        # TODO(stephenfin): The mock of 'migrate_disk_and_power_off' should",
            "        # probably be less...dumb",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            # TODO(stephenfin): Use a helper",
            "            self.api.post_server_action(server_a['id'], {'migrate': None})",
            "            server_a = self._wait_for_state_change(server_a, 'VERIFY_RESIZE')",
            "",
            "        # the instances should now be on the same host; ensure the source host",
            "        # still has one used PCI device while the destination now has two used",
            "        # test_compute0 initially",
            "        self.assertEqual(",
            "            server_a['OS-EXT-SRV-ATTR:host'], server_b['OS-EXT-SRV-ATTR:host'],",
            "        )",
            "        self.assertPCIDeviceCounts('test_compute0', total=2, free=1)",
            "        self.assertPCIDeviceCounts('test_compute1', total=2, free=0)",
            "",
            "        # now, confirm the migration and check our counts once again",
            "        self._confirm_resize(server_a)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=2, free=2)",
            "        self.assertPCIDeviceCounts('test_compute1', total=2, free=0)",
            "",
            "    def test_request_two_pci_but_host_has_one(self):",
            "        # simulate a single type-PCI device on the host",
            "        self.start_compute(pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "        self.assertPCIDeviceCounts('compute1', total=1, free=1)",
            "",
            "        alias = [jsonutils.dumps(x) for x in (",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': 'a1',",
            "            },",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': 'a2',",
            "            },",
            "        )]",
            "        self.flags(group='pci', alias=alias)",
            "        # request two PCI devices both are individually matching with the",
            "        # single available device on the host",
            "        extra_spec = {'pci_passthrough:alias': 'a1:1,a2:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        # so we expect that the boot fails with no valid host error as only",
            "        # one of the requested PCI device can be allocated",
            "        server = self._create_server(",
            "            flavor_id=flavor_id, networks=\"none\", expected_state='ERROR')",
            "        self.assertIn('fault', server)",
            "        self.assertIn('No valid host', server['fault']['message'])",
            "",
            "",
            "class PCIServersWithPreferredNUMATest(_PCIServersTestBase):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "        }",
            "    )]",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.PREFERRED,",
            "        }",
            "    )]",
            "    expected_state = 'ACTIVE'",
            "",
            "    def test_create_server_with_pci_dev_and_numa(self):",
            "        \"\"\"Validate behavior of 'preferred' PCI NUMA policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if* PCI",
            "        NUMA policies are in use.",
            "        \"\"\"",
            "",
            "        self.flags(cpu_dedicated_set='0-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=0)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # boot one instance with no PCI device to \"fill up\" NUMA node 0",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(vcpu=4, extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id)",
            "",
            "        # now boot one with a PCI device, which should succeed thanks to the",
            "        # use of the PCI policy",
            "        extra_spec['pci_passthrough:alias'] = '%s:1' % self.ALIAS_NAME",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(",
            "            flavor_id=flavor_id, expected_state=self.expected_state)",
            "",
            "",
            "class PCIServersWithRequiredNUMATest(PCIServersWithPreferredNUMATest):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.REQUIRED,",
            "        }",
            "    )]",
            "    expected_state = 'ERROR'",
            "",
            "",
            "@ddt.ddt",
            "class PCIServersWithSRIOVAffinityPoliciesTest(_PCIServersTestBase):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "        }",
            "    )]",
            "    # we set the numa_affinity policy to required to ensure strict affinity",
            "    # between pci devices and the guest cpu and memory will be enforced.",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.REQUIRED,",
            "        }",
            "    )]",
            "",
            "    # NOTE(sean-k-mooney): i could just apply the ddt decorators",
            "    # to this function for the most part but i have chosen to",
            "    # keep one top level function per policy to make documenting",
            "    # the test cases simpler.",
            "    def _test_policy(self, pci_numa_node, status, policy):",
            "        # only allow cpus on numa node 1 to be used for pinning",
            "        self.flags(cpu_dedicated_set='4-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pci=1, numa_node=pci_numa_node)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # request cpu pinning to create a numa topology and allow the test to",
            "        # force which numa node the vm would have to be pinned too.",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': policy",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, expected_state=status)",
            "",
            "        if status == 'ACTIVE':",
            "            self.assertTrue(self.mock_filter.called)",
            "        else:",
            "            # the PciPassthroughFilter should not have been called, since the",
            "            # NUMATopologyFilter should have eliminated the filter first",
            "            self.assertFalse(self.mock_filter.called)",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the preferred policy should always pass regardless of numa affinity",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ACTIVE'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_preferred(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'preferred' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the SR-IOV NUMA affinity policy is set to preferred.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'preferred')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the legacy policy allow a PCI device to be used if it has NUMA",
            "    # affinity or if no NUMA info is available so we set the NUMA",
            "    # node for this device to -1 which is the sentinel value use by the",
            "    # Linux kernel for a device with no NUMA affinity.",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_legacy(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'legacy' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the SR-IOV NUMA affinity policy is set to legacy and the device",
            "        does not report NUMA information.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'legacy')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # The required policy requires a PCI device to both report a NUMA",
            "    # and for the guest cpus and ram to be affinitized to the same",
            "    # NUMA node so we create 1 pci device in the first NUMA node.",
            "    @ddt.data((-1, 'ERROR'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_required(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'required' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is not* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the SR-IOV NUMA affinity policy is set to required and the device",
            "        does reports NUMA information.",
            "        \"\"\"",
            "",
            "        # we set the numa_affinity policy to preferred to allow the PCI device",
            "        # to be selected from any numa node so we can prove the flavor",
            "        # overrides the alias.",
            "        alias = [jsonutils.dumps(",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': self.ALIAS_NAME,",
            "                'device_type': fields.PciDeviceType.STANDARD,",
            "                'numa_policy': fields.PCINUMAAffinityPolicy.PREFERRED,",
            "            }",
            "        )]",
            "",
            "        self.flags(",
            "            device_spec=self.PCI_DEVICE_SPEC,",
            "            alias=alias,",
            "            group='pci'",
            "        )",
            "",
            "        self._test_policy(pci_numa_node, status, 'required')",
            "",
            "    def test_socket_policy_pass(self):",
            "        # With 1 socket containing 2 NUMA nodes, make the first node's CPU",
            "        # available for pinning, but affine the PCI device to the second node.",
            "        # This should pass.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=1, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=1)",
            "",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': 'socket'",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id)",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_socket_policy_fail(self):",
            "        # With 2 sockets containing 1 NUMA node each, make the first socket's",
            "        # CPUs available for pinning, but affine the PCI device to the second",
            "        # NUMA node in the second socket. This should fail.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=1, cpu_sockets=2, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=1)",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': 'socket'",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(",
            "            flavor_id=flavor_id, expected_state='ERROR')",
            "        self.assertIn('fault', server)",
            "        self.assertIn('No valid host', server['fault']['message'])",
            "",
            "    def test_socket_policy_multi_numa_pass(self):",
            "        # 2 sockets, 2 NUMA nodes each, with the PCI device on NUMA 0 and",
            "        # socket 0. If we restrict cpu_dedicated_set to NUMA 1, 2 and 3, we",
            "        # should still be able to boot an instance with hw:numa_nodes=3 and the",
            "        # `socket` policy, because one of the instance's NUMA nodes will be on",
            "        # the same socket as the PCI device (even if there is no direct NUMA",
            "        # node affinity).",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=2, cpu_cores=2, cpu_threads=1,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        self.flags(cpu_dedicated_set='2-7', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=0)",
            "",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:numa_nodes': '3',",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': 'socket'",
            "        }",
            "        flavor_id = self._create_flavor(vcpu=6, memory_mb=3144,",
            "                                        extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id)",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "",
            "@ddt.ddt",
            "class PCIServersWithPortNUMAPoliciesTest(_PCIServersTestBase):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "    )]",
            "    # we set the numa_affinity policy to required to ensure strict affinity",
            "    # between pci devices and the guest cpu and memory will be enforced.",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.REQUIRED,",
            "        }",
            "    )]",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        # The ultimate base class _IntegratedTestBase uses NeutronFixture but",
            "        # we need a bit more intelligent neutron for these tests. Applying the",
            "        # new fixture here means that we re-stub what the previous neutron",
            "        # fixture already stubbed.",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "        self.flags(disable_fallback_pcpu_query=True, group='workarounds')",
            "",
            "    def _create_port_with_policy(self, policy):",
            "        port_data = copy.deepcopy(",
            "            base.LibvirtNeutronFixture.network_4_port_1)",
            "        port_data[constants.NUMA_POLICY] = policy",
            "        # create the port",
            "        new_port = self.neutron.create_port({'port': port_data})",
            "        port_id = new_port['port']['id']",
            "        port = self.neutron.show_port(port_id)['port']",
            "        self.assertEqual(port[constants.NUMA_POLICY], policy)",
            "        return port_id",
            "",
            "    # NOTE(sean-k-mooney): i could just apply the ddt decorators",
            "    # to this function for the most part but i have chosen to",
            "    # keep one top level function per policy to make documenting",
            "    # the test cases simpler.",
            "    def _test_policy(self, pci_numa_node, status, policy):",
            "        # only allow cpus on numa node 1 to be used for pinning",
            "        self.flags(cpu_dedicated_set='4-7', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=2, numa_node=pci_numa_node)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # request cpu pinning to create a numa topology and allow the test to",
            "        # force which numa node the vm would have to be pinned too.",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "",
            "        port_id = self._create_port_with_policy(policy)",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "            expected_state=status",
            "        )",
            "",
            "        if status == 'ACTIVE':",
            "            self.assertTrue(self.mock_filter.called)",
            "        else:",
            "            # the PciPassthroughFilter should not have been called, since the",
            "            # NUMATopologyFilter should have eliminated the filter first",
            "            self.assertFalse(self.mock_filter.called)",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the preferred policy should always pass regardless of numa affinity",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ACTIVE'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_preferred(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'preferred' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the port NUMA affinity policy is set to preferred.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'preferred')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the legacy policy allow a PCI device to be used if it has NUMA",
            "    # affinity or if no NUMA info is available so we set the NUMA",
            "    # node for this device to -1 which is the sentinel value use by the",
            "    # Linux kernel for a device with no NUMA affinity.",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_legacy(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'legacy' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the port NUMA affinity policy is set to legacy and the device",
            "        does not report NUMA information.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'legacy')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # The required policy requires a PCI device to both report a NUMA",
            "    # and for the guest cpus and ram to be affinitized to the same",
            "    # NUMA node so we create 1 pci device in the first NUMA node.",
            "    @ddt.data((-1, 'ERROR'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_required(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'required' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is not* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the port NUMA affinity policy is set to required and the device",
            "        does reports NUMA information.",
            "        \"\"\"",
            "",
            "        # we set the numa_affinity policy to preferred to allow the PCI device",
            "        # to be selected from any numa node so we can prove the flavor",
            "        # overrides the alias.",
            "        alias = [jsonutils.dumps(",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': self.ALIAS_NAME,",
            "                'device_type': fields.PciDeviceType.STANDARD,",
            "                'numa_policy': fields.PCINUMAAffinityPolicy.PREFERRED,",
            "            }",
            "        )]",
            "",
            "        self.flags(",
            "            device_spec=self.PCI_DEVICE_SPEC,",
            "            alias=alias,",
            "            group='pci'",
            "        )",
            "",
            "        self._test_policy(pci_numa_node, status, 'required')",
            "",
            "    def test_socket_policy_pass(self):",
            "        # With 1 socket containing 2 NUMA nodes, make the first node's CPU",
            "        # available for pinning, but affine the PCI device to the second node.",
            "        # This should pass.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=1, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=1, numa_node=1)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        port_id = self._create_port_with_policy('socket')",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "        )",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_socket_policy_fail(self):",
            "        # With 2 sockets containing 1 NUMA node each, make the first socket's",
            "        # CPUs available for pinning, but affine the PCI device to the second",
            "        # NUMA node in the second socket. This should fail.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=1, cpu_sockets=2, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=1, numa_node=1)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        port_id = self._create_port_with_policy('socket')",
            "        # create a server using the VF via neutron",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "            expected_state='ERROR'",
            "        )",
            "        self.assertIn('fault', server)",
            "        self.assertIn('No valid host', server['fault']['message'])",
            "        self.assertFalse(self.mock_filter.called)",
            "",
            "    def test_socket_policy_multi_numa_pass(self):",
            "        # 2 sockets, 2 NUMA nodes each, with the PCI device on NUMA 0 and",
            "        # socket 0. If we restrict cpu_dedicated_set to NUMA 1, 2 and 3, we",
            "        # should still be able to boot an instance with hw:numa_nodes=3 and the",
            "        # `socket` policy, because one of the instance's NUMA nodes will be on",
            "        # the same socket as the PCI device (even if there is no direct NUMA",
            "        # node affinity).",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=2, cpu_cores=2, cpu_threads=1,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=1, numa_node=0)",
            "        self.flags(cpu_dedicated_set='2-7', group='compute')",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:numa_nodes': '3',",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(vcpu=6, memory_mb=3144,",
            "                                        extra_spec=extra_spec)",
            "        port_id = self._create_port_with_policy('socket')",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "        )",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "",
            "class RemoteManagedServersTest(_PCIServersWithMigrationTestBase):",
            "",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        # A PF with access to physnet4.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': 'a2dc',",
            "            'physical_network': 'physnet4',",
            "            'remote_managed': 'false',",
            "        },",
            "        # A VF with access to physnet4.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '1021',",
            "            'physical_network': 'physnet4',",
            "            'remote_managed': 'true',",
            "        },",
            "        # A PF programmed to forward traffic to an overlay network.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': 'a2d6',",
            "            'physical_network': None,",
            "            'remote_managed': 'false',",
            "        },",
            "        # A VF programmed to forward traffic to an overlay network.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '101e',",
            "            'physical_network': None,",
            "            'remote_managed': 'true',",
            "        },",
            "    )]",
            "",
            "    PCI_ALIAS = []",
            "",
            "    NUM_PFS = 1",
            "    NUM_VFS = 4",
            "    vf_ratio = NUM_VFS // NUM_PFS",
            "",
            "    # Min Libvirt version that supports working with PCI VPD.",
            "    FAKE_LIBVIRT_VERSION = 7_009_000  # 7.9.0",
            "    FAKE_QEMU_VERSION = 5_001_000  # 5.1.0",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.pci.utils.get_vf_num_by_pci_address',",
            "            new=mock.MagicMock(",
            "                side_effect=lambda addr: self._get_pci_function_number(addr))))",
            "",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.pci.utils.get_mac_by_pci_address',",
            "            new=mock.MagicMock(",
            "                side_effect=(",
            "                    lambda addr: {",
            "                        \"0000:80:00.0\": \"52:54:00:1e:59:42\",",
            "                        \"0000:81:00.0\": \"52:54:00:1e:59:01\",",
            "                        \"0000:82:00.0\": \"52:54:00:1e:59:02\",",
            "                    }.get(addr)",
            "                )",
            "            )",
            "        ))",
            "",
            "    @classmethod",
            "    def _get_pci_function_number(cls, pci_addr: str):",
            "        \"\"\"Get a VF function number based on a PCI address.",
            "",
            "        Assume that the PCI ARI capability is enabled (slot bits become a part",
            "        of a function number).",
            "        \"\"\"",
            "        _, _, slot, function = parse_address(pci_addr)",
            "        # The number of PFs is extracted to get a VF number.",
            "        return int(slot, 16) + int(function, 16) - cls.NUM_PFS",
            "",
            "    def start_compute(",
            "        self, hostname='test_compute0', host_info=None, pci_info=None,",
            "        mdev_info=None, vdpa_info=None,",
            "        libvirt_version=None,",
            "        qemu_version=None):",
            "",
            "        if not pci_info:",
            "            pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "                num_pci=0, num_pfs=0, num_vfs=0)",
            "",
            "            pci_info.add_device(",
            "                dev_type='PF',",
            "                bus=0x81,",
            "                slot=0x0,",
            "                function=0,",
            "                iommu_group=42,",
            "                numa_node=0,",
            "                vf_ratio=self.vf_ratio,",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='a2dc',",
            "                prod_name='BlueField-3 integrated ConnectX-7 controller',",
            "                driver_name='mlx5_core',",
            "                vpd_fields={",
            "                    'name': 'MT43244 BlueField-3 integrated ConnectX-7',",
            "                    'readonly': {",
            "                        'serial_number': 'MT0000X00001',",
            "                    },",
            "                }",
            "            )",
            "",
            "            for idx in range(self.NUM_VFS):",
            "                pci_info.add_device(",
            "                    dev_type='VF',",
            "                    bus=0x81,",
            "                    slot=0x0,",
            "                    function=idx + 1,",
            "                    iommu_group=idx + 43,",
            "                    numa_node=0,",
            "                    vf_ratio=self.vf_ratio,",
            "                    parent=(0x81, 0x0, 0),",
            "                    vend_id='15b3',",
            "                    vend_name='Mellanox Technologies',",
            "                    prod_id='1021',",
            "                    prod_name='MT2910 Family [ConnectX-7]',",
            "                    driver_name='mlx5_core',",
            "                    vpd_fields={",
            "                        'name': 'MT2910 Family [ConnectX-7]',",
            "                        'readonly': {",
            "                            'serial_number': 'MT0000X00001',",
            "                        },",
            "                    }",
            "                )",
            "",
            "            pci_info.add_device(",
            "                dev_type='PF',",
            "                bus=0x82,",
            "                slot=0x0,",
            "                function=0,",
            "                iommu_group=84,",
            "                numa_node=0,",
            "                vf_ratio=self.vf_ratio,",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='a2d6',",
            "                prod_name='MT42822 BlueField-2 integrated ConnectX-6',",
            "                driver_name='mlx5_core',",
            "                vpd_fields={",
            "                    'name': 'MT42822 BlueField-2 integrated ConnectX-6',",
            "                    'readonly': {",
            "                        'serial_number': 'MT0000X00002',",
            "                    },",
            "                }",
            "            )",
            "",
            "            for idx in range(self.NUM_VFS):",
            "                pci_info.add_device(",
            "                    dev_type='VF',",
            "                    bus=0x82,",
            "                    slot=0x0,",
            "                    function=idx + 1,",
            "                    iommu_group=idx + 85,",
            "                    numa_node=0,",
            "                    vf_ratio=self.vf_ratio,",
            "                    parent=(0x82, 0x0, 0),",
            "                    vend_id='15b3',",
            "                    vend_name='Mellanox Technologies',",
            "                    prod_id='101e',",
            "                    prod_name='ConnectX Family mlx5Gen Virtual Function',",
            "                    driver_name='mlx5_core')",
            "",
            "        return super().start_compute(",
            "            hostname=hostname, host_info=host_info, pci_info=pci_info,",
            "            mdev_info=mdev_info, vdpa_info=vdpa_info,",
            "            libvirt_version=libvirt_version or self.FAKE_LIBVIRT_VERSION,",
            "            qemu_version=qemu_version or self.FAKE_QEMU_VERSION)",
            "",
            "    def create_remote_managed_tunnel_port(self):",
            "        dpu_tunnel_port = {",
            "            'id': uuids.dpu_tunnel_port,",
            "            'network_id': self.neutron.network_3['id'],",
            "            'status': 'ACTIVE',",
            "            'mac_address': 'fa:16:3e:f0:a4:bb',",
            "            'fixed_ips': [",
            "                {",
            "                    'ip_address': '192.168.2.8',",
            "                    'subnet_id': self.neutron.subnet_3['id']",
            "                }",
            "            ],",
            "            'binding:vif_details': {},",
            "            'binding:vif_type': 'ovs',",
            "            'binding:vnic_type': 'remote-managed',",
            "        }",
            "",
            "        self.neutron.create_port({'port': dpu_tunnel_port})",
            "        return dpu_tunnel_port",
            "",
            "    def create_remote_managed_physnet_port(self):",
            "        dpu_physnet_port = {",
            "            'id': uuids.dpu_physnet_port,",
            "            'network_id': self.neutron.network_4['id'],",
            "            'status': 'ACTIVE',",
            "            'mac_address': 'd2:0b:fd:99:89:8b',",
            "            'fixed_ips': [",
            "                {",
            "                    'ip_address': '192.168.4.10',",
            "                    'subnet_id': self.neutron.subnet_4['id']",
            "                }",
            "            ],",
            "            'binding:vif_details': {},",
            "            'binding:vif_type': 'ovs',",
            "            'binding:vnic_type': 'remote-managed',",
            "        }",
            "",
            "        self.neutron.create_port({'port': dpu_physnet_port})",
            "        return dpu_physnet_port",
            "",
            "    def test_create_server_physnet(self):",
            "        \"\"\"Create an instance with a tunnel remote-managed port.\"\"\"",
            "",
            "        hostname = self.start_compute()",
            "        num_pci = (self.NUM_PFS + self.NUM_VFS) * 2",
            "",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        dpu_port = self.create_remote_managed_physnet_port()",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        self._create_server(networks=[{'port': dpu_port['id']}])",
            "",
            "        # Ensure there is one less VF available and that the PF",
            "        # is no longer usable.",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "",
            "        # Ensure the binding:profile details sent to Neutron are correct after",
            "        # a port update.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual({",
            "            'card_serial_number': 'MT0000X00001',",
            "            'pci_slot': '0000:81:00.4',",
            "            'pci_vendor_info': '15b3:1021',",
            "            'pf_mac_address': '52:54:00:1e:59:01',",
            "            'physical_network': 'physnet4',",
            "            'vf_num': 3",
            "        }, port['binding:profile'])",
            "",
            "    def test_create_server_tunnel(self):",
            "        \"\"\"Create an instance with a tunnel remote-managed port.\"\"\"",
            "",
            "        hostname = self.start_compute()",
            "        num_pci = (self.NUM_PFS + self.NUM_VFS) * 2",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        self._create_server(networks=[{'port': dpu_port['id']}])",
            "",
            "        # Ensure there is one less VF available and that the PF",
            "        # is no longer usable.",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "",
            "        # Ensure the binding:profile details sent to Neutron are correct after",
            "        # a port update.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual({",
            "            'card_serial_number': 'MT0000X00002',",
            "            'pci_slot': '0000:82:00.4',",
            "            'pci_vendor_info': '15b3:101e',",
            "            'pf_mac_address': '52:54:00:1e:59:02',",
            "            'physical_network': None,",
            "            'vf_num': 3",
            "        }, port['binding:profile'])",
            "",
            "    def _test_common(self, op, *args, **kwargs):",
            "        self.start_compute()",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        server = self._create_server(networks=[{'port': dpu_port['id']}])",
            "        op(server, *args, **kwargs)",
            "",
            "    def test_attach_interface(self):",
            "        self.start_compute()",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        server = self._create_server(networks='none')",
            "",
            "        self._attach_interface(server, dpu_port['id'])",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:82:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:02',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00002',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_detach_interface(self):",
            "        self._test_common(self._detach_interface, uuids.dpu_tunnel_port)",
            "",
            "        port = self.neutron.show_port(uuids.dpu_tunnel_port)['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual({}, port['binding:profile'])",
            "",
            "    def test_shelve(self):",
            "        self._test_common(self._shelve_server)",
            "",
            "        port = self.neutron.show_port(uuids.dpu_tunnel_port)['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:82:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:02',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00002',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_suspend(self):",
            "        self.start_compute()",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        server = self._create_server(networks=[{'port': dpu_port['id']}])",
            "        self._suspend_server(server)",
            "        # TODO(dmitriis): detachDevice does not properly handle hostdevs",
            "        # so full suspend/resume testing is problematic.",
            "",
            "    def _test_move_operation_with_neutron(self, move_operation, dpu_port):",
            "        \"\"\"Test a move operation with a remote-managed port.",
            "        \"\"\"",
            "        compute1_pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=0, num_vfs=0)",
            "",
            "        compute1_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x80,",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=84,",
            "            numa_node=1,",
            "            vf_ratio=self.vf_ratio,",
            "            vend_id='15b3',",
            "            vend_name='Mellanox Technologies',",
            "            prod_id='a2d6',",
            "            prod_name='MT42822 BlueField-2 integrated ConnectX-6',",
            "            driver_name='mlx5_core',",
            "            vpd_fields={",
            "                'name': 'MT42822 BlueField-2 integrated ConnectX-6',",
            "                'readonly': {",
            "                    'serial_number': 'MT0000X00042',",
            "                },",
            "            }",
            "        )",
            "        for idx in range(self.NUM_VFS):",
            "            compute1_pci_info.add_device(",
            "                dev_type='VF',",
            "                bus=0x80,",
            "                slot=0x0,",
            "                function=idx + 1,",
            "                iommu_group=idx + 85,",
            "                numa_node=1,",
            "                vf_ratio=self.vf_ratio,",
            "                parent=(0x80, 0x0, 0),",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='101e',",
            "                prod_name='ConnectX Family mlx5Gen Virtual Function',",
            "                driver_name='mlx5_core',",
            "                vpd_fields={",
            "                    'name': 'MT42822 BlueField-2 integrated ConnectX-6',",
            "                    'readonly': {",
            "                        'serial_number': 'MT0000X00042',",
            "                    },",
            "                }",
            "            )",
            "",
            "        self.start_compute(hostname='test_compute0')",
            "        self.start_compute(hostname='test_compute1',",
            "                           pci_info=compute1_pci_info)",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        flavor_id = self._create_flavor(vcpu=4)",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[{'port': dpu_port['id']}],",
            "            host='test_compute0',",
            "        )",
            "",
            "        self.assertEqual('test_compute0', server['OS-EXT-SRV-ATTR:host'])",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=5)",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:82:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:02',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00002',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "        move_operation(server)",
            "",
            "    def test_unshelve_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            self._shelve_server(source_server)",
            "            # Disable the source compute, to force unshelving on the dest.",
            "            self.api.put_service(",
            "                self.computes['test_compute0'].service_ref.uuid,",
            "                {'status': 'disabled'})",
            "            self._unshelve_server(source_server)",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=10)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "        # Ensure the binding:profile details got updated, including the",
            "        # fields relevant to remote-managed ports.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:80:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:42',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00042',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_cold_migrate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                            '.migrate_disk_and_power_off', return_value='{}'):",
            "                server = self._migrate_server(source_server)",
            "                self._confirm_resize(server)",
            "",
            "                self.assertPCIDeviceCounts('test_compute0', total=10, free=10)",
            "                self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "                # Ensure the binding:profile details got updated, including the",
            "                # fields relevant to remote-managed ports.",
            "                port = self.neutron.show_port(dpu_port['id'])['port']",
            "                self.assertIn('binding:profile', port)",
            "                self.assertEqual(",
            "                    {",
            "                        'pci_vendor_info': '15b3:101e',",
            "                        'pci_slot': '0000:80:00.4',",
            "                        'physical_network': None,",
            "                        'pf_mac_address': '52:54:00:1e:59:42',",
            "                        'vf_num': 3,",
            "                        'card_serial_number': 'MT0000X00042',",
            "                    },",
            "                    port['binding:profile'],",
            "                )",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "    def test_cold_migrate_server_with_neutron_revert(self):",
            "        def move_operation(source_server):",
            "            with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                            '.migrate_disk_and_power_off', return_value='{}'):",
            "                server = self._migrate_server(source_server)",
            "",
            "                self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "                self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "                self._revert_resize(server)",
            "",
            "                self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "                self.assertPCIDeviceCounts('test_compute1', total=5, free=5)",
            "",
            "                port = self.neutron.show_port(dpu_port['id'])['port']",
            "                self.assertIn('binding:profile', port)",
            "                self.assertEqual(",
            "                    {",
            "                        'pci_vendor_info': '15b3:101e',",
            "                        'pci_slot': '0000:82:00.4',",
            "                        'physical_network': None,",
            "                        'pf_mac_address': '52:54:00:1e:59:02',",
            "                        'vf_num': 3,",
            "                        'card_serial_number': 'MT0000X00002',",
            "                    },",
            "                    port['binding:profile'],",
            "                )",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "    def test_evacuate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            # Down the source compute to enable the evacuation",
            "            self.api.put_service(",
            "                self.computes['test_compute0'].service_ref.uuid,",
            "                {'forced_down': True})",
            "            self.computes['test_compute0'].stop()",
            "            self._evacuate_server(source_server)",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "        # Ensure the binding:profile details got updated, including the",
            "        # fields relevant to remote-managed ports.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:80:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:42',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00042',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_live_migrate_server_with_neutron(self):",
            "        \"\"\"Live migrate an instance using a remote-managed port.",
            "",
            "        This should succeed since we support this via detach and attach of the",
            "        PCI device similar to how this is done for SR-IOV ports.",
            "        \"\"\"",
            "        def move_operation(source_server):",
            "            self._live_migrate(source_server, 'completed')",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=10)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "        # Ensure the binding:profile details got updated, including the",
            "        # fields relevant to remote-managed ports.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:80:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:42',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00042',",
            "            },",
            "            port['binding:profile'],",
            "        )"
        ],
        "afterPatchFile": [
            "# Copyright (C) 2016 Red Hat, Inc",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "import copy",
            "from unittest import mock",
            "from urllib import parse as urlparse",
            "",
            "import ddt",
            "import fixtures",
            "from lxml import etree",
            "from oslo_config import cfg",
            "from oslo_log import log as logging",
            "from oslo_serialization import jsonutils",
            "from oslo_utils.fixture import uuidsentinel as uuids",
            "from oslo_utils import units",
            "",
            "import nova",
            "from nova import context",
            "from nova import exception",
            "from nova.network import constants",
            "from nova import objects",
            "from nova.objects import fields",
            "from nova.pci.utils import parse_address",
            "from nova.tests import fixtures as nova_fixtures",
            "from nova.tests.fixtures import libvirt as fakelibvirt",
            "from nova.tests.functional.api import client",
            "from nova.tests.functional.libvirt import base",
            "",
            "CONF = cfg.CONF",
            "LOG = logging.getLogger(__name__)",
            "",
            "",
            "class _PCIServersTestBase(base.ServersTestBase):",
            "",
            "    ADDITIONAL_FILTERS = ['NUMATopologyFilter', 'PciPassthroughFilter']",
            "",
            "    def setUp(self):",
            "        self.ctxt = context.get_admin_context()",
            "        self.flags(",
            "            device_spec=self.PCI_DEVICE_SPEC,",
            "            alias=self.PCI_ALIAS,",
            "            group='pci'",
            "        )",
            "",
            "        super(_PCIServersTestBase, self).setUp()",
            "",
            "        # Mock the 'PciPassthroughFilter' filter, as most tests need to inspect",
            "        # this",
            "        host_manager = self.scheduler.manager.host_manager",
            "        pci_filter_class = host_manager.filter_cls_map['PciPassthroughFilter']",
            "        host_pass_mock = mock.Mock(wraps=pci_filter_class().host_passes)",
            "        self.mock_filter = self.useFixture(fixtures.MockPatch(",
            "            'nova.scheduler.filters.pci_passthrough_filter'",
            "            '.PciPassthroughFilter.host_passes',",
            "            side_effect=host_pass_mock)).mock",
            "",
            "    def assertPCIDeviceCounts(self, hostname, total, free):",
            "        \"\"\"Ensure $hostname has $total devices, $free of which are free.\"\"\"",
            "        devices = objects.PciDeviceList.get_by_compute_node(",
            "            self.ctxt,",
            "            objects.ComputeNode.get_by_nodename(self.ctxt, hostname).id,",
            "        )",
            "        self.assertEqual(total, len(devices))",
            "        self.assertEqual(free, len([d for d in devices if d.is_available()]))",
            "",
            "    def _get_rp_by_name(self, name, rps):",
            "        for rp in rps:",
            "            if rp[\"name\"] == name:",
            "                return rp",
            "        self.fail(f'RP {name} is not found in Placement {rps}')",
            "",
            "    def assert_placement_pci_view(",
            "        self, hostname, inventories, traits, usages=None, allocations=None",
            "    ):",
            "        if not usages:",
            "            usages = {}",
            "",
            "        if not allocations:",
            "            allocations = {}",
            "",
            "        compute_rp_uuid = self.compute_rp_uuids[hostname]",
            "        rps = self._get_all_rps_in_a_tree(compute_rp_uuid)",
            "",
            "        # rps also contains the root provider so we subtract 1",
            "        self.assertEqual(",
            "            len(inventories),",
            "            len(rps) - 1,",
            "            f\"Number of RPs on {hostname} doesn't match. \"",
            "            f\"Expected {list(inventories)} actual {[rp['name'] for rp in rps]}\"",
            "        )",
            "",
            "        for rp_name, inv in inventories.items():",
            "            real_rp_name = f'{hostname}_{rp_name}'",
            "            rp = self._get_rp_by_name(real_rp_name, rps)",
            "            rp_inv = self._get_provider_inventory(rp['uuid'])",
            "",
            "            self.assertEqual(",
            "                len(inv),",
            "                len(rp_inv),",
            "                f\"Number of inventories on {real_rp_name} are not as \"",
            "                f\"expected. Expected {inv}, actual {rp_inv}\"",
            "            )",
            "            for rc, total in inv.items():",
            "                self.assertEqual(",
            "                    total,",
            "                    rp_inv[rc][\"total\"])",
            "                self.assertEqual(",
            "                    total,",
            "                    rp_inv[rc][\"max_unit\"])",
            "",
            "            rp_traits = self._get_provider_traits(rp['uuid'])",
            "            self.assertEqual(",
            "                # COMPUTE_MANAGED_PCI_DEVICE is automatically reported on",
            "                # PCI device RPs by nova",
            "                set(traits[rp_name]) | {\"COMPUTE_MANAGED_PCI_DEVICE\"},",
            "                set(rp_traits),",
            "                f\"Traits on RP {real_rp_name} does not match with expectation\"",
            "            )",
            "",
            "        for rp_name, usage in usages.items():",
            "            real_rp_name = f'{hostname}_{rp_name}'",
            "            rp = self._get_rp_by_name(real_rp_name, rps)",
            "            rp_usage = self._get_provider_usages(rp['uuid'])",
            "            self.assertEqual(",
            "                usage,",
            "                rp_usage,",
            "                f\"Usage on RP {real_rp_name} does not match with expectation\"",
            "            )",
            "",
            "        for consumer, expected_allocations in allocations.items():",
            "            actual_allocations = self._get_allocations_by_server_uuid(consumer)",
            "            self.assertEqual(",
            "                len(expected_allocations),",
            "                # actual_allocations also contains allocations against the",
            "                # root provider for VCPU, MEMORY_MB, and DISK_GB so subtract",
            "                # one",
            "                len(actual_allocations) - 1,",
            "                f\"The consumer {consumer} allocates from different number of \"",
            "                f\"RPs than expected. Expected: {expected_allocations}, \"",
            "                f\"Actual: {actual_allocations}\"",
            "            )",
            "            for rp_name, expected_rp_allocs in expected_allocations.items():",
            "                real_rp_name = f'{hostname}_{rp_name}'",
            "                rp = self._get_rp_by_name(real_rp_name, rps)",
            "                self.assertIn(",
            "                    rp['uuid'],",
            "                    actual_allocations,",
            "                    f\"The consumer {consumer} expected to allocate from \"",
            "                    f\"{rp['uuid']}. Expected: {expected_allocations}, \"",
            "                    f\"Actual: {actual_allocations}\"",
            "                )",
            "                actual_rp_allocs = actual_allocations[rp['uuid']]['resources']",
            "                self.assertEqual(",
            "                    expected_rp_allocs,",
            "                    actual_rp_allocs,",
            "                    f\"The consumer {consumer} expected to have allocation \"",
            "                    f\"{expected_rp_allocs} on {rp_name} but it has \"",
            "                    f\"{actual_rp_allocs} instead.\"",
            "                )",
            "",
            "",
            "class _PCIServersWithMigrationTestBase(_PCIServersTestBase):",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        self.useFixture(fixtures.MonkeyPatch(",
            "            'nova.tests.fixtures.libvirt.Domain.migrateToURI3',",
            "            self._migrate_stub))",
            "",
            "    def _migrate_stub(self, domain, destination, params, flags):",
            "        \"\"\"Stub out migrateToURI3.\"\"\"",
            "",
            "        src_hostname = domain._connection.hostname",
            "        dst_hostname = urlparse.urlparse(destination).netloc",
            "",
            "        # In a real live migration, libvirt and QEMU on the source and",
            "        # destination talk it out, resulting in the instance starting to exist",
            "        # on the destination. Fakelibvirt cannot do that, so we have to",
            "        # manually create the \"incoming\" instance on the destination",
            "        # fakelibvirt.",
            "        dst = self.computes[dst_hostname]",
            "        dst.driver._host.get_connection().createXML(",
            "            params['destination_xml'],",
            "            'fake-createXML-doesnt-care-about-flags')",
            "",
            "        src = self.computes[src_hostname]",
            "        conn = src.driver._host.get_connection()",
            "",
            "        # because migrateToURI3 is spawned in a background thread, this method",
            "        # does not block the upper nova layers. Because we don't want nova to",
            "        # think the live migration has finished until this method is done, the",
            "        # last thing we do is make fakelibvirt's Domain.jobStats() return",
            "        # VIR_DOMAIN_JOB_COMPLETED.",
            "        server = etree.fromstring(",
            "            params['destination_xml']",
            "        ).find('./uuid').text",
            "        dom = conn.lookupByUUIDString(server)",
            "        dom.complete_job()",
            "",
            "",
            "class SRIOVServersTest(_PCIServersWithMigrationTestBase):",
            "",
            "    # TODO(stephenfin): We're using this because we want to be able to force",
            "    # the host during scheduling. We should instead look at overriding policy",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    VFS_ALIAS_NAME = 'vfs'",
            "    PFS_ALIAS_NAME = 'pfs'",
            "",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "    )]",
            "    # PFs will be removed from pools unless they are specifically",
            "    # requested, so we explicitly request them with the 'device_type'",
            "    # attribute",
            "    PCI_ALIAS = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            'device_type': fields.PciDeviceType.SRIOV_PF,",
            "            'name': PFS_ALIAS_NAME,",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            'name': VFS_ALIAS_NAME,",
            "        },",
            "    )]",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        # The ultimate base class _IntegratedTestBase uses NeutronFixture but",
            "        # we need a bit more intelligent neutron for these tests. Applying the",
            "        # new fixture here means that we re-stub what the previous neutron",
            "        # fixture already stubbed.",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "",
            "    def _disable_sriov_in_pf(self, pci_info):",
            "        # Check for PF and change the capability from virt_functions",
            "        # Delete all the VFs",
            "        vfs_to_delete = []",
            "",
            "        for device_name, device in pci_info.devices.items():",
            "            if 'virt_functions' in device.pci_device:",
            "                device.generate_xml(skip_capability=True)",
            "            elif 'phys_function' in device.pci_device:",
            "                vfs_to_delete.append(device_name)",
            "",
            "        for device in vfs_to_delete:",
            "            del pci_info.devices[device]",
            "",
            "    def test_create_server_with_VF(self):",
            "        \"\"\"Create a server with an SR-IOV VF-type PCI device.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo()",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server",
            "        extra_spec = {\"pci_passthrough:alias\": \"%s:1\" % self.VFS_ALIAS_NAME}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # ensure the filter was called",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_create_server_with_PF(self):",
            "        \"\"\"Create a server with an SR-IOV PF-type PCI device.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo()",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server",
            "        extra_spec = {\"pci_passthrough:alias\": \"%s:1\" % self.PFS_ALIAS_NAME}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # ensure the filter was called",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_create_server_with_PF_no_VF(self):",
            "        \"\"\"Create a server with a PF and ensure the VFs are then reserved.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=4)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server using the PF",
            "        extra_spec_pfs = {\"pci_passthrough:alias\": f\"{self.PFS_ALIAS_NAME}:1\"}",
            "        flavor_id_pfs = self._create_flavor(extra_spec=extra_spec_pfs)",
            "        self._create_server(flavor_id=flavor_id_pfs, networks='none')",
            "",
            "        # now attempt to build another server, this time using the VF; this",
            "        # should fail because the VF is used by an instance",
            "        extra_spec_vfs = {\"pci_passthrough:alias\": f\"{self.VFS_ALIAS_NAME}:1\"}",
            "        flavor_id_vfs = self._create_flavor(extra_spec=extra_spec_vfs)",
            "        self._create_server(",
            "            flavor_id=flavor_id_vfs, networks='none', expected_state='ERROR',",
            "        )",
            "",
            "    def test_create_server_with_VF_no_PF(self):",
            "        \"\"\"Create a server with a VF and ensure the PF is then reserved.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=4)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a server using the VF",
            "        extra_spec_vfs = {'pci_passthrough:alias': f'{self.VFS_ALIAS_NAME}:1'}",
            "        flavor_id_vfs = self._create_flavor(extra_spec=extra_spec_vfs)",
            "        self._create_server(flavor_id=flavor_id_vfs, networks='none')",
            "",
            "        # now attempt to build another server, this time using the PF; this",
            "        # should fail because the PF is used by an instance",
            "        extra_spec_pfs = {'pci_passthrough:alias': f'{self.PFS_ALIAS_NAME}:1'}",
            "        flavor_id_pfs = self._create_flavor(extra_spec=extra_spec_pfs)",
            "        self._create_server(",
            "            flavor_id=flavor_id_pfs, networks='none', expected_state='ERROR',",
            "        )",
            "",
            "    def test_create_server_with_neutron(self):",
            "        \"\"\"Create an instance using a neutron-provisioned SR-IOV VIF.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)",
            "",
            "        orig_create = nova.virt.libvirt.guest.Guest.create",
            "",
            "        def fake_create(cls, xml, host):",
            "            tree = etree.fromstring(xml)",
            "            elem = tree.find('./devices/interface/source/address')",
            "",
            "            # compare address",
            "            expected = ('0x81', '0x00', '0x2')",
            "            actual = (",
            "                elem.get('bus'), elem.get('slot'), elem.get('function'),",
            "            )",
            "            self.assertEqual(expected, actual)",
            "",
            "            return orig_create(xml, host)",
            "",
            "        self.stub_out(",
            "            'nova.virt.libvirt.guest.Guest.create',",
            "            fake_create,",
            "        )",
            "",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create the port",
            "        self.neutron.create_port({'port': self.neutron.network_4_port_1})",
            "",
            "        # ensure the binding details are currently unset",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            networks=[",
            "                {'port': base.LibvirtNeutronFixture.network_4_port_1['id']},",
            "            ],",
            "        )",
            "",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1515',",
            "                'pci_slot': '0000:81:00.2',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_live_migrate_server_with_PF(self):",
            "        \"\"\"Live migrate an instance with a PCI PF.",
            "",
            "        This should fail because it's not possible to live migrate an instance",
            "        with a PCI passthrough device, even if it's a SR-IOV PF.",
            "        \"\"\"",
            "",
            "        # start two compute services",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "        self.start_compute(",
            "            hostname='test_compute1',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "",
            "        # create a server",
            "        extra_spec = {'pci_passthrough:alias': f'{self.PFS_ALIAS_NAME}:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now live migrate that server",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            self._live_migrate,",
            "            server, 'completed')",
            "        # NOTE(stephenfin): this wouldn't happen in a real deployment since",
            "        # live migration is a cast, but since we are using CastAsCallFixture",
            "        # this will bubble to the API",
            "        self.assertEqual(500, ex.response.status_code)",
            "        self.assertIn('NoValidHost', str(ex))",
            "",
            "    def test_live_migrate_server_with_VF(self):",
            "        \"\"\"Live migrate an instance with a PCI VF.",
            "",
            "        This should fail because it's not possible to live migrate an instance",
            "        with a PCI passthrough device, even if it's a SR-IOV VF.",
            "        \"\"\"",
            "",
            "        # start two compute services",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "        self.start_compute(",
            "            hostname='test_compute1',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pfs=2, num_vfs=4))",
            "",
            "        # create a server",
            "        extra_spec = {'pci_passthrough:alias': f'{self.VFS_ALIAS_NAME}:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now live migrate that server",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            self._live_migrate,",
            "            server, 'completed')",
            "        # NOTE(stephenfin): this wouldn't happen in a real deployment since",
            "        # live migration is a cast, but since we are using CastAsCallFixture",
            "        # this will bubble to the API",
            "        self.assertEqual(500, ex.response.status_code)",
            "        self.assertIn('NoValidHost', str(ex))",
            "",
            "    def _test_move_operation_with_neutron(self, move_operation,",
            "                                          expect_fail=False):",
            "        # The purpose here is to force an observable PCI slot update when",
            "        # moving from source to dest. This is accomplished by having a single",
            "        # PCI VF device on the source, 2 PCI VF devices on the dest, and",
            "        # relying on the fact that our fake HostPCIDevicesInfo creates",
            "        # predictable PCI addresses. The PCI VF device on source and the first",
            "        # PCI VF device on dest will have identical PCI addresses. By sticking",
            "        # a \"placeholder\" instance on that first PCI VF device on the dest, the",
            "        # incoming instance from source will be forced to consume the second",
            "        # dest PCI VF device, with a different PCI address.",
            "        # We want to test server operations with SRIOV VFs and SRIOV PFs so",
            "        # the config of the compute hosts also have one extra PCI PF devices",
            "        # without any VF children. But the two compute has different PCI PF",
            "        # addresses and MAC so that the test can observe the slot update as",
            "        # well as the MAC updated during migration and after revert.",
            "        source_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        source_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:aa',",
            "        )",
            "        self.start_compute(",
            "            hostname='source',",
            "            pci_info=source_pci_info)",
            "",
            "        dest_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        dest_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x6,  # make it different from the source host",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:bb',",
            "        )",
            "        self.start_compute(",
            "            hostname='dest',",
            "            pci_info=dest_pci_info)",
            "",
            "        source_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})",
            "        source_pf_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_pf})",
            "        dest_port1 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_2})",
            "        dest_port2 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_3})",
            "",
            "        source_server = self._create_server(",
            "            networks=[",
            "                {'port': source_port['port']['id']},",
            "                {'port': source_pf_port['port']['id']}",
            "            ],",
            "            host='source',",
            "        )",
            "        dest_server1 = self._create_server(",
            "            networks=[{'port': dest_port1['port']['id']}], host='dest')",
            "        dest_server2 = self._create_server(",
            "            networks=[{'port': dest_port2['port']['id']}], host='dest')",
            "",
            "        # Refresh the ports.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "        dest_port1 = self.neutron.show_port(dest_port1['port']['id'])",
            "        dest_port2 = self.neutron.show_port(dest_port2['port']['id'])",
            "",
            "        # Find the server on the dest compute that's using the same pci_slot as",
            "        # the server on the source compute, and delete the other one to make",
            "        # room for the incoming server from the source.",
            "        source_pci_slot = source_port['port']['binding:profile']['pci_slot']",
            "        dest_pci_slot1 = dest_port1['port']['binding:profile']['pci_slot']",
            "        if dest_pci_slot1 == source_pci_slot:",
            "            same_slot_port = dest_port1",
            "            self._delete_server(dest_server2)",
            "        else:",
            "            same_slot_port = dest_port2",
            "            self._delete_server(dest_server1)",
            "",
            "        # Before moving, explicitly assert that the servers on source and dest",
            "        # have the same pci_slot in their port's binding profile",
            "        self.assertEqual(source_port['port']['binding:profile']['pci_slot'],",
            "                         same_slot_port['port']['binding:profile']['pci_slot'])",
            "",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the source host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:00.0',  # which is in sync with the source host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the source host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:aa',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "",
            "        # Before moving, assert that the servers on source and dest have the",
            "        # same PCI source address in their XML for their SRIOV nic.",
            "        source_conn = self.computes['source'].driver._host.get_connection()",
            "        dest_conn = self.computes['source'].driver._host.get_connection()",
            "        source_vms = [vm._def for vm in source_conn._vms.values()]",
            "        dest_vms = [vm._def for vm in dest_conn._vms.values()]",
            "        self.assertEqual(1, len(source_vms))",
            "        self.assertEqual(1, len(dest_vms))",
            "        self.assertEqual(1, len(source_vms[0]['devices']['nics']))",
            "        self.assertEqual(1, len(dest_vms[0]['devices']['nics']))",
            "        self.assertEqual(source_vms[0]['devices']['nics'][0]['source'],",
            "                         dest_vms[0]['devices']['nics'][0]['source'])",
            "",
            "        move_operation(source_server)",
            "",
            "        # Refresh the ports again, keeping in mind the source_port is now bound",
            "        # on the dest after the move.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        same_slot_port = self.neutron.show_port(same_slot_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "",
            "        self.assertNotEqual(",
            "            source_port['port']['binding:profile']['pci_slot'],",
            "            same_slot_port['port']['binding:profile']['pci_slot'])",
            "",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the dest host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:06.0',  # which is in sync with the dest host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the dest host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:bb',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "",
            "        conn = self.computes['dest'].driver._host.get_connection()",
            "        vms = [vm._def for vm in conn._vms.values()]",
            "        self.assertEqual(2, len(vms))",
            "        for vm in vms:",
            "            self.assertEqual(1, len(vm['devices']['nics']))",
            "        self.assertNotEqual(vms[0]['devices']['nics'][0]['source'],",
            "                            vms[1]['devices']['nics'][0]['source'])",
            "",
            "    def test_unshelve_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            self._shelve_server(source_server)",
            "            # Disable the source compute, to force unshelving on the dest.",
            "            self.api.put_service(self.computes['source'].service_ref.uuid,",
            "                                 {'status': 'disabled'})",
            "            self._unshelve_server(source_server)",
            "        self._test_move_operation_with_neutron(move_operation)",
            "",
            "    def test_cold_migrate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            # TODO(stephenfin): The mock of 'migrate_disk_and_power_off' should",
            "            # probably be less...dumb",
            "            with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                            '.migrate_disk_and_power_off', return_value='{}'):",
            "                self._migrate_server(source_server)",
            "            self._confirm_resize(source_server)",
            "        self._test_move_operation_with_neutron(move_operation)",
            "",
            "    def test_cold_migrate_and_rever_server_with_neutron(self):",
            "        # The purpose here is to force an observable PCI slot update when",
            "        # moving from source to dest and the from dest to source after the",
            "        # revert. This is accomplished by having a single",
            "        # PCI VF device on the source, 2 PCI VF devices on the dest, and",
            "        # relying on the fact that our fake HostPCIDevicesInfo creates",
            "        # predictable PCI addresses. The PCI VF device on source and the first",
            "        # PCI VF device on dest will have identical PCI addresses. By sticking",
            "        # a \"placeholder\" instance on that first PCI VF device on the dest, the",
            "        # incoming instance from source will be forced to consume the second",
            "        # dest PCI VF device, with a different PCI address.",
            "        # We want to test server operations with SRIOV VFs and SRIOV PFs so",
            "        # the config of the compute hosts also have one extra PCI PF devices",
            "        # without any VF children. But the two compute has different PCI PF",
            "        # addresses and MAC so that the test can observe the slot update as",
            "        # well as the MAC updated during migration and after revert.",
            "        source_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        source_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:aa',",
            "        )",
            "        self.start_compute(",
            "            hostname='source',",
            "            pci_info=source_pci_info)",
            "        dest_pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        dest_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x6,  # make it different from the source host",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:bb',",
            "        )",
            "        self.start_compute(",
            "            hostname='dest',",
            "            pci_info=dest_pci_info)",
            "        source_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})",
            "        source_pf_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_pf})",
            "        dest_port1 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_2})",
            "        dest_port2 = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_3})",
            "        source_server = self._create_server(",
            "            networks=[",
            "                {'port': source_port['port']['id']},",
            "                {'port': source_pf_port['port']['id']}",
            "            ],",
            "            host='source',",
            "        )",
            "        dest_server1 = self._create_server(",
            "            networks=[{'port': dest_port1['port']['id']}], host='dest')",
            "        dest_server2 = self._create_server(",
            "            networks=[{'port': dest_port2['port']['id']}], host='dest')",
            "        # Refresh the ports.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "        dest_port1 = self.neutron.show_port(dest_port1['port']['id'])",
            "        dest_port2 = self.neutron.show_port(dest_port2['port']['id'])",
            "        # Find the server on the dest compute that's using the same pci_slot as",
            "        # the server on the source compute, and delete the other one to make",
            "        # room for the incoming server from the source.",
            "        source_pci_slot = source_port['port']['binding:profile']['pci_slot']",
            "        dest_pci_slot1 = dest_port1['port']['binding:profile']['pci_slot']",
            "        if dest_pci_slot1 == source_pci_slot:",
            "            same_slot_port = dest_port1",
            "            self._delete_server(dest_server2)",
            "        else:",
            "            same_slot_port = dest_port2",
            "            self._delete_server(dest_server1)",
            "        # Before moving, explicitly assert that the servers on source and dest",
            "        # have the same pci_slot in their port's binding profile",
            "        self.assertEqual(source_port['port']['binding:profile']['pci_slot'],",
            "                         same_slot_port['port']['binding:profile']['pci_slot'])",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the source host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:00.0',  # which is in sync with the source host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the source host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:aa',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "        # Before moving, assert that the servers on source and dest have the",
            "        # same PCI source address in their XML for their SRIOV nic.",
            "        source_conn = self.computes['source'].driver._host.get_connection()",
            "        dest_conn = self.computes['source'].driver._host.get_connection()",
            "        source_vms = [vm._def for vm in source_conn._vms.values()]",
            "        dest_vms = [vm._def for vm in dest_conn._vms.values()]",
            "        self.assertEqual(1, len(source_vms))",
            "        self.assertEqual(1, len(dest_vms))",
            "        self.assertEqual(1, len(source_vms[0]['devices']['nics']))",
            "        self.assertEqual(1, len(dest_vms[0]['devices']['nics']))",
            "        self.assertEqual(source_vms[0]['devices']['nics'][0]['source'],",
            "                         dest_vms[0]['devices']['nics'][0]['source'])",
            "",
            "        # TODO(stephenfin): The mock of 'migrate_disk_and_power_off' should",
            "        # probably be less...dumb",
            "        with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                        '.migrate_disk_and_power_off', return_value='{}'):",
            "            self._migrate_server(source_server)",
            "",
            "        # Refresh the ports again, keeping in mind the ports are now bound",
            "        # on the dest after migrating.",
            "        source_port = self.neutron.show_port(source_port['port']['id'])",
            "        same_slot_port = self.neutron.show_port(same_slot_port['port']['id'])",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "        self.assertNotEqual(",
            "            source_port['port']['binding:profile']['pci_slot'],",
            "            same_slot_port['port']['binding:profile']['pci_slot'])",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the dest host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:06.0',  # which is in sync with the dest host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the dest host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:bb',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "        conn = self.computes['dest'].driver._host.get_connection()",
            "        vms = [vm._def for vm in conn._vms.values()]",
            "        self.assertEqual(2, len(vms))",
            "        for vm in vms:",
            "            self.assertEqual(1, len(vm['devices']['nics']))",
            "        self.assertNotEqual(vms[0]['devices']['nics'][0]['source'],",
            "                            vms[1]['devices']['nics'][0]['source'])",
            "",
            "        self._revert_resize(source_server)",
            "",
            "        # Refresh the ports again, keeping in mind the ports are now bound",
            "        # on the source as the migration is reverted",
            "        source_pf_port = self.neutron.show_port(source_pf_port['port']['id'])",
            "",
            "        # Assert that the direct-physical port got the pci_slot information",
            "        # according to the source host PF PCI device.",
            "        self.assertEqual(",
            "            '0000:82:00.0',  # which is in sync with the source host pci_info",
            "            source_pf_port['port']['binding:profile']['pci_slot']",
            "        )",
            "        # Assert that the direct-physical port is updated with the MAC address",
            "        # of the PF device from the source host",
            "        self.assertEqual(",
            "            'b4:96:91:34:f4:aa',",
            "            source_pf_port['port']['binding:profile']['device_mac_address']",
            "        )",
            "",
            "    def test_evacuate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            # Down the source compute to enable the evacuation",
            "            self.api.put_service(self.computes['source'].service_ref.uuid,",
            "                                 {'forced_down': True})",
            "            self.computes['source'].stop()",
            "            self._evacuate_server(source_server)",
            "        self._test_move_operation_with_neutron(move_operation)",
            "",
            "    def test_live_migrate_server_with_neutron(self):",
            "        \"\"\"Live migrate an instance using a neutron-provisioned SR-IOV VIF.",
            "",
            "        This should succeed since we support this, via detach and attach of the",
            "        PCI device.",
            "        \"\"\"",
            "",
            "        # start two compute services with differing PCI device inventory",
            "        source_pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=2, num_vfs=8, numa_node=0)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        source_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=42,",
            "            numa_node=0,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:aa',",
            "        )",
            "        self.start_compute(hostname='test_compute0', pci_info=source_pci_info)",
            "",
            "        dest_pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=2, numa_node=1)",
            "        # add an extra PF without VF to be used by direct-physical ports",
            "        dest_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x82,  # the HostPCIDevicesInfo use the 0x81 by default",
            "            slot=0x6,  # make it different from the source host",
            "            function=0,",
            "            iommu_group=42,",
            "            # numa node needs to be aligned with the other pci devices in this",
            "            # host as the instance needs to fit into a single host numa node",
            "            numa_node=1,",
            "            vf_ratio=0,",
            "            mac_address='b4:96:91:34:f4:bb',",
            "        )",
            "",
            "        self.start_compute(hostname='test_compute1', pci_info=dest_pci_info)",
            "",
            "        # create the ports",
            "        port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})['port']",
            "        pf_port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_pf})['port']",
            "",
            "        # create a server using the VF via neutron",
            "        extra_spec = {'hw:cpu_policy': 'dedicated'}",
            "        flavor_id = self._create_flavor(vcpu=4, extra_spec=extra_spec)",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port['id']},",
            "                {'port': pf_port['id']},",
            "            ],",
            "            host='test_compute0',",
            "        )",
            "",
            "        # our source host should have marked two PCI devices as used, the VF",
            "        # and the parent PF, while the future destination is currently unused",
            "        self.assertEqual('test_compute0', server['OS-EXT-SRV-ATTR:host'])",
            "        self.assertPCIDeviceCounts('test_compute0', total=11, free=8)",
            "        self.assertPCIDeviceCounts('test_compute1', total=4, free=4)",
            "",
            "        # the instance should be on host NUMA node 0, since that's where our",
            "        # PCI devices are",
            "        host_numa = objects.NUMATopology.obj_from_db_obj(",
            "            objects.ComputeNode.get_by_nodename(",
            "                self.ctxt, 'test_compute0',",
            "            ).numa_topology",
            "        )",
            "        self.assertEqual({0, 1, 2, 3}, host_numa.cells[0].pinned_cpus)",
            "        self.assertEqual(set(), host_numa.cells[1].pinned_cpus)",
            "",
            "        # ensure the binding details sent to \"neutron\" are correct",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1515',",
            "                # TODO(stephenfin): Stop relying on a side-effect of how nova",
            "                # chooses from multiple PCI devices (apparently the last",
            "                # matching one)",
            "                'pci_slot': '0000:81:01.4',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "        # ensure the binding details sent to \"neutron\" are correct",
            "        pf_port = self.neutron.show_port(pf_port['id'],)['port']",
            "        self.assertIn('binding:profile', pf_port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1528',",
            "                'pci_slot': '0000:82:00.0',",
            "                'physical_network': 'physnet4',",
            "                'device_mac_address': 'b4:96:91:34:f4:aa',",
            "            },",
            "            pf_port['binding:profile'],",
            "        )",
            "",
            "        # now live migrate that server",
            "        self._live_migrate(server, 'completed')",
            "",
            "        # we should now have transitioned our usage to the destination, freeing",
            "        # up the source in the process",
            "        self.assertPCIDeviceCounts('test_compute0', total=11, free=11)",
            "        self.assertPCIDeviceCounts('test_compute1', total=4, free=1)",
            "",
            "        # the instance should now be on host NUMA node 1, since that's where",
            "        # our PCI devices are for this second host",
            "        host_numa = objects.NUMATopology.obj_from_db_obj(",
            "            objects.ComputeNode.get_by_nodename(",
            "                self.ctxt, 'test_compute1',",
            "            ).numa_topology",
            "        )",
            "        self.assertEqual(set(), host_numa.cells[0].pinned_cpus)",
            "        self.assertEqual({4, 5, 6, 7}, host_numa.cells[1].pinned_cpus)",
            "",
            "        # ensure the binding details sent to \"neutron\" have been updated",
            "        port = self.neutron.show_port(",
            "            base.LibvirtNeutronFixture.network_4_port_1['id'],",
            "        )['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1515',",
            "                'pci_slot': '0000:81:00.2',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "        # ensure the binding details sent to \"neutron\" are correct",
            "        pf_port = self.neutron.show_port(pf_port['id'],)['port']",
            "        self.assertIn('binding:profile', pf_port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '8086:1528',",
            "                'pci_slot': '0000:82:06.0',",
            "                'physical_network': 'physnet4',",
            "                'device_mac_address': 'b4:96:91:34:f4:bb',",
            "            },",
            "            pf_port['binding:profile'],",
            "        )",
            "",
            "    def test_get_server_diagnostics_server_with_VF(self):",
            "        \"\"\"Ensure server disagnostics include info on VF-type PCI devices.\"\"\"",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo()",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create the SR-IOV port",
            "        port = self.neutron.create_port(",
            "            {'port': self.neutron.network_4_port_1})",
            "",
            "        flavor_id = self._create_flavor()",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'uuid': base.LibvirtNeutronFixture.network_1['id']},",
            "                {'port': port['port']['id']},",
            "            ],",
            "        )",
            "",
            "        # now check the server diagnostics to ensure the VF-type PCI device is",
            "        # attached",
            "        diagnostics = self.api.get_server_diagnostics(",
            "            server['id']",
            "        )",
            "",
            "        self.assertEqual(",
            "            base.LibvirtNeutronFixture.network_1_port_2['mac_address'],",
            "            diagnostics['nic_details'][0]['mac_address'],",
            "        )",
            "",
            "        for key in ('rx_packets', 'tx_packets'):",
            "            self.assertIn(key, diagnostics['nic_details'][0])",
            "",
            "        self.assertEqual(",
            "            base.LibvirtNeutronFixture.network_4_port_1['mac_address'],",
            "            diagnostics['nic_details'][1]['mac_address'],",
            "        )",
            "        for key in ('rx_packets', 'tx_packets'):",
            "            self.assertIn(key, diagnostics['nic_details'][1])",
            "",
            "    def test_create_server_after_change_in_nonsriov_pf_to_sriov_pf(self):",
            "        # Starts a compute with PF not configured with SRIOV capabilities",
            "        # Updates the PF with SRIOV capability and restart the compute service",
            "        # Then starts a VM with the sriov port. The VM should be in active",
            "        # state with sriov port attached.",
            "",
            "        # To emulate the device type changing, we first create a",
            "        # HostPCIDevicesInfo object with PFs and VFs. Then we make a copy",
            "        # and remove the VFs and the virt_function capability. This is",
            "        # done to ensure the physical function product id is same in both",
            "        # the versions.",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        pci_info_no_sriov = copy.deepcopy(pci_info)",
            "",
            "        # Disable SRIOV capabilties in PF and delete the VFs",
            "        self._disable_sriov_in_pf(pci_info_no_sriov)",
            "",
            "        self.start_compute('test_compute0', pci_info=pci_info_no_sriov)",
            "        self.compute = self.computes['test_compute0']",
            "",
            "        ctxt = context.get_admin_context()",
            "        pci_devices = objects.PciDeviceList.get_by_compute_node(",
            "            ctxt,",
            "            objects.ComputeNode.get_by_nodename(",
            "                ctxt, 'test_compute0',",
            "            ).id,",
            "        )",
            "        self.assertEqual(1, len(pci_devices))",
            "        self.assertEqual('type-PCI', pci_devices[0].dev_type)",
            "",
            "        # Restart the compute service with sriov PFs",
            "        self.restart_compute_service(",
            "            self.compute.host, pci_info=pci_info, keep_hypervisor_state=False)",
            "",
            "        # Verify if PCI devices are of type type-PF or type-VF",
            "        pci_devices = objects.PciDeviceList.get_by_compute_node(",
            "            ctxt,",
            "            objects.ComputeNode.get_by_nodename(",
            "                ctxt, 'test_compute0',",
            "            ).id,",
            "        )",
            "        for pci_device in pci_devices:",
            "            self.assertIn(pci_device.dev_type, ['type-PF', 'type-VF'])",
            "",
            "        # create the port",
            "        self.neutron.create_port({'port': self.neutron.network_4_port_1})",
            "",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            networks=[",
            "                {'port': base.LibvirtNeutronFixture.network_4_port_1['id']},",
            "            ],",
            "        )",
            "",
            "    def test_change_bound_port_vnic_type_kills_compute_at_restart(self):",
            "        \"\"\"Create a server with a direct port and change the vnic_type of the",
            "        bound port to macvtap. Then restart the compute service.",
            "",
            "        As the vnic_type is changed on the port but the vif_type is hwveb",
            "        instead of macvtap the vif plug logic will try to look up the netdev",
            "        of the parent VF. Howvere that VF consumed by the instance so the",
            "        netdev does not exists. This causes that the compute service will fail",
            "        with an exception during startup",
            "        \"\"\"",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=2)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # create a direct port",
            "        port = self.neutron.network_4_port_1",
            "        self.neutron.create_port({'port': port})",
            "",
            "        # create a server using the VF via neutron",
            "        server = self._create_server(networks=[{'port': port['id']}])",
            "",
            "        # update the vnic_type of the port in neutron",
            "        port = copy.deepcopy(port)",
            "        port['binding:vnic_type'] = 'macvtap'",
            "        self.neutron.update_port(port['id'], {\"port\": port})",
            "",
            "        compute = self.computes['compute1']",
            "",
            "        # Force an update on the instance info cache to ensure nova gets the",
            "        # information about the updated port",
            "        with context.target_cell(",
            "            context.get_admin_context(),",
            "            self.host_mappings['compute1'].cell_mapping",
            "        ) as cctxt:",
            "            compute.manager._heal_instance_info_cache(cctxt)",
            "",
            "        def fake_get_ifname_by_pci_address(pci_addr: str, pf_interface=False):",
            "            # we want to fail the netdev lookup only if the pci_address is",
            "            # already consumed by our instance. So we look into the instance",
            "            # definition to see if the device is attached to the instance as VF",
            "            conn = compute.manager.driver._host.get_connection()",
            "            dom = conn.lookupByUUIDString(server['id'])",
            "            dev = dom._def['devices']['nics'][0]",
            "            lookup_addr = pci_addr.replace(':', '_').replace('.', '_')",
            "            if (",
            "                dev['type'] == 'hostdev' and",
            "                dev['source'] == 'pci_' + lookup_addr",
            "            ):",
            "                # nova tried to look up the netdev of an already consumed VF.",
            "                # So we have to fail",
            "                raise exception.PciDeviceNotFoundById(id=pci_addr)",
            "",
            "        # We need to simulate the actual failure manually as in our functional",
            "        # environment all the PCI lookup is mocked. In reality nova tries to",
            "        # look up the netdev of the pci device on the host used by the port as",
            "        # the parent of the macvtap. However, as the originally direct port is",
            "        # bound to the instance, the VF pci device is already consumed by the",
            "        # instance and therefore there is no netdev for the VF.",
            "        self.libvirt.mock_get_ifname_by_pci_address.side_effect = (",
            "            fake_get_ifname_by_pci_address",
            "        )",
            "        # This is bug 1981813 as the compute service fails to start with an",
            "        # exception.",
            "        # Nova cannot prevent the vnic_type change on a bound port. Neutron",
            "        # should prevent that instead. But the nova-compute should still",
            "        # be able to start up and only log an ERROR for this instance in",
            "        # inconsistent state.",
            "        self.assertRaises(",
            "            exception.PciDeviceNotFoundById,",
            "            self.restart_compute_service, 'compute1'",
            "        )",
            "",
            "",
            "class SRIOVAttachDetachTest(_PCIServersTestBase):",
            "    # no need for aliases as these test will request SRIOV via neutron",
            "    PCI_ALIAS = []",
            "",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            \"physical_network\": \"physnet2\",",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            \"physical_network\": \"physnet2\",",
            "        },",
            "    )]",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        self.neutron = self.useFixture(nova_fixtures.NeutronFixture(self))",
            "",
            "        # add extra ports and the related network to the neutron fixture",
            "        # specifically for these tests. It cannot be added globally in the",
            "        # fixture init as it adds a second network that makes auto allocation",
            "        # based test to fail due to ambiguous networks.",
            "        self.neutron._networks[",
            "            self.neutron.network_2['id']] = self.neutron.network_2",
            "        self.neutron._subnets[",
            "            self.neutron.subnet_2['id']] = self.neutron.subnet_2",
            "        for port in [self.neutron.sriov_port, self.neutron.sriov_port2,",
            "                     self.neutron.sriov_pf_port, self.neutron.sriov_pf_port2,",
            "                     self.neutron.macvtap_port, self.neutron.macvtap_port2]:",
            "            self.neutron._ports[port['id']] = copy.deepcopy(port)",
            "",
            "    def _get_attached_port_ids(self, instance_uuid):",
            "        return [",
            "            attachment['port_id']",
            "            for attachment in self.api.get_port_interfaces(instance_uuid)]",
            "",
            "    def _detach_port(self, instance_uuid, port_id):",
            "        self.api.detach_interface(instance_uuid, port_id)",
            "        self.notifier.wait_for_versioned_notifications(",
            "            'instance.interface_detach.end')",
            "",
            "    def _attach_port(self, instance_uuid, port_id):",
            "        self.api.attach_interface(",
            "            instance_uuid,",
            "            {'interfaceAttachment': {'port_id': port_id}})",
            "        self.notifier.wait_for_versioned_notifications(",
            "            'instance.interface_attach.end')",
            "",
            "    def _test_detach_attach(self, first_port_id, second_port_id):",
            "        # This test takes two ports that requires PCI claim.",
            "        # Starts a compute with one PF and one connected VF.",
            "        # Then starts a VM with the first port. Then detach it, then",
            "        # re-attach it. These expected to be successful. Then try to attach the",
            "        # second port and asserts that it fails as no free PCI device left on",
            "        # the host.",
            "        host_info = fakelibvirt.HostInfo(cpu_nodes=2, cpu_sockets=1,",
            "                                         cpu_cores=2, cpu_threads=2)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pfs=1, num_vfs=1)",
            "        self.start_compute(",
            "            'test_compute0', host_info=host_info, pci_info=pci_info)",
            "        self.compute = self.computes['test_compute0']",
            "",
            "        # Create server with a port",
            "        server = self._create_server(networks=[{'port': first_port_id}])",
            "",
            "        updated_port = self.neutron.show_port(first_port_id)['port']",
            "        self.assertEqual('test_compute0', updated_port['binding:host_id'])",
            "        self.assertIn(first_port_id, self._get_attached_port_ids(server['id']))",
            "",
            "        self._detach_port(server['id'], first_port_id)",
            "",
            "        updated_port = self.neutron.show_port(first_port_id)['port']",
            "        self.assertIsNone(updated_port['binding:host_id'])",
            "        self.assertNotIn(",
            "            first_port_id,",
            "            self._get_attached_port_ids(server['id']))",
            "",
            "        # Attach back the port",
            "        self._attach_port(server['id'], first_port_id)",
            "",
            "        updated_port = self.neutron.show_port(first_port_id)['port']",
            "        self.assertEqual('test_compute0', updated_port['binding:host_id'])",
            "        self.assertIn(first_port_id, self._get_attached_port_ids(server['id']))",
            "",
            "        # Try to attach the second port but no free PCI device left",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException, self._attach_port, server['id'],",
            "            second_port_id)",
            "",
            "        self.assertEqual(400, ex.response.status_code)",
            "        self.assertIn('Failed to claim PCI device', str(ex))",
            "        attached_ports = self._get_attached_port_ids(server['id'])",
            "        self.assertIn(first_port_id, attached_ports)",
            "        self.assertNotIn(second_port_id, attached_ports)",
            "",
            "    def test_detach_attach_direct(self):",
            "        self._test_detach_attach(",
            "            self.neutron.sriov_port['id'], self.neutron.sriov_port2['id'])",
            "",
            "    def test_detach_macvtap(self):",
            "        self._test_detach_attach(",
            "            self.neutron.macvtap_port['id'],",
            "            self.neutron.macvtap_port2['id'])",
            "",
            "    def test_detach_direct_physical(self):",
            "        self._test_detach_attach(",
            "            self.neutron.sriov_pf_port['id'],",
            "            self.neutron.sriov_pf_port2['id'])",
            "",
            "",
            "class VDPAServersTest(_PCIServersWithMigrationTestBase):",
            "",
            "    # this is needed for os_compute_api:os-migrate-server:migrate policy",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    # Whitelist both the PF and VF; in reality, you probably wouldn't do this",
            "    # but we want to make sure that the PF is correctly taken off the table",
            "    # once any VF is used",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '101d',",
            "            'physical_network': 'physnet4',",
            "        },",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '101e',",
            "            'physical_network': 'physnet4',",
            "        },",
            "    )]",
            "    # No need for aliases as these test will request SRIOV via neutron",
            "    PCI_ALIAS = []",
            "",
            "    NUM_PFS = 1",
            "    NUM_VFS = 4",
            "",
            "    FAKE_LIBVIRT_VERSION = 6_009_000  # 6.9.0",
            "    FAKE_QEMU_VERSION = 5_001_000  # 5.1.0",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        # The ultimate base class _IntegratedTestBase uses NeutronFixture but",
            "        # we need a bit more intelligent neutron for these tests. Applying the",
            "        # new fixture here means that we re-stub what the previous neutron",
            "        # fixture already stubbed.",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "",
            "    def start_vdpa_compute(self, hostname='compute-0'):",
            "        vf_ratio = self.NUM_VFS // self.NUM_PFS",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pci=0, num_pfs=0, num_vfs=0)",
            "        vdpa_info = fakelibvirt.HostVDPADevicesInfo()",
            "",
            "        pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x6,",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=40,  # totally arbitrary number",
            "            numa_node=0,",
            "            vf_ratio=vf_ratio,",
            "            vend_id='15b3',",
            "            vend_name='Mellanox Technologies',",
            "            prod_id='101d',",
            "            prod_name='MT2892 Family [ConnectX-6 Dx]',",
            "            driver_name='mlx5_core')",
            "",
            "        for idx in range(self.NUM_VFS):",
            "            vf = pci_info.add_device(",
            "                dev_type='VF',",
            "                bus=0x6,",
            "                slot=0x0,",
            "                function=idx + 1,",
            "                iommu_group=idx + 41,  # totally arbitrary number + offset",
            "                numa_node=0,",
            "                vf_ratio=vf_ratio,",
            "                parent=(0x6, 0x0, 0),",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='101e',",
            "                prod_name='ConnectX Family mlx5Gen Virtual Function',",
            "                driver_name='mlx5_core')",
            "            vdpa_info.add_device(f'vdpa_vdpa{idx}', idx, vf)",
            "",
            "        return super().start_compute(hostname=hostname,",
            "            pci_info=pci_info, vdpa_info=vdpa_info,",
            "            libvirt_version=self.FAKE_LIBVIRT_VERSION,",
            "            qemu_version=self.FAKE_QEMU_VERSION)",
            "",
            "    def create_vdpa_port(self):",
            "        vdpa_port = {",
            "            'id': uuids.vdpa_port,",
            "            'network_id': self.neutron.network_4['id'],",
            "            'status': 'ACTIVE',",
            "            'mac_address': 'b5:bc:2e:e7:51:ee',",
            "            'fixed_ips': [",
            "                {",
            "                    'ip_address': '192.168.4.6',",
            "                    'subnet_id': self.neutron.subnet_4['id']",
            "                }",
            "            ],",
            "            'binding:vif_details': {},",
            "            'binding:vif_type': 'ovs',",
            "            'binding:vnic_type': 'vdpa',",
            "        }",
            "",
            "        # create the port",
            "        self.neutron.create_port({'port': vdpa_port})",
            "        return vdpa_port",
            "",
            "    def test_create_server(self):",
            "        \"\"\"Create an instance using a neutron-provisioned vDPA VIF.\"\"\"",
            "",
            "        orig_create = nova.virt.libvirt.guest.Guest.create",
            "",
            "        def fake_create(cls, xml, host):",
            "            tree = etree.fromstring(xml)",
            "            elem = tree.find('./devices/interface/[@type=\"vdpa\"]')",
            "",
            "            # compare source device",
            "            # the MAC address is derived from the neutron port, while the",
            "            # source dev path assumes we attach vDPA devs in order",
            "            expected = \"\"\"",
            "                <interface type=\"vdpa\">",
            "                  <mac address=\"b5:bc:2e:e7:51:ee\"/>",
            "                  <source dev=\"/dev/vhost-vdpa-3\"/>",
            "                </interface>\"\"\"",
            "            actual = etree.tostring(elem, encoding='unicode')",
            "",
            "            self.assertXmlEqual(expected, actual)",
            "",
            "            return orig_create(xml, host)",
            "",
            "        self.stub_out(",
            "            'nova.virt.libvirt.guest.Guest.create',",
            "            fake_create,",
            "        )",
            "",
            "        hostname = self.start_vdpa_compute()",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "",
            "        # both the PF and VF with vDPA capabilities (dev_type=vdpa) should have",
            "        # been counted",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        # create the port",
            "        vdpa_port = self.create_vdpa_port()",
            "",
            "        # ensure the binding details are currently unset",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        # create a server using the vDPA device via neutron",
            "        self._create_server(networks=[{'port': vdpa_port['id']}])",
            "",
            "        # ensure there is one less VF available and that the PF is no longer",
            "        # usable",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:06:00.4',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def _create_port_and_server(self):",
            "        # create the port and a server, with the port attached to the server",
            "        vdpa_port = self.create_vdpa_port()",
            "        server = self._create_server(networks=[{'port': vdpa_port['id']}])",
            "        return vdpa_port, server",
            "",
            "    def _test_common(self, op, *args, **kwargs):",
            "        self.start_vdpa_compute()",
            "",
            "        vdpa_port, server = self._create_port_and_server()",
            "",
            "        # attempt the unsupported action and ensure it fails",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            op, server, *args, **kwargs)",
            "        self.assertIn(",
            "            'not supported for instance with vDPA ports',",
            "            ex.response.text)",
            "",
            "    def test_attach_interface_service_version_61(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=61",
            "        ):",
            "            self._test_common(self._attach_interface, uuids.vdpa_port)",
            "",
            "    def test_attach_interface(self):",
            "        hostname = self.start_vdpa_compute()",
            "        # create the port and a server, but don't attach the port to the server",
            "        # yet",
            "        server = self._create_server(networks='none')",
            "        vdpa_port = self.create_vdpa_port()",
            "        # attempt to attach the port to the server",
            "        self._attach_interface(server, vdpa_port['id'])",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:06:00.4',",
            "                'physical_network': 'physnet4',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "        self.assertEqual(server['id'], port['device_id'])",
            "",
            "    def test_detach_interface_service_version_61(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=61",
            "        ):",
            "            self._test_common(self._detach_interface, uuids.vdpa_port)",
            "",
            "    def test_detach_interface(self):",
            "        self.start_vdpa_compute()",
            "        vdpa_port, server = self._create_port_and_server()",
            "        # ensure the binding details sent to \"neutron\" were correct",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(server['id'], port['device_id'])",
            "        self._detach_interface(server, vdpa_port['id'])",
            "        # ensure the port is no longer owned by the vm",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual('', port['device_id'])",
            "        self.assertEqual({}, port['binding:profile'])",
            "",
            "    def test_shelve_offload(self):",
            "        hostname = self.start_vdpa_compute()",
            "        vdpa_port, server = self._create_port_and_server()",
            "        # assert the port is bound to the vm and the compute host",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(server['id'], port['device_id'])",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        # -2 we claim the vdpa device which make the parent PF unavailable",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "        server = self._shelve_server(server)",
            "        # now that the vm is shelve offloaded it should not be bound",
            "        # to any host but should still be owned by the vm",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(server['id'], port['device_id'])",
            "        # FIXME(sean-k-mooney): we should be unbinding the port from",
            "        # the host when we shelve offload but we don't today.",
            "        # This is unrelated to vdpa port and is a general issue.",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "        self.assertIn('binding:profile', port)",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "    def test_unshelve_to_same_host(self):",
            "        hostname = self.start_vdpa_compute()",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            hostname, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "",
            "        server = self._shelve_server(server)",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        # FIXME(sean-k-mooney): shelve  offload should unbind the port",
            "        # self.assertEqual('', port['binding:host_id'])",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "",
            "        server = self._unshelve_server(server)",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            hostname, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(hostname, port['binding:host_id'])",
            "",
            "    def test_unshelve_to_different_host(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(source, port['binding:host_id'])",
            "",
            "        server = self._shelve_server(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertIsNone(server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        # FIXME(sean-k-mooney): shelve should unbind the port",
            "        # self.assertEqual('', port['binding:host_id'])",
            "        self.assertEqual(source, port['binding:host_id'])",
            "",
            "        # force the unshelve to the other host",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'status': 'disabled'})",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "        server = self._unshelve_server(server)",
            "        # the dest devices should be claimed",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "        # and the source host devices should still be free",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertEqual(",
            "            dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(dest, port['binding:host_id'])",
            "",
            "    def test_evacute(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(source, port['binding:host_id'])",
            "",
            "        # stop the source compute and enable the dest",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        self.computes['source'].stop()",
            "        # Down the source compute to enable the evacuation",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'forced_down': True})",
            "",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "        server = self._evacuate_server(server)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "        port = self.neutron.show_port(vdpa_port['id'])['port']",
            "        self.assertEqual(dest, port['binding:host_id'])",
            "",
            "        # as the source compute is offline the pci claims will not be cleaned",
            "        # up on the source compute.",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        # but if you fix/restart the source node the allocations for evacuated",
            "        # instances should be released.",
            "        self.restart_compute_service(source)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "",
            "    def test_resize_same_host(self):",
            "        self.flags(allow_resize_to_same_host=True)",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        source = self.start_vdpa_compute()",
            "        vdpa_port, server = self._create_port_and_server()",
            "        # before we resize the vm should be using 1 VF but that will mark",
            "        # the PF as unavailable so we assert 2 devices are in use.",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        flavor_id = self._create_flavor(name='new-flavor')",
            "        self.assertNotEqual(server['flavor']['original_name'], 'new-flavor')",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._resize_server(server, flavor_id)",
            "            self.assertEqual(",
            "                server['flavor']['original_name'], 'new-flavor')",
            "            # in resize verify the VF claims should be doubled even",
            "            # for same host resize so assert that 3 are in devices in use",
            "            # 1 PF and 2 VFs .",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 3)",
            "            server = self._confirm_resize(server)",
            "            # but once we confrim it should be reduced back to 1 PF and 1 VF",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            # assert the hostname has not have changed as part",
            "            # of the resize.",
            "            self.assertEqual(",
            "                source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_resize_different_host(self):",
            "        self.flags(allow_resize_to_same_host=False)",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        flavor_id = self._create_flavor(name='new-flavor')",
            "        self.assertNotEqual(server['flavor']['original_name'], 'new-flavor')",
            "        # disable the source compute and enable the dest",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'status': 'disabled'})",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._resize_server(server, flavor_id)",
            "            self.assertEqual(",
            "                server['flavor']['original_name'], 'new-flavor')",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            server = self._confirm_resize(server)",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            self.assertEqual(",
            "                dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_resize_revert(self):",
            "        self.flags(allow_resize_to_same_host=False)",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        flavor_id = self._create_flavor(name='new-flavor')",
            "        self.assertNotEqual(server['flavor']['original_name'], 'new-flavor')",
            "        # disable the source compute and enable the dest",
            "        self.api.put_service(",
            "            self.computes['source'].service_ref.uuid, {'status': 'disabled'})",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._resize_server(server, flavor_id)",
            "            self.assertEqual(",
            "                server['flavor']['original_name'], 'new-flavor')",
            "            # in resize verify both the dest and source pci claims should be",
            "            # present.",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            server = self._revert_resize(server)",
            "            # but once we revert the dest claims should be freed.",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertEqual(",
            "                source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_cold_migrate(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        # enable the dest we do not need to disable the source since cold",
            "        # migrate wont happen to the same host in the libvirt driver",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            server = self._migrate_server(server)",
            "            self.assertEqual(",
            "                dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            server = self._confirm_resize(server)",
            "            self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "            self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "            self.assertEqual(",
            "                dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "    def test_suspend_and_resume_service_version_62(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=62",
            "        ):",
            "            self._test_common(self._suspend_server)",
            "",
            "    def test_suspend_and_resume(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        vdpa_port, server = self._create_port_and_server()",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        server = self._suspend_server(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual('SUSPENDED', server['status'])",
            "        server = self._resume_server(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual('ACTIVE', server['status'])",
            "",
            "    def test_live_migrate_service_version_62(self):",
            "        with mock.patch(",
            "                \"nova.objects.service.get_minimum_version_all_cells\",",
            "                return_value=62",
            "        ):",
            "            self._test_common(self._live_migrate)",
            "",
            "    def test_live_migrate(self):",
            "        source = self.start_vdpa_compute(hostname='source')",
            "        dest = self.start_vdpa_compute(hostname='dest')",
            "",
            "        num_pci = self.NUM_PFS + self.NUM_VFS",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci)",
            "",
            "        # ensure we boot the vm on the \"source\" compute",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'disabled'})",
            "        vdpa_port, server = self._create_port_and_server()",
            "        self.assertEqual(",
            "            source, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci - 2)",
            "        # enable the dest we do not need to disable the source since cold",
            "        # migrate wont happen to the same host in the libvirt driver",
            "        self.api.put_service(",
            "            self.computes['dest'].service_ref.uuid, {'status': 'enabled'})",
            "",
            "        with mock.patch(",
            "                'nova.virt.libvirt.LibvirtDriver.'",
            "                '_detach_direct_passthrough_vifs'",
            "        ):",
            "            server = self._live_migrate(server)",
            "        self.assertPCIDeviceCounts(source, total=num_pci, free=num_pci)",
            "        self.assertPCIDeviceCounts(dest, total=num_pci, free=num_pci - 2)",
            "        self.assertEqual(",
            "            dest, server['OS-EXT-SRV-ATTR:hypervisor_hostname'])",
            "",
            "",
            "class PCIServersTest(_PCIServersTestBase):",
            "",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "        }",
            "    )]",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "        }",
            "    )]",
            "    PCI_RC = f\"CUSTOM_PCI_{fakelibvirt.PCI_VEND_ID}_{fakelibvirt.PCI_PROD_ID}\"",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.flags(group=\"pci\", report_in_placement=True)",
            "",
            "    def test_create_server_with_pci_dev_and_numa(self):",
            "        \"\"\"Verifies that an instance can be booted with cpu pinning and with an",
            "           assigned pci device.",
            "        \"\"\"",
            "",
            "        self.flags(cpu_dedicated_set='0-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=1)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        self.assert_placement_pci_view(",
            "            \"compute1\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        # create a flavor",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "    def test_create_server_with_pci_dev_and_numa_fails(self):",
            "        \"\"\"This test ensures that it is not possible to allocated CPU and",
            "           memory resources from one NUMA node and a PCI device from another.",
            "        \"\"\"",
            "",
            "        self.flags(cpu_dedicated_set='0-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=0)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        self.assert_placement_pci_view(",
            "            \"compute1\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        # boot one instance with no PCI device to \"fill up\" NUMA node 0",
            "        extra_spec = {'hw:cpu_policy': 'dedicated'}",
            "        flavor_id = self._create_flavor(vcpu=4, extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now boot one with a PCI device, which should fail to boot",
            "        extra_spec['pci_passthrough:alias'] = '%s:1' % self.ALIAS_NAME",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(",
            "            flavor_id=flavor_id, networks='none', expected_state='ERROR')",
            "",
            "    def test_live_migrate_server_with_pci(self):",
            "        \"\"\"Live migrate an instance with a PCI passthrough device.",
            "",
            "        This should fail because it's not possible to live migrate an instance",
            "        with a PCI passthrough device, even if it's a SR-IOV VF.",
            "        \"\"\"",
            "",
            "        # start two compute services",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "",
            "        self.assert_placement_pci_view(",
            "            \"test_compute0\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        self.start_compute(",
            "            hostname='test_compute1',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "",
            "        self.assert_placement_pci_view(",
            "            \"test_compute1\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "",
            "        # create a server",
            "        extra_spec = {'pci_passthrough:alias': f'{self.ALIAS_NAME}:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=flavor_id, networks='none')",
            "",
            "        # now live migrate that server",
            "        ex = self.assertRaises(",
            "            client.OpenStackApiException,",
            "            self._live_migrate,",
            "            server, 'completed')",
            "        # NOTE(stephenfin): this wouldn't happen in a real deployment since",
            "        # live migration is a cast, but since we are using CastAsCallFixture",
            "        # this will bubble to the API",
            "        self.assertEqual(500, ex.response.status_code)",
            "        self.assertIn('NoValidHost', str(ex))",
            "",
            "    def test_resize_pci_to_vanilla(self):",
            "        # Start two computes, one with PCI and one without.",
            "        self.start_compute(",
            "            hostname='test_compute0',",
            "            pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "        self.assert_placement_pci_view(",
            "            \"test_compute0\",",
            "            inventories={\"0000:81:00.0\": {self.PCI_RC: 1}},",
            "            traits={\"0000:81:00.0\": []},",
            "        )",
            "        self.start_compute(hostname='test_compute1')",
            "        self.assert_placement_pci_view(",
            "            \"test_compute1\",",
            "            inventories={},",
            "            traits={},",
            "        )",
            "",
            "        # Boot a server with a single PCI device.",
            "        extra_spec = {'pci_passthrough:alias': f'{self.ALIAS_NAME}:1'}",
            "        pci_flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(flavor_id=pci_flavor_id, networks='none')",
            "",
            "        # Resize it to a flavor without PCI devices. We expect this to work, as",
            "        # test_compute1 is available.",
            "        flavor_id = self._create_flavor()",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off',",
            "            return_value='{}',",
            "        ):",
            "            self._resize_server(server, flavor_id)",
            "        self._confirm_resize(server)",
            "        self.assertPCIDeviceCounts('test_compute0', total=1, free=1)",
            "        self.assertPCIDeviceCounts('test_compute1', total=0, free=0)",
            "",
            "    def _confirm_resize(self, server, host='host1'):",
            "        # NOTE(sbauza): Unfortunately, _cleanup_resize() in libvirt checks the",
            "        # host option to know the source hostname but given we have a global",
            "        # CONF, the value will be the hostname of the last compute service that",
            "        # was created, so we need to change it here.",
            "        # TODO(sbauza): Remove the below once we stop using CONF.host in",
            "        # libvirt and rather looking at the compute host value.",
            "        orig_host = CONF.host",
            "        self.flags(host=host)",
            "        super()._confirm_resize(server)",
            "        self.flags(host=orig_host)",
            "",
            "    def test_cold_migrate_server_with_pci(self):",
            "",
            "        host_devices = {}",
            "        orig_create = nova.virt.libvirt.guest.Guest.create",
            "",
            "        def fake_create(cls, xml, host):",
            "            tree = etree.fromstring(xml)",
            "            elem = tree.find('./devices/hostdev/source/address')",
            "",
            "            hostname = host.get_hostname()",
            "            address = (",
            "                elem.get('bus'), elem.get('slot'), elem.get('function'),",
            "            )",
            "            if hostname in host_devices:",
            "                self.assertNotIn(address, host_devices[hostname])",
            "            else:",
            "                host_devices[hostname] = []",
            "            host_devices[host.get_hostname()].append(address)",
            "",
            "            return orig_create(xml, host)",
            "",
            "        self.stub_out(",
            "            'nova.virt.libvirt.guest.Guest.create',",
            "            fake_create,",
            "        )",
            "",
            "        # start two compute services",
            "        for hostname in ('test_compute0', 'test_compute1'):",
            "            pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=2)",
            "            self.start_compute(hostname=hostname, pci_info=pci_info)",
            "            self.assert_placement_pci_view(",
            "                hostname,",
            "                inventories={",
            "                    \"0000:81:00.0\": {self.PCI_RC: 1},",
            "                    \"0000:81:01.0\": {self.PCI_RC: 1},",
            "                },",
            "                traits={",
            "                    \"0000:81:00.0\": [],",
            "                    \"0000:81:01.0\": [],",
            "                },",
            "            )",
            "",
            "        # boot an instance with a PCI device on each host",
            "        extra_spec = {",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "",
            "        server_a = self._create_server(",
            "            flavor_id=flavor_id, networks='none', host='test_compute0')",
            "        server_b = self._create_server(",
            "            flavor_id=flavor_id, networks='none', host='test_compute1')",
            "",
            "        # the instances should have landed on separate hosts; ensure both hosts",
            "        # have one used PCI device and one free PCI device",
            "        self.assertNotEqual(",
            "            server_a['OS-EXT-SRV-ATTR:host'], server_b['OS-EXT-SRV-ATTR:host'],",
            "        )",
            "        for hostname in ('test_compute0', 'test_compute1'):",
            "            self.assertPCIDeviceCounts(hostname, total=2, free=1)",
            "",
            "        # TODO(stephenfin): The mock of 'migrate_disk_and_power_off' should",
            "        # probably be less...dumb",
            "        with mock.patch(",
            "            'nova.virt.libvirt.driver.LibvirtDriver'",
            "            '.migrate_disk_and_power_off', return_value='{}',",
            "        ):",
            "            # TODO(stephenfin): Use a helper",
            "            self.api.post_server_action(server_a['id'], {'migrate': None})",
            "            server_a = self._wait_for_state_change(server_a, 'VERIFY_RESIZE')",
            "",
            "        # the instances should now be on the same host; ensure the source host",
            "        # still has one used PCI device while the destination now has two used",
            "        # test_compute0 initially",
            "        self.assertEqual(",
            "            server_a['OS-EXT-SRV-ATTR:host'], server_b['OS-EXT-SRV-ATTR:host'],",
            "        )",
            "        self.assertPCIDeviceCounts('test_compute0', total=2, free=1)",
            "        self.assertPCIDeviceCounts('test_compute1', total=2, free=0)",
            "",
            "        # now, confirm the migration and check our counts once again",
            "        self._confirm_resize(server_a)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=2, free=2)",
            "        self.assertPCIDeviceCounts('test_compute1', total=2, free=0)",
            "",
            "    def test_request_two_pci_but_host_has_one(self):",
            "        # simulate a single type-PCI device on the host",
            "        self.start_compute(pci_info=fakelibvirt.HostPCIDevicesInfo(num_pci=1))",
            "        self.assertPCIDeviceCounts('compute1', total=1, free=1)",
            "",
            "        alias = [jsonutils.dumps(x) for x in (",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': 'a1',",
            "            },",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': 'a2',",
            "            },",
            "        )]",
            "        self.flags(group='pci', alias=alias)",
            "        # request two PCI devices both are individually matching with the",
            "        # single available device on the host",
            "        extra_spec = {'pci_passthrough:alias': 'a1:1,a2:1'}",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        # so we expect that the boot fails with no valid host error as only",
            "        # one of the requested PCI device can be allocated",
            "        server = self._create_server(",
            "            flavor_id=flavor_id, networks=\"none\", expected_state='ERROR')",
            "        self.assertIn('fault', server)",
            "        self.assertIn('No valid host', server['fault']['message'])",
            "",
            "",
            "class PCIServersWithPreferredNUMATest(_PCIServersTestBase):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "        }",
            "    )]",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.PREFERRED,",
            "        }",
            "    )]",
            "    expected_state = 'ACTIVE'",
            "",
            "    def test_create_server_with_pci_dev_and_numa(self):",
            "        \"\"\"Validate behavior of 'preferred' PCI NUMA policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if* PCI",
            "        NUMA policies are in use.",
            "        \"\"\"",
            "",
            "        self.flags(cpu_dedicated_set='0-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=0)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # boot one instance with no PCI device to \"fill up\" NUMA node 0",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(vcpu=4, extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id)",
            "",
            "        # now boot one with a PCI device, which should succeed thanks to the",
            "        # use of the PCI policy",
            "        extra_spec['pci_passthrough:alias'] = '%s:1' % self.ALIAS_NAME",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(",
            "            flavor_id=flavor_id, expected_state=self.expected_state)",
            "",
            "",
            "class PCIServersWithRequiredNUMATest(PCIServersWithPreferredNUMATest):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.REQUIRED,",
            "        }",
            "    )]",
            "    expected_state = 'ERROR'",
            "",
            "",
            "@ddt.ddt",
            "class PCIServersWithSRIOVAffinityPoliciesTest(_PCIServersTestBase):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "        }",
            "    )]",
            "    # we set the numa_affinity policy to required to ensure strict affinity",
            "    # between pci devices and the guest cpu and memory will be enforced.",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.REQUIRED,",
            "        }",
            "    )]",
            "",
            "    # NOTE(sean-k-mooney): i could just apply the ddt decorators",
            "    # to this function for the most part but i have chosen to",
            "    # keep one top level function per policy to make documenting",
            "    # the test cases simpler.",
            "    def _test_policy(self, pci_numa_node, status, policy):",
            "        # only allow cpus on numa node 1 to be used for pinning",
            "        self.flags(cpu_dedicated_set='4-7', group='compute')",
            "",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pci=1, numa_node=pci_numa_node)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # request cpu pinning to create a numa topology and allow the test to",
            "        # force which numa node the vm would have to be pinned too.",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': policy",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id, expected_state=status)",
            "",
            "        if status == 'ACTIVE':",
            "            self.assertTrue(self.mock_filter.called)",
            "        else:",
            "            # the PciPassthroughFilter should not have been called, since the",
            "            # NUMATopologyFilter should have eliminated the filter first",
            "            self.assertFalse(self.mock_filter.called)",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the preferred policy should always pass regardless of numa affinity",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ACTIVE'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_preferred(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'preferred' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the SR-IOV NUMA affinity policy is set to preferred.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'preferred')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the legacy policy allow a PCI device to be used if it has NUMA",
            "    # affinity or if no NUMA info is available so we set the NUMA",
            "    # node for this device to -1 which is the sentinel value use by the",
            "    # Linux kernel for a device with no NUMA affinity.",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_legacy(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'legacy' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the SR-IOV NUMA affinity policy is set to legacy and the device",
            "        does not report NUMA information.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'legacy')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # The required policy requires a PCI device to both report a NUMA",
            "    # and for the guest cpus and ram to be affinitized to the same",
            "    # NUMA node so we create 1 pci device in the first NUMA node.",
            "    @ddt.data((-1, 'ERROR'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_required(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'required' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is not* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the SR-IOV NUMA affinity policy is set to required and the device",
            "        does reports NUMA information.",
            "        \"\"\"",
            "",
            "        # we set the numa_affinity policy to preferred to allow the PCI device",
            "        # to be selected from any numa node so we can prove the flavor",
            "        # overrides the alias.",
            "        alias = [jsonutils.dumps(",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': self.ALIAS_NAME,",
            "                'device_type': fields.PciDeviceType.STANDARD,",
            "                'numa_policy': fields.PCINUMAAffinityPolicy.PREFERRED,",
            "            }",
            "        )]",
            "",
            "        self.flags(",
            "            device_spec=self.PCI_DEVICE_SPEC,",
            "            alias=alias,",
            "            group='pci'",
            "        )",
            "",
            "        self._test_policy(pci_numa_node, status, 'required')",
            "",
            "    def test_socket_policy_pass(self):",
            "        # With 1 socket containing 2 NUMA nodes, make the first node's CPU",
            "        # available for pinning, but affine the PCI device to the second node.",
            "        # This should pass.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=1, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=1)",
            "",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': 'socket'",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id)",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_socket_policy_fail(self):",
            "        # With 2 sockets containing 1 NUMA node each, make the first socket's",
            "        # CPUs available for pinning, but affine the PCI device to the second",
            "        # NUMA node in the second socket. This should fail.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=1, cpu_sockets=2, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=1)",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': 'socket'",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        server = self._create_server(",
            "            flavor_id=flavor_id, expected_state='ERROR')",
            "        self.assertIn('fault', server)",
            "        self.assertIn('No valid host', server['fault']['message'])",
            "",
            "    def test_socket_policy_multi_numa_pass(self):",
            "        # 2 sockets, 2 NUMA nodes each, with the PCI device on NUMA 0 and",
            "        # socket 0. If we restrict cpu_dedicated_set to NUMA 1, 2 and 3, we",
            "        # should still be able to boot an instance with hw:numa_nodes=3 and the",
            "        # `socket` policy, because one of the instance's NUMA nodes will be on",
            "        # the same socket as the PCI device (even if there is no direct NUMA",
            "        # node affinity).",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=2, cpu_cores=2, cpu_threads=1,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        self.flags(cpu_dedicated_set='2-7', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(num_pci=1, numa_node=0)",
            "",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:numa_nodes': '3',",
            "            'hw:cpu_policy': 'dedicated',",
            "            'pci_passthrough:alias': '%s:1' % self.ALIAS_NAME,",
            "            'hw:pci_numa_affinity_policy': 'socket'",
            "        }",
            "        flavor_id = self._create_flavor(vcpu=6, memory_mb=3144,",
            "                                        extra_spec=extra_spec)",
            "        self._create_server(flavor_id=flavor_id)",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "",
            "@ddt.ddt",
            "class PCIServersWithPortNUMAPoliciesTest(_PCIServersTestBase):",
            "",
            "    ALIAS_NAME = 'a1'",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.VF_PROD_ID,",
            "            'physical_network': 'physnet4',",
            "        },",
            "    )]",
            "    # we set the numa_affinity policy to required to ensure strict affinity",
            "    # between pci devices and the guest cpu and memory will be enforced.",
            "    PCI_ALIAS = [jsonutils.dumps(",
            "        {",
            "            'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "            'product_id': fakelibvirt.PCI_PROD_ID,",
            "            'name': ALIAS_NAME,",
            "            'device_type': fields.PciDeviceType.STANDARD,",
            "            'numa_policy': fields.PCINUMAAffinityPolicy.REQUIRED,",
            "        }",
            "    )]",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "",
            "        # The ultimate base class _IntegratedTestBase uses NeutronFixture but",
            "        # we need a bit more intelligent neutron for these tests. Applying the",
            "        # new fixture here means that we re-stub what the previous neutron",
            "        # fixture already stubbed.",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "        self.flags(disable_fallback_pcpu_query=True, group='workarounds')",
            "",
            "    def _create_port_with_policy(self, policy):",
            "        port_data = copy.deepcopy(",
            "            base.LibvirtNeutronFixture.network_4_port_1)",
            "        port_data[constants.NUMA_POLICY] = policy",
            "        # create the port",
            "        new_port = self.neutron.create_port({'port': port_data})",
            "        port_id = new_port['port']['id']",
            "        port = self.neutron.show_port(port_id)['port']",
            "        self.assertEqual(port[constants.NUMA_POLICY], policy)",
            "        return port_id",
            "",
            "    # NOTE(sean-k-mooney): i could just apply the ddt decorators",
            "    # to this function for the most part but i have chosen to",
            "    # keep one top level function per policy to make documenting",
            "    # the test cases simpler.",
            "    def _test_policy(self, pci_numa_node, status, policy):",
            "        # only allow cpus on numa node 1 to be used for pinning",
            "        self.flags(cpu_dedicated_set='4-7', group='compute')",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=2, numa_node=pci_numa_node)",
            "        self.start_compute(pci_info=pci_info)",
            "",
            "        # request cpu pinning to create a numa topology and allow the test to",
            "        # force which numa node the vm would have to be pinned too.",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "",
            "        port_id = self._create_port_with_policy(policy)",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "            expected_state=status",
            "        )",
            "",
            "        if status == 'ACTIVE':",
            "            self.assertTrue(self.mock_filter.called)",
            "        else:",
            "            # the PciPassthroughFilter should not have been called, since the",
            "            # NUMATopologyFilter should have eliminated the filter first",
            "            self.assertFalse(self.mock_filter.called)",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the preferred policy should always pass regardless of numa affinity",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ACTIVE'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_preferred(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'preferred' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the port NUMA affinity policy is set to preferred.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'preferred')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # the legacy policy allow a PCI device to be used if it has NUMA",
            "    # affinity or if no NUMA info is available so we set the NUMA",
            "    # node for this device to -1 which is the sentinel value use by the",
            "    # Linux kernel for a device with no NUMA affinity.",
            "    @ddt.data((-1, 'ACTIVE'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_legacy(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'legacy' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the port NUMA affinity policy is set to legacy and the device",
            "        does not report NUMA information.",
            "        \"\"\"",
            "        self._test_policy(pci_numa_node, status, 'legacy')",
            "",
            "    @ddt.unpack  # unpacks each sub-tuple e.g. *(pci_numa_node, status)",
            "    # The required policy requires a PCI device to both report a NUMA",
            "    # and for the guest cpus and ram to be affinitized to the same",
            "    # NUMA node so we create 1 pci device in the first NUMA node.",
            "    @ddt.data((-1, 'ERROR'), (0, 'ERROR'), (1, 'ACTIVE'))",
            "    def test_create_server_with_sriov_numa_affinity_policy_required(",
            "            self, pci_numa_node, status):",
            "        \"\"\"Validate behavior of 'required' PCI NUMA affinity policy.",
            "",
            "        This test ensures that it *is not* possible to allocate CPU and memory",
            "        resources from one NUMA node and a PCI device from another *if*",
            "        the port NUMA affinity policy is set to required and the device",
            "        does reports NUMA information.",
            "        \"\"\"",
            "",
            "        # we set the numa_affinity policy to preferred to allow the PCI device",
            "        # to be selected from any numa node so we can prove the flavor",
            "        # overrides the alias.",
            "        alias = [jsonutils.dumps(",
            "            {",
            "                'vendor_id': fakelibvirt.PCI_VEND_ID,",
            "                'product_id': fakelibvirt.PCI_PROD_ID,",
            "                'name': self.ALIAS_NAME,",
            "                'device_type': fields.PciDeviceType.STANDARD,",
            "                'numa_policy': fields.PCINUMAAffinityPolicy.PREFERRED,",
            "            }",
            "        )]",
            "",
            "        self.flags(",
            "            device_spec=self.PCI_DEVICE_SPEC,",
            "            alias=alias,",
            "            group='pci'",
            "        )",
            "",
            "        self._test_policy(pci_numa_node, status, 'required')",
            "",
            "    def test_socket_policy_pass(self):",
            "        # With 1 socket containing 2 NUMA nodes, make the first node's CPU",
            "        # available for pinning, but affine the PCI device to the second node.",
            "        # This should pass.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=1, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=1, numa_node=1)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        port_id = self._create_port_with_policy('socket')",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "        )",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "    def test_socket_policy_fail(self):",
            "        # With 2 sockets containing 1 NUMA node each, make the first socket's",
            "        # CPUs available for pinning, but affine the PCI device to the second",
            "        # NUMA node in the second socket. This should fail.",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=1, cpu_sockets=2, cpu_cores=2, cpu_threads=2,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=1, numa_node=1)",
            "        self.flags(cpu_dedicated_set='0-3', group='compute')",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(extra_spec=extra_spec)",
            "        port_id = self._create_port_with_policy('socket')",
            "        # create a server using the VF via neutron",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "            expected_state='ERROR'",
            "        )",
            "        self.assertIn('fault', server)",
            "        self.assertIn('No valid host', server['fault']['message'])",
            "        self.assertFalse(self.mock_filter.called)",
            "",
            "    def test_socket_policy_multi_numa_pass(self):",
            "        # 2 sockets, 2 NUMA nodes each, with the PCI device on NUMA 0 and",
            "        # socket 0. If we restrict cpu_dedicated_set to NUMA 1, 2 and 3, we",
            "        # should still be able to boot an instance with hw:numa_nodes=3 and the",
            "        # `socket` policy, because one of the instance's NUMA nodes will be on",
            "        # the same socket as the PCI device (even if there is no direct NUMA",
            "        # node affinity).",
            "        host_info = fakelibvirt.HostInfo(",
            "            cpu_nodes=2, cpu_sockets=2, cpu_cores=2, cpu_threads=1,",
            "            kB_mem=(16 * units.Gi) // units.Ki)",
            "        pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=1, num_vfs=1, numa_node=0)",
            "        self.flags(cpu_dedicated_set='2-7', group='compute')",
            "        self.start_compute(host_info=host_info, pci_info=pci_info)",
            "",
            "        extra_spec = {",
            "            'hw:numa_nodes': '3',",
            "            'hw:cpu_policy': 'dedicated',",
            "        }",
            "        flavor_id = self._create_flavor(vcpu=6, memory_mb=3144,",
            "                                        extra_spec=extra_spec)",
            "        port_id = self._create_port_with_policy('socket')",
            "        # create a server using the VF via neutron",
            "        self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[",
            "                {'port': port_id},",
            "            ],",
            "        )",
            "        self.assertTrue(self.mock_filter.called)",
            "",
            "",
            "class RemoteManagedServersTest(_PCIServersWithMigrationTestBase):",
            "",
            "    ADMIN_API = True",
            "    microversion = 'latest'",
            "",
            "    PCI_DEVICE_SPEC = [jsonutils.dumps(x) for x in (",
            "        # A PF with access to physnet4.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': 'a2dc',",
            "            'physical_network': 'physnet4',",
            "            'remote_managed': 'false',",
            "        },",
            "        # A VF with access to physnet4.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '1021',",
            "            'physical_network': 'physnet4',",
            "            'remote_managed': 'true',",
            "        },",
            "        # A PF programmed to forward traffic to an overlay network.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': 'a2d6',",
            "            'physical_network': None,",
            "            'remote_managed': 'false',",
            "        },",
            "        # A VF programmed to forward traffic to an overlay network.",
            "        {",
            "            'vendor_id': '15b3',",
            "            'product_id': '101e',",
            "            'physical_network': None,",
            "            'remote_managed': 'true',",
            "        },",
            "    )]",
            "",
            "    PCI_ALIAS = []",
            "",
            "    NUM_PFS = 1",
            "    NUM_VFS = 4",
            "    vf_ratio = NUM_VFS // NUM_PFS",
            "",
            "    # Min Libvirt version that supports working with PCI VPD.",
            "    FAKE_LIBVIRT_VERSION = 7_009_000  # 7.9.0",
            "    FAKE_QEMU_VERSION = 5_001_000  # 5.1.0",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.neutron = self.useFixture(base.LibvirtNeutronFixture(self))",
            "",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.pci.utils.get_vf_num_by_pci_address',",
            "            new=mock.MagicMock(",
            "                side_effect=lambda addr: self._get_pci_function_number(addr))))",
            "",
            "        self.useFixture(fixtures.MockPatch(",
            "            'nova.pci.utils.get_mac_by_pci_address',",
            "            new=mock.MagicMock(",
            "                side_effect=(",
            "                    lambda addr: {",
            "                        \"0000:80:00.0\": \"52:54:00:1e:59:42\",",
            "                        \"0000:81:00.0\": \"52:54:00:1e:59:01\",",
            "                        \"0000:82:00.0\": \"52:54:00:1e:59:02\",",
            "                    }.get(addr)",
            "                )",
            "            )",
            "        ))",
            "",
            "    @classmethod",
            "    def _get_pci_function_number(cls, pci_addr: str):",
            "        \"\"\"Get a VF function number based on a PCI address.",
            "",
            "        Assume that the PCI ARI capability is enabled (slot bits become a part",
            "        of a function number).",
            "        \"\"\"",
            "        _, _, slot, function = parse_address(pci_addr)",
            "        # The number of PFs is extracted to get a VF number.",
            "        return int(slot, 16) + int(function, 16) - cls.NUM_PFS",
            "",
            "    def start_compute(",
            "        self, hostname='test_compute0', host_info=None, pci_info=None,",
            "        mdev_info=None, vdpa_info=None,",
            "        libvirt_version=None,",
            "        qemu_version=None):",
            "",
            "        if not pci_info:",
            "            pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "                num_pci=0, num_pfs=0, num_vfs=0)",
            "",
            "            pci_info.add_device(",
            "                dev_type='PF',",
            "                bus=0x81,",
            "                slot=0x0,",
            "                function=0,",
            "                iommu_group=42,",
            "                numa_node=0,",
            "                vf_ratio=self.vf_ratio,",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='a2dc',",
            "                prod_name='BlueField-3 integrated ConnectX-7 controller',",
            "                driver_name='mlx5_core',",
            "                vpd_fields={",
            "                    'name': 'MT43244 BlueField-3 integrated ConnectX-7',",
            "                    'readonly': {",
            "                        'serial_number': 'MT0000X00001',",
            "                    },",
            "                }",
            "            )",
            "",
            "            for idx in range(self.NUM_VFS):",
            "                pci_info.add_device(",
            "                    dev_type='VF',",
            "                    bus=0x81,",
            "                    slot=0x0,",
            "                    function=idx + 1,",
            "                    iommu_group=idx + 43,",
            "                    numa_node=0,",
            "                    vf_ratio=self.vf_ratio,",
            "                    parent=(0x81, 0x0, 0),",
            "                    vend_id='15b3',",
            "                    vend_name='Mellanox Technologies',",
            "                    prod_id='1021',",
            "                    prod_name='MT2910 Family [ConnectX-7]',",
            "                    driver_name='mlx5_core',",
            "                    vpd_fields={",
            "                        'name': 'MT2910 Family [ConnectX-7]',",
            "                        'readonly': {",
            "                            'serial_number': 'MT0000X00001',",
            "                        },",
            "                    }",
            "                )",
            "",
            "            pci_info.add_device(",
            "                dev_type='PF',",
            "                bus=0x82,",
            "                slot=0x0,",
            "                function=0,",
            "                iommu_group=84,",
            "                numa_node=0,",
            "                vf_ratio=self.vf_ratio,",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='a2d6',",
            "                prod_name='MT42822 BlueField-2 integrated ConnectX-6',",
            "                driver_name='mlx5_core',",
            "                vpd_fields={",
            "                    'name': 'MT42822 BlueField-2 integrated ConnectX-6',",
            "                    'readonly': {",
            "                        'serial_number': 'MT0000X00002',",
            "                    },",
            "                }",
            "            )",
            "",
            "            for idx in range(self.NUM_VFS):",
            "                pci_info.add_device(",
            "                    dev_type='VF',",
            "                    bus=0x82,",
            "                    slot=0x0,",
            "                    function=idx + 1,",
            "                    iommu_group=idx + 85,",
            "                    numa_node=0,",
            "                    vf_ratio=self.vf_ratio,",
            "                    parent=(0x82, 0x0, 0),",
            "                    vend_id='15b3',",
            "                    vend_name='Mellanox Technologies',",
            "                    prod_id='101e',",
            "                    prod_name='ConnectX Family mlx5Gen Virtual Function',",
            "                    driver_name='mlx5_core')",
            "",
            "        return super().start_compute(",
            "            hostname=hostname, host_info=host_info, pci_info=pci_info,",
            "            mdev_info=mdev_info, vdpa_info=vdpa_info,",
            "            libvirt_version=libvirt_version or self.FAKE_LIBVIRT_VERSION,",
            "            qemu_version=qemu_version or self.FAKE_QEMU_VERSION)",
            "",
            "    def create_remote_managed_tunnel_port(self):",
            "        dpu_tunnel_port = {",
            "            'id': uuids.dpu_tunnel_port,",
            "            'network_id': self.neutron.network_3['id'],",
            "            'status': 'ACTIVE',",
            "            'mac_address': 'fa:16:3e:f0:a4:bb',",
            "            'fixed_ips': [",
            "                {",
            "                    'ip_address': '192.168.2.8',",
            "                    'subnet_id': self.neutron.subnet_3['id']",
            "                }",
            "            ],",
            "            'binding:vif_details': {},",
            "            'binding:vif_type': 'ovs',",
            "            'binding:vnic_type': 'remote-managed',",
            "        }",
            "",
            "        self.neutron.create_port({'port': dpu_tunnel_port})",
            "        return dpu_tunnel_port",
            "",
            "    def create_remote_managed_physnet_port(self):",
            "        dpu_physnet_port = {",
            "            'id': uuids.dpu_physnet_port,",
            "            'network_id': self.neutron.network_4['id'],",
            "            'status': 'ACTIVE',",
            "            'mac_address': 'd2:0b:fd:99:89:8b',",
            "            'fixed_ips': [",
            "                {",
            "                    'ip_address': '192.168.4.10',",
            "                    'subnet_id': self.neutron.subnet_4['id']",
            "                }",
            "            ],",
            "            'binding:vif_details': {},",
            "            'binding:vif_type': 'ovs',",
            "            'binding:vnic_type': 'remote-managed',",
            "        }",
            "",
            "        self.neutron.create_port({'port': dpu_physnet_port})",
            "        return dpu_physnet_port",
            "",
            "    def test_create_server_physnet(self):",
            "        \"\"\"Create an instance with a tunnel remote-managed port.\"\"\"",
            "",
            "        hostname = self.start_compute()",
            "        num_pci = (self.NUM_PFS + self.NUM_VFS) * 2",
            "",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        dpu_port = self.create_remote_managed_physnet_port()",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        self._create_server(networks=[{'port': dpu_port['id']}])",
            "",
            "        # Ensure there is one less VF available and that the PF",
            "        # is no longer usable.",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "",
            "        # Ensure the binding:profile details sent to Neutron are correct after",
            "        # a port update.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual({",
            "            'card_serial_number': 'MT0000X00001',",
            "            'pci_slot': '0000:81:00.4',",
            "            'pci_vendor_info': '15b3:1021',",
            "            'pf_mac_address': '52:54:00:1e:59:01',",
            "            'physical_network': 'physnet4',",
            "            'vf_num': 3",
            "        }, port['binding:profile'])",
            "",
            "    def test_create_server_tunnel(self):",
            "        \"\"\"Create an instance with a tunnel remote-managed port.\"\"\"",
            "",
            "        hostname = self.start_compute()",
            "        num_pci = (self.NUM_PFS + self.NUM_VFS) * 2",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci)",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        self._create_server(networks=[{'port': dpu_port['id']}])",
            "",
            "        # Ensure there is one less VF available and that the PF",
            "        # is no longer usable.",
            "        self.assertPCIDeviceCounts(hostname, total=num_pci, free=num_pci - 2)",
            "",
            "        # Ensure the binding:profile details sent to Neutron are correct after",
            "        # a port update.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual({",
            "            'card_serial_number': 'MT0000X00002',",
            "            'pci_slot': '0000:82:00.4',",
            "            'pci_vendor_info': '15b3:101e',",
            "            'pf_mac_address': '52:54:00:1e:59:02',",
            "            'physical_network': None,",
            "            'vf_num': 3",
            "        }, port['binding:profile'])",
            "",
            "    def _test_common(self, op, *args, **kwargs):",
            "        self.start_compute()",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        server = self._create_server(networks=[{'port': dpu_port['id']}])",
            "        op(server, *args, **kwargs)",
            "",
            "    def test_attach_interface(self):",
            "        self.start_compute()",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        server = self._create_server(networks='none')",
            "",
            "        self._attach_interface(server, dpu_port['id'])",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:82:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:02',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00002',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_detach_interface(self):",
            "        self._test_common(self._detach_interface, uuids.dpu_tunnel_port)",
            "",
            "        port = self.neutron.show_port(uuids.dpu_tunnel_port)['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual({}, port['binding:profile'])",
            "",
            "    def test_shelve(self):",
            "        self._test_common(self._shelve_server)",
            "",
            "        port = self.neutron.show_port(uuids.dpu_tunnel_port)['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:82:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:02',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00002',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_suspend(self):",
            "        self.start_compute()",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        server = self._create_server(networks=[{'port': dpu_port['id']}])",
            "        self._suspend_server(server)",
            "        # TODO(dmitriis): detachDevice does not properly handle hostdevs",
            "        # so full suspend/resume testing is problematic.",
            "",
            "    def _test_move_operation_with_neutron(self, move_operation, dpu_port):",
            "        \"\"\"Test a move operation with a remote-managed port.",
            "        \"\"\"",
            "        compute1_pci_info = fakelibvirt.HostPCIDevicesInfo(",
            "            num_pfs=0, num_vfs=0)",
            "",
            "        compute1_pci_info.add_device(",
            "            dev_type='PF',",
            "            bus=0x80,",
            "            slot=0x0,",
            "            function=0,",
            "            iommu_group=84,",
            "            numa_node=1,",
            "            vf_ratio=self.vf_ratio,",
            "            vend_id='15b3',",
            "            vend_name='Mellanox Technologies',",
            "            prod_id='a2d6',",
            "            prod_name='MT42822 BlueField-2 integrated ConnectX-6',",
            "            driver_name='mlx5_core',",
            "            vpd_fields={",
            "                'name': 'MT42822 BlueField-2 integrated ConnectX-6',",
            "                'readonly': {",
            "                    'serial_number': 'MT0000X00042',",
            "                },",
            "            }",
            "        )",
            "        for idx in range(self.NUM_VFS):",
            "            compute1_pci_info.add_device(",
            "                dev_type='VF',",
            "                bus=0x80,",
            "                slot=0x0,",
            "                function=idx + 1,",
            "                iommu_group=idx + 85,",
            "                numa_node=1,",
            "                vf_ratio=self.vf_ratio,",
            "                parent=(0x80, 0x0, 0),",
            "                vend_id='15b3',",
            "                vend_name='Mellanox Technologies',",
            "                prod_id='101e',",
            "                prod_name='ConnectX Family mlx5Gen Virtual Function',",
            "                driver_name='mlx5_core',",
            "                vpd_fields={",
            "                    'name': 'MT42822 BlueField-2 integrated ConnectX-6',",
            "                    'readonly': {",
            "                        'serial_number': 'MT0000X00042',",
            "                    },",
            "                }",
            "            )",
            "",
            "        self.start_compute(hostname='test_compute0')",
            "        self.start_compute(hostname='test_compute1',",
            "                           pci_info=compute1_pci_info)",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertNotIn('binding:profile', port)",
            "",
            "        flavor_id = self._create_flavor(vcpu=4)",
            "        server = self._create_server(",
            "            flavor_id=flavor_id,",
            "            networks=[{'port': dpu_port['id']}],",
            "            host='test_compute0',",
            "        )",
            "",
            "        self.assertEqual('test_compute0', server['OS-EXT-SRV-ATTR:host'])",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=5)",
            "",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:82:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:02',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00002',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "        move_operation(server)",
            "",
            "    def test_unshelve_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            self._shelve_server(source_server)",
            "            # Disable the source compute, to force unshelving on the dest.",
            "            self.api.put_service(",
            "                self.computes['test_compute0'].service_ref.uuid,",
            "                {'status': 'disabled'})",
            "            self._unshelve_server(source_server)",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=10)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "        # Ensure the binding:profile details got updated, including the",
            "        # fields relevant to remote-managed ports.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:80:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:42',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00042',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_cold_migrate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                            '.migrate_disk_and_power_off', return_value='{}'):",
            "                server = self._migrate_server(source_server)",
            "                self._confirm_resize(server)",
            "",
            "                self.assertPCIDeviceCounts('test_compute0', total=10, free=10)",
            "                self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "                # Ensure the binding:profile details got updated, including the",
            "                # fields relevant to remote-managed ports.",
            "                port = self.neutron.show_port(dpu_port['id'])['port']",
            "                self.assertIn('binding:profile', port)",
            "                self.assertEqual(",
            "                    {",
            "                        'pci_vendor_info': '15b3:101e',",
            "                        'pci_slot': '0000:80:00.4',",
            "                        'physical_network': None,",
            "                        'pf_mac_address': '52:54:00:1e:59:42',",
            "                        'vf_num': 3,",
            "                        'card_serial_number': 'MT0000X00042',",
            "                    },",
            "                    port['binding:profile'],",
            "                )",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "    def test_cold_migrate_server_with_neutron_revert(self):",
            "        def move_operation(source_server):",
            "            with mock.patch('nova.virt.libvirt.driver.LibvirtDriver'",
            "                            '.migrate_disk_and_power_off', return_value='{}'):",
            "                server = self._migrate_server(source_server)",
            "",
            "                self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "                self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "                self._revert_resize(server)",
            "",
            "                self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "                self.assertPCIDeviceCounts('test_compute1', total=5, free=5)",
            "",
            "                port = self.neutron.show_port(dpu_port['id'])['port']",
            "                self.assertIn('binding:profile', port)",
            "                self.assertEqual(",
            "                    {",
            "                        'pci_vendor_info': '15b3:101e',",
            "                        'pci_slot': '0000:82:00.4',",
            "                        'physical_network': None,",
            "                        'pf_mac_address': '52:54:00:1e:59:02',",
            "                        'vf_num': 3,",
            "                        'card_serial_number': 'MT0000X00002',",
            "                    },",
            "                    port['binding:profile'],",
            "                )",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "    def test_evacuate_server_with_neutron(self):",
            "        def move_operation(source_server):",
            "            # Down the source compute to enable the evacuation",
            "            self.api.put_service(",
            "                self.computes['test_compute0'].service_ref.uuid,",
            "                {'forced_down': True})",
            "            self.computes['test_compute0'].stop()",
            "            self._evacuate_server(source_server)",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=8)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "        # Ensure the binding:profile details got updated, including the",
            "        # fields relevant to remote-managed ports.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:80:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:42',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00042',",
            "            },",
            "            port['binding:profile'],",
            "        )",
            "",
            "    def test_live_migrate_server_with_neutron(self):",
            "        \"\"\"Live migrate an instance using a remote-managed port.",
            "",
            "        This should succeed since we support this via detach and attach of the",
            "        PCI device similar to how this is done for SR-IOV ports.",
            "        \"\"\"",
            "        def move_operation(source_server):",
            "            self._live_migrate(source_server, 'completed')",
            "",
            "        dpu_port = self.create_remote_managed_tunnel_port()",
            "        self._test_move_operation_with_neutron(move_operation, dpu_port)",
            "",
            "        self.assertPCIDeviceCounts('test_compute0', total=10, free=10)",
            "        self.assertPCIDeviceCounts('test_compute1', total=5, free=3)",
            "",
            "        # Ensure the binding:profile details got updated, including the",
            "        # fields relevant to remote-managed ports.",
            "        port = self.neutron.show_port(dpu_port['id'])['port']",
            "        self.assertIn('binding:profile', port)",
            "        self.assertEqual(",
            "            {",
            "                'pci_vendor_info': '15b3:101e',",
            "                'pci_slot': '0000:80:00.4',",
            "                'physical_network': None,",
            "                'pf_mac_address': '52:54:00:1e:59:42',",
            "                'vf_num': 3,",
            "                'card_serial_number': 'MT0000X00042',",
            "            },",
            "            port['binding:profile'],",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.pyload.core.database.user_database"
        ]
    }
}