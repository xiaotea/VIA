{
    "mindsdb/api/http/namespaces/file.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from flask_restx import Resource"
            },
            "1": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from mindsdb.api.http.namespaces.configs.files import ns_conf"
            },
            "3": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from mindsdb.api.http.utils import http_error, safe_extract"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 14,
                "PatchRowcode": "+from mindsdb.api.http.utils import http_error"
            },
            "5": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from mindsdb.utilities.config import Config"
            },
            "6": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from mindsdb.utilities.context import context as ctx"
            },
            "7": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from mindsdb.utilities import log"
            },
            "8": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from mindsdb.utilities.security import is_private_url, clear_filename"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+from mindsdb.utilities.fs import safe_extract"
            },
            "10": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " logger = log.getLogger(__name__)"
            },
            "12": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import os",
            "import shutil",
            "import tarfile",
            "import tempfile",
            "import zipfile",
            "",
            "import multipart",
            "import requests",
            "from flask import current_app as ca",
            "from flask import request",
            "from flask_restx import Resource",
            "",
            "from mindsdb.api.http.namespaces.configs.files import ns_conf",
            "from mindsdb.api.http.utils import http_error, safe_extract",
            "from mindsdb.utilities.config import Config",
            "from mindsdb.utilities.context import context as ctx",
            "from mindsdb.utilities import log",
            "from mindsdb.utilities.security import is_private_url, clear_filename",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "@ns_conf.route(\"/\")",
            "class FilesList(Resource):",
            "    @ns_conf.doc(\"get_files_list\")",
            "    def get(self):",
            "        \"\"\"List all files\"\"\"",
            "        return ca.file_controller.get_files()",
            "",
            "",
            "@ns_conf.route(\"/<name>\")",
            "@ns_conf.param(\"name\", \"MindsDB's name for file\")",
            "class File(Resource):",
            "    @ns_conf.doc(\"put_file\")",
            "    def put(self, name: str):",
            "        \"\"\"add new file",
            "        params in FormData:",
            "            - file",
            "            - original_file_name [optional]",
            "        \"\"\"",
            "",
            "        data = {}",
            "        mindsdb_file_name = name",
            "",
            "        existing_file_names = ca.file_controller.get_files_names()",
            "",
            "        def on_field(field):",
            "            name = field.field_name.decode()",
            "            value = field.value.decode()",
            "            data[name] = value",
            "",
            "        file_object = None",
            "",
            "        def on_file(file):",
            "            nonlocal file_object",
            "            data[\"file\"] = file.file_name.decode()",
            "            file_object = file.file_object",
            "",
            "        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")",
            "",
            "        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):",
            "            parser = multipart.create_form_parser(",
            "                headers=request.headers,",
            "                on_field=on_field,",
            "                on_file=on_file,",
            "                config={",
            "                    \"UPLOAD_DIR\": temp_dir_path.encode(),  # bytes required",
            "                    \"UPLOAD_KEEP_FILENAME\": True,",
            "                    \"UPLOAD_KEEP_EXTENSIONS\": True,",
            "                    \"MAX_MEMORY_FILE_SIZE\": 0,",
            "                },",
            "            )",
            "",
            "            while True:",
            "                chunk = request.stream.read(8192)",
            "                if not chunk:",
            "                    break",
            "                parser.write(chunk)",
            "            parser.finalize()",
            "            parser.close()",
            "",
            "            if file_object is not None and not file_object.closed:",
            "                file_object.close()",
            "        else:",
            "            data = request.json",
            "",
            "        if mindsdb_file_name in existing_file_names:",
            "            return http_error(",
            "                400,",
            "                \"File already exists\",",
            "                f\"File with name '{data['file']}' already exists\",",
            "            )",
            "",
            "        if data.get(\"source_type\") == \"url\":",
            "            url = data[\"source\"]",
            "            data[\"file\"] = clear_filename(data[\"name\"])",
            "",
            "            config = Config()",
            "            is_cloud = config.get(\"cloud\", False)",
            "            if is_cloud and is_private_url(url):",
            "                return http_error(",
            "                    400, f'URL is private: {url}'",
            "                )",
            "",
            "            if is_cloud is True and ctx.user_class != 1:",
            "                info = requests.head(url)",
            "                file_size = info.headers.get(\"Content-Length\")",
            "                try:",
            "                    file_size = int(file_size)",
            "                except Exception:",
            "                    pass",
            "",
            "                if file_size is None:",
            "                    return http_error(",
            "                        400,",
            "                        \"Error getting file info\",",
            "                        \"\u0421an't determine remote file size\",",
            "                    )",
            "                if file_size > 1024 * 1024 * 100:",
            "                    return http_error(",
            "                        400, \"File is too big\", \"Upload limit for file is 100Mb\"",
            "                    )",
            "            with requests.get(url, stream=True) as r:",
            "                if r.status_code != 200:",
            "                    return http_error(",
            "                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"",
            "                    )",
            "                file_path = os.path.join(temp_dir_path, data[\"file\"])",
            "                with open(file_path, \"wb\") as f:",
            "                    for chunk in r.iter_content(chunk_size=8192):",
            "                        f.write(chunk)",
            "",
            "        original_file_name = clear_filename(data.get(\"original_file_name\"))",
            "",
            "        file_path = os.path.join(temp_dir_path, data[\"file\"])",
            "        lp = file_path.lower()",
            "        if lp.endswith((\".zip\", \".tar.gz\")):",
            "            if lp.endswith(\".zip\"):",
            "                with zipfile.ZipFile(file_path) as f:",
            "                    f.extractall(temp_dir_path)",
            "            elif lp.endswith(\".tar.gz\"):",
            "                with tarfile.open(file_path) as f:",
            "                    safe_extract(f, temp_dir_path)",
            "            os.remove(file_path)",
            "            files = os.listdir(temp_dir_path)",
            "            if len(files) != 1:",
            "                os.rmdir(temp_dir_path)",
            "                return http_error(",
            "                    400, \"Wrong content.\", \"Archive must contain only one data file.\"",
            "                )",
            "            file_path = os.path.join(temp_dir_path, files[0])",
            "            mindsdb_file_name = files[0]",
            "            if not os.path.isfile(file_path):",
            "                os.rmdir(temp_dir_path)",
            "                return http_error(",
            "                    400, \"Wrong content.\", \"Archive must contain data file in root.\"",
            "                )",
            "",
            "        try:",
            "            ca.file_controller.save_file(",
            "                mindsdb_file_name, file_path, file_name=original_file_name",
            "            )",
            "        except Exception as e:",
            "            return http_error(500, 'Error', str(e))",
            "        finally:",
            "            shutil.rmtree(temp_dir_path, ignore_errors=True)",
            "",
            "        return \"\", 200",
            "",
            "    @ns_conf.doc(\"delete_file\")",
            "    def delete(self, name: str):",
            "        \"\"\"delete file\"\"\"",
            "",
            "        try:",
            "            ca.file_controller.delete_file(name)",
            "        except Exception as e:",
            "            logger.error(e)",
            "            return http_error(",
            "                400,",
            "                \"Error deleting file\",",
            "                f\"There was an error while tring to delete file with name '{name}'\",",
            "            )",
            "        return \"\", 200"
        ],
        "afterPatchFile": [
            "import os",
            "import shutil",
            "import tarfile",
            "import tempfile",
            "import zipfile",
            "",
            "import multipart",
            "import requests",
            "from flask import current_app as ca",
            "from flask import request",
            "from flask_restx import Resource",
            "",
            "from mindsdb.api.http.namespaces.configs.files import ns_conf",
            "from mindsdb.api.http.utils import http_error",
            "from mindsdb.utilities.config import Config",
            "from mindsdb.utilities.context import context as ctx",
            "from mindsdb.utilities import log",
            "from mindsdb.utilities.security import is_private_url, clear_filename",
            "from mindsdb.utilities.fs import safe_extract",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "@ns_conf.route(\"/\")",
            "class FilesList(Resource):",
            "    @ns_conf.doc(\"get_files_list\")",
            "    def get(self):",
            "        \"\"\"List all files\"\"\"",
            "        return ca.file_controller.get_files()",
            "",
            "",
            "@ns_conf.route(\"/<name>\")",
            "@ns_conf.param(\"name\", \"MindsDB's name for file\")",
            "class File(Resource):",
            "    @ns_conf.doc(\"put_file\")",
            "    def put(self, name: str):",
            "        \"\"\"add new file",
            "        params in FormData:",
            "            - file",
            "            - original_file_name [optional]",
            "        \"\"\"",
            "",
            "        data = {}",
            "        mindsdb_file_name = name",
            "",
            "        existing_file_names = ca.file_controller.get_files_names()",
            "",
            "        def on_field(field):",
            "            name = field.field_name.decode()",
            "            value = field.value.decode()",
            "            data[name] = value",
            "",
            "        file_object = None",
            "",
            "        def on_file(file):",
            "            nonlocal file_object",
            "            data[\"file\"] = file.file_name.decode()",
            "            file_object = file.file_object",
            "",
            "        temp_dir_path = tempfile.mkdtemp(prefix=\"mindsdb_file_\")",
            "",
            "        if request.headers[\"Content-Type\"].startswith(\"multipart/form-data\"):",
            "            parser = multipart.create_form_parser(",
            "                headers=request.headers,",
            "                on_field=on_field,",
            "                on_file=on_file,",
            "                config={",
            "                    \"UPLOAD_DIR\": temp_dir_path.encode(),  # bytes required",
            "                    \"UPLOAD_KEEP_FILENAME\": True,",
            "                    \"UPLOAD_KEEP_EXTENSIONS\": True,",
            "                    \"MAX_MEMORY_FILE_SIZE\": 0,",
            "                },",
            "            )",
            "",
            "            while True:",
            "                chunk = request.stream.read(8192)",
            "                if not chunk:",
            "                    break",
            "                parser.write(chunk)",
            "            parser.finalize()",
            "            parser.close()",
            "",
            "            if file_object is not None and not file_object.closed:",
            "                file_object.close()",
            "        else:",
            "            data = request.json",
            "",
            "        if mindsdb_file_name in existing_file_names:",
            "            return http_error(",
            "                400,",
            "                \"File already exists\",",
            "                f\"File with name '{data['file']}' already exists\",",
            "            )",
            "",
            "        if data.get(\"source_type\") == \"url\":",
            "            url = data[\"source\"]",
            "            data[\"file\"] = clear_filename(data[\"name\"])",
            "",
            "            config = Config()",
            "            is_cloud = config.get(\"cloud\", False)",
            "            if is_cloud and is_private_url(url):",
            "                return http_error(",
            "                    400, f'URL is private: {url}'",
            "                )",
            "",
            "            if is_cloud is True and ctx.user_class != 1:",
            "                info = requests.head(url)",
            "                file_size = info.headers.get(\"Content-Length\")",
            "                try:",
            "                    file_size = int(file_size)",
            "                except Exception:",
            "                    pass",
            "",
            "                if file_size is None:",
            "                    return http_error(",
            "                        400,",
            "                        \"Error getting file info\",",
            "                        \"\u0421an't determine remote file size\",",
            "                    )",
            "                if file_size > 1024 * 1024 * 100:",
            "                    return http_error(",
            "                        400, \"File is too big\", \"Upload limit for file is 100Mb\"",
            "                    )",
            "            with requests.get(url, stream=True) as r:",
            "                if r.status_code != 200:",
            "                    return http_error(",
            "                        400, \"Error getting file\", f\"Got status code: {r.status_code}\"",
            "                    )",
            "                file_path = os.path.join(temp_dir_path, data[\"file\"])",
            "                with open(file_path, \"wb\") as f:",
            "                    for chunk in r.iter_content(chunk_size=8192):",
            "                        f.write(chunk)",
            "",
            "        original_file_name = clear_filename(data.get(\"original_file_name\"))",
            "",
            "        file_path = os.path.join(temp_dir_path, data[\"file\"])",
            "        lp = file_path.lower()",
            "        if lp.endswith((\".zip\", \".tar.gz\")):",
            "            if lp.endswith(\".zip\"):",
            "                with zipfile.ZipFile(file_path) as f:",
            "                    f.extractall(temp_dir_path)",
            "            elif lp.endswith(\".tar.gz\"):",
            "                with tarfile.open(file_path) as f:",
            "                    safe_extract(f, temp_dir_path)",
            "            os.remove(file_path)",
            "            files = os.listdir(temp_dir_path)",
            "            if len(files) != 1:",
            "                os.rmdir(temp_dir_path)",
            "                return http_error(",
            "                    400, \"Wrong content.\", \"Archive must contain only one data file.\"",
            "                )",
            "            file_path = os.path.join(temp_dir_path, files[0])",
            "            mindsdb_file_name = files[0]",
            "            if not os.path.isfile(file_path):",
            "                os.rmdir(temp_dir_path)",
            "                return http_error(",
            "                    400, \"Wrong content.\", \"Archive must contain data file in root.\"",
            "                )",
            "",
            "        try:",
            "            ca.file_controller.save_file(",
            "                mindsdb_file_name, file_path, file_name=original_file_name",
            "            )",
            "        except Exception as e:",
            "            return http_error(500, 'Error', str(e))",
            "        finally:",
            "            shutil.rmtree(temp_dir_path, ignore_errors=True)",
            "",
            "        return \"\", 200",
            "",
            "    @ns_conf.doc(\"delete_file\")",
            "    def delete(self, name: str):",
            "        \"\"\"delete file\"\"\"",
            "",
            "        try:",
            "            ca.file_controller.delete_file(name)",
            "        except Exception as e:",
            "            logger.error(e)",
            "            return http_error(",
            "                400,",
            "                \"Error deleting file\",",
            "                f\"There was an error while tring to delete file with name '{name}'\",",
            "            )",
            "        return \"\", 200"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "14": []
        },
        "addLocation": []
    },
    "mindsdb/api/http/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": "             'Content-Type': 'application/problem+json'"
            },
            "1": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": "         }"
            },
            "2": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": "     )"
            },
            "3": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "4": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "5": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def __is_within_directory(directory, target):"
            },
            "6": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    abs_directory = os.path.abspath(directory)"
            },
            "7": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    abs_target = os.path.abspath(target)"
            },
            "8": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    prefix = os.path.commonprefix([abs_directory, abs_target])"
            },
            "9": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return prefix == abs_directory"
            },
            "10": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "11": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "12": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):"
            },
            "13": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for member in tar.getmembers():"
            },
            "14": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        member_path = os.path.join(path, member.name)"
            },
            "15": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not __is_within_directory(path, member_path):"
            },
            "16": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise Exception(\"Attempted Path Traversal in Tar File\")"
            },
            "17": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    tar.extractall(path, members, numeric_owner)"
            }
        },
        "frontPatchFile": [
            "import json",
            "import os",
            "",
            "from flask import Response",
            "",
            "",
            "def http_error(status_code, title, detail=''):",
            "    ''' Wrapper for error responce acoording with RFC 7807 (https://tools.ietf.org/html/rfc7807)",
            "",
            "        :param status_code: int - http status code for response",
            "        :param title: str",
            "        :param detail: str",
            "",
            "        :return: flask Response object",
            "    '''",
            "    return Response(",
            "        response=json.dumps({",
            "            'title': title,",
            "            'detail': detail",
            "        }),",
            "        status=status_code,",
            "        headers={",
            "            'Content-Type': 'application/problem+json'",
            "        }",
            "    )",
            "",
            "",
            "def __is_within_directory(directory, target):",
            "    abs_directory = os.path.abspath(directory)",
            "    abs_target = os.path.abspath(target)",
            "    prefix = os.path.commonprefix([abs_directory, abs_target])",
            "    return prefix == abs_directory",
            "",
            "",
            "def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):",
            "    for member in tar.getmembers():",
            "        member_path = os.path.join(path, member.name)",
            "        if not __is_within_directory(path, member_path):",
            "            raise Exception(\"Attempted Path Traversal in Tar File\")",
            "    tar.extractall(path, members, numeric_owner)"
        ],
        "afterPatchFile": [
            "import json",
            "import os",
            "",
            "from flask import Response",
            "",
            "",
            "def http_error(status_code, title, detail=''):",
            "    ''' Wrapper for error responce acoording with RFC 7807 (https://tools.ietf.org/html/rfc7807)",
            "",
            "        :param status_code: int - http status code for response",
            "        :param title: str",
            "        :param detail: str",
            "",
            "        :return: flask Response object",
            "    '''",
            "    return Response(",
            "        response=json.dumps({",
            "            'title': title,",
            "            'detail': detail",
            "        }),",
            "        status=status_code,",
            "        headers={",
            "            'Content-Type': 'application/problem+json'",
            "        }",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1"
        ],
        "dele_reviseLocation": {
            "26": [],
            "27": [],
            "28": [
                "__is_within_directory"
            ],
            "29": [
                "__is_within_directory"
            ],
            "30": [
                "__is_within_directory"
            ],
            "31": [
                "__is_within_directory"
            ],
            "32": [
                "__is_within_directory"
            ],
            "33": [],
            "34": [],
            "35": [
                "safe_extract"
            ],
            "36": [
                "safe_extract"
            ],
            "37": [
                "safe_extract"
            ],
            "38": [
                "safe_extract"
            ],
            "39": [
                "safe_extract"
            ],
            "40": [
                "safe_extract"
            ]
        },
        "addLocation": []
    },
    "mindsdb/integrations/handlers/byom_handler/byom_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from mindsdb.integrations.utilities.utils import format_exception_error"
            },
            "1": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE"
            },
            "2": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " import mindsdb.utilities.profiler as profiler"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+from mindsdb.utilities.fs import safe_extract"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " from .proc_wrapper import ("
            },
            "7": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "     pd_decode, pd_encode, encode, decode, BYOM_METHOD,"
            },
            "8": {
                "beforePatchRowNumber": 475,
                "afterPatchRowNumber": 477,
                "PatchRowcode": "                 tar_path = self.env_storage_path.with_suffix('.tar')"
            },
            "9": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 478,
                "PatchRowcode": "                 if self.env_path.exists() is False and tar_path.exists() is True:"
            },
            "10": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": 479,
                "PatchRowcode": "                     with tarfile.open(tar_path) as tar:"
            },
            "11": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        tar.extractall(path=bese_env_path)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 480,
                "PatchRowcode": "+                        safe_extract(tar, path=bese_env_path)"
            },
            "13": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": 481,
                "PatchRowcode": "             else:"
            },
            "14": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": 482,
                "PatchRowcode": "                 self.env_path = self.env_storage_path"
            },
            "15": {
                "beforePatchRowNumber": 481,
                "afterPatchRowNumber": 483,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\" BYOM: Bring Your Own Model",
            "",
            "env vars to contloll BYOM:",
            " - MINDSDB_BYOM_ENABLED - can BYOM be uysed or not. Locally enabled by default.",
            " - MINDSDB_BYOM_INHOUSE_ENABLED - enable or disable 'inhouse' BYOM usage. Locally enabled by default.",
            " - MINDSDB_BYOM_DEFAULT_TYPE - [inhouse|venv] default byom type. Locally it is 'venv' by default.",
            " - MINDSDB_BYOM_TYPE - [safe|unsafe] - obsolete, same as above.",
            "\"\"\"",
            "",
            "",
            "import os",
            "import re",
            "import sys",
            "import shutil",
            "import pickle",
            "import tarfile",
            "import tempfile",
            "import traceback",
            "import subprocess",
            "from enum import Enum",
            "from pathlib import Path",
            "from datetime import datetime",
            "from typing import Optional, Dict, Union",
            "from collections import OrderedDict",
            "",
            "import pandas as pd",
            "from pandas.api import types as pd_types",
            "",
            "from mindsdb.utilities import log",
            "from mindsdb.utilities.config import Config",
            "from mindsdb.interfaces.storage import db",
            "from mindsdb.integrations.libs.base import BaseMLEngine",
            "from mindsdb.integrations.libs.const import PREDICTOR_STATUS",
            "from mindsdb.integrations.utilities.utils import format_exception_error",
            "from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE",
            "import mindsdb.utilities.profiler as profiler",
            "",
            "from .proc_wrapper import (",
            "    pd_decode, pd_encode, encode, decode, BYOM_METHOD,",
            "    import_string, find_model_class",
            ")",
            "from .__about__ import __version__",
            "",
            "",
            "BYOM_TYPE = Enum('BYOM_TYPE', ['INHOUSE', 'VENV'])",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "class BYOMHandler(BaseMLEngine):",
            "",
            "    name = 'byom'",
            "",
            "    def __init__(self, model_storage, engine_storage, **kwargs) -> None:",
            "        # region check availability",
            "        is_cloud = Config().get('cloud', False)",
            "        if is_cloud is True:",
            "            byom_enabled = os.environ.get('MINDSDB_BYOM_ENABLED', 'false').lower()",
            "            if byom_enabled not in ('true', '1'):",
            "                raise RuntimeError('BYOM is disabled on cloud')",
            "        # endregion",
            "",
            "        self.model_wrapper = None",
            "",
            "        self.inhouse_model_wrapper = None",
            "        self.model_wrappers = {}",
            "",
            "        # region read and save set default byom type",
            "        try:",
            "            self._default_byom_type = BYOM_TYPE.VENV",
            "            if os.environ.get('MINDSDB_BYOM_DEFAULT_TYPE') is not None:",
            "                self._default_byom_type = BYOM_TYPE[",
            "                    os.environ.get('MINDSDB_BYOM_DEFAULT_TYPE').upper()",
            "                ]",
            "            else:",
            "                env_var = os.environ.get('MINDSDB_BYOM_DEFAULT_TYPE')",
            "                if env_var == 'SAVE':",
            "                    self._default_byom_type = BYOM_TYPE['VENV']",
            "                elif env_var == 'UNSAVE':",
            "                    self._default_byom_type = BYOM_TYPE['INHOUSE']",
            "                else:",
            "                    raise KeyError",
            "        except KeyError:",
            "            self._default_byom_type = BYOM_TYPE.VENV",
            "        # endregion",
            "",
            "        # region check if 'inhouse' BYOM is enabled",
            "        env_var = os.environ.get('MINDSDB_BYOM_INHOUSE_ENABLED')",
            "        if env_var is None:",
            "            self._inhouse_enabled = False if is_cloud else True",
            "        else:",
            "            self._inhouse_enabled = env_var.lower() in ('true', '1')",
            "        # endregion",
            "",
            "        super().__init__(model_storage, engine_storage, **kwargs)",
            "",
            "    @staticmethod",
            "    def normalize_engine_version(engine_version: Union[int, str, None]) -> int:",
            "        \"\"\"Cast engine version to int, or return `1` if can not be casted",
            "",
            "        Args:",
            "            engine_version (Union[int, str, None]): engine version",
            "",
            "        Returns:",
            "            int: engine version",
            "        \"\"\"",
            "        if isinstance(engine_version, str):",
            "            try:",
            "                engine_version = int(engine_version)",
            "            except Exception:",
            "                engine_version = 1",
            "        if isinstance(engine_version, int) is False:",
            "            engine_version = 1",
            "        return engine_version",
            "",
            "    @staticmethod",
            "    def create_validation(target: str, args: dict = None, **kwargs) -> None:",
            "        if isinstance(args, dict) is False:",
            "            return",
            "        using_args = args.get('using', {})",
            "        engine_version = using_args.get('engine_version')",
            "        if engine_version is not None:",
            "            engine_version = BYOMHandler.normalize_engine_version(engine_version)",
            "        else:",
            "            connection_args = kwargs['handler_storage'].get_connection_args()",
            "            versions = connection_args.get('versions')",
            "            if isinstance(versions, dict):",
            "                engine_version = max([int(x) for x in versions.keys()])",
            "            else:",
            "                engine_version = 1",
            "            using_args['engine_version'] = engine_version",
            "",
            "    def get_model_engine_version(self) -> int:",
            "        \"\"\"Return current model engine version",
            "",
            "        Returns:",
            "            int: engine version",
            "        \"\"\"",
            "        engine_version = self.model_storage.get_info()['learn_args'].get('using', {}).get('engine_version')",
            "        engine_version = BYOMHandler.normalize_engine_version(engine_version)",
            "        return engine_version",
            "",
            "    def normalize_byom_type(self, byom_type: Optional[str]) -> BYOM_TYPE:",
            "        if byom_type is not None:",
            "            byom_type = BYOM_TYPE[byom_type.upper()]",
            "        else:",
            "            byom_type = self._default_byom_type",
            "        if byom_type == BYOM_TYPE.INHOUSE and self._inhouse_enabled is False:",
            "            raise Exception(\"'Inhouse' BYOM engine type can not be used\")",
            "        return byom_type",
            "",
            "    def _get_model_proxy(self, version=None):",
            "        if version is None:",
            "            version = 1",
            "        if isinstance(version, str):",
            "            version = int(version)",
            "        version_mark = ''",
            "        if version > 1:",
            "            version_mark = f'_{version}'",
            "        version_str = str(version)",
            "",
            "        self.engine_storage.fileStorage.pull()",
            "        try:",
            "            code = self.engine_storage.fileStorage.file_get(f'code{version_mark}')",
            "            modules_str = self.engine_storage.fileStorage.file_get(f'modules{version_mark}')",
            "        except FileNotFoundError:",
            "            raise Exception(f\"Engine version '{version}' does not exists\")",
            "",
            "        if version_str not in self.model_wrappers:",
            "            connection_args = self.engine_storage.get_connection_args()",
            "            version_meta = connection_args['versions'][version_str]",
            "",
            "            try:",
            "                engine_version_type = BYOM_TYPE[",
            "                    version_meta.get('type', self._default_byom_type.name).upper()",
            "                ]",
            "            except KeyError:",
            "                raise Exception('Unknown BYOM engine type')",
            "",
            "            if engine_version_type == BYOM_TYPE.INHOUSE:",
            "                if self._inhouse_enabled is False:",
            "                    raise Exception(\"'Inhouse' BYOM engine type can not be used\")",
            "                if self.inhouse_model_wrapper is None:",
            "                    self.inhouse_model_wrapper = ModelWrapperUnsafe(",
            "                        code=code,",
            "                        modules_str=modules_str,",
            "                        engine_id=self.engine_storage.integration_id,",
            "                        engine_version=version",
            "                    )",
            "                self.model_wrappers[version_str] = self.inhouse_model_wrapper",
            "            elif engine_version_type == BYOM_TYPE.VENV:",
            "                if version_meta.get('venv_status') != 'ready':",
            "                    version_meta['venv_status'] = 'creating'",
            "                    self.engine_storage.update_connection_args(connection_args)",
            "                self.model_wrappers[version_str] = ModelWrapperSafe(",
            "                    code=code,",
            "                    modules_str=modules_str,",
            "                    engine_id=self.engine_storage.integration_id,",
            "                    engine_version=version",
            "                )",
            "                version_meta['venv_status'] = 'ready'",
            "                self.engine_storage.update_connection_args(connection_args)",
            "",
            "        return self.model_wrappers[version_str]",
            "",
            "    def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        engine_version = self.get_model_engine_version()",
            "        mp = self._get_model_proxy(engine_version)",
            "        model_state = self.model_storage.file_get('model')",
            "        return mp.describe(model_state, attribute)",
            "",
            "    def create(self, target, df=None, args=None, **kwargs):",
            "        using_args = args.get('using', {})",
            "        engine_version = using_args.get('engine_version')",
            "",
            "        model_proxy = self._get_model_proxy(engine_version)",
            "        model_state = model_proxy.train(df, target, args)",
            "",
            "        self.model_storage.file_set('model', model_state)",
            "",
            "        # TODO return columns?",
            "",
            "        def convert_type(field_type):",
            "            if pd_types.is_integer_dtype(field_type):",
            "                return 'integer'",
            "            elif pd_types.is_numeric_dtype(field_type):",
            "                return 'float'",
            "            elif pd_types.is_datetime64_any_dtype(field_type):",
            "                return 'datetime'",
            "            else:",
            "                return 'categorical'",
            "",
            "        columns = {",
            "            target: convert_type(object)",
            "        }",
            "",
            "        self.model_storage.columns_set(columns)",
            "",
            "    def predict(self, df, args=None):",
            "        pred_args = args.get('predict_params', {})",
            "",
            "        engine_version = pred_args.get('engine_version')",
            "        if engine_version is not None:",
            "            engine_version = int(engine_version)",
            "        else:",
            "            engine_version = self.get_model_engine_version()",
            "",
            "        model_proxy = self._get_model_proxy(engine_version)",
            "        model_state = self.model_storage.file_get('model')",
            "        pred_df = model_proxy.predict(df, model_state, pred_args)",
            "",
            "        return pred_df",
            "",
            "    def create_engine(self, connection_args):",
            "        code_path = Path(connection_args['code'])",
            "        self.engine_storage.fileStorage.file_set(",
            "            'code',",
            "            code_path.read_bytes()",
            "        )",
            "",
            "        requirements_path = Path(connection_args['modules'])",
            "        self.engine_storage.fileStorage.file_set(",
            "            'modules',",
            "            requirements_path.read_bytes()",
            "        )",
            "",
            "        self.engine_storage.fileStorage.push()",
            "",
            "        self.engine_storage.update_connection_args({",
            "            'handler_version': __version__,",
            "            'versions': {",
            "                '1': {",
            "                    'code': code_path.name,",
            "                    'requirements': requirements_path.name,",
            "                    'type': self.normalize_byom_type(",
            "                        connection_args.get('type')",
            "                    ).name.lower()",
            "                }",
            "            }",
            "        })",
            "",
            "        model_proxy = self._get_model_proxy()",
            "        try:",
            "            model_proxy.check()",
            "        except Exception as e:",
            "            model_proxy.remove_venv()",
            "            raise e",
            "",
            "    def update_engine(self, connection_args: dict) -> None:",
            "        \"\"\"Add new version of engine",
            "",
            "            Args:",
            "                connection_args (dict): paths to code and requirements",
            "        \"\"\"",
            "        code_path = Path(connection_args['code'])",
            "        requirements_path = Path(connection_args['modules'])",
            "",
            "        engine_connection_args = self.engine_storage.get_connection_args()",
            "        if isinstance(engine_connection_args, dict) is False or 'handler_version' not in engine_connection_args:",
            "            engine_connection_args = {",
            "                'handler_version': __version__,",
            "                'versions': {",
            "                    '1': {",
            "                        'code': 'code.py',",
            "                        'requirements': 'requirements.txt',",
            "                        'type': self._default_byom_type.name.lower()",
            "                    }",
            "                }",
            "            }",
            "        new_version = str(max([int(x) for x in engine_connection_args['versions'].keys()]) + 1)",
            "",
            "        engine_connection_args['versions'][new_version] = {",
            "            'code': code_path.name,",
            "            'requirements': requirements_path.name,",
            "            'type': self.normalize_byom_type(",
            "                connection_args.get('type')",
            "            ).name.lower()",
            "        }",
            "",
            "        self.engine_storage.fileStorage.file_set(",
            "            f'code_{new_version}',",
            "            code_path.read_bytes()",
            "        )",
            "",
            "        self.engine_storage.fileStorage.file_set(",
            "            f'modules_{new_version}',",
            "            requirements_path.read_bytes()",
            "        )",
            "        self.engine_storage.fileStorage.push()",
            "",
            "        self.engine_storage.update_connection_args(engine_connection_args)",
            "",
            "        model_proxy = self._get_model_proxy(new_version)",
            "        try:",
            "            model_proxy.check()",
            "        except Exception as e:",
            "            model_proxy.remove_venv()",
            "            raise e",
            "",
            "    def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:",
            "        using_args = args.get('using', {})",
            "        engine_version = using_args.get('engine_version')",
            "",
            "        model_storage = self.model_storage",
            "        # TODO: should probably refactor at some point, as a bit of the logic is shared with lightwood's finetune logic",
            "        try:",
            "            base_predictor_id = args['base_model_id']",
            "            base_predictor_record = db.Predictor.query.get(base_predictor_id)",
            "            if base_predictor_record.status != PREDICTOR_STATUS.COMPLETE:",
            "                raise Exception(\"Base model must be in status 'complete'\")",
            "",
            "            predictor_id = model_storage.predictor_id",
            "            predictor_record = db.Predictor.query.get(predictor_id)",
            "",
            "            predictor_record.data = {'training_log': 'training'}  # TODO move to ModelStorage (don't work w/ db directly)",
            "            predictor_record.training_start_at = datetime.now()",
            "            predictor_record.status = PREDICTOR_STATUS.FINETUNING  # TODO: parallel execution block",
            "            db.session.commit()",
            "",
            "            model_proxy = self._get_model_proxy(engine_version)",
            "            model_state = self.base_model_storage.file_get('model')",
            "            model_state = model_proxy.finetune(df, model_state, args=args.get('using', {}))",
            "",
            "            # region hack to speedup file saving",
            "            with profiler.Context('finetune-byom-write-file'):",
            "                dest_abs_path = model_storage.fileStorage.folder_path / 'model'",
            "                with open(dest_abs_path, 'wb') as fd:",
            "                    fd.write(model_state)",
            "                model_storage.fileStorage.push(compression_level=0)",
            "            # endregion",
            "",
            "            predictor_record.update_status = 'up_to_date'",
            "            predictor_record.status = PREDICTOR_STATUS.COMPLETE",
            "            predictor_record.training_stop_at = datetime.now()",
            "            db.session.commit()",
            "",
            "        except Exception as e:",
            "            logger.error(e)",
            "            predictor_id = model_storage.predictor_id",
            "            predictor_record = db.Predictor.query.with_for_update().get(predictor_id)",
            "            logger.error(traceback.format_exc())",
            "            error_message = format_exception_error(e)",
            "            predictor_record.data = {\"error\": error_message}",
            "            predictor_record.status = PREDICTOR_STATUS.ERROR",
            "            db.session.commit()",
            "            raise",
            "",
            "        finally:",
            "            if predictor_record.training_stop_at is None:",
            "                predictor_record.training_stop_at = datetime.now()",
            "                db.session.commit()",
            "",
            "",
            "class ModelWrapperUnsafe:",
            "    \"\"\" Model wrapper that executes learn/predict in current process",
            "    \"\"\"",
            "",
            "    def __init__(self, code, modules_str, engine_id, engine_version: int):",
            "        module = import_string(code)",
            "        model_class = find_model_class(module)",
            "        self.model_class = model_class",
            "        self.model_instance = self.model_class()",
            "",
            "    def train(self, df, target, args):",
            "        self.model_instance.train(df, target, args)",
            "        return pickle.dumps(self.model_instance.__dict__, protocol=5)",
            "",
            "    def predict(self, df, model_state, args):",
            "        model_state = pickle.loads(model_state)",
            "        self.model_instance.__dict__ = model_state",
            "        try:",
            "            result = self.model_instance.predict(df, args)",
            "        except Exception:",
            "            result = self.model_instance.predict(df)",
            "        return result",
            "",
            "    def finetune(self, df, model_state, args):",
            "        self.model_instance.__dict__ = pickle.loads(model_state)",
            "",
            "        call_args = [df]",
            "        if args:",
            "            call_args.append(args)",
            "",
            "        self.model_instance.finetune(df, args)",
            "",
            "        return pickle.dumps(self.model_instance.__dict__, protocol=5)",
            "",
            "    def describe(self, model_state, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        if hasattr(self.model_instance, 'describe'):",
            "            model_state = pickle.loads(model_state)",
            "            self.model_instance.__dict__ = model_state",
            "            return self.model_instance.describe(attribute)",
            "        return pd.DataFrame()",
            "",
            "    def check(self):",
            "        pass",
            "",
            "",
            "class ModelWrapperSafe:",
            "    \"\"\" Model wrapper that executes learn/predict in venv",
            "    \"\"\"",
            "",
            "    def __init__(self, code, modules_str, engine_id, engine_version: int):",
            "        self.code = code",
            "        modules = self.parse_requirements(modules_str)",
            "",
            "        self.config = Config()",
            "        self.is_cloud = Config().get('cloud', False)",
            "",
            "        self.env_path = None",
            "        self.env_storage_path = None",
            "        self.prepare_env(modules, engine_id, engine_version)",
            "",
            "    def prepare_env(self, modules, engine_id, engine_version: int):",
            "        try:",
            "            import virtualenv",
            "",
            "            base_path = self.config.get('byom', {}).get('venv_path')",
            "            if base_path is None:",
            "                # create in root path",
            "                base_path = Path(self.config.paths['root']) / 'venvs'",
            "            else:",
            "                base_path = Path(base_path)",
            "            base_path.mkdir(parents=True, exist_ok=True)",
            "",
            "            env_folder_name = f'env_{engine_id}'",
            "            if isinstance(engine_version, int) and engine_version > 1:",
            "                env_folder_name = f'{env_folder_name}_{engine_version}'",
            "",
            "            self.env_storage_path = base_path / env_folder_name",
            "            if self.is_cloud:",
            "                bese_env_path = Path(tempfile.gettempdir()) / 'mindsdb' / 'venv'",
            "                bese_env_path.mkdir(parents=True, exist_ok=True)",
            "                self.env_path = bese_env_path / env_folder_name",
            "                tar_path = self.env_storage_path.with_suffix('.tar')",
            "                if self.env_path.exists() is False and tar_path.exists() is True:",
            "                    with tarfile.open(tar_path) as tar:",
            "                        tar.extractall(path=bese_env_path)",
            "            else:",
            "                self.env_path = self.env_storage_path",
            "",
            "            if sys.platform in ('win32', 'cygwin'):",
            "                exectable_folder_name = 'Scripts'",
            "            else:",
            "                exectable_folder_name = 'bin'",
            "",
            "            pip_cmd = self.env_path / exectable_folder_name / 'pip'",
            "            self.python_path = self.env_path / exectable_folder_name / 'python'",
            "",
            "            if self.env_path.exists():",
            "                # already exists. it means requirements are already installed",
            "                return",
            "",
            "            # create",
            "            logger.info(f\"Creating new environment: {self.env_path}\")",
            "            virtualenv.cli_run(['-p', sys.executable, str(self.env_path)])",
            "            logger.info(f\"Created new environment: {self.env_path}\")",
            "",
            "            if len(modules) > 0:",
            "                self.install_modules(modules, pip_cmd=pip_cmd)",
            "        except Exception:",
            "            # DANGER !!! VENV MUST BE CREATED",
            "            logger.info(\"Can't create virtual environment. venv module should be installed\")",
            "",
            "            if self.is_cloud:",
            "                raise",
            "",
            "            self.python_path = Path(sys.executable)",
            "",
            "            # try to install modules everytime",
            "            self.install_modules(modules, pip_cmd=pip_cmd)",
            "",
            "        # fastest way to copy files if destination is NFS",
            "        if self.is_cloud and self.env_storage_path != self.env_path:",
            "            old_cwd = os.getcwd()",
            "            os.chdir(str(bese_env_path))",
            "            tar_path = self.env_path.with_suffix('.tar')",
            "            with tarfile.open(name=str(tar_path), mode='w') as tar:",
            "                tar.add(str(self.env_path.name))",
            "            os.chdir(old_cwd)",
            "            subprocess.run(",
            "                ['cp', '-R', '--no-preserve=mode,ownership', str(tar_path), str(base_path / tar_path.name)],",
            "                check=True, shell=False",
            "            )",
            "            tar_path.unlink()",
            "",
            "    def remove_venv(self):",
            "        if self.env_path is not None and self.env_path.exists():",
            "            shutil.rmtree(str(self.env_path))",
            "",
            "        if self.is_cloud:",
            "            tar_path = self.env_storage_path.with_suffix('.tar')",
            "            tar_path.unlink()",
            "",
            "    def parse_requirements(self, requirements):",
            "        # get requirements from string",
            "        # they should be located at the top of the file, before code",
            "",
            "        pattern = '^[\\w\\\\[\\\\]-]+[=!<>\\s]*[\\d\\.]*[,=!<>\\s]*[\\d\\.]*$'  # noqa",
            "        modules = []",
            "        for line in requirements.split(b'\\n'):",
            "            line = line.decode().strip()",
            "            if line:",
            "                if re.match(pattern, line):",
            "                    modules.append(line)",
            "                else:",
            "                    raise Exception(f'Wrong requirement: {line}')",
            "",
            "        is_pandas = any([m.lower().startswith('pandas') for m in modules])",
            "        if not is_pandas:",
            "            modules.append('pandas >=2.0.0, <2.1.0')",
            "",
            "        # for dataframe serialization",
            "        modules.append('pyarrow==11.0.0')",
            "        return modules",
            "",
            "    def install_modules(self, modules, pip_cmd):",
            "        # install in current environment using pip",
            "        for module in modules:",
            "            logger.debug(f\"BYOM install module: {module}\")",
            "            p = subprocess.Popen([pip_cmd, 'install', module], stderr=subprocess.PIPE)",
            "            p.wait()",
            "            if p.returncode != 0:",
            "                raise Exception(f'Problem with installing module {module}: {p.stderr.read()}')",
            "",
            "    def _run_command(self, params):",
            "        logger.debug(f\"BYOM run command: {params.get('method')}\")",
            "        params_enc = encode(params)",
            "",
            "        wrapper_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'proc_wrapper.py')",
            "        p = subprocess.Popen(",
            "            [str(self.python_path), wrapper_path],",
            "            stdin=subprocess.PIPE,",
            "            stdout=subprocess.PIPE,",
            "            stderr=subprocess.PIPE,",
            "        )",
            "",
            "        p.stdin.write(params_enc)",
            "        p.stdin.close()",
            "        ret_enc = p.stdout.read()",
            "",
            "        p.wait()",
            "",
            "        try:",
            "            ret = decode(ret_enc)",
            "        except (pickle.UnpicklingError, EOFError):",
            "            raise RuntimeError(p.stderr.read())",
            "        return ret",
            "",
            "    def check(self):",
            "        params = {",
            "            'method': BYOM_METHOD.CHECK.value,",
            "            'code': self.code,",
            "        }",
            "        return self._run_command(params)",
            "",
            "    def train(self, df, target, args):",
            "        params = {",
            "            'method': BYOM_METHOD.TRAIN.value,",
            "            'code': self.code,",
            "            'df': pd_encode(df),",
            "            'to_predict': target,",
            "            'args': args,",
            "        }",
            "",
            "        model_state = self._run_command(params)",
            "        return model_state",
            "",
            "    def predict(self, df, model_state, args):",
            "        params = {",
            "            'method': BYOM_METHOD.PREDICT.value,",
            "            'code': self.code,",
            "            'model_state': model_state,",
            "            'df': pd_encode(df),",
            "            'args': args,",
            "        }",
            "        pred_df = self._run_command(params)",
            "        return pd_decode(pred_df)",
            "",
            "    def finetune(self, df, model_state, args):",
            "        params = {",
            "            'method': BYOM_METHOD.FINETUNE.value,",
            "            'code': self.code,",
            "            'model_state': model_state,",
            "            'df': pd_encode(df),",
            "            'args': args,",
            "        }",
            "",
            "        model_state = self._run_command(params)",
            "        return model_state",
            "",
            "    def describe(self, model_state, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        params = {",
            "            'method': BYOM_METHOD.DESCRIBE.value,",
            "            'code': self.code,",
            "            'model_state': model_state,",
            "            'attribute': attribute",
            "        }",
            "        df = self._run_command(params)",
            "        return df",
            "",
            "",
            "connection_args = OrderedDict(",
            "    code={",
            "        'type': ARG_TYPE.PATH,",
            "        'description': 'The path to model code'",
            "    },",
            "    modules={",
            "        'type': ARG_TYPE.PATH,",
            "        'description': 'The path to model requirements'",
            "    }",
            ")"
        ],
        "afterPatchFile": [
            "\"\"\" BYOM: Bring Your Own Model",
            "",
            "env vars to contloll BYOM:",
            " - MINDSDB_BYOM_ENABLED - can BYOM be uysed or not. Locally enabled by default.",
            " - MINDSDB_BYOM_INHOUSE_ENABLED - enable or disable 'inhouse' BYOM usage. Locally enabled by default.",
            " - MINDSDB_BYOM_DEFAULT_TYPE - [inhouse|venv] default byom type. Locally it is 'venv' by default.",
            " - MINDSDB_BYOM_TYPE - [safe|unsafe] - obsolete, same as above.",
            "\"\"\"",
            "",
            "",
            "import os",
            "import re",
            "import sys",
            "import shutil",
            "import pickle",
            "import tarfile",
            "import tempfile",
            "import traceback",
            "import subprocess",
            "from enum import Enum",
            "from pathlib import Path",
            "from datetime import datetime",
            "from typing import Optional, Dict, Union",
            "from collections import OrderedDict",
            "",
            "import pandas as pd",
            "from pandas.api import types as pd_types",
            "",
            "from mindsdb.utilities import log",
            "from mindsdb.utilities.config import Config",
            "from mindsdb.interfaces.storage import db",
            "from mindsdb.integrations.libs.base import BaseMLEngine",
            "from mindsdb.integrations.libs.const import PREDICTOR_STATUS",
            "from mindsdb.integrations.utilities.utils import format_exception_error",
            "from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE",
            "import mindsdb.utilities.profiler as profiler",
            "from mindsdb.utilities.fs import safe_extract",
            "",
            "",
            "from .proc_wrapper import (",
            "    pd_decode, pd_encode, encode, decode, BYOM_METHOD,",
            "    import_string, find_model_class",
            ")",
            "from .__about__ import __version__",
            "",
            "",
            "BYOM_TYPE = Enum('BYOM_TYPE', ['INHOUSE', 'VENV'])",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "class BYOMHandler(BaseMLEngine):",
            "",
            "    name = 'byom'",
            "",
            "    def __init__(self, model_storage, engine_storage, **kwargs) -> None:",
            "        # region check availability",
            "        is_cloud = Config().get('cloud', False)",
            "        if is_cloud is True:",
            "            byom_enabled = os.environ.get('MINDSDB_BYOM_ENABLED', 'false').lower()",
            "            if byom_enabled not in ('true', '1'):",
            "                raise RuntimeError('BYOM is disabled on cloud')",
            "        # endregion",
            "",
            "        self.model_wrapper = None",
            "",
            "        self.inhouse_model_wrapper = None",
            "        self.model_wrappers = {}",
            "",
            "        # region read and save set default byom type",
            "        try:",
            "            self._default_byom_type = BYOM_TYPE.VENV",
            "            if os.environ.get('MINDSDB_BYOM_DEFAULT_TYPE') is not None:",
            "                self._default_byom_type = BYOM_TYPE[",
            "                    os.environ.get('MINDSDB_BYOM_DEFAULT_TYPE').upper()",
            "                ]",
            "            else:",
            "                env_var = os.environ.get('MINDSDB_BYOM_DEFAULT_TYPE')",
            "                if env_var == 'SAVE':",
            "                    self._default_byom_type = BYOM_TYPE['VENV']",
            "                elif env_var == 'UNSAVE':",
            "                    self._default_byom_type = BYOM_TYPE['INHOUSE']",
            "                else:",
            "                    raise KeyError",
            "        except KeyError:",
            "            self._default_byom_type = BYOM_TYPE.VENV",
            "        # endregion",
            "",
            "        # region check if 'inhouse' BYOM is enabled",
            "        env_var = os.environ.get('MINDSDB_BYOM_INHOUSE_ENABLED')",
            "        if env_var is None:",
            "            self._inhouse_enabled = False if is_cloud else True",
            "        else:",
            "            self._inhouse_enabled = env_var.lower() in ('true', '1')",
            "        # endregion",
            "",
            "        super().__init__(model_storage, engine_storage, **kwargs)",
            "",
            "    @staticmethod",
            "    def normalize_engine_version(engine_version: Union[int, str, None]) -> int:",
            "        \"\"\"Cast engine version to int, or return `1` if can not be casted",
            "",
            "        Args:",
            "            engine_version (Union[int, str, None]): engine version",
            "",
            "        Returns:",
            "            int: engine version",
            "        \"\"\"",
            "        if isinstance(engine_version, str):",
            "            try:",
            "                engine_version = int(engine_version)",
            "            except Exception:",
            "                engine_version = 1",
            "        if isinstance(engine_version, int) is False:",
            "            engine_version = 1",
            "        return engine_version",
            "",
            "    @staticmethod",
            "    def create_validation(target: str, args: dict = None, **kwargs) -> None:",
            "        if isinstance(args, dict) is False:",
            "            return",
            "        using_args = args.get('using', {})",
            "        engine_version = using_args.get('engine_version')",
            "        if engine_version is not None:",
            "            engine_version = BYOMHandler.normalize_engine_version(engine_version)",
            "        else:",
            "            connection_args = kwargs['handler_storage'].get_connection_args()",
            "            versions = connection_args.get('versions')",
            "            if isinstance(versions, dict):",
            "                engine_version = max([int(x) for x in versions.keys()])",
            "            else:",
            "                engine_version = 1",
            "            using_args['engine_version'] = engine_version",
            "",
            "    def get_model_engine_version(self) -> int:",
            "        \"\"\"Return current model engine version",
            "",
            "        Returns:",
            "            int: engine version",
            "        \"\"\"",
            "        engine_version = self.model_storage.get_info()['learn_args'].get('using', {}).get('engine_version')",
            "        engine_version = BYOMHandler.normalize_engine_version(engine_version)",
            "        return engine_version",
            "",
            "    def normalize_byom_type(self, byom_type: Optional[str]) -> BYOM_TYPE:",
            "        if byom_type is not None:",
            "            byom_type = BYOM_TYPE[byom_type.upper()]",
            "        else:",
            "            byom_type = self._default_byom_type",
            "        if byom_type == BYOM_TYPE.INHOUSE and self._inhouse_enabled is False:",
            "            raise Exception(\"'Inhouse' BYOM engine type can not be used\")",
            "        return byom_type",
            "",
            "    def _get_model_proxy(self, version=None):",
            "        if version is None:",
            "            version = 1",
            "        if isinstance(version, str):",
            "            version = int(version)",
            "        version_mark = ''",
            "        if version > 1:",
            "            version_mark = f'_{version}'",
            "        version_str = str(version)",
            "",
            "        self.engine_storage.fileStorage.pull()",
            "        try:",
            "            code = self.engine_storage.fileStorage.file_get(f'code{version_mark}')",
            "            modules_str = self.engine_storage.fileStorage.file_get(f'modules{version_mark}')",
            "        except FileNotFoundError:",
            "            raise Exception(f\"Engine version '{version}' does not exists\")",
            "",
            "        if version_str not in self.model_wrappers:",
            "            connection_args = self.engine_storage.get_connection_args()",
            "            version_meta = connection_args['versions'][version_str]",
            "",
            "            try:",
            "                engine_version_type = BYOM_TYPE[",
            "                    version_meta.get('type', self._default_byom_type.name).upper()",
            "                ]",
            "            except KeyError:",
            "                raise Exception('Unknown BYOM engine type')",
            "",
            "            if engine_version_type == BYOM_TYPE.INHOUSE:",
            "                if self._inhouse_enabled is False:",
            "                    raise Exception(\"'Inhouse' BYOM engine type can not be used\")",
            "                if self.inhouse_model_wrapper is None:",
            "                    self.inhouse_model_wrapper = ModelWrapperUnsafe(",
            "                        code=code,",
            "                        modules_str=modules_str,",
            "                        engine_id=self.engine_storage.integration_id,",
            "                        engine_version=version",
            "                    )",
            "                self.model_wrappers[version_str] = self.inhouse_model_wrapper",
            "            elif engine_version_type == BYOM_TYPE.VENV:",
            "                if version_meta.get('venv_status') != 'ready':",
            "                    version_meta['venv_status'] = 'creating'",
            "                    self.engine_storage.update_connection_args(connection_args)",
            "                self.model_wrappers[version_str] = ModelWrapperSafe(",
            "                    code=code,",
            "                    modules_str=modules_str,",
            "                    engine_id=self.engine_storage.integration_id,",
            "                    engine_version=version",
            "                )",
            "                version_meta['venv_status'] = 'ready'",
            "                self.engine_storage.update_connection_args(connection_args)",
            "",
            "        return self.model_wrappers[version_str]",
            "",
            "    def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        engine_version = self.get_model_engine_version()",
            "        mp = self._get_model_proxy(engine_version)",
            "        model_state = self.model_storage.file_get('model')",
            "        return mp.describe(model_state, attribute)",
            "",
            "    def create(self, target, df=None, args=None, **kwargs):",
            "        using_args = args.get('using', {})",
            "        engine_version = using_args.get('engine_version')",
            "",
            "        model_proxy = self._get_model_proxy(engine_version)",
            "        model_state = model_proxy.train(df, target, args)",
            "",
            "        self.model_storage.file_set('model', model_state)",
            "",
            "        # TODO return columns?",
            "",
            "        def convert_type(field_type):",
            "            if pd_types.is_integer_dtype(field_type):",
            "                return 'integer'",
            "            elif pd_types.is_numeric_dtype(field_type):",
            "                return 'float'",
            "            elif pd_types.is_datetime64_any_dtype(field_type):",
            "                return 'datetime'",
            "            else:",
            "                return 'categorical'",
            "",
            "        columns = {",
            "            target: convert_type(object)",
            "        }",
            "",
            "        self.model_storage.columns_set(columns)",
            "",
            "    def predict(self, df, args=None):",
            "        pred_args = args.get('predict_params', {})",
            "",
            "        engine_version = pred_args.get('engine_version')",
            "        if engine_version is not None:",
            "            engine_version = int(engine_version)",
            "        else:",
            "            engine_version = self.get_model_engine_version()",
            "",
            "        model_proxy = self._get_model_proxy(engine_version)",
            "        model_state = self.model_storage.file_get('model')",
            "        pred_df = model_proxy.predict(df, model_state, pred_args)",
            "",
            "        return pred_df",
            "",
            "    def create_engine(self, connection_args):",
            "        code_path = Path(connection_args['code'])",
            "        self.engine_storage.fileStorage.file_set(",
            "            'code',",
            "            code_path.read_bytes()",
            "        )",
            "",
            "        requirements_path = Path(connection_args['modules'])",
            "        self.engine_storage.fileStorage.file_set(",
            "            'modules',",
            "            requirements_path.read_bytes()",
            "        )",
            "",
            "        self.engine_storage.fileStorage.push()",
            "",
            "        self.engine_storage.update_connection_args({",
            "            'handler_version': __version__,",
            "            'versions': {",
            "                '1': {",
            "                    'code': code_path.name,",
            "                    'requirements': requirements_path.name,",
            "                    'type': self.normalize_byom_type(",
            "                        connection_args.get('type')",
            "                    ).name.lower()",
            "                }",
            "            }",
            "        })",
            "",
            "        model_proxy = self._get_model_proxy()",
            "        try:",
            "            model_proxy.check()",
            "        except Exception as e:",
            "            model_proxy.remove_venv()",
            "            raise e",
            "",
            "    def update_engine(self, connection_args: dict) -> None:",
            "        \"\"\"Add new version of engine",
            "",
            "            Args:",
            "                connection_args (dict): paths to code and requirements",
            "        \"\"\"",
            "        code_path = Path(connection_args['code'])",
            "        requirements_path = Path(connection_args['modules'])",
            "",
            "        engine_connection_args = self.engine_storage.get_connection_args()",
            "        if isinstance(engine_connection_args, dict) is False or 'handler_version' not in engine_connection_args:",
            "            engine_connection_args = {",
            "                'handler_version': __version__,",
            "                'versions': {",
            "                    '1': {",
            "                        'code': 'code.py',",
            "                        'requirements': 'requirements.txt',",
            "                        'type': self._default_byom_type.name.lower()",
            "                    }",
            "                }",
            "            }",
            "        new_version = str(max([int(x) for x in engine_connection_args['versions'].keys()]) + 1)",
            "",
            "        engine_connection_args['versions'][new_version] = {",
            "            'code': code_path.name,",
            "            'requirements': requirements_path.name,",
            "            'type': self.normalize_byom_type(",
            "                connection_args.get('type')",
            "            ).name.lower()",
            "        }",
            "",
            "        self.engine_storage.fileStorage.file_set(",
            "            f'code_{new_version}',",
            "            code_path.read_bytes()",
            "        )",
            "",
            "        self.engine_storage.fileStorage.file_set(",
            "            f'modules_{new_version}',",
            "            requirements_path.read_bytes()",
            "        )",
            "        self.engine_storage.fileStorage.push()",
            "",
            "        self.engine_storage.update_connection_args(engine_connection_args)",
            "",
            "        model_proxy = self._get_model_proxy(new_version)",
            "        try:",
            "            model_proxy.check()",
            "        except Exception as e:",
            "            model_proxy.remove_venv()",
            "            raise e",
            "",
            "    def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:",
            "        using_args = args.get('using', {})",
            "        engine_version = using_args.get('engine_version')",
            "",
            "        model_storage = self.model_storage",
            "        # TODO: should probably refactor at some point, as a bit of the logic is shared with lightwood's finetune logic",
            "        try:",
            "            base_predictor_id = args['base_model_id']",
            "            base_predictor_record = db.Predictor.query.get(base_predictor_id)",
            "            if base_predictor_record.status != PREDICTOR_STATUS.COMPLETE:",
            "                raise Exception(\"Base model must be in status 'complete'\")",
            "",
            "            predictor_id = model_storage.predictor_id",
            "            predictor_record = db.Predictor.query.get(predictor_id)",
            "",
            "            predictor_record.data = {'training_log': 'training'}  # TODO move to ModelStorage (don't work w/ db directly)",
            "            predictor_record.training_start_at = datetime.now()",
            "            predictor_record.status = PREDICTOR_STATUS.FINETUNING  # TODO: parallel execution block",
            "            db.session.commit()",
            "",
            "            model_proxy = self._get_model_proxy(engine_version)",
            "            model_state = self.base_model_storage.file_get('model')",
            "            model_state = model_proxy.finetune(df, model_state, args=args.get('using', {}))",
            "",
            "            # region hack to speedup file saving",
            "            with profiler.Context('finetune-byom-write-file'):",
            "                dest_abs_path = model_storage.fileStorage.folder_path / 'model'",
            "                with open(dest_abs_path, 'wb') as fd:",
            "                    fd.write(model_state)",
            "                model_storage.fileStorage.push(compression_level=0)",
            "            # endregion",
            "",
            "            predictor_record.update_status = 'up_to_date'",
            "            predictor_record.status = PREDICTOR_STATUS.COMPLETE",
            "            predictor_record.training_stop_at = datetime.now()",
            "            db.session.commit()",
            "",
            "        except Exception as e:",
            "            logger.error(e)",
            "            predictor_id = model_storage.predictor_id",
            "            predictor_record = db.Predictor.query.with_for_update().get(predictor_id)",
            "            logger.error(traceback.format_exc())",
            "            error_message = format_exception_error(e)",
            "            predictor_record.data = {\"error\": error_message}",
            "            predictor_record.status = PREDICTOR_STATUS.ERROR",
            "            db.session.commit()",
            "            raise",
            "",
            "        finally:",
            "            if predictor_record.training_stop_at is None:",
            "                predictor_record.training_stop_at = datetime.now()",
            "                db.session.commit()",
            "",
            "",
            "class ModelWrapperUnsafe:",
            "    \"\"\" Model wrapper that executes learn/predict in current process",
            "    \"\"\"",
            "",
            "    def __init__(self, code, modules_str, engine_id, engine_version: int):",
            "        module = import_string(code)",
            "        model_class = find_model_class(module)",
            "        self.model_class = model_class",
            "        self.model_instance = self.model_class()",
            "",
            "    def train(self, df, target, args):",
            "        self.model_instance.train(df, target, args)",
            "        return pickle.dumps(self.model_instance.__dict__, protocol=5)",
            "",
            "    def predict(self, df, model_state, args):",
            "        model_state = pickle.loads(model_state)",
            "        self.model_instance.__dict__ = model_state",
            "        try:",
            "            result = self.model_instance.predict(df, args)",
            "        except Exception:",
            "            result = self.model_instance.predict(df)",
            "        return result",
            "",
            "    def finetune(self, df, model_state, args):",
            "        self.model_instance.__dict__ = pickle.loads(model_state)",
            "",
            "        call_args = [df]",
            "        if args:",
            "            call_args.append(args)",
            "",
            "        self.model_instance.finetune(df, args)",
            "",
            "        return pickle.dumps(self.model_instance.__dict__, protocol=5)",
            "",
            "    def describe(self, model_state, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        if hasattr(self.model_instance, 'describe'):",
            "            model_state = pickle.loads(model_state)",
            "            self.model_instance.__dict__ = model_state",
            "            return self.model_instance.describe(attribute)",
            "        return pd.DataFrame()",
            "",
            "    def check(self):",
            "        pass",
            "",
            "",
            "class ModelWrapperSafe:",
            "    \"\"\" Model wrapper that executes learn/predict in venv",
            "    \"\"\"",
            "",
            "    def __init__(self, code, modules_str, engine_id, engine_version: int):",
            "        self.code = code",
            "        modules = self.parse_requirements(modules_str)",
            "",
            "        self.config = Config()",
            "        self.is_cloud = Config().get('cloud', False)",
            "",
            "        self.env_path = None",
            "        self.env_storage_path = None",
            "        self.prepare_env(modules, engine_id, engine_version)",
            "",
            "    def prepare_env(self, modules, engine_id, engine_version: int):",
            "        try:",
            "            import virtualenv",
            "",
            "            base_path = self.config.get('byom', {}).get('venv_path')",
            "            if base_path is None:",
            "                # create in root path",
            "                base_path = Path(self.config.paths['root']) / 'venvs'",
            "            else:",
            "                base_path = Path(base_path)",
            "            base_path.mkdir(parents=True, exist_ok=True)",
            "",
            "            env_folder_name = f'env_{engine_id}'",
            "            if isinstance(engine_version, int) and engine_version > 1:",
            "                env_folder_name = f'{env_folder_name}_{engine_version}'",
            "",
            "            self.env_storage_path = base_path / env_folder_name",
            "            if self.is_cloud:",
            "                bese_env_path = Path(tempfile.gettempdir()) / 'mindsdb' / 'venv'",
            "                bese_env_path.mkdir(parents=True, exist_ok=True)",
            "                self.env_path = bese_env_path / env_folder_name",
            "                tar_path = self.env_storage_path.with_suffix('.tar')",
            "                if self.env_path.exists() is False and tar_path.exists() is True:",
            "                    with tarfile.open(tar_path) as tar:",
            "                        safe_extract(tar, path=bese_env_path)",
            "            else:",
            "                self.env_path = self.env_storage_path",
            "",
            "            if sys.platform in ('win32', 'cygwin'):",
            "                exectable_folder_name = 'Scripts'",
            "            else:",
            "                exectable_folder_name = 'bin'",
            "",
            "            pip_cmd = self.env_path / exectable_folder_name / 'pip'",
            "            self.python_path = self.env_path / exectable_folder_name / 'python'",
            "",
            "            if self.env_path.exists():",
            "                # already exists. it means requirements are already installed",
            "                return",
            "",
            "            # create",
            "            logger.info(f\"Creating new environment: {self.env_path}\")",
            "            virtualenv.cli_run(['-p', sys.executable, str(self.env_path)])",
            "            logger.info(f\"Created new environment: {self.env_path}\")",
            "",
            "            if len(modules) > 0:",
            "                self.install_modules(modules, pip_cmd=pip_cmd)",
            "        except Exception:",
            "            # DANGER !!! VENV MUST BE CREATED",
            "            logger.info(\"Can't create virtual environment. venv module should be installed\")",
            "",
            "            if self.is_cloud:",
            "                raise",
            "",
            "            self.python_path = Path(sys.executable)",
            "",
            "            # try to install modules everytime",
            "            self.install_modules(modules, pip_cmd=pip_cmd)",
            "",
            "        # fastest way to copy files if destination is NFS",
            "        if self.is_cloud and self.env_storage_path != self.env_path:",
            "            old_cwd = os.getcwd()",
            "            os.chdir(str(bese_env_path))",
            "            tar_path = self.env_path.with_suffix('.tar')",
            "            with tarfile.open(name=str(tar_path), mode='w') as tar:",
            "                tar.add(str(self.env_path.name))",
            "            os.chdir(old_cwd)",
            "            subprocess.run(",
            "                ['cp', '-R', '--no-preserve=mode,ownership', str(tar_path), str(base_path / tar_path.name)],",
            "                check=True, shell=False",
            "            )",
            "            tar_path.unlink()",
            "",
            "    def remove_venv(self):",
            "        if self.env_path is not None and self.env_path.exists():",
            "            shutil.rmtree(str(self.env_path))",
            "",
            "        if self.is_cloud:",
            "            tar_path = self.env_storage_path.with_suffix('.tar')",
            "            tar_path.unlink()",
            "",
            "    def parse_requirements(self, requirements):",
            "        # get requirements from string",
            "        # they should be located at the top of the file, before code",
            "",
            "        pattern = '^[\\w\\\\[\\\\]-]+[=!<>\\s]*[\\d\\.]*[,=!<>\\s]*[\\d\\.]*$'  # noqa",
            "        modules = []",
            "        for line in requirements.split(b'\\n'):",
            "            line = line.decode().strip()",
            "            if line:",
            "                if re.match(pattern, line):",
            "                    modules.append(line)",
            "                else:",
            "                    raise Exception(f'Wrong requirement: {line}')",
            "",
            "        is_pandas = any([m.lower().startswith('pandas') for m in modules])",
            "        if not is_pandas:",
            "            modules.append('pandas >=2.0.0, <2.1.0')",
            "",
            "        # for dataframe serialization",
            "        modules.append('pyarrow==11.0.0')",
            "        return modules",
            "",
            "    def install_modules(self, modules, pip_cmd):",
            "        # install in current environment using pip",
            "        for module in modules:",
            "            logger.debug(f\"BYOM install module: {module}\")",
            "            p = subprocess.Popen([pip_cmd, 'install', module], stderr=subprocess.PIPE)",
            "            p.wait()",
            "            if p.returncode != 0:",
            "                raise Exception(f'Problem with installing module {module}: {p.stderr.read()}')",
            "",
            "    def _run_command(self, params):",
            "        logger.debug(f\"BYOM run command: {params.get('method')}\")",
            "        params_enc = encode(params)",
            "",
            "        wrapper_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'proc_wrapper.py')",
            "        p = subprocess.Popen(",
            "            [str(self.python_path), wrapper_path],",
            "            stdin=subprocess.PIPE,",
            "            stdout=subprocess.PIPE,",
            "            stderr=subprocess.PIPE,",
            "        )",
            "",
            "        p.stdin.write(params_enc)",
            "        p.stdin.close()",
            "        ret_enc = p.stdout.read()",
            "",
            "        p.wait()",
            "",
            "        try:",
            "            ret = decode(ret_enc)",
            "        except (pickle.UnpicklingError, EOFError):",
            "            raise RuntimeError(p.stderr.read())",
            "        return ret",
            "",
            "    def check(self):",
            "        params = {",
            "            'method': BYOM_METHOD.CHECK.value,",
            "            'code': self.code,",
            "        }",
            "        return self._run_command(params)",
            "",
            "    def train(self, df, target, args):",
            "        params = {",
            "            'method': BYOM_METHOD.TRAIN.value,",
            "            'code': self.code,",
            "            'df': pd_encode(df),",
            "            'to_predict': target,",
            "            'args': args,",
            "        }",
            "",
            "        model_state = self._run_command(params)",
            "        return model_state",
            "",
            "    def predict(self, df, model_state, args):",
            "        params = {",
            "            'method': BYOM_METHOD.PREDICT.value,",
            "            'code': self.code,",
            "            'model_state': model_state,",
            "            'df': pd_encode(df),",
            "            'args': args,",
            "        }",
            "        pred_df = self._run_command(params)",
            "        return pd_decode(pred_df)",
            "",
            "    def finetune(self, df, model_state, args):",
            "        params = {",
            "            'method': BYOM_METHOD.FINETUNE.value,",
            "            'code': self.code,",
            "            'model_state': model_state,",
            "            'df': pd_encode(df),",
            "            'args': args,",
            "        }",
            "",
            "        model_state = self._run_command(params)",
            "        return model_state",
            "",
            "    def describe(self, model_state, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        params = {",
            "            'method': BYOM_METHOD.DESCRIBE.value,",
            "            'code': self.code,",
            "            'model_state': model_state,",
            "            'attribute': attribute",
            "        }",
            "        df = self._run_command(params)",
            "        return df",
            "",
            "",
            "connection_args = OrderedDict(",
            "    code={",
            "        'type': ARG_TYPE.PATH,",
            "        'description': 'The path to model code'",
            "    },",
            "    modules={",
            "        'type': ARG_TYPE.PATH,",
            "        'description': 'The path to model requirements'",
            "    }",
            ")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "478": [
                "ModelWrapperSafe",
                "prepare_env"
            ]
        },
        "addLocation": []
    },
    "mindsdb/interfaces/storage/fs.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from mindsdb.utilities.context import context as ctx"
            },
            "1": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " import mindsdb.utilities.profiler as profiler"
            },
            "2": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " from mindsdb.utilities import log"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+from mindsdb.utilities.fs import safe_extract"
            },
            "4": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " logger = log.getLogger(__name__)"
            },
            "6": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "             fh = io.BytesIO()"
            },
            "8": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": 301,
                "PatchRowcode": "             self.s3.download_fileobj(self.bucket, remote_ziped_name, fh)"
            },
            "9": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": 302,
                "PatchRowcode": "             with tarfile.open(fileobj=fh) as tar:"
            },
            "10": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                tar.extractall(path=base_dir)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+                safe_extract(tar, path=base_dir)"
            },
            "12": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "         else:"
            },
            "13": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "             self.s3.download_file(self.bucket, remote_ziped_name, local_ziped_path)"
            },
            "14": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "             shutil.unpack_archive(local_ziped_path, base_dir)"
            }
        },
        "frontPatchFile": [
            "import os",
            "import io",
            "import shutil",
            "import tarfile",
            "import hashlib",
            "from pathlib import Path",
            "from abc import ABC, abstractmethod",
            "from typing import Union, Optional",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import threading",
            "",
            "if os.name == 'posix':",
            "    import fcntl",
            "",
            "import psutil",
            "from checksumdir import dirhash",
            "try:",
            "    import boto3",
            "    from botocore.exceptions import ClientError as S3ClientError",
            "except Exception:",
            "    # Only required for remote storage on s3",
            "    S3ClientError = FileNotFoundError",
            "    pass",
            "",
            "",
            "from mindsdb.utilities.config import Config",
            "from mindsdb.utilities.context import context as ctx",
            "import mindsdb.utilities.profiler as profiler",
            "from mindsdb.utilities import log",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "@dataclass(frozen=True)",
            "class RESOURCE_GROUP:",
            "    PREDICTOR = 'predictor'",
            "    INTEGRATION = 'integration'",
            "    TAB = 'tab'",
            "",
            "",
            "RESOURCE_GROUP = RESOURCE_GROUP()",
            "",
            "",
            "DIR_LOCK_FILE_NAME = 'dir.lock'",
            "DIR_LAST_MODIFIED_FILE_NAME = 'last_modified.txt'",
            "SERVICE_FILES_NAMES = (DIR_LOCK_FILE_NAME, DIR_LAST_MODIFIED_FILE_NAME)",
            "",
            "",
            "def copy(src, dst):",
            "    if os.path.isdir(src):",
            "        if os.path.exists(dst):",
            "            if dirhash(src) == dirhash(dst):",
            "                return",
            "        shutil.rmtree(dst, ignore_errors=True)",
            "        shutil.copytree(src, dst, dirs_exist_ok=True)",
            "    else:",
            "        if os.path.exists(dst):",
            "            if hashlib.md5(open(src, 'rb').read()).hexdigest() == hashlib.md5(open(dst, 'rb').read()).hexdigest():",
            "                return",
            "        try:",
            "            os.remove(dst)",
            "        except Exception:",
            "            pass",
            "        shutil.copy2(src, dst)",
            "",
            "",
            "class BaseFSStore(ABC):",
            "    \"\"\"Base class for file storage",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        self.config = Config()",
            "        self.storage = self.config['paths']['storage']",
            "",
            "    @abstractmethod",
            "    def get(self, local_name, base_dir):",
            "        \"\"\"Copy file/folder from storage to {base_dir}",
            "",
            "        Args:",
            "            local_name (str): name of resource (file/folder)",
            "            base_dir (str): path to copy the resource",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def put(self, local_name, base_dir):",
            "        \"\"\"Copy file/folder from {base_dir} to storage",
            "",
            "        Args:",
            "            local_name (str): name of resource (file/folder)",
            "            base_dir (str): path to folder with the resource",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def delete(self, remote_name):",
            "        \"\"\"Delete file/folder from storage",
            "",
            "        Args:",
            "            remote_name (str): name of resource",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "def get_dir_size(path: str):",
            "    total = 0",
            "    with os.scandir(path) as it:",
            "        for entry in it:",
            "            if entry.is_file():",
            "                total += entry.stat().st_size",
            "            elif entry.is_dir():",
            "                total += get_dir_size(entry.path)",
            "    return total",
            "",
            "",
            "class LocalFSStore(BaseFSStore):",
            "    \"\"\"Storage that stores files locally",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "",
            "    def get(self, local_name, base_dir):",
            "        remote_name = local_name",
            "        src = os.path.join(self.storage, remote_name)",
            "        dest = os.path.join(base_dir, local_name)",
            "        if not os.path.exists(dest) or get_dir_size(src) != get_dir_size(dest):",
            "            copy(src, dest)",
            "",
            "    def put(self, local_name, base_dir, compression_level=9):",
            "        remote_name = local_name",
            "        copy(",
            "            os.path.join(base_dir, local_name),",
            "            os.path.join(self.storage, remote_name)",
            "        )",
            "",
            "    def delete(self, remote_name):",
            "        path = Path(self.storage).joinpath(remote_name)",
            "        try:",
            "            if path.is_file():",
            "                path.unlink()",
            "            else:",
            "                shutil.rmtree(path)",
            "        except FileNotFoundError:",
            "            pass",
            "",
            "",
            "class FileLock:",
            "    \"\"\" file lock to make safe concurrent access to directory",
            "        works as context",
            "    \"\"\"",
            "",
            "    @staticmethod",
            "    def lock_folder_path(relative_path: Path) -> Path:",
            "        \"\"\" Args:",
            "                relative_path (Path): path to resource directory relative to storage root",
            "",
            "            Returns:",
            "                Path: abs path to folder with lock file",
            "        \"\"\"",
            "        config = Config()",
            "        root_storage_path = Path(config.paths['root'])",
            "        return config.paths['locks'] / relative_path.relative_to(root_storage_path)",
            "",
            "    def __init__(self, relative_path: Path, mode: str = 'w'):",
            "        \"\"\" Args:",
            "                relative_path (Path): path to resource directory relative to storage root",
            "                mode (str): lock for read (r) or write (w)",
            "        \"\"\"",
            "        if os.name != 'posix':",
            "            return",
            "",
            "        self._local_path = FileLock.lock_folder_path(relative_path)",
            "        self._lock_file_name = DIR_LOCK_FILE_NAME",
            "        self._lock_file_path = self._local_path / self._lock_file_name",
            "        self._mode = fcntl.LOCK_EX if mode == 'w' else fcntl.LOCK_SH",
            "",
            "        if self._lock_file_path.is_file() is False:",
            "            self._local_path.mkdir(parents=True, exist_ok=True)",
            "            try:",
            "                self._lock_file_path.write_text('')",
            "            except Exception:",
            "                pass",
            "",
            "    def __enter__(self):",
            "        if os.name != 'posix':",
            "            return",
            "",
            "        try:",
            "            # On at least some systems, LOCK_EX can only be used if the file",
            "            # descriptor refers to a file opened for writing.",
            "            self._lock_fd = os.open(self._lock_file_path, os.O_RDWR | os.O_CREAT)",
            "            fcntl.lockf(self._lock_fd, self._mode | fcntl.LOCK_NB)",
            "        except (ValueError, FileNotFoundError):",
            "            # file probably was deleted between open and lock",
            "            logger.error(f'Cant accure lock on {self._local_path}')",
            "            raise FileNotFoundError",
            "        except BlockingIOError:",
            "            logger.error(f'Directory is locked by another process: {self._local_path}')",
            "            fcntl.lockf(self._lock_fd, self._mode)",
            "",
            "    def __exit__(self, exc_type, exc_value, traceback):",
            "        if os.name != 'posix':",
            "            return",
            "",
            "        try:",
            "            fcntl.lockf(self._lock_fd, fcntl.LOCK_UN)",
            "            os.close(self._lock_fd)",
            "        except Exception:",
            "            pass",
            "",
            "",
            "class S3FSStore(BaseFSStore):",
            "    \"\"\"Storage that stores files in amazon s3",
            "    \"\"\"",
            "",
            "    dt_format = '%d.%m.%y %H:%M:%S.%f'",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "        if 's3_credentials' in self.config['permanent_storage']:",
            "            self.s3 = boto3.client('s3', **self.config['permanent_storage']['s3_credentials'])",
            "        else:",
            "            self.s3 = boto3.client('s3')",
            "        self.bucket = self.config['permanent_storage']['bucket']",
            "        self._thread_lock = threading.Lock()",
            "",
            "    def _get_remote_last_modified(self, object_name: str) -> datetime:",
            "        \"\"\" get time when object was created/modified",
            "",
            "            Args:",
            "                object_name (str): name if file in bucket",
            "",
            "            Returns:",
            "                datetime",
            "        \"\"\"",
            "        last_modified = self.s3.get_object_attributes(",
            "            Bucket=self.bucket,",
            "            Key=object_name,",
            "            ObjectAttributes=['Checksum']",
            "        )['LastModified']",
            "        last_modified = last_modified.replace(tzinfo=None)",
            "        return last_modified",
            "",
            "    @profiler.profile()",
            "    def _get_local_last_modified(self, base_dir: str, local_name: str) -> datetime:",
            "        \"\"\" get 'last_modified' that saved locally",
            "",
            "            Args:",
            "                base_dir (str): path to base folder",
            "                local_name (str): folder name",
            "",
            "            Returns:",
            "                datetime | None",
            "        \"\"\"",
            "        last_modified_file_path = Path(base_dir) / local_name / DIR_LAST_MODIFIED_FILE_NAME",
            "        if last_modified_file_path.is_file() is False:",
            "            return None",
            "        try:",
            "            last_modified_text = last_modified_file_path.read_text()",
            "            last_modified_datetime = datetime.strptime(last_modified_text, self.dt_format)",
            "        except Exception:",
            "            return None",
            "        return last_modified_datetime",
            "",
            "    @profiler.profile()",
            "    def _save_local_last_modified(self, base_dir: str, local_name: str, last_modified: datetime):",
            "        \"\"\" Save 'last_modified' to local folder",
            "",
            "            Args:",
            "                base_dir (str): path to base folder",
            "                local_name (str): folder name",
            "                last_modified (datetime)",
            "        \"\"\"",
            "        last_modified_file_path = Path(base_dir) / local_name / DIR_LAST_MODIFIED_FILE_NAME",
            "        last_modified_text = last_modified.strftime(self.dt_format)",
            "        last_modified_file_path.write_text(last_modified_text)",
            "",
            "    @profiler.profile()",
            "    def _download(self, base_dir: str, remote_ziped_name: str,",
            "                  local_ziped_path: str, last_modified: datetime = None):",
            "        \"\"\" download file to s3 and unarchive it",
            "",
            "            Args:",
            "                base_dir (str)",
            "                remote_ziped_name (str)",
            "                local_ziped_path (str)",
            "                last_modified (datetime, optional)",
            "        \"\"\"",
            "        os.makedirs(base_dir, exist_ok=True)",
            "",
            "        remote_size = self.s3.get_object_attributes(",
            "            Bucket=self.bucket,",
            "            Key=remote_ziped_name,",
            "            ObjectAttributes=['ObjectSize']",
            "        )['ObjectSize']",
            "        if (remote_size * 2) > psutil.virtual_memory().available:",
            "            fh = io.BytesIO()",
            "            self.s3.download_fileobj(self.bucket, remote_ziped_name, fh)",
            "            with tarfile.open(fileobj=fh) as tar:",
            "                tar.extractall(path=base_dir)",
            "        else:",
            "            self.s3.download_file(self.bucket, remote_ziped_name, local_ziped_path)",
            "            shutil.unpack_archive(local_ziped_path, base_dir)",
            "            os.remove(local_ziped_path)",
            "",
            "        # os.system(f'chmod -R 777 {base_dir}')",
            "",
            "        if last_modified is None:",
            "            last_modified = self._get_remote_last_modified(remote_ziped_name)",
            "        self._save_local_last_modified(",
            "            base_dir,",
            "            remote_ziped_name.replace('.tar.gz', ''),",
            "            last_modified",
            "        )",
            "",
            "    @profiler.profile()",
            "    def get(self, local_name, base_dir):",
            "        remote_name = local_name",
            "        remote_ziped_name = f'{remote_name}.tar.gz'",
            "        local_ziped_name = f'{local_name}.tar.gz'",
            "        local_ziped_path = os.path.join(base_dir, local_ziped_name)",
            "",
            "        folder_path = Path(base_dir) / local_name",
            "        with FileLock(folder_path, mode='r'):",
            "            local_last_modified = self._get_local_last_modified(base_dir, local_name)",
            "            remote_last_modified = self._get_remote_last_modified(remote_ziped_name)",
            "            if (",
            "                local_last_modified is not None",
            "                and local_last_modified == remote_last_modified",
            "            ):",
            "                return",
            "",
            "        with FileLock(folder_path, mode='w'):",
            "            self._download(",
            "                base_dir,",
            "                remote_ziped_name,",
            "                local_ziped_path,",
            "                last_modified=remote_last_modified",
            "            )",
            "",
            "    @profiler.profile()",
            "    def put(self, local_name, base_dir, compression_level=9):",
            "        # NOTE: This `make_archive` function is implemente poorly and will create an empty archive file even if",
            "        # the file/dir to be archived doesn't exist or for some other reason can't be archived",
            "        remote_name = local_name",
            "        remote_zipped_name = f'{remote_name}.tar.gz'",
            "",
            "        dir_path = Path(base_dir) / remote_name",
            "        dir_size = sum(f.stat().st_size for f in dir_path.glob('**/*') if f.is_file())",
            "        if (dir_size * 2) < psutil.virtual_memory().available:",
            "            old_cwd = os.getcwd()",
            "            fh = io.BytesIO()",
            "            with self._thread_lock:",
            "                os.chdir(base_dir)",
            "                with tarfile.open(fileobj=fh, mode='w:gz', compresslevel=compression_level) as tar:",
            "                    for path in dir_path.iterdir():",
            "                        if path.is_file() and path.name in SERVICE_FILES_NAMES:",
            "                            continue",
            "                        tar.add(path.relative_to(base_dir))",
            "                os.chdir(old_cwd)",
            "            fh.seek(0)",
            "",
            "            self.s3.upload_fileobj(",
            "                fh,",
            "                self.bucket,",
            "                remote_zipped_name",
            "            )",
            "        else:",
            "            shutil.make_archive(",
            "                os.path.join(base_dir, remote_name),",
            "                'gztar',",
            "                root_dir=base_dir,",
            "                base_dir=local_name",
            "            )",
            "",
            "            self.s3.upload_file(",
            "                os.path.join(base_dir, remote_zipped_name),",
            "                self.bucket,",
            "                remote_zipped_name",
            "            )",
            "            os.remove(os.path.join(base_dir, remote_zipped_name))",
            "",
            "        last_modified = self._get_remote_last_modified(remote_zipped_name)",
            "        self._save_local_last_modified(base_dir, local_name, last_modified)",
            "",
            "    @profiler.profile()",
            "    def delete(self, remote_name):",
            "        self.s3.delete_object(Bucket=self.bucket, Key=remote_name)",
            "",
            "",
            "def FsStore():",
            "    storage_location = Config()['permanent_storage']['location']",
            "    if storage_location == 'local':",
            "        return LocalFSStore()",
            "    elif storage_location == 's3':",
            "        return S3FSStore()",
            "    else:",
            "        raise Exception(f\"Location: '{storage_location}' not supported\")",
            "",
            "",
            "class FileStorage:",
            "    def __init__(self, resource_group: str, resource_id: int,",
            "                 root_dir: str = 'content', sync: bool = True):",
            "        \"\"\"",
            "            Args:",
            "                resource_group (str)",
            "                resource_id (int)",
            "                root_dir (str)",
            "                sync (bool)",
            "        \"\"\"",
            "",
            "        self.resource_group = resource_group",
            "        self.resource_id = resource_id",
            "        self.root_dir = root_dir",
            "        self.sync = sync",
            "",
            "        self.folder_name = f'{resource_group}_{ctx.company_id}_{resource_id}'",
            "",
            "        config = Config()",
            "        self.fs_store = FsStore()",
            "        self.content_path = Path(config['paths'][root_dir])",
            "        self.resource_group_path = self.content_path / resource_group",
            "        self.folder_path = self.resource_group_path / self.folder_name",
            "        if self.folder_path.exists() is False:",
            "            self.folder_path.mkdir(parents=True, exist_ok=True)",
            "",
            "    @profiler.profile()",
            "    def push(self, compression_level: int = 9):",
            "        with FileLock(self.folder_path, mode='r'):",
            "            self._push_no_lock(compression_level=compression_level)",
            "",
            "    @profiler.profile()",
            "    def _push_no_lock(self, compression_level: int = 9):",
            "        self.fs_store.put(",
            "            str(self.folder_name),",
            "            str(self.resource_group_path),",
            "            compression_level=compression_level",
            "        )",
            "",
            "    @profiler.profile()",
            "    def push_path(self, path, compression_level: int = 9):",
            "        # TODO implement push per element",
            "        self.push(compression_level=compression_level)",
            "",
            "    @profiler.profile()",
            "    def pull(self):",
            "        try:",
            "            self.fs_store.get(",
            "                str(self.folder_name),",
            "                str(self.resource_group_path)",
            "            )",
            "        except (FileNotFoundError, S3ClientError):",
            "            pass",
            "",
            "    @profiler.profile()",
            "    def pull_path(self, path):",
            "        # TODO implement pull per element",
            "        self.pull()",
            "",
            "    @profiler.profile()",
            "    def file_set(self, name, content):",
            "        if self.sync is True:",
            "            self.pull()",
            "",
            "        with FileLock(self.folder_path, mode='w'):",
            "",
            "            dest_abs_path = self.folder_path / name",
            "",
            "            with open(dest_abs_path, 'wb') as fd:",
            "                fd.write(content)",
            "",
            "            if self.sync is True:",
            "                self._push_no_lock()",
            "",
            "    @profiler.profile()",
            "    def file_get(self, name):",
            "        if self.sync is True:",
            "            self.pull()",
            "        dest_abs_path = self.folder_path / name",
            "        with FileLock(self.folder_path, mode='r'):",
            "            with open(dest_abs_path, 'rb') as fd:",
            "                return fd.read()",
            "",
            "    @profiler.profile()",
            "    def add(self, path: Union[str, Path], dest_rel_path: Optional[Union[str, Path]] = None):",
            "        \"\"\"Copy file/folder to persist storage",
            "",
            "        Examples:",
            "            Copy file 'args.json' to '{storage}/args.json'",
            "            >>> fs.add('/path/args.json')",
            "",
            "            Copy file 'args.json' to '{storage}/folder/opts.json'",
            "            >>> fs.add('/path/args.json', 'folder/opts.json')",
            "",
            "            Copy folder 'folder' to '{storage}/folder'",
            "            >>> fs.add('/path/folder')",
            "",
            "            Copy folder 'folder' to '{storage}/path/folder'",
            "            >>> fs.add('/path/folder', 'path/folder')",
            "",
            "        Args:",
            "            path (Union[str, Path]): path to the resource",
            "            dest_rel_path (Optional[Union[str, Path]]): relative path in storage to file or folder",
            "        \"\"\"",
            "        if self.sync is True:",
            "            self.pull()",
            "        with FileLock(self.folder_path, mode='w'):",
            "",
            "            path = Path(path)",
            "            if isinstance(dest_rel_path, str):",
            "                dest_rel_path = Path(dest_rel_path)",
            "",
            "            if dest_rel_path is None:",
            "                dest_abs_path = self.folder_path / path.name",
            "            else:",
            "                dest_abs_path = self.folder_path / dest_rel_path",
            "",
            "            copy(",
            "                str(path),",
            "                str(dest_abs_path)",
            "            )",
            "",
            "            if self.sync is True:",
            "                self._push_no_lock()",
            "",
            "    @profiler.profile()",
            "    def get_path(self, relative_path: Union[str, Path]) -> Path:",
            "        \"\"\" Return path to file or folder",
            "",
            "        Examples:",
            "            get path to 'opts.json':",
            "            >>> fs.get_path('folder/opts.json')",
            "            ... /path/{storage}/folder/opts.json",
            "",
            "        Args:",
            "            relative_path (Union[str, Path]): Path relative to the storage folder",
            "",
            "        Returns:",
            "            Path: path to requested file or folder",
            "        \"\"\"",
            "        if self.sync is True:",
            "            self.pull()",
            "",
            "        with FileLock(self.folder_path, mode='r'):",
            "            if isinstance(relative_path, str):",
            "                relative_path = Path(relative_path)",
            "            # relative_path = relative_path.resolve()",
            "",
            "            if relative_path.is_absolute():",
            "                raise TypeError('FSStorage.get_path() got absolute path as argument')",
            "",
            "            ret_path = self.folder_path / relative_path",
            "            if not ret_path.exists():",
            "                # raise Exception('Path does not exists')",
            "                os.makedirs(ret_path)",
            "",
            "        return ret_path",
            "",
            "    def delete(self, relative_path: Union[str, Path] = '.'):",
            "        path = (self.folder_path / relative_path).resolve()",
            "        if isinstance(relative_path, str):",
            "            relative_path = Path(relative_path)",
            "",
            "        if relative_path.is_absolute():",
            "            raise TypeError('FSStorage.delete() got absolute path as argument')",
            "",
            "        # complete removal",
            "        if path == self.folder_path.resolve():",
            "            with FileLock(self.folder_path, mode='w'):",
            "                self.fs_store.delete(self.folder_name)",
            "                # NOTE on some fs .rmtree is not working if any file is open",
            "                shutil.rmtree(str(self.folder_path))",
            "",
            "            # region del file lock",
            "            lock_folder_path = FileLock.lock_folder_path(self.folder_path)",
            "            try:",
            "                shutil.rmtree(lock_folder_path)",
            "            except FileNotFoundError:",
            "                logger.warning('Tried to delete file not found: %s', lock_folder_path)",
            "            except Exception as e:",
            "                raise e",
            "            # endregion",
            "            return",
            "",
            "        with FileLock(self.folder_path, mode='w'):",
            "            if self.sync is True:",
            "                self._pull_no_lock()",
            "",
            "            if path.exists() is False:",
            "                raise Exception('Path does not exists')",
            "",
            "            if path.is_file():",
            "                path.unlink()",
            "            else:",
            "                path.rmdir()",
            "",
            "            if self.sync is True:",
            "                self._push_no_lock()",
            "",
            "",
            "class FileStorageFactory:",
            "    def __init__(self, resource_group: str,",
            "                 root_dir: str = 'content', sync: bool = True):",
            "        self.resource_group = resource_group",
            "        self.root_dir = root_dir",
            "        self.sync = sync",
            "",
            "    def __call__(self, resource_id: int):",
            "        return FileStorage(",
            "            resource_group=self.resource_group,",
            "            root_dir=self.root_dir,",
            "            sync=self.sync,",
            "            resource_id=resource_id",
            "        )"
        ],
        "afterPatchFile": [
            "import os",
            "import io",
            "import shutil",
            "import tarfile",
            "import hashlib",
            "from pathlib import Path",
            "from abc import ABC, abstractmethod",
            "from typing import Union, Optional",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "import threading",
            "",
            "if os.name == 'posix':",
            "    import fcntl",
            "",
            "import psutil",
            "from checksumdir import dirhash",
            "try:",
            "    import boto3",
            "    from botocore.exceptions import ClientError as S3ClientError",
            "except Exception:",
            "    # Only required for remote storage on s3",
            "    S3ClientError = FileNotFoundError",
            "    pass",
            "",
            "",
            "from mindsdb.utilities.config import Config",
            "from mindsdb.utilities.context import context as ctx",
            "import mindsdb.utilities.profiler as profiler",
            "from mindsdb.utilities import log",
            "from mindsdb.utilities.fs import safe_extract",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "@dataclass(frozen=True)",
            "class RESOURCE_GROUP:",
            "    PREDICTOR = 'predictor'",
            "    INTEGRATION = 'integration'",
            "    TAB = 'tab'",
            "",
            "",
            "RESOURCE_GROUP = RESOURCE_GROUP()",
            "",
            "",
            "DIR_LOCK_FILE_NAME = 'dir.lock'",
            "DIR_LAST_MODIFIED_FILE_NAME = 'last_modified.txt'",
            "SERVICE_FILES_NAMES = (DIR_LOCK_FILE_NAME, DIR_LAST_MODIFIED_FILE_NAME)",
            "",
            "",
            "def copy(src, dst):",
            "    if os.path.isdir(src):",
            "        if os.path.exists(dst):",
            "            if dirhash(src) == dirhash(dst):",
            "                return",
            "        shutil.rmtree(dst, ignore_errors=True)",
            "        shutil.copytree(src, dst, dirs_exist_ok=True)",
            "    else:",
            "        if os.path.exists(dst):",
            "            if hashlib.md5(open(src, 'rb').read()).hexdigest() == hashlib.md5(open(dst, 'rb').read()).hexdigest():",
            "                return",
            "        try:",
            "            os.remove(dst)",
            "        except Exception:",
            "            pass",
            "        shutil.copy2(src, dst)",
            "",
            "",
            "class BaseFSStore(ABC):",
            "    \"\"\"Base class for file storage",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        self.config = Config()",
            "        self.storage = self.config['paths']['storage']",
            "",
            "    @abstractmethod",
            "    def get(self, local_name, base_dir):",
            "        \"\"\"Copy file/folder from storage to {base_dir}",
            "",
            "        Args:",
            "            local_name (str): name of resource (file/folder)",
            "            base_dir (str): path to copy the resource",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def put(self, local_name, base_dir):",
            "        \"\"\"Copy file/folder from {base_dir} to storage",
            "",
            "        Args:",
            "            local_name (str): name of resource (file/folder)",
            "            base_dir (str): path to folder with the resource",
            "        \"\"\"",
            "        pass",
            "",
            "    @abstractmethod",
            "    def delete(self, remote_name):",
            "        \"\"\"Delete file/folder from storage",
            "",
            "        Args:",
            "            remote_name (str): name of resource",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "def get_dir_size(path: str):",
            "    total = 0",
            "    with os.scandir(path) as it:",
            "        for entry in it:",
            "            if entry.is_file():",
            "                total += entry.stat().st_size",
            "            elif entry.is_dir():",
            "                total += get_dir_size(entry.path)",
            "    return total",
            "",
            "",
            "class LocalFSStore(BaseFSStore):",
            "    \"\"\"Storage that stores files locally",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "",
            "    def get(self, local_name, base_dir):",
            "        remote_name = local_name",
            "        src = os.path.join(self.storage, remote_name)",
            "        dest = os.path.join(base_dir, local_name)",
            "        if not os.path.exists(dest) or get_dir_size(src) != get_dir_size(dest):",
            "            copy(src, dest)",
            "",
            "    def put(self, local_name, base_dir, compression_level=9):",
            "        remote_name = local_name",
            "        copy(",
            "            os.path.join(base_dir, local_name),",
            "            os.path.join(self.storage, remote_name)",
            "        )",
            "",
            "    def delete(self, remote_name):",
            "        path = Path(self.storage).joinpath(remote_name)",
            "        try:",
            "            if path.is_file():",
            "                path.unlink()",
            "            else:",
            "                shutil.rmtree(path)",
            "        except FileNotFoundError:",
            "            pass",
            "",
            "",
            "class FileLock:",
            "    \"\"\" file lock to make safe concurrent access to directory",
            "        works as context",
            "    \"\"\"",
            "",
            "    @staticmethod",
            "    def lock_folder_path(relative_path: Path) -> Path:",
            "        \"\"\" Args:",
            "                relative_path (Path): path to resource directory relative to storage root",
            "",
            "            Returns:",
            "                Path: abs path to folder with lock file",
            "        \"\"\"",
            "        config = Config()",
            "        root_storage_path = Path(config.paths['root'])",
            "        return config.paths['locks'] / relative_path.relative_to(root_storage_path)",
            "",
            "    def __init__(self, relative_path: Path, mode: str = 'w'):",
            "        \"\"\" Args:",
            "                relative_path (Path): path to resource directory relative to storage root",
            "                mode (str): lock for read (r) or write (w)",
            "        \"\"\"",
            "        if os.name != 'posix':",
            "            return",
            "",
            "        self._local_path = FileLock.lock_folder_path(relative_path)",
            "        self._lock_file_name = DIR_LOCK_FILE_NAME",
            "        self._lock_file_path = self._local_path / self._lock_file_name",
            "        self._mode = fcntl.LOCK_EX if mode == 'w' else fcntl.LOCK_SH",
            "",
            "        if self._lock_file_path.is_file() is False:",
            "            self._local_path.mkdir(parents=True, exist_ok=True)",
            "            try:",
            "                self._lock_file_path.write_text('')",
            "            except Exception:",
            "                pass",
            "",
            "    def __enter__(self):",
            "        if os.name != 'posix':",
            "            return",
            "",
            "        try:",
            "            # On at least some systems, LOCK_EX can only be used if the file",
            "            # descriptor refers to a file opened for writing.",
            "            self._lock_fd = os.open(self._lock_file_path, os.O_RDWR | os.O_CREAT)",
            "            fcntl.lockf(self._lock_fd, self._mode | fcntl.LOCK_NB)",
            "        except (ValueError, FileNotFoundError):",
            "            # file probably was deleted between open and lock",
            "            logger.error(f'Cant accure lock on {self._local_path}')",
            "            raise FileNotFoundError",
            "        except BlockingIOError:",
            "            logger.error(f'Directory is locked by another process: {self._local_path}')",
            "            fcntl.lockf(self._lock_fd, self._mode)",
            "",
            "    def __exit__(self, exc_type, exc_value, traceback):",
            "        if os.name != 'posix':",
            "            return",
            "",
            "        try:",
            "            fcntl.lockf(self._lock_fd, fcntl.LOCK_UN)",
            "            os.close(self._lock_fd)",
            "        except Exception:",
            "            pass",
            "",
            "",
            "class S3FSStore(BaseFSStore):",
            "    \"\"\"Storage that stores files in amazon s3",
            "    \"\"\"",
            "",
            "    dt_format = '%d.%m.%y %H:%M:%S.%f'",
            "",
            "    def __init__(self):",
            "        super().__init__()",
            "        if 's3_credentials' in self.config['permanent_storage']:",
            "            self.s3 = boto3.client('s3', **self.config['permanent_storage']['s3_credentials'])",
            "        else:",
            "            self.s3 = boto3.client('s3')",
            "        self.bucket = self.config['permanent_storage']['bucket']",
            "        self._thread_lock = threading.Lock()",
            "",
            "    def _get_remote_last_modified(self, object_name: str) -> datetime:",
            "        \"\"\" get time when object was created/modified",
            "",
            "            Args:",
            "                object_name (str): name if file in bucket",
            "",
            "            Returns:",
            "                datetime",
            "        \"\"\"",
            "        last_modified = self.s3.get_object_attributes(",
            "            Bucket=self.bucket,",
            "            Key=object_name,",
            "            ObjectAttributes=['Checksum']",
            "        )['LastModified']",
            "        last_modified = last_modified.replace(tzinfo=None)",
            "        return last_modified",
            "",
            "    @profiler.profile()",
            "    def _get_local_last_modified(self, base_dir: str, local_name: str) -> datetime:",
            "        \"\"\" get 'last_modified' that saved locally",
            "",
            "            Args:",
            "                base_dir (str): path to base folder",
            "                local_name (str): folder name",
            "",
            "            Returns:",
            "                datetime | None",
            "        \"\"\"",
            "        last_modified_file_path = Path(base_dir) / local_name / DIR_LAST_MODIFIED_FILE_NAME",
            "        if last_modified_file_path.is_file() is False:",
            "            return None",
            "        try:",
            "            last_modified_text = last_modified_file_path.read_text()",
            "            last_modified_datetime = datetime.strptime(last_modified_text, self.dt_format)",
            "        except Exception:",
            "            return None",
            "        return last_modified_datetime",
            "",
            "    @profiler.profile()",
            "    def _save_local_last_modified(self, base_dir: str, local_name: str, last_modified: datetime):",
            "        \"\"\" Save 'last_modified' to local folder",
            "",
            "            Args:",
            "                base_dir (str): path to base folder",
            "                local_name (str): folder name",
            "                last_modified (datetime)",
            "        \"\"\"",
            "        last_modified_file_path = Path(base_dir) / local_name / DIR_LAST_MODIFIED_FILE_NAME",
            "        last_modified_text = last_modified.strftime(self.dt_format)",
            "        last_modified_file_path.write_text(last_modified_text)",
            "",
            "    @profiler.profile()",
            "    def _download(self, base_dir: str, remote_ziped_name: str,",
            "                  local_ziped_path: str, last_modified: datetime = None):",
            "        \"\"\" download file to s3 and unarchive it",
            "",
            "            Args:",
            "                base_dir (str)",
            "                remote_ziped_name (str)",
            "                local_ziped_path (str)",
            "                last_modified (datetime, optional)",
            "        \"\"\"",
            "        os.makedirs(base_dir, exist_ok=True)",
            "",
            "        remote_size = self.s3.get_object_attributes(",
            "            Bucket=self.bucket,",
            "            Key=remote_ziped_name,",
            "            ObjectAttributes=['ObjectSize']",
            "        )['ObjectSize']",
            "        if (remote_size * 2) > psutil.virtual_memory().available:",
            "            fh = io.BytesIO()",
            "            self.s3.download_fileobj(self.bucket, remote_ziped_name, fh)",
            "            with tarfile.open(fileobj=fh) as tar:",
            "                safe_extract(tar, path=base_dir)",
            "        else:",
            "            self.s3.download_file(self.bucket, remote_ziped_name, local_ziped_path)",
            "            shutil.unpack_archive(local_ziped_path, base_dir)",
            "            os.remove(local_ziped_path)",
            "",
            "        # os.system(f'chmod -R 777 {base_dir}')",
            "",
            "        if last_modified is None:",
            "            last_modified = self._get_remote_last_modified(remote_ziped_name)",
            "        self._save_local_last_modified(",
            "            base_dir,",
            "            remote_ziped_name.replace('.tar.gz', ''),",
            "            last_modified",
            "        )",
            "",
            "    @profiler.profile()",
            "    def get(self, local_name, base_dir):",
            "        remote_name = local_name",
            "        remote_ziped_name = f'{remote_name}.tar.gz'",
            "        local_ziped_name = f'{local_name}.tar.gz'",
            "        local_ziped_path = os.path.join(base_dir, local_ziped_name)",
            "",
            "        folder_path = Path(base_dir) / local_name",
            "        with FileLock(folder_path, mode='r'):",
            "            local_last_modified = self._get_local_last_modified(base_dir, local_name)",
            "            remote_last_modified = self._get_remote_last_modified(remote_ziped_name)",
            "            if (",
            "                local_last_modified is not None",
            "                and local_last_modified == remote_last_modified",
            "            ):",
            "                return",
            "",
            "        with FileLock(folder_path, mode='w'):",
            "            self._download(",
            "                base_dir,",
            "                remote_ziped_name,",
            "                local_ziped_path,",
            "                last_modified=remote_last_modified",
            "            )",
            "",
            "    @profiler.profile()",
            "    def put(self, local_name, base_dir, compression_level=9):",
            "        # NOTE: This `make_archive` function is implemente poorly and will create an empty archive file even if",
            "        # the file/dir to be archived doesn't exist or for some other reason can't be archived",
            "        remote_name = local_name",
            "        remote_zipped_name = f'{remote_name}.tar.gz'",
            "",
            "        dir_path = Path(base_dir) / remote_name",
            "        dir_size = sum(f.stat().st_size for f in dir_path.glob('**/*') if f.is_file())",
            "        if (dir_size * 2) < psutil.virtual_memory().available:",
            "            old_cwd = os.getcwd()",
            "            fh = io.BytesIO()",
            "            with self._thread_lock:",
            "                os.chdir(base_dir)",
            "                with tarfile.open(fileobj=fh, mode='w:gz', compresslevel=compression_level) as tar:",
            "                    for path in dir_path.iterdir():",
            "                        if path.is_file() and path.name in SERVICE_FILES_NAMES:",
            "                            continue",
            "                        tar.add(path.relative_to(base_dir))",
            "                os.chdir(old_cwd)",
            "            fh.seek(0)",
            "",
            "            self.s3.upload_fileobj(",
            "                fh,",
            "                self.bucket,",
            "                remote_zipped_name",
            "            )",
            "        else:",
            "            shutil.make_archive(",
            "                os.path.join(base_dir, remote_name),",
            "                'gztar',",
            "                root_dir=base_dir,",
            "                base_dir=local_name",
            "            )",
            "",
            "            self.s3.upload_file(",
            "                os.path.join(base_dir, remote_zipped_name),",
            "                self.bucket,",
            "                remote_zipped_name",
            "            )",
            "            os.remove(os.path.join(base_dir, remote_zipped_name))",
            "",
            "        last_modified = self._get_remote_last_modified(remote_zipped_name)",
            "        self._save_local_last_modified(base_dir, local_name, last_modified)",
            "",
            "    @profiler.profile()",
            "    def delete(self, remote_name):",
            "        self.s3.delete_object(Bucket=self.bucket, Key=remote_name)",
            "",
            "",
            "def FsStore():",
            "    storage_location = Config()['permanent_storage']['location']",
            "    if storage_location == 'local':",
            "        return LocalFSStore()",
            "    elif storage_location == 's3':",
            "        return S3FSStore()",
            "    else:",
            "        raise Exception(f\"Location: '{storage_location}' not supported\")",
            "",
            "",
            "class FileStorage:",
            "    def __init__(self, resource_group: str, resource_id: int,",
            "                 root_dir: str = 'content', sync: bool = True):",
            "        \"\"\"",
            "            Args:",
            "                resource_group (str)",
            "                resource_id (int)",
            "                root_dir (str)",
            "                sync (bool)",
            "        \"\"\"",
            "",
            "        self.resource_group = resource_group",
            "        self.resource_id = resource_id",
            "        self.root_dir = root_dir",
            "        self.sync = sync",
            "",
            "        self.folder_name = f'{resource_group}_{ctx.company_id}_{resource_id}'",
            "",
            "        config = Config()",
            "        self.fs_store = FsStore()",
            "        self.content_path = Path(config['paths'][root_dir])",
            "        self.resource_group_path = self.content_path / resource_group",
            "        self.folder_path = self.resource_group_path / self.folder_name",
            "        if self.folder_path.exists() is False:",
            "            self.folder_path.mkdir(parents=True, exist_ok=True)",
            "",
            "    @profiler.profile()",
            "    def push(self, compression_level: int = 9):",
            "        with FileLock(self.folder_path, mode='r'):",
            "            self._push_no_lock(compression_level=compression_level)",
            "",
            "    @profiler.profile()",
            "    def _push_no_lock(self, compression_level: int = 9):",
            "        self.fs_store.put(",
            "            str(self.folder_name),",
            "            str(self.resource_group_path),",
            "            compression_level=compression_level",
            "        )",
            "",
            "    @profiler.profile()",
            "    def push_path(self, path, compression_level: int = 9):",
            "        # TODO implement push per element",
            "        self.push(compression_level=compression_level)",
            "",
            "    @profiler.profile()",
            "    def pull(self):",
            "        try:",
            "            self.fs_store.get(",
            "                str(self.folder_name),",
            "                str(self.resource_group_path)",
            "            )",
            "        except (FileNotFoundError, S3ClientError):",
            "            pass",
            "",
            "    @profiler.profile()",
            "    def pull_path(self, path):",
            "        # TODO implement pull per element",
            "        self.pull()",
            "",
            "    @profiler.profile()",
            "    def file_set(self, name, content):",
            "        if self.sync is True:",
            "            self.pull()",
            "",
            "        with FileLock(self.folder_path, mode='w'):",
            "",
            "            dest_abs_path = self.folder_path / name",
            "",
            "            with open(dest_abs_path, 'wb') as fd:",
            "                fd.write(content)",
            "",
            "            if self.sync is True:",
            "                self._push_no_lock()",
            "",
            "    @profiler.profile()",
            "    def file_get(self, name):",
            "        if self.sync is True:",
            "            self.pull()",
            "        dest_abs_path = self.folder_path / name",
            "        with FileLock(self.folder_path, mode='r'):",
            "            with open(dest_abs_path, 'rb') as fd:",
            "                return fd.read()",
            "",
            "    @profiler.profile()",
            "    def add(self, path: Union[str, Path], dest_rel_path: Optional[Union[str, Path]] = None):",
            "        \"\"\"Copy file/folder to persist storage",
            "",
            "        Examples:",
            "            Copy file 'args.json' to '{storage}/args.json'",
            "            >>> fs.add('/path/args.json')",
            "",
            "            Copy file 'args.json' to '{storage}/folder/opts.json'",
            "            >>> fs.add('/path/args.json', 'folder/opts.json')",
            "",
            "            Copy folder 'folder' to '{storage}/folder'",
            "            >>> fs.add('/path/folder')",
            "",
            "            Copy folder 'folder' to '{storage}/path/folder'",
            "            >>> fs.add('/path/folder', 'path/folder')",
            "",
            "        Args:",
            "            path (Union[str, Path]): path to the resource",
            "            dest_rel_path (Optional[Union[str, Path]]): relative path in storage to file or folder",
            "        \"\"\"",
            "        if self.sync is True:",
            "            self.pull()",
            "        with FileLock(self.folder_path, mode='w'):",
            "",
            "            path = Path(path)",
            "            if isinstance(dest_rel_path, str):",
            "                dest_rel_path = Path(dest_rel_path)",
            "",
            "            if dest_rel_path is None:",
            "                dest_abs_path = self.folder_path / path.name",
            "            else:",
            "                dest_abs_path = self.folder_path / dest_rel_path",
            "",
            "            copy(",
            "                str(path),",
            "                str(dest_abs_path)",
            "            )",
            "",
            "            if self.sync is True:",
            "                self._push_no_lock()",
            "",
            "    @profiler.profile()",
            "    def get_path(self, relative_path: Union[str, Path]) -> Path:",
            "        \"\"\" Return path to file or folder",
            "",
            "        Examples:",
            "            get path to 'opts.json':",
            "            >>> fs.get_path('folder/opts.json')",
            "            ... /path/{storage}/folder/opts.json",
            "",
            "        Args:",
            "            relative_path (Union[str, Path]): Path relative to the storage folder",
            "",
            "        Returns:",
            "            Path: path to requested file or folder",
            "        \"\"\"",
            "        if self.sync is True:",
            "            self.pull()",
            "",
            "        with FileLock(self.folder_path, mode='r'):",
            "            if isinstance(relative_path, str):",
            "                relative_path = Path(relative_path)",
            "            # relative_path = relative_path.resolve()",
            "",
            "            if relative_path.is_absolute():",
            "                raise TypeError('FSStorage.get_path() got absolute path as argument')",
            "",
            "            ret_path = self.folder_path / relative_path",
            "            if not ret_path.exists():",
            "                # raise Exception('Path does not exists')",
            "                os.makedirs(ret_path)",
            "",
            "        return ret_path",
            "",
            "    def delete(self, relative_path: Union[str, Path] = '.'):",
            "        path = (self.folder_path / relative_path).resolve()",
            "        if isinstance(relative_path, str):",
            "            relative_path = Path(relative_path)",
            "",
            "        if relative_path.is_absolute():",
            "            raise TypeError('FSStorage.delete() got absolute path as argument')",
            "",
            "        # complete removal",
            "        if path == self.folder_path.resolve():",
            "            with FileLock(self.folder_path, mode='w'):",
            "                self.fs_store.delete(self.folder_name)",
            "                # NOTE on some fs .rmtree is not working if any file is open",
            "                shutil.rmtree(str(self.folder_path))",
            "",
            "            # region del file lock",
            "            lock_folder_path = FileLock.lock_folder_path(self.folder_path)",
            "            try:",
            "                shutil.rmtree(lock_folder_path)",
            "            except FileNotFoundError:",
            "                logger.warning('Tried to delete file not found: %s', lock_folder_path)",
            "            except Exception as e:",
            "                raise e",
            "            # endregion",
            "            return",
            "",
            "        with FileLock(self.folder_path, mode='w'):",
            "            if self.sync is True:",
            "                self._pull_no_lock()",
            "",
            "            if path.exists() is False:",
            "                raise Exception('Path does not exists')",
            "",
            "            if path.is_file():",
            "                path.unlink()",
            "            else:",
            "                path.rmdir()",
            "",
            "            if self.sync is True:",
            "                self._push_no_lock()",
            "",
            "",
            "class FileStorageFactory:",
            "    def __init__(self, resource_group: str,",
            "                 root_dir: str = 'content', sync: bool = True):",
            "        self.resource_group = resource_group",
            "        self.root_dir = root_dir",
            "        self.sync = sync",
            "",
            "    def __call__(self, resource_id: int):",
            "        return FileStorage(",
            "            resource_group=self.resource_group,",
            "            root_dir=self.root_dir,",
            "            sync=self.sync,",
            "            resource_id=resource_id",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "302": [
                "S3FSStore",
                "_download"
            ]
        },
        "addLocation": []
    },
    "mindsdb/utilities/fs.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 162,
                "PatchRowcode": "                     f\"We have mark for process/thread {process_id}/{thread_id} but it does not exists\""
            },
            "1": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 163,
                "PatchRowcode": "                 )"
            },
            "2": {
                "beforePatchRowNumber": 164,
                "afterPatchRowNumber": 164,
                "PatchRowcode": "                 file.unlink()"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 166,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+def __is_within_directory(directory, target):"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+    abs_directory = os.path.abspath(directory)"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+    abs_target = os.path.abspath(target)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+    prefix = os.path.commonprefix([abs_directory, abs_target])"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+    return prefix == abs_directory"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 172,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 173,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 174,
                "PatchRowcode": "+def safe_extract(tarfile, path=\".\", members=None, *, numeric_owner=False):"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 175,
                "PatchRowcode": "+    # for py >= 3.12"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+    if hasattr(tarfile, 'data_filter'):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+        tarfile.extractall(path, members, numeric_owner, filter='data')"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+        return"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 179,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+    # for py < 3.12"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+    for member in tarfile.getmembers():"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+        member_path = os.path.join(path, member.name)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+        if not __is_within_directory(path, member_path):"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+            raise Exception(\"Attempted Path Traversal in Tar File\")"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+    tarfile.extractall(path, members, numeric_owner)"
            }
        },
        "frontPatchFile": [
            "import os",
            "import tempfile",
            "import threading",
            "import time",
            "from pathlib import Path",
            "from typing import Optional",
            "",
            "import psutil",
            "from appdirs import user_data_dir",
            "",
            "from mindsdb.utilities import log",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "def create_directory(path):",
            "    path = Path(path)",
            "    path.mkdir(mode=0o777, exist_ok=True, parents=True)",
            "",
            "",
            "def get_root_path():",
            "    mindsdb_path = user_data_dir(\"mindsdb\", \"mindsdb\")",
            "    return os.path.join(mindsdb_path, \"var/\")",
            "",
            "",
            "def get_or_create_data_dir():",
            "    data_dir = user_data_dir(\"mindsdb\", \"mindsdb\")",
            "    mindsdb_data_dir = os.path.join(data_dir, \"var/\")",
            "",
            "    if os.path.exists(mindsdb_data_dir) is False:",
            "        create_directory(mindsdb_data_dir)",
            "",
            "    try:",
            "        assert os.path.exists(mindsdb_data_dir)",
            "        assert os.access(mindsdb_data_dir, os.W_OK) is True",
            "    except Exception:",
            "        raise Exception(",
            "            \"MindsDB storage directory does not exist and could not be created\"",
            "        )",
            "",
            "    return mindsdb_data_dir",
            "",
            "",
            "def create_dirs_recursive(path):",
            "    if isinstance(path, dict):",
            "        for p in path.values():",
            "            create_dirs_recursive(p)",
            "    elif isinstance(path, str):",
            "        create_directory(path)",
            "    else:",
            "        raise ValueError(f\"Wrong path: {path}\")",
            "",
            "",
            "def _get_process_mark_id(unified: bool = False) -> str:",
            "    \"\"\"Creates a text that can be used to identify process+thread",
            "    Args:",
            "        unified: bool, if True then result will be same for same process+thread",
            "    Returns:",
            "        mark of process+thread",
            "    \"\"\"",
            "    mark = f\"{os.getpid()}-{threading.get_native_id()}\"",
            "    if unified is True:",
            "        return mark",
            "    return f\"{mark}-{str(time.time()).replace('.', '')}\"",
            "",
            "",
            "def create_process_mark(folder=\"learn\"):",
            "    mark = None",
            "    if os.name == \"posix\":",
            "        p = Path(tempfile.gettempdir()).joinpath(f\"mindsdb/processes/{folder}/\")",
            "        p.mkdir(parents=True, exist_ok=True)",
            "        mark = _get_process_mark_id()",
            "        p.joinpath(mark).touch()",
            "    return mark",
            "",
            "",
            "def set_process_mark(folder: str, mark: str) -> None:",
            "    \"\"\"touch new file which will be process mark",
            "",
            "    Args:",
            "        folder (str): where create the file",
            "        mark (str): file name",
            "",
            "    Returns:",
            "        str: process mark",
            "    \"\"\"",
            "    if os.name != \"posix\":",
            "        return",
            "    p = Path(tempfile.gettempdir()).joinpath(f\"mindsdb/processes/{folder}/\")",
            "    p.mkdir(parents=True, exist_ok=True)",
            "    mark = f\"{os.getpid()}-{threading.get_native_id()}-{mark}\"",
            "    p.joinpath(mark).touch()",
            "    return mark",
            "",
            "",
            "def delete_process_mark(folder: str = \"learn\", mark: Optional[str] = None):",
            "    if mark is None:",
            "        mark = _get_process_mark_id()",
            "    if os.name == \"posix\":",
            "        p = (",
            "            Path(tempfile.gettempdir())",
            "            .joinpath(f\"mindsdb/processes/{folder}/\")",
            "            .joinpath(mark)",
            "        )",
            "        if p.exists():",
            "            p.unlink()",
            "",
            "",
            "def clean_process_marks():",
            "    \"\"\"delete all existing processes marks\"\"\"",
            "    if os.name != \"posix\":",
            "        return",
            "",
            "    logger.debug(\"Deleting PIDs..\")",
            "    p = Path(tempfile.gettempdir()).joinpath(\"mindsdb/processes/\")",
            "    if p.exists() is False:",
            "        return",
            "    for path in p.iterdir():",
            "        if path.is_dir() is False:",
            "            return",
            "        for file in path.iterdir():",
            "            file.unlink()",
            "",
            "",
            "def clean_unlinked_process_marks():",
            "    \"\"\"delete marks that does not have corresponded processes/threads\"\"\"",
            "    if os.name != \"posix\":",
            "        return",
            "",
            "    p = Path(tempfile.gettempdir()).joinpath(\"mindsdb/processes/\")",
            "    if p.exists() is False:",
            "        return",
            "    for path in p.iterdir():",
            "        if path.is_dir() is False:",
            "            return",
            "        for file in path.iterdir():",
            "            parts = file.name.split(\"-\")",
            "            process_id = int(parts[0])",
            "            thread_id = int(parts[1])",
            "",
            "            try:",
            "                process = psutil.Process(process_id)",
            "                if process.status() in (psutil.STATUS_ZOMBIE, psutil.STATUS_DEAD):",
            "                    raise psutil.NoSuchProcess(process_id)",
            "",
            "                threads = process.threads()",
            "                try:",
            "                    next(t for t in threads if t.id == thread_id)",
            "                except StopIteration:",
            "                    logger.warning(",
            "                        f\"We have mark for process/thread {process_id}/{thread_id} but it does not exists\"",
            "                    )",
            "                    file.unlink()",
            "",
            "            except psutil.AccessDenied:",
            "                logger.warning(f\"access to {process_id} denied\")",
            "",
            "                continue",
            "",
            "            except psutil.NoSuchProcess:",
            "                logger.warning(",
            "                    f\"We have mark for process/thread {process_id}/{thread_id} but it does not exists\"",
            "                )",
            "                file.unlink()"
        ],
        "afterPatchFile": [
            "import os",
            "import tempfile",
            "import threading",
            "import time",
            "from pathlib import Path",
            "from typing import Optional",
            "",
            "import psutil",
            "from appdirs import user_data_dir",
            "",
            "from mindsdb.utilities import log",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "def create_directory(path):",
            "    path = Path(path)",
            "    path.mkdir(mode=0o777, exist_ok=True, parents=True)",
            "",
            "",
            "def get_root_path():",
            "    mindsdb_path = user_data_dir(\"mindsdb\", \"mindsdb\")",
            "    return os.path.join(mindsdb_path, \"var/\")",
            "",
            "",
            "def get_or_create_data_dir():",
            "    data_dir = user_data_dir(\"mindsdb\", \"mindsdb\")",
            "    mindsdb_data_dir = os.path.join(data_dir, \"var/\")",
            "",
            "    if os.path.exists(mindsdb_data_dir) is False:",
            "        create_directory(mindsdb_data_dir)",
            "",
            "    try:",
            "        assert os.path.exists(mindsdb_data_dir)",
            "        assert os.access(mindsdb_data_dir, os.W_OK) is True",
            "    except Exception:",
            "        raise Exception(",
            "            \"MindsDB storage directory does not exist and could not be created\"",
            "        )",
            "",
            "    return mindsdb_data_dir",
            "",
            "",
            "def create_dirs_recursive(path):",
            "    if isinstance(path, dict):",
            "        for p in path.values():",
            "            create_dirs_recursive(p)",
            "    elif isinstance(path, str):",
            "        create_directory(path)",
            "    else:",
            "        raise ValueError(f\"Wrong path: {path}\")",
            "",
            "",
            "def _get_process_mark_id(unified: bool = False) -> str:",
            "    \"\"\"Creates a text that can be used to identify process+thread",
            "    Args:",
            "        unified: bool, if True then result will be same for same process+thread",
            "    Returns:",
            "        mark of process+thread",
            "    \"\"\"",
            "    mark = f\"{os.getpid()}-{threading.get_native_id()}\"",
            "    if unified is True:",
            "        return mark",
            "    return f\"{mark}-{str(time.time()).replace('.', '')}\"",
            "",
            "",
            "def create_process_mark(folder=\"learn\"):",
            "    mark = None",
            "    if os.name == \"posix\":",
            "        p = Path(tempfile.gettempdir()).joinpath(f\"mindsdb/processes/{folder}/\")",
            "        p.mkdir(parents=True, exist_ok=True)",
            "        mark = _get_process_mark_id()",
            "        p.joinpath(mark).touch()",
            "    return mark",
            "",
            "",
            "def set_process_mark(folder: str, mark: str) -> None:",
            "    \"\"\"touch new file which will be process mark",
            "",
            "    Args:",
            "        folder (str): where create the file",
            "        mark (str): file name",
            "",
            "    Returns:",
            "        str: process mark",
            "    \"\"\"",
            "    if os.name != \"posix\":",
            "        return",
            "    p = Path(tempfile.gettempdir()).joinpath(f\"mindsdb/processes/{folder}/\")",
            "    p.mkdir(parents=True, exist_ok=True)",
            "    mark = f\"{os.getpid()}-{threading.get_native_id()}-{mark}\"",
            "    p.joinpath(mark).touch()",
            "    return mark",
            "",
            "",
            "def delete_process_mark(folder: str = \"learn\", mark: Optional[str] = None):",
            "    if mark is None:",
            "        mark = _get_process_mark_id()",
            "    if os.name == \"posix\":",
            "        p = (",
            "            Path(tempfile.gettempdir())",
            "            .joinpath(f\"mindsdb/processes/{folder}/\")",
            "            .joinpath(mark)",
            "        )",
            "        if p.exists():",
            "            p.unlink()",
            "",
            "",
            "def clean_process_marks():",
            "    \"\"\"delete all existing processes marks\"\"\"",
            "    if os.name != \"posix\":",
            "        return",
            "",
            "    logger.debug(\"Deleting PIDs..\")",
            "    p = Path(tempfile.gettempdir()).joinpath(\"mindsdb/processes/\")",
            "    if p.exists() is False:",
            "        return",
            "    for path in p.iterdir():",
            "        if path.is_dir() is False:",
            "            return",
            "        for file in path.iterdir():",
            "            file.unlink()",
            "",
            "",
            "def clean_unlinked_process_marks():",
            "    \"\"\"delete marks that does not have corresponded processes/threads\"\"\"",
            "    if os.name != \"posix\":",
            "        return",
            "",
            "    p = Path(tempfile.gettempdir()).joinpath(\"mindsdb/processes/\")",
            "    if p.exists() is False:",
            "        return",
            "    for path in p.iterdir():",
            "        if path.is_dir() is False:",
            "            return",
            "        for file in path.iterdir():",
            "            parts = file.name.split(\"-\")",
            "            process_id = int(parts[0])",
            "            thread_id = int(parts[1])",
            "",
            "            try:",
            "                process = psutil.Process(process_id)",
            "                if process.status() in (psutil.STATUS_ZOMBIE, psutil.STATUS_DEAD):",
            "                    raise psutil.NoSuchProcess(process_id)",
            "",
            "                threads = process.threads()",
            "                try:",
            "                    next(t for t in threads if t.id == thread_id)",
            "                except StopIteration:",
            "                    logger.warning(",
            "                        f\"We have mark for process/thread {process_id}/{thread_id} but it does not exists\"",
            "                    )",
            "                    file.unlink()",
            "",
            "            except psutil.AccessDenied:",
            "                logger.warning(f\"access to {process_id} denied\")",
            "",
            "                continue",
            "",
            "            except psutil.NoSuchProcess:",
            "                logger.warning(",
            "                    f\"We have mark for process/thread {process_id}/{thread_id} but it does not exists\"",
            "                )",
            "                file.unlink()",
            "",
            "",
            "def __is_within_directory(directory, target):",
            "    abs_directory = os.path.abspath(directory)",
            "    abs_target = os.path.abspath(target)",
            "    prefix = os.path.commonprefix([abs_directory, abs_target])",
            "    return prefix == abs_directory",
            "",
            "",
            "def safe_extract(tarfile, path=\".\", members=None, *, numeric_owner=False):",
            "    # for py >= 3.12",
            "    if hasattr(tarfile, 'data_filter'):",
            "        tarfile.extractall(path, members, numeric_owner, filter='data')",
            "        return",
            "",
            "    # for py < 3.12",
            "    for member in tarfile.getmembers():",
            "        member_path = os.path.join(path, member.name)",
            "        if not __is_within_directory(path, member_path):",
            "            raise Exception(\"Attempted Path Traversal in Tar File\")",
            "    tarfile.extractall(path, members, numeric_owner)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "tenable_jira.cli"
        ]
    }
}