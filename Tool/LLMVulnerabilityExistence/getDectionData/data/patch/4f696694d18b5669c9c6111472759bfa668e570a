{
    "salt/crypt.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 696,
                "afterPatchRowNumber": 696,
                "PatchRowcode": "                 self._authenticate_future.set_exception(error)"
            },
            "1": {
                "beforePatchRowNumber": 697,
                "afterPatchRowNumber": 697,
                "PatchRowcode": "             else:"
            },
            "2": {
                "beforePatchRowNumber": 698,
                "afterPatchRowNumber": 698,
                "PatchRowcode": "                 key = self.__key(self.opts)"
            },
            "3": {
                "beforePatchRowNumber": 699,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                AsyncAuth.creds_map[key] = creds"
            },
            "4": {
                "beforePatchRowNumber": 700,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self._creds = creds"
            },
            "5": {
                "beforePatchRowNumber": 701,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self._crypticle = Crypticle(self.opts, creds[\"aes\"])"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 699,
                "PatchRowcode": "+                if key not in AsyncAuth.creds_map:"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 700,
                "PatchRowcode": "+                    log.debug(\"%s Got new master aes key.\", self)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 701,
                "PatchRowcode": "+                    AsyncAuth.creds_map[key] = creds"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 702,
                "PatchRowcode": "+                    self._creds = creds"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 703,
                "PatchRowcode": "+                    self._crypticle = Crypticle(self.opts, creds[\"aes\"])"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 704,
                "PatchRowcode": "+                elif self._creds[\"aes\"] != creds[\"aes\"]:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 705,
                "PatchRowcode": "+                    log.debug(\"%s The master's aes key has changed.\", self)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 706,
                "PatchRowcode": "+                    AsyncAuth.creds_map[key] = creds"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 707,
                "PatchRowcode": "+                    self._creds = creds"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 708,
                "PatchRowcode": "+                    self._crypticle = Crypticle(self.opts, creds[\"aes\"])"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 709,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": 702,
                "afterPatchRowNumber": 710,
                "PatchRowcode": "                 self._authenticate_future.set_result("
            },
            "18": {
                "beforePatchRowNumber": 703,
                "afterPatchRowNumber": 711,
                "PatchRowcode": "                     True"
            },
            "19": {
                "beforePatchRowNumber": 704,
                "afterPatchRowNumber": 712,
                "PatchRowcode": "                 )  # mark the sign-in as complete"
            },
            "20": {
                "beforePatchRowNumber": 1277,
                "afterPatchRowNumber": 1285,
                "PatchRowcode": "         self.serial = salt.payload.Serial(self.opts)"
            },
            "21": {
                "beforePatchRowNumber": 1278,
                "afterPatchRowNumber": 1286,
                "PatchRowcode": "         self.pub_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pub\")"
            },
            "22": {
                "beforePatchRowNumber": 1279,
                "afterPatchRowNumber": 1287,
                "PatchRowcode": "         self.rsa_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1288,
                "PatchRowcode": "+        self._creds = None"
            },
            "24": {
                "beforePatchRowNumber": 1280,
                "afterPatchRowNumber": 1289,
                "PatchRowcode": "         if \"syndic_master\" in self.opts:"
            },
            "25": {
                "beforePatchRowNumber": 1281,
                "afterPatchRowNumber": 1290,
                "PatchRowcode": "             self.mpub = \"syndic_master.pub\""
            },
            "26": {
                "beforePatchRowNumber": 1282,
                "afterPatchRowNumber": 1291,
                "PatchRowcode": "         elif \"alert_master\" in self.opts:"
            },
            "27": {
                "beforePatchRowNumber": 1346,
                "afterPatchRowNumber": 1355,
                "PatchRowcode": "                         )"
            },
            "28": {
                "beforePatchRowNumber": 1347,
                "afterPatchRowNumber": 1356,
                "PatchRowcode": "                     continue"
            },
            "29": {
                "beforePatchRowNumber": 1348,
                "afterPatchRowNumber": 1357,
                "PatchRowcode": "                 break"
            },
            "30": {
                "beforePatchRowNumber": 1349,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._creds = creds"
            },
            "31": {
                "beforePatchRowNumber": 1350,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._crypticle = Crypticle(self.opts, creds[\"aes\"])"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1358,
                "PatchRowcode": "+            if self._creds is None:"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1359,
                "PatchRowcode": "+                log.error(\"%s Got new master aes key.\", self)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1360,
                "PatchRowcode": "+                self._creds = creds"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1361,
                "PatchRowcode": "+                self._crypticle = Crypticle(self.opts, creds[\"aes\"])"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1362,
                "PatchRowcode": "+            elif self._creds[\"aes\"] != creds[\"aes\"]:"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1363,
                "PatchRowcode": "+                log.error(\"%s The master's aes key has changed.\", self)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1364,
                "PatchRowcode": "+                self._creds = creds"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1365,
                "PatchRowcode": "+                self._crypticle = Crypticle(self.opts, creds[\"aes\"])"
            },
            "40": {
                "beforePatchRowNumber": 1351,
                "afterPatchRowNumber": 1366,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": 1352,
                "afterPatchRowNumber": 1367,
                "PatchRowcode": "     def sign_in(self, timeout=60, safe=True, tries=1, channel=None):"
            },
            "42": {
                "beforePatchRowNumber": 1353,
                "afterPatchRowNumber": 1368,
                "PatchRowcode": "         \"\"\""
            },
            "43": {
                "beforePatchRowNumber": 1415,
                "afterPatchRowNumber": 1430,
                "PatchRowcode": "     AES_BLOCK_SIZE = 16"
            },
            "44": {
                "beforePatchRowNumber": 1416,
                "afterPatchRowNumber": 1431,
                "PatchRowcode": "     SIG_SIZE = hashlib.sha256().digest_size"
            },
            "45": {
                "beforePatchRowNumber": 1417,
                "afterPatchRowNumber": 1432,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 1418,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __init__(self, opts, key_string, key_size=192):"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1433,
                "PatchRowcode": "+    def __init__(self, opts, key_string, key_size=192, serial=0):"
            },
            "48": {
                "beforePatchRowNumber": 1419,
                "afterPatchRowNumber": 1434,
                "PatchRowcode": "         self.key_string = key_string"
            },
            "49": {
                "beforePatchRowNumber": 1420,
                "afterPatchRowNumber": 1435,
                "PatchRowcode": "         self.keys = self.extract_keys(self.key_string, key_size)"
            },
            "50": {
                "beforePatchRowNumber": 1421,
                "afterPatchRowNumber": 1436,
                "PatchRowcode": "         self.key_size = key_size"
            },
            "51": {
                "beforePatchRowNumber": 1422,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.serial = salt.payload.Serial(opts)"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1437,
                "PatchRowcode": "+        self.serial = serial"
            },
            "53": {
                "beforePatchRowNumber": 1423,
                "afterPatchRowNumber": 1438,
                "PatchRowcode": " "
            },
            "54": {
                "beforePatchRowNumber": 1424,
                "afterPatchRowNumber": 1439,
                "PatchRowcode": "     @classmethod"
            },
            "55": {
                "beforePatchRowNumber": 1425,
                "afterPatchRowNumber": 1440,
                "PatchRowcode": "     def generate_key_string(cls, key_size=192):"
            },
            "56": {
                "beforePatchRowNumber": 1489,
                "afterPatchRowNumber": 1504,
                "PatchRowcode": "             data = cypher.decrypt(data)"
            },
            "57": {
                "beforePatchRowNumber": 1490,
                "afterPatchRowNumber": 1505,
                "PatchRowcode": "         return data[: -data[-1]]"
            },
            "58": {
                "beforePatchRowNumber": 1491,
                "afterPatchRowNumber": 1506,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": 1492,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def dumps(self, obj):"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1507,
                "PatchRowcode": "+    def dumps(self, obj, nonce=None):"
            },
            "61": {
                "beforePatchRowNumber": 1493,
                "afterPatchRowNumber": 1508,
                "PatchRowcode": "         \"\"\""
            },
            "62": {
                "beforePatchRowNumber": 1494,
                "afterPatchRowNumber": 1509,
                "PatchRowcode": "         Serialize and encrypt a python object"
            },
            "63": {
                "beforePatchRowNumber": 1495,
                "afterPatchRowNumber": 1510,
                "PatchRowcode": "         \"\"\""
            },
            "64": {
                "beforePatchRowNumber": 1496,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return self.encrypt(self.PICKLE_PAD + self.serial.dumps(obj))"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1511,
                "PatchRowcode": "+        if nonce:"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1512,
                "PatchRowcode": "+            toencrypt = ("
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1513,
                "PatchRowcode": "+                self.PICKLE_PAD + nonce.encode() + salt.payload.Serial({}).dumps(obj)"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1514,
                "PatchRowcode": "+            )"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1515,
                "PatchRowcode": "+        else:"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1516,
                "PatchRowcode": "+            toencrypt = self.PICKLE_PAD + salt.payload.Serial({}).dumps(obj)"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1517,
                "PatchRowcode": "+        return self.encrypt(toencrypt)"
            },
            "72": {
                "beforePatchRowNumber": 1497,
                "afterPatchRowNumber": 1518,
                "PatchRowcode": " "
            },
            "73": {
                "beforePatchRowNumber": 1498,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def loads(self, data, raw=False):"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1519,
                "PatchRowcode": "+    def loads(self, data, raw=False, nonce=None):"
            },
            "75": {
                "beforePatchRowNumber": 1499,
                "afterPatchRowNumber": 1520,
                "PatchRowcode": "         \"\"\""
            },
            "76": {
                "beforePatchRowNumber": 1500,
                "afterPatchRowNumber": 1521,
                "PatchRowcode": "         Decrypt and un-serialize a python object"
            },
            "77": {
                "beforePatchRowNumber": 1501,
                "afterPatchRowNumber": 1522,
                "PatchRowcode": "         \"\"\""
            },
            "78": {
                "beforePatchRowNumber": 1502,
                "afterPatchRowNumber": 1523,
                "PatchRowcode": "         data = self.decrypt(data)"
            },
            "79": {
                "beforePatchRowNumber": 1503,
                "afterPatchRowNumber": 1524,
                "PatchRowcode": "         # simple integrity check to verify that we got meaningful data"
            },
            "80": {
                "beforePatchRowNumber": 1504,
                "afterPatchRowNumber": 1525,
                "PatchRowcode": "         if not data.startswith(self.PICKLE_PAD):"
            },
            "81": {
                "beforePatchRowNumber": 1505,
                "afterPatchRowNumber": 1526,
                "PatchRowcode": "             return {}"
            },
            "82": {
                "beforePatchRowNumber": 1506,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        load = self.serial.loads(data[len(self.PICKLE_PAD) :], raw=raw)"
            },
            "83": {
                "beforePatchRowNumber": 1507,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return load"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1527,
                "PatchRowcode": "+        data = data[len(self.PICKLE_PAD) :]"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1528,
                "PatchRowcode": "+        if nonce:"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1529,
                "PatchRowcode": "+            ret_nonce = data[:32].decode()"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1530,
                "PatchRowcode": "+            data = data[32:]"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1531,
                "PatchRowcode": "+            if ret_nonce != nonce:"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1532,
                "PatchRowcode": "+                raise SaltClientError(\"Nonce verification error\")"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1533,
                "PatchRowcode": "+        payload = salt.payload.Serial({}).loads(data, raw=raw)"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1534,
                "PatchRowcode": "+        if isinstance(payload, dict):"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1535,
                "PatchRowcode": "+            if \"serial\" in payload:"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1536,
                "PatchRowcode": "+                serial = payload.pop(\"serial\")"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1537,
                "PatchRowcode": "+                if serial <= self.serial:"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1538,
                "PatchRowcode": "+                    log.critical("
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1539,
                "PatchRowcode": "+                        \"A message with an invalid serial was received.\\n\""
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1540,
                "PatchRowcode": "+                        \"this serial: %d\\n\""
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1541,
                "PatchRowcode": "+                        \"last serial: %d\\n\""
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1542,
                "PatchRowcode": "+                        \"The minion will not honor this request.\","
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1543,
                "PatchRowcode": "+                        serial,"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1544,
                "PatchRowcode": "+                        self.serial,"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1545,
                "PatchRowcode": "+                    )"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1546,
                "PatchRowcode": "+                    return {}"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1547,
                "PatchRowcode": "+                self.serial = serial"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1548,
                "PatchRowcode": "+        return payload"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "The crypt module manages all of the cryptography functions for minions and",
            "masters, encrypting and decrypting payloads, preparing messages, and",
            "authenticating peers",
            "\"\"\"",
            "",
            "import base64",
            "import binascii",
            "import copy",
            "import getpass",
            "import hashlib",
            "import hmac",
            "import logging",
            "import os",
            "import random",
            "import stat",
            "import sys",
            "import time",
            "import traceback",
            "import uuid",
            "import weakref",
            "",
            "import salt.defaults.exitcodes",
            "import salt.ext.tornado.gen",
            "import salt.payload",
            "import salt.transport.client",
            "import salt.transport.frame",
            "import salt.utils.crypt",
            "import salt.utils.decorators",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.rsax931",
            "import salt.utils.sdb",
            "import salt.utils.stringutils",
            "import salt.utils.user",
            "import salt.utils.verify",
            "import salt.version",
            "from salt.exceptions import (",
            "    AuthenticationError,",
            "    MasterExit,",
            "    SaltClientError,",
            "    SaltReqTimeoutError,",
            ")",
            "",
            "try:",
            "    from M2Crypto import RSA, EVP, BIO",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "",
            "if not HAS_M2:",
            "    try:",
            "        from Cryptodome.Cipher import AES, PKCS1_OAEP, PKCS1_v1_5 as PKCS1_v1_5_CIPHER",
            "        from Cryptodome.Hash import SHA",
            "        from Cryptodome.PublicKey import RSA",
            "        from Cryptodome.Signature import PKCS1_v1_5",
            "        from Cryptodome import Random",
            "",
            "        HAS_CRYPTO = True",
            "    except ImportError:",
            "        HAS_CRYPTO = False",
            "",
            "if not HAS_M2 and not HAS_CRYPTO:",
            "    try:",
            "        from Crypto.Cipher import (  # nosec",
            "            AES,",
            "            PKCS1_OAEP,",
            "            PKCS1_v1_5 as PKCS1_v1_5_CIPHER,",
            "        )",
            "        from Crypto.Hash import SHA  # nosec",
            "        from Crypto.PublicKey import RSA  # nosec",
            "        from Crypto.Signature import PKCS1_v1_5  # nosec",
            "",
            "        # let this be imported, if possible",
            "        from Crypto import Random  # nosec pylint: disable=unused-import",
            "",
            "        HAS_CRYPTO = True",
            "    except ImportError:",
            "        HAS_CRYPTO = False",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def dropfile(cachedir, user=None):",
            "    \"\"\"",
            "    Set an AES dropfile to request the master update the publish session key",
            "    \"\"\"",
            "    dfn = os.path.join(cachedir, \".dfn\")",
            "    # set a mask (to avoid a race condition on file creation) and store original.",
            "    with salt.utils.files.set_umask(0o277):",
            "        log.info(\"Rotating AES key\")",
            "        if os.path.isfile(dfn):",
            "            log.info(\"AES key rotation already requested\")",
            "            return",
            "",
            "        if os.path.isfile(dfn) and not os.access(dfn, os.W_OK):",
            "            os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)",
            "        with salt.utils.files.fopen(dfn, \"wb+\") as fp_:",
            "            fp_.write(b\"\")",
            "        os.chmod(dfn, stat.S_IRUSR)",
            "        if user:",
            "            try:",
            "                import pwd",
            "",
            "                uid = pwd.getpwnam(user).pw_uid",
            "                os.chown(dfn, uid, -1)",
            "            except (KeyError, ImportError, OSError):",
            "                pass",
            "",
            "",
            "def gen_keys(keydir, keyname, keysize, user=None, passphrase=None):",
            "    \"\"\"",
            "    Generate a RSA public keypair for use with salt",
            "",
            "    :param str keydir: The directory to write the keypair to",
            "    :param str keyname: The type of salt server for whom this key should be written. (i.e. 'master' or 'minion')",
            "    :param int keysize: The number of bits in the key",
            "    :param str user: The user on the system who should own this keypair",
            "    :param str passphrase: The passphrase which should be used to encrypt the private key",
            "",
            "    :rtype: str",
            "    :return: Path on the filesystem to the RSA private key",
            "    \"\"\"",
            "    base = os.path.join(keydir, keyname)",
            "    priv = \"{}.pem\".format(base)",
            "    pub = \"{}.pub\".format(base)",
            "",
            "    if HAS_M2:",
            "        gen = RSA.gen_key(keysize, 65537, lambda: None)",
            "    else:",
            "        salt.utils.crypt.reinit_crypto()",
            "        gen = RSA.generate(bits=keysize, e=65537)",
            "    if os.path.isfile(priv):",
            "        # Between first checking and the generation another process has made",
            "        # a key! Use the winner's key",
            "        return priv",
            "",
            "    # Do not try writing anything, if directory has no permissions.",
            "    if not os.access(keydir, os.W_OK):",
            "        raise OSError(",
            "            'Write access denied to \"{}\" for user \"{}\".'.format(",
            "                os.path.abspath(keydir), getpass.getuser()",
            "            )",
            "        )",
            "",
            "    with salt.utils.files.set_umask(0o277):",
            "        if HAS_M2:",
            "            # if passphrase is empty or None use no cipher",
            "            if not passphrase:",
            "                gen.save_pem(priv, cipher=None)",
            "            else:",
            "                gen.save_pem(",
            "                    priv,",
            "                    cipher=\"des_ede3_cbc\",",
            "                    callback=lambda x: salt.utils.stringutils.to_bytes(passphrase),",
            "                )",
            "        else:",
            "            with salt.utils.files.fopen(priv, \"wb+\") as f:",
            "                f.write(gen.exportKey(\"PEM\", passphrase))",
            "    if HAS_M2:",
            "        gen.save_pub_key(pub)",
            "    else:",
            "        with salt.utils.files.fopen(pub, \"wb+\") as f:",
            "            f.write(gen.publickey().exportKey(\"PEM\"))",
            "    os.chmod(priv, 0o400)",
            "    if user:",
            "        try:",
            "            import pwd",
            "",
            "            uid = pwd.getpwnam(user).pw_uid",
            "            os.chown(priv, uid, -1)",
            "            os.chown(pub, uid, -1)",
            "        except (KeyError, ImportError, OSError):",
            "            # The specified user was not found, allow the backup systems to",
            "            # report the error",
            "            pass",
            "    return priv",
            "",
            "",
            "@salt.utils.decorators.memoize",
            "def _get_key_with_evict(path, timestamp, passphrase):",
            "    \"\"\"",
            "    Load a private key from disk.  `timestamp` above is intended to be the",
            "    timestamp of the file's last modification. This fn is memoized so if it is",
            "    called with the same path and timestamp (the file's last modified time) the",
            "    second time the result is returned from the memoiziation.  If the file gets",
            "    modified then the params are different and the key is loaded from disk.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt._get_key_with_evict: Loading private key\")",
            "    if HAS_M2:",
            "        key = RSA.load_key(path, lambda x: bytes(passphrase))",
            "    else:",
            "        with salt.utils.files.fopen(path) as f:",
            "            key = RSA.importKey(f.read(), passphrase)",
            "    return key",
            "",
            "",
            "def get_rsa_key(path, passphrase):",
            "    \"\"\"",
            "    Read a private key off the disk.  Poor man's simple cache in effect here,",
            "    we memoize the result of calling _get_rsa_with_evict.  This means the first",
            "    time _get_key_with_evict is called with a path and a timestamp the result",
            "    is cached.  If the file (the private key) does not change then its",
            "    timestamp will not change and the next time the result is returned from the",
            "    cache.  If the key DOES change the next time _get_rsa_with_evict is called",
            "    it is called with different parameters and the fn is run fully to retrieve",
            "    the key from disk.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt.get_rsa_key: Loading private key\")",
            "    return _get_key_with_evict(path, str(os.path.getmtime(path)), passphrase)",
            "",
            "",
            "def get_rsa_pub_key(path):",
            "    \"\"\"",
            "    Read a public key off the disk.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt.get_rsa_pub_key: Loading public key\")",
            "    if HAS_M2:",
            "        with salt.utils.files.fopen(path, \"rb\") as f:",
            "            data = f.read().replace(b\"RSA \", b\"\")",
            "        bio = BIO.MemoryBuffer(data)",
            "        key = RSA.load_pub_key_bio(bio)",
            "    else:",
            "        with salt.utils.files.fopen(path) as f:",
            "            key = RSA.importKey(f.read())",
            "    return key",
            "",
            "",
            "def sign_message(privkey_path, message, passphrase=None):",
            "    \"\"\"",
            "    Use Crypto.Signature.PKCS1_v1_5 to sign a message. Returns the signature.",
            "    \"\"\"",
            "    key = get_rsa_key(privkey_path, passphrase)",
            "    log.debug(\"salt.crypt.sign_message: Signing message.\")",
            "    if HAS_M2:",
            "        md = EVP.MessageDigest(\"sha1\")",
            "        md.update(salt.utils.stringutils.to_bytes(message))",
            "        digest = md.final()",
            "        return key.sign(digest)",
            "    else:",
            "        signer = PKCS1_v1_5.new(key)",
            "        return signer.sign(SHA.new(salt.utils.stringutils.to_bytes(message)))",
            "",
            "",
            "def verify_signature(pubkey_path, message, signature):",
            "    \"\"\"",
            "    Use Crypto.Signature.PKCS1_v1_5 to verify the signature on a message.",
            "    Returns True for valid signature.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt.verify_signature: Loading public key\")",
            "    pubkey = get_rsa_pub_key(pubkey_path)",
            "    log.debug(\"salt.crypt.verify_signature: Verifying signature\")",
            "    if HAS_M2:",
            "        md = EVP.MessageDigest(\"sha1\")",
            "        md.update(salt.utils.stringutils.to_bytes(message))",
            "        digest = md.final()",
            "        try:",
            "            return pubkey.verify(digest, signature)",
            "        except RSA.RSAError as exc:",
            "            if exc.args[0] == \"bad signature\":",
            "                return False",
            "            raise",
            "    else:",
            "        verifier = PKCS1_v1_5.new(pubkey)",
            "        return verifier.verify(",
            "            SHA.new(salt.utils.stringutils.to_bytes(message)), signature",
            "        )",
            "",
            "",
            "def gen_signature(priv_path, pub_path, sign_path, passphrase=None):",
            "    \"\"\"",
            "    creates a signature for the given public-key with",
            "    the given private key and writes it to sign_path",
            "    \"\"\"",
            "",
            "    with salt.utils.files.fopen(pub_path) as fp_:",
            "        mpub_64 = fp_.read()",
            "",
            "    mpub_sig = sign_message(priv_path, mpub_64, passphrase)",
            "    mpub_sig_64 = binascii.b2a_base64(mpub_sig)",
            "    if os.path.isfile(sign_path):",
            "        return False",
            "    log.trace(",
            "        \"Calculating signature for %s with %s\",",
            "        os.path.basename(pub_path),",
            "        os.path.basename(priv_path),",
            "    )",
            "",
            "    if os.path.isfile(sign_path):",
            "        log.trace(",
            "            \"Signature file %s already exists, please remove it first and \" \"try again\",",
            "            sign_path,",
            "        )",
            "    else:",
            "        with salt.utils.files.fopen(sign_path, \"wb+\") as sig_f:",
            "            sig_f.write(salt.utils.stringutils.to_bytes(mpub_sig_64))",
            "        log.trace(\"Wrote signature to %s\", sign_path)",
            "    return True",
            "",
            "",
            "def private_encrypt(key, message):",
            "    \"\"\"",
            "    Generate an M2Crypto-compatible signature",
            "",
            "    :param Crypto.PublicKey.RSA._RSAobj key: The RSA key object",
            "    :param str message: The message to sign",
            "    :rtype: str",
            "    :return: The signature, or an empty string if the signature operation failed",
            "    \"\"\"",
            "    if HAS_M2:",
            "        return key.private_encrypt(message, salt.utils.rsax931.RSA_X931_PADDING)",
            "    else:",
            "        signer = salt.utils.rsax931.RSAX931Signer(key.exportKey(\"PEM\"))",
            "        return signer.sign(message)",
            "",
            "",
            "def public_decrypt(pub, message):",
            "    \"\"\"",
            "    Verify an M2Crypto-compatible signature",
            "",
            "    :param Crypto.PublicKey.RSA._RSAobj key: The RSA public key object",
            "    :param str message: The signed message to verify",
            "    :rtype: str",
            "    :return: The message (or digest) recovered from the signature, or an",
            "        empty string if the verification failed",
            "    \"\"\"",
            "    if HAS_M2:",
            "        return pub.public_decrypt(message, salt.utils.rsax931.RSA_X931_PADDING)",
            "    else:",
            "        verifier = salt.utils.rsax931.RSAX931Verifier(pub.exportKey(\"PEM\"))",
            "        return verifier.verify(message)",
            "",
            "",
            "def pwdata_decrypt(rsa_key, pwdata):",
            "    if HAS_M2:",
            "        key = RSA.load_key_string(salt.utils.stringutils.to_bytes(rsa_key, \"ascii\"))",
            "        password = key.private_decrypt(pwdata, RSA.pkcs1_padding)",
            "    else:",
            "        dsize = SHA.digest_size",
            "        sentinel = Random.new().read(15 + dsize)",
            "        key_obj = RSA.importKey(rsa_key)",
            "        key_obj = PKCS1_v1_5_CIPHER.new(key_obj)",
            "        password = key_obj.decrypt(pwdata, sentinel)",
            "    return salt.utils.stringutils.to_unicode(password)",
            "",
            "",
            "class MasterKeys(dict):",
            "    \"\"\"",
            "    The Master Keys class is used to manage the RSA public key pair used for",
            "    authentication by the master.",
            "",
            "    It also generates a signing key-pair if enabled with master_sign_key_name.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts):",
            "        super().__init__()",
            "        self.opts = opts",
            "        self.pub_path = os.path.join(self.opts[\"pki_dir\"], \"master.pub\")",
            "        self.rsa_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "",
            "        key_pass = salt.utils.sdb.sdb_get(self.opts[\"key_pass\"], self.opts)",
            "        self.key = self.__get_keys(passphrase=key_pass)",
            "",
            "        self.pub_signature = None",
            "",
            "        # set names for the signing key-pairs",
            "        if opts[\"master_sign_pubkey\"]:",
            "",
            "            # if only the signature is available, use that",
            "            if opts[\"master_use_pubkey_signature\"]:",
            "                self.sig_path = os.path.join(",
            "                    self.opts[\"pki_dir\"], opts[\"master_pubkey_signature\"]",
            "                )",
            "                if os.path.isfile(self.sig_path):",
            "                    with salt.utils.files.fopen(self.sig_path) as fp_:",
            "                        self.pub_signature = fp_.read()",
            "                    log.info(",
            "                        \"Read %s's signature from %s\",",
            "                        os.path.basename(self.pub_path),",
            "                        self.opts[\"master_pubkey_signature\"],",
            "                    )",
            "                else:",
            "                    log.error(",
            "                        \"Signing the master.pub key with a signature is \"",
            "                        \"enabled but no signature file found at the defined \"",
            "                        \"location %s\",",
            "                        self.sig_path,",
            "                    )",
            "                    log.error(",
            "                        \"The signature-file may be either named differently \"",
            "                        \"or has to be created with 'salt-key --gen-signature'\"",
            "                    )",
            "                    sys.exit(1)",
            "",
            "            # create a new signing key-pair to sign the masters",
            "            # auth-replies when a minion tries to connect",
            "            else:",
            "                key_pass = salt.utils.sdb.sdb_get(",
            "                    self.opts[\"signing_key_pass\"], self.opts",
            "                )",
            "                self.pub_sign_path = os.path.join(",
            "                    self.opts[\"pki_dir\"], opts[\"master_sign_key_name\"] + \".pub\"",
            "                )",
            "                self.rsa_sign_path = os.path.join(",
            "                    self.opts[\"pki_dir\"], opts[\"master_sign_key_name\"] + \".pem\"",
            "                )",
            "                self.sign_key = self.__get_keys(name=opts[\"master_sign_key_name\"])",
            "",
            "    # We need __setstate__ and __getstate__ to avoid pickling errors since",
            "    # some of the member variables correspond to Cython objects which are",
            "    # not picklable.",
            "    # These methods are only used when pickling so will not be used on",
            "    # non-Windows platforms.",
            "    def __setstate__(self, state):",
            "        self.__init__(state[\"opts\"])",
            "",
            "    def __getstate__(self):",
            "        return {\"opts\": self.opts}",
            "",
            "    def __get_keys(self, name=\"master\", passphrase=None):",
            "        \"\"\"",
            "        Returns a key object for a key in the pki-dir",
            "        \"\"\"",
            "        path = os.path.join(self.opts[\"pki_dir\"], name + \".pem\")",
            "        if not os.path.exists(path):",
            "            log.info(\"Generating %s keys: %s\", name, self.opts[\"pki_dir\"])",
            "            gen_keys(",
            "                self.opts[\"pki_dir\"],",
            "                name,",
            "                self.opts[\"keysize\"],",
            "                self.opts.get(\"user\"),",
            "                passphrase,",
            "            )",
            "        if HAS_M2:",
            "            key_error = RSA.RSAError",
            "        else:",
            "            key_error = ValueError",
            "        try:",
            "            key = get_rsa_key(path, passphrase)",
            "        except key_error as e:",
            "            message = \"Unable to read key: {}; passphrase may be incorrect\".format(path)",
            "            log.error(message)",
            "            raise MasterExit(message)",
            "        log.debug(\"Loaded %s key: %s\", name, path)",
            "        return key",
            "",
            "    def get_pub_str(self, name=\"master\"):",
            "        \"\"\"",
            "        Return the string representation of a public key",
            "        in the pki-directory",
            "        \"\"\"",
            "        path = os.path.join(self.opts[\"pki_dir\"], name + \".pub\")",
            "        if not os.path.isfile(path):",
            "            key = self.__get_keys()",
            "            if HAS_M2:",
            "                key.save_pub_key(path)",
            "            else:",
            "                with salt.utils.files.fopen(path, \"wb+\") as wfh:",
            "                    wfh.write(key.publickey().exportKey(\"PEM\"))",
            "        with salt.utils.files.fopen(path) as rfh:",
            "            return rfh.read()",
            "",
            "    def get_mkey_paths(self):",
            "        return self.pub_path, self.rsa_path",
            "",
            "    def get_sign_paths(self):",
            "        return self.pub_sign_path, self.rsa_sign_path",
            "",
            "    def pubkey_signature(self):",
            "        \"\"\"",
            "        returns the base64 encoded signature from the signature file",
            "        or None if the master has its own signing keys",
            "        \"\"\"",
            "        return self.pub_signature",
            "",
            "",
            "class AsyncAuth:",
            "    \"\"\"",
            "    Set up an Async object to maintain authentication with the salt master",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    # mapping of io_loop -> {key -> auth}",
            "    instance_map = weakref.WeakKeyDictionary()",
            "",
            "    # mapping of key -> creds",
            "    creds_map = {}",
            "",
            "    def __new__(cls, opts, io_loop=None):",
            "        \"\"\"",
            "        Only create one instance of AsyncAuth per __key()",
            "        \"\"\"",
            "        # do we have any mapping for this io_loop",
            "        io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()",
            "        if io_loop not in AsyncAuth.instance_map:",
            "            AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary()",
            "        loop_instance_map = AsyncAuth.instance_map[io_loop]",
            "",
            "        key = cls.__key(opts)",
            "        auth = loop_instance_map.get(key)",
            "        if auth is None:",
            "            log.debug(\"Initializing new AsyncAuth for %s\", key)",
            "            # we need to make a local variable for this, as we are going to store",
            "            # it in a WeakValueDictionary-- which will remove the item if no one",
            "            # references it-- this forces a reference while we return to the caller",
            "            auth = object.__new__(cls)",
            "            auth.__singleton_init__(opts, io_loop=io_loop)",
            "            loop_instance_map[key] = auth",
            "        else:",
            "            log.debug(\"Re-using AsyncAuth for %s\", key)",
            "        return auth",
            "",
            "    @classmethod",
            "    def __key(cls, opts, io_loop=None):",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            opts[\"master_uri\"],  # master ID",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, io_loop=None):",
            "        pass",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, io_loop=None):",
            "        \"\"\"",
            "        Init an Auth instance",
            "",
            "        :param dict opts: Options for this server",
            "        :return: Auth instance",
            "        :rtype: Auth",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.pub_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pub\")",
            "        self.rsa_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "        if self.opts[\"__role\"] == \"syndic\":",
            "            self.mpub = \"syndic_master.pub\"",
            "        else:",
            "            self.mpub = \"minion_master.pub\"",
            "        if not os.path.isfile(self.pub_path):",
            "            self.get_keys()",
            "",
            "        self.io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        salt.utils.crypt.reinit_crypto()",
            "        key = self.__key(self.opts)",
            "        # TODO: if we already have creds for this key, lets just re-use",
            "        if key in AsyncAuth.creds_map:",
            "            creds = AsyncAuth.creds_map[key]",
            "            self._creds = creds",
            "            self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "            self._authenticate_future = salt.ext.tornado.concurrent.Future()",
            "            self._authenticate_future.set_result(True)",
            "        else:",
            "            self.authenticate()",
            "",
            "    def __deepcopy__(self, memo):",
            "        cls = self.__class__",
            "        result = cls.__new__(cls, copy.deepcopy(self.opts, memo))",
            "        memo[id(self)] = result",
            "        for key in self.__dict__:",
            "            if key in (\"io_loop\",):",
            "                # The io_loop has a thread Lock which will fail to be deep",
            "                # copied. Skip it because it will just be recreated on the",
            "                # new copy.",
            "                continue",
            "            setattr(result, key, copy.deepcopy(self.__dict__[key], memo))",
            "        return result",
            "",
            "    @property",
            "    def creds(self):",
            "        return self._creds",
            "",
            "    @property",
            "    def crypticle(self):",
            "        return self._crypticle",
            "",
            "    @property",
            "    def authenticated(self):",
            "        return (",
            "            hasattr(self, \"_authenticate_future\")",
            "            and self._authenticate_future.done()",
            "            and self._authenticate_future.exception() is None",
            "        )",
            "",
            "    def invalidate(self):",
            "        if self.authenticated:",
            "            del self._authenticate_future",
            "            key = self.__key(self.opts)",
            "            if key in AsyncAuth.creds_map:",
            "                del AsyncAuth.creds_map[key]",
            "",
            "    def authenticate(self, callback=None):",
            "        \"\"\"",
            "        Ask for this client to reconnect to the origin",
            "",
            "        This function will de-dupe all calls here and return a *single* future",
            "        for the sign-in-- whis way callers can all assume there aren't others",
            "        \"\"\"",
            "        # if an auth is in flight-- and not done-- just pass that back as the future to wait on",
            "        if (",
            "            hasattr(self, \"_authenticate_future\")",
            "            and not self._authenticate_future.done()",
            "        ):",
            "            future = self._authenticate_future",
            "        else:",
            "            future = salt.ext.tornado.concurrent.Future()",
            "            self._authenticate_future = future",
            "            self.io_loop.add_callback(self._authenticate)",
            "",
            "        if callback is not None:",
            "",
            "            def handle_future(future):",
            "                response = future.result()",
            "                self.io_loop.add_callback(callback, response)",
            "",
            "            future.add_done_callback(handle_future)",
            "",
            "        return future",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _authenticate(self):",
            "        \"\"\"",
            "        Authenticate with the master, this method breaks the functional",
            "        paradigm, it will update the master information from a fresh sign",
            "        in, signing in can occur as often as needed to keep up with the",
            "        revolving master AES key.",
            "",
            "        :rtype: Crypticle",
            "        :returns: A crypticle used for encryption operations",
            "        \"\"\"",
            "        acceptance_wait_time = self.opts[\"acceptance_wait_time\"]",
            "        acceptance_wait_time_max = self.opts[\"acceptance_wait_time_max\"]",
            "        if not acceptance_wait_time_max:",
            "            acceptance_wait_time_max = acceptance_wait_time",
            "        creds = None",
            "",
            "        with salt.transport.client.AsyncReqChannel.factory(",
            "            self.opts, crypt=\"clear\", io_loop=self.io_loop",
            "        ) as channel:",
            "            error = None",
            "            while True:",
            "                try:",
            "                    creds = yield self.sign_in(channel=channel)",
            "                except SaltClientError as exc:",
            "                    error = exc",
            "                    break",
            "                if creds == \"retry\":",
            "                    if self.opts.get(\"detect_mode\") is True:",
            "                        error = SaltClientError(\"Detect mode is on\")",
            "                        break",
            "                    if self.opts.get(\"caller\"):",
            "                        # We have a list of masters, so we should break",
            "                        # and try the next one in the list.",
            "                        if self.opts.get(\"local_masters\", None):",
            "                            error = SaltClientError(",
            "                                \"Minion failed to authenticate\"",
            "                                \" with the master, has the \"",
            "                                \"minion key been accepted?\"",
            "                            )",
            "                            break",
            "                        else:",
            "                            print(",
            "                                \"Minion failed to authenticate with the master, \"",
            "                                \"has the minion key been accepted?\"",
            "                            )",
            "                            sys.exit(2)",
            "                    if acceptance_wait_time:",
            "                        log.info(",
            "                            \"Waiting %s seconds before retry.\", acceptance_wait_time",
            "                        )",
            "                        yield salt.ext.tornado.gen.sleep(acceptance_wait_time)",
            "                    if acceptance_wait_time < acceptance_wait_time_max:",
            "                        acceptance_wait_time += acceptance_wait_time",
            "                        log.debug(",
            "                            \"Authentication wait time is %s\", acceptance_wait_time",
            "                        )",
            "                    continue",
            "                break",
            "            if not isinstance(creds, dict) or \"aes\" not in creds:",
            "                if self.opts.get(\"detect_mode\") is True:",
            "                    error = SaltClientError(\"-|RETRY|-\")",
            "                try:",
            "                    del AsyncAuth.creds_map[self.__key(self.opts)]",
            "                except KeyError:",
            "                    pass",
            "                if not error:",
            "                    error = SaltClientError(",
            "                        \"Attempt to authenticate with the salt master failed\"",
            "                    )",
            "                self._authenticate_future.set_exception(error)",
            "            else:",
            "                key = self.__key(self.opts)",
            "                AsyncAuth.creds_map[key] = creds",
            "                self._creds = creds",
            "                self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "                self._authenticate_future.set_result(",
            "                    True",
            "                )  # mark the sign-in as complete",
            "                # Notify the bus about creds change",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    with salt.utils.event.get_event(",
            "                        self.opts.get(\"__role\"), opts=self.opts, listen=False",
            "                    ) as event:",
            "                        event.fire_event(",
            "                            {\"key\": key, \"creds\": creds},",
            "                            salt.utils.event.tagify(prefix=\"auth\", suffix=\"creds\"),",
            "                        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):",
            "        \"\"\"",
            "        Send a sign in request to the master, sets the key information and",
            "        returns a dict containing the master publish interface to bind to",
            "        and the decrypted aes key for transport decryption.",
            "",
            "        :param int timeout: Number of seconds to wait before timing out the sign-in request",
            "        :param bool safe: If True, do not raise an exception on timeout. Retry instead.",
            "        :param int tries: The number of times to try to authenticate before giving up.",
            "",
            "        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set",
            "",
            "        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary",
            "        with the publication port and the shared AES key.",
            "",
            "        \"\"\"",
            "",
            "        auth_timeout = self.opts.get(\"auth_timeout\", None)",
            "        if auth_timeout is not None:",
            "            timeout = auth_timeout",
            "        auth_safemode = self.opts.get(\"auth_safemode\", None)",
            "        if auth_safemode is not None:",
            "            safe = auth_safemode",
            "        auth_tries = self.opts.get(\"auth_tries\", None)",
            "        if auth_tries is not None:",
            "            tries = auth_tries",
            "",
            "        close_channel = False",
            "        if not channel:",
            "            close_channel = True",
            "            channel = salt.transport.client.AsyncReqChannel.factory(",
            "                self.opts, crypt=\"clear\", io_loop=self.io_loop",
            "            )",
            "",
            "        sign_in_payload = self.minion_sign_in_payload()",
            "        try:",
            "            payload = yield channel.send(sign_in_payload, tries=tries, timeout=timeout)",
            "        except SaltReqTimeoutError as e:",
            "            if safe:",
            "                log.warning(\"SaltReqTimeoutError: %s\", e)",
            "                raise salt.ext.tornado.gen.Return(\"retry\")",
            "            if self.opts.get(\"detect_mode\") is True:",
            "                raise salt.ext.tornado.gen.Return(\"retry\")",
            "            else:",
            "                raise SaltClientError(",
            "                    \"Attempt to authenticate with the salt master failed with timeout error\"",
            "                )",
            "        finally:",
            "            if close_channel:",
            "                channel.close()",
            "        ret = self.handle_signin_response(sign_in_payload, payload)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    def handle_signin_response(self, sign_in_payload, payload):",
            "        auth = {}",
            "        m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "        auth[\"master_uri\"] = self.opts[\"master_uri\"]",
            "        if not isinstance(payload, dict) or \"load\" not in payload:",
            "            log.error(\"Sign-in attempt failed: %s\", payload)",
            "            return False",
            "",
            "        clear_signed_data = payload[\"load\"]",
            "        clear_signature = payload[\"sig\"]",
            "        payload = self.serial.loads(clear_signed_data)",
            "",
            "        if \"pub_key\" in payload:",
            "            auth[\"aes\"] = self.verify_master(",
            "                payload, master_pub=\"token\" in sign_in_payload",
            "            )",
            "            if not auth[\"aes\"]:",
            "                log.critical(",
            "                    \"The Salt Master server's public key did not authenticate!\\n\"",
            "                    \"The master may need to be updated if it is a version of Salt \"",
            "                    \"lower than %s, or\\n\"",
            "                    \"If you are confident that you are connecting to a valid Salt \"",
            "                    \"Master, then remove the master public key and restart the \"",
            "                    \"Salt Minion.\\nThe master public key can be found \"",
            "                    \"at:\\n%s\",",
            "                    salt.version.__version__,",
            "                    m_pub_fn,",
            "                )",
            "                raise SaltClientError(\"Invalid master key\")",
            "",
            "        master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "        if os.path.exists(master_pubkey_path) and not verify_signature(",
            "            master_pubkey_path, clear_signed_data, clear_signature",
            "        ):",
            "            log.critical(\"The payload signature did not validate.\")",
            "            raise SaltClientError(\"Invalid signature\")",
            "",
            "        if payload[\"nonce\"] != sign_in_payload[\"nonce\"]:",
            "            log.critical(\"The payload nonce did not validate.\")",
            "            raise SaltClientError(\"Invalid nonce\")",
            "",
            "        if \"ret\" in payload:",
            "            if not payload[\"ret\"]:",
            "                if self.opts[\"rejected_retry\"]:",
            "                    log.error(",
            "                        \"The Salt Master has rejected this minion's public \"",
            "                        \"key.\\nTo repair this issue, delete the public key \"",
            "                        \"for this minion on the Salt Master.\\nThe Salt \"",
            "                        \"Minion will attempt to re-authenicate.\"",
            "                    )",
            "                    return \"retry\"",
            "                else:",
            "                    log.critical(",
            "                        \"The Salt Master has rejected this minion's public \"",
            "                        \"key!\\nTo repair this issue, delete the public key \"",
            "                        \"for this minion on the Salt Master and restart this \"",
            "                        \"minion.\\nOr restart the Salt Master in open mode to \"",
            "                        \"clean out the keys. The Salt Minion will now exit.\"",
            "                    )",
            "                    # Add a random sleep here for systems that are using a",
            "                    # a service manager to immediately restart the service",
            "                    # to avoid overloading the system",
            "                    time.sleep(random.randint(10, 20))",
            "                    sys.exit(salt.defaults.exitcodes.EX_NOPERM)",
            "            # has the master returned that its maxed out with minions?",
            "            elif payload[\"ret\"] == \"full\":",
            "                return \"full\"",
            "            else:",
            "                log.error(",
            "                    \"The Salt Master has cached the public key for this \"",
            "                    \"node, this salt minion will wait for %s seconds \"",
            "                    \"before attempting to re-authenticate\",",
            "                    self.opts[\"acceptance_wait_time\"],",
            "                )",
            "                return \"retry\"",
            "",
            "        if self.opts.get(\"syndic_master\", False):  # Is syndic",
            "            syndic_finger = self.opts.get(",
            "                \"syndic_finger\", self.opts.get(\"master_finger\", False)",
            "            )",
            "            if syndic_finger:",
            "                if (",
            "                    salt.utils.crypt.pem_finger(",
            "                        m_pub_fn, sum_type=self.opts[\"hash_type\"]",
            "                    )",
            "                    != syndic_finger",
            "                ):",
            "                    self._finger_fail(syndic_finger, m_pub_fn)",
            "        else:",
            "            if self.opts.get(\"master_finger\", False):",
            "                if (",
            "                    salt.utils.crypt.pem_finger(",
            "                        m_pub_fn, sum_type=self.opts[\"hash_type\"]",
            "                    )",
            "                    != self.opts[\"master_finger\"]",
            "                ):",
            "                    self._finger_fail(self.opts[\"master_finger\"], m_pub_fn)",
            "",
            "        auth[\"publish_port\"] = payload[\"publish_port\"]",
            "        return auth",
            "",
            "    def get_keys(self):",
            "        \"\"\"",
            "        Return keypair object for the minion.",
            "",
            "        :rtype: Crypto.PublicKey.RSA._RSAobj",
            "        :return: The RSA keypair",
            "        \"\"\"",
            "        # Make sure all key parent directories are accessible",
            "        user = self.opts.get(\"user\", \"root\")",
            "        salt.utils.verify.check_path_traversal(self.opts[\"pki_dir\"], user)",
            "",
            "        if not os.path.exists(self.rsa_path):",
            "            log.info(\"Generating keys: %s\", self.opts[\"pki_dir\"])",
            "            gen_keys(",
            "                self.opts[\"pki_dir\"],",
            "                \"minion\",",
            "                self.opts[\"keysize\"],",
            "                self.opts.get(\"user\"),",
            "            )",
            "        key = get_rsa_key(self.rsa_path, None)",
            "        log.debug(\"Loaded minion key: %s\", self.rsa_path)",
            "        return key",
            "",
            "    def gen_token(self, clear_tok):",
            "        \"\"\"",
            "        Encrypt a string with the minion private key to verify identity",
            "        with the master.",
            "",
            "        :param str clear_tok: A plaintext token to encrypt",
            "        :return: Encrypted token",
            "        :rtype: str",
            "        \"\"\"",
            "        return private_encrypt(self.get_keys(), clear_tok)",
            "",
            "    def minion_sign_in_payload(self):",
            "        \"\"\"",
            "        Generates the payload used to authenticate with the master",
            "        server. This payload consists of the passed in id_ and the ssh",
            "        public key to encrypt the AES key sent back from the master.",
            "",
            "        :return: Payload dictionary",
            "        :rtype: dict",
            "        \"\"\"",
            "        payload = {}",
            "        payload[\"cmd\"] = \"_auth\"",
            "        payload[\"id\"] = self.opts[\"id\"]",
            "        payload[\"nonce\"] = uuid.uuid4().hex",
            "        if \"autosign_grains\" in self.opts:",
            "            autosign_grains = {}",
            "            for grain in self.opts[\"autosign_grains\"]:",
            "                autosign_grains[grain] = self.opts[\"grains\"].get(grain, None)",
            "            payload[\"autosign_grains\"] = autosign_grains",
            "        try:",
            "            pubkey_path = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "            pub = get_rsa_pub_key(pubkey_path)",
            "            if HAS_M2:",
            "                payload[\"token\"] = pub.public_encrypt(",
            "                    self.token, RSA.pkcs1_oaep_padding",
            "                )",
            "            else:",
            "                cipher = PKCS1_OAEP.new(pub)",
            "                payload[\"token\"] = cipher.encrypt(self.token)",
            "        except Exception:  # pylint: disable=broad-except",
            "            pass",
            "        with salt.utils.files.fopen(self.pub_path) as f:",
            "            payload[\"pub\"] = f.read()",
            "        return payload",
            "",
            "    def decrypt_aes(self, payload, master_pub=True):",
            "        \"\"\"",
            "        This function is used to decrypt the AES seed phrase returned from",
            "        the master server. The seed phrase is decrypted with the SSH RSA",
            "        host key.",
            "",
            "        Pass in the encrypted AES key.",
            "        Returns the decrypted AES seed key, a string",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', etc)",
            "            'sig': The message signature",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The public key of the sender.",
            "",
            "        :rtype: str",
            "        :return: The decrypted token that was provided, with padding.",
            "",
            "        :rtype: str",
            "        :return: The decrypted AES seed key",
            "        \"\"\"",
            "        if self.opts.get(\"auth_trb\", False):",
            "            log.warning(\"Auth Called: %s\", \"\".join(traceback.format_stack()))",
            "        else:",
            "            log.debug(\"Decrypting the current master AES key\")",
            "        key = self.get_keys()",
            "        if HAS_M2:",
            "            key_str = key.private_decrypt(payload[\"aes\"], RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(key)",
            "            key_str = cipher.decrypt(payload[\"aes\"])",
            "        if \"sig\" in payload:",
            "            m_path = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "            if os.path.exists(m_path):",
            "                try:",
            "                    mkey = get_rsa_pub_key(m_path)",
            "                except Exception:  # pylint: disable=broad-except",
            "                    return \"\", \"\"",
            "                digest = hashlib.sha256(key_str).hexdigest()",
            "                digest = salt.utils.stringutils.to_bytes(digest)",
            "                if HAS_M2:",
            "                    m_digest = public_decrypt(mkey, payload[\"sig\"])",
            "                else:",
            "                    m_digest = public_decrypt(mkey.publickey(), payload[\"sig\"])",
            "                if m_digest != digest:",
            "                    return \"\", \"\"",
            "        else:",
            "            return \"\", \"\"",
            "",
            "        key_str = salt.utils.stringutils.to_str(key_str)",
            "",
            "        if \"_|-\" in key_str:",
            "            return key_str.split(\"_|-\")",
            "        else:",
            "            if \"token\" in payload:",
            "                if HAS_M2:",
            "                    token = key.private_decrypt(",
            "                        payload[\"token\"], RSA.pkcs1_oaep_padding",
            "                    )",
            "                else:",
            "                    token = cipher.decrypt(payload[\"token\"])",
            "                return key_str, token",
            "            elif not master_pub:",
            "                return key_str, \"\"",
            "        return \"\", \"\"",
            "",
            "    def verify_pubkey_sig(self, message, sig):",
            "        \"\"\"",
            "        Wraps the verify_signature method so we have",
            "        additional checks.",
            "",
            "        :rtype: bool",
            "        :return: Success or failure of public key verification",
            "        \"\"\"",
            "        if self.opts[\"master_sign_key_name\"]:",
            "            path = os.path.join(",
            "                self.opts[\"pki_dir\"], self.opts[\"master_sign_key_name\"] + \".pub\"",
            "            )",
            "",
            "            if os.path.isfile(path):",
            "                res = verify_signature(path, message, binascii.a2b_base64(sig))",
            "            else:",
            "                log.error(",
            "                    \"Verification public key %s does not exist. You need to \"",
            "                    \"copy it from the master to the minions pki directory\",",
            "                    os.path.basename(path),",
            "                )",
            "                return False",
            "            if res:",
            "                log.debug(",
            "                    \"Successfully verified signature of master public key \"",
            "                    \"with verification public key %s\",",
            "                    self.opts[\"master_sign_key_name\"] + \".pub\",",
            "                )",
            "                return True",
            "            else:",
            "                log.debug(\"Failed to verify signature of public key\")",
            "                return False",
            "        else:",
            "            log.error(",
            "                \"Failed to verify the signature of the message because the \"",
            "                \"verification key-pairs name is not defined. Please make \"",
            "                \"sure that master_sign_key_name is defined.\"",
            "            )",
            "            return False",
            "",
            "    def verify_signing_master(self, payload):",
            "        try:",
            "            if self.verify_pubkey_sig(payload[\"pub_key\"], payload[\"pub_sig\"]):",
            "                log.info(",
            "                    \"Received signed and verified master pubkey from master %s\",",
            "                    self.opts[\"master\"],",
            "                )",
            "                m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "                uid = salt.utils.user.get_uid(self.opts.get(\"user\", None))",
            "                with salt.utils.files.fpopen(m_pub_fn, \"wb+\", uid=uid) as wfh:",
            "                    wfh.write(salt.utils.stringutils.to_bytes(payload[\"pub_key\"]))",
            "                return True",
            "            else:",
            "                log.error(",
            "                    \"Received signed public-key from master %s but signature \"",
            "                    \"verification failed!\",",
            "                    self.opts[\"master\"],",
            "                )",
            "                return False",
            "        except Exception as sign_exc:  # pylint: disable=broad-except",
            "            log.error(",
            "                \"There was an error while verifying the masters public-key \" \"signature\"",
            "            )",
            "            raise Exception(sign_exc)",
            "",
            "    def check_auth_deps(self, payload):",
            "        \"\"\"",
            "        Checks if both master and minion either sign (master) and",
            "        verify (minion). If one side does not, it should fail.",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', 'aes')",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The RSA public key of the sender.",
            "        \"\"\"",
            "        # master and minion sign and verify",
            "        if \"pub_sig\" in payload and self.opts[\"verify_master_pubkey_sign\"]:",
            "            return True",
            "        # master and minion do NOT sign and do NOT verify",
            "        elif \"pub_sig\" not in payload and not self.opts[\"verify_master_pubkey_sign\"]:",
            "            return True",
            "",
            "        # master signs, but minion does NOT verify",
            "        elif \"pub_sig\" in payload and not self.opts[\"verify_master_pubkey_sign\"]:",
            "            log.error(",
            "                \"The masters sent its public-key signature, but signature \"",
            "                \"verification is not enabled on the minion. Either enable \"",
            "                \"signature verification on the minion or disable signing \"",
            "                \"the public key on the master!\"",
            "            )",
            "            return False",
            "        # master does NOT sign but minion wants to verify",
            "        elif \"pub_sig\" not in payload and self.opts[\"verify_master_pubkey_sign\"]:",
            "            log.error(",
            "                \"The master did not send its public-key signature, but \"",
            "                \"signature verification is enabled on the minion. Either \"",
            "                \"disable signature verification on the minion or enable \"",
            "                \"signing the public on the master!\"",
            "            )",
            "            return False",
            "",
            "    def extract_aes(self, payload, master_pub=True):",
            "        \"\"\"",
            "        Return the AES key received from the master after the minion has been",
            "        successfully authenticated.",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', etc)",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The RSA public key of the sender.",
            "",
            "        :rtype: str",
            "        :return: The shared AES key received from the master.",
            "        \"\"\"",
            "        if master_pub:",
            "            try:",
            "                aes, token = self.decrypt_aes(payload, master_pub)",
            "                if token != self.token:",
            "                    log.error(\"The master failed to decrypt the random minion token\")",
            "                    return \"\"",
            "            except Exception:  # pylint: disable=broad-except",
            "                log.error(\"The master failed to decrypt the random minion token\")",
            "                return \"\"",
            "            return aes",
            "        else:",
            "            aes, token = self.decrypt_aes(payload, master_pub)",
            "            return aes",
            "",
            "    def verify_master(self, payload, master_pub=True):",
            "        \"\"\"",
            "        Verify that the master is the same one that was previously accepted.",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', etc)",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The RSA public key of the sender.",
            "        :param bool master_pub: Operate as if minion had no master pubkey when it sent auth request, i.e. don't verify",
            "        the minion signature",
            "",
            "        :rtype: str",
            "        :return: An empty string on verification failure. On success, the decrypted AES message in the payload.",
            "        \"\"\"",
            "        m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "        m_pub_exists = os.path.isfile(m_pub_fn)",
            "        if m_pub_exists and master_pub and not self.opts[\"open_mode\"]:",
            "            with salt.utils.files.fopen(m_pub_fn) as fp_:",
            "                local_master_pub = fp_.read()",
            "",
            "            if payload[\"pub_key\"].replace(\"\\n\", \"\").replace(",
            "                \"\\r\", \"\"",
            "            ) != local_master_pub.replace(\"\\n\", \"\").replace(\"\\r\", \"\"):",
            "                if not self.check_auth_deps(payload):",
            "                    return \"\"",
            "",
            "                if self.opts[\"verify_master_pubkey_sign\"]:",
            "                    if self.verify_signing_master(payload):",
            "                        return self.extract_aes(payload, master_pub=False)",
            "                    else:",
            "                        return \"\"",
            "                else:",
            "                    # This is not the last master we connected to",
            "                    log.error(",
            "                        \"The master key has changed, the salt master could \"",
            "                        \"have been subverted, verify salt master's public \"",
            "                        \"key\"",
            "                    )",
            "                    return \"\"",
            "",
            "            else:",
            "                if not self.check_auth_deps(payload):",
            "                    return \"\"",
            "                # verify the signature of the pubkey even if it has",
            "                # not changed compared with the one we already have",
            "                if self.opts[\"always_verify_signature\"]:",
            "                    if self.verify_signing_master(payload):",
            "                        return self.extract_aes(payload)",
            "                    else:",
            "                        log.error(",
            "                            \"The masters public could not be verified. Is the \"",
            "                            \"verification pubkey %s up to date?\",",
            "                            self.opts[\"master_sign_key_name\"] + \".pub\",",
            "                        )",
            "                        return \"\"",
            "",
            "                else:",
            "                    return self.extract_aes(payload)",
            "        else:",
            "            if not self.check_auth_deps(payload):",
            "                return \"\"",
            "",
            "            # verify the masters pubkey signature if the minion",
            "            # has not received any masters pubkey before",
            "            if self.opts[\"verify_master_pubkey_sign\"]:",
            "                if self.verify_signing_master(payload):",
            "                    return self.extract_aes(payload, master_pub=False)",
            "                else:",
            "                    return \"\"",
            "            else:",
            "                if not m_pub_exists:",
            "                    # the minion has not received any masters pubkey yet, write",
            "                    # the newly received pubkey to minion_master.pub",
            "                    with salt.utils.files.fopen(m_pub_fn, \"wb+\") as fp_:",
            "                        fp_.write(salt.utils.stringutils.to_bytes(payload[\"pub_key\"]))",
            "                return self.extract_aes(payload, master_pub=False)",
            "",
            "    def _finger_fail(self, finger, master_key):",
            "        log.critical(",
            "            \"The specified fingerprint in the master configuration \"",
            "            \"file:\\n%s\\nDoes not match the authenticating master's \"",
            "            \"key:\\n%s\\nVerify that the configured fingerprint \"",
            "            \"matches the fingerprint of the correct master and that \"",
            "            \"this minion is not subject to a man-in-the-middle attack.\",",
            "            finger,",
            "            salt.utils.crypt.pem_finger(master_key, sum_type=self.opts[\"hash_type\"]),",
            "        )",
            "        sys.exit(42)",
            "",
            "",
            "# TODO: remove, we should just return a sync wrapper of AsyncAuth",
            "class SAuth(AsyncAuth):",
            "    \"\"\"",
            "    Set up an object to maintain authentication with the salt master",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    instances = weakref.WeakValueDictionary()",
            "",
            "    def __new__(cls, opts, io_loop=None):",
            "        \"\"\"",
            "        Only create one instance of SAuth per __key()",
            "        \"\"\"",
            "        key = cls.__key(opts)",
            "        auth = SAuth.instances.get(key)",
            "        if auth is None:",
            "            log.debug(\"Initializing new SAuth for %s\", key)",
            "            auth = object.__new__(cls)",
            "            auth.__singleton_init__(opts)",
            "            SAuth.instances[key] = auth",
            "        else:",
            "            log.debug(\"Re-using SAuth for %s\", key)",
            "        return auth",
            "",
            "    @classmethod",
            "    def __key(cls, opts, io_loop=None):",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            opts[\"master_uri\"],  # master ID",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, io_loop=None):",
            "        super().__init__(opts, io_loop=io_loop)",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, io_loop=None):",
            "        \"\"\"",
            "        Init an Auth instance",
            "",
            "        :param dict opts: Options for this server",
            "        :return: Auth instance",
            "        :rtype: Auth",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.pub_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pub\")",
            "        self.rsa_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "        if \"syndic_master\" in self.opts:",
            "            self.mpub = \"syndic_master.pub\"",
            "        elif \"alert_master\" in self.opts:",
            "            self.mpub = \"monitor_master.pub\"",
            "        else:",
            "            self.mpub = \"minion_master.pub\"",
            "        if not os.path.isfile(self.pub_path):",
            "            self.get_keys()",
            "",
            "    @property",
            "    def creds(self):",
            "        if not hasattr(self, \"_creds\"):",
            "            self.authenticate()",
            "        return self._creds",
            "",
            "    @property",
            "    def crypticle(self):",
            "        if not hasattr(self, \"_crypticle\"):",
            "            self.authenticate()",
            "        return self._crypticle",
            "",
            "    def authenticate(self, _=None):  # TODO: remove unused var",
            "        \"\"\"",
            "        Authenticate with the master, this method breaks the functional",
            "        paradigm, it will update the master information from a fresh sign",
            "        in, signing in can occur as often as needed to keep up with the",
            "        revolving master AES key.",
            "",
            "        :rtype: Crypticle",
            "        :returns: A crypticle used for encryption operations",
            "        \"\"\"",
            "        acceptance_wait_time = self.opts[\"acceptance_wait_time\"]",
            "        acceptance_wait_time_max = self.opts[\"acceptance_wait_time_max\"]",
            "        if not acceptance_wait_time_max:",
            "            acceptance_wait_time_max = acceptance_wait_time",
            "        with salt.transport.client.ReqChannel.factory(",
            "            self.opts, crypt=\"clear\"",
            "        ) as channel:",
            "            while True:",
            "                creds = self.sign_in(channel=channel)",
            "                if creds == \"retry\":",
            "                    if self.opts.get(\"caller\"):",
            "                        # We have a list of masters, so we should break",
            "                        # and try the next one in the list.",
            "                        if self.opts.get(\"local_masters\", None):",
            "                            error = SaltClientError(",
            "                                \"Minion failed to authenticate\"",
            "                                \" with the master, has the \"",
            "                                \"minion key been accepted?\"",
            "                            )",
            "                            break",
            "                        else:",
            "                            print(",
            "                                \"Minion failed to authenticate with the master, \"",
            "                                \"has the minion key been accepted?\"",
            "                            )",
            "                            sys.exit(2)",
            "                    if acceptance_wait_time:",
            "                        log.info(",
            "                            \"Waiting %s seconds before retry.\", acceptance_wait_time",
            "                        )",
            "                        time.sleep(acceptance_wait_time)",
            "                    if acceptance_wait_time < acceptance_wait_time_max:",
            "                        acceptance_wait_time += acceptance_wait_time",
            "                        log.debug(",
            "                            \"Authentication wait time is %s\", acceptance_wait_time",
            "                        )",
            "                    continue",
            "                break",
            "            self._creds = creds",
            "            self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "",
            "    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):",
            "        \"\"\"",
            "        Send a sign in request to the master, sets the key information and",
            "        returns a dict containing the master publish interface to bind to",
            "        and the decrypted aes key for transport decryption.",
            "",
            "        :param int timeout: Number of seconds to wait before timing out the sign-in request",
            "        :param bool safe: If True, do not raise an exception on timeout. Retry instead.",
            "        :param int tries: The number of times to try to authenticate before giving up.",
            "",
            "        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set",
            "",
            "        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary",
            "        with the publication port and the shared AES key.",
            "",
            "        \"\"\"",
            "        auth = {}",
            "",
            "        auth_timeout = self.opts.get(\"auth_timeout\", None)",
            "        if auth_timeout is not None:",
            "            timeout = auth_timeout",
            "        auth_safemode = self.opts.get(\"auth_safemode\", None)",
            "        if auth_safemode is not None:",
            "            safe = auth_safemode",
            "        auth_tries = self.opts.get(\"auth_tries\", None)",
            "        if auth_tries is not None:",
            "            tries = auth_tries",
            "",
            "        m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "",
            "        auth[\"master_uri\"] = self.opts[\"master_uri\"]",
            "",
            "        close_channel = False",
            "        if not channel:",
            "            close_channel = True",
            "            channel = salt.transport.client.ReqChannel.factory(self.opts, crypt=\"clear\")",
            "",
            "        sign_in_payload = self.minion_sign_in_payload()",
            "        try:",
            "            payload = channel.send(sign_in_payload, tries=tries, timeout=timeout)",
            "        except SaltReqTimeoutError as e:",
            "            if safe:",
            "                log.warning(\"SaltReqTimeoutError: %s\", e)",
            "                return \"retry\"",
            "            raise SaltClientError(",
            "                \"Attempt to authenticate with the salt master failed with timeout error\"",
            "            )",
            "        finally:",
            "            if close_channel:",
            "                channel.close()",
            "",
            "        return self.handle_signin_response(sign_in_payload, payload)",
            "",
            "",
            "class Crypticle:",
            "    \"\"\"",
            "    Authenticated encryption class",
            "",
            "    Encryption algorithm: AES-CBC",
            "    Signing algorithm: HMAC-SHA256",
            "    \"\"\"",
            "",
            "    PICKLE_PAD = b\"pickle::\"",
            "    AES_BLOCK_SIZE = 16",
            "    SIG_SIZE = hashlib.sha256().digest_size",
            "",
            "    def __init__(self, opts, key_string, key_size=192):",
            "        self.key_string = key_string",
            "        self.keys = self.extract_keys(self.key_string, key_size)",
            "        self.key_size = key_size",
            "        self.serial = salt.payload.Serial(opts)",
            "",
            "    @classmethod",
            "    def generate_key_string(cls, key_size=192):",
            "        key = os.urandom(key_size // 8 + cls.SIG_SIZE)",
            "        b64key = base64.b64encode(key)",
            "        b64key = b64key.decode(\"utf-8\")",
            "        # Return data must be a base64-encoded string, not a unicode type",
            "        return b64key.replace(\"\\n\", \"\")",
            "",
            "    @classmethod",
            "    def extract_keys(cls, key_string, key_size):",
            "        key = salt.utils.stringutils.to_bytes(base64.b64decode(key_string))",
            "        assert len(key) == key_size / 8 + cls.SIG_SIZE, \"invalid key\"",
            "        return key[: -cls.SIG_SIZE], key[-cls.SIG_SIZE :]",
            "",
            "    def encrypt(self, data):",
            "        \"\"\"",
            "        encrypt data with AES-CBC and sign it with HMAC-SHA256",
            "        \"\"\"",
            "        aes_key, hmac_key = self.keys",
            "        pad = self.AES_BLOCK_SIZE - len(data) % self.AES_BLOCK_SIZE",
            "        data = data + salt.utils.stringutils.to_bytes(pad * chr(pad))",
            "        iv_bytes = os.urandom(self.AES_BLOCK_SIZE)",
            "        if HAS_M2:",
            "            cypher = EVP.Cipher(",
            "                alg=\"aes_192_cbc\", key=aes_key, iv=iv_bytes, op=1, padding=False",
            "            )",
            "            encr = cypher.update(data)",
            "            encr += cypher.final()",
            "        else:",
            "            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)",
            "            encr = cypher.encrypt(data)",
            "        data = iv_bytes + encr",
            "        sig = hmac.new(hmac_key, data, hashlib.sha256).digest()",
            "        return data + sig",
            "",
            "    def decrypt(self, data):",
            "        \"\"\"",
            "        verify HMAC-SHA256 signature and decrypt data with AES-CBC",
            "        \"\"\"",
            "        aes_key, hmac_key = self.keys",
            "        sig = data[-self.SIG_SIZE :]",
            "        data = data[: -self.SIG_SIZE]",
            "        if not isinstance(data, bytes):",
            "            data = salt.utils.stringutils.to_bytes(data)",
            "        mac_bytes = hmac.new(hmac_key, data, hashlib.sha256).digest()",
            "        if len(mac_bytes) != len(sig):",
            "            log.debug(\"Failed to authenticate message\")",
            "            raise AuthenticationError(\"message authentication failed\")",
            "        result = 0",
            "",
            "        for zipped_x, zipped_y in zip(mac_bytes, sig):",
            "            result |= zipped_x ^ zipped_y",
            "        if result != 0:",
            "            log.debug(\"Failed to authenticate message\")",
            "            raise AuthenticationError(\"message authentication failed\")",
            "        iv_bytes = data[: self.AES_BLOCK_SIZE]",
            "        data = data[self.AES_BLOCK_SIZE :]",
            "        if HAS_M2:",
            "            cypher = EVP.Cipher(",
            "                alg=\"aes_192_cbc\", key=aes_key, iv=iv_bytes, op=0, padding=False",
            "            )",
            "            encr = cypher.update(data)",
            "            data = encr + cypher.final()",
            "        else:",
            "            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)",
            "            data = cypher.decrypt(data)",
            "        return data[: -data[-1]]",
            "",
            "    def dumps(self, obj):",
            "        \"\"\"",
            "        Serialize and encrypt a python object",
            "        \"\"\"",
            "        return self.encrypt(self.PICKLE_PAD + self.serial.dumps(obj))",
            "",
            "    def loads(self, data, raw=False):",
            "        \"\"\"",
            "        Decrypt and un-serialize a python object",
            "        \"\"\"",
            "        data = self.decrypt(data)",
            "        # simple integrity check to verify that we got meaningful data",
            "        if not data.startswith(self.PICKLE_PAD):",
            "            return {}",
            "        load = self.serial.loads(data[len(self.PICKLE_PAD) :], raw=raw)",
            "        return load"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "The crypt module manages all of the cryptography functions for minions and",
            "masters, encrypting and decrypting payloads, preparing messages, and",
            "authenticating peers",
            "\"\"\"",
            "",
            "import base64",
            "import binascii",
            "import copy",
            "import getpass",
            "import hashlib",
            "import hmac",
            "import logging",
            "import os",
            "import random",
            "import stat",
            "import sys",
            "import time",
            "import traceback",
            "import uuid",
            "import weakref",
            "",
            "import salt.defaults.exitcodes",
            "import salt.ext.tornado.gen",
            "import salt.payload",
            "import salt.transport.client",
            "import salt.transport.frame",
            "import salt.utils.crypt",
            "import salt.utils.decorators",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.rsax931",
            "import salt.utils.sdb",
            "import salt.utils.stringutils",
            "import salt.utils.user",
            "import salt.utils.verify",
            "import salt.version",
            "from salt.exceptions import (",
            "    AuthenticationError,",
            "    MasterExit,",
            "    SaltClientError,",
            "    SaltReqTimeoutError,",
            ")",
            "",
            "try:",
            "    from M2Crypto import RSA, EVP, BIO",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "",
            "if not HAS_M2:",
            "    try:",
            "        from Cryptodome.Cipher import AES, PKCS1_OAEP, PKCS1_v1_5 as PKCS1_v1_5_CIPHER",
            "        from Cryptodome.Hash import SHA",
            "        from Cryptodome.PublicKey import RSA",
            "        from Cryptodome.Signature import PKCS1_v1_5",
            "        from Cryptodome import Random",
            "",
            "        HAS_CRYPTO = True",
            "    except ImportError:",
            "        HAS_CRYPTO = False",
            "",
            "if not HAS_M2 and not HAS_CRYPTO:",
            "    try:",
            "        from Crypto.Cipher import (  # nosec",
            "            AES,",
            "            PKCS1_OAEP,",
            "            PKCS1_v1_5 as PKCS1_v1_5_CIPHER,",
            "        )",
            "        from Crypto.Hash import SHA  # nosec",
            "        from Crypto.PublicKey import RSA  # nosec",
            "        from Crypto.Signature import PKCS1_v1_5  # nosec",
            "",
            "        # let this be imported, if possible",
            "        from Crypto import Random  # nosec pylint: disable=unused-import",
            "",
            "        HAS_CRYPTO = True",
            "    except ImportError:",
            "        HAS_CRYPTO = False",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def dropfile(cachedir, user=None):",
            "    \"\"\"",
            "    Set an AES dropfile to request the master update the publish session key",
            "    \"\"\"",
            "    dfn = os.path.join(cachedir, \".dfn\")",
            "    # set a mask (to avoid a race condition on file creation) and store original.",
            "    with salt.utils.files.set_umask(0o277):",
            "        log.info(\"Rotating AES key\")",
            "        if os.path.isfile(dfn):",
            "            log.info(\"AES key rotation already requested\")",
            "            return",
            "",
            "        if os.path.isfile(dfn) and not os.access(dfn, os.W_OK):",
            "            os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)",
            "        with salt.utils.files.fopen(dfn, \"wb+\") as fp_:",
            "            fp_.write(b\"\")",
            "        os.chmod(dfn, stat.S_IRUSR)",
            "        if user:",
            "            try:",
            "                import pwd",
            "",
            "                uid = pwd.getpwnam(user).pw_uid",
            "                os.chown(dfn, uid, -1)",
            "            except (KeyError, ImportError, OSError):",
            "                pass",
            "",
            "",
            "def gen_keys(keydir, keyname, keysize, user=None, passphrase=None):",
            "    \"\"\"",
            "    Generate a RSA public keypair for use with salt",
            "",
            "    :param str keydir: The directory to write the keypair to",
            "    :param str keyname: The type of salt server for whom this key should be written. (i.e. 'master' or 'minion')",
            "    :param int keysize: The number of bits in the key",
            "    :param str user: The user on the system who should own this keypair",
            "    :param str passphrase: The passphrase which should be used to encrypt the private key",
            "",
            "    :rtype: str",
            "    :return: Path on the filesystem to the RSA private key",
            "    \"\"\"",
            "    base = os.path.join(keydir, keyname)",
            "    priv = \"{}.pem\".format(base)",
            "    pub = \"{}.pub\".format(base)",
            "",
            "    if HAS_M2:",
            "        gen = RSA.gen_key(keysize, 65537, lambda: None)",
            "    else:",
            "        salt.utils.crypt.reinit_crypto()",
            "        gen = RSA.generate(bits=keysize, e=65537)",
            "    if os.path.isfile(priv):",
            "        # Between first checking and the generation another process has made",
            "        # a key! Use the winner's key",
            "        return priv",
            "",
            "    # Do not try writing anything, if directory has no permissions.",
            "    if not os.access(keydir, os.W_OK):",
            "        raise OSError(",
            "            'Write access denied to \"{}\" for user \"{}\".'.format(",
            "                os.path.abspath(keydir), getpass.getuser()",
            "            )",
            "        )",
            "",
            "    with salt.utils.files.set_umask(0o277):",
            "        if HAS_M2:",
            "            # if passphrase is empty or None use no cipher",
            "            if not passphrase:",
            "                gen.save_pem(priv, cipher=None)",
            "            else:",
            "                gen.save_pem(",
            "                    priv,",
            "                    cipher=\"des_ede3_cbc\",",
            "                    callback=lambda x: salt.utils.stringutils.to_bytes(passphrase),",
            "                )",
            "        else:",
            "            with salt.utils.files.fopen(priv, \"wb+\") as f:",
            "                f.write(gen.exportKey(\"PEM\", passphrase))",
            "    if HAS_M2:",
            "        gen.save_pub_key(pub)",
            "    else:",
            "        with salt.utils.files.fopen(pub, \"wb+\") as f:",
            "            f.write(gen.publickey().exportKey(\"PEM\"))",
            "    os.chmod(priv, 0o400)",
            "    if user:",
            "        try:",
            "            import pwd",
            "",
            "            uid = pwd.getpwnam(user).pw_uid",
            "            os.chown(priv, uid, -1)",
            "            os.chown(pub, uid, -1)",
            "        except (KeyError, ImportError, OSError):",
            "            # The specified user was not found, allow the backup systems to",
            "            # report the error",
            "            pass",
            "    return priv",
            "",
            "",
            "@salt.utils.decorators.memoize",
            "def _get_key_with_evict(path, timestamp, passphrase):",
            "    \"\"\"",
            "    Load a private key from disk.  `timestamp` above is intended to be the",
            "    timestamp of the file's last modification. This fn is memoized so if it is",
            "    called with the same path and timestamp (the file's last modified time) the",
            "    second time the result is returned from the memoiziation.  If the file gets",
            "    modified then the params are different and the key is loaded from disk.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt._get_key_with_evict: Loading private key\")",
            "    if HAS_M2:",
            "        key = RSA.load_key(path, lambda x: bytes(passphrase))",
            "    else:",
            "        with salt.utils.files.fopen(path) as f:",
            "            key = RSA.importKey(f.read(), passphrase)",
            "    return key",
            "",
            "",
            "def get_rsa_key(path, passphrase):",
            "    \"\"\"",
            "    Read a private key off the disk.  Poor man's simple cache in effect here,",
            "    we memoize the result of calling _get_rsa_with_evict.  This means the first",
            "    time _get_key_with_evict is called with a path and a timestamp the result",
            "    is cached.  If the file (the private key) does not change then its",
            "    timestamp will not change and the next time the result is returned from the",
            "    cache.  If the key DOES change the next time _get_rsa_with_evict is called",
            "    it is called with different parameters and the fn is run fully to retrieve",
            "    the key from disk.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt.get_rsa_key: Loading private key\")",
            "    return _get_key_with_evict(path, str(os.path.getmtime(path)), passphrase)",
            "",
            "",
            "def get_rsa_pub_key(path):",
            "    \"\"\"",
            "    Read a public key off the disk.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt.get_rsa_pub_key: Loading public key\")",
            "    if HAS_M2:",
            "        with salt.utils.files.fopen(path, \"rb\") as f:",
            "            data = f.read().replace(b\"RSA \", b\"\")",
            "        bio = BIO.MemoryBuffer(data)",
            "        key = RSA.load_pub_key_bio(bio)",
            "    else:",
            "        with salt.utils.files.fopen(path) as f:",
            "            key = RSA.importKey(f.read())",
            "    return key",
            "",
            "",
            "def sign_message(privkey_path, message, passphrase=None):",
            "    \"\"\"",
            "    Use Crypto.Signature.PKCS1_v1_5 to sign a message. Returns the signature.",
            "    \"\"\"",
            "    key = get_rsa_key(privkey_path, passphrase)",
            "    log.debug(\"salt.crypt.sign_message: Signing message.\")",
            "    if HAS_M2:",
            "        md = EVP.MessageDigest(\"sha1\")",
            "        md.update(salt.utils.stringutils.to_bytes(message))",
            "        digest = md.final()",
            "        return key.sign(digest)",
            "    else:",
            "        signer = PKCS1_v1_5.new(key)",
            "        return signer.sign(SHA.new(salt.utils.stringutils.to_bytes(message)))",
            "",
            "",
            "def verify_signature(pubkey_path, message, signature):",
            "    \"\"\"",
            "    Use Crypto.Signature.PKCS1_v1_5 to verify the signature on a message.",
            "    Returns True for valid signature.",
            "    \"\"\"",
            "    log.debug(\"salt.crypt.verify_signature: Loading public key\")",
            "    pubkey = get_rsa_pub_key(pubkey_path)",
            "    log.debug(\"salt.crypt.verify_signature: Verifying signature\")",
            "    if HAS_M2:",
            "        md = EVP.MessageDigest(\"sha1\")",
            "        md.update(salt.utils.stringutils.to_bytes(message))",
            "        digest = md.final()",
            "        try:",
            "            return pubkey.verify(digest, signature)",
            "        except RSA.RSAError as exc:",
            "            if exc.args[0] == \"bad signature\":",
            "                return False",
            "            raise",
            "    else:",
            "        verifier = PKCS1_v1_5.new(pubkey)",
            "        return verifier.verify(",
            "            SHA.new(salt.utils.stringutils.to_bytes(message)), signature",
            "        )",
            "",
            "",
            "def gen_signature(priv_path, pub_path, sign_path, passphrase=None):",
            "    \"\"\"",
            "    creates a signature for the given public-key with",
            "    the given private key and writes it to sign_path",
            "    \"\"\"",
            "",
            "    with salt.utils.files.fopen(pub_path) as fp_:",
            "        mpub_64 = fp_.read()",
            "",
            "    mpub_sig = sign_message(priv_path, mpub_64, passphrase)",
            "    mpub_sig_64 = binascii.b2a_base64(mpub_sig)",
            "    if os.path.isfile(sign_path):",
            "        return False",
            "    log.trace(",
            "        \"Calculating signature for %s with %s\",",
            "        os.path.basename(pub_path),",
            "        os.path.basename(priv_path),",
            "    )",
            "",
            "    if os.path.isfile(sign_path):",
            "        log.trace(",
            "            \"Signature file %s already exists, please remove it first and \" \"try again\",",
            "            sign_path,",
            "        )",
            "    else:",
            "        with salt.utils.files.fopen(sign_path, \"wb+\") as sig_f:",
            "            sig_f.write(salt.utils.stringutils.to_bytes(mpub_sig_64))",
            "        log.trace(\"Wrote signature to %s\", sign_path)",
            "    return True",
            "",
            "",
            "def private_encrypt(key, message):",
            "    \"\"\"",
            "    Generate an M2Crypto-compatible signature",
            "",
            "    :param Crypto.PublicKey.RSA._RSAobj key: The RSA key object",
            "    :param str message: The message to sign",
            "    :rtype: str",
            "    :return: The signature, or an empty string if the signature operation failed",
            "    \"\"\"",
            "    if HAS_M2:",
            "        return key.private_encrypt(message, salt.utils.rsax931.RSA_X931_PADDING)",
            "    else:",
            "        signer = salt.utils.rsax931.RSAX931Signer(key.exportKey(\"PEM\"))",
            "        return signer.sign(message)",
            "",
            "",
            "def public_decrypt(pub, message):",
            "    \"\"\"",
            "    Verify an M2Crypto-compatible signature",
            "",
            "    :param Crypto.PublicKey.RSA._RSAobj key: The RSA public key object",
            "    :param str message: The signed message to verify",
            "    :rtype: str",
            "    :return: The message (or digest) recovered from the signature, or an",
            "        empty string if the verification failed",
            "    \"\"\"",
            "    if HAS_M2:",
            "        return pub.public_decrypt(message, salt.utils.rsax931.RSA_X931_PADDING)",
            "    else:",
            "        verifier = salt.utils.rsax931.RSAX931Verifier(pub.exportKey(\"PEM\"))",
            "        return verifier.verify(message)",
            "",
            "",
            "def pwdata_decrypt(rsa_key, pwdata):",
            "    if HAS_M2:",
            "        key = RSA.load_key_string(salt.utils.stringutils.to_bytes(rsa_key, \"ascii\"))",
            "        password = key.private_decrypt(pwdata, RSA.pkcs1_padding)",
            "    else:",
            "        dsize = SHA.digest_size",
            "        sentinel = Random.new().read(15 + dsize)",
            "        key_obj = RSA.importKey(rsa_key)",
            "        key_obj = PKCS1_v1_5_CIPHER.new(key_obj)",
            "        password = key_obj.decrypt(pwdata, sentinel)",
            "    return salt.utils.stringutils.to_unicode(password)",
            "",
            "",
            "class MasterKeys(dict):",
            "    \"\"\"",
            "    The Master Keys class is used to manage the RSA public key pair used for",
            "    authentication by the master.",
            "",
            "    It also generates a signing key-pair if enabled with master_sign_key_name.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts):",
            "        super().__init__()",
            "        self.opts = opts",
            "        self.pub_path = os.path.join(self.opts[\"pki_dir\"], \"master.pub\")",
            "        self.rsa_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "",
            "        key_pass = salt.utils.sdb.sdb_get(self.opts[\"key_pass\"], self.opts)",
            "        self.key = self.__get_keys(passphrase=key_pass)",
            "",
            "        self.pub_signature = None",
            "",
            "        # set names for the signing key-pairs",
            "        if opts[\"master_sign_pubkey\"]:",
            "",
            "            # if only the signature is available, use that",
            "            if opts[\"master_use_pubkey_signature\"]:",
            "                self.sig_path = os.path.join(",
            "                    self.opts[\"pki_dir\"], opts[\"master_pubkey_signature\"]",
            "                )",
            "                if os.path.isfile(self.sig_path):",
            "                    with salt.utils.files.fopen(self.sig_path) as fp_:",
            "                        self.pub_signature = fp_.read()",
            "                    log.info(",
            "                        \"Read %s's signature from %s\",",
            "                        os.path.basename(self.pub_path),",
            "                        self.opts[\"master_pubkey_signature\"],",
            "                    )",
            "                else:",
            "                    log.error(",
            "                        \"Signing the master.pub key with a signature is \"",
            "                        \"enabled but no signature file found at the defined \"",
            "                        \"location %s\",",
            "                        self.sig_path,",
            "                    )",
            "                    log.error(",
            "                        \"The signature-file may be either named differently \"",
            "                        \"or has to be created with 'salt-key --gen-signature'\"",
            "                    )",
            "                    sys.exit(1)",
            "",
            "            # create a new signing key-pair to sign the masters",
            "            # auth-replies when a minion tries to connect",
            "            else:",
            "                key_pass = salt.utils.sdb.sdb_get(",
            "                    self.opts[\"signing_key_pass\"], self.opts",
            "                )",
            "                self.pub_sign_path = os.path.join(",
            "                    self.opts[\"pki_dir\"], opts[\"master_sign_key_name\"] + \".pub\"",
            "                )",
            "                self.rsa_sign_path = os.path.join(",
            "                    self.opts[\"pki_dir\"], opts[\"master_sign_key_name\"] + \".pem\"",
            "                )",
            "                self.sign_key = self.__get_keys(name=opts[\"master_sign_key_name\"])",
            "",
            "    # We need __setstate__ and __getstate__ to avoid pickling errors since",
            "    # some of the member variables correspond to Cython objects which are",
            "    # not picklable.",
            "    # These methods are only used when pickling so will not be used on",
            "    # non-Windows platforms.",
            "    def __setstate__(self, state):",
            "        self.__init__(state[\"opts\"])",
            "",
            "    def __getstate__(self):",
            "        return {\"opts\": self.opts}",
            "",
            "    def __get_keys(self, name=\"master\", passphrase=None):",
            "        \"\"\"",
            "        Returns a key object for a key in the pki-dir",
            "        \"\"\"",
            "        path = os.path.join(self.opts[\"pki_dir\"], name + \".pem\")",
            "        if not os.path.exists(path):",
            "            log.info(\"Generating %s keys: %s\", name, self.opts[\"pki_dir\"])",
            "            gen_keys(",
            "                self.opts[\"pki_dir\"],",
            "                name,",
            "                self.opts[\"keysize\"],",
            "                self.opts.get(\"user\"),",
            "                passphrase,",
            "            )",
            "        if HAS_M2:",
            "            key_error = RSA.RSAError",
            "        else:",
            "            key_error = ValueError",
            "        try:",
            "            key = get_rsa_key(path, passphrase)",
            "        except key_error as e:",
            "            message = \"Unable to read key: {}; passphrase may be incorrect\".format(path)",
            "            log.error(message)",
            "            raise MasterExit(message)",
            "        log.debug(\"Loaded %s key: %s\", name, path)",
            "        return key",
            "",
            "    def get_pub_str(self, name=\"master\"):",
            "        \"\"\"",
            "        Return the string representation of a public key",
            "        in the pki-directory",
            "        \"\"\"",
            "        path = os.path.join(self.opts[\"pki_dir\"], name + \".pub\")",
            "        if not os.path.isfile(path):",
            "            key = self.__get_keys()",
            "            if HAS_M2:",
            "                key.save_pub_key(path)",
            "            else:",
            "                with salt.utils.files.fopen(path, \"wb+\") as wfh:",
            "                    wfh.write(key.publickey().exportKey(\"PEM\"))",
            "        with salt.utils.files.fopen(path) as rfh:",
            "            return rfh.read()",
            "",
            "    def get_mkey_paths(self):",
            "        return self.pub_path, self.rsa_path",
            "",
            "    def get_sign_paths(self):",
            "        return self.pub_sign_path, self.rsa_sign_path",
            "",
            "    def pubkey_signature(self):",
            "        \"\"\"",
            "        returns the base64 encoded signature from the signature file",
            "        or None if the master has its own signing keys",
            "        \"\"\"",
            "        return self.pub_signature",
            "",
            "",
            "class AsyncAuth:",
            "    \"\"\"",
            "    Set up an Async object to maintain authentication with the salt master",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    # mapping of io_loop -> {key -> auth}",
            "    instance_map = weakref.WeakKeyDictionary()",
            "",
            "    # mapping of key -> creds",
            "    creds_map = {}",
            "",
            "    def __new__(cls, opts, io_loop=None):",
            "        \"\"\"",
            "        Only create one instance of AsyncAuth per __key()",
            "        \"\"\"",
            "        # do we have any mapping for this io_loop",
            "        io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()",
            "        if io_loop not in AsyncAuth.instance_map:",
            "            AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary()",
            "        loop_instance_map = AsyncAuth.instance_map[io_loop]",
            "",
            "        key = cls.__key(opts)",
            "        auth = loop_instance_map.get(key)",
            "        if auth is None:",
            "            log.debug(\"Initializing new AsyncAuth for %s\", key)",
            "            # we need to make a local variable for this, as we are going to store",
            "            # it in a WeakValueDictionary-- which will remove the item if no one",
            "            # references it-- this forces a reference while we return to the caller",
            "            auth = object.__new__(cls)",
            "            auth.__singleton_init__(opts, io_loop=io_loop)",
            "            loop_instance_map[key] = auth",
            "        else:",
            "            log.debug(\"Re-using AsyncAuth for %s\", key)",
            "        return auth",
            "",
            "    @classmethod",
            "    def __key(cls, opts, io_loop=None):",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            opts[\"master_uri\"],  # master ID",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, io_loop=None):",
            "        pass",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, io_loop=None):",
            "        \"\"\"",
            "        Init an Auth instance",
            "",
            "        :param dict opts: Options for this server",
            "        :return: Auth instance",
            "        :rtype: Auth",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.pub_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pub\")",
            "        self.rsa_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "        if self.opts[\"__role\"] == \"syndic\":",
            "            self.mpub = \"syndic_master.pub\"",
            "        else:",
            "            self.mpub = \"minion_master.pub\"",
            "        if not os.path.isfile(self.pub_path):",
            "            self.get_keys()",
            "",
            "        self.io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        salt.utils.crypt.reinit_crypto()",
            "        key = self.__key(self.opts)",
            "        # TODO: if we already have creds for this key, lets just re-use",
            "        if key in AsyncAuth.creds_map:",
            "            creds = AsyncAuth.creds_map[key]",
            "            self._creds = creds",
            "            self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "            self._authenticate_future = salt.ext.tornado.concurrent.Future()",
            "            self._authenticate_future.set_result(True)",
            "        else:",
            "            self.authenticate()",
            "",
            "    def __deepcopy__(self, memo):",
            "        cls = self.__class__",
            "        result = cls.__new__(cls, copy.deepcopy(self.opts, memo))",
            "        memo[id(self)] = result",
            "        for key in self.__dict__:",
            "            if key in (\"io_loop\",):",
            "                # The io_loop has a thread Lock which will fail to be deep",
            "                # copied. Skip it because it will just be recreated on the",
            "                # new copy.",
            "                continue",
            "            setattr(result, key, copy.deepcopy(self.__dict__[key], memo))",
            "        return result",
            "",
            "    @property",
            "    def creds(self):",
            "        return self._creds",
            "",
            "    @property",
            "    def crypticle(self):",
            "        return self._crypticle",
            "",
            "    @property",
            "    def authenticated(self):",
            "        return (",
            "            hasattr(self, \"_authenticate_future\")",
            "            and self._authenticate_future.done()",
            "            and self._authenticate_future.exception() is None",
            "        )",
            "",
            "    def invalidate(self):",
            "        if self.authenticated:",
            "            del self._authenticate_future",
            "            key = self.__key(self.opts)",
            "            if key in AsyncAuth.creds_map:",
            "                del AsyncAuth.creds_map[key]",
            "",
            "    def authenticate(self, callback=None):",
            "        \"\"\"",
            "        Ask for this client to reconnect to the origin",
            "",
            "        This function will de-dupe all calls here and return a *single* future",
            "        for the sign-in-- whis way callers can all assume there aren't others",
            "        \"\"\"",
            "        # if an auth is in flight-- and not done-- just pass that back as the future to wait on",
            "        if (",
            "            hasattr(self, \"_authenticate_future\")",
            "            and not self._authenticate_future.done()",
            "        ):",
            "            future = self._authenticate_future",
            "        else:",
            "            future = salt.ext.tornado.concurrent.Future()",
            "            self._authenticate_future = future",
            "            self.io_loop.add_callback(self._authenticate)",
            "",
            "        if callback is not None:",
            "",
            "            def handle_future(future):",
            "                response = future.result()",
            "                self.io_loop.add_callback(callback, response)",
            "",
            "            future.add_done_callback(handle_future)",
            "",
            "        return future",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _authenticate(self):",
            "        \"\"\"",
            "        Authenticate with the master, this method breaks the functional",
            "        paradigm, it will update the master information from a fresh sign",
            "        in, signing in can occur as often as needed to keep up with the",
            "        revolving master AES key.",
            "",
            "        :rtype: Crypticle",
            "        :returns: A crypticle used for encryption operations",
            "        \"\"\"",
            "        acceptance_wait_time = self.opts[\"acceptance_wait_time\"]",
            "        acceptance_wait_time_max = self.opts[\"acceptance_wait_time_max\"]",
            "        if not acceptance_wait_time_max:",
            "            acceptance_wait_time_max = acceptance_wait_time",
            "        creds = None",
            "",
            "        with salt.transport.client.AsyncReqChannel.factory(",
            "            self.opts, crypt=\"clear\", io_loop=self.io_loop",
            "        ) as channel:",
            "            error = None",
            "            while True:",
            "                try:",
            "                    creds = yield self.sign_in(channel=channel)",
            "                except SaltClientError as exc:",
            "                    error = exc",
            "                    break",
            "                if creds == \"retry\":",
            "                    if self.opts.get(\"detect_mode\") is True:",
            "                        error = SaltClientError(\"Detect mode is on\")",
            "                        break",
            "                    if self.opts.get(\"caller\"):",
            "                        # We have a list of masters, so we should break",
            "                        # and try the next one in the list.",
            "                        if self.opts.get(\"local_masters\", None):",
            "                            error = SaltClientError(",
            "                                \"Minion failed to authenticate\"",
            "                                \" with the master, has the \"",
            "                                \"minion key been accepted?\"",
            "                            )",
            "                            break",
            "                        else:",
            "                            print(",
            "                                \"Minion failed to authenticate with the master, \"",
            "                                \"has the minion key been accepted?\"",
            "                            )",
            "                            sys.exit(2)",
            "                    if acceptance_wait_time:",
            "                        log.info(",
            "                            \"Waiting %s seconds before retry.\", acceptance_wait_time",
            "                        )",
            "                        yield salt.ext.tornado.gen.sleep(acceptance_wait_time)",
            "                    if acceptance_wait_time < acceptance_wait_time_max:",
            "                        acceptance_wait_time += acceptance_wait_time",
            "                        log.debug(",
            "                            \"Authentication wait time is %s\", acceptance_wait_time",
            "                        )",
            "                    continue",
            "                break",
            "            if not isinstance(creds, dict) or \"aes\" not in creds:",
            "                if self.opts.get(\"detect_mode\") is True:",
            "                    error = SaltClientError(\"-|RETRY|-\")",
            "                try:",
            "                    del AsyncAuth.creds_map[self.__key(self.opts)]",
            "                except KeyError:",
            "                    pass",
            "                if not error:",
            "                    error = SaltClientError(",
            "                        \"Attempt to authenticate with the salt master failed\"",
            "                    )",
            "                self._authenticate_future.set_exception(error)",
            "            else:",
            "                key = self.__key(self.opts)",
            "                if key not in AsyncAuth.creds_map:",
            "                    log.debug(\"%s Got new master aes key.\", self)",
            "                    AsyncAuth.creds_map[key] = creds",
            "                    self._creds = creds",
            "                    self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "                elif self._creds[\"aes\"] != creds[\"aes\"]:",
            "                    log.debug(\"%s The master's aes key has changed.\", self)",
            "                    AsyncAuth.creds_map[key] = creds",
            "                    self._creds = creds",
            "                    self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "",
            "                self._authenticate_future.set_result(",
            "                    True",
            "                )  # mark the sign-in as complete",
            "                # Notify the bus about creds change",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    with salt.utils.event.get_event(",
            "                        self.opts.get(\"__role\"), opts=self.opts, listen=False",
            "                    ) as event:",
            "                        event.fire_event(",
            "                            {\"key\": key, \"creds\": creds},",
            "                            salt.utils.event.tagify(prefix=\"auth\", suffix=\"creds\"),",
            "                        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):",
            "        \"\"\"",
            "        Send a sign in request to the master, sets the key information and",
            "        returns a dict containing the master publish interface to bind to",
            "        and the decrypted aes key for transport decryption.",
            "",
            "        :param int timeout: Number of seconds to wait before timing out the sign-in request",
            "        :param bool safe: If True, do not raise an exception on timeout. Retry instead.",
            "        :param int tries: The number of times to try to authenticate before giving up.",
            "",
            "        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set",
            "",
            "        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary",
            "        with the publication port and the shared AES key.",
            "",
            "        \"\"\"",
            "",
            "        auth_timeout = self.opts.get(\"auth_timeout\", None)",
            "        if auth_timeout is not None:",
            "            timeout = auth_timeout",
            "        auth_safemode = self.opts.get(\"auth_safemode\", None)",
            "        if auth_safemode is not None:",
            "            safe = auth_safemode",
            "        auth_tries = self.opts.get(\"auth_tries\", None)",
            "        if auth_tries is not None:",
            "            tries = auth_tries",
            "",
            "        close_channel = False",
            "        if not channel:",
            "            close_channel = True",
            "            channel = salt.transport.client.AsyncReqChannel.factory(",
            "                self.opts, crypt=\"clear\", io_loop=self.io_loop",
            "            )",
            "",
            "        sign_in_payload = self.minion_sign_in_payload()",
            "        try:",
            "            payload = yield channel.send(sign_in_payload, tries=tries, timeout=timeout)",
            "        except SaltReqTimeoutError as e:",
            "            if safe:",
            "                log.warning(\"SaltReqTimeoutError: %s\", e)",
            "                raise salt.ext.tornado.gen.Return(\"retry\")",
            "            if self.opts.get(\"detect_mode\") is True:",
            "                raise salt.ext.tornado.gen.Return(\"retry\")",
            "            else:",
            "                raise SaltClientError(",
            "                    \"Attempt to authenticate with the salt master failed with timeout error\"",
            "                )",
            "        finally:",
            "            if close_channel:",
            "                channel.close()",
            "        ret = self.handle_signin_response(sign_in_payload, payload)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    def handle_signin_response(self, sign_in_payload, payload):",
            "        auth = {}",
            "        m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "        auth[\"master_uri\"] = self.opts[\"master_uri\"]",
            "        if not isinstance(payload, dict) or \"load\" not in payload:",
            "            log.error(\"Sign-in attempt failed: %s\", payload)",
            "            return False",
            "",
            "        clear_signed_data = payload[\"load\"]",
            "        clear_signature = payload[\"sig\"]",
            "        payload = self.serial.loads(clear_signed_data)",
            "",
            "        if \"pub_key\" in payload:",
            "            auth[\"aes\"] = self.verify_master(",
            "                payload, master_pub=\"token\" in sign_in_payload",
            "            )",
            "            if not auth[\"aes\"]:",
            "                log.critical(",
            "                    \"The Salt Master server's public key did not authenticate!\\n\"",
            "                    \"The master may need to be updated if it is a version of Salt \"",
            "                    \"lower than %s, or\\n\"",
            "                    \"If you are confident that you are connecting to a valid Salt \"",
            "                    \"Master, then remove the master public key and restart the \"",
            "                    \"Salt Minion.\\nThe master public key can be found \"",
            "                    \"at:\\n%s\",",
            "                    salt.version.__version__,",
            "                    m_pub_fn,",
            "                )",
            "                raise SaltClientError(\"Invalid master key\")",
            "",
            "        master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "        if os.path.exists(master_pubkey_path) and not verify_signature(",
            "            master_pubkey_path, clear_signed_data, clear_signature",
            "        ):",
            "            log.critical(\"The payload signature did not validate.\")",
            "            raise SaltClientError(\"Invalid signature\")",
            "",
            "        if payload[\"nonce\"] != sign_in_payload[\"nonce\"]:",
            "            log.critical(\"The payload nonce did not validate.\")",
            "            raise SaltClientError(\"Invalid nonce\")",
            "",
            "        if \"ret\" in payload:",
            "            if not payload[\"ret\"]:",
            "                if self.opts[\"rejected_retry\"]:",
            "                    log.error(",
            "                        \"The Salt Master has rejected this minion's public \"",
            "                        \"key.\\nTo repair this issue, delete the public key \"",
            "                        \"for this minion on the Salt Master.\\nThe Salt \"",
            "                        \"Minion will attempt to re-authenicate.\"",
            "                    )",
            "                    return \"retry\"",
            "                else:",
            "                    log.critical(",
            "                        \"The Salt Master has rejected this minion's public \"",
            "                        \"key!\\nTo repair this issue, delete the public key \"",
            "                        \"for this minion on the Salt Master and restart this \"",
            "                        \"minion.\\nOr restart the Salt Master in open mode to \"",
            "                        \"clean out the keys. The Salt Minion will now exit.\"",
            "                    )",
            "                    # Add a random sleep here for systems that are using a",
            "                    # a service manager to immediately restart the service",
            "                    # to avoid overloading the system",
            "                    time.sleep(random.randint(10, 20))",
            "                    sys.exit(salt.defaults.exitcodes.EX_NOPERM)",
            "            # has the master returned that its maxed out with minions?",
            "            elif payload[\"ret\"] == \"full\":",
            "                return \"full\"",
            "            else:",
            "                log.error(",
            "                    \"The Salt Master has cached the public key for this \"",
            "                    \"node, this salt minion will wait for %s seconds \"",
            "                    \"before attempting to re-authenticate\",",
            "                    self.opts[\"acceptance_wait_time\"],",
            "                )",
            "                return \"retry\"",
            "",
            "        if self.opts.get(\"syndic_master\", False):  # Is syndic",
            "            syndic_finger = self.opts.get(",
            "                \"syndic_finger\", self.opts.get(\"master_finger\", False)",
            "            )",
            "            if syndic_finger:",
            "                if (",
            "                    salt.utils.crypt.pem_finger(",
            "                        m_pub_fn, sum_type=self.opts[\"hash_type\"]",
            "                    )",
            "                    != syndic_finger",
            "                ):",
            "                    self._finger_fail(syndic_finger, m_pub_fn)",
            "        else:",
            "            if self.opts.get(\"master_finger\", False):",
            "                if (",
            "                    salt.utils.crypt.pem_finger(",
            "                        m_pub_fn, sum_type=self.opts[\"hash_type\"]",
            "                    )",
            "                    != self.opts[\"master_finger\"]",
            "                ):",
            "                    self._finger_fail(self.opts[\"master_finger\"], m_pub_fn)",
            "",
            "        auth[\"publish_port\"] = payload[\"publish_port\"]",
            "        return auth",
            "",
            "    def get_keys(self):",
            "        \"\"\"",
            "        Return keypair object for the minion.",
            "",
            "        :rtype: Crypto.PublicKey.RSA._RSAobj",
            "        :return: The RSA keypair",
            "        \"\"\"",
            "        # Make sure all key parent directories are accessible",
            "        user = self.opts.get(\"user\", \"root\")",
            "        salt.utils.verify.check_path_traversal(self.opts[\"pki_dir\"], user)",
            "",
            "        if not os.path.exists(self.rsa_path):",
            "            log.info(\"Generating keys: %s\", self.opts[\"pki_dir\"])",
            "            gen_keys(",
            "                self.opts[\"pki_dir\"],",
            "                \"minion\",",
            "                self.opts[\"keysize\"],",
            "                self.opts.get(\"user\"),",
            "            )",
            "        key = get_rsa_key(self.rsa_path, None)",
            "        log.debug(\"Loaded minion key: %s\", self.rsa_path)",
            "        return key",
            "",
            "    def gen_token(self, clear_tok):",
            "        \"\"\"",
            "        Encrypt a string with the minion private key to verify identity",
            "        with the master.",
            "",
            "        :param str clear_tok: A plaintext token to encrypt",
            "        :return: Encrypted token",
            "        :rtype: str",
            "        \"\"\"",
            "        return private_encrypt(self.get_keys(), clear_tok)",
            "",
            "    def minion_sign_in_payload(self):",
            "        \"\"\"",
            "        Generates the payload used to authenticate with the master",
            "        server. This payload consists of the passed in id_ and the ssh",
            "        public key to encrypt the AES key sent back from the master.",
            "",
            "        :return: Payload dictionary",
            "        :rtype: dict",
            "        \"\"\"",
            "        payload = {}",
            "        payload[\"cmd\"] = \"_auth\"",
            "        payload[\"id\"] = self.opts[\"id\"]",
            "        payload[\"nonce\"] = uuid.uuid4().hex",
            "        if \"autosign_grains\" in self.opts:",
            "            autosign_grains = {}",
            "            for grain in self.opts[\"autosign_grains\"]:",
            "                autosign_grains[grain] = self.opts[\"grains\"].get(grain, None)",
            "            payload[\"autosign_grains\"] = autosign_grains",
            "        try:",
            "            pubkey_path = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "            pub = get_rsa_pub_key(pubkey_path)",
            "            if HAS_M2:",
            "                payload[\"token\"] = pub.public_encrypt(",
            "                    self.token, RSA.pkcs1_oaep_padding",
            "                )",
            "            else:",
            "                cipher = PKCS1_OAEP.new(pub)",
            "                payload[\"token\"] = cipher.encrypt(self.token)",
            "        except Exception:  # pylint: disable=broad-except",
            "            pass",
            "        with salt.utils.files.fopen(self.pub_path) as f:",
            "            payload[\"pub\"] = f.read()",
            "        return payload",
            "",
            "    def decrypt_aes(self, payload, master_pub=True):",
            "        \"\"\"",
            "        This function is used to decrypt the AES seed phrase returned from",
            "        the master server. The seed phrase is decrypted with the SSH RSA",
            "        host key.",
            "",
            "        Pass in the encrypted AES key.",
            "        Returns the decrypted AES seed key, a string",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', etc)",
            "            'sig': The message signature",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The public key of the sender.",
            "",
            "        :rtype: str",
            "        :return: The decrypted token that was provided, with padding.",
            "",
            "        :rtype: str",
            "        :return: The decrypted AES seed key",
            "        \"\"\"",
            "        if self.opts.get(\"auth_trb\", False):",
            "            log.warning(\"Auth Called: %s\", \"\".join(traceback.format_stack()))",
            "        else:",
            "            log.debug(\"Decrypting the current master AES key\")",
            "        key = self.get_keys()",
            "        if HAS_M2:",
            "            key_str = key.private_decrypt(payload[\"aes\"], RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(key)",
            "            key_str = cipher.decrypt(payload[\"aes\"])",
            "        if \"sig\" in payload:",
            "            m_path = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "            if os.path.exists(m_path):",
            "                try:",
            "                    mkey = get_rsa_pub_key(m_path)",
            "                except Exception:  # pylint: disable=broad-except",
            "                    return \"\", \"\"",
            "                digest = hashlib.sha256(key_str).hexdigest()",
            "                digest = salt.utils.stringutils.to_bytes(digest)",
            "                if HAS_M2:",
            "                    m_digest = public_decrypt(mkey, payload[\"sig\"])",
            "                else:",
            "                    m_digest = public_decrypt(mkey.publickey(), payload[\"sig\"])",
            "                if m_digest != digest:",
            "                    return \"\", \"\"",
            "        else:",
            "            return \"\", \"\"",
            "",
            "        key_str = salt.utils.stringutils.to_str(key_str)",
            "",
            "        if \"_|-\" in key_str:",
            "            return key_str.split(\"_|-\")",
            "        else:",
            "            if \"token\" in payload:",
            "                if HAS_M2:",
            "                    token = key.private_decrypt(",
            "                        payload[\"token\"], RSA.pkcs1_oaep_padding",
            "                    )",
            "                else:",
            "                    token = cipher.decrypt(payload[\"token\"])",
            "                return key_str, token",
            "            elif not master_pub:",
            "                return key_str, \"\"",
            "        return \"\", \"\"",
            "",
            "    def verify_pubkey_sig(self, message, sig):",
            "        \"\"\"",
            "        Wraps the verify_signature method so we have",
            "        additional checks.",
            "",
            "        :rtype: bool",
            "        :return: Success or failure of public key verification",
            "        \"\"\"",
            "        if self.opts[\"master_sign_key_name\"]:",
            "            path = os.path.join(",
            "                self.opts[\"pki_dir\"], self.opts[\"master_sign_key_name\"] + \".pub\"",
            "            )",
            "",
            "            if os.path.isfile(path):",
            "                res = verify_signature(path, message, binascii.a2b_base64(sig))",
            "            else:",
            "                log.error(",
            "                    \"Verification public key %s does not exist. You need to \"",
            "                    \"copy it from the master to the minions pki directory\",",
            "                    os.path.basename(path),",
            "                )",
            "                return False",
            "            if res:",
            "                log.debug(",
            "                    \"Successfully verified signature of master public key \"",
            "                    \"with verification public key %s\",",
            "                    self.opts[\"master_sign_key_name\"] + \".pub\",",
            "                )",
            "                return True",
            "            else:",
            "                log.debug(\"Failed to verify signature of public key\")",
            "                return False",
            "        else:",
            "            log.error(",
            "                \"Failed to verify the signature of the message because the \"",
            "                \"verification key-pairs name is not defined. Please make \"",
            "                \"sure that master_sign_key_name is defined.\"",
            "            )",
            "            return False",
            "",
            "    def verify_signing_master(self, payload):",
            "        try:",
            "            if self.verify_pubkey_sig(payload[\"pub_key\"], payload[\"pub_sig\"]):",
            "                log.info(",
            "                    \"Received signed and verified master pubkey from master %s\",",
            "                    self.opts[\"master\"],",
            "                )",
            "                m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "                uid = salt.utils.user.get_uid(self.opts.get(\"user\", None))",
            "                with salt.utils.files.fpopen(m_pub_fn, \"wb+\", uid=uid) as wfh:",
            "                    wfh.write(salt.utils.stringutils.to_bytes(payload[\"pub_key\"]))",
            "                return True",
            "            else:",
            "                log.error(",
            "                    \"Received signed public-key from master %s but signature \"",
            "                    \"verification failed!\",",
            "                    self.opts[\"master\"],",
            "                )",
            "                return False",
            "        except Exception as sign_exc:  # pylint: disable=broad-except",
            "            log.error(",
            "                \"There was an error while verifying the masters public-key \" \"signature\"",
            "            )",
            "            raise Exception(sign_exc)",
            "",
            "    def check_auth_deps(self, payload):",
            "        \"\"\"",
            "        Checks if both master and minion either sign (master) and",
            "        verify (minion). If one side does not, it should fail.",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', 'aes')",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The RSA public key of the sender.",
            "        \"\"\"",
            "        # master and minion sign and verify",
            "        if \"pub_sig\" in payload and self.opts[\"verify_master_pubkey_sign\"]:",
            "            return True",
            "        # master and minion do NOT sign and do NOT verify",
            "        elif \"pub_sig\" not in payload and not self.opts[\"verify_master_pubkey_sign\"]:",
            "            return True",
            "",
            "        # master signs, but minion does NOT verify",
            "        elif \"pub_sig\" in payload and not self.opts[\"verify_master_pubkey_sign\"]:",
            "            log.error(",
            "                \"The masters sent its public-key signature, but signature \"",
            "                \"verification is not enabled on the minion. Either enable \"",
            "                \"signature verification on the minion or disable signing \"",
            "                \"the public key on the master!\"",
            "            )",
            "            return False",
            "        # master does NOT sign but minion wants to verify",
            "        elif \"pub_sig\" not in payload and self.opts[\"verify_master_pubkey_sign\"]:",
            "            log.error(",
            "                \"The master did not send its public-key signature, but \"",
            "                \"signature verification is enabled on the minion. Either \"",
            "                \"disable signature verification on the minion or enable \"",
            "                \"signing the public on the master!\"",
            "            )",
            "            return False",
            "",
            "    def extract_aes(self, payload, master_pub=True):",
            "        \"\"\"",
            "        Return the AES key received from the master after the minion has been",
            "        successfully authenticated.",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', etc)",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The RSA public key of the sender.",
            "",
            "        :rtype: str",
            "        :return: The shared AES key received from the master.",
            "        \"\"\"",
            "        if master_pub:",
            "            try:",
            "                aes, token = self.decrypt_aes(payload, master_pub)",
            "                if token != self.token:",
            "                    log.error(\"The master failed to decrypt the random minion token\")",
            "                    return \"\"",
            "            except Exception:  # pylint: disable=broad-except",
            "                log.error(\"The master failed to decrypt the random minion token\")",
            "                return \"\"",
            "            return aes",
            "        else:",
            "            aes, token = self.decrypt_aes(payload, master_pub)",
            "            return aes",
            "",
            "    def verify_master(self, payload, master_pub=True):",
            "        \"\"\"",
            "        Verify that the master is the same one that was previously accepted.",
            "",
            "        :param dict payload: The incoming payload. This is a dictionary which may have the following keys:",
            "            'aes': The shared AES key",
            "            'enc': The format of the message. ('clear', 'pub', etc)",
            "            'publish_port': The TCP port which published the message",
            "            'token': The encrypted token used to verify the message.",
            "            'pub_key': The RSA public key of the sender.",
            "        :param bool master_pub: Operate as if minion had no master pubkey when it sent auth request, i.e. don't verify",
            "        the minion signature",
            "",
            "        :rtype: str",
            "        :return: An empty string on verification failure. On success, the decrypted AES message in the payload.",
            "        \"\"\"",
            "        m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "        m_pub_exists = os.path.isfile(m_pub_fn)",
            "        if m_pub_exists and master_pub and not self.opts[\"open_mode\"]:",
            "            with salt.utils.files.fopen(m_pub_fn) as fp_:",
            "                local_master_pub = fp_.read()",
            "",
            "            if payload[\"pub_key\"].replace(\"\\n\", \"\").replace(",
            "                \"\\r\", \"\"",
            "            ) != local_master_pub.replace(\"\\n\", \"\").replace(\"\\r\", \"\"):",
            "                if not self.check_auth_deps(payload):",
            "                    return \"\"",
            "",
            "                if self.opts[\"verify_master_pubkey_sign\"]:",
            "                    if self.verify_signing_master(payload):",
            "                        return self.extract_aes(payload, master_pub=False)",
            "                    else:",
            "                        return \"\"",
            "                else:",
            "                    # This is not the last master we connected to",
            "                    log.error(",
            "                        \"The master key has changed, the salt master could \"",
            "                        \"have been subverted, verify salt master's public \"",
            "                        \"key\"",
            "                    )",
            "                    return \"\"",
            "",
            "            else:",
            "                if not self.check_auth_deps(payload):",
            "                    return \"\"",
            "                # verify the signature of the pubkey even if it has",
            "                # not changed compared with the one we already have",
            "                if self.opts[\"always_verify_signature\"]:",
            "                    if self.verify_signing_master(payload):",
            "                        return self.extract_aes(payload)",
            "                    else:",
            "                        log.error(",
            "                            \"The masters public could not be verified. Is the \"",
            "                            \"verification pubkey %s up to date?\",",
            "                            self.opts[\"master_sign_key_name\"] + \".pub\",",
            "                        )",
            "                        return \"\"",
            "",
            "                else:",
            "                    return self.extract_aes(payload)",
            "        else:",
            "            if not self.check_auth_deps(payload):",
            "                return \"\"",
            "",
            "            # verify the masters pubkey signature if the minion",
            "            # has not received any masters pubkey before",
            "            if self.opts[\"verify_master_pubkey_sign\"]:",
            "                if self.verify_signing_master(payload):",
            "                    return self.extract_aes(payload, master_pub=False)",
            "                else:",
            "                    return \"\"",
            "            else:",
            "                if not m_pub_exists:",
            "                    # the minion has not received any masters pubkey yet, write",
            "                    # the newly received pubkey to minion_master.pub",
            "                    with salt.utils.files.fopen(m_pub_fn, \"wb+\") as fp_:",
            "                        fp_.write(salt.utils.stringutils.to_bytes(payload[\"pub_key\"]))",
            "                return self.extract_aes(payload, master_pub=False)",
            "",
            "    def _finger_fail(self, finger, master_key):",
            "        log.critical(",
            "            \"The specified fingerprint in the master configuration \"",
            "            \"file:\\n%s\\nDoes not match the authenticating master's \"",
            "            \"key:\\n%s\\nVerify that the configured fingerprint \"",
            "            \"matches the fingerprint of the correct master and that \"",
            "            \"this minion is not subject to a man-in-the-middle attack.\",",
            "            finger,",
            "            salt.utils.crypt.pem_finger(master_key, sum_type=self.opts[\"hash_type\"]),",
            "        )",
            "        sys.exit(42)",
            "",
            "",
            "# TODO: remove, we should just return a sync wrapper of AsyncAuth",
            "class SAuth(AsyncAuth):",
            "    \"\"\"",
            "    Set up an object to maintain authentication with the salt master",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    instances = weakref.WeakValueDictionary()",
            "",
            "    def __new__(cls, opts, io_loop=None):",
            "        \"\"\"",
            "        Only create one instance of SAuth per __key()",
            "        \"\"\"",
            "        key = cls.__key(opts)",
            "        auth = SAuth.instances.get(key)",
            "        if auth is None:",
            "            log.debug(\"Initializing new SAuth for %s\", key)",
            "            auth = object.__new__(cls)",
            "            auth.__singleton_init__(opts)",
            "            SAuth.instances[key] = auth",
            "        else:",
            "            log.debug(\"Re-using SAuth for %s\", key)",
            "        return auth",
            "",
            "    @classmethod",
            "    def __key(cls, opts, io_loop=None):",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            opts[\"master_uri\"],  # master ID",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, io_loop=None):",
            "        super().__init__(opts, io_loop=io_loop)",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, io_loop=None):",
            "        \"\"\"",
            "        Init an Auth instance",
            "",
            "        :param dict opts: Options for this server",
            "        :return: Auth instance",
            "        :rtype: Auth",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.pub_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pub\")",
            "        self.rsa_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "        self._creds = None",
            "        if \"syndic_master\" in self.opts:",
            "            self.mpub = \"syndic_master.pub\"",
            "        elif \"alert_master\" in self.opts:",
            "            self.mpub = \"monitor_master.pub\"",
            "        else:",
            "            self.mpub = \"minion_master.pub\"",
            "        if not os.path.isfile(self.pub_path):",
            "            self.get_keys()",
            "",
            "    @property",
            "    def creds(self):",
            "        if not hasattr(self, \"_creds\"):",
            "            self.authenticate()",
            "        return self._creds",
            "",
            "    @property",
            "    def crypticle(self):",
            "        if not hasattr(self, \"_crypticle\"):",
            "            self.authenticate()",
            "        return self._crypticle",
            "",
            "    def authenticate(self, _=None):  # TODO: remove unused var",
            "        \"\"\"",
            "        Authenticate with the master, this method breaks the functional",
            "        paradigm, it will update the master information from a fresh sign",
            "        in, signing in can occur as often as needed to keep up with the",
            "        revolving master AES key.",
            "",
            "        :rtype: Crypticle",
            "        :returns: A crypticle used for encryption operations",
            "        \"\"\"",
            "        acceptance_wait_time = self.opts[\"acceptance_wait_time\"]",
            "        acceptance_wait_time_max = self.opts[\"acceptance_wait_time_max\"]",
            "        if not acceptance_wait_time_max:",
            "            acceptance_wait_time_max = acceptance_wait_time",
            "        with salt.transport.client.ReqChannel.factory(",
            "            self.opts, crypt=\"clear\"",
            "        ) as channel:",
            "            while True:",
            "                creds = self.sign_in(channel=channel)",
            "                if creds == \"retry\":",
            "                    if self.opts.get(\"caller\"):",
            "                        # We have a list of masters, so we should break",
            "                        # and try the next one in the list.",
            "                        if self.opts.get(\"local_masters\", None):",
            "                            error = SaltClientError(",
            "                                \"Minion failed to authenticate\"",
            "                                \" with the master, has the \"",
            "                                \"minion key been accepted?\"",
            "                            )",
            "                            break",
            "                        else:",
            "                            print(",
            "                                \"Minion failed to authenticate with the master, \"",
            "                                \"has the minion key been accepted?\"",
            "                            )",
            "                            sys.exit(2)",
            "                    if acceptance_wait_time:",
            "                        log.info(",
            "                            \"Waiting %s seconds before retry.\", acceptance_wait_time",
            "                        )",
            "                        time.sleep(acceptance_wait_time)",
            "                    if acceptance_wait_time < acceptance_wait_time_max:",
            "                        acceptance_wait_time += acceptance_wait_time",
            "                        log.debug(",
            "                            \"Authentication wait time is %s\", acceptance_wait_time",
            "                        )",
            "                    continue",
            "                break",
            "            if self._creds is None:",
            "                log.error(\"%s Got new master aes key.\", self)",
            "                self._creds = creds",
            "                self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "            elif self._creds[\"aes\"] != creds[\"aes\"]:",
            "                log.error(\"%s The master's aes key has changed.\", self)",
            "                self._creds = creds",
            "                self._crypticle = Crypticle(self.opts, creds[\"aes\"])",
            "",
            "    def sign_in(self, timeout=60, safe=True, tries=1, channel=None):",
            "        \"\"\"",
            "        Send a sign in request to the master, sets the key information and",
            "        returns a dict containing the master publish interface to bind to",
            "        and the decrypted aes key for transport decryption.",
            "",
            "        :param int timeout: Number of seconds to wait before timing out the sign-in request",
            "        :param bool safe: If True, do not raise an exception on timeout. Retry instead.",
            "        :param int tries: The number of times to try to authenticate before giving up.",
            "",
            "        :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set",
            "",
            "        :return: Return a string on failure indicating the reason for failure. On success, return a dictionary",
            "        with the publication port and the shared AES key.",
            "",
            "        \"\"\"",
            "        auth = {}",
            "",
            "        auth_timeout = self.opts.get(\"auth_timeout\", None)",
            "        if auth_timeout is not None:",
            "            timeout = auth_timeout",
            "        auth_safemode = self.opts.get(\"auth_safemode\", None)",
            "        if auth_safemode is not None:",
            "            safe = auth_safemode",
            "        auth_tries = self.opts.get(\"auth_tries\", None)",
            "        if auth_tries is not None:",
            "            tries = auth_tries",
            "",
            "        m_pub_fn = os.path.join(self.opts[\"pki_dir\"], self.mpub)",
            "",
            "        auth[\"master_uri\"] = self.opts[\"master_uri\"]",
            "",
            "        close_channel = False",
            "        if not channel:",
            "            close_channel = True",
            "            channel = salt.transport.client.ReqChannel.factory(self.opts, crypt=\"clear\")",
            "",
            "        sign_in_payload = self.minion_sign_in_payload()",
            "        try:",
            "            payload = channel.send(sign_in_payload, tries=tries, timeout=timeout)",
            "        except SaltReqTimeoutError as e:",
            "            if safe:",
            "                log.warning(\"SaltReqTimeoutError: %s\", e)",
            "                return \"retry\"",
            "            raise SaltClientError(",
            "                \"Attempt to authenticate with the salt master failed with timeout error\"",
            "            )",
            "        finally:",
            "            if close_channel:",
            "                channel.close()",
            "",
            "        return self.handle_signin_response(sign_in_payload, payload)",
            "",
            "",
            "class Crypticle:",
            "    \"\"\"",
            "    Authenticated encryption class",
            "",
            "    Encryption algorithm: AES-CBC",
            "    Signing algorithm: HMAC-SHA256",
            "    \"\"\"",
            "",
            "    PICKLE_PAD = b\"pickle::\"",
            "    AES_BLOCK_SIZE = 16",
            "    SIG_SIZE = hashlib.sha256().digest_size",
            "",
            "    def __init__(self, opts, key_string, key_size=192, serial=0):",
            "        self.key_string = key_string",
            "        self.keys = self.extract_keys(self.key_string, key_size)",
            "        self.key_size = key_size",
            "        self.serial = serial",
            "",
            "    @classmethod",
            "    def generate_key_string(cls, key_size=192):",
            "        key = os.urandom(key_size // 8 + cls.SIG_SIZE)",
            "        b64key = base64.b64encode(key)",
            "        b64key = b64key.decode(\"utf-8\")",
            "        # Return data must be a base64-encoded string, not a unicode type",
            "        return b64key.replace(\"\\n\", \"\")",
            "",
            "    @classmethod",
            "    def extract_keys(cls, key_string, key_size):",
            "        key = salt.utils.stringutils.to_bytes(base64.b64decode(key_string))",
            "        assert len(key) == key_size / 8 + cls.SIG_SIZE, \"invalid key\"",
            "        return key[: -cls.SIG_SIZE], key[-cls.SIG_SIZE :]",
            "",
            "    def encrypt(self, data):",
            "        \"\"\"",
            "        encrypt data with AES-CBC and sign it with HMAC-SHA256",
            "        \"\"\"",
            "        aes_key, hmac_key = self.keys",
            "        pad = self.AES_BLOCK_SIZE - len(data) % self.AES_BLOCK_SIZE",
            "        data = data + salt.utils.stringutils.to_bytes(pad * chr(pad))",
            "        iv_bytes = os.urandom(self.AES_BLOCK_SIZE)",
            "        if HAS_M2:",
            "            cypher = EVP.Cipher(",
            "                alg=\"aes_192_cbc\", key=aes_key, iv=iv_bytes, op=1, padding=False",
            "            )",
            "            encr = cypher.update(data)",
            "            encr += cypher.final()",
            "        else:",
            "            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)",
            "            encr = cypher.encrypt(data)",
            "        data = iv_bytes + encr",
            "        sig = hmac.new(hmac_key, data, hashlib.sha256).digest()",
            "        return data + sig",
            "",
            "    def decrypt(self, data):",
            "        \"\"\"",
            "        verify HMAC-SHA256 signature and decrypt data with AES-CBC",
            "        \"\"\"",
            "        aes_key, hmac_key = self.keys",
            "        sig = data[-self.SIG_SIZE :]",
            "        data = data[: -self.SIG_SIZE]",
            "        if not isinstance(data, bytes):",
            "            data = salt.utils.stringutils.to_bytes(data)",
            "        mac_bytes = hmac.new(hmac_key, data, hashlib.sha256).digest()",
            "        if len(mac_bytes) != len(sig):",
            "            log.debug(\"Failed to authenticate message\")",
            "            raise AuthenticationError(\"message authentication failed\")",
            "        result = 0",
            "",
            "        for zipped_x, zipped_y in zip(mac_bytes, sig):",
            "            result |= zipped_x ^ zipped_y",
            "        if result != 0:",
            "            log.debug(\"Failed to authenticate message\")",
            "            raise AuthenticationError(\"message authentication failed\")",
            "        iv_bytes = data[: self.AES_BLOCK_SIZE]",
            "        data = data[self.AES_BLOCK_SIZE :]",
            "        if HAS_M2:",
            "            cypher = EVP.Cipher(",
            "                alg=\"aes_192_cbc\", key=aes_key, iv=iv_bytes, op=0, padding=False",
            "            )",
            "            encr = cypher.update(data)",
            "            data = encr + cypher.final()",
            "        else:",
            "            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)",
            "            data = cypher.decrypt(data)",
            "        return data[: -data[-1]]",
            "",
            "    def dumps(self, obj, nonce=None):",
            "        \"\"\"",
            "        Serialize and encrypt a python object",
            "        \"\"\"",
            "        if nonce:",
            "            toencrypt = (",
            "                self.PICKLE_PAD + nonce.encode() + salt.payload.Serial({}).dumps(obj)",
            "            )",
            "        else:",
            "            toencrypt = self.PICKLE_PAD + salt.payload.Serial({}).dumps(obj)",
            "        return self.encrypt(toencrypt)",
            "",
            "    def loads(self, data, raw=False, nonce=None):",
            "        \"\"\"",
            "        Decrypt and un-serialize a python object",
            "        \"\"\"",
            "        data = self.decrypt(data)",
            "        # simple integrity check to verify that we got meaningful data",
            "        if not data.startswith(self.PICKLE_PAD):",
            "            return {}",
            "        data = data[len(self.PICKLE_PAD) :]",
            "        if nonce:",
            "            ret_nonce = data[:32].decode()",
            "            data = data[32:]",
            "            if ret_nonce != nonce:",
            "                raise SaltClientError(\"Nonce verification error\")",
            "        payload = salt.payload.Serial({}).loads(data, raw=raw)",
            "        if isinstance(payload, dict):",
            "            if \"serial\" in payload:",
            "                serial = payload.pop(\"serial\")",
            "                if serial <= self.serial:",
            "                    log.critical(",
            "                        \"A message with an invalid serial was received.\\n\"",
            "                        \"this serial: %d\\n\"",
            "                        \"last serial: %d\\n\"",
            "                        \"The minion will not honor this request.\",",
            "                        serial,",
            "                        self.serial,",
            "                    )",
            "                    return {}",
            "                self.serial = serial",
            "        return payload"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "699": [
                "AsyncAuth",
                "_authenticate"
            ],
            "700": [
                "AsyncAuth",
                "_authenticate"
            ],
            "701": [
                "AsyncAuth",
                "_authenticate"
            ],
            "1349": [
                "SAuth",
                "authenticate"
            ],
            "1350": [
                "SAuth",
                "authenticate"
            ],
            "1418": [
                "Crypticle",
                "__init__"
            ],
            "1422": [
                "Crypticle",
                "__init__"
            ],
            "1492": [
                "Crypticle",
                "dumps"
            ],
            "1496": [
                "Crypticle",
                "dumps"
            ],
            "1498": [
                "Crypticle",
                "loads"
            ],
            "1506": [
                "Crypticle",
                "loads"
            ],
            "1507": [
                "Crypticle",
                "loads"
            ]
        },
        "addLocation": []
    },
    "salt/master.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "         \"\"\""
            },
            "1": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "         return salt.daemons.masterapi.access_keys(self.opts)"
            },
            "2": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 144,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 145,
                "PatchRowcode": "+    @classmethod"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+    def get_serial(cls, opts=None, event=None):"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+        with cls.secrets[\"aes\"][\"secret\"].get_lock():"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 148,
                "PatchRowcode": "+            if cls.secrets[\"aes\"][\"serial\"].value == sys.maxsize:"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+                cls.rotate_secrets(opts, event, use_lock=False)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+            else:"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 151,
                "PatchRowcode": "+                cls.secrets[\"aes\"][\"serial\"].value += 1"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 152,
                "PatchRowcode": "+            return cls.secrets[\"aes\"][\"serial\"].value"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 153,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+    @classmethod"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 155,
                "PatchRowcode": "+    def rotate_secrets(cls, opts=None, event=None, use_lock=True):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 156,
                "PatchRowcode": "+        log.info(\"Rotating master AES key\")"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 157,
                "PatchRowcode": "+        if opts is None:"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 158,
                "PatchRowcode": "+            opts = {}"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 159,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 160,
                "PatchRowcode": "+        for secret_key, secret_map in cls.secrets.items():"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 161,
                "PatchRowcode": "+            # should be unnecessary-- since no one else should be modifying"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 162,
                "PatchRowcode": "+            if use_lock:"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 163,
                "PatchRowcode": "+                with secret_map[\"secret\"].get_lock():"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 164,
                "PatchRowcode": "+                    secret_map[\"secret\"].value = salt.utils.stringutils.to_bytes("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 165,
                "PatchRowcode": "+                        secret_map[\"reload\"]()"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 166,
                "PatchRowcode": "+                    )"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 167,
                "PatchRowcode": "+                    if \"serial\" in secret_map:"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+                        secret_map[\"serial\"].value = 0"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+            else:"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+                secret_map[\"secret\"].value = salt.utils.stringutils.to_bytes("
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+                    secret_map[\"reload\"]()"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 172,
                "PatchRowcode": "+                )"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 173,
                "PatchRowcode": "+                if \"serial\" in secret_map:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 174,
                "PatchRowcode": "+                    secret_map[\"serial\"].value = 0"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 175,
                "PatchRowcode": "+            if event:"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 176,
                "PatchRowcode": "+                event.fire_event({\"rotate_{}_key\".format(secret_key): True}, tag=\"key\")"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 178,
                "PatchRowcode": "+        if opts.get(\"ping_on_rotate\"):"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 179,
                "PatchRowcode": "+            # Ping all minions to get them to pick up the new key"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 180,
                "PatchRowcode": "+            log.debug(\"Pinging all connected minions due to key rotation\")"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 181,
                "PatchRowcode": "+            salt.utils.master.ping_all_connected_minions(opts)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 182,
                "PatchRowcode": "+"
            },
            "41": {
                "beforePatchRowNumber": 145,
                "afterPatchRowNumber": 183,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 146,
                "afterPatchRowNumber": 184,
                "PatchRowcode": " class Maintenance(salt.utils.process.SignalHandlingProcess):"
            },
            "43": {
                "beforePatchRowNumber": 147,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "     \"\"\""
            },
            "44": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 350,
                "PatchRowcode": "                 to_rotate = True"
            },
            "45": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 351,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": 352,
                "PatchRowcode": "         if to_rotate:"
            },
            "47": {
                "beforePatchRowNumber": 315,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            log.info(\"Rotating master AES key\")"
            },
            "48": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            for secret_key, secret_map in SMaster.secrets.items():"
            },
            "49": {
                "beforePatchRowNumber": 317,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # should be unnecessary-- since no one else should be modifying"
            },
            "50": {
                "beforePatchRowNumber": 318,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                with secret_map[\"secret\"].get_lock():"
            },
            "51": {
                "beforePatchRowNumber": 319,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    secret_map[\"secret\"].value = salt.utils.stringutils.to_bytes("
            },
            "52": {
                "beforePatchRowNumber": 320,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        secret_map[\"reload\"]()"
            },
            "53": {
                "beforePatchRowNumber": 321,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    )"
            },
            "54": {
                "beforePatchRowNumber": 322,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.event.fire_event("
            },
            "55": {
                "beforePatchRowNumber": 323,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    {\"rotate_{}_key\".format(secret_key): True}, tag=\"key\""
            },
            "56": {
                "beforePatchRowNumber": 324,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                )"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 353,
                "PatchRowcode": "+            SMaster.rotate_secrets(self.opts, self.event)"
            },
            "58": {
                "beforePatchRowNumber": 325,
                "afterPatchRowNumber": 354,
                "PatchRowcode": "             self.rotate = now"
            },
            "59": {
                "beforePatchRowNumber": 326,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if self.opts.get(\"ping_on_rotate\"):"
            },
            "60": {
                "beforePatchRowNumber": 327,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # Ping all minions to get them to pick up the new key"
            },
            "61": {
                "beforePatchRowNumber": 328,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                log.debug(\"Pinging all connected minions \" \"due to key rotation\")"
            },
            "62": {
                "beforePatchRowNumber": 329,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                salt.utils.master.ping_all_connected_minions(self.opts)"
            },
            "63": {
                "beforePatchRowNumber": 330,
                "afterPatchRowNumber": 355,
                "PatchRowcode": " "
            },
            "64": {
                "beforePatchRowNumber": 331,
                "afterPatchRowNumber": 356,
                "PatchRowcode": "     def handle_git_pillar(self):"
            },
            "65": {
                "beforePatchRowNumber": 332,
                "afterPatchRowNumber": 357,
                "PatchRowcode": "         \"\"\""
            },
            "66": {
                "beforePatchRowNumber": 712,
                "afterPatchRowNumber": 737,
                "PatchRowcode": "                         salt.crypt.Crypticle.generate_key_string()"
            },
            "67": {
                "beforePatchRowNumber": 713,
                "afterPatchRowNumber": 738,
                "PatchRowcode": "                     ),"
            },
            "68": {
                "beforePatchRowNumber": 714,
                "afterPatchRowNumber": 739,
                "PatchRowcode": "                 ),"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 740,
                "PatchRowcode": "+                \"serial\": multiprocessing.Value("
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 741,
                "PatchRowcode": "+                    ctypes.c_longlong, lock=False  # We'll use the lock from 'secret'"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 742,
                "PatchRowcode": "+                ),"
            },
            "72": {
                "beforePatchRowNumber": 715,
                "afterPatchRowNumber": 743,
                "PatchRowcode": "                 \"reload\": salt.crypt.Crypticle.generate_key_string,"
            },
            "73": {
                "beforePatchRowNumber": 716,
                "afterPatchRowNumber": 744,
                "PatchRowcode": "             }"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 745,
                "PatchRowcode": "+"
            },
            "75": {
                "beforePatchRowNumber": 717,
                "afterPatchRowNumber": 746,
                "PatchRowcode": "             log.info(\"Creating master process manager\")"
            },
            "76": {
                "beforePatchRowNumber": 718,
                "afterPatchRowNumber": 747,
                "PatchRowcode": "             # Since there are children having their own ProcessManager we should wait for kill more time."
            },
            "77": {
                "beforePatchRowNumber": 719,
                "afterPatchRowNumber": 748,
                "PatchRowcode": "             self.process_manager = salt.utils.process.ProcessManager(wait_for_kill=5)"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "This module contains all of the routines needed to set up a master server, this",
            "involves preparing the three listeners and the workers needed by the master.",
            "\"\"\"",
            "",
            "",
            "import collections",
            "import copy",
            "import ctypes",
            "import functools",
            "import logging",
            "import multiprocessing",
            "import os",
            "import re",
            "import signal",
            "import stat",
            "import sys",
            "import threading",
            "import time",
            "",
            "import salt.acl",
            "import salt.auth",
            "import salt.client",
            "import salt.client.ssh.client",
            "import salt.crypt",
            "import salt.daemons.masterapi",
            "import salt.defaults.exitcodes",
            "import salt.engines",
            "import salt.exceptions",
            "import salt.ext.tornado.gen  # pylint: disable=F0401",
            "import salt.key",
            "import salt.log.setup",
            "import salt.minion",
            "import salt.payload",
            "import salt.pillar",
            "import salt.runner",
            "import salt.serializers.msgpack",
            "import salt.state",
            "import salt.transport.server",
            "import salt.utils.args",
            "import salt.utils.atomicfile",
            "import salt.utils.crypt",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.gitfs",
            "import salt.utils.gzip_util",
            "import salt.utils.jid",
            "import salt.utils.job",
            "import salt.utils.master",
            "import salt.utils.minions",
            "import salt.utils.platform",
            "import salt.utils.process",
            "import salt.utils.schedule",
            "import salt.utils.ssdp",
            "import salt.utils.stringutils",
            "import salt.utils.user",
            "import salt.utils.verify",
            "import salt.utils.zeromq",
            "import salt.wheel",
            "from salt.config import DEFAULT_INTERVAL",
            "from salt.defaults import DEFAULT_TARGET_DELIM",
            "",
            "# pylint: disable=import-error,no-name-in-module,redefined-builtin",
            "from salt.ext import six",
            "from salt.ext.six.moves import range",
            "from salt.ext.tornado.stack_context import StackContext",
            "from salt.transport import iter_transport_opts",
            "from salt.utils.ctx import RequestContext",
            "from salt.utils.debug import (",
            "    enable_sigusr1_handler,",
            "    enable_sigusr2_handler,",
            "    inspect_stack,",
            ")",
            "from salt.utils.event import tagify",
            "from salt.utils.odict import OrderedDict",
            "from salt.utils.zeromq import ZMQ_VERSION_INFO, ZMQDefaultLoop, install_zmq, zmq",
            "",
            "# pylint: enable=import-error,no-name-in-module,redefined-builtin",
            "",
            "",
            "try:",
            "    import resource",
            "",
            "    HAS_RESOURCE = True",
            "except ImportError:",
            "    # resource is not available on windows",
            "    HAS_RESOURCE = False",
            "",
            "try:",
            "    import halite  # pylint: disable=import-error",
            "",
            "    HAS_HALITE = True",
            "except ImportError:",
            "    HAS_HALITE = False",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "class SMaster:",
            "    \"\"\"",
            "    Create a simple salt-master, this will generate the top-level master",
            "    \"\"\"",
            "",
            "    secrets = (",
            "        {}",
            "    )  # mapping of key -> {'secret': multiprocessing type, 'reload': FUNCTION}",
            "",
            "    def __init__(self, opts):",
            "        \"\"\"",
            "        Create a salt master server instance",
            "",
            "        :param dict opts: The salt options dictionary",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.master_key = salt.crypt.MasterKeys(self.opts)",
            "        self.key = self.__prep_key()",
            "",
            "    # We need __setstate__ and __getstate__ to also pickle 'SMaster.secrets'.",
            "    # Otherwise, 'SMaster.secrets' won't be copied over to the spawned process",
            "    # on Windows since spawning processes on Windows requires pickling.",
            "    # These methods are only used when pickling so will not be used on",
            "    # non-Windows platforms.",
            "    def __setstate__(self, state):",
            "        self.opts = state[\"opts\"]",
            "        self.master_key = state[\"master_key\"]",
            "        self.key = state[\"key\"]",
            "        SMaster.secrets = state[\"secrets\"]",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"master_key\": self.master_key,",
            "            \"key\": self.key,",
            "            \"secrets\": SMaster.secrets,",
            "        }",
            "",
            "    def __prep_key(self):",
            "        \"\"\"",
            "        A key needs to be placed in the filesystem with permissions 0400 so",
            "        clients are required to run as root.",
            "        \"\"\"",
            "        return salt.daemons.masterapi.access_keys(self.opts)",
            "",
            "",
            "class Maintenance(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    A generalized maintenance process which performs maintenance routines.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        \"\"\"",
            "        Create a maintenance instance",
            "",
            "        :param dict opts: The salt options",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        # How often do we perform the maintenance tasks",
            "        self.loop_interval = int(self.opts[\"loop_interval\"])",
            "        # Track key rotation intervals",
            "        self.rotate = int(time.time())",
            "        # A serializer for general maint operations",
            "        self.serial = salt.payload.Serial(self.opts)",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"opts\"],",
            "            log_queue=state[\"log_queue\"],",
            "            log_queue_level=state[\"log_queue_level\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def _post_fork_init(self):",
            "        \"\"\"",
            "        Some things need to be init'd after the fork has completed",
            "        The easiest example is that one of these module types creates a thread",
            "        in the parent process, then once the fork happens you'll start getting",
            "        errors like \"WARNING: Mixing fork() and threads detected; memory leaked.\"",
            "        \"\"\"",
            "        # Load Runners",
            "        ropts = dict(self.opts)",
            "        ropts[\"quiet\"] = True",
            "        runner_client = salt.runner.RunnerClient(ropts)",
            "        # Load Returners",
            "        self.returners = salt.loader.returners(self.opts, {})",
            "",
            "        # Init Scheduler",
            "        self.schedule = salt.utils.schedule.Schedule(",
            "            self.opts, runner_client.functions_dict(), returners=self.returners",
            "        )",
            "        self.ckminions = salt.utils.minions.CkMinions(self.opts)",
            "        # Make Event bus for firing",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        # Init any values needed by the git ext pillar",
            "        self.git_pillar = salt.daemons.masterapi.init_git_pillar(self.opts)",
            "",
            "        if self.opts[\"maintenance_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting Maintenance niceness to %d\", self.opts[\"maintenance_niceness\"]",
            "            )",
            "            os.nice(self.opts[\"maintenance_niceness\"])",
            "",
            "        self.presence_events = False",
            "        if self.opts.get(\"presence_events\", False):",
            "            tcp_only = True",
            "            for transport, _ in iter_transport_opts(self.opts):",
            "                if transport != \"tcp\":",
            "                    tcp_only = False",
            "            if not tcp_only:",
            "                # For a TCP only transport, the presence events will be",
            "                # handled in the transport code.",
            "                self.presence_events = True",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        This is the general passive maintenance process controller for the Salt",
            "        master.",
            "",
            "        This is where any data that needs to be cleanly maintained from the",
            "        master is maintained.",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        # init things that need to be done after the process is forked",
            "        self._post_fork_init()",
            "",
            "        # Make Start Times",
            "        last = int(time.time())",
            "        # update git_pillar on first loop",
            "        last_git_pillar_update = 0",
            "",
            "        git_pillar_update_interval = self.opts.get(\"git_pillar_update_interval\", 0)",
            "        old_present = set()",
            "        while True:",
            "            now = int(time.time())",
            "            if (now - last) >= self.loop_interval:",
            "                salt.daemons.masterapi.clean_old_jobs(self.opts)",
            "                salt.daemons.masterapi.clean_expired_tokens(self.opts)",
            "                salt.daemons.masterapi.clean_pub_auth(self.opts)",
            "            if (now - last_git_pillar_update) >= git_pillar_update_interval:",
            "                last_git_pillar_update = now",
            "                self.handle_git_pillar()",
            "            self.handle_schedule()",
            "            self.handle_key_cache()",
            "            self.handle_presence(old_present)",
            "            self.handle_key_rotate(now)",
            "            salt.utils.verify.check_max_open_files(self.opts)",
            "            last = now",
            "            time.sleep(self.loop_interval)",
            "",
            "    def handle_key_cache(self):",
            "        \"\"\"",
            "        Evaluate accepted keys and create a msgpack file",
            "        which contains a list",
            "        \"\"\"",
            "        if self.opts[\"key_cache\"] == \"sched\":",
            "            keys = []",
            "            # TODO DRY from CKMinions",
            "            if self.opts[\"transport\"] in (\"zeromq\", \"tcp\"):",
            "                acc = \"minions\"",
            "            else:",
            "                acc = \"accepted\"",
            "",
            "            for fn_ in os.listdir(os.path.join(self.opts[\"pki_dir\"], acc)):",
            "                if not fn_.startswith(\".\") and os.path.isfile(",
            "                    os.path.join(self.opts[\"pki_dir\"], acc, fn_)",
            "                ):",
            "                    keys.append(fn_)",
            "            log.debug(\"Writing master key cache\")",
            "            # Write a temporary file securely",
            "            with salt.utils.atomicfile.atomic_open(",
            "                os.path.join(self.opts[\"pki_dir\"], acc, \".key_cache\"), mode=\"wb\"",
            "            ) as cache_file:",
            "                self.serial.dump(keys, cache_file)",
            "",
            "    def handle_key_rotate(self, now):",
            "        \"\"\"",
            "        Rotate the AES key rotation",
            "        \"\"\"",
            "        to_rotate = False",
            "        dfn = os.path.join(self.opts[\"cachedir\"], \".dfn\")",
            "        try:",
            "            stats = os.stat(dfn)",
            "            # Basic Windows permissions don't distinguish between",
            "            # user/group/all. Check for read-only state instead.",
            "            if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):",
            "                to_rotate = True",
            "                # Cannot delete read-only files on Windows.",
            "                os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)",
            "            elif stats.st_mode == 0o100400:",
            "                to_rotate = True",
            "            else:",
            "                log.error(\"Found dropfile with incorrect permissions, ignoring...\")",
            "            os.remove(dfn)",
            "        except os.error:",
            "            pass",
            "",
            "        if self.opts.get(\"publish_session\"):",
            "            if now - self.rotate >= self.opts[\"publish_session\"]:",
            "                to_rotate = True",
            "",
            "        if to_rotate:",
            "            log.info(\"Rotating master AES key\")",
            "            for secret_key, secret_map in SMaster.secrets.items():",
            "                # should be unnecessary-- since no one else should be modifying",
            "                with secret_map[\"secret\"].get_lock():",
            "                    secret_map[\"secret\"].value = salt.utils.stringutils.to_bytes(",
            "                        secret_map[\"reload\"]()",
            "                    )",
            "                self.event.fire_event(",
            "                    {\"rotate_{}_key\".format(secret_key): True}, tag=\"key\"",
            "                )",
            "            self.rotate = now",
            "            if self.opts.get(\"ping_on_rotate\"):",
            "                # Ping all minions to get them to pick up the new key",
            "                log.debug(\"Pinging all connected minions \" \"due to key rotation\")",
            "                salt.utils.master.ping_all_connected_minions(self.opts)",
            "",
            "    def handle_git_pillar(self):",
            "        \"\"\"",
            "        Update git pillar",
            "        \"\"\"",
            "        try:",
            "            for pillar in self.git_pillar:",
            "                pillar.fetch_remotes()",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception caught while updating git_pillar\", exc_info=True)",
            "",
            "    def handle_schedule(self):",
            "        \"\"\"",
            "        Evaluate the scheduler",
            "        \"\"\"",
            "        try:",
            "            self.schedule.eval()",
            "            # Check if scheduler requires lower loop interval than",
            "            # the loop_interval setting",
            "            if self.schedule.loop_interval < self.loop_interval:",
            "                self.loop_interval = self.schedule.loop_interval",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception %s occurred in scheduled job\", exc)",
            "        self.schedule.cleanup_subprocesses()",
            "",
            "    def handle_presence(self, old_present):",
            "        \"\"\"",
            "        Fire presence events if enabled",
            "        \"\"\"",
            "        # On the first run it may need more time for the EventPublisher",
            "        # to come up and be ready. Set the timeout to account for this.",
            "        if self.presence_events and self.event.connect_pull(timeout=3):",
            "            present = self.ckminions.connected_ids()",
            "            new = present.difference(old_present)",
            "            lost = old_present.difference(present)",
            "            if new or lost:",
            "                # Fire new minions present event",
            "                data = {\"new\": list(new), \"lost\": list(lost)}",
            "                self.event.fire_event(data, tagify(\"change\", \"presence\"))",
            "            data = {\"present\": list(present)}",
            "            self.event.fire_event(data, tagify(\"present\", \"presence\"))",
            "            old_present.clear()",
            "            old_present.update(present)",
            "",
            "",
            "class FileserverUpdate(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    A process from which to update any dynamic fileserver backends",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        self.update_threads = {}",
            "        # Avoid circular import",
            "        import salt.fileserver",
            "",
            "        self.fileserver = salt.fileserver.Fileserver(self.opts)",
            "        self.fill_buckets()",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"opts\"], log_queue=state[\"log_queue\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"log_queue\": self.log_queue,",
            "        }",
            "",
            "    def fill_buckets(self):",
            "        \"\"\"",
            "        Get the configured backends and the intervals for any backend which",
            "        supports them, and set up the update \"buckets\". There will be one",
            "        bucket for each thing being updated at a given interval.",
            "        \"\"\"",
            "        update_intervals = self.fileserver.update_intervals()",
            "        self.buckets = {}",
            "        for backend in self.fileserver.backends():",
            "            fstr = \"{}.update\".format(backend)",
            "            try:",
            "                update_func = self.fileserver.servers[fstr]",
            "            except KeyError:",
            "                log.debug(\"No update function for the %s filserver backend\", backend)",
            "                continue",
            "            if backend in update_intervals:",
            "                # Variable intervals are supported for this backend",
            "                for id_, interval in update_intervals[backend].items():",
            "                    if not interval:",
            "                        # Don't allow an interval of 0",
            "                        interval = DEFAULT_INTERVAL",
            "                        log.debug(",
            "                            \"An update_interval of 0 is not supported, \"",
            "                            \"falling back to %s\",",
            "                            interval,",
            "                        )",
            "                    i_ptr = self.buckets.setdefault(interval, OrderedDict())",
            "                    # Backend doesn't technically need to be present in the",
            "                    # key, all we *really* need is the function reference, but",
            "                    # having it there makes it easier to provide meaningful",
            "                    # debug logging in the update threads.",
            "                    i_ptr.setdefault((backend, update_func), []).append(id_)",
            "            else:",
            "                # Variable intervals are not supported for this backend, so",
            "                # fall back to the global interval for that fileserver. Since",
            "                # this backend doesn't support variable updates, we have",
            "                # nothing to pass to the backend's update func, so we'll just",
            "                # set the value to None.",
            "                try:",
            "                    interval_key = \"{}_update_interval\".format(backend)",
            "                    interval = self.opts[interval_key]",
            "                except KeyError:",
            "                    interval = DEFAULT_INTERVAL",
            "                    log.warning(",
            "                        \"%s key missing from configuration. Falling back to \"",
            "                        \"default interval of %d seconds\",",
            "                        interval_key,",
            "                        interval,",
            "                    )",
            "                self.buckets.setdefault(interval, OrderedDict())[",
            "                    (backend, update_func)",
            "                ] = None",
            "",
            "    def update_fileserver(self, interval, backends):",
            "        \"\"\"",
            "        Threading target which handles all updates for a given wait interval",
            "        \"\"\"",
            "",
            "        def _do_update():",
            "            log.debug(",
            "                \"Performing fileserver updates for items with an update \"",
            "                \"interval of %d\",",
            "                interval,",
            "            )",
            "            for backend, update_args in backends.items():",
            "                backend_name, update_func = backend",
            "                try:",
            "                    if update_args:",
            "                        log.debug(",
            "                            \"Updating %s fileserver cache for the following \"",
            "                            \"targets: %s\",",
            "                            backend_name,",
            "                            update_args,",
            "                        )",
            "                        args = (update_args,)",
            "                    else:",
            "                        log.debug(\"Updating %s fileserver cache\", backend_name)",
            "                        args = ()",
            "",
            "                    update_func(*args)",
            "                except Exception as exc:  # pylint: disable=broad-except",
            "                    log.exception(",
            "                        \"Uncaught exception while updating %s fileserver \" \"cache\",",
            "                        backend_name,",
            "                    )",
            "",
            "            log.debug(",
            "                \"Completed fileserver updates for items with an update \"",
            "                \"interval of %d, waiting %d seconds\",",
            "                interval,",
            "                interval,",
            "            )",
            "",
            "        condition = threading.Condition()",
            "        _do_update()",
            "        while True:",
            "            with condition:",
            "                condition.wait(interval)",
            "            _do_update()",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Start the update threads",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        if (",
            "            self.opts[\"fileserver_update_niceness\"]",
            "            and not salt.utils.platform.is_windows()",
            "        ):",
            "            log.info(",
            "                \"setting FileServerUpdate niceness to %d\",",
            "                self.opts[\"fileserver_update_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"fileserver_update_niceness\"])",
            "",
            "        # Clean out the fileserver backend cache",
            "        salt.daemons.masterapi.clean_fsbackend(self.opts)",
            "",
            "        for interval in self.buckets:",
            "            self.update_threads[interval] = threading.Thread(",
            "                target=self.update_fileserver, args=(interval, self.buckets[interval]),",
            "            )",
            "            self.update_threads[interval].start()",
            "",
            "        # Keep the process alive",
            "        while True:",
            "            time.sleep(60)",
            "",
            "",
            "class Master(SMaster):",
            "    \"\"\"",
            "    The salt master server",
            "    \"\"\"",
            "",
            "    def __init__(self, opts):",
            "        \"\"\"",
            "        Create a salt master server instance",
            "",
            "        :param dict: The salt options",
            "        \"\"\"",
            "        if zmq and ZMQ_VERSION_INFO < (3, 2):",
            "            log.warning(",
            "                \"You have a version of ZMQ less than ZMQ 3.2! There are \"",
            "                \"known connection keep-alive issues with ZMQ < 3.2 which \"",
            "                \"may result in loss of contact with minions. Please \"",
            "                \"upgrade your ZMQ!\"",
            "            )",
            "        SMaster.__init__(self, opts)",
            "",
            "    def __set_max_open_files(self):",
            "        if not HAS_RESOURCE:",
            "            return",
            "        # Let's check to see how our max open files(ulimit -n) setting is",
            "        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)",
            "        if mof_h == resource.RLIM_INFINITY:",
            "            # Unclear what to do with infinity... macOS reports RLIM_INFINITY as",
            "            # hard limit,but raising to anything above soft limit fails...",
            "            mof_h = mof_s",
            "        log.info(",
            "            \"Current values for max open files soft/hard setting: %s/%s\", mof_s, mof_h",
            "        )",
            "        # Let's grab, from the configuration file, the value to raise max open",
            "        # files to",
            "        mof_c = self.opts[\"max_open_files\"]",
            "        if mof_c > mof_h:",
            "            # The configured value is higher than what's allowed",
            "            log.info(",
            "                \"The value for the 'max_open_files' setting, %s, is higher \"",
            "                \"than the highest value the user running salt is allowed to \"",
            "                \"set (%s). Defaulting to %s.\",",
            "                mof_c,",
            "                mof_h,",
            "                mof_h,",
            "            )",
            "            mof_c = mof_h",
            "",
            "        if mof_s < mof_c:",
            "            # There's room to raise the value. Raise it!",
            "            log.info(\"Raising max open files value to %s\", mof_c)",
            "            resource.setrlimit(resource.RLIMIT_NOFILE, (mof_c, mof_h))",
            "            try:",
            "                mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)",
            "                log.info(",
            "                    \"New values for max open files soft/hard values: %s/%s\",",
            "                    mof_s,",
            "                    mof_h,",
            "                )",
            "            except ValueError:",
            "                # https://github.com/saltstack/salt/issues/1991#issuecomment-13025595",
            "                # A user under macOS reported that our 100000 default value is",
            "                # still too high.",
            "                log.critical(",
            "                    \"Failed to raise max open files setting to %s. If this \"",
            "                    \"value is too low, the salt-master will most likely fail \"",
            "                    \"to run properly.\",",
            "                    mof_c,",
            "                )",
            "",
            "    def _pre_flight(self):",
            "        \"\"\"",
            "        Run pre flight checks. If anything in this method fails then the master",
            "        should not start up.",
            "        \"\"\"",
            "        errors = []",
            "        critical_errors = []",
            "",
            "        try:",
            "            os.chdir(\"/\")",
            "        except OSError as err:",
            "            errors.append(\"Cannot change to root directory ({})\".format(err))",
            "",
            "        if self.opts.get(\"fileserver_verify_config\", True):",
            "            # Avoid circular import",
            "            import salt.fileserver",
            "",
            "            fileserver = salt.fileserver.Fileserver(self.opts)",
            "            if not fileserver.servers:",
            "                errors.append(",
            "                    \"Failed to load fileserver backends, the configured backends \"",
            "                    \"are: {}\".format(\", \".join(self.opts[\"fileserver_backend\"]))",
            "                )",
            "            else:",
            "                # Run init() for all backends which support the function, to",
            "                # double-check configuration",
            "                try:",
            "                    fileserver.init()",
            "                except salt.exceptions.FileserverConfigError as exc:",
            "                    critical_errors.append(\"{}\".format(exc))",
            "",
            "        if not self.opts[\"fileserver_backend\"]:",
            "            errors.append(\"No fileserver backends are configured\")",
            "",
            "        # Check to see if we need to create a pillar cache dir",
            "        if self.opts[\"pillar_cache\"] and not os.path.isdir(",
            "            os.path.join(self.opts[\"cachedir\"], \"pillar_cache\")",
            "        ):",
            "            try:",
            "                with salt.utils.files.set_umask(0o077):",
            "                    os.mkdir(os.path.join(self.opts[\"cachedir\"], \"pillar_cache\"))",
            "            except OSError:",
            "                pass",
            "",
            "        if self.opts.get(\"git_pillar_verify_config\", True):",
            "            try:",
            "                git_pillars = [",
            "                    x",
            "                    for x in self.opts.get(\"ext_pillar\", [])",
            "                    if \"git\" in x and not isinstance(x[\"git\"], str)",
            "                ]",
            "            except TypeError:",
            "                git_pillars = []",
            "                critical_errors.append(",
            "                    \"Invalid ext_pillar configuration. It is likely that the \"",
            "                    \"external pillar type was not specified for one or more \"",
            "                    \"external pillars.\"",
            "                )",
            "            if git_pillars:",
            "                try:",
            "                    new_opts = copy.deepcopy(self.opts)",
            "                    import salt.pillar.git_pillar",
            "",
            "                    for repo in git_pillars:",
            "                        new_opts[\"ext_pillar\"] = [repo]",
            "                        try:",
            "                            git_pillar = salt.utils.gitfs.GitPillar(",
            "                                new_opts,",
            "                                repo[\"git\"],",
            "                                per_remote_overrides=salt.pillar.git_pillar.PER_REMOTE_OVERRIDES,",
            "                                per_remote_only=salt.pillar.git_pillar.PER_REMOTE_ONLY,",
            "                                global_only=salt.pillar.git_pillar.GLOBAL_ONLY,",
            "                            )",
            "                        except salt.exceptions.FileserverConfigError as exc:",
            "                            critical_errors.append(exc.strerror)",
            "                finally:",
            "                    del new_opts",
            "",
            "        if errors or critical_errors:",
            "            for error in errors:",
            "                log.error(error)",
            "            for error in critical_errors:",
            "                log.critical(error)",
            "            log.critical(\"Master failed pre flight checks, exiting\\n\")",
            "            sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "",
            "    def start(self):",
            "        \"\"\"",
            "        Turn on the master server components",
            "        \"\"\"",
            "        self._pre_flight()",
            "        log.info(\"salt-master is starting as user '%s'\", salt.utils.user.get_user())",
            "",
            "        enable_sigusr1_handler()",
            "        enable_sigusr2_handler()",
            "",
            "        self.__set_max_open_files()",
            "",
            "        # Reset signals to default ones before adding processes to the process",
            "        # manager. We don't want the processes being started to inherit those",
            "        # signal handlers",
            "        with salt.utils.process.default_signals(signal.SIGINT, signal.SIGTERM):",
            "",
            "            # Setup the secrets here because the PubServerChannel may need",
            "            # them as well.",
            "            SMaster.secrets[\"aes\"] = {",
            "                \"secret\": multiprocessing.Array(",
            "                    ctypes.c_char,",
            "                    salt.utils.stringutils.to_bytes(",
            "                        salt.crypt.Crypticle.generate_key_string()",
            "                    ),",
            "                ),",
            "                \"reload\": salt.crypt.Crypticle.generate_key_string,",
            "            }",
            "            log.info(\"Creating master process manager\")",
            "            # Since there are children having their own ProcessManager we should wait for kill more time.",
            "            self.process_manager = salt.utils.process.ProcessManager(wait_for_kill=5)",
            "            pub_channels = []",
            "            log.info(\"Creating master publisher process\")",
            "            log_queue = salt.log.setup.get_multiprocessing_logging_queue()",
            "            for _, opts in iter_transport_opts(self.opts):",
            "                chan = salt.transport.server.PubServerChannel.factory(opts)",
            "                chan.pre_fork(self.process_manager, kwargs={\"log_queue\": log_queue})",
            "                pub_channels.append(chan)",
            "",
            "            log.info(\"Creating master event publisher process\")",
            "            self.process_manager.add_process(",
            "                salt.utils.event.EventPublisher, args=(self.opts,)",
            "            )",
            "",
            "            if self.opts.get(\"reactor\"):",
            "                if isinstance(self.opts[\"engines\"], list):",
            "                    rine = False",
            "                    for item in self.opts[\"engines\"]:",
            "                        if \"reactor\" in item:",
            "                            rine = True",
            "                            break",
            "                    if not rine:",
            "                        self.opts[\"engines\"].append({\"reactor\": {}})",
            "                else:",
            "                    if \"reactor\" not in self.opts[\"engines\"]:",
            "                        log.info(\"Enabling the reactor engine\")",
            "                        self.opts[\"engines\"][\"reactor\"] = {}",
            "",
            "            salt.engines.start_engines(self.opts, self.process_manager)",
            "",
            "            # must be after channels",
            "            log.info(\"Creating master maintenance process\")",
            "            self.process_manager.add_process(Maintenance, args=(self.opts,))",
            "",
            "            if self.opts.get(\"event_return\"):",
            "                log.info(\"Creating master event return process\")",
            "                self.process_manager.add_process(",
            "                    salt.utils.event.EventReturn, args=(self.opts,)",
            "                )",
            "",
            "            ext_procs = self.opts.get(\"ext_processes\", [])",
            "            for proc in ext_procs:",
            "                log.info(\"Creating ext_processes process: %s\", proc)",
            "                try:",
            "                    mod = \".\".join(proc.split(\".\")[:-1])",
            "                    cls = proc.split(\".\")[-1]",
            "                    _tmp = __import__(mod, globals(), locals(), [cls], -1)",
            "                    cls = _tmp.__getattribute__(cls)",
            "                    self.process_manager.add_process(cls, args=(self.opts,))",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.error(\"Error creating ext_processes process: %s\", proc)",
            "",
            "            if HAS_HALITE and \"halite\" in self.opts:",
            "                log.info(\"Creating master halite process\")",
            "                self.process_manager.add_process(Halite, args=(self.opts[\"halite\"],))",
            "",
            "            # TODO: remove, or at least push into the transport stuff (pre-fork probably makes sense there)",
            "            if self.opts[\"con_cache\"]:",
            "                log.info(\"Creating master concache process\")",
            "                self.process_manager.add_process(",
            "                    salt.utils.master.ConnectedCache, args=(self.opts,)",
            "                )",
            "                # workaround for issue #16315, race condition",
            "                log.debug(\"Sleeping for two seconds to let concache rest\")",
            "                time.sleep(2)",
            "",
            "            log.info(\"Creating master request server process\")",
            "            kwargs = {}",
            "            if salt.utils.platform.is_windows():",
            "                kwargs[\"log_queue\"] = log_queue",
            "                kwargs[",
            "                    \"log_queue_level\"",
            "                ] = salt.log.setup.get_multiprocessing_logging_level()",
            "                kwargs[\"secrets\"] = SMaster.secrets",
            "",
            "            self.process_manager.add_process(",
            "                ReqServer,",
            "                args=(self.opts, self.key, self.master_key),",
            "                kwargs=kwargs,",
            "                name=\"ReqServer\",",
            "            )",
            "",
            "            self.process_manager.add_process(FileserverUpdate, args=(self.opts,))",
            "",
            "            # Fire up SSDP discovery publisher",
            "            if self.opts[\"discovery\"]:",
            "                if salt.utils.ssdp.SSDPDiscoveryServer.is_available():",
            "                    self.process_manager.add_process(",
            "                        salt.utils.ssdp.SSDPDiscoveryServer(",
            "                            port=self.opts[\"discovery\"][\"port\"],",
            "                            listen_ip=self.opts[\"interface\"],",
            "                            answer={",
            "                                \"mapping\": self.opts[\"discovery\"].get(\"mapping\", {})",
            "                            },",
            "                        ).run",
            "                    )",
            "                else:",
            "                    log.error(\"Unable to load SSDP: asynchronous IO is not available.\")",
            "                    if sys.version_info.major == 2:",
            "                        log.error(",
            "                            'You are using Python 2, please install \"trollius\" module to enable SSDP discovery.'",
            "                        )",
            "",
            "        # Install the SIGINT/SIGTERM handlers if not done so far",
            "        if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGINT, self._handle_signals)",
            "",
            "        if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGTERM, self._handle_signals)",
            "",
            "        self.process_manager.run()",
            "",
            "    def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument",
            "        # escalate the signals to the process manager",
            "        self.process_manager.stop_restarting()",
            "        self.process_manager.send_signal_to_processes(signum)",
            "        # kill any remaining processes",
            "        self.process_manager.kill_children()",
            "        time.sleep(1)",
            "        sys.exit(0)",
            "",
            "",
            "class Halite(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    Manage the Halite server",
            "    \"\"\"",
            "",
            "    def __init__(self, hopts, **kwargs):",
            "        \"\"\"",
            "        Create a halite instance",
            "",
            "        :param dict hopts: The halite options",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.hopts = hopts",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"hopts\"],",
            "            log_queue=state[\"log_queue\"],",
            "            log_queue_level=state[\"log_queue_level\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"hopts\": self.hopts,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Fire up halite!",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "        halite.start(self.hopts)",
            "",
            "",
            "class ReqServer(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    Starts up the master request server, minions send results to this",
            "    interface.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, key, mkey, secrets=None, **kwargs):",
            "        \"\"\"",
            "        Create a request server",
            "",
            "        :param dict opts: The salt options dictionary",
            "        :key dict: The user starting the server and the AES key",
            "        :mkey dict: The user starting the server and the RSA key",
            "",
            "        :rtype: ReqServer",
            "        :returns: Request server",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        self.master_key = mkey",
            "        # Prepare the AES key",
            "        self.key = key",
            "        self.secrets = secrets",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"opts\"],",
            "            state[\"key\"],",
            "            state[\"mkey\"],",
            "            secrets=state[\"secrets\"],",
            "            log_queue=state[\"log_queue\"],",
            "            log_queue_level=state[\"log_queue_level\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"key\": self.key,",
            "            \"mkey\": self.master_key,",
            "            \"secrets\": self.secrets,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument",
            "        self.destroy(signum)",
            "        super()._handle_signals(signum, sigframe)",
            "",
            "    def __bind(self):",
            "        \"\"\"",
            "        Binds the reply server",
            "        \"\"\"",
            "        if self.log_queue is not None:",
            "            salt.log.setup.set_multiprocessing_logging_queue(self.log_queue)",
            "        if self.log_queue_level is not None:",
            "            salt.log.setup.set_multiprocessing_logging_level(self.log_queue_level)",
            "        salt.log.setup.setup_multiprocessing_logging(self.log_queue)",
            "        if self.secrets is not None:",
            "            SMaster.secrets = self.secrets",
            "",
            "        dfn = os.path.join(self.opts[\"cachedir\"], \".dfn\")",
            "        if os.path.isfile(dfn):",
            "            try:",
            "                if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):",
            "                    # Cannot delete read-only files on Windows.",
            "                    os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)",
            "                os.remove(dfn)",
            "            except os.error:",
            "                pass",
            "",
            "        # Wait for kill should be less then parent's ProcessManager.",
            "        self.process_manager = salt.utils.process.ProcessManager(",
            "            name=\"ReqServer_ProcessManager\", wait_for_kill=1",
            "        )",
            "",
            "        req_channels = []",
            "        tcp_only = True",
            "        for transport, opts in iter_transport_opts(self.opts):",
            "            chan = salt.transport.server.ReqServerChannel.factory(opts)",
            "            chan.pre_fork(self.process_manager)",
            "            req_channels.append(chan)",
            "            if transport != \"tcp\":",
            "                tcp_only = False",
            "",
            "        kwargs = {}",
            "        if salt.utils.platform.is_windows():",
            "            kwargs[\"log_queue\"] = self.log_queue",
            "            kwargs[\"log_queue_level\"] = self.log_queue_level",
            "            # Use one worker thread if only the TCP transport is set up on",
            "            # Windows and we are using Python 2. There is load balancer",
            "            # support on Windows for the TCP transport when using Python 3.",
            "            if tcp_only and six.PY2 and int(self.opts[\"worker_threads\"]) != 1:",
            "                log.warning(",
            "                    \"TCP transport supports only 1 worker on Windows \"",
            "                    \"when using Python 2.\"",
            "                )",
            "                self.opts[\"worker_threads\"] = 1",
            "",
            "        if self.opts[\"req_server_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting ReqServer_ProcessManager niceness to %d\",",
            "                self.opts[\"req_server_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"req_server_niceness\"])",
            "",
            "        # Reset signals to default ones before adding processes to the process",
            "        # manager. We don't want the processes being started to inherit those",
            "        # signal handlers",
            "        with salt.utils.process.default_signals(signal.SIGINT, signal.SIGTERM):",
            "            for ind in range(int(self.opts[\"worker_threads\"])):",
            "                name = \"MWorker-{}\".format(ind)",
            "                self.process_manager.add_process(",
            "                    MWorker,",
            "                    args=(self.opts, self.master_key, self.key, req_channels, name),",
            "                    kwargs=kwargs,",
            "                    name=name,",
            "                )",
            "        self.process_manager.run()",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Start up the ReqServer",
            "        \"\"\"",
            "        self.__bind()",
            "",
            "    def destroy(self, signum=signal.SIGTERM):",
            "        if hasattr(self, \"process_manager\"):",
            "            self.process_manager.stop_restarting()",
            "            self.process_manager.send_signal_to_processes(signum)",
            "            self.process_manager.kill_children()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.destroy()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "class MWorker(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    The worker multiprocess instance to manage the backend operations for the",
            "    salt master.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, mkey, key, req_channels, name, **kwargs):",
            "        \"\"\"",
            "        Create a salt master worker process",
            "",
            "        :param dict opts: The salt options",
            "        :param dict mkey: The user running the salt master and the AES key",
            "        :param dict key: The user running the salt master and the RSA key",
            "",
            "        :rtype: MWorker",
            "        :return: Master worker",
            "        \"\"\"",
            "        kwargs[\"name\"] = name",
            "        self.name = name",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        self.req_channels = req_channels",
            "",
            "        self.mkey = mkey",
            "        self.key = key",
            "        self.k_mtime = 0",
            "        self.stats = collections.defaultdict(lambda: {\"mean\": 0, \"runs\": 0})",
            "        self.stat_clock = time.time()",
            "",
            "    # We need __setstate__ and __getstate__ to also pickle 'SMaster.secrets'.",
            "    # Otherwise, 'SMaster.secrets' won't be copied over to the spawned process",
            "    # on Windows since spawning processes on Windows requires pickling.",
            "    # These methods are only used when pickling so will not be used on",
            "    # non-Windows platforms.",
            "    def __setstate__(self, state):",
            "        super().__init__(",
            "            log_queue=state[\"log_queue\"], log_queue_level=state[\"log_queue_level\"]",
            "        )",
            "        self.opts = state[\"opts\"]",
            "        self.req_channels = state[\"req_channels\"]",
            "        self.mkey = state[\"mkey\"]",
            "        self.key = state[\"key\"]",
            "        self.k_mtime = state[\"k_mtime\"]",
            "        SMaster.secrets = state[\"secrets\"]",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"req_channels\": self.req_channels,",
            "            \"mkey\": self.mkey,",
            "            \"key\": self.key,",
            "            \"k_mtime\": self.k_mtime,",
            "            \"secrets\": SMaster.secrets,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def _handle_signals(self, signum, sigframe):",
            "        for channel in getattr(self, \"req_channels\", ()):",
            "            channel.close()",
            "        super()._handle_signals(signum, sigframe)",
            "",
            "    def __bind(self):",
            "        \"\"\"",
            "        Bind to the local port",
            "        \"\"\"",
            "        # using ZMQIOLoop since we *might* need zmq in there",
            "        install_zmq()",
            "        self.io_loop = ZMQDefaultLoop()",
            "        self.io_loop.make_current()",
            "        for req_channel in self.req_channels:",
            "            req_channel.post_fork(",
            "                self._handle_payload, io_loop=self.io_loop",
            "            )  # TODO: cleaner? Maybe lazily?",
            "        try:",
            "            self.io_loop.start()",
            "        except (KeyboardInterrupt, SystemExit):",
            "            # Tornado knows what to do",
            "            pass",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _handle_payload(self, payload):",
            "        \"\"\"",
            "        The _handle_payload method is the key method used to figure out what",
            "        needs to be done with communication to the server",
            "",
            "        Example cleartext payload generated for 'salt myminion test.ping':",
            "",
            "        {'enc': 'clear',",
            "         'load': {'arg': [],",
            "                  'cmd': 'publish',",
            "                  'fun': 'test.ping',",
            "                  'jid': '',",
            "                  'key': 'alsdkjfa.,maljf-==adflkjadflkjalkjadfadflkajdflkj',",
            "                  'kwargs': {'show_jid': False, 'show_timeout': False},",
            "                  'ret': '',",
            "                  'tgt': 'myminion',",
            "                  'tgt_type': 'glob',",
            "                  'user': 'root'}}",
            "",
            "        :param dict payload: The payload route to the appropriate handler",
            "        \"\"\"",
            "        key = payload[\"enc\"]",
            "        load = payload[\"load\"]",
            "        ret = {\"aes\": self._handle_aes, \"clear\": self._handle_clear}[key](load)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    def _post_stats(self, start, cmd):",
            "        \"\"\"",
            "        Calculate the master stats and fire events with stat info",
            "        \"\"\"",
            "        end = time.time()",
            "        duration = end - start",
            "        self.stats[cmd][\"mean\"] = (",
            "            self.stats[cmd][\"mean\"] * (self.stats[cmd][\"runs\"] - 1) + duration",
            "        ) / self.stats[cmd][\"runs\"]",
            "        if end - self.stat_clock > self.opts[\"master_stats_event_iter\"]:",
            "            # Fire the event with the stats and wipe the tracker",
            "            self.aes_funcs.event.fire_event(",
            "                {",
            "                    \"time\": end - self.stat_clock,",
            "                    \"worker\": self.name,",
            "                    \"stats\": self.stats,",
            "                },",
            "                tagify(self.name, \"stats\"),",
            "            )",
            "            self.stats = collections.defaultdict(lambda: {\"mean\": 0, \"runs\": 0})",
            "            self.stat_clock = end",
            "",
            "    def _handle_clear(self, load):",
            "        \"\"\"",
            "        Process a cleartext command",
            "",
            "        :param dict load: Cleartext payload",
            "        :return: The result of passing the load to a function in ClearFuncs corresponding to",
            "                 the command specified in the load's 'cmd' key.",
            "        \"\"\"",
            "        log.trace(\"Clear payload received with command %s\", load[\"cmd\"])",
            "        cmd = load[\"cmd\"]",
            "        method = self.clear_funcs.get_method(cmd)",
            "        if not method:",
            "            return {}, {\"fun\": \"send_clear\"}",
            "        if self.opts[\"master_stats\"]:",
            "            start = time.time()",
            "            self.stats[cmd][\"runs\"] += 1",
            "        ret = method(load), {\"fun\": \"send_clear\"}",
            "        if self.opts[\"master_stats\"]:",
            "            self._post_stats(start, cmd)",
            "        return ret",
            "",
            "    def _handle_aes(self, data):",
            "        \"\"\"",
            "        Process a command sent via an AES key",
            "",
            "        :param str load: Encrypted payload",
            "        :return: The result of passing the load to a function in AESFuncs corresponding to",
            "                 the command specified in the load's 'cmd' key.",
            "        \"\"\"",
            "        if \"cmd\" not in data:",
            "            log.error(\"Received malformed command %s\", data)",
            "            return {}",
            "        cmd = data[\"cmd\"]",
            "        log.trace(\"AES payload received with command %s\", data[\"cmd\"])",
            "        method = self.aes_funcs.get_method(cmd)",
            "        if not method:",
            "            return {}, {\"fun\": \"send\"}",
            "        if self.opts[\"master_stats\"]:",
            "            start = time.time()",
            "            self.stats[cmd][\"runs\"] += 1",
            "",
            "        def run_func(data):",
            "            return self.aes_funcs.run_func(data[\"cmd\"], data)",
            "",
            "        with StackContext(",
            "            functools.partial(RequestContext, {\"data\": data, \"opts\": self.opts})",
            "        ):",
            "            ret = run_func(data)",
            "",
            "        if self.opts[\"master_stats\"]:",
            "            self._post_stats(start, cmd)",
            "        return ret",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Start a Master Worker",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.name)",
            "",
            "        # if we inherit req_server level without our own, reset it",
            "        if not salt.utils.platform.is_windows():",
            "            enforce_mworker_niceness = True",
            "            if self.opts[\"req_server_niceness\"]:",
            "                if salt.utils.user.get_user() == \"root\":",
            "                    log.info(",
            "                        \"%s decrementing inherited ReqServer niceness to 0\", self.name",
            "                    )",
            "                    log.info(os.nice())",
            "                    os.nice(-1 * self.opts[\"req_server_niceness\"])",
            "                else:",
            "                    log.error(",
            "                        \"%s unable to decrement niceness for MWorker, not running as root\",",
            "                        self.name,",
            "                    )",
            "                    enforce_mworker_niceness = False",
            "",
            "            # else set what we're explicitly asked for",
            "            if enforce_mworker_niceness and self.opts[\"mworker_niceness\"]:",
            "                log.info(",
            "                    \"setting %s niceness to %i\",",
            "                    self.name,",
            "                    self.opts[\"mworker_niceness\"],",
            "                )",
            "                os.nice(self.opts[\"mworker_niceness\"])",
            "",
            "        self.clear_funcs = ClearFuncs(self.opts, self.key,)",
            "        self.aes_funcs = AESFuncs(self.opts)",
            "        salt.utils.crypt.reinit_crypto()",
            "        self.__bind()",
            "",
            "",
            "class TransportMethods:",
            "    \"\"\"",
            "    Expose methods to the transport layer, methods with their names found in",
            "    the class attribute 'expose_methods' will be exposed to the transport layer",
            "    via 'get_method'.",
            "    \"\"\"",
            "",
            "    expose_methods = ()",
            "",
            "    def get_method(self, name):",
            "        \"\"\"",
            "        Get a method which should be exposed to the transport layer",
            "        \"\"\"",
            "        if name in self.expose_methods:",
            "            try:",
            "                return getattr(self, name)",
            "            except AttributeError:",
            "                log.error(\"Requested method not exposed: %s\", name)",
            "        else:",
            "            log.error(\"Requested method not exposed: %s\", name)",
            "",
            "",
            "# TODO: rename? No longer tied to \"AES\", just \"encrypted\" or \"private\" requests",
            "class AESFuncs(TransportMethods):",
            "    \"\"\"",
            "    Set up functions that are available when the load is encrypted with AES",
            "    \"\"\"",
            "",
            "    expose_methods = (",
            "        \"verify_minion\",",
            "        \"_master_tops\",",
            "        \"_ext_nodes\",",
            "        \"_master_opts\",",
            "        \"_mine_get\",",
            "        \"_mine\",",
            "        \"_mine_delete\",",
            "        \"_mine_flush\",",
            "        \"_file_recv\",",
            "        \"_pillar\",",
            "        \"_minion_event\",",
            "        \"_handle_minion_event\",",
            "        \"_return\",",
            "        \"_syndic_return\",",
            "        \"minion_runner\",",
            "        \"pub_ret\",",
            "        \"minion_pub\",",
            "        \"minion_publish\",",
            "        \"revoke_auth\",",
            "        \"_serve_file\",",
            "        \"_file_find\",",
            "        \"_file_hash\",",
            "        \"_file_hash_and_stat\",",
            "        \"_file_list\",",
            "        \"_file_list_emptydirs\",",
            "        \"_dir_list\",",
            "        \"_symlink_list\",",
            "        \"_file_envs\",",
            "    )",
            "",
            "    def __init__(self, opts):",
            "        \"\"\"",
            "        Create a new AESFuncs",
            "",
            "        :param dict opts: The salt options",
            "",
            "        :rtype: AESFuncs",
            "        :returns: Instance for handling AES operations",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        self.serial = salt.payload.Serial(opts)",
            "        self.ckminions = salt.utils.minions.CkMinions(opts)",
            "        # Make a client",
            "        self.local = salt.client.get_local_client(self.opts[\"conf_file\"])",
            "        # Create the master minion to access the external job cache",
            "        self.mminion = salt.minion.MasterMinion(",
            "            self.opts, states=False, rend=False, ignore_config_errors=True",
            "        )",
            "        self.__setup_fileserver()",
            "        self.masterapi = salt.daemons.masterapi.RemoteFuncs(opts)",
            "",
            "    def __setup_fileserver(self):",
            "        \"\"\"",
            "        Set the local file objects from the file server interface",
            "        \"\"\"",
            "        # Avoid circular import",
            "        import salt.fileserver",
            "",
            "        self.fs_ = salt.fileserver.Fileserver(self.opts)",
            "        self._serve_file = self.fs_.serve_file",
            "        self._file_find = self.fs_._find_file",
            "        self._file_hash = self.fs_.file_hash",
            "        self._file_hash_and_stat = self.fs_.file_hash_and_stat",
            "        self._file_list = self.fs_.file_list",
            "        self._file_list_emptydirs = self.fs_.file_list_emptydirs",
            "        self._dir_list = self.fs_.dir_list",
            "        self._symlink_list = self.fs_.symlink_list",
            "        self._file_envs = self.fs_.file_envs",
            "",
            "    def __verify_minion(self, id_, token):",
            "        \"\"\"",
            "        Take a minion id and a string signed with the minion private key",
            "        The string needs to verify as 'salt' with the minion public key",
            "",
            "        :param str id_: A minion ID",
            "        :param str token: A string signed with the minion private key",
            "",
            "        :rtype: bool",
            "        :return: Boolean indicating whether or not the token can be verified.",
            "        \"\"\"",
            "        if not salt.utils.verify.valid_id(self.opts, id_):",
            "            return False",
            "        pub_path = os.path.join(self.opts[\"pki_dir\"], \"minions\", id_)",
            "",
            "        try:",
            "            pub = salt.crypt.get_rsa_pub_key(pub_path)",
            "        except OSError:",
            "            log.warning(",
            "                \"Salt minion claiming to be %s attempted to communicate with \"",
            "                \"master, but key could not be read and verification was denied.\",",
            "                id_,",
            "            )",
            "            return False",
            "        except (ValueError, IndexError, TypeError) as err:",
            "            log.error('Unable to load public key \"%s\": %s', pub_path, err)",
            "        try:",
            "            if salt.crypt.public_decrypt(pub, token) == b\"salt\":",
            "                return True",
            "        except ValueError as err:",
            "            log.error(\"Unable to decrypt token: %s\", err)",
            "",
            "        log.error(",
            "            \"Salt minion claiming to be %s has attempted to communicate with \"",
            "            \"the master and could not be verified\",",
            "            id_,",
            "        )",
            "        return False",
            "",
            "    def verify_minion(self, id_, token):",
            "        \"\"\"",
            "        Take a minion id and a string signed with the minion private key",
            "        The string needs to verify as 'salt' with the minion public key",
            "",
            "        :param str id_: A minion ID",
            "        :param str token: A string signed with the minion private key",
            "",
            "        :rtype: bool",
            "        :return: Boolean indicating whether or not the token can be verified.",
            "        \"\"\"",
            "        return self.__verify_minion(id_, token)",
            "",
            "    def __verify_minion_publish(self, clear_load):",
            "        \"\"\"",
            "        Verify that the passed information authorized a minion to execute",
            "",
            "        :param dict clear_load: A publication load from a minion",
            "",
            "        :rtype: bool",
            "        :return: A boolean indicating if the minion is allowed to publish the command in the load",
            "        \"\"\"",
            "        # Verify that the load is valid",
            "        if \"peer\" not in self.opts:",
            "            return False",
            "        if not isinstance(self.opts[\"peer\"], dict):",
            "            return False",
            "        if any(",
            "            key not in clear_load for key in (\"fun\", \"arg\", \"tgt\", \"ret\", \"tok\", \"id\")",
            "        ):",
            "            return False",
            "        # If the command will make a recursive publish don't run",
            "        if clear_load[\"fun\"].startswith(\"publish.\"):",
            "            return False",
            "        # Check the permissions for this minion",
            "        if not self.__verify_minion(clear_load[\"id\"], clear_load[\"tok\"]):",
            "            # The minion is not who it says it is!",
            "            # We don't want to listen to it!",
            "            log.warning(",
            "                \"Minion id %s is not who it says it is and is attempting \"",
            "                \"to issue a peer command\",",
            "                clear_load[\"id\"],",
            "            )",
            "            return False",
            "        clear_load.pop(\"tok\")",
            "        perms = []",
            "        for match in self.opts[\"peer\"]:",
            "            if re.match(match, clear_load[\"id\"]):",
            "                # This is the list of funcs/modules!",
            "                if isinstance(self.opts[\"peer\"][match], list):",
            "                    perms.extend(self.opts[\"peer\"][match])",
            "        if \",\" in clear_load[\"fun\"]:",
            "            # 'arg': [['cat', '/proc/cpuinfo'], [], ['foo']]",
            "            clear_load[\"fun\"] = clear_load[\"fun\"].split(\",\")",
            "            arg_ = []",
            "            for arg in clear_load[\"arg\"]:",
            "                arg_.append(arg.split())",
            "            clear_load[\"arg\"] = arg_",
            "",
            "        # finally, check the auth of the load",
            "        return self.ckminions.auth_check(",
            "            perms,",
            "            clear_load[\"fun\"],",
            "            clear_load[\"arg\"],",
            "            clear_load[\"tgt\"],",
            "            clear_load.get(\"tgt_type\", \"glob\"),",
            "            publish_validate=True,",
            "        )",
            "",
            "    def __verify_load(self, load, verify_keys):",
            "        \"\"\"",
            "        A utility function to perform common verification steps.",
            "",
            "        :param dict load: A payload received from a minion",
            "        :param list verify_keys: A list of strings that should be present in a",
            "        given load",
            "",
            "        :rtype: bool",
            "        :rtype: dict",
            "        :return: The original load (except for the token) if the load can be",
            "        verified. False if the load is invalid.",
            "        \"\"\"",
            "        if any(key not in load for key in verify_keys):",
            "            return False",
            "        if \"tok\" not in load:",
            "            log.error(",
            "                \"Received incomplete call from %s for '%s', missing '%s'\",",
            "                load[\"id\"],",
            "                inspect_stack()[\"co_name\"],",
            "                \"tok\",",
            "            )",
            "            return False",
            "        if not self.__verify_minion(load[\"id\"], load[\"tok\"]):",
            "            # The minion is not who it says it is!",
            "            # We don't want to listen to it!",
            "            log.warning(\"Minion id %s is not who it says it is!\", load[\"id\"])",
            "            return False",
            "",
            "        if \"tok\" in load:",
            "            load.pop(\"tok\")",
            "",
            "        return load",
            "",
            "    def _master_tops(self, load):",
            "        \"\"\"",
            "        Return the results from an external node classifier if one is",
            "        specified",
            "",
            "        :param dict load: A payload received from a minion",
            "        :return: The results from an external node classifier",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        return self.masterapi._master_tops(load, skip_verify=True)",
            "",
            "    # Needed so older minions can request master_tops",
            "    _ext_nodes = _master_tops",
            "",
            "    def _master_opts(self, load):",
            "        \"\"\"",
            "        Return the master options to the minion",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: dict",
            "        :return: The master options",
            "        \"\"\"",
            "        mopts = {}",
            "        file_roots = {}",
            "        envs = self._file_envs()",
            "        for saltenv in envs:",
            "            if saltenv not in file_roots:",
            "                file_roots[saltenv] = []",
            "        mopts[\"file_roots\"] = file_roots",
            "        mopts[\"top_file_merging_strategy\"] = self.opts[\"top_file_merging_strategy\"]",
            "        mopts[\"env_order\"] = self.opts[\"env_order\"]",
            "        mopts[\"default_top\"] = self.opts[\"default_top\"]",
            "        if load.get(\"env_only\"):",
            "            return mopts",
            "        mopts[\"renderer\"] = self.opts[\"renderer\"]",
            "        mopts[\"failhard\"] = self.opts[\"failhard\"]",
            "        mopts[\"state_top\"] = self.opts[\"state_top\"]",
            "        mopts[\"state_top_saltenv\"] = self.opts[\"state_top_saltenv\"]",
            "        mopts[\"nodegroups\"] = self.opts[\"nodegroups\"]",
            "        mopts[\"state_auto_order\"] = self.opts[\"state_auto_order\"]",
            "        mopts[\"state_events\"] = self.opts[\"state_events\"]",
            "        mopts[\"state_aggregate\"] = self.opts[\"state_aggregate\"]",
            "        mopts[\"jinja_env\"] = self.opts[\"jinja_env\"]",
            "        mopts[\"jinja_sls_env\"] = self.opts[\"jinja_sls_env\"]",
            "        mopts[\"jinja_lstrip_blocks\"] = self.opts[\"jinja_lstrip_blocks\"]",
            "        mopts[\"jinja_trim_blocks\"] = self.opts[\"jinja_trim_blocks\"]",
            "        return mopts",
            "",
            "    def _mine_get(self, load):",
            "        \"\"\"",
            "        Gathers the data from the specified minions' mine",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: dict",
            "        :return: Mine data from the specified minions",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tgt\", \"fun\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi._mine_get(load, skip_verify=True)",
            "",
            "    def _mine(self, load):",
            "        \"\"\"",
            "        Store the mine data",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: bool",
            "        :return: True if the data has been stored in the mine",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"data\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        return self.masterapi._mine(load, skip_verify=True)",
            "",
            "    def _mine_delete(self, load):",
            "        \"\"\"",
            "        Allow the minion to delete a specific function from its own mine",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: bool",
            "        :return: Boolean indicating whether or not the given function was deleted from the mine",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"fun\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi._mine_delete(load)",
            "",
            "    def _mine_flush(self, load):",
            "        \"\"\"",
            "        Allow the minion to delete all of its own mine contents",
            "",
            "        :param dict load: A payload received from a minion",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi._mine_flush(load, skip_verify=True)",
            "",
            "    def _file_recv(self, load):",
            "        \"\"\"",
            "        Allows minions to send files to the master, files are sent to the",
            "        master file cache",
            "        \"\"\"",
            "        if any(key not in load for key in (\"id\", \"path\", \"loc\")):",
            "            return False",
            "        if not isinstance(load[\"path\"], list):",
            "            return False",
            "        if not self.opts[\"file_recv\"]:",
            "            return False",
            "        if not salt.utils.verify.valid_id(self.opts, load[\"id\"]):",
            "            return False",
            "        file_recv_max_size = 1024 * 1024 * self.opts[\"file_recv_max_size\"]",
            "",
            "        if \"loc\" in load and load[\"loc\"] < 0:",
            "            log.error(\"Invalid file pointer: load[loc] < 0\")",
            "            return False",
            "",
            "        if len(load[\"data\"]) + load.get(\"loc\", 0) > file_recv_max_size:",
            "            log.error(",
            "                \"file_recv_max_size limit of %d MB exceeded! %s will be \"",
            "                \"truncated. To successfully push this file, adjust \"",
            "                \"file_recv_max_size to an integer (in MB) large enough to \"",
            "                \"accommodate it.\",",
            "                file_recv_max_size,",
            "                load[\"path\"],",
            "            )",
            "            return False",
            "        if \"tok\" not in load:",
            "            log.error(",
            "                \"Received incomplete call from %s for '%s', missing '%s'\",",
            "                load[\"id\"],",
            "                inspect_stack()[\"co_name\"],",
            "                \"tok\",",
            "            )",
            "            return False",
            "        if not self.__verify_minion(load[\"id\"], load[\"tok\"]):",
            "            # The minion is not who it says it is!",
            "            # We don't want to listen to it!",
            "            log.warning(\"Minion id %s is not who it says it is!\", load[\"id\"])",
            "            return {}",
            "        load.pop(\"tok\")",
            "",
            "        # Join path",
            "        sep_path = os.sep.join(load[\"path\"])",
            "",
            "        # Path normalization should have been done by the sending",
            "        # minion but we can't guarantee it. Re-do it here.",
            "        normpath = os.path.normpath(sep_path)",
            "",
            "        # Ensure that this safety check is done after the path",
            "        # have been normalized.",
            "        if os.path.isabs(normpath) or \"../\" in load[\"path\"]:",
            "            # Can overwrite master files!!",
            "            return False",
            "",
            "        cpath = os.path.join(",
            "            self.opts[\"cachedir\"], \"minions\", load[\"id\"], \"files\", normpath",
            "        )",
            "        # One last safety check here",
            "        if not os.path.normpath(cpath).startswith(self.opts[\"cachedir\"]):",
            "            log.warning(",
            "                \"Attempt to write received file outside of master cache \"",
            "                \"directory! Requested path: %s. Access denied.\",",
            "                cpath,",
            "            )",
            "            return False",
            "        cdir = os.path.dirname(cpath)",
            "        if not os.path.isdir(cdir):",
            "            try:",
            "                os.makedirs(cdir)",
            "            except os.error:",
            "                pass",
            "        if os.path.isfile(cpath) and load[\"loc\"] != 0:",
            "            mode = \"ab\"",
            "        else:",
            "            mode = \"wb\"",
            "        with salt.utils.files.fopen(cpath, mode) as fp_:",
            "            if load[\"loc\"]:",
            "                fp_.seek(load[\"loc\"])",
            "",
            "            fp_.write(salt.utils.stringutils.to_bytes(load[\"data\"]))",
            "        return True",
            "",
            "    def _pillar(self, load):",
            "        \"\"\"",
            "        Return the pillar data for the minion",
            "",
            "        :param dict load: Minion payload",
            "",
            "        :rtype: dict",
            "        :return: The pillar data for the minion",
            "        \"\"\"",
            "        if any(key not in load for key in (\"id\", \"grains\")):",
            "            return False",
            "        if not salt.utils.verify.valid_id(self.opts, load[\"id\"]):",
            "            return False",
            "        load[\"grains\"][\"id\"] = load[\"id\"]",
            "",
            "        pillar = salt.pillar.get_pillar(",
            "            self.opts,",
            "            load[\"grains\"],",
            "            load[\"id\"],",
            "            load.get(\"saltenv\", load.get(\"env\")),",
            "            ext=load.get(\"ext\"),",
            "            pillar_override=load.get(\"pillar_override\", {}),",
            "            pillarenv=load.get(\"pillarenv\"),",
            "            extra_minion_data=load.get(\"extra_minion_data\"),",
            "        )",
            "        data = pillar.compile_pillar()",
            "        self.fs_.update_opts()",
            "        if self.opts.get(\"minion_data_cache\", False):",
            "            self.masterapi.cache.store(",
            "                \"minions/{}\".format(load[\"id\"]),",
            "                \"data\",",
            "                {\"grains\": load[\"grains\"], \"pillar\": data},",
            "            )",
            "            if self.opts.get(\"minion_data_cache_events\") is True:",
            "                self.event.fire_event(",
            "                    {\"Minion data cache refresh\": load[\"id\"]},",
            "                    tagify(load[\"id\"], \"refresh\", \"minion\"),",
            "                )",
            "        return data",
            "",
            "    def _minion_event(self, load):",
            "        \"\"\"",
            "        Receive an event from the minion and fire it on the master event",
            "        interface",
            "",
            "        :param dict load: The minion payload",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        # Route to master event bus",
            "        self.masterapi._minion_event(load)",
            "        # Process locally",
            "        self._handle_minion_event(load)",
            "",
            "    def _handle_minion_event(self, load):",
            "        \"\"\"",
            "        Act on specific events from minions",
            "        \"\"\"",
            "        id_ = load[\"id\"]",
            "        if load.get(\"tag\", \"\") == \"_salt_error\":",
            "            log.error(",
            "                \"Received minion error from [%s]: %s\", id_, load[\"data\"][\"message\"]",
            "            )",
            "",
            "        for event in load.get(\"events\", []):",
            "            event_data = event.get(\"data\", {})",
            "            if \"minions\" in event_data:",
            "                jid = event_data.get(\"jid\")",
            "                if not jid:",
            "                    continue",
            "                minions = event_data[\"minions\"]",
            "                try:",
            "                    salt.utils.job.store_minions(",
            "                        self.opts, jid, minions, mminion=self.mminion, syndic_id=id_",
            "                    )",
            "                except (KeyError, salt.exceptions.SaltCacheError) as exc:",
            "                    log.error(",
            "                        \"Could not add minion(s) %s for job %s: %s\", minions, jid, exc",
            "                    )",
            "",
            "    def _return(self, load):",
            "        \"\"\"",
            "        Handle the return data sent from the minions.",
            "",
            "        Takes the return, verifies it and fires it on the master event bus.",
            "        Typically, this event is consumed by the Salt CLI waiting on the other",
            "        end of the event bus but could be heard by any listener on the bus.",
            "",
            "        :param dict load: The minion payload",
            "        \"\"\"",
            "        if self.opts[\"require_minion_sign_messages\"] and \"sig\" not in load:",
            "            log.critical(",
            "                \"_return: Master is requiring minions to sign their \"",
            "                \"messages, but there is no signature in this payload from \"",
            "                \"%s.\",",
            "                load[\"id\"],",
            "            )",
            "            return False",
            "",
            "        if \"sig\" in load:",
            "            log.trace(\"Verifying signed event publish from minion\")",
            "            sig = load.pop(\"sig\")",
            "            this_minion_pubkey = os.path.join(",
            "                self.opts[\"pki_dir\"], \"minions/{}\".format(load[\"id\"])",
            "            )",
            "            serialized_load = salt.serializers.msgpack.serialize(load)",
            "            if not salt.crypt.verify_signature(",
            "                this_minion_pubkey, serialized_load, sig",
            "            ):",
            "                log.info(\"Failed to verify event signature from minion %s.\", load[\"id\"])",
            "                if self.opts[\"drop_messages_signature_fail\"]:",
            "                    log.critical(",
            "                        \"drop_messages_signature_fail is enabled, dropping \"",
            "                        \"message from %s\",",
            "                        load[\"id\"],",
            "                    )",
            "                    return False",
            "                else:",
            "                    log.info(",
            "                        \"But 'drop_message_signature_fail' is disabled, so message is still accepted.\"",
            "                    )",
            "            load[\"sig\"] = sig",
            "",
            "        try:",
            "            salt.utils.job.store_job(",
            "                self.opts, load, event=self.event, mminion=self.mminion",
            "            )",
            "        except salt.exceptions.SaltCacheError:",
            "            log.error(\"Could not store job information for load: %s\", load)",
            "",
            "    def _syndic_return(self, load):",
            "        \"\"\"",
            "        Receive a syndic minion return and format it to look like returns from",
            "        individual minions.",
            "",
            "        :param dict load: The minion payload",
            "        \"\"\"",
            "        loads = load.get(\"load\")",
            "        if not isinstance(loads, list):",
            "            loads = [load]  # support old syndics not aggregating returns",
            "        for load in loads:",
            "            # Verify the load",
            "            if any(key not in load for key in (\"return\", \"jid\", \"id\")):",
            "                continue",
            "            # if we have a load, save it",
            "            if load.get(\"load\"):",
            "                fstr = \"{}.save_load\".format(self.opts[\"master_job_cache\"])",
            "                self.mminion.returners[fstr](load[\"jid\"], load[\"load\"])",
            "",
            "            # Register the syndic",
            "            syndic_cache_path = os.path.join(",
            "                self.opts[\"cachedir\"], \"syndics\", load[\"id\"]",
            "            )",
            "            if not os.path.exists(syndic_cache_path):",
            "                path_name = os.path.split(syndic_cache_path)[0]",
            "                if not os.path.exists(path_name):",
            "                    os.makedirs(path_name)",
            "                with salt.utils.files.fopen(syndic_cache_path, \"w\") as wfh:",
            "                    wfh.write(\"\")",
            "",
            "            # Format individual return loads",
            "            for key, item in load[\"return\"].items():",
            "                ret = {\"jid\": load[\"jid\"], \"id\": key}",
            "                ret.update(item)",
            "                if \"master_id\" in load:",
            "                    ret[\"master_id\"] = load[\"master_id\"]",
            "                if \"fun\" in load:",
            "                    ret[\"fun\"] = load[\"fun\"]",
            "                if \"arg\" in load:",
            "                    ret[\"fun_args\"] = load[\"arg\"]",
            "                if \"out\" in load:",
            "                    ret[\"out\"] = load[\"out\"]",
            "                if \"sig\" in load:",
            "                    ret[\"sig\"] = load[\"sig\"]",
            "                self._return(ret)",
            "",
            "    def minion_runner(self, clear_load):",
            "        \"\"\"",
            "        Execute a runner from a minion, return the runner's function data",
            "",
            "        :param dict clear_load: The minion payload",
            "",
            "        :rtype: dict",
            "        :return: The runner function data",
            "        \"\"\"",
            "        load = self.__verify_load(clear_load, (\"fun\", \"arg\", \"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi.minion_runner(clear_load)",
            "",
            "    def pub_ret(self, load):",
            "        \"\"\"",
            "        Request the return data from a specific jid, only allowed",
            "        if the requesting minion also initialted the execution.",
            "",
            "        :param dict load: The minion payload",
            "",
            "        :rtype: dict",
            "        :return: Return data corresponding to a given JID",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"jid\", \"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        # Check that this minion can access this data",
            "        auth_cache = os.path.join(self.opts[\"cachedir\"], \"publish_auth\")",
            "        if not os.path.isdir(auth_cache):",
            "            os.makedirs(auth_cache)",
            "        jid_fn = os.path.join(auth_cache, str(load[\"jid\"]))",
            "        with salt.utils.files.fopen(jid_fn, \"r\") as fp_:",
            "            if not load[\"id\"] == fp_.read():",
            "                return {}",
            "        # Grab the latest and return",
            "        return self.local.get_cache_returns(load[\"jid\"])",
            "",
            "    def minion_pub(self, clear_load):",
            "        \"\"\"",
            "        Publish a command initiated from a minion, this method executes minion",
            "        restrictions so that the minion publication will only work if it is",
            "        enabled in the config.",
            "",
            "        The configuration on the master allows minions to be matched to",
            "        salt functions, so the minions can only publish allowed salt functions",
            "",
            "        The config will look like this:",
            "",
            "        .. code-block:: bash",
            "",
            "            peer:",
            "                .*:",
            "                    - .*",
            "",
            "        This configuration will enable all minions to execute all commands:",
            "",
            "        .. code-block:: bash",
            "",
            "            peer:",
            "                foo.example.com:",
            "                    - test.*",
            "",
            "        The above configuration will only allow the minion foo.example.com to",
            "        execute commands from the test module.",
            "",
            "        :param dict clear_load: The minion pay",
            "        \"\"\"",
            "        if not self.__verify_minion_publish(clear_load):",
            "            return {}",
            "        else:",
            "            return self.masterapi.minion_pub(clear_load)",
            "",
            "    def minion_publish(self, clear_load):",
            "        \"\"\"",
            "        Publish a command initiated from a minion, this method executes minion",
            "        restrictions so that the minion publication will only work if it is",
            "        enabled in the config.",
            "",
            "        The configuration on the master allows minions to be matched to",
            "        salt functions, so the minions can only publish allowed salt functions",
            "",
            "        The config will look like this:",
            "",
            "        .. code-block:: bash",
            "",
            "            peer:",
            "                .*:",
            "                    - .*",
            "",
            "        This configuration will enable all minions to execute all commands.",
            "        peer:",
            "",
            "        .. code-block:: bash",
            "",
            "            foo.example.com:",
            "                - test.*",
            "",
            "        The above configuration will only allow the minion foo.example.com to",
            "        execute commands from the test module.",
            "",
            "        :param dict clear_load: The minion payload",
            "        \"\"\"",
            "        if not self.__verify_minion_publish(clear_load):",
            "            return {}",
            "        else:",
            "            return self.masterapi.minion_publish(clear_load)",
            "",
            "    def revoke_auth(self, load):",
            "        \"\"\"",
            "        Allow a minion to request revocation of its own key",
            "",
            "        :param dict load: The minion payload",
            "",
            "        :rtype: dict",
            "        :return: If the load is invalid, it may be returned. No key operation is performed.",
            "",
            "        :rtype: bool",
            "        :return: True if key was revoked, False if not",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "",
            "        if not self.opts.get(\"allow_minion_key_revoke\", False):",
            "            log.warning(",
            "                \"Minion %s requested key revoke, but allow_minion_key_revoke \"",
            "                \"is set to False\",",
            "                load[\"id\"],",
            "            )",
            "            return load",
            "",
            "        if load is False:",
            "            return load",
            "        else:",
            "            return self.masterapi.revoke_auth(load)",
            "",
            "    def run_func(self, func, load):",
            "        \"\"\"",
            "        Wrapper for running functions executed with AES encryption",
            "",
            "        :param function func: The function to run",
            "        :return: The result of the master function that was called",
            "        \"\"\"",
            "        # Don't honor private functions",
            "        if func.startswith(\"__\"):",
            "            # TODO: return some error? Seems odd to return {}",
            "            return {}, {\"fun\": \"send\"}",
            "        # Run the func",
            "        if hasattr(self, func):",
            "            try:",
            "                start = time.time()",
            "                ret = getattr(self, func)(load)",
            "                log.trace(",
            "                    \"Master function call %s took %s seconds\", func, time.time() - start",
            "                )",
            "            except Exception:  # pylint: disable=broad-except",
            "                ret = \"\"",
            "                log.error(\"Error in function %s:\\n\", func, exc_info=True)",
            "        else:",
            "            log.error(",
            "                \"Received function %s which is unavailable on the master, \"",
            "                \"returning False\",",
            "                func,",
            "            )",
            "            return False, {\"fun\": \"send\"}",
            "        # Don't encrypt the return value for the _return func",
            "        # (we don't care about the return value, so why encrypt it?)",
            "        if func == \"_return\":",
            "            return ret, {\"fun\": \"send\"}",
            "        if func == \"_pillar\" and \"id\" in load:",
            "            if load.get(\"ver\") != \"2\" and self.opts[\"pillar_version\"] == 1:",
            "                # Authorized to return old pillar proto",
            "                return ret, {\"fun\": \"send\"}",
            "            return ret, {\"fun\": \"send_private\", \"key\": \"pillar\", \"tgt\": load[\"id\"]}",
            "        # Encrypt the return",
            "        return ret, {\"fun\": \"send\"}",
            "",
            "",
            "class ClearFuncs(TransportMethods):",
            "    \"\"\"",
            "    Set up functions that are safe to execute when commands sent to the master",
            "    without encryption and authentication",
            "    \"\"\"",
            "",
            "    # These methods will be exposed to the transport layer by",
            "    # MWorker._handle_clear",
            "    expose_methods = (",
            "        \"ping\",",
            "        \"publish\",",
            "        \"get_token\",",
            "        \"mk_token\",",
            "        \"wheel\",",
            "        \"runner\",",
            "    )",
            "",
            "    # The ClearFuncs object encapsulates the functions that can be executed in",
            "    # the clear:",
            "    # publish (The publish from the LocalClient)",
            "    # _auth",
            "    def __init__(self, opts, key):",
            "        self.opts = opts",
            "        self.key = key",
            "        # Create the event manager",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        # Make a client",
            "        self.local = salt.client.get_local_client(self.opts[\"conf_file\"])",
            "        # Make an minion checker object",
            "        self.ckminions = salt.utils.minions.CkMinions(opts)",
            "        # Make an Auth object",
            "        self.loadauth = salt.auth.LoadAuth(opts)",
            "        # Stand up the master Minion to access returner data",
            "        self.mminion = salt.minion.MasterMinion(",
            "            self.opts, states=False, rend=False, ignore_config_errors=True",
            "        )",
            "        # Make a wheel object",
            "        self.wheel_ = salt.wheel.Wheel(opts)",
            "        # Make a masterapi object",
            "        self.masterapi = salt.daemons.masterapi.LocalFuncs(opts, key)",
            "",
            "    def runner(self, clear_load):",
            "        \"\"\"",
            "        Send a master control function back to the runner system",
            "        \"\"\"",
            "        # All runner ops pass through eauth",
            "        auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)",
            "",
            "        # Authenticate",
            "        auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)",
            "        error = auth_check.get(\"error\")",
            "",
            "        if error:",
            "            # Authentication error occurred: do not continue.",
            "            return {\"error\": error}",
            "",
            "        # Authorize",
            "        username = auth_check.get(\"username\")",
            "        if auth_type != \"user\":",
            "            runner_check = self.ckminions.runner_check(",
            "                auth_check.get(\"auth_list\", []),",
            "                clear_load[\"fun\"],",
            "                clear_load.get(\"kwarg\", {}),",
            "            )",
            "            if not runner_check:",
            "                return {",
            "                    \"error\": {",
            "                        \"name\": err_name,",
            "                        \"message\": 'Authentication failure of type \"{}\" occurred for '",
            "                        \"user {}.\".format(auth_type, username),",
            "                    }",
            "                }",
            "            elif isinstance(runner_check, dict) and \"error\" in runner_check:",
            "                # A dictionary with an error name/message was handled by ckminions.runner_check",
            "                return runner_check",
            "",
            "            # No error occurred, consume sensitive settings from the clear_load if passed.",
            "            for item in sensitive_load_keys:",
            "                clear_load.pop(item, None)",
            "        else:",
            "            if \"user\" in clear_load:",
            "                username = clear_load[\"user\"]",
            "                if salt.auth.AuthUser(username).is_sudo():",
            "                    username = self.opts.get(\"user\", \"root\")",
            "            else:",
            "                username = salt.utils.user.get_user()",
            "",
            "        # Authorized. Do the job!",
            "        try:",
            "            fun = clear_load.pop(\"fun\")",
            "            runner_client = salt.runner.RunnerClient(self.opts)",
            "            return runner_client.asynchronous(",
            "                fun, clear_load.get(\"kwarg\", {}), username, local=True",
            "            )",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception occurred while introspecting %s: %s\", fun, exc)",
            "            return {",
            "                \"error\": {",
            "                    \"name\": exc.__class__.__name__,",
            "                    \"args\": exc.args,",
            "                    \"message\": str(exc),",
            "                }",
            "            }",
            "",
            "    def wheel(self, clear_load):",
            "        \"\"\"",
            "        Send a master control function back to the wheel system",
            "        \"\"\"",
            "        # All wheel ops pass through eauth",
            "        auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)",
            "",
            "        # Authenticate",
            "        auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)",
            "        error = auth_check.get(\"error\")",
            "",
            "        if error:",
            "            # Authentication error occurred: do not continue.",
            "            return {\"error\": error}",
            "",
            "        # Authorize",
            "        username = auth_check.get(\"username\")",
            "        if auth_type != \"user\":",
            "            wheel_check = self.ckminions.wheel_check(",
            "                auth_check.get(\"auth_list\", []),",
            "                clear_load[\"fun\"],",
            "                clear_load.get(\"kwarg\", {}),",
            "            )",
            "            if not wheel_check:",
            "                return {",
            "                    \"error\": {",
            "                        \"name\": err_name,",
            "                        \"message\": 'Authentication failure of type \"{}\" occurred for '",
            "                        \"user {}.\".format(auth_type, username),",
            "                    }",
            "                }",
            "            elif isinstance(wheel_check, dict) and \"error\" in wheel_check:",
            "                # A dictionary with an error name/message was handled by ckminions.wheel_check",
            "                return wheel_check",
            "",
            "            # No error occurred, consume sensitive settings from the clear_load if passed.",
            "            for item in sensitive_load_keys:",
            "                clear_load.pop(item, None)",
            "        else:",
            "            if \"user\" in clear_load:",
            "                username = clear_load[\"user\"]",
            "                if salt.auth.AuthUser(username).is_sudo():",
            "                    username = self.opts.get(\"user\", \"root\")",
            "            else:",
            "                username = salt.utils.user.get_user()",
            "",
            "        # Authorized. Do the job!",
            "        try:",
            "            jid = salt.utils.jid.gen_jid(self.opts)",
            "            fun = clear_load.pop(\"fun\")",
            "            tag = tagify(jid, prefix=\"wheel\")",
            "            data = {",
            "                \"fun\": \"wheel.{}\".format(fun),",
            "                \"jid\": jid,",
            "                \"tag\": tag,",
            "                \"user\": username,",
            "            }",
            "",
            "            self.event.fire_event(data, tagify([jid, \"new\"], \"wheel\"))",
            "            ret = self.wheel_.call_func(fun, full_return=True, **clear_load)",
            "            data[\"return\"] = ret[\"return\"]",
            "            data[\"success\"] = ret[\"success\"]",
            "            self.event.fire_event(data, tagify([jid, \"ret\"], \"wheel\"))",
            "            return {\"tag\": tag, \"data\": data}",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception occurred while introspecting %s: %s\", fun, exc)",
            "            data[\"return\"] = \"Exception occurred in wheel {}: {}: {}\".format(",
            "                fun, exc.__class__.__name__, exc,",
            "            )",
            "            data[\"success\"] = False",
            "            self.event.fire_event(data, tagify([jid, \"ret\"], \"wheel\"))",
            "            return {\"tag\": tag, \"data\": data}",
            "",
            "    def mk_token(self, clear_load):",
            "        \"\"\"",
            "        Create and return an authentication token, the clear load needs to",
            "        contain the eauth key and the needed authentication creds.",
            "        \"\"\"",
            "        token = self.loadauth.mk_token(clear_load)",
            "        if not token:",
            "            log.warning('Authentication failure of type \"eauth\" occurred.')",
            "            return \"\"",
            "        return token",
            "",
            "    def get_token(self, clear_load):",
            "        \"\"\"",
            "        Return the name associated with a token or False if the token is invalid",
            "        \"\"\"",
            "        if \"token\" not in clear_load:",
            "            return False",
            "        return self.loadauth.get_tok(clear_load[\"token\"])",
            "",
            "    def publish(self, clear_load):",
            "        \"\"\"",
            "        This method sends out publications to the minions, it can only be used",
            "        by the LocalClient.",
            "        \"\"\"",
            "        extra = clear_load.get(\"kwargs\", {})",
            "",
            "        publisher_acl = salt.acl.PublisherACL(self.opts[\"publisher_acl_blacklist\"])",
            "",
            "        if publisher_acl.user_is_blacklisted(",
            "            clear_load[\"user\"]",
            "        ) or publisher_acl.cmd_is_blacklisted(clear_load[\"fun\"]):",
            "            log.error(",
            "                \"%s does not have permissions to run %s. Please contact \"",
            "                \"your local administrator if you believe this is in \"",
            "                \"error.\\n\",",
            "                clear_load[\"user\"],",
            "                clear_load[\"fun\"],",
            "            )",
            "            return {",
            "                \"error\": {",
            "                    \"name\": \"AuthorizationError\",",
            "                    \"message\": \"Authorization error occurred.\",",
            "                }",
            "            }",
            "",
            "        # Retrieve the minions list",
            "        delimiter = clear_load.get(\"kwargs\", {}).get(\"delimiter\", DEFAULT_TARGET_DELIM)",
            "        _res = self.ckminions.check_minions(",
            "            clear_load[\"tgt\"], clear_load.get(\"tgt_type\", \"glob\"), delimiter",
            "        )",
            "        minions = _res.get(\"minions\", list())",
            "        missing = _res.get(\"missing\", list())",
            "        ssh_minions = _res.get(\"ssh_minions\", False)",
            "",
            "        # Check for external auth calls and authenticate",
            "        auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(extra)",
            "        if auth_type == \"user\":",
            "            auth_check = self.loadauth.check_authentication(",
            "                clear_load, auth_type, key=key",
            "            )",
            "        else:",
            "            auth_check = self.loadauth.check_authentication(extra, auth_type)",
            "",
            "        # Setup authorization list variable and error information",
            "        auth_list = auth_check.get(\"auth_list\", [])",
            "        err_msg = 'Authentication failure of type \"{}\" occurred.'.format(auth_type)",
            "",
            "        if auth_check.get(\"error\"):",
            "            # Authentication error occurred: do not continue.",
            "            log.warning(err_msg)",
            "            return {",
            "                \"error\": {",
            "                    \"name\": \"AuthenticationError\",",
            "                    \"message\": \"Authentication error occurred.\",",
            "                }",
            "            }",
            "",
            "        # All Token, Eauth, and non-root users must pass the authorization check",
            "        if auth_type != \"user\" or (auth_type == \"user\" and auth_list):",
            "            # Authorize the request",
            "            authorized = self.ckminions.auth_check(",
            "                auth_list,",
            "                clear_load[\"fun\"],",
            "                clear_load[\"arg\"],",
            "                clear_load[\"tgt\"],",
            "                clear_load.get(\"tgt_type\", \"glob\"),",
            "                minions=minions,",
            "                # always accept find_job",
            "                whitelist=[\"saltutil.find_job\"],",
            "            )",
            "",
            "            if not authorized:",
            "                # Authorization error occurred. Do not continue.",
            "                if (",
            "                    auth_type == \"eauth\"",
            "                    and not auth_list",
            "                    and \"username\" in extra",
            "                    and \"eauth\" in extra",
            "                ):",
            "                    log.debug(",
            "                        'Auth configuration for eauth \"%s\" and user \"%s\" is empty',",
            "                        extra[\"eauth\"],",
            "                        extra[\"username\"],",
            "                    )",
            "                log.warning(err_msg)",
            "                return {",
            "                    \"error\": {",
            "                        \"name\": \"AuthorizationError\",",
            "                        \"message\": \"Authorization error occurred.\",",
            "                    }",
            "                }",
            "",
            "            # Perform some specific auth_type tasks after the authorization check",
            "            if auth_type == \"token\":",
            "                username = auth_check.get(\"username\")",
            "                clear_load[\"user\"] = username",
            "                log.debug('Minion tokenized user = \"%s\"', username)",
            "            elif auth_type == \"eauth\":",
            "                # The username we are attempting to auth with",
            "                clear_load[\"user\"] = self.loadauth.load_name(extra)",
            "",
            "        # If we order masters (via a syndic), don't short circuit if no minions",
            "        # are found",
            "        if not self.opts.get(\"order_masters\"):",
            "            # Check for no minions",
            "            if not minions:",
            "                return {",
            "                    \"enc\": \"clear\",",
            "                    \"load\": {",
            "                        \"jid\": None,",
            "                        \"minions\": minions,",
            "                        \"error\": \"Master could not resolve minions for target {}\".format(",
            "                            clear_load[\"tgt\"]",
            "                        ),",
            "                    },",
            "                }",
            "        jid = self._prep_jid(clear_load, extra)",
            "        if jid is None:",
            "            return {\"enc\": \"clear\", \"load\": {\"error\": \"Master failed to assign jid\"}}",
            "        payload = self._prep_pub(minions, jid, clear_load, extra, missing)",
            "",
            "        # Send it!",
            "        self._send_ssh_pub(payload, ssh_minions=ssh_minions)",
            "        self._send_pub(payload)",
            "",
            "        return {",
            "            \"enc\": \"clear\",",
            "            \"load\": {\"jid\": clear_load[\"jid\"], \"minions\": minions, \"missing\": missing},",
            "        }",
            "",
            "    def _prep_auth_info(self, clear_load):",
            "        sensitive_load_keys = []",
            "        key = None",
            "        if \"token\" in clear_load:",
            "            auth_type = \"token\"",
            "            err_name = \"TokenAuthenticationError\"",
            "            sensitive_load_keys = [\"token\"]",
            "        elif \"eauth\" in clear_load:",
            "            auth_type = \"eauth\"",
            "            err_name = \"EauthAuthenticationError\"",
            "            sensitive_load_keys = [\"username\", \"password\"]",
            "        else:",
            "            auth_type = \"user\"",
            "            err_name = \"UserAuthenticationError\"",
            "            key = self.key",
            "",
            "        return auth_type, err_name, key, sensitive_load_keys",
            "",
            "    def _prep_jid(self, clear_load, extra):",
            "        \"\"\"",
            "        Return a jid for this publication",
            "        \"\"\"",
            "        # the jid in clear_load can be None, '', or something else. this is an",
            "        # attempt to clean up the value before passing to plugins",
            "        passed_jid = clear_load[\"jid\"] if clear_load.get(\"jid\") else None",
            "        nocache = extra.get(\"nocache\", False)",
            "",
            "        # Retrieve the jid",
            "        fstr = \"{}.prep_jid\".format(self.opts[\"master_job_cache\"])",
            "        try:",
            "            # Retrieve the jid",
            "            jid = self.mminion.returners[fstr](nocache=nocache, passed_jid=passed_jid)",
            "        except (KeyError, TypeError):",
            "            # The returner is not present",
            "            msg = (",
            "                \"Failed to allocate a jid. The requested returner '{}' \"",
            "                \"could not be loaded.\".format(fstr.split(\".\")[0])",
            "            )",
            "            log.error(msg)",
            "            return {\"error\": msg}",
            "        return jid",
            "",
            "    def _send_pub(self, load):",
            "        \"\"\"",
            "        Take a load and send it across the network to connected minions",
            "        \"\"\"",
            "        for transport, opts in iter_transport_opts(self.opts):",
            "            chan = salt.transport.server.PubServerChannel.factory(opts)",
            "            chan.publish(load)",
            "",
            "    @property",
            "    def ssh_client(self):",
            "        if not hasattr(self, \"_ssh_client\"):",
            "            self._ssh_client = salt.client.ssh.client.SSHClient(mopts=self.opts)",
            "        return self._ssh_client",
            "",
            "    def _send_ssh_pub(self, load, ssh_minions=False):",
            "        \"\"\"",
            "        Take a load and send it across the network to ssh minions",
            "        \"\"\"",
            "        if self.opts[\"enable_ssh_minions\"] is True and ssh_minions is True:",
            "            log.debug(\"Send payload to ssh minions\")",
            "            threading.Thread(target=self.ssh_client.cmd, kwargs=load).start()",
            "",
            "    def _prep_pub(self, minions, jid, clear_load, extra, missing):",
            "        \"\"\"",
            "        Take a given load and perform the necessary steps",
            "        to prepare a publication.",
            "",
            "        TODO: This is really only bound by temporal cohesion",
            "        and thus should be refactored even further.",
            "        \"\"\"",
            "        clear_load[\"jid\"] = jid",
            "        delimiter = clear_load.get(\"kwargs\", {}).get(\"delimiter\", DEFAULT_TARGET_DELIM)",
            "",
            "        # TODO Error reporting over the master event bus",
            "        self.event.fire_event({\"minions\": minions}, clear_load[\"jid\"])",
            "        new_job_load = {",
            "            \"jid\": clear_load[\"jid\"],",
            "            \"tgt_type\": clear_load[\"tgt_type\"],",
            "            \"tgt\": clear_load[\"tgt\"],",
            "            \"user\": clear_load[\"user\"],",
            "            \"fun\": clear_load[\"fun\"],",
            "            \"arg\": clear_load[\"arg\"],",
            "            \"minions\": minions,",
            "            \"missing\": missing,",
            "        }",
            "",
            "        # Announce the job on the event bus",
            "        self.event.fire_event(new_job_load, tagify([clear_load[\"jid\"], \"new\"], \"job\"))",
            "",
            "        if self.opts[\"ext_job_cache\"]:",
            "            fstr = \"{}.save_load\".format(self.opts[\"ext_job_cache\"])",
            "            save_load_func = True",
            "",
            "            # Get the returner's save_load arg_spec.",
            "            try:",
            "                arg_spec = salt.utils.args.get_function_argspec(",
            "                    self.mminion.returners[fstr]",
            "                )",
            "",
            "                # Check if 'minions' is included in returner's save_load arg_spec.",
            "                # This may be missing in custom returners, which we should warn about.",
            "                if \"minions\" not in arg_spec.args:",
            "                    log.critical(",
            "                        \"The specified returner used for the external job cache \"",
            "                        \"'%s' does not have a 'minions' kwarg in the returner's \"",
            "                        \"save_load function.\",",
            "                        self.opts[\"ext_job_cache\"],",
            "                    )",
            "            except (AttributeError, KeyError):",
            "                save_load_func = False",
            "                log.critical(",
            "                    \"The specified returner used for the external job cache \"",
            "                    '\"%s\" does not have a save_load function!',",
            "                    self.opts[\"ext_job_cache\"],",
            "                )",
            "",
            "            if save_load_func:",
            "                try:",
            "                    self.mminion.returners[fstr](",
            "                        clear_load[\"jid\"], clear_load, minions=minions",
            "                    )",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.critical(",
            "                        \"The specified returner threw a stack trace:\\n\", exc_info=True",
            "                    )",
            "",
            "        # always write out to the master job caches",
            "        try:",
            "            fstr = \"{}.save_load\".format(self.opts[\"master_job_cache\"])",
            "            self.mminion.returners[fstr](clear_load[\"jid\"], clear_load, minions)",
            "        except KeyError:",
            "            log.critical(",
            "                \"The specified returner used for the master job cache \"",
            "                '\"%s\" does not have a save_load function!',",
            "                self.opts[\"master_job_cache\"],",
            "            )",
            "        except Exception:  # pylint: disable=broad-except",
            "            log.critical(\"The specified returner threw a stack trace:\\n\", exc_info=True)",
            "        # Set up the payload",
            "        payload = {\"enc\": \"aes\"}",
            "        # Altering the contents of the publish load is serious!! Changes here",
            "        # break compatibility with minion/master versions and even tiny",
            "        # additions can have serious implications on the performance of the",
            "        # publish commands.",
            "        #",
            "        # In short, check with Thomas Hatch before you even think about",
            "        # touching this stuff, we can probably do what you want to do another",
            "        # way that won't have a negative impact.",
            "        load = {",
            "            \"fun\": clear_load[\"fun\"],",
            "            \"arg\": clear_load[\"arg\"],",
            "            \"tgt\": clear_load[\"tgt\"],",
            "            \"jid\": clear_load[\"jid\"],",
            "            \"ret\": clear_load[\"ret\"],",
            "        }",
            "        # if you specified a master id, lets put that in the load",
            "        if \"master_id\" in self.opts:",
            "            load[\"master_id\"] = self.opts[\"master_id\"]",
            "        # if someone passed us one, use that",
            "        if \"master_id\" in extra:",
            "            load[\"master_id\"] = extra[\"master_id\"]",
            "        # Only add the delimiter to the pub data if it is non-default",
            "        if delimiter != DEFAULT_TARGET_DELIM:",
            "            load[\"delimiter\"] = delimiter",
            "",
            "        if \"id\" in extra:",
            "            load[\"id\"] = extra[\"id\"]",
            "        if \"tgt_type\" in clear_load:",
            "            load[\"tgt_type\"] = clear_load[\"tgt_type\"]",
            "        if \"to\" in clear_load:",
            "            load[\"to\"] = clear_load[\"to\"]",
            "",
            "        if \"kwargs\" in clear_load:",
            "            if \"ret_config\" in clear_load[\"kwargs\"]:",
            "                load[\"ret_config\"] = clear_load[\"kwargs\"].get(\"ret_config\")",
            "",
            "            if \"metadata\" in clear_load[\"kwargs\"]:",
            "                load[\"metadata\"] = clear_load[\"kwargs\"].get(\"metadata\")",
            "",
            "            if \"module_executors\" in clear_load[\"kwargs\"]:",
            "                load[\"module_executors\"] = clear_load[\"kwargs\"].get(\"module_executors\")",
            "",
            "            if \"executor_opts\" in clear_load[\"kwargs\"]:",
            "                load[\"executor_opts\"] = clear_load[\"kwargs\"].get(\"executor_opts\")",
            "",
            "            if \"ret_kwargs\" in clear_load[\"kwargs\"]:",
            "                load[\"ret_kwargs\"] = clear_load[\"kwargs\"].get(\"ret_kwargs\")",
            "",
            "        if \"user\" in clear_load:",
            "            log.info(",
            "                \"User %s Published command %s with jid %s\",",
            "                clear_load[\"user\"],",
            "                clear_load[\"fun\"],",
            "                clear_load[\"jid\"],",
            "            )",
            "            load[\"user\"] = clear_load[\"user\"]",
            "        else:",
            "            log.info(",
            "                \"Published command %s with jid %s\", clear_load[\"fun\"], clear_load[\"jid\"]",
            "            )",
            "        log.debug(\"Published command details %s\", load)",
            "        return load",
            "",
            "    def ping(self, clear_load):",
            "        \"\"\"",
            "        Send the load back to the sender.",
            "        \"\"\"",
            "        return clear_load"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "This module contains all of the routines needed to set up a master server, this",
            "involves preparing the three listeners and the workers needed by the master.",
            "\"\"\"",
            "",
            "",
            "import collections",
            "import copy",
            "import ctypes",
            "import functools",
            "import logging",
            "import multiprocessing",
            "import os",
            "import re",
            "import signal",
            "import stat",
            "import sys",
            "import threading",
            "import time",
            "",
            "import salt.acl",
            "import salt.auth",
            "import salt.client",
            "import salt.client.ssh.client",
            "import salt.crypt",
            "import salt.daemons.masterapi",
            "import salt.defaults.exitcodes",
            "import salt.engines",
            "import salt.exceptions",
            "import salt.ext.tornado.gen  # pylint: disable=F0401",
            "import salt.key",
            "import salt.log.setup",
            "import salt.minion",
            "import salt.payload",
            "import salt.pillar",
            "import salt.runner",
            "import salt.serializers.msgpack",
            "import salt.state",
            "import salt.transport.server",
            "import salt.utils.args",
            "import salt.utils.atomicfile",
            "import salt.utils.crypt",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.gitfs",
            "import salt.utils.gzip_util",
            "import salt.utils.jid",
            "import salt.utils.job",
            "import salt.utils.master",
            "import salt.utils.minions",
            "import salt.utils.platform",
            "import salt.utils.process",
            "import salt.utils.schedule",
            "import salt.utils.ssdp",
            "import salt.utils.stringutils",
            "import salt.utils.user",
            "import salt.utils.verify",
            "import salt.utils.zeromq",
            "import salt.wheel",
            "from salt.config import DEFAULT_INTERVAL",
            "from salt.defaults import DEFAULT_TARGET_DELIM",
            "",
            "# pylint: disable=import-error,no-name-in-module,redefined-builtin",
            "from salt.ext import six",
            "from salt.ext.six.moves import range",
            "from salt.ext.tornado.stack_context import StackContext",
            "from salt.transport import iter_transport_opts",
            "from salt.utils.ctx import RequestContext",
            "from salt.utils.debug import (",
            "    enable_sigusr1_handler,",
            "    enable_sigusr2_handler,",
            "    inspect_stack,",
            ")",
            "from salt.utils.event import tagify",
            "from salt.utils.odict import OrderedDict",
            "from salt.utils.zeromq import ZMQ_VERSION_INFO, ZMQDefaultLoop, install_zmq, zmq",
            "",
            "# pylint: enable=import-error,no-name-in-module,redefined-builtin",
            "",
            "",
            "try:",
            "    import resource",
            "",
            "    HAS_RESOURCE = True",
            "except ImportError:",
            "    # resource is not available on windows",
            "    HAS_RESOURCE = False",
            "",
            "try:",
            "    import halite  # pylint: disable=import-error",
            "",
            "    HAS_HALITE = True",
            "except ImportError:",
            "    HAS_HALITE = False",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "class SMaster:",
            "    \"\"\"",
            "    Create a simple salt-master, this will generate the top-level master",
            "    \"\"\"",
            "",
            "    secrets = (",
            "        {}",
            "    )  # mapping of key -> {'secret': multiprocessing type, 'reload': FUNCTION}",
            "",
            "    def __init__(self, opts):",
            "        \"\"\"",
            "        Create a salt master server instance",
            "",
            "        :param dict opts: The salt options dictionary",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.master_key = salt.crypt.MasterKeys(self.opts)",
            "        self.key = self.__prep_key()",
            "",
            "    # We need __setstate__ and __getstate__ to also pickle 'SMaster.secrets'.",
            "    # Otherwise, 'SMaster.secrets' won't be copied over to the spawned process",
            "    # on Windows since spawning processes on Windows requires pickling.",
            "    # These methods are only used when pickling so will not be used on",
            "    # non-Windows platforms.",
            "    def __setstate__(self, state):",
            "        self.opts = state[\"opts\"]",
            "        self.master_key = state[\"master_key\"]",
            "        self.key = state[\"key\"]",
            "        SMaster.secrets = state[\"secrets\"]",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"master_key\": self.master_key,",
            "            \"key\": self.key,",
            "            \"secrets\": SMaster.secrets,",
            "        }",
            "",
            "    def __prep_key(self):",
            "        \"\"\"",
            "        A key needs to be placed in the filesystem with permissions 0400 so",
            "        clients are required to run as root.",
            "        \"\"\"",
            "        return salt.daemons.masterapi.access_keys(self.opts)",
            "",
            "    @classmethod",
            "    def get_serial(cls, opts=None, event=None):",
            "        with cls.secrets[\"aes\"][\"secret\"].get_lock():",
            "            if cls.secrets[\"aes\"][\"serial\"].value == sys.maxsize:",
            "                cls.rotate_secrets(opts, event, use_lock=False)",
            "            else:",
            "                cls.secrets[\"aes\"][\"serial\"].value += 1",
            "            return cls.secrets[\"aes\"][\"serial\"].value",
            "",
            "    @classmethod",
            "    def rotate_secrets(cls, opts=None, event=None, use_lock=True):",
            "        log.info(\"Rotating master AES key\")",
            "        if opts is None:",
            "            opts = {}",
            "",
            "        for secret_key, secret_map in cls.secrets.items():",
            "            # should be unnecessary-- since no one else should be modifying",
            "            if use_lock:",
            "                with secret_map[\"secret\"].get_lock():",
            "                    secret_map[\"secret\"].value = salt.utils.stringutils.to_bytes(",
            "                        secret_map[\"reload\"]()",
            "                    )",
            "                    if \"serial\" in secret_map:",
            "                        secret_map[\"serial\"].value = 0",
            "            else:",
            "                secret_map[\"secret\"].value = salt.utils.stringutils.to_bytes(",
            "                    secret_map[\"reload\"]()",
            "                )",
            "                if \"serial\" in secret_map:",
            "                    secret_map[\"serial\"].value = 0",
            "            if event:",
            "                event.fire_event({\"rotate_{}_key\".format(secret_key): True}, tag=\"key\")",
            "",
            "        if opts.get(\"ping_on_rotate\"):",
            "            # Ping all minions to get them to pick up the new key",
            "            log.debug(\"Pinging all connected minions due to key rotation\")",
            "            salt.utils.master.ping_all_connected_minions(opts)",
            "",
            "",
            "class Maintenance(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    A generalized maintenance process which performs maintenance routines.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        \"\"\"",
            "        Create a maintenance instance",
            "",
            "        :param dict opts: The salt options",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        # How often do we perform the maintenance tasks",
            "        self.loop_interval = int(self.opts[\"loop_interval\"])",
            "        # Track key rotation intervals",
            "        self.rotate = int(time.time())",
            "        # A serializer for general maint operations",
            "        self.serial = salt.payload.Serial(self.opts)",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"opts\"],",
            "            log_queue=state[\"log_queue\"],",
            "            log_queue_level=state[\"log_queue_level\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def _post_fork_init(self):",
            "        \"\"\"",
            "        Some things need to be init'd after the fork has completed",
            "        The easiest example is that one of these module types creates a thread",
            "        in the parent process, then once the fork happens you'll start getting",
            "        errors like \"WARNING: Mixing fork() and threads detected; memory leaked.\"",
            "        \"\"\"",
            "        # Load Runners",
            "        ropts = dict(self.opts)",
            "        ropts[\"quiet\"] = True",
            "        runner_client = salt.runner.RunnerClient(ropts)",
            "        # Load Returners",
            "        self.returners = salt.loader.returners(self.opts, {})",
            "",
            "        # Init Scheduler",
            "        self.schedule = salt.utils.schedule.Schedule(",
            "            self.opts, runner_client.functions_dict(), returners=self.returners",
            "        )",
            "        self.ckminions = salt.utils.minions.CkMinions(self.opts)",
            "        # Make Event bus for firing",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        # Init any values needed by the git ext pillar",
            "        self.git_pillar = salt.daemons.masterapi.init_git_pillar(self.opts)",
            "",
            "        if self.opts[\"maintenance_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting Maintenance niceness to %d\", self.opts[\"maintenance_niceness\"]",
            "            )",
            "            os.nice(self.opts[\"maintenance_niceness\"])",
            "",
            "        self.presence_events = False",
            "        if self.opts.get(\"presence_events\", False):",
            "            tcp_only = True",
            "            for transport, _ in iter_transport_opts(self.opts):",
            "                if transport != \"tcp\":",
            "                    tcp_only = False",
            "            if not tcp_only:",
            "                # For a TCP only transport, the presence events will be",
            "                # handled in the transport code.",
            "                self.presence_events = True",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        This is the general passive maintenance process controller for the Salt",
            "        master.",
            "",
            "        This is where any data that needs to be cleanly maintained from the",
            "        master is maintained.",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        # init things that need to be done after the process is forked",
            "        self._post_fork_init()",
            "",
            "        # Make Start Times",
            "        last = int(time.time())",
            "        # update git_pillar on first loop",
            "        last_git_pillar_update = 0",
            "",
            "        git_pillar_update_interval = self.opts.get(\"git_pillar_update_interval\", 0)",
            "        old_present = set()",
            "        while True:",
            "            now = int(time.time())",
            "            if (now - last) >= self.loop_interval:",
            "                salt.daemons.masterapi.clean_old_jobs(self.opts)",
            "                salt.daemons.masterapi.clean_expired_tokens(self.opts)",
            "                salt.daemons.masterapi.clean_pub_auth(self.opts)",
            "            if (now - last_git_pillar_update) >= git_pillar_update_interval:",
            "                last_git_pillar_update = now",
            "                self.handle_git_pillar()",
            "            self.handle_schedule()",
            "            self.handle_key_cache()",
            "            self.handle_presence(old_present)",
            "            self.handle_key_rotate(now)",
            "            salt.utils.verify.check_max_open_files(self.opts)",
            "            last = now",
            "            time.sleep(self.loop_interval)",
            "",
            "    def handle_key_cache(self):",
            "        \"\"\"",
            "        Evaluate accepted keys and create a msgpack file",
            "        which contains a list",
            "        \"\"\"",
            "        if self.opts[\"key_cache\"] == \"sched\":",
            "            keys = []",
            "            # TODO DRY from CKMinions",
            "            if self.opts[\"transport\"] in (\"zeromq\", \"tcp\"):",
            "                acc = \"minions\"",
            "            else:",
            "                acc = \"accepted\"",
            "",
            "            for fn_ in os.listdir(os.path.join(self.opts[\"pki_dir\"], acc)):",
            "                if not fn_.startswith(\".\") and os.path.isfile(",
            "                    os.path.join(self.opts[\"pki_dir\"], acc, fn_)",
            "                ):",
            "                    keys.append(fn_)",
            "            log.debug(\"Writing master key cache\")",
            "            # Write a temporary file securely",
            "            with salt.utils.atomicfile.atomic_open(",
            "                os.path.join(self.opts[\"pki_dir\"], acc, \".key_cache\"), mode=\"wb\"",
            "            ) as cache_file:",
            "                self.serial.dump(keys, cache_file)",
            "",
            "    def handle_key_rotate(self, now):",
            "        \"\"\"",
            "        Rotate the AES key rotation",
            "        \"\"\"",
            "        to_rotate = False",
            "        dfn = os.path.join(self.opts[\"cachedir\"], \".dfn\")",
            "        try:",
            "            stats = os.stat(dfn)",
            "            # Basic Windows permissions don't distinguish between",
            "            # user/group/all. Check for read-only state instead.",
            "            if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):",
            "                to_rotate = True",
            "                # Cannot delete read-only files on Windows.",
            "                os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)",
            "            elif stats.st_mode == 0o100400:",
            "                to_rotate = True",
            "            else:",
            "                log.error(\"Found dropfile with incorrect permissions, ignoring...\")",
            "            os.remove(dfn)",
            "        except os.error:",
            "            pass",
            "",
            "        if self.opts.get(\"publish_session\"):",
            "            if now - self.rotate >= self.opts[\"publish_session\"]:",
            "                to_rotate = True",
            "",
            "        if to_rotate:",
            "            SMaster.rotate_secrets(self.opts, self.event)",
            "            self.rotate = now",
            "",
            "    def handle_git_pillar(self):",
            "        \"\"\"",
            "        Update git pillar",
            "        \"\"\"",
            "        try:",
            "            for pillar in self.git_pillar:",
            "                pillar.fetch_remotes()",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception caught while updating git_pillar\", exc_info=True)",
            "",
            "    def handle_schedule(self):",
            "        \"\"\"",
            "        Evaluate the scheduler",
            "        \"\"\"",
            "        try:",
            "            self.schedule.eval()",
            "            # Check if scheduler requires lower loop interval than",
            "            # the loop_interval setting",
            "            if self.schedule.loop_interval < self.loop_interval:",
            "                self.loop_interval = self.schedule.loop_interval",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception %s occurred in scheduled job\", exc)",
            "        self.schedule.cleanup_subprocesses()",
            "",
            "    def handle_presence(self, old_present):",
            "        \"\"\"",
            "        Fire presence events if enabled",
            "        \"\"\"",
            "        # On the first run it may need more time for the EventPublisher",
            "        # to come up and be ready. Set the timeout to account for this.",
            "        if self.presence_events and self.event.connect_pull(timeout=3):",
            "            present = self.ckminions.connected_ids()",
            "            new = present.difference(old_present)",
            "            lost = old_present.difference(present)",
            "            if new or lost:",
            "                # Fire new minions present event",
            "                data = {\"new\": list(new), \"lost\": list(lost)}",
            "                self.event.fire_event(data, tagify(\"change\", \"presence\"))",
            "            data = {\"present\": list(present)}",
            "            self.event.fire_event(data, tagify(\"present\", \"presence\"))",
            "            old_present.clear()",
            "            old_present.update(present)",
            "",
            "",
            "class FileserverUpdate(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    A process from which to update any dynamic fileserver backends",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        self.update_threads = {}",
            "        # Avoid circular import",
            "        import salt.fileserver",
            "",
            "        self.fileserver = salt.fileserver.Fileserver(self.opts)",
            "        self.fill_buckets()",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"opts\"], log_queue=state[\"log_queue\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"log_queue\": self.log_queue,",
            "        }",
            "",
            "    def fill_buckets(self):",
            "        \"\"\"",
            "        Get the configured backends and the intervals for any backend which",
            "        supports them, and set up the update \"buckets\". There will be one",
            "        bucket for each thing being updated at a given interval.",
            "        \"\"\"",
            "        update_intervals = self.fileserver.update_intervals()",
            "        self.buckets = {}",
            "        for backend in self.fileserver.backends():",
            "            fstr = \"{}.update\".format(backend)",
            "            try:",
            "                update_func = self.fileserver.servers[fstr]",
            "            except KeyError:",
            "                log.debug(\"No update function for the %s filserver backend\", backend)",
            "                continue",
            "            if backend in update_intervals:",
            "                # Variable intervals are supported for this backend",
            "                for id_, interval in update_intervals[backend].items():",
            "                    if not interval:",
            "                        # Don't allow an interval of 0",
            "                        interval = DEFAULT_INTERVAL",
            "                        log.debug(",
            "                            \"An update_interval of 0 is not supported, \"",
            "                            \"falling back to %s\",",
            "                            interval,",
            "                        )",
            "                    i_ptr = self.buckets.setdefault(interval, OrderedDict())",
            "                    # Backend doesn't technically need to be present in the",
            "                    # key, all we *really* need is the function reference, but",
            "                    # having it there makes it easier to provide meaningful",
            "                    # debug logging in the update threads.",
            "                    i_ptr.setdefault((backend, update_func), []).append(id_)",
            "            else:",
            "                # Variable intervals are not supported for this backend, so",
            "                # fall back to the global interval for that fileserver. Since",
            "                # this backend doesn't support variable updates, we have",
            "                # nothing to pass to the backend's update func, so we'll just",
            "                # set the value to None.",
            "                try:",
            "                    interval_key = \"{}_update_interval\".format(backend)",
            "                    interval = self.opts[interval_key]",
            "                except KeyError:",
            "                    interval = DEFAULT_INTERVAL",
            "                    log.warning(",
            "                        \"%s key missing from configuration. Falling back to \"",
            "                        \"default interval of %d seconds\",",
            "                        interval_key,",
            "                        interval,",
            "                    )",
            "                self.buckets.setdefault(interval, OrderedDict())[",
            "                    (backend, update_func)",
            "                ] = None",
            "",
            "    def update_fileserver(self, interval, backends):",
            "        \"\"\"",
            "        Threading target which handles all updates for a given wait interval",
            "        \"\"\"",
            "",
            "        def _do_update():",
            "            log.debug(",
            "                \"Performing fileserver updates for items with an update \"",
            "                \"interval of %d\",",
            "                interval,",
            "            )",
            "            for backend, update_args in backends.items():",
            "                backend_name, update_func = backend",
            "                try:",
            "                    if update_args:",
            "                        log.debug(",
            "                            \"Updating %s fileserver cache for the following \"",
            "                            \"targets: %s\",",
            "                            backend_name,",
            "                            update_args,",
            "                        )",
            "                        args = (update_args,)",
            "                    else:",
            "                        log.debug(\"Updating %s fileserver cache\", backend_name)",
            "                        args = ()",
            "",
            "                    update_func(*args)",
            "                except Exception as exc:  # pylint: disable=broad-except",
            "                    log.exception(",
            "                        \"Uncaught exception while updating %s fileserver \" \"cache\",",
            "                        backend_name,",
            "                    )",
            "",
            "            log.debug(",
            "                \"Completed fileserver updates for items with an update \"",
            "                \"interval of %d, waiting %d seconds\",",
            "                interval,",
            "                interval,",
            "            )",
            "",
            "        condition = threading.Condition()",
            "        _do_update()",
            "        while True:",
            "            with condition:",
            "                condition.wait(interval)",
            "            _do_update()",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Start the update threads",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        if (",
            "            self.opts[\"fileserver_update_niceness\"]",
            "            and not salt.utils.platform.is_windows()",
            "        ):",
            "            log.info(",
            "                \"setting FileServerUpdate niceness to %d\",",
            "                self.opts[\"fileserver_update_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"fileserver_update_niceness\"])",
            "",
            "        # Clean out the fileserver backend cache",
            "        salt.daemons.masterapi.clean_fsbackend(self.opts)",
            "",
            "        for interval in self.buckets:",
            "            self.update_threads[interval] = threading.Thread(",
            "                target=self.update_fileserver, args=(interval, self.buckets[interval]),",
            "            )",
            "            self.update_threads[interval].start()",
            "",
            "        # Keep the process alive",
            "        while True:",
            "            time.sleep(60)",
            "",
            "",
            "class Master(SMaster):",
            "    \"\"\"",
            "    The salt master server",
            "    \"\"\"",
            "",
            "    def __init__(self, opts):",
            "        \"\"\"",
            "        Create a salt master server instance",
            "",
            "        :param dict: The salt options",
            "        \"\"\"",
            "        if zmq and ZMQ_VERSION_INFO < (3, 2):",
            "            log.warning(",
            "                \"You have a version of ZMQ less than ZMQ 3.2! There are \"",
            "                \"known connection keep-alive issues with ZMQ < 3.2 which \"",
            "                \"may result in loss of contact with minions. Please \"",
            "                \"upgrade your ZMQ!\"",
            "            )",
            "        SMaster.__init__(self, opts)",
            "",
            "    def __set_max_open_files(self):",
            "        if not HAS_RESOURCE:",
            "            return",
            "        # Let's check to see how our max open files(ulimit -n) setting is",
            "        mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)",
            "        if mof_h == resource.RLIM_INFINITY:",
            "            # Unclear what to do with infinity... macOS reports RLIM_INFINITY as",
            "            # hard limit,but raising to anything above soft limit fails...",
            "            mof_h = mof_s",
            "        log.info(",
            "            \"Current values for max open files soft/hard setting: %s/%s\", mof_s, mof_h",
            "        )",
            "        # Let's grab, from the configuration file, the value to raise max open",
            "        # files to",
            "        mof_c = self.opts[\"max_open_files\"]",
            "        if mof_c > mof_h:",
            "            # The configured value is higher than what's allowed",
            "            log.info(",
            "                \"The value for the 'max_open_files' setting, %s, is higher \"",
            "                \"than the highest value the user running salt is allowed to \"",
            "                \"set (%s). Defaulting to %s.\",",
            "                mof_c,",
            "                mof_h,",
            "                mof_h,",
            "            )",
            "            mof_c = mof_h",
            "",
            "        if mof_s < mof_c:",
            "            # There's room to raise the value. Raise it!",
            "            log.info(\"Raising max open files value to %s\", mof_c)",
            "            resource.setrlimit(resource.RLIMIT_NOFILE, (mof_c, mof_h))",
            "            try:",
            "                mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)",
            "                log.info(",
            "                    \"New values for max open files soft/hard values: %s/%s\",",
            "                    mof_s,",
            "                    mof_h,",
            "                )",
            "            except ValueError:",
            "                # https://github.com/saltstack/salt/issues/1991#issuecomment-13025595",
            "                # A user under macOS reported that our 100000 default value is",
            "                # still too high.",
            "                log.critical(",
            "                    \"Failed to raise max open files setting to %s. If this \"",
            "                    \"value is too low, the salt-master will most likely fail \"",
            "                    \"to run properly.\",",
            "                    mof_c,",
            "                )",
            "",
            "    def _pre_flight(self):",
            "        \"\"\"",
            "        Run pre flight checks. If anything in this method fails then the master",
            "        should not start up.",
            "        \"\"\"",
            "        errors = []",
            "        critical_errors = []",
            "",
            "        try:",
            "            os.chdir(\"/\")",
            "        except OSError as err:",
            "            errors.append(\"Cannot change to root directory ({})\".format(err))",
            "",
            "        if self.opts.get(\"fileserver_verify_config\", True):",
            "            # Avoid circular import",
            "            import salt.fileserver",
            "",
            "            fileserver = salt.fileserver.Fileserver(self.opts)",
            "            if not fileserver.servers:",
            "                errors.append(",
            "                    \"Failed to load fileserver backends, the configured backends \"",
            "                    \"are: {}\".format(\", \".join(self.opts[\"fileserver_backend\"]))",
            "                )",
            "            else:",
            "                # Run init() for all backends which support the function, to",
            "                # double-check configuration",
            "                try:",
            "                    fileserver.init()",
            "                except salt.exceptions.FileserverConfigError as exc:",
            "                    critical_errors.append(\"{}\".format(exc))",
            "",
            "        if not self.opts[\"fileserver_backend\"]:",
            "            errors.append(\"No fileserver backends are configured\")",
            "",
            "        # Check to see if we need to create a pillar cache dir",
            "        if self.opts[\"pillar_cache\"] and not os.path.isdir(",
            "            os.path.join(self.opts[\"cachedir\"], \"pillar_cache\")",
            "        ):",
            "            try:",
            "                with salt.utils.files.set_umask(0o077):",
            "                    os.mkdir(os.path.join(self.opts[\"cachedir\"], \"pillar_cache\"))",
            "            except OSError:",
            "                pass",
            "",
            "        if self.opts.get(\"git_pillar_verify_config\", True):",
            "            try:",
            "                git_pillars = [",
            "                    x",
            "                    for x in self.opts.get(\"ext_pillar\", [])",
            "                    if \"git\" in x and not isinstance(x[\"git\"], str)",
            "                ]",
            "            except TypeError:",
            "                git_pillars = []",
            "                critical_errors.append(",
            "                    \"Invalid ext_pillar configuration. It is likely that the \"",
            "                    \"external pillar type was not specified for one or more \"",
            "                    \"external pillars.\"",
            "                )",
            "            if git_pillars:",
            "                try:",
            "                    new_opts = copy.deepcopy(self.opts)",
            "                    import salt.pillar.git_pillar",
            "",
            "                    for repo in git_pillars:",
            "                        new_opts[\"ext_pillar\"] = [repo]",
            "                        try:",
            "                            git_pillar = salt.utils.gitfs.GitPillar(",
            "                                new_opts,",
            "                                repo[\"git\"],",
            "                                per_remote_overrides=salt.pillar.git_pillar.PER_REMOTE_OVERRIDES,",
            "                                per_remote_only=salt.pillar.git_pillar.PER_REMOTE_ONLY,",
            "                                global_only=salt.pillar.git_pillar.GLOBAL_ONLY,",
            "                            )",
            "                        except salt.exceptions.FileserverConfigError as exc:",
            "                            critical_errors.append(exc.strerror)",
            "                finally:",
            "                    del new_opts",
            "",
            "        if errors or critical_errors:",
            "            for error in errors:",
            "                log.error(error)",
            "            for error in critical_errors:",
            "                log.critical(error)",
            "            log.critical(\"Master failed pre flight checks, exiting\\n\")",
            "            sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "",
            "    def start(self):",
            "        \"\"\"",
            "        Turn on the master server components",
            "        \"\"\"",
            "        self._pre_flight()",
            "        log.info(\"salt-master is starting as user '%s'\", salt.utils.user.get_user())",
            "",
            "        enable_sigusr1_handler()",
            "        enable_sigusr2_handler()",
            "",
            "        self.__set_max_open_files()",
            "",
            "        # Reset signals to default ones before adding processes to the process",
            "        # manager. We don't want the processes being started to inherit those",
            "        # signal handlers",
            "        with salt.utils.process.default_signals(signal.SIGINT, signal.SIGTERM):",
            "",
            "            # Setup the secrets here because the PubServerChannel may need",
            "            # them as well.",
            "            SMaster.secrets[\"aes\"] = {",
            "                \"secret\": multiprocessing.Array(",
            "                    ctypes.c_char,",
            "                    salt.utils.stringutils.to_bytes(",
            "                        salt.crypt.Crypticle.generate_key_string()",
            "                    ),",
            "                ),",
            "                \"serial\": multiprocessing.Value(",
            "                    ctypes.c_longlong, lock=False  # We'll use the lock from 'secret'",
            "                ),",
            "                \"reload\": salt.crypt.Crypticle.generate_key_string,",
            "            }",
            "",
            "            log.info(\"Creating master process manager\")",
            "            # Since there are children having their own ProcessManager we should wait for kill more time.",
            "            self.process_manager = salt.utils.process.ProcessManager(wait_for_kill=5)",
            "            pub_channels = []",
            "            log.info(\"Creating master publisher process\")",
            "            log_queue = salt.log.setup.get_multiprocessing_logging_queue()",
            "            for _, opts in iter_transport_opts(self.opts):",
            "                chan = salt.transport.server.PubServerChannel.factory(opts)",
            "                chan.pre_fork(self.process_manager, kwargs={\"log_queue\": log_queue})",
            "                pub_channels.append(chan)",
            "",
            "            log.info(\"Creating master event publisher process\")",
            "            self.process_manager.add_process(",
            "                salt.utils.event.EventPublisher, args=(self.opts,)",
            "            )",
            "",
            "            if self.opts.get(\"reactor\"):",
            "                if isinstance(self.opts[\"engines\"], list):",
            "                    rine = False",
            "                    for item in self.opts[\"engines\"]:",
            "                        if \"reactor\" in item:",
            "                            rine = True",
            "                            break",
            "                    if not rine:",
            "                        self.opts[\"engines\"].append({\"reactor\": {}})",
            "                else:",
            "                    if \"reactor\" not in self.opts[\"engines\"]:",
            "                        log.info(\"Enabling the reactor engine\")",
            "                        self.opts[\"engines\"][\"reactor\"] = {}",
            "",
            "            salt.engines.start_engines(self.opts, self.process_manager)",
            "",
            "            # must be after channels",
            "            log.info(\"Creating master maintenance process\")",
            "            self.process_manager.add_process(Maintenance, args=(self.opts,))",
            "",
            "            if self.opts.get(\"event_return\"):",
            "                log.info(\"Creating master event return process\")",
            "                self.process_manager.add_process(",
            "                    salt.utils.event.EventReturn, args=(self.opts,)",
            "                )",
            "",
            "            ext_procs = self.opts.get(\"ext_processes\", [])",
            "            for proc in ext_procs:",
            "                log.info(\"Creating ext_processes process: %s\", proc)",
            "                try:",
            "                    mod = \".\".join(proc.split(\".\")[:-1])",
            "                    cls = proc.split(\".\")[-1]",
            "                    _tmp = __import__(mod, globals(), locals(), [cls], -1)",
            "                    cls = _tmp.__getattribute__(cls)",
            "                    self.process_manager.add_process(cls, args=(self.opts,))",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.error(\"Error creating ext_processes process: %s\", proc)",
            "",
            "            if HAS_HALITE and \"halite\" in self.opts:",
            "                log.info(\"Creating master halite process\")",
            "                self.process_manager.add_process(Halite, args=(self.opts[\"halite\"],))",
            "",
            "            # TODO: remove, or at least push into the transport stuff (pre-fork probably makes sense there)",
            "            if self.opts[\"con_cache\"]:",
            "                log.info(\"Creating master concache process\")",
            "                self.process_manager.add_process(",
            "                    salt.utils.master.ConnectedCache, args=(self.opts,)",
            "                )",
            "                # workaround for issue #16315, race condition",
            "                log.debug(\"Sleeping for two seconds to let concache rest\")",
            "                time.sleep(2)",
            "",
            "            log.info(\"Creating master request server process\")",
            "            kwargs = {}",
            "            if salt.utils.platform.is_windows():",
            "                kwargs[\"log_queue\"] = log_queue",
            "                kwargs[",
            "                    \"log_queue_level\"",
            "                ] = salt.log.setup.get_multiprocessing_logging_level()",
            "                kwargs[\"secrets\"] = SMaster.secrets",
            "",
            "            self.process_manager.add_process(",
            "                ReqServer,",
            "                args=(self.opts, self.key, self.master_key),",
            "                kwargs=kwargs,",
            "                name=\"ReqServer\",",
            "            )",
            "",
            "            self.process_manager.add_process(FileserverUpdate, args=(self.opts,))",
            "",
            "            # Fire up SSDP discovery publisher",
            "            if self.opts[\"discovery\"]:",
            "                if salt.utils.ssdp.SSDPDiscoveryServer.is_available():",
            "                    self.process_manager.add_process(",
            "                        salt.utils.ssdp.SSDPDiscoveryServer(",
            "                            port=self.opts[\"discovery\"][\"port\"],",
            "                            listen_ip=self.opts[\"interface\"],",
            "                            answer={",
            "                                \"mapping\": self.opts[\"discovery\"].get(\"mapping\", {})",
            "                            },",
            "                        ).run",
            "                    )",
            "                else:",
            "                    log.error(\"Unable to load SSDP: asynchronous IO is not available.\")",
            "                    if sys.version_info.major == 2:",
            "                        log.error(",
            "                            'You are using Python 2, please install \"trollius\" module to enable SSDP discovery.'",
            "                        )",
            "",
            "        # Install the SIGINT/SIGTERM handlers if not done so far",
            "        if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGINT, self._handle_signals)",
            "",
            "        if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGTERM, self._handle_signals)",
            "",
            "        self.process_manager.run()",
            "",
            "    def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument",
            "        # escalate the signals to the process manager",
            "        self.process_manager.stop_restarting()",
            "        self.process_manager.send_signal_to_processes(signum)",
            "        # kill any remaining processes",
            "        self.process_manager.kill_children()",
            "        time.sleep(1)",
            "        sys.exit(0)",
            "",
            "",
            "class Halite(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    Manage the Halite server",
            "    \"\"\"",
            "",
            "    def __init__(self, hopts, **kwargs):",
            "        \"\"\"",
            "        Create a halite instance",
            "",
            "        :param dict hopts: The halite options",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.hopts = hopts",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"hopts\"],",
            "            log_queue=state[\"log_queue\"],",
            "            log_queue_level=state[\"log_queue_level\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"hopts\": self.hopts,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Fire up halite!",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "        halite.start(self.hopts)",
            "",
            "",
            "class ReqServer(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    Starts up the master request server, minions send results to this",
            "    interface.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, key, mkey, secrets=None, **kwargs):",
            "        \"\"\"",
            "        Create a request server",
            "",
            "        :param dict opts: The salt options dictionary",
            "        :key dict: The user starting the server and the AES key",
            "        :mkey dict: The user starting the server and the RSA key",
            "",
            "        :rtype: ReqServer",
            "        :returns: Request server",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        self.master_key = mkey",
            "        # Prepare the AES key",
            "        self.key = key",
            "        self.secrets = secrets",
            "",
            "    # __setstate__ and __getstate__ are only used on Windows.",
            "    # We do this so that __init__ will be invoked on Windows in the child",
            "    # process so that a register_after_fork() equivalent will work on Windows.",
            "    def __setstate__(self, state):",
            "        self.__init__(",
            "            state[\"opts\"],",
            "            state[\"key\"],",
            "            state[\"mkey\"],",
            "            secrets=state[\"secrets\"],",
            "            log_queue=state[\"log_queue\"],",
            "            log_queue_level=state[\"log_queue_level\"],",
            "        )",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"key\": self.key,",
            "            \"mkey\": self.master_key,",
            "            \"secrets\": self.secrets,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument",
            "        self.destroy(signum)",
            "        super()._handle_signals(signum, sigframe)",
            "",
            "    def __bind(self):",
            "        \"\"\"",
            "        Binds the reply server",
            "        \"\"\"",
            "        if self.log_queue is not None:",
            "            salt.log.setup.set_multiprocessing_logging_queue(self.log_queue)",
            "        if self.log_queue_level is not None:",
            "            salt.log.setup.set_multiprocessing_logging_level(self.log_queue_level)",
            "        salt.log.setup.setup_multiprocessing_logging(self.log_queue)",
            "        if self.secrets is not None:",
            "            SMaster.secrets = self.secrets",
            "",
            "        dfn = os.path.join(self.opts[\"cachedir\"], \".dfn\")",
            "        if os.path.isfile(dfn):",
            "            try:",
            "                if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):",
            "                    # Cannot delete read-only files on Windows.",
            "                    os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)",
            "                os.remove(dfn)",
            "            except os.error:",
            "                pass",
            "",
            "        # Wait for kill should be less then parent's ProcessManager.",
            "        self.process_manager = salt.utils.process.ProcessManager(",
            "            name=\"ReqServer_ProcessManager\", wait_for_kill=1",
            "        )",
            "",
            "        req_channels = []",
            "        tcp_only = True",
            "        for transport, opts in iter_transport_opts(self.opts):",
            "            chan = salt.transport.server.ReqServerChannel.factory(opts)",
            "            chan.pre_fork(self.process_manager)",
            "            req_channels.append(chan)",
            "            if transport != \"tcp\":",
            "                tcp_only = False",
            "",
            "        kwargs = {}",
            "        if salt.utils.platform.is_windows():",
            "            kwargs[\"log_queue\"] = self.log_queue",
            "            kwargs[\"log_queue_level\"] = self.log_queue_level",
            "            # Use one worker thread if only the TCP transport is set up on",
            "            # Windows and we are using Python 2. There is load balancer",
            "            # support on Windows for the TCP transport when using Python 3.",
            "            if tcp_only and six.PY2 and int(self.opts[\"worker_threads\"]) != 1:",
            "                log.warning(",
            "                    \"TCP transport supports only 1 worker on Windows \"",
            "                    \"when using Python 2.\"",
            "                )",
            "                self.opts[\"worker_threads\"] = 1",
            "",
            "        if self.opts[\"req_server_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting ReqServer_ProcessManager niceness to %d\",",
            "                self.opts[\"req_server_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"req_server_niceness\"])",
            "",
            "        # Reset signals to default ones before adding processes to the process",
            "        # manager. We don't want the processes being started to inherit those",
            "        # signal handlers",
            "        with salt.utils.process.default_signals(signal.SIGINT, signal.SIGTERM):",
            "            for ind in range(int(self.opts[\"worker_threads\"])):",
            "                name = \"MWorker-{}\".format(ind)",
            "                self.process_manager.add_process(",
            "                    MWorker,",
            "                    args=(self.opts, self.master_key, self.key, req_channels, name),",
            "                    kwargs=kwargs,",
            "                    name=name,",
            "                )",
            "        self.process_manager.run()",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Start up the ReqServer",
            "        \"\"\"",
            "        self.__bind()",
            "",
            "    def destroy(self, signum=signal.SIGTERM):",
            "        if hasattr(self, \"process_manager\"):",
            "            self.process_manager.stop_restarting()",
            "            self.process_manager.send_signal_to_processes(signum)",
            "            self.process_manager.kill_children()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.destroy()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "class MWorker(salt.utils.process.SignalHandlingProcess):",
            "    \"\"\"",
            "    The worker multiprocess instance to manage the backend operations for the",
            "    salt master.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, mkey, key, req_channels, name, **kwargs):",
            "        \"\"\"",
            "        Create a salt master worker process",
            "",
            "        :param dict opts: The salt options",
            "        :param dict mkey: The user running the salt master and the AES key",
            "        :param dict key: The user running the salt master and the RSA key",
            "",
            "        :rtype: MWorker",
            "        :return: Master worker",
            "        \"\"\"",
            "        kwargs[\"name\"] = name",
            "        self.name = name",
            "        super().__init__(**kwargs)",
            "        self.opts = opts",
            "        self.req_channels = req_channels",
            "",
            "        self.mkey = mkey",
            "        self.key = key",
            "        self.k_mtime = 0",
            "        self.stats = collections.defaultdict(lambda: {\"mean\": 0, \"runs\": 0})",
            "        self.stat_clock = time.time()",
            "",
            "    # We need __setstate__ and __getstate__ to also pickle 'SMaster.secrets'.",
            "    # Otherwise, 'SMaster.secrets' won't be copied over to the spawned process",
            "    # on Windows since spawning processes on Windows requires pickling.",
            "    # These methods are only used when pickling so will not be used on",
            "    # non-Windows platforms.",
            "    def __setstate__(self, state):",
            "        super().__init__(",
            "            log_queue=state[\"log_queue\"], log_queue_level=state[\"log_queue_level\"]",
            "        )",
            "        self.opts = state[\"opts\"]",
            "        self.req_channels = state[\"req_channels\"]",
            "        self.mkey = state[\"mkey\"]",
            "        self.key = state[\"key\"]",
            "        self.k_mtime = state[\"k_mtime\"]",
            "        SMaster.secrets = state[\"secrets\"]",
            "",
            "    def __getstate__(self):",
            "        return {",
            "            \"opts\": self.opts,",
            "            \"req_channels\": self.req_channels,",
            "            \"mkey\": self.mkey,",
            "            \"key\": self.key,",
            "            \"k_mtime\": self.k_mtime,",
            "            \"secrets\": SMaster.secrets,",
            "            \"log_queue\": self.log_queue,",
            "            \"log_queue_level\": self.log_queue_level,",
            "        }",
            "",
            "    def _handle_signals(self, signum, sigframe):",
            "        for channel in getattr(self, \"req_channels\", ()):",
            "            channel.close()",
            "        super()._handle_signals(signum, sigframe)",
            "",
            "    def __bind(self):",
            "        \"\"\"",
            "        Bind to the local port",
            "        \"\"\"",
            "        # using ZMQIOLoop since we *might* need zmq in there",
            "        install_zmq()",
            "        self.io_loop = ZMQDefaultLoop()",
            "        self.io_loop.make_current()",
            "        for req_channel in self.req_channels:",
            "            req_channel.post_fork(",
            "                self._handle_payload, io_loop=self.io_loop",
            "            )  # TODO: cleaner? Maybe lazily?",
            "        try:",
            "            self.io_loop.start()",
            "        except (KeyboardInterrupt, SystemExit):",
            "            # Tornado knows what to do",
            "            pass",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _handle_payload(self, payload):",
            "        \"\"\"",
            "        The _handle_payload method is the key method used to figure out what",
            "        needs to be done with communication to the server",
            "",
            "        Example cleartext payload generated for 'salt myminion test.ping':",
            "",
            "        {'enc': 'clear',",
            "         'load': {'arg': [],",
            "                  'cmd': 'publish',",
            "                  'fun': 'test.ping',",
            "                  'jid': '',",
            "                  'key': 'alsdkjfa.,maljf-==adflkjadflkjalkjadfadflkajdflkj',",
            "                  'kwargs': {'show_jid': False, 'show_timeout': False},",
            "                  'ret': '',",
            "                  'tgt': 'myminion',",
            "                  'tgt_type': 'glob',",
            "                  'user': 'root'}}",
            "",
            "        :param dict payload: The payload route to the appropriate handler",
            "        \"\"\"",
            "        key = payload[\"enc\"]",
            "        load = payload[\"load\"]",
            "        ret = {\"aes\": self._handle_aes, \"clear\": self._handle_clear}[key](load)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    def _post_stats(self, start, cmd):",
            "        \"\"\"",
            "        Calculate the master stats and fire events with stat info",
            "        \"\"\"",
            "        end = time.time()",
            "        duration = end - start",
            "        self.stats[cmd][\"mean\"] = (",
            "            self.stats[cmd][\"mean\"] * (self.stats[cmd][\"runs\"] - 1) + duration",
            "        ) / self.stats[cmd][\"runs\"]",
            "        if end - self.stat_clock > self.opts[\"master_stats_event_iter\"]:",
            "            # Fire the event with the stats and wipe the tracker",
            "            self.aes_funcs.event.fire_event(",
            "                {",
            "                    \"time\": end - self.stat_clock,",
            "                    \"worker\": self.name,",
            "                    \"stats\": self.stats,",
            "                },",
            "                tagify(self.name, \"stats\"),",
            "            )",
            "            self.stats = collections.defaultdict(lambda: {\"mean\": 0, \"runs\": 0})",
            "            self.stat_clock = end",
            "",
            "    def _handle_clear(self, load):",
            "        \"\"\"",
            "        Process a cleartext command",
            "",
            "        :param dict load: Cleartext payload",
            "        :return: The result of passing the load to a function in ClearFuncs corresponding to",
            "                 the command specified in the load's 'cmd' key.",
            "        \"\"\"",
            "        log.trace(\"Clear payload received with command %s\", load[\"cmd\"])",
            "        cmd = load[\"cmd\"]",
            "        method = self.clear_funcs.get_method(cmd)",
            "        if not method:",
            "            return {}, {\"fun\": \"send_clear\"}",
            "        if self.opts[\"master_stats\"]:",
            "            start = time.time()",
            "            self.stats[cmd][\"runs\"] += 1",
            "        ret = method(load), {\"fun\": \"send_clear\"}",
            "        if self.opts[\"master_stats\"]:",
            "            self._post_stats(start, cmd)",
            "        return ret",
            "",
            "    def _handle_aes(self, data):",
            "        \"\"\"",
            "        Process a command sent via an AES key",
            "",
            "        :param str load: Encrypted payload",
            "        :return: The result of passing the load to a function in AESFuncs corresponding to",
            "                 the command specified in the load's 'cmd' key.",
            "        \"\"\"",
            "        if \"cmd\" not in data:",
            "            log.error(\"Received malformed command %s\", data)",
            "            return {}",
            "        cmd = data[\"cmd\"]",
            "        log.trace(\"AES payload received with command %s\", data[\"cmd\"])",
            "        method = self.aes_funcs.get_method(cmd)",
            "        if not method:",
            "            return {}, {\"fun\": \"send\"}",
            "        if self.opts[\"master_stats\"]:",
            "            start = time.time()",
            "            self.stats[cmd][\"runs\"] += 1",
            "",
            "        def run_func(data):",
            "            return self.aes_funcs.run_func(data[\"cmd\"], data)",
            "",
            "        with StackContext(",
            "            functools.partial(RequestContext, {\"data\": data, \"opts\": self.opts})",
            "        ):",
            "            ret = run_func(data)",
            "",
            "        if self.opts[\"master_stats\"]:",
            "            self._post_stats(start, cmd)",
            "        return ret",
            "",
            "    def run(self):",
            "        \"\"\"",
            "        Start a Master Worker",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.name)",
            "",
            "        # if we inherit req_server level without our own, reset it",
            "        if not salt.utils.platform.is_windows():",
            "            enforce_mworker_niceness = True",
            "            if self.opts[\"req_server_niceness\"]:",
            "                if salt.utils.user.get_user() == \"root\":",
            "                    log.info(",
            "                        \"%s decrementing inherited ReqServer niceness to 0\", self.name",
            "                    )",
            "                    log.info(os.nice())",
            "                    os.nice(-1 * self.opts[\"req_server_niceness\"])",
            "                else:",
            "                    log.error(",
            "                        \"%s unable to decrement niceness for MWorker, not running as root\",",
            "                        self.name,",
            "                    )",
            "                    enforce_mworker_niceness = False",
            "",
            "            # else set what we're explicitly asked for",
            "            if enforce_mworker_niceness and self.opts[\"mworker_niceness\"]:",
            "                log.info(",
            "                    \"setting %s niceness to %i\",",
            "                    self.name,",
            "                    self.opts[\"mworker_niceness\"],",
            "                )",
            "                os.nice(self.opts[\"mworker_niceness\"])",
            "",
            "        self.clear_funcs = ClearFuncs(self.opts, self.key,)",
            "        self.aes_funcs = AESFuncs(self.opts)",
            "        salt.utils.crypt.reinit_crypto()",
            "        self.__bind()",
            "",
            "",
            "class TransportMethods:",
            "    \"\"\"",
            "    Expose methods to the transport layer, methods with their names found in",
            "    the class attribute 'expose_methods' will be exposed to the transport layer",
            "    via 'get_method'.",
            "    \"\"\"",
            "",
            "    expose_methods = ()",
            "",
            "    def get_method(self, name):",
            "        \"\"\"",
            "        Get a method which should be exposed to the transport layer",
            "        \"\"\"",
            "        if name in self.expose_methods:",
            "            try:",
            "                return getattr(self, name)",
            "            except AttributeError:",
            "                log.error(\"Requested method not exposed: %s\", name)",
            "        else:",
            "            log.error(\"Requested method not exposed: %s\", name)",
            "",
            "",
            "# TODO: rename? No longer tied to \"AES\", just \"encrypted\" or \"private\" requests",
            "class AESFuncs(TransportMethods):",
            "    \"\"\"",
            "    Set up functions that are available when the load is encrypted with AES",
            "    \"\"\"",
            "",
            "    expose_methods = (",
            "        \"verify_minion\",",
            "        \"_master_tops\",",
            "        \"_ext_nodes\",",
            "        \"_master_opts\",",
            "        \"_mine_get\",",
            "        \"_mine\",",
            "        \"_mine_delete\",",
            "        \"_mine_flush\",",
            "        \"_file_recv\",",
            "        \"_pillar\",",
            "        \"_minion_event\",",
            "        \"_handle_minion_event\",",
            "        \"_return\",",
            "        \"_syndic_return\",",
            "        \"minion_runner\",",
            "        \"pub_ret\",",
            "        \"minion_pub\",",
            "        \"minion_publish\",",
            "        \"revoke_auth\",",
            "        \"_serve_file\",",
            "        \"_file_find\",",
            "        \"_file_hash\",",
            "        \"_file_hash_and_stat\",",
            "        \"_file_list\",",
            "        \"_file_list_emptydirs\",",
            "        \"_dir_list\",",
            "        \"_symlink_list\",",
            "        \"_file_envs\",",
            "    )",
            "",
            "    def __init__(self, opts):",
            "        \"\"\"",
            "        Create a new AESFuncs",
            "",
            "        :param dict opts: The salt options",
            "",
            "        :rtype: AESFuncs",
            "        :returns: Instance for handling AES operations",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        self.serial = salt.payload.Serial(opts)",
            "        self.ckminions = salt.utils.minions.CkMinions(opts)",
            "        # Make a client",
            "        self.local = salt.client.get_local_client(self.opts[\"conf_file\"])",
            "        # Create the master minion to access the external job cache",
            "        self.mminion = salt.minion.MasterMinion(",
            "            self.opts, states=False, rend=False, ignore_config_errors=True",
            "        )",
            "        self.__setup_fileserver()",
            "        self.masterapi = salt.daemons.masterapi.RemoteFuncs(opts)",
            "",
            "    def __setup_fileserver(self):",
            "        \"\"\"",
            "        Set the local file objects from the file server interface",
            "        \"\"\"",
            "        # Avoid circular import",
            "        import salt.fileserver",
            "",
            "        self.fs_ = salt.fileserver.Fileserver(self.opts)",
            "        self._serve_file = self.fs_.serve_file",
            "        self._file_find = self.fs_._find_file",
            "        self._file_hash = self.fs_.file_hash",
            "        self._file_hash_and_stat = self.fs_.file_hash_and_stat",
            "        self._file_list = self.fs_.file_list",
            "        self._file_list_emptydirs = self.fs_.file_list_emptydirs",
            "        self._dir_list = self.fs_.dir_list",
            "        self._symlink_list = self.fs_.symlink_list",
            "        self._file_envs = self.fs_.file_envs",
            "",
            "    def __verify_minion(self, id_, token):",
            "        \"\"\"",
            "        Take a minion id and a string signed with the minion private key",
            "        The string needs to verify as 'salt' with the minion public key",
            "",
            "        :param str id_: A minion ID",
            "        :param str token: A string signed with the minion private key",
            "",
            "        :rtype: bool",
            "        :return: Boolean indicating whether or not the token can be verified.",
            "        \"\"\"",
            "        if not salt.utils.verify.valid_id(self.opts, id_):",
            "            return False",
            "        pub_path = os.path.join(self.opts[\"pki_dir\"], \"minions\", id_)",
            "",
            "        try:",
            "            pub = salt.crypt.get_rsa_pub_key(pub_path)",
            "        except OSError:",
            "            log.warning(",
            "                \"Salt minion claiming to be %s attempted to communicate with \"",
            "                \"master, but key could not be read and verification was denied.\",",
            "                id_,",
            "            )",
            "            return False",
            "        except (ValueError, IndexError, TypeError) as err:",
            "            log.error('Unable to load public key \"%s\": %s', pub_path, err)",
            "        try:",
            "            if salt.crypt.public_decrypt(pub, token) == b\"salt\":",
            "                return True",
            "        except ValueError as err:",
            "            log.error(\"Unable to decrypt token: %s\", err)",
            "",
            "        log.error(",
            "            \"Salt minion claiming to be %s has attempted to communicate with \"",
            "            \"the master and could not be verified\",",
            "            id_,",
            "        )",
            "        return False",
            "",
            "    def verify_minion(self, id_, token):",
            "        \"\"\"",
            "        Take a minion id and a string signed with the minion private key",
            "        The string needs to verify as 'salt' with the minion public key",
            "",
            "        :param str id_: A minion ID",
            "        :param str token: A string signed with the minion private key",
            "",
            "        :rtype: bool",
            "        :return: Boolean indicating whether or not the token can be verified.",
            "        \"\"\"",
            "        return self.__verify_minion(id_, token)",
            "",
            "    def __verify_minion_publish(self, clear_load):",
            "        \"\"\"",
            "        Verify that the passed information authorized a minion to execute",
            "",
            "        :param dict clear_load: A publication load from a minion",
            "",
            "        :rtype: bool",
            "        :return: A boolean indicating if the minion is allowed to publish the command in the load",
            "        \"\"\"",
            "        # Verify that the load is valid",
            "        if \"peer\" not in self.opts:",
            "            return False",
            "        if not isinstance(self.opts[\"peer\"], dict):",
            "            return False",
            "        if any(",
            "            key not in clear_load for key in (\"fun\", \"arg\", \"tgt\", \"ret\", \"tok\", \"id\")",
            "        ):",
            "            return False",
            "        # If the command will make a recursive publish don't run",
            "        if clear_load[\"fun\"].startswith(\"publish.\"):",
            "            return False",
            "        # Check the permissions for this minion",
            "        if not self.__verify_minion(clear_load[\"id\"], clear_load[\"tok\"]):",
            "            # The minion is not who it says it is!",
            "            # We don't want to listen to it!",
            "            log.warning(",
            "                \"Minion id %s is not who it says it is and is attempting \"",
            "                \"to issue a peer command\",",
            "                clear_load[\"id\"],",
            "            )",
            "            return False",
            "        clear_load.pop(\"tok\")",
            "        perms = []",
            "        for match in self.opts[\"peer\"]:",
            "            if re.match(match, clear_load[\"id\"]):",
            "                # This is the list of funcs/modules!",
            "                if isinstance(self.opts[\"peer\"][match], list):",
            "                    perms.extend(self.opts[\"peer\"][match])",
            "        if \",\" in clear_load[\"fun\"]:",
            "            # 'arg': [['cat', '/proc/cpuinfo'], [], ['foo']]",
            "            clear_load[\"fun\"] = clear_load[\"fun\"].split(\",\")",
            "            arg_ = []",
            "            for arg in clear_load[\"arg\"]:",
            "                arg_.append(arg.split())",
            "            clear_load[\"arg\"] = arg_",
            "",
            "        # finally, check the auth of the load",
            "        return self.ckminions.auth_check(",
            "            perms,",
            "            clear_load[\"fun\"],",
            "            clear_load[\"arg\"],",
            "            clear_load[\"tgt\"],",
            "            clear_load.get(\"tgt_type\", \"glob\"),",
            "            publish_validate=True,",
            "        )",
            "",
            "    def __verify_load(self, load, verify_keys):",
            "        \"\"\"",
            "        A utility function to perform common verification steps.",
            "",
            "        :param dict load: A payload received from a minion",
            "        :param list verify_keys: A list of strings that should be present in a",
            "        given load",
            "",
            "        :rtype: bool",
            "        :rtype: dict",
            "        :return: The original load (except for the token) if the load can be",
            "        verified. False if the load is invalid.",
            "        \"\"\"",
            "        if any(key not in load for key in verify_keys):",
            "            return False",
            "        if \"tok\" not in load:",
            "            log.error(",
            "                \"Received incomplete call from %s for '%s', missing '%s'\",",
            "                load[\"id\"],",
            "                inspect_stack()[\"co_name\"],",
            "                \"tok\",",
            "            )",
            "            return False",
            "        if not self.__verify_minion(load[\"id\"], load[\"tok\"]):",
            "            # The minion is not who it says it is!",
            "            # We don't want to listen to it!",
            "            log.warning(\"Minion id %s is not who it says it is!\", load[\"id\"])",
            "            return False",
            "",
            "        if \"tok\" in load:",
            "            load.pop(\"tok\")",
            "",
            "        return load",
            "",
            "    def _master_tops(self, load):",
            "        \"\"\"",
            "        Return the results from an external node classifier if one is",
            "        specified",
            "",
            "        :param dict load: A payload received from a minion",
            "        :return: The results from an external node classifier",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        return self.masterapi._master_tops(load, skip_verify=True)",
            "",
            "    # Needed so older minions can request master_tops",
            "    _ext_nodes = _master_tops",
            "",
            "    def _master_opts(self, load):",
            "        \"\"\"",
            "        Return the master options to the minion",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: dict",
            "        :return: The master options",
            "        \"\"\"",
            "        mopts = {}",
            "        file_roots = {}",
            "        envs = self._file_envs()",
            "        for saltenv in envs:",
            "            if saltenv not in file_roots:",
            "                file_roots[saltenv] = []",
            "        mopts[\"file_roots\"] = file_roots",
            "        mopts[\"top_file_merging_strategy\"] = self.opts[\"top_file_merging_strategy\"]",
            "        mopts[\"env_order\"] = self.opts[\"env_order\"]",
            "        mopts[\"default_top\"] = self.opts[\"default_top\"]",
            "        if load.get(\"env_only\"):",
            "            return mopts",
            "        mopts[\"renderer\"] = self.opts[\"renderer\"]",
            "        mopts[\"failhard\"] = self.opts[\"failhard\"]",
            "        mopts[\"state_top\"] = self.opts[\"state_top\"]",
            "        mopts[\"state_top_saltenv\"] = self.opts[\"state_top_saltenv\"]",
            "        mopts[\"nodegroups\"] = self.opts[\"nodegroups\"]",
            "        mopts[\"state_auto_order\"] = self.opts[\"state_auto_order\"]",
            "        mopts[\"state_events\"] = self.opts[\"state_events\"]",
            "        mopts[\"state_aggregate\"] = self.opts[\"state_aggregate\"]",
            "        mopts[\"jinja_env\"] = self.opts[\"jinja_env\"]",
            "        mopts[\"jinja_sls_env\"] = self.opts[\"jinja_sls_env\"]",
            "        mopts[\"jinja_lstrip_blocks\"] = self.opts[\"jinja_lstrip_blocks\"]",
            "        mopts[\"jinja_trim_blocks\"] = self.opts[\"jinja_trim_blocks\"]",
            "        return mopts",
            "",
            "    def _mine_get(self, load):",
            "        \"\"\"",
            "        Gathers the data from the specified minions' mine",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: dict",
            "        :return: Mine data from the specified minions",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tgt\", \"fun\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi._mine_get(load, skip_verify=True)",
            "",
            "    def _mine(self, load):",
            "        \"\"\"",
            "        Store the mine data",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: bool",
            "        :return: True if the data has been stored in the mine",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"data\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        return self.masterapi._mine(load, skip_verify=True)",
            "",
            "    def _mine_delete(self, load):",
            "        \"\"\"",
            "        Allow the minion to delete a specific function from its own mine",
            "",
            "        :param dict load: A payload received from a minion",
            "",
            "        :rtype: bool",
            "        :return: Boolean indicating whether or not the given function was deleted from the mine",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"fun\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi._mine_delete(load)",
            "",
            "    def _mine_flush(self, load):",
            "        \"\"\"",
            "        Allow the minion to delete all of its own mine contents",
            "",
            "        :param dict load: A payload received from a minion",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi._mine_flush(load, skip_verify=True)",
            "",
            "    def _file_recv(self, load):",
            "        \"\"\"",
            "        Allows minions to send files to the master, files are sent to the",
            "        master file cache",
            "        \"\"\"",
            "        if any(key not in load for key in (\"id\", \"path\", \"loc\")):",
            "            return False",
            "        if not isinstance(load[\"path\"], list):",
            "            return False",
            "        if not self.opts[\"file_recv\"]:",
            "            return False",
            "        if not salt.utils.verify.valid_id(self.opts, load[\"id\"]):",
            "            return False",
            "        file_recv_max_size = 1024 * 1024 * self.opts[\"file_recv_max_size\"]",
            "",
            "        if \"loc\" in load and load[\"loc\"] < 0:",
            "            log.error(\"Invalid file pointer: load[loc] < 0\")",
            "            return False",
            "",
            "        if len(load[\"data\"]) + load.get(\"loc\", 0) > file_recv_max_size:",
            "            log.error(",
            "                \"file_recv_max_size limit of %d MB exceeded! %s will be \"",
            "                \"truncated. To successfully push this file, adjust \"",
            "                \"file_recv_max_size to an integer (in MB) large enough to \"",
            "                \"accommodate it.\",",
            "                file_recv_max_size,",
            "                load[\"path\"],",
            "            )",
            "            return False",
            "        if \"tok\" not in load:",
            "            log.error(",
            "                \"Received incomplete call from %s for '%s', missing '%s'\",",
            "                load[\"id\"],",
            "                inspect_stack()[\"co_name\"],",
            "                \"tok\",",
            "            )",
            "            return False",
            "        if not self.__verify_minion(load[\"id\"], load[\"tok\"]):",
            "            # The minion is not who it says it is!",
            "            # We don't want to listen to it!",
            "            log.warning(\"Minion id %s is not who it says it is!\", load[\"id\"])",
            "            return {}",
            "        load.pop(\"tok\")",
            "",
            "        # Join path",
            "        sep_path = os.sep.join(load[\"path\"])",
            "",
            "        # Path normalization should have been done by the sending",
            "        # minion but we can't guarantee it. Re-do it here.",
            "        normpath = os.path.normpath(sep_path)",
            "",
            "        # Ensure that this safety check is done after the path",
            "        # have been normalized.",
            "        if os.path.isabs(normpath) or \"../\" in load[\"path\"]:",
            "            # Can overwrite master files!!",
            "            return False",
            "",
            "        cpath = os.path.join(",
            "            self.opts[\"cachedir\"], \"minions\", load[\"id\"], \"files\", normpath",
            "        )",
            "        # One last safety check here",
            "        if not os.path.normpath(cpath).startswith(self.opts[\"cachedir\"]):",
            "            log.warning(",
            "                \"Attempt to write received file outside of master cache \"",
            "                \"directory! Requested path: %s. Access denied.\",",
            "                cpath,",
            "            )",
            "            return False",
            "        cdir = os.path.dirname(cpath)",
            "        if not os.path.isdir(cdir):",
            "            try:",
            "                os.makedirs(cdir)",
            "            except os.error:",
            "                pass",
            "        if os.path.isfile(cpath) and load[\"loc\"] != 0:",
            "            mode = \"ab\"",
            "        else:",
            "            mode = \"wb\"",
            "        with salt.utils.files.fopen(cpath, mode) as fp_:",
            "            if load[\"loc\"]:",
            "                fp_.seek(load[\"loc\"])",
            "",
            "            fp_.write(salt.utils.stringutils.to_bytes(load[\"data\"]))",
            "        return True",
            "",
            "    def _pillar(self, load):",
            "        \"\"\"",
            "        Return the pillar data for the minion",
            "",
            "        :param dict load: Minion payload",
            "",
            "        :rtype: dict",
            "        :return: The pillar data for the minion",
            "        \"\"\"",
            "        if any(key not in load for key in (\"id\", \"grains\")):",
            "            return False",
            "        if not salt.utils.verify.valid_id(self.opts, load[\"id\"]):",
            "            return False",
            "        load[\"grains\"][\"id\"] = load[\"id\"]",
            "",
            "        pillar = salt.pillar.get_pillar(",
            "            self.opts,",
            "            load[\"grains\"],",
            "            load[\"id\"],",
            "            load.get(\"saltenv\", load.get(\"env\")),",
            "            ext=load.get(\"ext\"),",
            "            pillar_override=load.get(\"pillar_override\", {}),",
            "            pillarenv=load.get(\"pillarenv\"),",
            "            extra_minion_data=load.get(\"extra_minion_data\"),",
            "        )",
            "        data = pillar.compile_pillar()",
            "        self.fs_.update_opts()",
            "        if self.opts.get(\"minion_data_cache\", False):",
            "            self.masterapi.cache.store(",
            "                \"minions/{}\".format(load[\"id\"]),",
            "                \"data\",",
            "                {\"grains\": load[\"grains\"], \"pillar\": data},",
            "            )",
            "            if self.opts.get(\"minion_data_cache_events\") is True:",
            "                self.event.fire_event(",
            "                    {\"Minion data cache refresh\": load[\"id\"]},",
            "                    tagify(load[\"id\"], \"refresh\", \"minion\"),",
            "                )",
            "        return data",
            "",
            "    def _minion_event(self, load):",
            "        \"\"\"",
            "        Receive an event from the minion and fire it on the master event",
            "        interface",
            "",
            "        :param dict load: The minion payload",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        # Route to master event bus",
            "        self.masterapi._minion_event(load)",
            "        # Process locally",
            "        self._handle_minion_event(load)",
            "",
            "    def _handle_minion_event(self, load):",
            "        \"\"\"",
            "        Act on specific events from minions",
            "        \"\"\"",
            "        id_ = load[\"id\"]",
            "        if load.get(\"tag\", \"\") == \"_salt_error\":",
            "            log.error(",
            "                \"Received minion error from [%s]: %s\", id_, load[\"data\"][\"message\"]",
            "            )",
            "",
            "        for event in load.get(\"events\", []):",
            "            event_data = event.get(\"data\", {})",
            "            if \"minions\" in event_data:",
            "                jid = event_data.get(\"jid\")",
            "                if not jid:",
            "                    continue",
            "                minions = event_data[\"minions\"]",
            "                try:",
            "                    salt.utils.job.store_minions(",
            "                        self.opts, jid, minions, mminion=self.mminion, syndic_id=id_",
            "                    )",
            "                except (KeyError, salt.exceptions.SaltCacheError) as exc:",
            "                    log.error(",
            "                        \"Could not add minion(s) %s for job %s: %s\", minions, jid, exc",
            "                    )",
            "",
            "    def _return(self, load):",
            "        \"\"\"",
            "        Handle the return data sent from the minions.",
            "",
            "        Takes the return, verifies it and fires it on the master event bus.",
            "        Typically, this event is consumed by the Salt CLI waiting on the other",
            "        end of the event bus but could be heard by any listener on the bus.",
            "",
            "        :param dict load: The minion payload",
            "        \"\"\"",
            "        if self.opts[\"require_minion_sign_messages\"] and \"sig\" not in load:",
            "            log.critical(",
            "                \"_return: Master is requiring minions to sign their \"",
            "                \"messages, but there is no signature in this payload from \"",
            "                \"%s.\",",
            "                load[\"id\"],",
            "            )",
            "            return False",
            "",
            "        if \"sig\" in load:",
            "            log.trace(\"Verifying signed event publish from minion\")",
            "            sig = load.pop(\"sig\")",
            "            this_minion_pubkey = os.path.join(",
            "                self.opts[\"pki_dir\"], \"minions/{}\".format(load[\"id\"])",
            "            )",
            "            serialized_load = salt.serializers.msgpack.serialize(load)",
            "            if not salt.crypt.verify_signature(",
            "                this_minion_pubkey, serialized_load, sig",
            "            ):",
            "                log.info(\"Failed to verify event signature from minion %s.\", load[\"id\"])",
            "                if self.opts[\"drop_messages_signature_fail\"]:",
            "                    log.critical(",
            "                        \"drop_messages_signature_fail is enabled, dropping \"",
            "                        \"message from %s\",",
            "                        load[\"id\"],",
            "                    )",
            "                    return False",
            "                else:",
            "                    log.info(",
            "                        \"But 'drop_message_signature_fail' is disabled, so message is still accepted.\"",
            "                    )",
            "            load[\"sig\"] = sig",
            "",
            "        try:",
            "            salt.utils.job.store_job(",
            "                self.opts, load, event=self.event, mminion=self.mminion",
            "            )",
            "        except salt.exceptions.SaltCacheError:",
            "            log.error(\"Could not store job information for load: %s\", load)",
            "",
            "    def _syndic_return(self, load):",
            "        \"\"\"",
            "        Receive a syndic minion return and format it to look like returns from",
            "        individual minions.",
            "",
            "        :param dict load: The minion payload",
            "        \"\"\"",
            "        loads = load.get(\"load\")",
            "        if not isinstance(loads, list):",
            "            loads = [load]  # support old syndics not aggregating returns",
            "        for load in loads:",
            "            # Verify the load",
            "            if any(key not in load for key in (\"return\", \"jid\", \"id\")):",
            "                continue",
            "            # if we have a load, save it",
            "            if load.get(\"load\"):",
            "                fstr = \"{}.save_load\".format(self.opts[\"master_job_cache\"])",
            "                self.mminion.returners[fstr](load[\"jid\"], load[\"load\"])",
            "",
            "            # Register the syndic",
            "            syndic_cache_path = os.path.join(",
            "                self.opts[\"cachedir\"], \"syndics\", load[\"id\"]",
            "            )",
            "            if not os.path.exists(syndic_cache_path):",
            "                path_name = os.path.split(syndic_cache_path)[0]",
            "                if not os.path.exists(path_name):",
            "                    os.makedirs(path_name)",
            "                with salt.utils.files.fopen(syndic_cache_path, \"w\") as wfh:",
            "                    wfh.write(\"\")",
            "",
            "            # Format individual return loads",
            "            for key, item in load[\"return\"].items():",
            "                ret = {\"jid\": load[\"jid\"], \"id\": key}",
            "                ret.update(item)",
            "                if \"master_id\" in load:",
            "                    ret[\"master_id\"] = load[\"master_id\"]",
            "                if \"fun\" in load:",
            "                    ret[\"fun\"] = load[\"fun\"]",
            "                if \"arg\" in load:",
            "                    ret[\"fun_args\"] = load[\"arg\"]",
            "                if \"out\" in load:",
            "                    ret[\"out\"] = load[\"out\"]",
            "                if \"sig\" in load:",
            "                    ret[\"sig\"] = load[\"sig\"]",
            "                self._return(ret)",
            "",
            "    def minion_runner(self, clear_load):",
            "        \"\"\"",
            "        Execute a runner from a minion, return the runner's function data",
            "",
            "        :param dict clear_load: The minion payload",
            "",
            "        :rtype: dict",
            "        :return: The runner function data",
            "        \"\"\"",
            "        load = self.__verify_load(clear_load, (\"fun\", \"arg\", \"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        else:",
            "            return self.masterapi.minion_runner(clear_load)",
            "",
            "    def pub_ret(self, load):",
            "        \"\"\"",
            "        Request the return data from a specific jid, only allowed",
            "        if the requesting minion also initialted the execution.",
            "",
            "        :param dict load: The minion payload",
            "",
            "        :rtype: dict",
            "        :return: Return data corresponding to a given JID",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"jid\", \"id\", \"tok\"))",
            "        if load is False:",
            "            return {}",
            "        # Check that this minion can access this data",
            "        auth_cache = os.path.join(self.opts[\"cachedir\"], \"publish_auth\")",
            "        if not os.path.isdir(auth_cache):",
            "            os.makedirs(auth_cache)",
            "        jid_fn = os.path.join(auth_cache, str(load[\"jid\"]))",
            "        with salt.utils.files.fopen(jid_fn, \"r\") as fp_:",
            "            if not load[\"id\"] == fp_.read():",
            "                return {}",
            "        # Grab the latest and return",
            "        return self.local.get_cache_returns(load[\"jid\"])",
            "",
            "    def minion_pub(self, clear_load):",
            "        \"\"\"",
            "        Publish a command initiated from a minion, this method executes minion",
            "        restrictions so that the minion publication will only work if it is",
            "        enabled in the config.",
            "",
            "        The configuration on the master allows minions to be matched to",
            "        salt functions, so the minions can only publish allowed salt functions",
            "",
            "        The config will look like this:",
            "",
            "        .. code-block:: bash",
            "",
            "            peer:",
            "                .*:",
            "                    - .*",
            "",
            "        This configuration will enable all minions to execute all commands:",
            "",
            "        .. code-block:: bash",
            "",
            "            peer:",
            "                foo.example.com:",
            "                    - test.*",
            "",
            "        The above configuration will only allow the minion foo.example.com to",
            "        execute commands from the test module.",
            "",
            "        :param dict clear_load: The minion pay",
            "        \"\"\"",
            "        if not self.__verify_minion_publish(clear_load):",
            "            return {}",
            "        else:",
            "            return self.masterapi.minion_pub(clear_load)",
            "",
            "    def minion_publish(self, clear_load):",
            "        \"\"\"",
            "        Publish a command initiated from a minion, this method executes minion",
            "        restrictions so that the minion publication will only work if it is",
            "        enabled in the config.",
            "",
            "        The configuration on the master allows minions to be matched to",
            "        salt functions, so the minions can only publish allowed salt functions",
            "",
            "        The config will look like this:",
            "",
            "        .. code-block:: bash",
            "",
            "            peer:",
            "                .*:",
            "                    - .*",
            "",
            "        This configuration will enable all minions to execute all commands.",
            "        peer:",
            "",
            "        .. code-block:: bash",
            "",
            "            foo.example.com:",
            "                - test.*",
            "",
            "        The above configuration will only allow the minion foo.example.com to",
            "        execute commands from the test module.",
            "",
            "        :param dict clear_load: The minion payload",
            "        \"\"\"",
            "        if not self.__verify_minion_publish(clear_load):",
            "            return {}",
            "        else:",
            "            return self.masterapi.minion_publish(clear_load)",
            "",
            "    def revoke_auth(self, load):",
            "        \"\"\"",
            "        Allow a minion to request revocation of its own key",
            "",
            "        :param dict load: The minion payload",
            "",
            "        :rtype: dict",
            "        :return: If the load is invalid, it may be returned. No key operation is performed.",
            "",
            "        :rtype: bool",
            "        :return: True if key was revoked, False if not",
            "        \"\"\"",
            "        load = self.__verify_load(load, (\"id\", \"tok\"))",
            "",
            "        if not self.opts.get(\"allow_minion_key_revoke\", False):",
            "            log.warning(",
            "                \"Minion %s requested key revoke, but allow_minion_key_revoke \"",
            "                \"is set to False\",",
            "                load[\"id\"],",
            "            )",
            "            return load",
            "",
            "        if load is False:",
            "            return load",
            "        else:",
            "            return self.masterapi.revoke_auth(load)",
            "",
            "    def run_func(self, func, load):",
            "        \"\"\"",
            "        Wrapper for running functions executed with AES encryption",
            "",
            "        :param function func: The function to run",
            "        :return: The result of the master function that was called",
            "        \"\"\"",
            "        # Don't honor private functions",
            "        if func.startswith(\"__\"):",
            "            # TODO: return some error? Seems odd to return {}",
            "            return {}, {\"fun\": \"send\"}",
            "        # Run the func",
            "        if hasattr(self, func):",
            "            try:",
            "                start = time.time()",
            "                ret = getattr(self, func)(load)",
            "                log.trace(",
            "                    \"Master function call %s took %s seconds\", func, time.time() - start",
            "                )",
            "            except Exception:  # pylint: disable=broad-except",
            "                ret = \"\"",
            "                log.error(\"Error in function %s:\\n\", func, exc_info=True)",
            "        else:",
            "            log.error(",
            "                \"Received function %s which is unavailable on the master, \"",
            "                \"returning False\",",
            "                func,",
            "            )",
            "            return False, {\"fun\": \"send\"}",
            "        # Don't encrypt the return value for the _return func",
            "        # (we don't care about the return value, so why encrypt it?)",
            "        if func == \"_return\":",
            "            return ret, {\"fun\": \"send\"}",
            "        if func == \"_pillar\" and \"id\" in load:",
            "            if load.get(\"ver\") != \"2\" and self.opts[\"pillar_version\"] == 1:",
            "                # Authorized to return old pillar proto",
            "                return ret, {\"fun\": \"send\"}",
            "            return ret, {\"fun\": \"send_private\", \"key\": \"pillar\", \"tgt\": load[\"id\"]}",
            "        # Encrypt the return",
            "        return ret, {\"fun\": \"send\"}",
            "",
            "",
            "class ClearFuncs(TransportMethods):",
            "    \"\"\"",
            "    Set up functions that are safe to execute when commands sent to the master",
            "    without encryption and authentication",
            "    \"\"\"",
            "",
            "    # These methods will be exposed to the transport layer by",
            "    # MWorker._handle_clear",
            "    expose_methods = (",
            "        \"ping\",",
            "        \"publish\",",
            "        \"get_token\",",
            "        \"mk_token\",",
            "        \"wheel\",",
            "        \"runner\",",
            "    )",
            "",
            "    # The ClearFuncs object encapsulates the functions that can be executed in",
            "    # the clear:",
            "    # publish (The publish from the LocalClient)",
            "    # _auth",
            "    def __init__(self, opts, key):",
            "        self.opts = opts",
            "        self.key = key",
            "        # Create the event manager",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        # Make a client",
            "        self.local = salt.client.get_local_client(self.opts[\"conf_file\"])",
            "        # Make an minion checker object",
            "        self.ckminions = salt.utils.minions.CkMinions(opts)",
            "        # Make an Auth object",
            "        self.loadauth = salt.auth.LoadAuth(opts)",
            "        # Stand up the master Minion to access returner data",
            "        self.mminion = salt.minion.MasterMinion(",
            "            self.opts, states=False, rend=False, ignore_config_errors=True",
            "        )",
            "        # Make a wheel object",
            "        self.wheel_ = salt.wheel.Wheel(opts)",
            "        # Make a masterapi object",
            "        self.masterapi = salt.daemons.masterapi.LocalFuncs(opts, key)",
            "",
            "    def runner(self, clear_load):",
            "        \"\"\"",
            "        Send a master control function back to the runner system",
            "        \"\"\"",
            "        # All runner ops pass through eauth",
            "        auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)",
            "",
            "        # Authenticate",
            "        auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)",
            "        error = auth_check.get(\"error\")",
            "",
            "        if error:",
            "            # Authentication error occurred: do not continue.",
            "            return {\"error\": error}",
            "",
            "        # Authorize",
            "        username = auth_check.get(\"username\")",
            "        if auth_type != \"user\":",
            "            runner_check = self.ckminions.runner_check(",
            "                auth_check.get(\"auth_list\", []),",
            "                clear_load[\"fun\"],",
            "                clear_load.get(\"kwarg\", {}),",
            "            )",
            "            if not runner_check:",
            "                return {",
            "                    \"error\": {",
            "                        \"name\": err_name,",
            "                        \"message\": 'Authentication failure of type \"{}\" occurred for '",
            "                        \"user {}.\".format(auth_type, username),",
            "                    }",
            "                }",
            "            elif isinstance(runner_check, dict) and \"error\" in runner_check:",
            "                # A dictionary with an error name/message was handled by ckminions.runner_check",
            "                return runner_check",
            "",
            "            # No error occurred, consume sensitive settings from the clear_load if passed.",
            "            for item in sensitive_load_keys:",
            "                clear_load.pop(item, None)",
            "        else:",
            "            if \"user\" in clear_load:",
            "                username = clear_load[\"user\"]",
            "                if salt.auth.AuthUser(username).is_sudo():",
            "                    username = self.opts.get(\"user\", \"root\")",
            "            else:",
            "                username = salt.utils.user.get_user()",
            "",
            "        # Authorized. Do the job!",
            "        try:",
            "            fun = clear_load.pop(\"fun\")",
            "            runner_client = salt.runner.RunnerClient(self.opts)",
            "            return runner_client.asynchronous(",
            "                fun, clear_load.get(\"kwarg\", {}), username, local=True",
            "            )",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception occurred while introspecting %s: %s\", fun, exc)",
            "            return {",
            "                \"error\": {",
            "                    \"name\": exc.__class__.__name__,",
            "                    \"args\": exc.args,",
            "                    \"message\": str(exc),",
            "                }",
            "            }",
            "",
            "    def wheel(self, clear_load):",
            "        \"\"\"",
            "        Send a master control function back to the wheel system",
            "        \"\"\"",
            "        # All wheel ops pass through eauth",
            "        auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)",
            "",
            "        # Authenticate",
            "        auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)",
            "        error = auth_check.get(\"error\")",
            "",
            "        if error:",
            "            # Authentication error occurred: do not continue.",
            "            return {\"error\": error}",
            "",
            "        # Authorize",
            "        username = auth_check.get(\"username\")",
            "        if auth_type != \"user\":",
            "            wheel_check = self.ckminions.wheel_check(",
            "                auth_check.get(\"auth_list\", []),",
            "                clear_load[\"fun\"],",
            "                clear_load.get(\"kwarg\", {}),",
            "            )",
            "            if not wheel_check:",
            "                return {",
            "                    \"error\": {",
            "                        \"name\": err_name,",
            "                        \"message\": 'Authentication failure of type \"{}\" occurred for '",
            "                        \"user {}.\".format(auth_type, username),",
            "                    }",
            "                }",
            "            elif isinstance(wheel_check, dict) and \"error\" in wheel_check:",
            "                # A dictionary with an error name/message was handled by ckminions.wheel_check",
            "                return wheel_check",
            "",
            "            # No error occurred, consume sensitive settings from the clear_load if passed.",
            "            for item in sensitive_load_keys:",
            "                clear_load.pop(item, None)",
            "        else:",
            "            if \"user\" in clear_load:",
            "                username = clear_load[\"user\"]",
            "                if salt.auth.AuthUser(username).is_sudo():",
            "                    username = self.opts.get(\"user\", \"root\")",
            "            else:",
            "                username = salt.utils.user.get_user()",
            "",
            "        # Authorized. Do the job!",
            "        try:",
            "            jid = salt.utils.jid.gen_jid(self.opts)",
            "            fun = clear_load.pop(\"fun\")",
            "            tag = tagify(jid, prefix=\"wheel\")",
            "            data = {",
            "                \"fun\": \"wheel.{}\".format(fun),",
            "                \"jid\": jid,",
            "                \"tag\": tag,",
            "                \"user\": username,",
            "            }",
            "",
            "            self.event.fire_event(data, tagify([jid, \"new\"], \"wheel\"))",
            "            ret = self.wheel_.call_func(fun, full_return=True, **clear_load)",
            "            data[\"return\"] = ret[\"return\"]",
            "            data[\"success\"] = ret[\"success\"]",
            "            self.event.fire_event(data, tagify([jid, \"ret\"], \"wheel\"))",
            "            return {\"tag\": tag, \"data\": data}",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception occurred while introspecting %s: %s\", fun, exc)",
            "            data[\"return\"] = \"Exception occurred in wheel {}: {}: {}\".format(",
            "                fun, exc.__class__.__name__, exc,",
            "            )",
            "            data[\"success\"] = False",
            "            self.event.fire_event(data, tagify([jid, \"ret\"], \"wheel\"))",
            "            return {\"tag\": tag, \"data\": data}",
            "",
            "    def mk_token(self, clear_load):",
            "        \"\"\"",
            "        Create and return an authentication token, the clear load needs to",
            "        contain the eauth key and the needed authentication creds.",
            "        \"\"\"",
            "        token = self.loadauth.mk_token(clear_load)",
            "        if not token:",
            "            log.warning('Authentication failure of type \"eauth\" occurred.')",
            "            return \"\"",
            "        return token",
            "",
            "    def get_token(self, clear_load):",
            "        \"\"\"",
            "        Return the name associated with a token or False if the token is invalid",
            "        \"\"\"",
            "        if \"token\" not in clear_load:",
            "            return False",
            "        return self.loadauth.get_tok(clear_load[\"token\"])",
            "",
            "    def publish(self, clear_load):",
            "        \"\"\"",
            "        This method sends out publications to the minions, it can only be used",
            "        by the LocalClient.",
            "        \"\"\"",
            "        extra = clear_load.get(\"kwargs\", {})",
            "",
            "        publisher_acl = salt.acl.PublisherACL(self.opts[\"publisher_acl_blacklist\"])",
            "",
            "        if publisher_acl.user_is_blacklisted(",
            "            clear_load[\"user\"]",
            "        ) or publisher_acl.cmd_is_blacklisted(clear_load[\"fun\"]):",
            "            log.error(",
            "                \"%s does not have permissions to run %s. Please contact \"",
            "                \"your local administrator if you believe this is in \"",
            "                \"error.\\n\",",
            "                clear_load[\"user\"],",
            "                clear_load[\"fun\"],",
            "            )",
            "            return {",
            "                \"error\": {",
            "                    \"name\": \"AuthorizationError\",",
            "                    \"message\": \"Authorization error occurred.\",",
            "                }",
            "            }",
            "",
            "        # Retrieve the minions list",
            "        delimiter = clear_load.get(\"kwargs\", {}).get(\"delimiter\", DEFAULT_TARGET_DELIM)",
            "        _res = self.ckminions.check_minions(",
            "            clear_load[\"tgt\"], clear_load.get(\"tgt_type\", \"glob\"), delimiter",
            "        )",
            "        minions = _res.get(\"minions\", list())",
            "        missing = _res.get(\"missing\", list())",
            "        ssh_minions = _res.get(\"ssh_minions\", False)",
            "",
            "        # Check for external auth calls and authenticate",
            "        auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(extra)",
            "        if auth_type == \"user\":",
            "            auth_check = self.loadauth.check_authentication(",
            "                clear_load, auth_type, key=key",
            "            )",
            "        else:",
            "            auth_check = self.loadauth.check_authentication(extra, auth_type)",
            "",
            "        # Setup authorization list variable and error information",
            "        auth_list = auth_check.get(\"auth_list\", [])",
            "        err_msg = 'Authentication failure of type \"{}\" occurred.'.format(auth_type)",
            "",
            "        if auth_check.get(\"error\"):",
            "            # Authentication error occurred: do not continue.",
            "            log.warning(err_msg)",
            "            return {",
            "                \"error\": {",
            "                    \"name\": \"AuthenticationError\",",
            "                    \"message\": \"Authentication error occurred.\",",
            "                }",
            "            }",
            "",
            "        # All Token, Eauth, and non-root users must pass the authorization check",
            "        if auth_type != \"user\" or (auth_type == \"user\" and auth_list):",
            "            # Authorize the request",
            "            authorized = self.ckminions.auth_check(",
            "                auth_list,",
            "                clear_load[\"fun\"],",
            "                clear_load[\"arg\"],",
            "                clear_load[\"tgt\"],",
            "                clear_load.get(\"tgt_type\", \"glob\"),",
            "                minions=minions,",
            "                # always accept find_job",
            "                whitelist=[\"saltutil.find_job\"],",
            "            )",
            "",
            "            if not authorized:",
            "                # Authorization error occurred. Do not continue.",
            "                if (",
            "                    auth_type == \"eauth\"",
            "                    and not auth_list",
            "                    and \"username\" in extra",
            "                    and \"eauth\" in extra",
            "                ):",
            "                    log.debug(",
            "                        'Auth configuration for eauth \"%s\" and user \"%s\" is empty',",
            "                        extra[\"eauth\"],",
            "                        extra[\"username\"],",
            "                    )",
            "                log.warning(err_msg)",
            "                return {",
            "                    \"error\": {",
            "                        \"name\": \"AuthorizationError\",",
            "                        \"message\": \"Authorization error occurred.\",",
            "                    }",
            "                }",
            "",
            "            # Perform some specific auth_type tasks after the authorization check",
            "            if auth_type == \"token\":",
            "                username = auth_check.get(\"username\")",
            "                clear_load[\"user\"] = username",
            "                log.debug('Minion tokenized user = \"%s\"', username)",
            "            elif auth_type == \"eauth\":",
            "                # The username we are attempting to auth with",
            "                clear_load[\"user\"] = self.loadauth.load_name(extra)",
            "",
            "        # If we order masters (via a syndic), don't short circuit if no minions",
            "        # are found",
            "        if not self.opts.get(\"order_masters\"):",
            "            # Check for no minions",
            "            if not minions:",
            "                return {",
            "                    \"enc\": \"clear\",",
            "                    \"load\": {",
            "                        \"jid\": None,",
            "                        \"minions\": minions,",
            "                        \"error\": \"Master could not resolve minions for target {}\".format(",
            "                            clear_load[\"tgt\"]",
            "                        ),",
            "                    },",
            "                }",
            "        jid = self._prep_jid(clear_load, extra)",
            "        if jid is None:",
            "            return {\"enc\": \"clear\", \"load\": {\"error\": \"Master failed to assign jid\"}}",
            "        payload = self._prep_pub(minions, jid, clear_load, extra, missing)",
            "",
            "        # Send it!",
            "        self._send_ssh_pub(payload, ssh_minions=ssh_minions)",
            "        self._send_pub(payload)",
            "",
            "        return {",
            "            \"enc\": \"clear\",",
            "            \"load\": {\"jid\": clear_load[\"jid\"], \"minions\": minions, \"missing\": missing},",
            "        }",
            "",
            "    def _prep_auth_info(self, clear_load):",
            "        sensitive_load_keys = []",
            "        key = None",
            "        if \"token\" in clear_load:",
            "            auth_type = \"token\"",
            "            err_name = \"TokenAuthenticationError\"",
            "            sensitive_load_keys = [\"token\"]",
            "        elif \"eauth\" in clear_load:",
            "            auth_type = \"eauth\"",
            "            err_name = \"EauthAuthenticationError\"",
            "            sensitive_load_keys = [\"username\", \"password\"]",
            "        else:",
            "            auth_type = \"user\"",
            "            err_name = \"UserAuthenticationError\"",
            "            key = self.key",
            "",
            "        return auth_type, err_name, key, sensitive_load_keys",
            "",
            "    def _prep_jid(self, clear_load, extra):",
            "        \"\"\"",
            "        Return a jid for this publication",
            "        \"\"\"",
            "        # the jid in clear_load can be None, '', or something else. this is an",
            "        # attempt to clean up the value before passing to plugins",
            "        passed_jid = clear_load[\"jid\"] if clear_load.get(\"jid\") else None",
            "        nocache = extra.get(\"nocache\", False)",
            "",
            "        # Retrieve the jid",
            "        fstr = \"{}.prep_jid\".format(self.opts[\"master_job_cache\"])",
            "        try:",
            "            # Retrieve the jid",
            "            jid = self.mminion.returners[fstr](nocache=nocache, passed_jid=passed_jid)",
            "        except (KeyError, TypeError):",
            "            # The returner is not present",
            "            msg = (",
            "                \"Failed to allocate a jid. The requested returner '{}' \"",
            "                \"could not be loaded.\".format(fstr.split(\".\")[0])",
            "            )",
            "            log.error(msg)",
            "            return {\"error\": msg}",
            "        return jid",
            "",
            "    def _send_pub(self, load):",
            "        \"\"\"",
            "        Take a load and send it across the network to connected minions",
            "        \"\"\"",
            "        for transport, opts in iter_transport_opts(self.opts):",
            "            chan = salt.transport.server.PubServerChannel.factory(opts)",
            "            chan.publish(load)",
            "",
            "    @property",
            "    def ssh_client(self):",
            "        if not hasattr(self, \"_ssh_client\"):",
            "            self._ssh_client = salt.client.ssh.client.SSHClient(mopts=self.opts)",
            "        return self._ssh_client",
            "",
            "    def _send_ssh_pub(self, load, ssh_minions=False):",
            "        \"\"\"",
            "        Take a load and send it across the network to ssh minions",
            "        \"\"\"",
            "        if self.opts[\"enable_ssh_minions\"] is True and ssh_minions is True:",
            "            log.debug(\"Send payload to ssh minions\")",
            "            threading.Thread(target=self.ssh_client.cmd, kwargs=load).start()",
            "",
            "    def _prep_pub(self, minions, jid, clear_load, extra, missing):",
            "        \"\"\"",
            "        Take a given load and perform the necessary steps",
            "        to prepare a publication.",
            "",
            "        TODO: This is really only bound by temporal cohesion",
            "        and thus should be refactored even further.",
            "        \"\"\"",
            "        clear_load[\"jid\"] = jid",
            "        delimiter = clear_load.get(\"kwargs\", {}).get(\"delimiter\", DEFAULT_TARGET_DELIM)",
            "",
            "        # TODO Error reporting over the master event bus",
            "        self.event.fire_event({\"minions\": minions}, clear_load[\"jid\"])",
            "        new_job_load = {",
            "            \"jid\": clear_load[\"jid\"],",
            "            \"tgt_type\": clear_load[\"tgt_type\"],",
            "            \"tgt\": clear_load[\"tgt\"],",
            "            \"user\": clear_load[\"user\"],",
            "            \"fun\": clear_load[\"fun\"],",
            "            \"arg\": clear_load[\"arg\"],",
            "            \"minions\": minions,",
            "            \"missing\": missing,",
            "        }",
            "",
            "        # Announce the job on the event bus",
            "        self.event.fire_event(new_job_load, tagify([clear_load[\"jid\"], \"new\"], \"job\"))",
            "",
            "        if self.opts[\"ext_job_cache\"]:",
            "            fstr = \"{}.save_load\".format(self.opts[\"ext_job_cache\"])",
            "            save_load_func = True",
            "",
            "            # Get the returner's save_load arg_spec.",
            "            try:",
            "                arg_spec = salt.utils.args.get_function_argspec(",
            "                    self.mminion.returners[fstr]",
            "                )",
            "",
            "                # Check if 'minions' is included in returner's save_load arg_spec.",
            "                # This may be missing in custom returners, which we should warn about.",
            "                if \"minions\" not in arg_spec.args:",
            "                    log.critical(",
            "                        \"The specified returner used for the external job cache \"",
            "                        \"'%s' does not have a 'minions' kwarg in the returner's \"",
            "                        \"save_load function.\",",
            "                        self.opts[\"ext_job_cache\"],",
            "                    )",
            "            except (AttributeError, KeyError):",
            "                save_load_func = False",
            "                log.critical(",
            "                    \"The specified returner used for the external job cache \"",
            "                    '\"%s\" does not have a save_load function!',",
            "                    self.opts[\"ext_job_cache\"],",
            "                )",
            "",
            "            if save_load_func:",
            "                try:",
            "                    self.mminion.returners[fstr](",
            "                        clear_load[\"jid\"], clear_load, minions=minions",
            "                    )",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.critical(",
            "                        \"The specified returner threw a stack trace:\\n\", exc_info=True",
            "                    )",
            "",
            "        # always write out to the master job caches",
            "        try:",
            "            fstr = \"{}.save_load\".format(self.opts[\"master_job_cache\"])",
            "            self.mminion.returners[fstr](clear_load[\"jid\"], clear_load, minions)",
            "        except KeyError:",
            "            log.critical(",
            "                \"The specified returner used for the master job cache \"",
            "                '\"%s\" does not have a save_load function!',",
            "                self.opts[\"master_job_cache\"],",
            "            )",
            "        except Exception:  # pylint: disable=broad-except",
            "            log.critical(\"The specified returner threw a stack trace:\\n\", exc_info=True)",
            "        # Set up the payload",
            "        payload = {\"enc\": \"aes\"}",
            "        # Altering the contents of the publish load is serious!! Changes here",
            "        # break compatibility with minion/master versions and even tiny",
            "        # additions can have serious implications on the performance of the",
            "        # publish commands.",
            "        #",
            "        # In short, check with Thomas Hatch before you even think about",
            "        # touching this stuff, we can probably do what you want to do another",
            "        # way that won't have a negative impact.",
            "        load = {",
            "            \"fun\": clear_load[\"fun\"],",
            "            \"arg\": clear_load[\"arg\"],",
            "            \"tgt\": clear_load[\"tgt\"],",
            "            \"jid\": clear_load[\"jid\"],",
            "            \"ret\": clear_load[\"ret\"],",
            "        }",
            "        # if you specified a master id, lets put that in the load",
            "        if \"master_id\" in self.opts:",
            "            load[\"master_id\"] = self.opts[\"master_id\"]",
            "        # if someone passed us one, use that",
            "        if \"master_id\" in extra:",
            "            load[\"master_id\"] = extra[\"master_id\"]",
            "        # Only add the delimiter to the pub data if it is non-default",
            "        if delimiter != DEFAULT_TARGET_DELIM:",
            "            load[\"delimiter\"] = delimiter",
            "",
            "        if \"id\" in extra:",
            "            load[\"id\"] = extra[\"id\"]",
            "        if \"tgt_type\" in clear_load:",
            "            load[\"tgt_type\"] = clear_load[\"tgt_type\"]",
            "        if \"to\" in clear_load:",
            "            load[\"to\"] = clear_load[\"to\"]",
            "",
            "        if \"kwargs\" in clear_load:",
            "            if \"ret_config\" in clear_load[\"kwargs\"]:",
            "                load[\"ret_config\"] = clear_load[\"kwargs\"].get(\"ret_config\")",
            "",
            "            if \"metadata\" in clear_load[\"kwargs\"]:",
            "                load[\"metadata\"] = clear_load[\"kwargs\"].get(\"metadata\")",
            "",
            "            if \"module_executors\" in clear_load[\"kwargs\"]:",
            "                load[\"module_executors\"] = clear_load[\"kwargs\"].get(\"module_executors\")",
            "",
            "            if \"executor_opts\" in clear_load[\"kwargs\"]:",
            "                load[\"executor_opts\"] = clear_load[\"kwargs\"].get(\"executor_opts\")",
            "",
            "            if \"ret_kwargs\" in clear_load[\"kwargs\"]:",
            "                load[\"ret_kwargs\"] = clear_load[\"kwargs\"].get(\"ret_kwargs\")",
            "",
            "        if \"user\" in clear_load:",
            "            log.info(",
            "                \"User %s Published command %s with jid %s\",",
            "                clear_load[\"user\"],",
            "                clear_load[\"fun\"],",
            "                clear_load[\"jid\"],",
            "            )",
            "            load[\"user\"] = clear_load[\"user\"]",
            "        else:",
            "            log.info(",
            "                \"Published command %s with jid %s\", clear_load[\"fun\"], clear_load[\"jid\"]",
            "            )",
            "        log.debug(\"Published command details %s\", load)",
            "        return load",
            "",
            "    def ping(self, clear_load):",
            "        \"\"\"",
            "        Send the load back to the sender.",
            "        \"\"\"",
            "        return clear_load"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "315": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "316": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "317": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "318": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "319": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "320": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "321": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "322": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "323": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "324": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "326": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "327": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "328": [
                "Maintenance",
                "handle_key_rotate"
            ],
            "329": [
                "Maintenance",
                "handle_key_rotate"
            ]
        },
        "addLocation": [
            "shuup.front.urls",
            "salt.master.SMaster.self",
            "salt.master.SMaster.secrets",
            "salt.master.Maintenance.handle_key_rotate"
        ]
    },
    "salt/minion.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1660,
                "afterPatchRowNumber": 1660,
                "PatchRowcode": "         Override this method if you wish to handle the decoded data"
            },
            "1": {
                "beforePatchRowNumber": 1661,
                "afterPatchRowNumber": 1661,
                "PatchRowcode": "         differently."
            },
            "2": {
                "beforePatchRowNumber": 1662,
                "afterPatchRowNumber": 1662,
                "PatchRowcode": "         \"\"\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1663,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": 1663,
                "afterPatchRowNumber": 1664,
                "PatchRowcode": "         # Ensure payload is unicode. Disregard failure to decode binary blobs."
            },
            "5": {
                "beforePatchRowNumber": 1664,
                "afterPatchRowNumber": 1665,
                "PatchRowcode": "         if six.PY2:"
            },
            "6": {
                "beforePatchRowNumber": 1665,
                "afterPatchRowNumber": 1666,
                "PatchRowcode": "             data = salt.utils.data.decode(data, keep=True)"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Routines to set up a minion",
            "\"\"\"",
            "",
            "import contextlib",
            "import copy",
            "import functools",
            "import logging",
            "import multiprocessing",
            "import os",
            "import random",
            "import signal",
            "import sys",
            "import threading",
            "import time",
            "import traceback",
            "import types",
            "from binascii import crc32",
            "from random import randint, shuffle",
            "from stat import S_IMODE",
            "",
            "import salt",
            "import salt.beacons",
            "import salt.cli.daemons",
            "import salt.client",
            "import salt.crypt",
            "import salt.defaults.events",
            "import salt.defaults.exitcodes",
            "import salt.engines",
            "",
            "# pylint: enable=no-name-in-module,redefined-builtin",
            "import salt.ext.tornado",
            "import salt.ext.tornado.gen  # pylint: disable=F0401",
            "import salt.ext.tornado.ioloop  # pylint: disable=F0401",
            "import salt.loader",
            "import salt.log.setup",
            "import salt.payload",
            "import salt.pillar",
            "import salt.serializers.msgpack",
            "import salt.syspaths",
            "import salt.transport.client",
            "import salt.utils.args",
            "import salt.utils.context",
            "import salt.utils.crypt",
            "import salt.utils.data",
            "import salt.utils.dictupdate",
            "import salt.utils.error",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.jid",
            "import salt.utils.minion",
            "import salt.utils.minions",
            "import salt.utils.network",
            "import salt.utils.platform",
            "import salt.utils.process",
            "import salt.utils.schedule",
            "import salt.utils.ssdp",
            "import salt.utils.user",
            "import salt.utils.zeromq",
            "from salt._compat import ipaddress",
            "from salt.config import DEFAULT_MINION_OPTS",
            "from salt.defaults import DEFAULT_TARGET_DELIM",
            "from salt.exceptions import (",
            "    CommandExecutionError,",
            "    CommandNotFoundError,",
            "    SaltClientError,",
            "    SaltDaemonNotRunning,",
            "    SaltException,",
            "    SaltInvocationError,",
            "    SaltMasterUnresolvableError,",
            "    SaltReqTimeoutError,",
            "    SaltSystemExit,",
            ")",
            "",
            "# pylint: disable=import-error,no-name-in-module,redefined-builtin",
            "from salt.ext import six",
            "from salt.ext.six.moves import range",
            "from salt.template import SLS_ENCODING",
            "from salt.utils.ctx import RequestContext",
            "from salt.utils.debug import enable_sigusr1_handler",
            "from salt.utils.event import tagify",
            "from salt.utils.network import parse_host_port",
            "from salt.utils.odict import OrderedDict",
            "from salt.utils.process import ProcessManager, SignalHandlingProcess, default_signals",
            "from salt.utils.zeromq import ZMQ_VERSION_INFO, ZMQDefaultLoop, install_zmq, zmq",
            "",
            "HAS_PSUTIL = False",
            "try:",
            "    import salt.utils.psutil_compat as psutil",
            "",
            "    HAS_PSUTIL = True",
            "except ImportError:",
            "    pass",
            "",
            "HAS_RESOURCE = False",
            "try:",
            "    import resource",
            "",
            "    HAS_RESOURCE = True",
            "except ImportError:",
            "    pass",
            "",
            "try:",
            "    import salt.utils.win_functions",
            "",
            "    HAS_WIN_FUNCTIONS = True",
            "except ImportError:",
            "    HAS_WIN_FUNCTIONS = False",
            "# pylint: enable=import-error",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "# To set up a minion:",
            "# 1. Read in the configuration",
            "# 2. Generate the function mapping dict",
            "# 3. Authenticate with the master",
            "# 4. Store the AES key",
            "# 5. Connect to the publisher",
            "# 6. Handle publications",
            "",
            "",
            "def resolve_dns(opts, fallback=True):",
            "    \"\"\"",
            "    Resolves the master_ip and master_uri options",
            "    \"\"\"",
            "    ret = {}",
            "    check_dns = True",
            "    if opts.get(\"file_client\", \"remote\") == \"local\" and not opts.get(",
            "        \"use_master_when_local\", False",
            "    ):",
            "        check_dns = False",
            "    # Since salt.log is imported below, salt.utils.network needs to be imported here as well",
            "    import salt.utils.network",
            "",
            "    if check_dns is True:",
            "        try:",
            "            if opts[\"master\"] == \"\":",
            "                raise SaltSystemExit",
            "            ret[\"master_ip\"] = salt.utils.network.dns_check(",
            "                opts[\"master\"], int(opts[\"master_port\"]), True, opts[\"ipv6\"]",
            "            )",
            "        except SaltClientError:",
            "            retry_dns_count = opts.get(\"retry_dns_count\", None)",
            "            if opts[\"retry_dns\"]:",
            "                while True:",
            "                    if retry_dns_count is not None:",
            "                        if retry_dns_count == 0:",
            "                            raise SaltMasterUnresolvableError",
            "                        retry_dns_count -= 1",
            "                    import salt.log",
            "",
            "                    msg = (",
            "                        \"Master hostname: '{}' not found or not responsive. \"",
            "                        \"Retrying in {} seconds\"",
            "                    ).format(opts[\"master\"], opts[\"retry_dns\"])",
            "                    if salt.log.setup.is_console_configured():",
            "                        log.error(msg)",
            "                    else:",
            "                        print(\"WARNING: {}\".format(msg))",
            "                    time.sleep(opts[\"retry_dns\"])",
            "                    try:",
            "                        ret[\"master_ip\"] = salt.utils.network.dns_check(",
            "                            opts[\"master\"], int(opts[\"master_port\"]), True, opts[\"ipv6\"]",
            "                        )",
            "                        break",
            "                    except SaltClientError:",
            "                        pass",
            "            else:",
            "                if fallback:",
            "                    ret[\"master_ip\"] = \"127.0.0.1\"",
            "                else:",
            "                    raise",
            "        except SaltSystemExit:",
            "            unknown_str = \"unknown address\"",
            "            master = opts.get(\"master\", unknown_str)",
            "            if master == \"\":",
            "                master = unknown_str",
            "            if opts.get(\"__role\") == \"syndic\":",
            "                err = (",
            "                    \"Master address: '{}' could not be resolved. Invalid or unresolveable address. \"",
            "                    \"Set 'syndic_master' value in minion config.\".format(master)",
            "                )",
            "            else:",
            "                err = (",
            "                    \"Master address: '{}' could not be resolved. Invalid or unresolveable address. \"",
            "                    \"Set 'master' value in minion config.\".format(master)",
            "                )",
            "            log.error(err)",
            "            raise SaltSystemExit(code=42, msg=err)",
            "    else:",
            "        ret[\"master_ip\"] = \"127.0.0.1\"",
            "",
            "    if \"master_ip\" in ret and \"master_ip\" in opts:",
            "        if ret[\"master_ip\"] != opts[\"master_ip\"]:",
            "            log.warning(",
            "                \"Master ip address changed from %s to %s\",",
            "                opts[\"master_ip\"],",
            "                ret[\"master_ip\"],",
            "            )",
            "    if opts[\"source_interface_name\"]:",
            "        log.trace(\"Custom source interface required: %s\", opts[\"source_interface_name\"])",
            "        interfaces = salt.utils.network.interfaces()",
            "        log.trace(\"The following interfaces are available on this Minion:\")",
            "        log.trace(interfaces)",
            "        if opts[\"source_interface_name\"] in interfaces:",
            "            if interfaces[opts[\"source_interface_name\"]][\"up\"]:",
            "                addrs = (",
            "                    interfaces[opts[\"source_interface_name\"]][\"inet\"]",
            "                    if not opts[\"ipv6\"]",
            "                    else interfaces[opts[\"source_interface_name\"]][\"inet6\"]",
            "                )",
            "                ret[\"source_ip\"] = addrs[0][\"address\"]",
            "                log.debug(\"Using %s as source IP address\", ret[\"source_ip\"])",
            "            else:",
            "                log.warning(",
            "                    \"The interface %s is down so it cannot be used as source to connect to the Master\",",
            "                    opts[\"source_interface_name\"],",
            "                )",
            "        else:",
            "            log.warning(",
            "                \"%s is not a valid interface. Ignoring.\", opts[\"source_interface_name\"]",
            "            )",
            "    elif opts[\"source_address\"]:",
            "        ret[\"source_ip\"] = salt.utils.network.dns_check(",
            "            opts[\"source_address\"], int(opts[\"source_ret_port\"]), True, opts[\"ipv6\"]",
            "        )",
            "        log.debug(\"Using %s as source IP address\", ret[\"source_ip\"])",
            "    if opts[\"source_ret_port\"]:",
            "        ret[\"source_ret_port\"] = int(opts[\"source_ret_port\"])",
            "        log.debug(\"Using %d as source port for the ret server\", ret[\"source_ret_port\"])",
            "    if opts[\"source_publish_port\"]:",
            "        ret[\"source_publish_port\"] = int(opts[\"source_publish_port\"])",
            "        log.debug(",
            "            \"Using %d as source port for the master pub\", ret[\"source_publish_port\"]",
            "        )",
            "    ret[\"master_uri\"] = \"tcp://{ip}:{port}\".format(",
            "        ip=ret[\"master_ip\"], port=opts[\"master_port\"]",
            "    )",
            "    log.debug(\"Master URI: %s\", ret[\"master_uri\"])",
            "",
            "    return ret",
            "",
            "",
            "def prep_ip_port(opts):",
            "    \"\"\"",
            "    parse host:port values from opts['master'] and return valid:",
            "        master: ip address or hostname as a string",
            "        master_port: (optional) master returner port as integer",
            "",
            "    e.g.:",
            "      - master: 'localhost:1234' -> {'master': 'localhost', 'master_port': 1234}",
            "      - master: '127.0.0.1:1234' -> {'master': '127.0.0.1', 'master_port' :1234}",
            "      - master: '[::1]:1234' -> {'master': '::1', 'master_port': 1234}",
            "      - master: 'fe80::a00:27ff:fedc:ba98' -> {'master': 'fe80::a00:27ff:fedc:ba98'}",
            "    \"\"\"",
            "    ret = {}",
            "    # Use given master IP if \"ip_only\" is set or if master_ip is an ipv6 address without",
            "    # a port specified. The is_ipv6 check returns False if brackets are used in the IP",
            "    # definition such as master: '[::1]:1234'.",
            "    if opts[\"master_uri_format\"] == \"ip_only\":",
            "        ret[\"master\"] = ipaddress.ip_address(opts[\"master\"])",
            "    else:",
            "        try:",
            "            host, port = parse_host_port(opts[\"master\"])",
            "        except ValueError as exc:",
            "            raise SaltClientError(exc)",
            "        ret = {\"master\": host}",
            "        if port:",
            "            ret.update({\"master_port\": port})",
            "",
            "    return ret",
            "",
            "",
            "def get_proc_dir(cachedir, **kwargs):",
            "    \"\"\"",
            "    Given the cache directory, return the directory that process data is",
            "    stored in, creating it if it doesn't exist.",
            "    The following optional Keyword Arguments are handled:",
            "",
            "    mode: which is anything os.makedir would accept as mode.",
            "",
            "    uid: the uid to set, if not set, or it is None or -1 no changes are",
            "         made. Same applies if the directory is already owned by this",
            "         uid. Must be int. Works only on unix/unix like systems.",
            "",
            "    gid: the gid to set, if not set, or it is None or -1 no changes are",
            "         made. Same applies if the directory is already owned by this",
            "         gid. Must be int. Works only on unix/unix like systems.",
            "    \"\"\"",
            "    fn_ = os.path.join(cachedir, \"proc\")",
            "    mode = kwargs.pop(\"mode\", None)",
            "",
            "    if mode is None:",
            "        mode = {}",
            "    else:",
            "        mode = {\"mode\": mode}",
            "",
            "    if not os.path.isdir(fn_):",
            "        # proc_dir is not present, create it with mode settings",
            "        os.makedirs(fn_, **mode)",
            "",
            "    d_stat = os.stat(fn_)",
            "",
            "    # if mode is not an empty dict then we have an explicit",
            "    # dir mode. So lets check if mode needs to be changed.",
            "    if mode:",
            "        mode_part = S_IMODE(d_stat.st_mode)",
            "        if mode_part != mode[\"mode\"]:",
            "            os.chmod(fn_, (d_stat.st_mode ^ mode_part) | mode[\"mode\"])",
            "",
            "    if hasattr(os, \"chown\"):",
            "        # only on unix/unix like systems",
            "        uid = kwargs.pop(\"uid\", -1)",
            "        gid = kwargs.pop(\"gid\", -1)",
            "",
            "        # if uid and gid are both -1 then go ahead with",
            "        # no changes at all",
            "        if (d_stat.st_uid != uid or d_stat.st_gid != gid) and [",
            "            i for i in (uid, gid) if i != -1",
            "        ]:",
            "            os.chown(fn_, uid, gid)",
            "",
            "    return fn_",
            "",
            "",
            "def load_args_and_kwargs(func, args, data=None, ignore_invalid=False):",
            "    \"\"\"",
            "    Detect the args and kwargs that need to be passed to a function call, and",
            "    check them against what was passed.",
            "    \"\"\"",
            "    argspec = salt.utils.args.get_function_argspec(func)",
            "    _args = []",
            "    _kwargs = {}",
            "    invalid_kwargs = []",
            "",
            "    for arg in args:",
            "        if isinstance(arg, dict) and arg.pop(\"__kwarg__\", False) is True:",
            "            # if the arg is a dict with __kwarg__ == True, then its a kwarg",
            "            for key, val in arg.items():",
            "                if argspec.keywords or key in argspec.args:",
            "                    # Function supports **kwargs or is a positional argument to",
            "                    # the function.",
            "                    _kwargs[key] = val",
            "                else:",
            "                    # **kwargs not in argspec and parsed argument name not in",
            "                    # list of positional arguments. This keyword argument is",
            "                    # invalid.",
            "                    invalid_kwargs.append(\"{}={}\".format(key, val))",
            "            continue",
            "",
            "        else:",
            "            string_kwarg = salt.utils.args.parse_input([arg], condition=False)[",
            "                1",
            "            ]  # pylint: disable=W0632",
            "            if string_kwarg:",
            "                if argspec.keywords or next(iter(string_kwarg.keys())) in argspec.args:",
            "                    # Function supports **kwargs or is a positional argument to",
            "                    # the function.",
            "                    _kwargs.update(string_kwarg)",
            "                else:",
            "                    # **kwargs not in argspec and parsed argument name not in",
            "                    # list of positional arguments. This keyword argument is",
            "                    # invalid.",
            "                    for key, val in string_kwarg.items():",
            "                        invalid_kwargs.append(\"{}={}\".format(key, val))",
            "            else:",
            "                _args.append(arg)",
            "",
            "    if invalid_kwargs and not ignore_invalid:",
            "        salt.utils.args.invalid_kwargs(invalid_kwargs)",
            "",
            "    if argspec.keywords and isinstance(data, dict):",
            "        # this function accepts **kwargs, pack in the publish data",
            "        for key, val in data.items():",
            "            _kwargs[\"__pub_{}\".format(key)] = val",
            "",
            "    return _args, _kwargs",
            "",
            "",
            "def eval_master_func(opts):",
            "    \"\"\"",
            "    Evaluate master function if master type is 'func'",
            "    and save it result in opts['master']",
            "    \"\"\"",
            "    if \"__master_func_evaluated\" not in opts:",
            "        # split module and function and try loading the module",
            "        mod_fun = opts[\"master\"]",
            "        mod, fun = mod_fun.split(\".\")",
            "        try:",
            "            master_mod = salt.loader.raw_mod(opts, mod, fun)",
            "            if not master_mod:",
            "                raise KeyError",
            "            # we take whatever the module returns as master address",
            "            opts[\"master\"] = master_mod[mod_fun]()",
            "            # Check for valid types",
            "            if not isinstance(opts[\"master\"], ((str,), list)):",
            "                raise TypeError",
            "            opts[\"__master_func_evaluated\"] = True",
            "        except KeyError:",
            "            log.error(\"Failed to load module %s\", mod_fun)",
            "            sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "        except TypeError:",
            "            log.error(\"%s returned from %s is not a string\", opts[\"master\"], mod_fun)",
            "            sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "        log.info(\"Evaluated master from module: %s\", mod_fun)",
            "",
            "",
            "def master_event(type, master=None):",
            "    \"\"\"",
            "    Centralized master event function which will return event type based on event_map",
            "    \"\"\"",
            "    event_map = {",
            "        \"connected\": \"__master_connected\",",
            "        \"disconnected\": \"__master_disconnected\",",
            "        \"failback\": \"__master_failback\",",
            "        \"alive\": \"__master_alive\",",
            "    }",
            "",
            "    if type == \"alive\" and master is not None:",
            "        return \"{}_{}\".format(event_map.get(type), master)",
            "",
            "    return event_map.get(type, None)",
            "",
            "",
            "def service_name():",
            "    \"\"\"",
            "    Return the proper service name based on platform",
            "    \"\"\"",
            "    return \"salt_minion\" if \"bsd\" in sys.platform else \"salt-minion\"",
            "",
            "",
            "class MinionBase:",
            "    def __init__(self, opts):",
            "        self.opts = opts",
            "        self.beacons_leader = opts.get(\"beacons_leader\", True)",
            "",
            "    def gen_modules(self, initial_load=False, context=None):",
            "        \"\"\"",
            "        Tell the minion to reload the execution modules",
            "",
            "        CLI Example:",
            "",
            "        .. code-block:: bash",
            "",
            "            salt '*' sys.reload_modules",
            "        \"\"\"",
            "        if initial_load:",
            "            self.opts[\"pillar\"] = salt.pillar.get_pillar(",
            "                self.opts,",
            "                self.opts[\"grains\"],",
            "                self.opts[\"id\"],",
            "                self.opts[\"saltenv\"],",
            "                pillarenv=self.opts.get(\"pillarenv\"),",
            "            ).compile_pillar()",
            "",
            "        self.utils = salt.loader.utils(self.opts, context=context)",
            "        self.functions = salt.loader.minion_mods(",
            "            self.opts, utils=self.utils, context=context",
            "        )",
            "        self.serializers = salt.loader.serializers(self.opts)",
            "        self.returners = salt.loader.returners(",
            "            self.opts, functions=self.functions, context=context",
            "        )",
            "        self.proxy = salt.loader.proxy(",
            "            self.opts, functions=self.functions, returners=self.returners",
            "        )",
            "        # TODO: remove",
            "        self.function_errors = {}  # Keep the funcs clean",
            "        self.states = salt.loader.states(",
            "            self.opts,",
            "            functions=self.functions,",
            "            utils=self.utils,",
            "            serializers=self.serializers,",
            "            context=context,",
            "        )",
            "        self.rend = salt.loader.render(",
            "            self.opts, functions=self.functions, context=context",
            "        )",
            "        #        self.matcher = Matcher(self.opts, self.functions)",
            "        self.matchers = salt.loader.matchers(self.opts)",
            "        self.functions[\"sys.reload_modules\"] = self.gen_modules",
            "        self.executors = salt.loader.executors(",
            "            self.opts, functions=self.functions, proxy=self.proxy, context=context",
            "        )",
            "",
            "    @staticmethod",
            "    def process_schedule(minion, loop_interval):",
            "        try:",
            "            if hasattr(minion, \"schedule\"):",
            "                minion.schedule.eval()",
            "            else:",
            "                log.error(",
            "                    \"Minion scheduler not initialized. Scheduled jobs will not be run.\"",
            "                )",
            "                return",
            "            # Check if scheduler requires lower loop interval than",
            "            # the loop_interval setting",
            "            if minion.schedule.loop_interval < loop_interval:",
            "                loop_interval = minion.schedule.loop_interval",
            "                log.debug(\"Overriding loop_interval because of scheduled jobs.\")",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception %s occurred in scheduled job\", exc)",
            "        return loop_interval",
            "",
            "    def process_beacons(self, functions):",
            "        \"\"\"",
            "        Evaluate all of the configured beacons, grab the config again in case",
            "        the pillar or grains changed",
            "        \"\"\"",
            "        if \"config.merge\" in functions:",
            "            b_conf = functions[\"config.merge\"](",
            "                \"beacons\", self.opts[\"beacons\"], omit_opts=True",
            "            )",
            "            if b_conf:",
            "                return self.beacons.process(",
            "                    b_conf, self.opts[\"grains\"]",
            "                )  # pylint: disable=no-member",
            "        return []",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def eval_master(self, opts, timeout=60, safe=True, failed=False, failback=False):",
            "        \"\"\"",
            "        Evaluates and returns a tuple of the current master address and the pub_channel.",
            "",
            "        In standard mode, just creates a pub_channel with the given master address.",
            "",
            "        With master_type=func evaluates the current master address from the given",
            "        module and then creates a pub_channel.",
            "",
            "        With master_type=failover takes the list of masters and loops through them.",
            "        The first one that allows the minion to create a pub_channel is then",
            "        returned. If this function is called outside the minions initialization",
            "        phase (for example from the minions main event-loop when a master connection",
            "        loss was detected), 'failed' should be set to True. The current",
            "        (possibly failed) master will then be removed from the list of masters.",
            "        \"\"\"",
            "        # return early if we are not connecting to a master",
            "        if opts[\"master_type\"] == \"disable\":",
            "            log.warning(\"Master is set to disable, skipping connection\")",
            "            self.connected = False",
            "            raise salt.ext.tornado.gen.Return((None, None))",
            "",
            "        # Run masters discovery over SSDP. This may modify the whole configuration,",
            "        # depending of the networking and sets of masters.",
            "        self._discover_masters()",
            "",
            "        # check if master_type was altered from its default",
            "        if opts[\"master_type\"] != \"str\" and opts[\"__role\"] != \"syndic\":",
            "            # check for a valid keyword",
            "            if opts[\"master_type\"] == \"func\":",
            "                eval_master_func(opts)",
            "",
            "            # if failover or distributed is set, master has to be of type list",
            "            elif opts[\"master_type\"] in (\"failover\", \"distributed\"):",
            "                if isinstance(opts[\"master\"], list):",
            "                    log.info(",
            "                        \"Got list of available master addresses: %s\", opts[\"master\"]",
            "                    )",
            "",
            "                    if opts[\"master_type\"] == \"distributed\":",
            "                        master_len = len(opts[\"master\"])",
            "                        if master_len > 1:",
            "                            secondary_masters = opts[\"master\"][1:]",
            "                            master_idx = crc32(opts[\"id\"]) % master_len",
            "                            try:",
            "                                preferred_masters = opts[\"master\"]",
            "                                preferred_masters[0] = opts[\"master\"][master_idx]",
            "                                preferred_masters[1:] = [",
            "                                    m",
            "                                    for m in opts[\"master\"]",
            "                                    if m != preferred_masters[0]",
            "                                ]",
            "                                opts[\"master\"] = preferred_masters",
            "                                log.info(",
            "                                    \"Distributed to the master at '%s'.\",",
            "                                    opts[\"master\"][0],",
            "                                )",
            "                            except (KeyError, AttributeError, TypeError):",
            "                                log.warning(",
            "                                    \"Failed to distribute to a specific master.\"",
            "                                )",
            "                        else:",
            "                            log.warning(",
            "                                \"master_type = distributed needs more than 1 master.\"",
            "                            )",
            "",
            "                    if opts[\"master_shuffle\"]:",
            "                        log.warning(",
            "                            \"Use of 'master_shuffle' detected. 'master_shuffle' is deprecated in favor \"",
            "                            \"of 'random_master'. Please update your minion config file.\"",
            "                        )",
            "                        opts[\"random_master\"] = opts[\"master_shuffle\"]",
            "",
            "                    opts[\"auth_tries\"] = 0",
            "                    if (",
            "                        opts[\"master_failback\"]",
            "                        and opts[\"master_failback_interval\"] == 0",
            "                    ):",
            "                        opts[\"master_failback_interval\"] = opts[\"master_alive_interval\"]",
            "                # if opts['master'] is a str and we have never created opts['master_list']",
            "                elif isinstance(opts[\"master\"], str) and (\"master_list\" not in opts):",
            "                    # We have a string, but a list was what was intended. Convert.",
            "                    # See issue 23611 for details",
            "                    opts[\"master\"] = [opts[\"master\"]]",
            "                elif opts[\"__role\"] == \"syndic\":",
            "                    log.info(\"Syndic setting master_syndic to '%s'\", opts[\"master\"])",
            "",
            "                # if failed=True, the minion was previously connected",
            "                # we're probably called from the minions main-event-loop",
            "                # because a master connection loss was detected. remove",
            "                # the possibly failed master from the list of masters.",
            "                elif failed:",
            "                    if failback:",
            "                        # failback list of masters to original config",
            "                        opts[\"master\"] = opts[\"master_list\"]",
            "                    else:",
            "                        log.info(",
            "                            \"Moving possibly failed master %s to the end of \"",
            "                            \"the list of masters\",",
            "                            opts[\"master\"],",
            "                        )",
            "                        if opts[\"master\"] in opts[\"local_masters\"]:",
            "                            # create new list of master with the possibly failed",
            "                            # one moved to the end",
            "                            failed_master = opts[\"master\"]",
            "                            opts[\"master\"] = [",
            "                                x for x in opts[\"local_masters\"] if opts[\"master\"] != x",
            "                            ]",
            "                            opts[\"master\"].append(failed_master)",
            "                        else:",
            "                            opts[\"master\"] = opts[\"master_list\"]",
            "                else:",
            "                    msg = (",
            "                        \"master_type set to 'failover' but 'master' \"",
            "                        \"is not of type list but of type \"",
            "                        \"{}\".format(type(opts[\"master\"]))",
            "                    )",
            "                    log.error(msg)",
            "                    sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "                # If failover is set, minion have to failover on DNS errors instead of retry DNS resolve.",
            "                # See issue 21082 for details",
            "                if opts[\"retry_dns\"] and opts[\"master_type\"] == \"failover\":",
            "                    msg = (",
            "                        \"'master_type' set to 'failover' but 'retry_dns' is not 0. \"",
            "                        \"Setting 'retry_dns' to 0 to failover to the next master on DNS errors.\"",
            "                    )",
            "                    log.critical(msg)",
            "                    opts[\"retry_dns\"] = 0",
            "            else:",
            "                msg = \"Invalid keyword '{}' for variable \" \"'master_type'\".format(",
            "                    opts[\"master_type\"]",
            "                )",
            "                log.error(msg)",
            "                sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "",
            "        # FIXME: if SMinion don't define io_loop, it can't switch master see #29088",
            "        # Specify kwargs for the channel factory so that SMinion doesn't need to define an io_loop",
            "        # (The channel factories will set a default if the kwarg isn't passed)",
            "        factory_kwargs = {\"timeout\": timeout, \"safe\": safe}",
            "        if getattr(self, \"io_loop\", None):",
            "            factory_kwargs[\"io_loop\"] = self.io_loop  # pylint: disable=no-member",
            "",
            "        tries = opts.get(\"master_tries\", 1)",
            "        attempts = 0",
            "",
            "        # if we have a list of masters, loop through them and be",
            "        # happy with the first one that allows us to connect",
            "        if isinstance(opts[\"master\"], list):",
            "            conn = False",
            "            last_exc = None",
            "            opts[\"master_uri_list\"] = []",
            "            opts[\"local_masters\"] = copy.copy(opts[\"master\"])",
            "",
            "            # shuffle the masters and then loop through them",
            "            if opts[\"random_master\"]:",
            "                # master_failback is only used when master_type is set to failover",
            "                if opts[\"master_type\"] == \"failover\" and opts[\"master_failback\"]:",
            "                    secondary_masters = opts[\"local_masters\"][1:]",
            "                    shuffle(secondary_masters)",
            "                    opts[\"local_masters\"][1:] = secondary_masters",
            "                else:",
            "                    shuffle(opts[\"local_masters\"])",
            "",
            "            # This sits outside of the connection loop below because it needs to set",
            "            # up a list of master URIs regardless of which masters are available",
            "            # to connect _to_. This is primarily used for masterless mode, when",
            "            # we need a list of master URIs to fire calls back to.",
            "            for master in opts[\"local_masters\"]:",
            "                opts[\"master\"] = master",
            "                opts.update(prep_ip_port(opts))",
            "                if opts[\"master_type\"] == \"failover\":",
            "                    try:",
            "                        opts[\"master_uri_list\"].append(",
            "                            resolve_dns(opts, False)[\"master_uri\"]",
            "                        )",
            "                    except SaltClientError:",
            "                        continue",
            "                else:",
            "                    opts[\"master_uri_list\"].append(resolve_dns(opts)[\"master_uri\"])",
            "",
            "            if not opts[\"master_uri_list\"]:",
            "                msg = \"No master could be resolved\"",
            "                log.error(msg)",
            "                raise SaltClientError(msg)",
            "",
            "            pub_channel = None",
            "            while True:",
            "                if attempts != 0:",
            "                    # Give up a little time between connection attempts",
            "                    # to allow the IOLoop to run any other scheduled tasks.",
            "                    yield salt.ext.tornado.gen.sleep(opts[\"acceptance_wait_time\"])",
            "                attempts += 1",
            "                if tries > 0:",
            "                    log.debug(\"Connecting to master. Attempt %s of %s\", attempts, tries)",
            "                else:",
            "                    log.debug(",
            "                        \"Connecting to master. Attempt %s (infinite attempts)\", attempts",
            "                    )",
            "                for master in opts[\"local_masters\"]:",
            "                    opts[\"master\"] = master",
            "                    opts.update(prep_ip_port(opts))",
            "                    if opts[\"master_type\"] == \"failover\":",
            "                        try:",
            "                            opts.update(resolve_dns(opts, False))",
            "                        except SaltClientError:",
            "                            continue",
            "                    else:",
            "                        opts.update(resolve_dns(opts))",
            "",
            "                    # on first run, update self.opts with the whole master list",
            "                    # to enable a minion to re-use old masters if they get fixed",
            "                    if \"master_list\" not in opts:",
            "                        opts[\"master_list\"] = copy.copy(opts[\"local_masters\"])",
            "",
            "                    self.opts = opts",
            "",
            "                    pub_channel = salt.transport.client.AsyncPubChannel.factory(",
            "                        opts, **factory_kwargs",
            "                    )",
            "                    try:",
            "                        yield pub_channel.connect()",
            "                        conn = True",
            "                        break",
            "                    except SaltClientError as exc:",
            "                        last_exc = exc",
            "                        if exc.strerror.startswith(\"Could not access\"):",
            "                            msg = (",
            "                                \"Failed to initiate connection with Master \"",
            "                                \"%s: check ownership/permissions. Error \"",
            "                                \"message: %s\",",
            "                                opts[\"master\"],",
            "                                exc,",
            "                            )",
            "                        else:",
            "                            msg = (",
            "                                \"Master %s could not be reached, trying next \"",
            "                                \"next master (if any)\",",
            "                                opts[\"master\"],",
            "                            )",
            "                        log.info(msg)",
            "                        pub_channel.close()",
            "                        pub_channel = None",
            "                        continue",
            "",
            "                if not conn:",
            "                    if attempts == tries:",
            "                        # Exhausted all attempts. Return exception.",
            "                        self.connected = False",
            "                        self.opts[\"master\"] = copy.copy(self.opts[\"local_masters\"])",
            "                        log.error(",
            "                            \"No master could be reached or all masters \"",
            "                            \"denied the minion's connection attempt.\"",
            "                        )",
            "                        if pub_channel:",
            "                            pub_channel.close()",
            "                        # If the code reaches this point, 'last_exc'",
            "                        # should already be set.",
            "                        raise last_exc  # pylint: disable=E0702",
            "                else:",
            "                    self.tok = pub_channel.auth.gen_token(b\"salt\")",
            "                    self.connected = True",
            "                    raise salt.ext.tornado.gen.Return((opts[\"master\"], pub_channel))",
            "",
            "        # single master sign in",
            "        else:",
            "            if opts[\"random_master\"]:",
            "                log.warning(",
            "                    \"random_master is True but there is only one master specified. Ignoring.\"",
            "                )",
            "            pub_channel = None",
            "            while True:",
            "                if attempts != 0:",
            "                    # Give up a little time between connection attempts",
            "                    # to allow the IOLoop to run any other scheduled tasks.",
            "                    yield salt.ext.tornado.gen.sleep(opts[\"acceptance_wait_time\"])",
            "                attempts += 1",
            "                if tries > 0:",
            "                    log.debug(\"Connecting to master. Attempt %s of %s\", attempts, tries)",
            "                else:",
            "                    log.debug(",
            "                        \"Connecting to master. Attempt %s (infinite attempts)\", attempts",
            "                    )",
            "                opts.update(prep_ip_port(opts))",
            "                opts.update(resolve_dns(opts))",
            "                try:",
            "                    if self.opts[\"transport\"] == \"detect\":",
            "                        self.opts[\"detect_mode\"] = True",
            "                        for trans in (\"zeromq\", \"tcp\"):",
            "                            if trans == \"zeromq\" and not zmq:",
            "                                continue",
            "                            self.opts[\"transport\"] = trans",
            "                            pub_channel = salt.transport.client.AsyncPubChannel.factory(",
            "                                self.opts, **factory_kwargs",
            "                            )",
            "                            yield pub_channel.connect()",
            "                            if not pub_channel.auth.authenticated:",
            "                                continue",
            "                            del self.opts[\"detect_mode\"]",
            "                            break",
            "                    else:",
            "                        pub_channel = salt.transport.client.AsyncPubChannel.factory(",
            "                            self.opts, **factory_kwargs",
            "                        )",
            "                        yield pub_channel.connect()",
            "                    self.tok = pub_channel.auth.gen_token(b\"salt\")",
            "                    self.connected = True",
            "                    raise salt.ext.tornado.gen.Return((opts[\"master\"], pub_channel))",
            "                except SaltClientError:",
            "                    if attempts == tries:",
            "                        # Exhausted all attempts. Return exception.",
            "                        self.connected = False",
            "                        if pub_channel:",
            "                            pub_channel.close()",
            "                        raise",
            "",
            "    def _discover_masters(self):",
            "        \"\"\"",
            "        Discover master(s) and decide where to connect, if SSDP is around.",
            "        This modifies the configuration on the fly.",
            "        :return:",
            "        \"\"\"",
            "        if (",
            "            self.opts[\"master\"] == DEFAULT_MINION_OPTS[\"master\"]",
            "            and self.opts[\"discovery\"] is not False",
            "        ):",
            "            master_discovery_client = salt.utils.ssdp.SSDPDiscoveryClient()",
            "            masters = {}",
            "            for att in range(self.opts[\"discovery\"].get(\"attempts\", 3)):",
            "                try:",
            "                    att += 1",
            "                    log.info(\"Attempting %s time(s) to discover masters\", att)",
            "                    masters.update(master_discovery_client.discover())",
            "                    if not masters:",
            "                        time.sleep(self.opts[\"discovery\"].get(\"pause\", 5))",
            "                    else:",
            "                        break",
            "                except Exception as err:  # pylint: disable=broad-except",
            "                    log.error(\"SSDP discovery failure: %s\", err)",
            "                    break",
            "",
            "            if masters:",
            "                policy = self.opts.get(\"discovery\", {}).get(\"match\", \"any\")",
            "                if policy not in [\"any\", \"all\"]:",
            "                    log.error(",
            "                        'SSDP configuration matcher failure: unknown value \"%s\". '",
            "                        'Should be \"any\" or \"all\"',",
            "                        policy,",
            "                    )",
            "                else:",
            "                    mapping = self.opts[\"discovery\"].get(\"mapping\", {})",
            "                    for addr, mappings in masters.items():",
            "                        for proto_data in mappings:",
            "                            cnt = len(",
            "                                [",
            "                                    key",
            "                                    for key, value in mapping.items()",
            "                                    if proto_data.get(\"mapping\", {}).get(key) == value",
            "                                ]",
            "                            )",
            "                            if policy == \"any\" and bool(cnt) or cnt == len(mapping):",
            "                                self.opts[\"master\"] = proto_data[\"master\"]",
            "                                return",
            "",
            "    def _return_retry_timer(self):",
            "        \"\"\"",
            "        Based on the minion configuration, either return a randomized timer or",
            "        just return the value of the return_retry_timer.",
            "        \"\"\"",
            "        msg = \"Minion return retry timer set to %s seconds\"",
            "        if self.opts.get(\"return_retry_timer_max\"):",
            "            try:",
            "                random_retry = randint(",
            "                    self.opts[\"return_retry_timer\"], self.opts[\"return_retry_timer_max\"]",
            "                )",
            "                retry_msg = msg % random_retry",
            "                log.debug(\"%s (randomized)\", msg % random_retry)",
            "                return random_retry",
            "            except ValueError:",
            "                # Catch wiseguys using negative integers here",
            "                log.error(",
            "                    \"Invalid value (return_retry_timer: %s or \"",
            "                    \"return_retry_timer_max: %s). Both must be positive \"",
            "                    \"integers.\",",
            "                    self.opts[\"return_retry_timer\"],",
            "                    self.opts[\"return_retry_timer_max\"],",
            "                )",
            "                log.debug(msg, DEFAULT_MINION_OPTS[\"return_retry_timer\"])",
            "                return DEFAULT_MINION_OPTS[\"return_retry_timer\"]",
            "        else:",
            "            log.debug(msg, self.opts.get(\"return_retry_timer\"))",
            "            return self.opts.get(\"return_retry_timer\")",
            "",
            "",
            "class SMinion(MinionBase):",
            "    \"\"\"",
            "    Create an object that has loaded all of the minion module functions,",
            "    grains, modules, returners etc.  The SMinion allows developers to",
            "    generate all of the salt minion functions and present them with these",
            "    functions for general use.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, context=None):",
            "        # Late setup of the opts grains, so we can log from the grains module",
            "        import salt.loader",
            "",
            "        opts[\"grains\"] = salt.loader.grains(opts)",
            "        super().__init__(opts)",
            "",
            "        # Clean out the proc directory (default /var/cache/salt/minion/proc)",
            "        if self.opts.get(\"file_client\", \"remote\") == \"remote\" or self.opts.get(",
            "            \"use_master_when_local\", False",
            "        ):",
            "            install_zmq()",
            "            io_loop = ZMQDefaultLoop.current()",
            "            io_loop.run_sync(lambda: self.eval_master(self.opts, failed=True))",
            "        self.gen_modules(initial_load=True, context=context or {})",
            "",
            "        # If configured, cache pillar data on the minion",
            "        if self.opts[\"file_client\"] == \"remote\" and self.opts.get(",
            "            \"minion_pillar_cache\", False",
            "        ):",
            "            import salt.utils.yaml",
            "",
            "            pdir = os.path.join(self.opts[\"cachedir\"], \"pillar\")",
            "            if not os.path.isdir(pdir):",
            "                os.makedirs(pdir, 0o700)",
            "            ptop = os.path.join(pdir, \"top.sls\")",
            "            if self.opts[\"saltenv\"] is not None:",
            "                penv = self.opts[\"saltenv\"]",
            "            else:",
            "                penv = \"base\"",
            "            cache_top = {penv: {self.opts[\"id\"]: [\"cache\"]}}",
            "            with salt.utils.files.fopen(ptop, \"wb\") as fp_:",
            "                salt.utils.yaml.safe_dump(cache_top, fp_, encoding=SLS_ENCODING)",
            "                os.chmod(ptop, 0o600)",
            "            cache_sls = os.path.join(pdir, \"cache.sls\")",
            "            with salt.utils.files.fopen(cache_sls, \"wb\") as fp_:",
            "                salt.utils.yaml.safe_dump(",
            "                    self.opts[\"pillar\"], fp_, encoding=SLS_ENCODING",
            "                )",
            "                os.chmod(cache_sls, 0o600)",
            "",
            "",
            "class MasterMinion:",
            "    \"\"\"",
            "    Create a fully loaded minion function object for generic use on the",
            "    master. What makes this class different is that the pillar is",
            "    omitted, otherwise everything else is loaded cleanly.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        opts,",
            "        returners=True,",
            "        states=True,",
            "        rend=True,",
            "        matcher=True,",
            "        whitelist=None,",
            "        ignore_config_errors=True,",
            "    ):",
            "        self.opts = salt.config.minion_config(",
            "            opts[\"conf_file\"], ignore_config_errors=ignore_config_errors, role=\"master\"",
            "        )",
            "        self.opts.update(opts)",
            "        self.whitelist = whitelist",
            "        self.opts[\"grains\"] = salt.loader.grains(opts)",
            "        self.opts[\"pillar\"] = {}",
            "        self.mk_returners = returners",
            "        self.mk_states = states",
            "        self.mk_rend = rend",
            "        self.mk_matcher = matcher",
            "        self.gen_modules(initial_load=True)",
            "",
            "    def gen_modules(self, initial_load=False):",
            "        \"\"\"",
            "        Tell the minion to reload the execution modules",
            "",
            "        CLI Example:",
            "",
            "        .. code-block:: bash",
            "",
            "            salt '*' sys.reload_modules",
            "        \"\"\"",
            "        self.utils = salt.loader.utils(self.opts)",
            "        self.functions = salt.loader.minion_mods(",
            "            self.opts,",
            "            utils=self.utils,",
            "            whitelist=self.whitelist,",
            "            initial_load=initial_load,",
            "        )",
            "        self.serializers = salt.loader.serializers(self.opts)",
            "        if self.mk_returners:",
            "            self.returners = salt.loader.returners(self.opts, self.functions)",
            "        if self.mk_states:",
            "            self.states = salt.loader.states(",
            "                self.opts, self.functions, self.utils, self.serializers",
            "            )",
            "        if self.mk_rend:",
            "            self.rend = salt.loader.render(self.opts, self.functions)",
            "        if self.mk_matcher:",
            "            self.matchers = salt.loader.matchers(self.opts)",
            "        self.functions[\"sys.reload_modules\"] = self.gen_modules",
            "",
            "",
            "class MinionManager(MinionBase):",
            "    \"\"\"",
            "    Create a multi minion interface, this creates as many minions as are",
            "    defined in the master option and binds each minion object to a respective",
            "    master.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts):",
            "        super().__init__(opts)",
            "        self.auth_wait = self.opts[\"acceptance_wait_time\"]",
            "        self.max_auth_wait = self.opts[\"acceptance_wait_time_max\"]",
            "        self.minions = []",
            "        self.jid_queue = []",
            "",
            "        install_zmq()",
            "        self.io_loop = ZMQDefaultLoop.current()",
            "        self.process_manager = ProcessManager(name=\"MultiMinionProcessManager\")",
            "        self.io_loop.spawn_callback(",
            "            self.process_manager.run, **{\"asynchronous\": True}",
            "        )  # Tornado backward compat",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.destroy()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _bind(self):",
            "        # start up the event publisher, so we can see events during startup",
            "        self.event_publisher = salt.utils.event.AsyncEventPublisher(",
            "            self.opts, io_loop=self.io_loop,",
            "        )",
            "        self.event = salt.utils.event.get_event(",
            "            \"minion\", opts=self.opts, io_loop=self.io_loop",
            "        )",
            "        self.event.subscribe(\"\")",
            "        self.event.set_event_handler(self.handle_event)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_event(self, package):",
            "        for minion in self.minions:",
            "            minion.handle_event(package)",
            "",
            "    def _create_minion_object(",
            "        self, opts, timeout, safe, io_loop=None, loaded_base_name=None, jid_queue=None",
            "    ):",
            "        \"\"\"",
            "        Helper function to return the correct type of object",
            "        \"\"\"",
            "        return Minion(",
            "            opts,",
            "            timeout,",
            "            safe,",
            "            io_loop=io_loop,",
            "            loaded_base_name=loaded_base_name,",
            "            jid_queue=jid_queue,",
            "        )",
            "",
            "    def _check_minions(self):",
            "        \"\"\"",
            "        Check the size of self.minions and raise an error if it's empty",
            "        \"\"\"",
            "        if not self.minions:",
            "            err = \"Minion unable to successfully connect to \" \"a Salt Master.\"",
            "            log.error(err)",
            "",
            "    def _spawn_minions(self, timeout=60):",
            "        \"\"\"",
            "        Spawn all the coroutines which will sign in to masters",
            "        \"\"\"",
            "        masters = self.opts[\"master\"]",
            "        if (self.opts[\"master_type\"] in (\"failover\", \"distributed\")) or not isinstance(",
            "            self.opts[\"master\"], list",
            "        ):",
            "            masters = [masters]",
            "",
            "        beacons_leader = True",
            "        for master in masters:",
            "            s_opts = copy.deepcopy(self.opts)",
            "            s_opts[\"master\"] = master",
            "            s_opts[\"multimaster\"] = True",
            "            s_opts[\"beacons_leader\"] = beacons_leader",
            "            if beacons_leader:",
            "                beacons_leader = False",
            "            minion = self._create_minion_object(",
            "                s_opts,",
            "                s_opts[\"auth_timeout\"],",
            "                False,",
            "                io_loop=self.io_loop,",
            "                loaded_base_name=\"salt.loader.{}\".format(s_opts[\"master\"]),",
            "                jid_queue=self.jid_queue,",
            "            )",
            "            self.io_loop.spawn_callback(self._connect_minion, minion)",
            "        self.io_loop.call_later(timeout, self._check_minions)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _connect_minion(self, minion):",
            "        \"\"\"",
            "        Create a minion, and asynchronously connect it to a master",
            "        \"\"\"",
            "        last = 0  # never have we signed in",
            "        auth_wait = minion.opts[\"acceptance_wait_time\"]",
            "        failed = False",
            "        while True:",
            "            try:",
            "                if minion.opts.get(\"beacons_before_connect\", False):",
            "                    minion.setup_beacons(before_connect=True)",
            "                if minion.opts.get(\"scheduler_before_connect\", False):",
            "                    minion.setup_scheduler(before_connect=True)",
            "                yield minion.connect_master(failed=failed)",
            "                minion.tune_in(start=False)",
            "                self.minions.append(minion)",
            "                break",
            "            except SaltClientError as exc:",
            "                failed = True",
            "                log.error(",
            "                    \"Error while bringing up minion for multi-master. Is \"",
            "                    \"master at %s responding?\",",
            "                    minion.opts[\"master\"],",
            "                )",
            "                last = time.time()",
            "                if auth_wait < self.max_auth_wait:",
            "                    auth_wait += self.auth_wait",
            "                yield salt.ext.tornado.gen.sleep(auth_wait)  # TODO: log?",
            "            except SaltMasterUnresolvableError:",
            "                err = (",
            "                    \"Master address: '{}' could not be resolved. Invalid or unresolveable address. \"",
            "                    \"Set 'master' value in minion config.\".format(minion.opts[\"master\"])",
            "                )",
            "                log.error(err)",
            "                break",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                failed = True",
            "                log.critical(",
            "                    \"Unexpected error while connecting to %s\",",
            "                    minion.opts[\"master\"],",
            "                    exc_info=True,",
            "                )",
            "",
            "    # Multi Master Tune In",
            "    def tune_in(self):",
            "        \"\"\"",
            "        Bind to the masters",
            "",
            "        This loop will attempt to create connections to masters it hasn't connected",
            "        to yet, but once the initial connection is made it is up to ZMQ to do the",
            "        reconnect (don't know of an API to get the state here in salt)",
            "        \"\"\"",
            "        self._bind()",
            "",
            "        # Fire off all the minion coroutines",
            "        self._spawn_minions()",
            "",
            "        # serve forever!",
            "        self.io_loop.start()",
            "",
            "    @property",
            "    def restart(self):",
            "        for minion in self.minions:",
            "            if minion.restart:",
            "                return True",
            "        return False",
            "",
            "    def stop(self, signum):",
            "        for minion in self.minions:",
            "            minion.process_manager.stop_restarting()",
            "            minion.process_manager.send_signal_to_processes(signum)",
            "            # kill any remaining processes",
            "            minion.process_manager.kill_children()",
            "            minion.destroy()",
            "",
            "    def destroy(self):",
            "        for minion in self.minions:",
            "            minion.destroy()",
            "",
            "",
            "class Minion(MinionBase):",
            "    \"\"\"",
            "    This class instantiates a minion, runs connections for a minion,",
            "    and loads all of the functions into the minion",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        opts,",
            "        timeout=60,",
            "        safe=True,",
            "        loaded_base_name=None,",
            "        io_loop=None,",
            "        jid_queue=None,",
            "    ):  # pylint: disable=W0231",
            "        \"\"\"",
            "        Pass in the options dict",
            "        \"\"\"",
            "        # this means that the parent class doesn't know *which* master we connect to",
            "        super().__init__(opts)",
            "        self.timeout = timeout",
            "        self.safe = safe",
            "",
            "        self._running = None",
            "        self.win_proc = []",
            "        self.subprocess_list = salt.utils.process.SubprocessList()",
            "        self.loaded_base_name = loaded_base_name",
            "        self.connected = False",
            "        self.restart = False",
            "        # Flag meaning minion has finished initialization including first connect to the master.",
            "        # True means the Minion is fully functional and ready to handle events.",
            "        self.ready = False",
            "        self.jid_queue = [] if jid_queue is None else jid_queue",
            "        self.periodic_callbacks = {}",
            "",
            "        if io_loop is None:",
            "            install_zmq()",
            "            self.io_loop = ZMQDefaultLoop.current()",
            "        else:",
            "            self.io_loop = io_loop",
            "",
            "        # Warn if ZMQ < 3.2",
            "        if zmq:",
            "            if ZMQ_VERSION_INFO < (3, 2):",
            "                log.warning(",
            "                    \"You have a version of ZMQ less than ZMQ 3.2! There are \"",
            "                    \"known connection keep-alive issues with ZMQ < 3.2 which \"",
            "                    \"may result in loss of contact with minions. Please \"",
            "                    \"upgrade your ZMQ!\"",
            "                )",
            "        # Late setup of the opts grains, so we can log from the grains",
            "        # module.  If this is a proxy, however, we need to init the proxymodule",
            "        # before we can get the grains.  We do this for proxies in the",
            "        # post_master_init",
            "        if not salt.utils.platform.is_proxy():",
            "            if not self.opts.get(\"grains\", {}):",
            "                self.opts[\"grains\"] = salt.loader.grains(opts)",
            "        else:",
            "            if self.opts.get(\"beacons_before_connect\", False):",
            "                log.warning(",
            "                    \"'beacons_before_connect' is not supported \"",
            "                    \"for proxy minions. Setting to False\"",
            "                )",
            "                self.opts[\"beacons_before_connect\"] = False",
            "            if self.opts.get(\"scheduler_before_connect\", False):",
            "                log.warning(",
            "                    \"'scheduler_before_connect' is not supported \"",
            "                    \"for proxy minions. Setting to False\"",
            "                )",
            "                self.opts[\"scheduler_before_connect\"] = False",
            "",
            "        log.info(\"Creating minion process manager\")",
            "",
            "        if self.opts[\"random_startup_delay\"]:",
            "            sleep_time = random.randint(0, self.opts[\"random_startup_delay\"])",
            "            log.info(",
            "                \"Minion sleeping for %s seconds due to configured \"",
            "                \"startup_delay between 0 and %s seconds\",",
            "                sleep_time,",
            "                self.opts[\"random_startup_delay\"],",
            "            )",
            "            time.sleep(sleep_time)",
            "",
            "        self.process_manager = ProcessManager(name=\"MinionProcessManager\")",
            "        self.io_loop.spawn_callback(self.process_manager.run, **{\"asynchronous\": True})",
            "        # We don't have the proxy setup yet, so we can't start engines",
            "        # Engines need to be able to access __proxy__",
            "        if not salt.utils.platform.is_proxy():",
            "            self.io_loop.spawn_callback(",
            "                salt.engines.start_engines, self.opts, self.process_manager",
            "            )",
            "",
            "        # Install the SIGINT/SIGTERM handlers if not done so far",
            "        if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGINT, self._handle_signals)",
            "",
            "        if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGTERM, self._handle_signals)",
            "",
            "    def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument",
            "        self._running = False",
            "        # escalate the signals to the process manager",
            "        self.process_manager.stop_restarting()",
            "        self.process_manager.send_signal_to_processes(signum)",
            "        # kill any remaining processes",
            "        self.process_manager.kill_children()",
            "        time.sleep(1)",
            "        sys.exit(0)",
            "",
            "    def sync_connect_master(self, timeout=None, failed=False):",
            "        \"\"\"",
            "        Block until we are connected to a master",
            "        \"\"\"",
            "        self._sync_connect_master_success = False",
            "        log.debug(\"sync_connect_master\")",
            "",
            "        def on_connect_master_future_done(future):",
            "            self._sync_connect_master_success = True",
            "            self.io_loop.stop()",
            "",
            "        self._connect_master_future = self.connect_master(failed=failed)",
            "        # finish connecting to master",
            "        self._connect_master_future.add_done_callback(on_connect_master_future_done)",
            "        if timeout:",
            "            self.io_loop.call_later(timeout, self.io_loop.stop)",
            "        try:",
            "            self.io_loop.start()",
            "        except KeyboardInterrupt:",
            "            self.destroy()",
            "        # I made the following 3 line oddity to preserve traceback.",
            "        # Please read PR #23978 before changing, hopefully avoiding regressions.",
            "        # Good luck, we're all counting on you.  Thanks.",
            "        if self._connect_master_future.done():",
            "            future_exception = self._connect_master_future.exception()",
            "            if future_exception:",
            "                # This needs to be re-raised to preserve restart_on_error behavior.",
            "                raise six.reraise(*future_exception)",
            "        if timeout and self._sync_connect_master_success is False:",
            "            raise SaltDaemonNotRunning(\"Failed to connect to the salt-master\")",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect_master(self, failed=False):",
            "        \"\"\"",
            "        Return a future which will complete when you are connected to a master",
            "        \"\"\"",
            "        master, self.pub_channel = yield self.eval_master(",
            "            self.opts, self.timeout, self.safe, failed",
            "        )",
            "        yield self._post_master_init(master)",
            "",
            "    # TODO: better name...",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _post_master_init(self, master):",
            "        \"\"\"",
            "        Function to finish init after connecting to a master",
            "",
            "        This is primarily loading modules, pillars, etc. (since they need",
            "        to know which master they connected to)",
            "",
            "        If this function is changed, please check ProxyMinion._post_master_init",
            "        to see if those changes need to be propagated.",
            "",
            "        Minions and ProxyMinions need significantly different post master setups,",
            "        which is why the differences are not factored out into separate helper",
            "        functions.",
            "        \"\"\"",
            "        if self.connected:",
            "            self.opts[\"master\"] = master",
            "",
            "            # Initialize pillar before loader to make pillar accessible in modules",
            "            async_pillar = salt.pillar.get_async_pillar(",
            "                self.opts,",
            "                self.opts[\"grains\"],",
            "                self.opts[\"id\"],",
            "                self.opts[\"saltenv\"],",
            "                pillarenv=self.opts.get(\"pillarenv\"),",
            "            )",
            "            self.opts[\"pillar\"] = yield async_pillar.compile_pillar()",
            "            async_pillar.destroy()",
            "",
            "        if not self.ready:",
            "            self._setup_core()",
            "        elif self.connected and self.opts[\"pillar\"]:",
            "            # The pillar has changed due to the connection to the master.",
            "            # Reload the functions so that they can use the new pillar data.",
            "            (",
            "                self.functions,",
            "                self.returners,",
            "                self.function_errors,",
            "                self.executors,",
            "            ) = self._load_modules()",
            "            if hasattr(self, \"schedule\"):",
            "                self.schedule.functions = self.functions",
            "                self.schedule.returners = self.returners",
            "",
            "        if not hasattr(self, \"schedule\"):",
            "            self.schedule = salt.utils.schedule.Schedule(",
            "                self.opts,",
            "                self.functions,",
            "                self.returners,",
            "                cleanup=[master_event(type=\"alive\")],",
            "            )",
            "",
            "        # add default scheduling jobs to the minions scheduler",
            "        if self.opts[\"mine_enabled\"] and \"mine.update\" in self.functions:",
            "            self.schedule.add_job(",
            "                {",
            "                    \"__mine_interval\": {",
            "                        \"function\": \"mine.update\",",
            "                        \"minutes\": self.opts[\"mine_interval\"],",
            "                        \"jid_include\": True,",
            "                        \"maxrunning\": 2,",
            "                        \"run_on_start\": True,",
            "                        \"return_job\": self.opts.get(\"mine_return_job\", False),",
            "                    }",
            "                },",
            "                persist=True,",
            "            )",
            "            log.info(\"Added mine.update to scheduler\")",
            "        else:",
            "            self.schedule.delete_job(\"__mine_interval\", persist=True)",
            "",
            "        # add master_alive job if enabled",
            "        if (",
            "            self.opts[\"transport\"] != \"tcp\"",
            "            and self.opts[\"master_alive_interval\"] > 0",
            "            and self.connected",
            "        ):",
            "            self.schedule.add_job(",
            "                {",
            "                    master_event(type=\"alive\", master=self.opts[\"master\"]): {",
            "                        \"function\": \"status.master\",",
            "                        \"seconds\": self.opts[\"master_alive_interval\"],",
            "                        \"jid_include\": True,",
            "                        \"maxrunning\": 1,",
            "                        \"return_job\": False,",
            "                        \"kwargs\": {\"master\": self.opts[\"master\"], \"connected\": True},",
            "                    }",
            "                },",
            "                persist=True,",
            "            )",
            "            if (",
            "                self.opts[\"master_failback\"]",
            "                and \"master_list\" in self.opts",
            "                and self.opts[\"master\"] != self.opts[\"master_list\"][0]",
            "            ):",
            "                self.schedule.add_job(",
            "                    {",
            "                        master_event(type=\"failback\"): {",
            "                            \"function\": \"status.ping_master\",",
            "                            \"seconds\": self.opts[\"master_failback_interval\"],",
            "                            \"jid_include\": True,",
            "                            \"maxrunning\": 1,",
            "                            \"return_job\": False,",
            "                            \"kwargs\": {\"master\": self.opts[\"master_list\"][0]},",
            "                        }",
            "                    },",
            "                    persist=True,",
            "                )",
            "            else:",
            "                self.schedule.delete_job(master_event(type=\"failback\"), persist=True)",
            "        else:",
            "            self.schedule.delete_job(",
            "                master_event(type=\"alive\", master=self.opts[\"master\"]), persist=True",
            "            )",
            "            self.schedule.delete_job(master_event(type=\"failback\"), persist=True)",
            "",
            "    def _prep_mod_opts(self):",
            "        \"\"\"",
            "        Returns a copy of the opts with key bits stripped out",
            "        \"\"\"",
            "        mod_opts = {}",
            "        for key, val in self.opts.items():",
            "            if key == \"logger\":",
            "                continue",
            "            mod_opts[key] = val",
            "        return mod_opts",
            "",
            "    def _load_modules(",
            "        self, force_refresh=False, notify=False, grains=None, opts=None, context=None",
            "    ):",
            "        \"\"\"",
            "        Return the functions and the returners loaded up from the loader",
            "        module",
            "        \"\"\"",
            "        opt_in = True",
            "        if not opts:",
            "            opts = self.opts",
            "            opt_in = False",
            "        # if this is a *nix system AND modules_max_memory is set, lets enforce",
            "        # a memory limit on module imports",
            "        # this feature ONLY works on *nix like OSs (resource module doesn't work on windows)",
            "        modules_max_memory = False",
            "        if opts.get(\"modules_max_memory\", -1) > 0 and HAS_PSUTIL and HAS_RESOURCE:",
            "            log.debug(",
            "                \"modules_max_memory set, enforcing a maximum of %s\",",
            "                opts[\"modules_max_memory\"],",
            "            )",
            "            modules_max_memory = True",
            "            old_mem_limit = resource.getrlimit(resource.RLIMIT_AS)",
            "            rss, vms = psutil.Process(os.getpid()).memory_info()[:2]",
            "            mem_limit = rss + vms + opts[\"modules_max_memory\"]",
            "            resource.setrlimit(resource.RLIMIT_AS, (mem_limit, mem_limit))",
            "        elif opts.get(\"modules_max_memory\", -1) > 0:",
            "            if not HAS_PSUTIL:",
            "                log.error(",
            "                    \"Unable to enforce modules_max_memory because psutil is missing\"",
            "                )",
            "            if not HAS_RESOURCE:",
            "                log.error(",
            "                    \"Unable to enforce modules_max_memory because resource is missing\"",
            "                )",
            "",
            "        # This might be a proxy minion",
            "        if hasattr(self, \"proxy\"):",
            "            proxy = self.proxy",
            "        else:",
            "            proxy = None",
            "",
            "        if context is None:",
            "            context = {}",
            "",
            "        if grains is None:",
            "            opts[\"grains\"] = salt.loader.grains(",
            "                opts, force_refresh, proxy=proxy, context=context",
            "            )",
            "        self.utils = salt.loader.utils(opts, proxy=proxy, context=context)",
            "",
            "        if opts.get(\"multimaster\", False):",
            "            s_opts = copy.deepcopy(opts)",
            "            functions = salt.loader.minion_mods(",
            "                s_opts,",
            "                utils=self.utils,",
            "                proxy=proxy,",
            "                loaded_base_name=self.loaded_base_name,",
            "                notify=notify,",
            "                context=context,",
            "            )",
            "        else:",
            "            functions = salt.loader.minion_mods(",
            "                opts, utils=self.utils, notify=notify, proxy=proxy, context=context,",
            "            )",
            "        returners = salt.loader.returners(opts, functions, proxy=proxy, context=context)",
            "        errors = {}",
            "        if \"_errors\" in functions:",
            "            errors = functions[\"_errors\"]",
            "            functions.pop(\"_errors\")",
            "",
            "        # we're done, reset the limits!",
            "        if modules_max_memory is True:",
            "            resource.setrlimit(resource.RLIMIT_AS, old_mem_limit)",
            "",
            "        executors = salt.loader.executors(opts, functions, proxy=proxy, context=context)",
            "",
            "        if opt_in:",
            "            self.opts = opts",
            "",
            "        return functions, returners, errors, executors",
            "",
            "    def _send_req_sync(self, load, timeout):",
            "",
            "        if self.opts[\"minion_sign_messages\"]:",
            "            log.trace(\"Signing event to be published onto the bus.\")",
            "            minion_privkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "            sig = salt.crypt.sign_message(",
            "                minion_privkey_path, salt.serializers.msgpack.serialize(load)",
            "            )",
            "            load[\"sig\"] = sig",
            "",
            "        with salt.transport.client.ReqChannel.factory(self.opts) as channel:",
            "            return channel.send(load, timeout=timeout)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _send_req_async(self, load, timeout):",
            "",
            "        if self.opts[\"minion_sign_messages\"]:",
            "            log.trace(\"Signing event to be published onto the bus.\")",
            "            minion_privkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "            sig = salt.crypt.sign_message(",
            "                minion_privkey_path, salt.serializers.msgpack.serialize(load)",
            "            )",
            "            load[\"sig\"] = sig",
            "",
            "        with salt.transport.client.AsyncReqChannel.factory(self.opts) as channel:",
            "            ret = yield channel.send(load, timeout=timeout)",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    def _fire_master(",
            "        self,",
            "        data=None,",
            "        tag=None,",
            "        events=None,",
            "        pretag=None,",
            "        timeout=60,",
            "        sync=True,",
            "        timeout_handler=None,",
            "        include_startup_grains=False,",
            "    ):",
            "        \"\"\"",
            "        Fire an event on the master, or drop message if unable to send.",
            "        \"\"\"",
            "        load = {",
            "            \"id\": self.opts[\"id\"],",
            "            \"cmd\": \"_minion_event\",",
            "            \"pretag\": pretag,",
            "            \"tok\": self.tok,",
            "        }",
            "        if events:",
            "            load[\"events\"] = events",
            "        elif data and tag:",
            "            load[\"data\"] = data",
            "            load[\"tag\"] = tag",
            "        elif not data and tag:",
            "            load[\"data\"] = {}",
            "            load[\"tag\"] = tag",
            "        else:",
            "            return",
            "",
            "        if include_startup_grains:",
            "            grains_to_add = {",
            "                k: v",
            "                for k, v in self.opts.get(\"grains\", {}).items()",
            "                if k in self.opts[\"start_event_grains\"]",
            "            }",
            "            load[\"grains\"] = grains_to_add",
            "",
            "        if sync:",
            "            try:",
            "                self._send_req_sync(load, timeout)",
            "            except salt.exceptions.SaltReqTimeoutError:",
            "                log.info(",
            "                    \"fire_master failed: master could not be contacted. Request timed out.\"",
            "                )",
            "                return False",
            "            except Exception:  # pylint: disable=broad-except",
            "                log.info(\"fire_master failed: %s\", traceback.format_exc())",
            "                return False",
            "        else:",
            "            if timeout_handler is None:",
            "",
            "                def handle_timeout(*_):",
            "                    log.info(",
            "                        \"fire_master failed: master could not be contacted. Request timed out.\"",
            "                    )",
            "                    return True",
            "",
            "                timeout_handler = handle_timeout",
            "",
            "            with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "                # pylint: disable=unexpected-keyword-arg",
            "                self._send_req_async(load, timeout, callback=lambda f: None)",
            "                # pylint: enable=unexpected-keyword-arg",
            "        return True",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _handle_decoded_payload(self, data):",
            "        \"\"\"",
            "        Override this method if you wish to handle the decoded data",
            "        differently.",
            "        \"\"\"",
            "        # Ensure payload is unicode. Disregard failure to decode binary blobs.",
            "        if six.PY2:",
            "            data = salt.utils.data.decode(data, keep=True)",
            "        if \"user\" in data:",
            "            log.info(",
            "                \"User %s Executing command %s with jid %s\",",
            "                data[\"user\"],",
            "                data[\"fun\"],",
            "                data[\"jid\"],",
            "            )",
            "        else:",
            "            log.info(\"Executing command %s with jid %s\", data[\"fun\"], data[\"jid\"])",
            "        log.debug(\"Command details %s\", data)",
            "",
            "        # Don't duplicate jobs",
            "        log.trace(\"Started JIDs: %s\", self.jid_queue)",
            "        if self.jid_queue is not None:",
            "            if data[\"jid\"] in self.jid_queue:",
            "                return",
            "            else:",
            "                self.jid_queue.append(data[\"jid\"])",
            "                if len(self.jid_queue) > self.opts[\"minion_jid_queue_hwm\"]:",
            "                    self.jid_queue.pop(0)",
            "",
            "        if isinstance(data[\"fun\"], str):",
            "            if data[\"fun\"] == \"sys.reload_modules\":",
            "                (",
            "                    self.functions,",
            "                    self.returners,",
            "                    self.function_errors,",
            "                    self.executors,",
            "                ) = self._load_modules()",
            "                self.schedule.functions = self.functions",
            "                self.schedule.returners = self.returners",
            "",
            "        process_count_max = self.opts.get(\"process_count_max\")",
            "        if process_count_max > 0:",
            "            process_count = len(salt.utils.minion.running(self.opts))",
            "            while process_count >= process_count_max:",
            "                log.warning(",
            "                    \"Maximum number of processes reached while executing jid %s, waiting...\",",
            "                    data[\"jid\"],",
            "                )",
            "                yield salt.ext.tornado.gen.sleep(10)",
            "                process_count = len(salt.utils.minion.running(self.opts))",
            "",
            "        # We stash an instance references to allow for the socket",
            "        # communication in Windows. You can't pickle functions, and thus",
            "        # python needs to be able to reconstruct the reference on the other",
            "        # side.",
            "        instance = self",
            "        multiprocessing_enabled = self.opts.get(\"multiprocessing\", True)",
            "        if multiprocessing_enabled:",
            "            if sys.platform.startswith(\"win\"):",
            "                # let python reconstruct the minion on the other side if we're",
            "                # running on windows",
            "                instance = None",
            "            with default_signals(signal.SIGINT, signal.SIGTERM):",
            "                process = SignalHandlingProcess(",
            "                    target=self._target,",
            "                    name=\"ProcessPayload\",",
            "                    args=(instance, self.opts, data, self.connected),",
            "                )",
            "                process._after_fork_methods.append(",
            "                    (salt.utils.crypt.reinit_crypto, [], {})",
            "                )",
            "        else:",
            "            process = threading.Thread(",
            "                target=self._target,",
            "                args=(instance, self.opts, data, self.connected),",
            "                name=data[\"jid\"],",
            "            )",
            "",
            "        if multiprocessing_enabled:",
            "            with default_signals(signal.SIGINT, signal.SIGTERM):",
            "                # Reset current signals before starting the process in",
            "                # order not to inherit the current signal handlers",
            "                process.start()",
            "        else:",
            "            process.start()",
            "        process.name = \"{}-Job-{}\".format(process.name, data[\"jid\"])",
            "        self.subprocess_list.add(process)",
            "",
            "    def ctx(self):",
            "        \"\"\"",
            "        Return a single context manager for the minion's data",
            "        \"\"\"",
            "        exitstack = contextlib.ExitStack()",
            "        exitstack.enter_context(self.functions.context_dict.clone())",
            "        exitstack.enter_context(self.returners.context_dict.clone())",
            "        exitstack.enter_context(self.executors.context_dict.clone())",
            "        return exitstack",
            "",
            "    @classmethod",
            "    def _target(cls, minion_instance, opts, data, connected):",
            "        if not minion_instance:",
            "            minion_instance = cls(opts)",
            "            minion_instance.connected = connected",
            "            if not hasattr(minion_instance, \"functions\"):",
            "                (",
            "                    functions,",
            "                    returners,",
            "                    function_errors,",
            "                    executors,",
            "                ) = minion_instance._load_modules(grains=opts[\"grains\"])",
            "                minion_instance.functions = functions",
            "                minion_instance.returners = returners",
            "                minion_instance.function_errors = function_errors",
            "                minion_instance.executors = executors",
            "            if not hasattr(minion_instance, \"serial\"):",
            "                minion_instance.serial = salt.payload.Serial(opts)",
            "            if not hasattr(minion_instance, \"proc_dir\"):",
            "                uid = salt.utils.user.get_uid(user=opts.get(\"user\", None))",
            "                minion_instance.proc_dir = get_proc_dir(opts[\"cachedir\"], uid=uid)",
            "",
            "        def run_func(minion_instance, opts, data):",
            "            if isinstance(data[\"fun\"], tuple) or isinstance(data[\"fun\"], list):",
            "                return Minion._thread_multi_return(minion_instance, opts, data)",
            "            else:",
            "                return Minion._thread_return(minion_instance, opts, data)",
            "",
            "        with salt.ext.tornado.stack_context.StackContext(",
            "            functools.partial(RequestContext, {\"data\": data, \"opts\": opts})",
            "        ):",
            "            with salt.ext.tornado.stack_context.StackContext(minion_instance.ctx):",
            "                run_func(minion_instance, opts, data)",
            "",
            "    def _execute_job_function(",
            "        self, function_name, function_args, executors, opts, data",
            "    ):",
            "        \"\"\"",
            "        Executes a function within a job given it's name, the args and the executors.",
            "        It also checks if the function is allowed to run if 'blackout mode' is enabled.",
            "        \"\"\"",
            "        minion_blackout_violation = False",
            "        if self.connected and self.opts[\"pillar\"].get(\"minion_blackout\", False):",
            "            whitelist = self.opts[\"pillar\"].get(\"minion_blackout_whitelist\", [])",
            "            # this minion is blacked out. Only allow saltutil.refresh_pillar and the whitelist",
            "            if (",
            "                function_name != \"saltutil.refresh_pillar\"",
            "                and function_name not in whitelist",
            "            ):",
            "                minion_blackout_violation = True",
            "        # use minion_blackout_whitelist from grains if it exists",
            "        if self.opts[\"grains\"].get(\"minion_blackout\", False):",
            "            whitelist = self.opts[\"grains\"].get(\"minion_blackout_whitelist\", [])",
            "            if (",
            "                function_name != \"saltutil.refresh_pillar\"",
            "                and function_name not in whitelist",
            "            ):",
            "                minion_blackout_violation = True",
            "        if minion_blackout_violation:",
            "            raise SaltInvocationError(",
            "                \"Minion in blackout mode. Set 'minion_blackout' \"",
            "                \"to False in pillar or grains to resume operations. Only \"",
            "                \"saltutil.refresh_pillar allowed in blackout mode.\"",
            "            )",
            "",
            "        if function_name in self.functions:",
            "            func = self.functions[function_name]",
            "            args, kwargs = load_args_and_kwargs(func, function_args, data)",
            "        else:",
            "            # only run if function_name is not in minion_instance.functions and allow_missing_funcs is True",
            "            func = function_name",
            "            args, kwargs = function_args, data",
            "        self.functions.pack[\"__context__\"][\"retcode\"] = 0",
            "",
            "        if isinstance(executors, str):",
            "            executors = [executors]",
            "        elif not isinstance(executors, list) or not executors:",
            "            raise SaltInvocationError(",
            "                \"Wrong executors specification: {}. String or non-empty list expected\".format(",
            "                    executors",
            "                )",
            "            )",
            "        if opts.get(\"sudo_user\", \"\") and executors[-1] != \"sudo\":",
            "            executors[-1] = \"sudo\"  # replace the last one with sudo",
            "        log.trace(\"Executors list %s\", executors)  # pylint: disable=no-member",
            "",
            "        for name in executors:",
            "            fname = \"{}.execute\".format(name)",
            "            if fname not in self.executors:",
            "                raise SaltInvocationError(\"Executor '{}' is not available\".format(name))",
            "            return_data = self.executors[fname](opts, data, func, args, kwargs)",
            "            if return_data is not None:",
            "                return return_data",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def _thread_return(cls, minion_instance, opts, data):",
            "        \"\"\"",
            "        This method should be used as a threading target, start the actual",
            "        minion side execution.",
            "        \"\"\"",
            "        minion_instance.gen_modules()",
            "        fn_ = os.path.join(minion_instance.proc_dir, data[\"jid\"])",
            "",
            "        salt.utils.process.appendproctitle(",
            "            \"{}._thread_return {}\".format(cls.__name__, data[\"jid\"])",
            "        )",
            "",
            "        sdata = {\"pid\": os.getpid()}",
            "        sdata.update(data)",
            "        log.info(\"Starting a new job %s with PID %s\", data[\"jid\"], sdata[\"pid\"])",
            "        with salt.utils.files.fopen(fn_, \"w+b\") as fp_:",
            "            fp_.write(minion_instance.serial.dumps(sdata))",
            "        ret = {\"success\": False}",
            "        function_name = data[\"fun\"]",
            "        function_args = data[\"arg\"]",
            "        executors = (",
            "            data.get(\"module_executors\")",
            "            or getattr(minion_instance, \"module_executors\", [])",
            "            or opts.get(\"module_executors\", [\"direct_call\"])",
            "        )",
            "        allow_missing_funcs = any(",
            "            [",
            "                minion_instance.executors[\"{}.allow_missing_func\".format(executor)](",
            "                    function_name",
            "                )",
            "                for executor in executors",
            "                if \"{}.allow_missing_func\".format(executor) in minion_instance.executors",
            "            ]",
            "        )",
            "        if function_name in minion_instance.functions or allow_missing_funcs is True:",
            "            try:",
            "                return_data = minion_instance._execute_job_function(",
            "                    function_name, function_args, executors, opts, data",
            "                )",
            "",
            "                if isinstance(return_data, types.GeneratorType):",
            "                    ind = 0",
            "                    iret = {}",
            "                    for single in return_data:",
            "                        if isinstance(single, dict) and isinstance(iret, dict):",
            "                            iret.update(single)",
            "                        else:",
            "                            if not iret:",
            "                                iret = []",
            "                            iret.append(single)",
            "                        tag = tagify([data[\"jid\"], \"prog\", opts[\"id\"], str(ind)], \"job\")",
            "                        event_data = {\"return\": single}",
            "                        minion_instance._fire_master(event_data, tag)",
            "                        ind += 1",
            "                    ret[\"return\"] = iret",
            "                else:",
            "                    ret[\"return\"] = return_data",
            "",
            "                retcode = minion_instance.functions.pack[\"__context__\"].get(",
            "                    \"retcode\", salt.defaults.exitcodes.EX_OK",
            "                )",
            "                if retcode == salt.defaults.exitcodes.EX_OK:",
            "                    # No nonzero retcode in __context__ dunder. Check if return",
            "                    # is a dictionary with a \"result\" or \"success\" key.",
            "                    try:",
            "                        func_result = all(",
            "                            return_data.get(x, True) for x in (\"result\", \"success\")",
            "                        )",
            "                    except Exception:  # pylint: disable=broad-except",
            "                        # return data is not a dict",
            "                        func_result = True",
            "                    if not func_result:",
            "                        retcode = salt.defaults.exitcodes.EX_GENERIC",
            "",
            "                ret[\"retcode\"] = retcode",
            "                ret[\"success\"] = retcode == salt.defaults.exitcodes.EX_OK",
            "            except CommandNotFoundError as exc:",
            "                msg = \"Command required for '{}' not found\".format(function_name)",
            "                log.debug(msg, exc_info=True)",
            "                ret[\"return\"] = \"{}: {}\".format(msg, exc)",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except CommandExecutionError as exc:",
            "                log.error(",
            "                    \"A command in '%s' had a problem: %s\",",
            "                    function_name,",
            "                    exc,",
            "                    exc_info_on_loglevel=logging.DEBUG,",
            "                )",
            "                ret[\"return\"] = \"ERROR: {}\".format(exc)",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except SaltInvocationError as exc:",
            "                log.error(",
            "                    \"Problem executing '%s': %s\",",
            "                    function_name,",
            "                    exc,",
            "                    exc_info_on_loglevel=logging.DEBUG,",
            "                )",
            "                ret[\"return\"] = \"ERROR executing '{}': {}\".format(function_name, exc)",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except TypeError as exc:",
            "                msg = \"Passed invalid arguments to {}: {}\\n{}\".format(",
            "                    function_name,",
            "                    exc,",
            "                    minion_instance.functions[function_name].__doc__ or \"\",",
            "                )",
            "                log.warning(msg, exc_info_on_loglevel=logging.DEBUG)",
            "                ret[\"return\"] = msg",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except Exception:  # pylint: disable=broad-except",
            "                msg = \"The minion function caused an exception\"",
            "                log.warning(msg, exc_info_on_loglevel=True)",
            "                salt.utils.error.fire_exception(",
            "                    salt.exceptions.MinionError(msg), opts, job=data",
            "                )",
            "                ret[\"return\"] = \"{}: {}\".format(msg, traceback.format_exc())",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "        else:",
            "            docs = minion_instance.functions[\"sys.doc\"](\"{}*\".format(function_name))",
            "            if docs:",
            "                docs[function_name] = minion_instance.functions.missing_fun_string(",
            "                    function_name",
            "                )",
            "                ret[\"return\"] = docs",
            "            else:",
            "                ret[\"return\"] = minion_instance.functions.missing_fun_string(",
            "                    function_name",
            "                )",
            "                mod_name = function_name.split(\".\")[0]",
            "                if mod_name in minion_instance.function_errors:",
            "                    ret[\"return\"] += \" Possible reasons: '{}'\".format(",
            "                        minion_instance.function_errors[mod_name]",
            "                    )",
            "            ret[\"success\"] = False",
            "            ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            ret[\"out\"] = \"nested\"",
            "",
            "        ret[\"jid\"] = data[\"jid\"]",
            "        ret[\"fun\"] = data[\"fun\"]",
            "        ret[\"fun_args\"] = data[\"arg\"]",
            "        if \"master_id\" in data:",
            "            ret[\"master_id\"] = data[\"master_id\"]",
            "        if \"metadata\" in data:",
            "            if isinstance(data[\"metadata\"], dict):",
            "                ret[\"metadata\"] = data[\"metadata\"]",
            "            else:",
            "                log.warning(\"The metadata parameter must be a dictionary. Ignoring.\")",
            "        if minion_instance.connected:",
            "            minion_instance._return_pub(",
            "                ret, timeout=minion_instance._return_retry_timer()",
            "            )",
            "",
            "        # Add default returners from minion config",
            "        # Should have been coverted to comma-delimited string already",
            "        if isinstance(opts.get(\"return\"), str):",
            "            if data[\"ret\"]:",
            "                data[\"ret\"] = \",\".join((data[\"ret\"], opts[\"return\"]))",
            "            else:",
            "                data[\"ret\"] = opts[\"return\"]",
            "",
            "        log.debug(\"minion return: %s\", ret)",
            "        # TODO: make a list? Seems odd to split it this late :/",
            "        if data[\"ret\"] and isinstance(data[\"ret\"], str):",
            "            if \"ret_config\" in data:",
            "                ret[\"ret_config\"] = data[\"ret_config\"]",
            "            if \"ret_kwargs\" in data:",
            "                ret[\"ret_kwargs\"] = data[\"ret_kwargs\"]",
            "            ret[\"id\"] = opts[\"id\"]",
            "            for returner in set(data[\"ret\"].split(\",\")):",
            "                try:",
            "                    returner_str = \"{}.returner\".format(returner)",
            "                    if returner_str in minion_instance.returners:",
            "                        minion_instance.returners[returner_str](ret)",
            "                    else:",
            "                        returner_err = minion_instance.returners.missing_fun_string(",
            "                            returner_str",
            "                        )",
            "                        log.error(",
            "                            \"Returner %s could not be loaded: %s\",",
            "                            returner_str,",
            "                            returner_err,",
            "                        )",
            "                except Exception as exc:  # pylint: disable=broad-except",
            "                    log.exception(\"The return failed for job %s: %s\", data[\"jid\"], exc)",
            "",
            "    @classmethod",
            "    def _thread_multi_return(cls, minion_instance, opts, data):",
            "        \"\"\"",
            "        This method should be used as a threading target, start the actual",
            "        minion side execution.",
            "        \"\"\"",
            "        minion_instance.gen_modules()",
            "        fn_ = os.path.join(minion_instance.proc_dir, data[\"jid\"])",
            "",
            "        salt.utils.process.appendproctitle(",
            "            \"{}._thread_multi_return {}\".format(cls.__name__, data[\"jid\"])",
            "        )",
            "",
            "        sdata = {\"pid\": os.getpid()}",
            "        sdata.update(data)",
            "        log.info(\"Starting a new job with PID %s\", sdata[\"pid\"])",
            "        with salt.utils.files.fopen(fn_, \"w+b\") as fp_:",
            "            fp_.write(minion_instance.serial.dumps(sdata))",
            "",
            "        multifunc_ordered = opts.get(\"multifunc_ordered\", False)",
            "        num_funcs = len(data[\"fun\"])",
            "        if multifunc_ordered:",
            "            ret = {",
            "                \"return\": [None] * num_funcs,",
            "                \"retcode\": [None] * num_funcs,",
            "                \"success\": [False] * num_funcs,",
            "            }",
            "        else:",
            "            ret = {\"return\": {}, \"retcode\": {}, \"success\": {}}",
            "        executors = (",
            "            data.get(\"module_executors\")",
            "            or getattr(minion_instance, \"module_executors\", [])",
            "            or opts.get(\"module_executors\", [\"direct_call\"])",
            "        )",
            "",
            "        for ind in range(0, num_funcs):",
            "            function_name = data[\"fun\"][ind]",
            "            function_args = data[\"arg\"][ind]",
            "            if not multifunc_ordered:",
            "                ret[\"success\"][function_name] = False",
            "            try:",
            "                return_data = minion_instance._execute_job_function(",
            "                    function_name, function_args, executors, opts, data",
            "                )",
            "",
            "                key = ind if multifunc_ordered else data[\"fun\"][ind]",
            "                ret[\"return\"][key] = return_data",
            "                retcode = minion_instance.functions.pack[\"__context__\"].get(",
            "                    \"retcode\", 0",
            "                )",
            "                if retcode == 0:",
            "                    # No nonzero retcode in __context__ dunder. Check if return",
            "                    # is a dictionary with a \"result\" or \"success\" key.",
            "                    try:",
            "                        func_result = all(",
            "                            ret[\"return\"][key].get(x, True)",
            "                            for x in (\"result\", \"success\")",
            "                        )",
            "                    except Exception:  # pylint: disable=broad-except",
            "                        # return data is not a dict",
            "                        func_result = True",
            "                    if not func_result:",
            "                        retcode = 1",
            "",
            "                ret[\"retcode\"][key] = retcode",
            "                ret[\"success\"][key] = retcode == 0",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                trb = traceback.format_exc()",
            "                log.warning(\"The minion function caused an exception: %s\", exc)",
            "                if multifunc_ordered:",
            "                    ret[\"return\"][ind] = trb",
            "                else:",
            "                    ret[\"return\"][data[\"fun\"][ind]] = trb",
            "            ret[\"jid\"] = data[\"jid\"]",
            "            ret[\"fun\"] = data[\"fun\"]",
            "            ret[\"fun_args\"] = data[\"arg\"]",
            "        if \"metadata\" in data:",
            "            ret[\"metadata\"] = data[\"metadata\"]",
            "        if minion_instance.connected:",
            "            minion_instance._return_pub(",
            "                ret, timeout=minion_instance._return_retry_timer()",
            "            )",
            "        if data[\"ret\"]:",
            "            if \"ret_config\" in data:",
            "                ret[\"ret_config\"] = data[\"ret_config\"]",
            "            if \"ret_kwargs\" in data:",
            "                ret[\"ret_kwargs\"] = data[\"ret_kwargs\"]",
            "            for returner in set(data[\"ret\"].split(\",\")):",
            "                ret[\"id\"] = opts[\"id\"]",
            "                try:",
            "                    minion_instance.returners[\"{}.returner\".format(returner)](ret)",
            "                except Exception as exc:  # pylint: disable=broad-except",
            "                    log.error(\"The return failed for job %s: %s\", data[\"jid\"], exc)",
            "",
            "    def _return_pub(self, ret, ret_cmd=\"_return\", timeout=60, sync=True):",
            "        \"\"\"",
            "        Return the data from the executed command to the master server",
            "        \"\"\"",
            "        jid = ret.get(\"jid\", ret.get(\"__jid__\"))",
            "        fun = ret.get(\"fun\", ret.get(\"__fun__\"))",
            "        if self.opts[\"multiprocessing\"]:",
            "            fn_ = os.path.join(self.proc_dir, jid)",
            "            if os.path.isfile(fn_):",
            "                try:",
            "                    os.remove(fn_)",
            "                except OSError:",
            "                    # The file is gone already",
            "                    pass",
            "        log.info(\"Returning information for job: %s\", jid)",
            "        log.trace(\"Return data: %s\", ret)",
            "        if ret_cmd == \"_syndic_return\":",
            "            load = {",
            "                \"cmd\": ret_cmd,",
            "                \"id\": self.opts[\"uid\"],",
            "                \"jid\": jid,",
            "                \"fun\": fun,",
            "                \"arg\": ret.get(\"arg\"),",
            "                \"tgt\": ret.get(\"tgt\"),",
            "                \"tgt_type\": ret.get(\"tgt_type\"),",
            "                \"load\": ret.get(\"__load__\"),",
            "            }",
            "            if \"__master_id__\" in ret:",
            "                load[\"master_id\"] = ret[\"__master_id__\"]",
            "            load[\"return\"] = {}",
            "            for key, value in ret.items():",
            "                if key.startswith(\"__\"):",
            "                    continue",
            "                load[\"return\"][key] = value",
            "        else:",
            "            load = {\"cmd\": ret_cmd, \"id\": self.opts[\"id\"]}",
            "            for key, value in ret.items():",
            "                load[key] = value",
            "",
            "        if \"out\" in ret:",
            "            if isinstance(ret[\"out\"], str):",
            "                load[\"out\"] = ret[\"out\"]",
            "            else:",
            "                log.error(\"Invalid outputter %s. This is likely a bug.\", ret[\"out\"])",
            "        else:",
            "            try:",
            "                oput = self.functions[fun].__outputter__",
            "            except (KeyError, AttributeError, TypeError):",
            "                pass",
            "            else:",
            "                if isinstance(oput, str):",
            "                    load[\"out\"] = oput",
            "        if self.opts[\"cache_jobs\"]:",
            "            # Local job cache has been enabled",
            "            if ret[\"jid\"] == \"req\":",
            "                ret[\"jid\"] = salt.utils.jid.gen_jid(self.opts)",
            "            salt.utils.minion.cache_jobs(self.opts, ret[\"jid\"], ret)",
            "",
            "        if not self.opts[\"pub_ret\"]:",
            "            return \"\"",
            "",
            "        def timeout_handler(*_):",
            "            log.warning(",
            "                \"The minion failed to return the job information for job %s. \"",
            "                \"This is often due to the master being shut down or \"",
            "                \"overloaded. If the master is running, consider increasing \"",
            "                \"the worker_threads value.\",",
            "                jid,",
            "            )",
            "            return True",
            "",
            "        if sync:",
            "            try:",
            "                ret_val = self._send_req_sync(load, timeout=timeout)",
            "            except SaltReqTimeoutError:",
            "                timeout_handler()",
            "                return \"\"",
            "        else:",
            "            with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "                # pylint: disable=unexpected-keyword-arg",
            "                ret_val = self._send_req_async(",
            "                    load, timeout=timeout, callback=lambda f: None",
            "                )",
            "                # pylint: enable=unexpected-keyword-arg",
            "",
            "        log.trace(\"ret_val = %s\", ret_val)  # pylint: disable=no-member",
            "        return ret_val",
            "",
            "    def _return_pub_multi(self, rets, ret_cmd=\"_return\", timeout=60, sync=True):",
            "        \"\"\"",
            "        Return the data from the executed command to the master server",
            "        \"\"\"",
            "        if not isinstance(rets, list):",
            "            rets = [rets]",
            "        jids = {}",
            "        for ret in rets:",
            "            jid = ret.get(\"jid\", ret.get(\"__jid__\"))",
            "            fun = ret.get(\"fun\", ret.get(\"__fun__\"))",
            "            if self.opts[\"multiprocessing\"]:",
            "                fn_ = os.path.join(self.proc_dir, jid)",
            "                if os.path.isfile(fn_):",
            "                    try:",
            "                        os.remove(fn_)",
            "                    except OSError:",
            "                        # The file is gone already",
            "                        pass",
            "            log.info(\"Returning information for job: %s\", jid)",
            "            load = jids.setdefault(jid, {})",
            "            if ret_cmd == \"_syndic_return\":",
            "                if not load:",
            "                    load.update(",
            "                        {",
            "                            \"id\": self.opts[\"id\"],",
            "                            \"jid\": jid,",
            "                            \"fun\": fun,",
            "                            \"arg\": ret.get(\"arg\"),",
            "                            \"tgt\": ret.get(\"tgt\"),",
            "                            \"tgt_type\": ret.get(\"tgt_type\"),",
            "                            \"load\": ret.get(\"__load__\"),",
            "                            \"return\": {},",
            "                        }",
            "                    )",
            "                if \"__master_id__\" in ret:",
            "                    load[\"master_id\"] = ret[\"__master_id__\"]",
            "                for key, value in ret.items():",
            "                    if key.startswith(\"__\"):",
            "                        continue",
            "                    load[\"return\"][key] = value",
            "            else:",
            "                load.update({\"id\": self.opts[\"id\"]})",
            "                for key, value in ret.items():",
            "                    load[key] = value",
            "",
            "            if \"out\" in ret:",
            "                if isinstance(ret[\"out\"], str):",
            "                    load[\"out\"] = ret[\"out\"]",
            "                else:",
            "                    log.error(\"Invalid outputter %s. This is likely a bug.\", ret[\"out\"])",
            "            else:",
            "                try:",
            "                    oput = self.functions[fun].__outputter__",
            "                except (KeyError, AttributeError, TypeError):",
            "                    pass",
            "                else:",
            "                    if isinstance(oput, str):",
            "                        load[\"out\"] = oput",
            "            if self.opts[\"cache_jobs\"]:",
            "                # Local job cache has been enabled",
            "                salt.utils.minion.cache_jobs(self.opts, load[\"jid\"], ret)",
            "",
            "        load = {\"cmd\": ret_cmd, \"load\": list(jids.values())}",
            "",
            "        def timeout_handler(*_):",
            "            log.warning(",
            "                \"The minion failed to return the job information for job %s. \"",
            "                \"This is often due to the master being shut down or \"",
            "                \"overloaded. If the master is running, consider increasing \"",
            "                \"the worker_threads value.\",",
            "                jid,",
            "            )",
            "            return True",
            "",
            "        if sync:",
            "            try:",
            "                ret_val = self._send_req_sync(load, timeout=timeout)",
            "            except SaltReqTimeoutError:",
            "                timeout_handler()",
            "                return \"\"",
            "        else:",
            "            with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "                # pylint: disable=unexpected-keyword-arg",
            "                ret_val = self._send_req_async(",
            "                    load, timeout=timeout, callback=lambda f: None",
            "                )",
            "                # pylint: enable=unexpected-keyword-arg",
            "",
            "        log.trace(\"ret_val = %s\", ret_val)  # pylint: disable=no-member",
            "        return ret_val",
            "",
            "    def _state_run(self):",
            "        \"\"\"",
            "        Execute a state run based on information set in the minion config file",
            "        \"\"\"",
            "        if self.opts[\"startup_states\"]:",
            "            if (",
            "                self.opts.get(\"master_type\", \"str\") == \"disable\"",
            "                and self.opts.get(\"file_client\", \"remote\") == \"remote\"",
            "            ):",
            "                log.warning(",
            "                    \"Cannot run startup_states when 'master_type' is set \"",
            "                    \"to 'disable' and 'file_client' is set to \"",
            "                    \"'remote'. Skipping.\"",
            "                )",
            "            else:",
            "                data = {\"jid\": \"req\", \"ret\": self.opts.get(\"ext_job_cache\", \"\")}",
            "                if self.opts[\"startup_states\"] == \"sls\":",
            "                    data[\"fun\"] = \"state.sls\"",
            "                    data[\"arg\"] = [self.opts[\"sls_list\"]]",
            "                elif self.opts[\"startup_states\"] == \"top\":",
            "                    data[\"fun\"] = \"state.top\"",
            "                    data[\"arg\"] = [self.opts[\"top_file\"]]",
            "                else:",
            "                    data[\"fun\"] = \"state.highstate\"",
            "                    data[\"arg\"] = []",
            "                self._handle_decoded_payload(data)",
            "",
            "    def _refresh_grains_watcher(self, refresh_interval_in_minutes):",
            "        \"\"\"",
            "        Create a loop that will fire a pillar refresh to inform a master about a change in the grains of this minion",
            "        :param refresh_interval_in_minutes:",
            "        :return: None",
            "        \"\"\"",
            "        if \"__update_grains\" not in self.opts.get(\"schedule\", {}):",
            "            if \"schedule\" not in self.opts:",
            "                self.opts[\"schedule\"] = {}",
            "            self.opts[\"schedule\"].update(",
            "                {",
            "                    \"__update_grains\": {",
            "                        \"function\": \"event.fire\",",
            "                        \"args\": [{}, \"grains_refresh\"],",
            "                        \"minutes\": refresh_interval_in_minutes,",
            "                    }",
            "                }",
            "            )",
            "",
            "    def _fire_master_minion_start(self):",
            "        include_grains = False",
            "        if self.opts[\"start_event_grains\"]:",
            "            include_grains = True",
            "        # Send an event to the master that the minion is live",
            "        if self.opts[\"enable_legacy_startup_events\"]:",
            "            # Old style event. Defaults to False in 3001 release.",
            "            self._fire_master(",
            "                \"Minion {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "                \"minion_start\",",
            "                include_startup_grains=include_grains,",
            "            )",
            "        # send name spaced event",
            "        self._fire_master(",
            "            \"Minion {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "            tagify([self.opts[\"id\"], \"start\"], \"minion\"),",
            "            include_startup_grains=include_grains,",
            "        )",
            "",
            "    def module_refresh(self, force_refresh=False, notify=False):",
            "        \"\"\"",
            "        Refresh the functions and returners.",
            "        \"\"\"",
            "        log.debug(\"Refreshing modules. Notify=%s\", notify)",
            "        self.functions, self.returners, _, self.executors = self._load_modules(",
            "            force_refresh, notify=notify",
            "        )",
            "",
            "        self.schedule.functions = self.functions",
            "        self.schedule.returners = self.returners",
            "",
            "    def beacons_refresh(self):",
            "        \"\"\"",
            "        Refresh the functions and returners.",
            "        \"\"\"",
            "        if not self.beacons_leader:",
            "            return",
            "        log.debug(\"Refreshing beacons.\")",
            "        self.beacons = salt.beacons.Beacon(self.opts, self.functions)",
            "",
            "    def matchers_refresh(self):",
            "        \"\"\"",
            "        Refresh the matchers",
            "        \"\"\"",
            "        log.debug(\"Refreshing matchers.\")",
            "        self.matchers = salt.loader.matchers(self.opts)",
            "",
            "    # TODO: only allow one future in flight at a time?",
            "    @salt.ext.tornado.gen.coroutine",
            "    def pillar_refresh(self, force_refresh=False):",
            "        \"\"\"",
            "        Refresh the pillar",
            "        \"\"\"",
            "        self.module_refresh(force_refresh)",
            "",
            "        if self.connected:",
            "            log.debug(\"Refreshing pillar.\")",
            "            async_pillar = salt.pillar.get_async_pillar(",
            "                self.opts,",
            "                self.opts[\"grains\"],",
            "                self.opts[\"id\"],",
            "                self.opts[\"saltenv\"],",
            "                pillarenv=self.opts.get(\"pillarenv\"),",
            "            )",
            "            try:",
            "                self.opts[\"pillar\"] = yield async_pillar.compile_pillar()",
            "            except SaltClientError:",
            "                # Do not exit if a pillar refresh fails.",
            "                log.error(",
            "                    \"Pillar data could not be refreshed. \"",
            "                    \"One or more masters may be down!\"",
            "                )",
            "            finally:",
            "                async_pillar.destroy()",
            "        self.matchers_refresh()",
            "        self.beacons_refresh()",
            "        with salt.utils.event.get_event(\"minion\", opts=self.opts, listen=False) as evt:",
            "            evt.fire_event(",
            "                {\"complete\": True},",
            "                tag=salt.defaults.events.MINION_PILLAR_REFRESH_COMPLETE,",
            "            )",
            "",
            "    def manage_schedule(self, tag, data):",
            "        \"\"\"",
            "        Refresh the functions and returners.",
            "        \"\"\"",
            "        func = data.get(\"func\", None)",
            "        name = data.get(\"name\", None)",
            "        schedule = data.get(\"schedule\", None)",
            "        where = data.get(\"where\", None)",
            "        persist = data.get(\"persist\", None)",
            "",
            "        funcs = {",
            "            \"delete\": (\"delete_job\", (name, persist)),",
            "            \"add\": (\"add_job\", (schedule, persist)),",
            "            \"modify\": (\"modify_job\", (name, schedule, persist)),",
            "            \"enable\": (\"enable_schedule\", (persist,)),",
            "            \"disable\": (\"disable_schedule\", (persist,)),",
            "            \"enable_job\": (\"enable_job\", (name, persist)),",
            "            \"run_job\": (\"run_job\", (name,)),",
            "            \"disable_job\": (\"disable_job\", (name, persist)),",
            "            \"postpone_job\": (\"postpone_job\", (name, data)),",
            "            \"skip_job\": (\"skip_job\", (name, data)),",
            "            \"reload\": (\"reload\", (schedule,)),",
            "            \"list\": (\"list\", (where,)),",
            "            \"save_schedule\": (\"save_schedule\", ()),",
            "            \"get_next_fire_time\": (\"get_next_fire_time\", (name,)),",
            "        }",
            "",
            "        # Call the appropriate schedule function",
            "        try:",
            "            alias, params = funcs.get(func)",
            "            getattr(self.schedule, alias)(*params)",
            "        except TypeError:",
            "            log.error('Function \"%s\" is unavailable in salt.utils.scheduler', func)",
            "",
            "    def manage_beacons(self, tag, data):",
            "        \"\"\"",
            "        Manage Beacons",
            "        \"\"\"",
            "        if not self.beacons_leader:",
            "            return",
            "",
            "        func = data.get(\"func\", None)",
            "        name = data.get(\"name\", None)",
            "        beacon_data = data.get(\"beacon_data\", None)",
            "        include_pillar = data.get(\"include_pillar\", None)",
            "        include_opts = data.get(\"include_opts\", None)",
            "",
            "        funcs = {",
            "            \"add\": (\"add_beacon\", (name, beacon_data)),",
            "            \"modify\": (\"modify_beacon\", (name, beacon_data)),",
            "            \"delete\": (\"delete_beacon\", (name,)),",
            "            \"enable\": (\"enable_beacons\", ()),",
            "            \"disable\": (\"disable_beacons\", ()),",
            "            \"enable_beacon\": (\"enable_beacon\", (name,)),",
            "            \"disable_beacon\": (\"disable_beacon\", (name,)),",
            "            \"list\": (\"list_beacons\", (include_opts, include_pillar)),",
            "            \"list_available\": (\"list_available_beacons\", ()),",
            "            \"validate_beacon\": (\"validate_beacon\", (name, beacon_data)),",
            "            \"reset\": (\"reset\", ()),",
            "        }",
            "",
            "        # Call the appropriate beacon function",
            "        try:",
            "            alias, params = funcs.get(func)",
            "            getattr(self.beacons, alias)(*params)",
            "        except TypeError:",
            "            log.error('Function \"%s\" is unavailable in salt.utils.beacons', func)",
            "",
            "    def environ_setenv(self, tag, data):",
            "        \"\"\"",
            "        Set the salt-minion main process environment according to",
            "        the data contained in the minion event data",
            "        \"\"\"",
            "        environ = data.get(\"environ\", None)",
            "        if environ is None:",
            "            return False",
            "        false_unsets = data.get(\"false_unsets\", False)",
            "        clear_all = data.get(\"clear_all\", False)",
            "        import salt.modules.environ as mod_environ",
            "",
            "        return mod_environ.setenv(environ, false_unsets, clear_all)",
            "",
            "    def _pre_tune(self):",
            "        \"\"\"",
            "        Set the minion running flag and issue the appropriate warnings if",
            "        the minion cannot be started or is already running",
            "        \"\"\"",
            "        if self._running is None:",
            "            self._running = True",
            "        elif self._running is False:",
            "            log.error(",
            "                \"This %s was scheduled to stop. Not running %s.tune_in()\",",
            "                self.__class__.__name__,",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "        elif self._running is True:",
            "            log.error(",
            "                \"This %s is already running. Not running %s.tune_in()\",",
            "                self.__class__.__name__,",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "",
            "        try:",
            "            log.info(",
            "                \"%s is starting as user '%s'\",",
            "                self.__class__.__name__,",
            "                salt.utils.user.get_user(),",
            "            )",
            "        except Exception as err:  # pylint: disable=broad-except",
            "            # Only windows is allowed to fail here. See #3189. Log as debug in",
            "            # that case. Else, error.",
            "            log.log(",
            "                salt.utils.platform.is_windows() and logging.DEBUG or logging.ERROR,",
            "                \"Failed to get the user who is starting %s\",",
            "                self.__class__.__name__,",
            "                exc_info=err,",
            "            )",
            "",
            "    def _mine_send(self, tag, data):",
            "        \"\"\"",
            "        Send mine data to the master",
            "        \"\"\"",
            "        with salt.transport.client.ReqChannel.factory(self.opts) as channel:",
            "            data[\"tok\"] = self.tok",
            "            try:",
            "                ret = channel.send(data)",
            "                return ret",
            "            except SaltReqTimeoutError:",
            "                log.warning(\"Unable to send mine data to master.\")",
            "                return None",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_event(self, package):",
            "        \"\"\"",
            "        Handle an event from the epull_sock (all local minion events)",
            "        \"\"\"",
            "        if not self.ready:",
            "            raise salt.ext.tornado.gen.Return()",
            "        tag, data = salt.utils.event.SaltEvent.unpack(package)",
            "",
            "        if \"proxy_target\" in data and self.opts.get(\"metaproxy\") == \"deltaproxy\":",
            "            proxy_target = data[\"proxy_target\"]",
            "            _minion = self.deltaproxy_objs[proxy_target]",
            "        else:",
            "            _minion = self",
            "",
            "        log.debug(\"Minion of '%s' is handling event tag '%s'\", self.opts[\"master\"], tag)",
            "        if tag.startswith(\"module_refresh\"):",
            "            _minion.module_refresh(",
            "                force_refresh=data.get(\"force_refresh\", False),",
            "                notify=data.get(\"notify\", False),",
            "            )",
            "        elif tag.startswith(\"pillar_refresh\"):",
            "            yield _minion.pillar_refresh(force_refresh=data.get(\"force_refresh\", False))",
            "        elif tag.startswith(\"beacons_refresh\"):",
            "            _minion.beacons_refresh()",
            "        elif tag.startswith(\"matchers_refresh\"):",
            "            _minion.matchers_refresh()",
            "        elif tag.startswith(\"manage_schedule\"):",
            "            _minion.manage_schedule(tag, data)",
            "        elif tag.startswith(\"manage_beacons\"):",
            "            _minion.manage_beacons(tag, data)",
            "        elif tag.startswith(\"grains_refresh\"):",
            "            if (",
            "                data.get(\"force_refresh\", False)",
            "                or _minion.grains_cache != _minion.opts[\"grains\"]",
            "            ):",
            "                _minion.pillar_refresh(force_refresh=True)",
            "                _minion.grains_cache = _minion.opts[\"grains\"]",
            "        elif tag.startswith(\"environ_setenv\"):",
            "            self.environ_setenv(tag, data)",
            "        elif tag.startswith(\"_minion_mine\"):",
            "            self._mine_send(tag, data)",
            "        elif tag.startswith(\"fire_master\"):",
            "            if self.connected:",
            "                log.debug(\"Forwarding master event tag=%s\", data[\"tag\"])",
            "                self._fire_master(",
            "                    data[\"data\"],",
            "                    data[\"tag\"],",
            "                    data[\"events\"],",
            "                    data[\"pretag\"],",
            "                    sync=False,",
            "                )",
            "        elif tag.startswith(master_event(type=\"disconnected\")) or tag.startswith(",
            "            master_event(type=\"failback\")",
            "        ):",
            "            # if the master disconnect event is for a different master, raise an exception",
            "            if (",
            "                tag.startswith(master_event(type=\"disconnected\"))",
            "                and data[\"master\"] != self.opts[\"master\"]",
            "            ):",
            "                # not mine master, ignore",
            "                raise salt.ext.tornado.gen.Return()",
            "            if tag.startswith(master_event(type=\"failback\")):",
            "                # if the master failback event is not for the top master, raise an exception",
            "                if data[\"master\"] != self.opts[\"master_list\"][0]:",
            "                    raise SaltException(",
            "                        \"Bad master '{}' when mine failback is '{}'\".format(",
            "                            data[\"master\"], self.opts[\"master\"]",
            "                        )",
            "                    )",
            "                # if the master failback event is for the current master, raise an exception",
            "                elif data[\"master\"] == self.opts[\"master\"][0]:",
            "                    raise SaltException(",
            "                        \"Already connected to '{}'\".format(data[\"master\"])",
            "                    )",
            "",
            "            if self.connected:",
            "                # we are not connected anymore",
            "                self.connected = False",
            "                log.info(\"Connection to master %s lost\", self.opts[\"master\"])",
            "",
            "                if self.opts[\"master_type\"] != \"failover\":",
            "                    # modify the scheduled job to fire on reconnect",
            "                    if self.opts[\"transport\"] != \"tcp\":",
            "                        schedule = {",
            "                            \"function\": \"status.master\",",
            "                            \"seconds\": self.opts[\"master_alive_interval\"],",
            "                            \"jid_include\": True,",
            "                            \"maxrunning\": 1,",
            "                            \"return_job\": False,",
            "                            \"kwargs\": {",
            "                                \"master\": self.opts[\"master\"],",
            "                                \"connected\": False,",
            "                            },",
            "                        }",
            "                        self.schedule.modify_job(",
            "                            name=master_event(type=\"alive\", master=self.opts[\"master\"]),",
            "                            schedule=schedule,",
            "                        )",
            "                else:",
            "                    # delete the scheduled job to don't interfere with the failover process",
            "                    if self.opts[\"transport\"] != \"tcp\":",
            "                        self.schedule.delete_job(name=master_event(type=\"alive\"))",
            "",
            "                    log.info(\"Trying to tune in to next master from master-list\")",
            "",
            "                    if hasattr(self, \"pub_channel\"):",
            "                        self.pub_channel.on_recv(None)",
            "                        if hasattr(self.pub_channel, \"auth\"):",
            "                            self.pub_channel.auth.invalidate()",
            "                        if hasattr(self.pub_channel, \"close\"):",
            "                            self.pub_channel.close()",
            "                        del self.pub_channel",
            "",
            "                    # if eval_master finds a new master for us, self.connected",
            "                    # will be True again on successful master authentication",
            "                    try:",
            "                        master, self.pub_channel = yield self.eval_master(",
            "                            opts=self.opts,",
            "                            failed=True,",
            "                            failback=tag.startswith(master_event(type=\"failback\")),",
            "                        )",
            "                    except SaltClientError:",
            "                        pass",
            "",
            "                    if self.connected:",
            "                        self.opts[\"master\"] = master",
            "",
            "                        # re-init the subsystems to work with the new master",
            "                        log.info(",
            "                            \"Re-initialising subsystems for new master %s\",",
            "                            self.opts[\"master\"],",
            "                        )",
            "                        # put the current schedule into the new loaders",
            "                        self.opts[\"schedule\"] = self.schedule.option(\"schedule\")",
            "                        (",
            "                            self.functions,",
            "                            self.returners,",
            "                            self.function_errors,",
            "                            self.executors,",
            "                        ) = self._load_modules()",
            "                        # make the schedule to use the new 'functions' loader",
            "                        self.schedule.functions = self.functions",
            "                        self.pub_channel.on_recv(self._handle_payload)",
            "                        self._fire_master_minion_start()",
            "                        log.info(\"Minion is ready to receive requests!\")",
            "",
            "                        # update scheduled job to run with the new master addr",
            "                        if self.opts[\"transport\"] != \"tcp\":",
            "                            schedule = {",
            "                                \"function\": \"status.master\",",
            "                                \"seconds\": self.opts[\"master_alive_interval\"],",
            "                                \"jid_include\": True,",
            "                                \"maxrunning\": 1,",
            "                                \"return_job\": False,",
            "                                \"kwargs\": {",
            "                                    \"master\": self.opts[\"master\"],",
            "                                    \"connected\": True,",
            "                                },",
            "                            }",
            "                            self.schedule.modify_job(",
            "                                name=master_event(",
            "                                    type=\"alive\", master=self.opts[\"master\"]",
            "                                ),",
            "                                schedule=schedule,",
            "                            )",
            "",
            "                            if (",
            "                                self.opts[\"master_failback\"]",
            "                                and \"master_list\" in self.opts",
            "                            ):",
            "                                if self.opts[\"master\"] != self.opts[\"master_list\"][0]:",
            "                                    schedule = {",
            "                                        \"function\": \"status.ping_master\",",
            "                                        \"seconds\": self.opts[",
            "                                            \"master_failback_interval\"",
            "                                        ],",
            "                                        \"jid_include\": True,",
            "                                        \"maxrunning\": 1,",
            "                                        \"return_job\": False,",
            "                                        \"kwargs\": {",
            "                                            \"master\": self.opts[\"master_list\"][0]",
            "                                        },",
            "                                    }",
            "                                    self.schedule.modify_job(",
            "                                        name=master_event(type=\"failback\"),",
            "                                        schedule=schedule,",
            "                                    )",
            "                                else:",
            "                                    self.schedule.delete_job(",
            "                                        name=master_event(type=\"failback\"), persist=True",
            "                                    )",
            "                    else:",
            "                        self.restart = True",
            "                        self.io_loop.stop()",
            "",
            "        elif tag.startswith(master_event(type=\"connected\")):",
            "            # handle this event only once. otherwise it will pollute the log",
            "            # also if master type is failover all the reconnection work is done",
            "            # by `disconnected` event handler and this event must never happen,",
            "            # anyway check it to be sure",
            "            if not self.connected and self.opts[\"master_type\"] != \"failover\":",
            "                log.info(\"Connection to master %s re-established\", self.opts[\"master\"])",
            "                self.connected = True",
            "                # modify the __master_alive job to only fire,",
            "                # if the connection is lost again",
            "                if self.opts[\"transport\"] != \"tcp\":",
            "                    schedule = {",
            "                        \"function\": \"status.master\",",
            "                        \"seconds\": self.opts[\"master_alive_interval\"],",
            "                        \"jid_include\": True,",
            "                        \"maxrunning\": 1,",
            "                        \"return_job\": False,",
            "                        \"kwargs\": {\"master\": self.opts[\"master\"], \"connected\": True},",
            "                    }",
            "",
            "                    self.schedule.modify_job(",
            "                        name=master_event(type=\"alive\", master=self.opts[\"master\"]),",
            "                        schedule=schedule,",
            "                    )",
            "        elif tag.startswith(\"__schedule_return\"):",
            "            # reporting current connection with master",
            "            if data[\"schedule\"].startswith(master_event(type=\"alive\", master=\"\")):",
            "                if data[\"return\"]:",
            "                    log.debug(",
            "                        \"Connected to master %s\",",
            "                        data[\"schedule\"].split(master_event(type=\"alive\", master=\"\"))[",
            "                            1",
            "                        ],",
            "                    )",
            "            self._return_pub(data, ret_cmd=\"_return\", sync=False)",
            "        elif tag.startswith(\"_salt_error\"):",
            "            if self.connected:",
            "                log.debug(\"Forwarding salt error event tag=%s\", tag)",
            "                self._fire_master(data, tag, sync=False)",
            "        elif tag.startswith(\"salt/auth/creds\"):",
            "            key = tuple(data[\"key\"])",
            "            log.debug(",
            "                \"Updating auth data for %s: %s -> %s\",",
            "                key,",
            "                salt.crypt.AsyncAuth.creds_map.get(key),",
            "                data[\"creds\"],",
            "            )",
            "            salt.crypt.AsyncAuth.creds_map[tuple(data[\"key\"])] = data[\"creds\"]",
            "        elif tag.startswith(\"__beacons_return\"):",
            "            if self.connected:",
            "                log.debug(\"Firing beacons to master\")",
            "                self._fire_master(events=data[\"beacons\"])",
            "",
            "    def cleanup_subprocesses(self):",
            "        \"\"\"",
            "        Clean up subprocesses and spawned threads.",
            "        \"\"\"",
            "        # Add an extra fallback in case a forked process leaks through",
            "        multiprocessing.active_children()",
            "        self.subprocess_list.cleanup()",
            "        if self.schedule:",
            "            self.schedule.cleanup_subprocesses()",
            "",
            "    def _setup_core(self):",
            "        \"\"\"",
            "        Set up the core minion attributes.",
            "        This is safe to call multiple times.",
            "        \"\"\"",
            "        if not self.ready:",
            "            # First call. Initialize.",
            "            (",
            "                self.functions,",
            "                self.returners,",
            "                self.function_errors,",
            "                self.executors,",
            "            ) = self._load_modules()",
            "            self.serial = salt.payload.Serial(self.opts)",
            "            self.mod_opts = self._prep_mod_opts()",
            "            #            self.matcher = Matcher(self.opts, self.functions)",
            "            self.matchers = salt.loader.matchers(self.opts)",
            "            if self.beacons_leader:",
            "                self.beacons = salt.beacons.Beacon(self.opts, self.functions)",
            "            uid = salt.utils.user.get_uid(user=self.opts.get(\"user\", None))",
            "            self.proc_dir = get_proc_dir(self.opts[\"cachedir\"], uid=uid)",
            "            self.grains_cache = self.opts[\"grains\"]",
            "            self.ready = True",
            "",
            "    def setup_beacons(self, before_connect=False):",
            "        \"\"\"",
            "        Set up the beacons.",
            "        This is safe to call multiple times.",
            "        \"\"\"",
            "        # In multimaster configuration the only one minion shall execute beacons",
            "        if not self.beacons_leader:",
            "            return",
            "",
            "        self._setup_core()",
            "        loop_interval = self.opts[\"loop_interval\"]",
            "        if \"beacons\" not in self.periodic_callbacks:",
            "            self.beacons = salt.beacons.Beacon(self.opts, self.functions)",
            "",
            "            def handle_beacons():",
            "                # Process Beacons",
            "                beacons = None",
            "                try:",
            "                    beacons = self.process_beacons(self.functions)",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.critical(\"The beacon errored: \", exc_info=True)",
            "                if beacons:",
            "                    event = salt.utils.event.get_event(",
            "                        \"minion\", opts=self.opts, listen=False",
            "                    )",
            "                    event.fire_event({\"beacons\": beacons}, \"__beacons_return\")",
            "                    event.destroy()",
            "",
            "            if before_connect:",
            "                # Make sure there is a chance for one iteration to occur before connect",
            "                handle_beacons()",
            "",
            "            self.add_periodic_callback(\"beacons\", handle_beacons)",
            "",
            "    def setup_scheduler(self, before_connect=False):",
            "        \"\"\"",
            "        Set up the scheduler.",
            "        This is safe to call multiple times.",
            "        \"\"\"",
            "        self._setup_core()",
            "",
            "        loop_interval = self.opts[\"loop_interval\"]",
            "",
            "        if \"schedule\" not in self.periodic_callbacks:",
            "            if \"schedule\" not in self.opts:",
            "                self.opts[\"schedule\"] = {}",
            "            if not hasattr(self, \"schedule\"):",
            "                self.schedule = salt.utils.schedule.Schedule(",
            "                    self.opts,",
            "                    self.functions,",
            "                    self.returners,",
            "                    utils=self.utils,",
            "                    cleanup=[master_event(type=\"alive\")],",
            "                )",
            "",
            "            try:",
            "                if self.opts[\"grains_refresh_every\"]:  # In minutes, not seconds!",
            "                    log.debug(",
            "                        \"Enabling the grains refresher. Will run every %d minute(s).\",",
            "                        self.opts[\"grains_refresh_every\"],",
            "                    )",
            "                    self._refresh_grains_watcher(abs(self.opts[\"grains_refresh_every\"]))",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                log.error(",
            "                    \"Exception occurred in attempt to initialize grain refresh \"",
            "                    \"routine during minion tune-in: %s\",",
            "                    exc,",
            "                )",
            "",
            "            # TODO: actually listen to the return and change period",
            "            def handle_schedule():",
            "                self.process_schedule(self, loop_interval)",
            "",
            "            if before_connect:",
            "                # Make sure there is a chance for one iteration to occur before connect",
            "                handle_schedule()",
            "",
            "            self.add_periodic_callback(\"schedule\", handle_schedule)",
            "",
            "    def add_periodic_callback(self, name, method, interval=1):",
            "        \"\"\"",
            "        Add a periodic callback to the event loop and call its start method.",
            "        If a callback by the given name exists this method returns False",
            "        \"\"\"",
            "        if name in self.periodic_callbacks:",
            "            return False",
            "        self.periodic_callbacks[name] = salt.ext.tornado.ioloop.PeriodicCallback(",
            "            method, interval * 1000,",
            "        )",
            "        self.periodic_callbacks[name].start()",
            "        return True",
            "",
            "    def remove_periodic_callback(self, name):",
            "        \"\"\"",
            "        Remove a periodic callback.",
            "        If a callback by the given name does not exist this method returns False",
            "        \"\"\"",
            "        callback = self.periodic_callbacks.pop(name, None)",
            "        if callback is None:",
            "            return False",
            "        callback.stop()",
            "        return True",
            "",
            "    # Main Minion Tune In",
            "    def tune_in(self, start=True):",
            "        \"\"\"",
            "        Lock onto the publisher. This is the main event loop for the minion",
            "        :rtype : None",
            "        \"\"\"",
            "        self._pre_tune()",
            "",
            "        log.debug(\"Minion '%s' trying to tune in\", self.opts[\"id\"])",
            "",
            "        if start:",
            "            if self.opts.get(\"beacons_before_connect\", False):",
            "                self.setup_beacons(before_connect=True)",
            "            if self.opts.get(\"scheduler_before_connect\", False):",
            "                self.setup_scheduler(before_connect=True)",
            "            self.sync_connect_master()",
            "        if self.connected:",
            "            self._fire_master_minion_start()",
            "            log.info(\"Minion is ready to receive requests!\")",
            "",
            "        # Make sure to gracefully handle SIGUSR1",
            "        enable_sigusr1_handler()",
            "",
            "        # Make sure to gracefully handle CTRL_LOGOFF_EVENT",
            "        if HAS_WIN_FUNCTIONS:",
            "            salt.utils.win_functions.enable_ctrl_logoff_handler()",
            "",
            "        # On first startup execute a state run if configured to do so",
            "        self._state_run()",
            "",
            "        self.setup_beacons()",
            "        self.setup_scheduler()",
            "        self.add_periodic_callback(\"cleanup\", self.cleanup_subprocesses)",
            "",
            "        # schedule the stuff that runs every interval",
            "        ping_interval = self.opts.get(\"ping_interval\", 0) * 60",
            "        if ping_interval > 0 and self.connected:",
            "",
            "            def ping_master():",
            "                try:",
            "",
            "                    def ping_timeout_handler(*_):",
            "                        if self.opts.get(\"auth_safemode\", False):",
            "                            log.error(",
            "                                \"** Master Ping failed. Attempting to restart minion**\"",
            "                            )",
            "                            delay = self.opts.get(\"random_reauth_delay\", 5)",
            "                            log.info(\"delaying random_reauth_delay %ss\", delay)",
            "                            try:",
            "                                self.functions[\"service.restart\"](service_name())",
            "                            except KeyError:",
            "                                # Probably no init system (running in docker?)",
            "                                log.warning(",
            "                                    \"ping_interval reached without response \"",
            "                                    \"from the master, but service.restart \"",
            "                                    \"could not be run to restart the minion \"",
            "                                    \"daemon. ping_interval requires that the \"",
            "                                    \"minion is running under an init system.\"",
            "                                )",
            "",
            "                    self._fire_master(",
            "                        \"ping\",",
            "                        \"minion_ping\",",
            "                        sync=False,",
            "                        timeout_handler=ping_timeout_handler,",
            "                    )",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.warning(",
            "                        \"Attempt to ping master failed.\", exc_on_loglevel=logging.DEBUG",
            "                    )",
            "",
            "            self.remove_periodic_callback(\"ping\")",
            "            self.add_periodic_callback(\"ping\", ping_master, ping_interval)",
            "",
            "        # add handler to subscriber",
            "        if hasattr(self, \"pub_channel\") and self.pub_channel is not None:",
            "            self.pub_channel.on_recv(self._handle_payload)",
            "        elif self.opts.get(\"master_type\") != \"disable\":",
            "            log.error(\"No connection to master found. Scheduled jobs will not run.\")",
            "",
            "        if start:",
            "            try:",
            "                self.io_loop.start()",
            "                if self.restart:",
            "                    self.destroy()",
            "            except (",
            "                KeyboardInterrupt,",
            "                RuntimeError,",
            "            ):  # A RuntimeError can be re-raised by Tornado on shutdown",
            "                self.destroy()",
            "",
            "    def _handle_payload(self, payload):",
            "        if payload is not None and payload[\"enc\"] == \"aes\":",
            "            if self._target_load(payload[\"load\"]):",
            "                self._handle_decoded_payload(payload[\"load\"])",
            "            elif self.opts[\"zmq_filtering\"]:",
            "                # In the filtering enabled case, we'd like to know when minion sees something it shouldnt",
            "                log.trace(",
            "                    \"Broadcast message received not for this minion, Load: %s\",",
            "                    payload[\"load\"],",
            "                )",
            "        # If it's not AES, and thus has not been verified, we do nothing.",
            "        # In the future, we could add support for some clearfuncs, but",
            "        # the minion currently has no need.",
            "",
            "    def _target_load(self, load):",
            "        # Verify that the publication is valid",
            "        if (",
            "            \"tgt\" not in load",
            "            or \"jid\" not in load",
            "            or \"fun\" not in load",
            "            or \"arg\" not in load",
            "        ):",
            "            return False",
            "        # Verify that the publication applies to this minion",
            "",
            "        # It's important to note that the master does some pre-processing",
            "        # to determine which minions to send a request to. So for example,",
            "        # a \"salt -G 'grain_key:grain_val' test.ping\" will invoke some",
            "        # pre-processing on the master and this minion should not see the",
            "        # publication if the master does not determine that it should.",
            "",
            "        if \"tgt_type\" in load:",
            "            match_func = self.matchers.get(",
            "                \"{}_match.match\".format(load[\"tgt_type\"]), None",
            "            )",
            "            if match_func is None:",
            "                return False",
            "            if load[\"tgt_type\"] in (\"grain\", \"grain_pcre\", \"pillar\"):",
            "                delimiter = load.get(\"delimiter\", DEFAULT_TARGET_DELIM)",
            "                if not match_func(load[\"tgt\"], delimiter=delimiter):",
            "                    return False",
            "            elif not match_func(load[\"tgt\"]):",
            "                return False",
            "        else:",
            "            if not self.matchers[\"glob_match.match\"](load[\"tgt\"]):",
            "                return False",
            "",
            "        return True",
            "",
            "    def destroy(self):",
            "        \"\"\"",
            "        Tear down the minion",
            "        \"\"\"",
            "        if self._running is False:",
            "            return",
            "",
            "        self._running = False",
            "        if hasattr(self, \"schedule\"):",
            "            del self.schedule",
            "        if hasattr(self, \"pub_channel\") and self.pub_channel is not None:",
            "            self.pub_channel.on_recv(None)",
            "            if hasattr(self.pub_channel, \"close\"):",
            "                self.pub_channel.close()",
            "            del self.pub_channel",
            "        if hasattr(self, \"periodic_callbacks\"):",
            "            for cb in self.periodic_callbacks.values():",
            "                cb.stop()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.destroy()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "class Syndic(Minion):",
            "    \"\"\"",
            "    Make a Syndic minion, this minion will use the minion keys on the",
            "    master to authenticate with a higher level master.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        self._syndic_interface = opts.get(\"interface\")",
            "        self._syndic = True",
            "        # force auth_safemode True because Syndic don't support autorestart",
            "        opts[\"auth_safemode\"] = True",
            "        opts[\"loop_interval\"] = 1",
            "        super().__init__(opts, **kwargs)",
            "        self.mminion = salt.minion.MasterMinion(opts)",
            "        self.jid_forward_cache = set()",
            "        self.jids = {}",
            "        self.raw_events = []",
            "        self.pub_future = None",
            "",
            "    def _handle_decoded_payload(self, data):",
            "        \"\"\"",
            "        Override this method if you wish to handle the decoded data",
            "        differently.",
            "        \"\"\"",
            "        # TODO: even do this??",
            "        data[\"to\"] = int(data.get(\"to\", self.opts[\"timeout\"])) - 1",
            "        # Only forward the command if it didn't originate from ourselves",
            "        if data.get(\"master_id\", 0) != self.opts.get(\"master_id\", 1):",
            "            self.syndic_cmd(data)",
            "",
            "    def syndic_cmd(self, data):",
            "        \"\"\"",
            "        Take the now clear load and forward it on to the client cmd",
            "        \"\"\"",
            "        # Set up default tgt_type",
            "        if \"tgt_type\" not in data:",
            "            data[\"tgt_type\"] = \"glob\"",
            "        kwargs = {}",
            "",
            "        # optionally add a few fields to the publish data",
            "        for field in (",
            "            \"master_id\",  # which master the job came from",
            "            \"user\",  # which user ran the job",
            "        ):",
            "            if field in data:",
            "                kwargs[field] = data[field]",
            "",
            "        def timeout_handler(*args):",
            "            log.warning(\"Unable to forward pub data: %s\", args[1])",
            "            return True",
            "",
            "        with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "            self.local.pub_async(",
            "                data[\"tgt\"],",
            "                data[\"fun\"],",
            "                data[\"arg\"],",
            "                data[\"tgt_type\"],",
            "                data[\"ret\"],",
            "                data[\"jid\"],",
            "                data[\"to\"],",
            "                io_loop=self.io_loop,",
            "                callback=lambda _: None,",
            "                **kwargs",
            "            )",
            "",
            "    def fire_master_syndic_start(self):",
            "        # Send an event to the master that the minion is live",
            "        if self.opts[\"enable_legacy_startup_events\"]:",
            "            # Old style event. Defaults to false in 3001 release.",
            "            self._fire_master(",
            "                \"Syndic {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "                \"syndic_start\",",
            "                sync=False,",
            "            )",
            "        self._fire_master(",
            "            \"Syndic {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "            tagify([self.opts[\"id\"], \"start\"], \"syndic\"),",
            "            sync=False,",
            "        )",
            "",
            "    # TODO: clean up docs",
            "    def tune_in_no_block(self):",
            "        \"\"\"",
            "        Executes the tune_in sequence but omits extra logging and the",
            "        management of the event bus assuming that these are handled outside",
            "        the tune_in sequence",
            "        \"\"\"",
            "        # Instantiate the local client",
            "        self.local = salt.client.get_local_client(",
            "            self.opts[\"_minion_conf_file\"], io_loop=self.io_loop",
            "        )",
            "",
            "        # add handler to subscriber",
            "        self.pub_channel.on_recv(self._process_cmd_socket)",
            "",
            "    def _process_cmd_socket(self, payload):",
            "        if payload is not None and payload[\"enc\"] == \"aes\":",
            "            log.trace(\"Handling payload\")",
            "            self._handle_decoded_payload(payload[\"load\"])",
            "        # If it's not AES, and thus has not been verified, we do nothing.",
            "        # In the future, we could add support for some clearfuncs, but",
            "        # the syndic currently has no need.",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def reconnect(self):",
            "        if hasattr(self, \"pub_channel\"):",
            "            self.pub_channel.on_recv(None)",
            "            if hasattr(self.pub_channel, \"close\"):",
            "                self.pub_channel.close()",
            "            del self.pub_channel",
            "",
            "        # if eval_master finds a new master for us, self.connected",
            "        # will be True again on successful master authentication",
            "        master, self.pub_channel = yield self.eval_master(opts=self.opts)",
            "",
            "        if self.connected:",
            "            self.opts[\"master\"] = master",
            "            self.pub_channel.on_recv(self._process_cmd_socket)",
            "            log.info(\"Minion is ready to receive requests!\")",
            "",
            "        raise salt.ext.tornado.gen.Return(self)",
            "",
            "    def destroy(self):",
            "        \"\"\"",
            "        Tear down the syndic minion",
            "        \"\"\"",
            "        # We borrowed the local clients poller so give it back before",
            "        # it's destroyed. Reset the local poller reference.",
            "        super().destroy()",
            "        if hasattr(self, \"local\"):",
            "            del self.local",
            "",
            "        if hasattr(self, \"forward_events\"):",
            "            self.forward_events.stop()",
            "",
            "",
            "# TODO: need a way of knowing if the syndic connection is busted",
            "class SyndicManager(MinionBase):",
            "    \"\"\"",
            "    Make a MultiMaster syndic minion, this minion will handle relaying jobs and returns from",
            "    all minions connected to it to the list of masters it is connected to.",
            "",
            "    Modes (controlled by `syndic_mode`:",
            "        sync: This mode will synchronize all events and publishes from higher level masters",
            "        cluster: This mode will only sync job publishes and returns",
            "",
            "    Note: jobs will be returned best-effort to the requesting master. This also means",
            "    (since we are using zmq) that if a job was fired and the master disconnects",
            "    between the publish and return, that the return will end up in a zmq buffer",
            "    in this Syndic headed to that original master.",
            "",
            "    In addition, since these classes all seem to use a mix of blocking and non-blocking",
            "    calls (with varying timeouts along the way) this daemon does not handle failure well,",
            "    it will (under most circumstances) stall the daemon for ~15s trying to forward events",
            "    to the down master",
            "    \"\"\"",
            "",
            "    # time to connect to upstream master",
            "    SYNDIC_CONNECT_TIMEOUT = 5",
            "    SYNDIC_EVENT_TIMEOUT = 5",
            "",
            "    def __init__(self, opts, io_loop=None):",
            "        opts[\"loop_interval\"] = 1",
            "        super().__init__(opts)",
            "        self.mminion = salt.minion.MasterMinion(opts)",
            "        # sync (old behavior), cluster (only returns and publishes)",
            "        self.syndic_mode = self.opts.get(\"syndic_mode\", \"sync\")",
            "        self.syndic_failover = self.opts.get(\"syndic_failover\", \"random\")",
            "",
            "        self.auth_wait = self.opts[\"acceptance_wait_time\"]",
            "        self.max_auth_wait = self.opts[\"acceptance_wait_time_max\"]",
            "",
            "        self._has_master = threading.Event()",
            "        self.jid_forward_cache = set()",
            "",
            "        if io_loop is None:",
            "            install_zmq()",
            "            self.io_loop = ZMQDefaultLoop.current()",
            "        else:",
            "            self.io_loop = io_loop",
            "",
            "        # List of events",
            "        self.raw_events = []",
            "        # Dict of rets: {master_id: {event_tag: job_ret, ...}, ...}",
            "        self.job_rets = {}",
            "        # List of delayed job_rets which was unable to send for some reason and will be resend to",
            "        # any available master",
            "        self.delayed = []",
            "        # Active pub futures: {master_id: (future, [job_ret, ...]), ...}",
            "        self.pub_futures = {}",
            "",
            "    def _spawn_syndics(self):",
            "        \"\"\"",
            "        Spawn all the coroutines which will sign in the syndics",
            "        \"\"\"",
            "        self._syndics = OrderedDict()  # mapping of opts['master'] -> syndic",
            "        masters = self.opts[\"master\"]",
            "        if not isinstance(masters, list):",
            "            masters = [masters]",
            "        for master in masters:",
            "            s_opts = copy.copy(self.opts)",
            "            s_opts[\"master\"] = master",
            "            self._syndics[master] = self._connect_syndic(s_opts)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _connect_syndic(self, opts):",
            "        \"\"\"",
            "        Create a syndic, and asynchronously connect it to a master",
            "        \"\"\"",
            "        last = 0  # never have we signed in",
            "        auth_wait = opts[\"acceptance_wait_time\"]",
            "        failed = False",
            "        while True:",
            "            log.debug(\"Syndic attempting to connect to %s\", opts[\"master\"])",
            "            try:",
            "                syndic = Syndic(",
            "                    opts,",
            "                    timeout=self.SYNDIC_CONNECT_TIMEOUT,",
            "                    safe=False,",
            "                    io_loop=self.io_loop,",
            "                )",
            "                yield syndic.connect_master(failed=failed)",
            "                # set up the syndic to handle publishes (specifically not event forwarding)",
            "                syndic.tune_in_no_block()",
            "",
            "                # Send an event to the master that the minion is live",
            "                syndic.fire_master_syndic_start()",
            "",
            "                log.info(\"Syndic successfully connected to %s\", opts[\"master\"])",
            "                break",
            "            except SaltClientError as exc:",
            "                failed = True",
            "                log.error(",
            "                    \"Error while bringing up syndic for multi-syndic. Is the \"",
            "                    \"master at %s responding?\",",
            "                    opts[\"master\"],",
            "                )",
            "                last = time.time()",
            "                if auth_wait < self.max_auth_wait:",
            "                    auth_wait += self.auth_wait",
            "                yield salt.ext.tornado.gen.sleep(auth_wait)  # TODO: log?",
            "            except (KeyboardInterrupt, SystemExit):  # pylint: disable=try-except-raise",
            "                raise",
            "            except Exception:  # pylint: disable=broad-except",
            "                failed = True",
            "                log.critical(",
            "                    \"Unexpected error while connecting to %s\",",
            "                    opts[\"master\"],",
            "                    exc_info=True,",
            "                )",
            "",
            "        raise salt.ext.tornado.gen.Return(syndic)",
            "",
            "    def _mark_master_dead(self, master):",
            "        \"\"\"",
            "        Mark a master as dead. This will start the sign-in routine",
            "        \"\"\"",
            "        # if its connected, mark it dead",
            "        if self._syndics[master].done():",
            "            syndic = self._syndics[master].result()  # pylint: disable=no-member",
            "            self._syndics[master] = syndic.reconnect()",
            "        else:",
            "            # TODO: debug?",
            "            log.info(",
            "                \"Attempting to mark %s as dead, although it is already \" \"marked dead\",",
            "                master,",
            "            )",
            "",
            "    def _call_syndic(self, func, args=(), kwargs=None, master_id=None):",
            "        \"\"\"",
            "        Wrapper to call a given func on a syndic, best effort to get the one you asked for",
            "        \"\"\"",
            "        if kwargs is None:",
            "            kwargs = {}",
            "        successful = False",
            "        # Call for each master",
            "        for master, syndic_future in self.iter_master_options(master_id):",
            "            if not syndic_future.done() or syndic_future.exception():",
            "                log.error(",
            "                    \"Unable to call %s on %s, that syndic is not connected\",",
            "                    func,",
            "                    master,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                getattr(syndic_future.result(), func)(*args, **kwargs)",
            "                successful = True",
            "            except SaltClientError:",
            "                log.error(\"Unable to call %s on %s, trying another...\", func, master)",
            "                self._mark_master_dead(master)",
            "        if not successful:",
            "            log.critical(\"Unable to call %s on any masters!\", func)",
            "",
            "    def _return_pub_syndic(self, values, master_id=None):",
            "        \"\"\"",
            "        Wrapper to call the '_return_pub_multi' a syndic, best effort to get the one you asked for",
            "        \"\"\"",
            "        func = \"_return_pub_multi\"",
            "        for master, syndic_future in self.iter_master_options(master_id):",
            "            if not syndic_future.done() or syndic_future.exception():",
            "                log.error(",
            "                    \"Unable to call %s on %s, that syndic is not connected\",",
            "                    func,",
            "                    master,",
            "                )",
            "                continue",
            "",
            "            future, data = self.pub_futures.get(master, (None, None))",
            "            if future is not None:",
            "                if not future.done():",
            "                    if master == master_id:",
            "                        # Targeted master previous send not done yet, call again later",
            "                        return False",
            "                    else:",
            "                        # Fallback master is busy, try the next one",
            "                        continue",
            "                elif future.exception():",
            "                    # Previous execution on this master returned an error",
            "                    log.error(",
            "                        \"Unable to call %s on %s, trying another...\", func, master",
            "                    )",
            "                    self._mark_master_dead(master)",
            "                    del self.pub_futures[master]",
            "                    # Add not sent data to the delayed list and try the next master",
            "                    self.delayed.extend(data)",
            "                    continue",
            "            future = getattr(syndic_future.result(), func)(",
            "                values, \"_syndic_return\", timeout=self._return_retry_timer(), sync=False",
            "            )",
            "            self.pub_futures[master] = (future, values)",
            "            return True",
            "        # Loop done and didn't exit: wasn't sent, try again later",
            "        return False",
            "",
            "    def iter_master_options(self, master_id=None):",
            "        \"\"\"",
            "        Iterate (in order) over your options for master",
            "        \"\"\"",
            "        masters = list(self._syndics.keys())",
            "        if self.opts[\"syndic_failover\"] == \"random\":",
            "            shuffle(masters)",
            "        if master_id not in self._syndics:",
            "            master_id = masters.pop(0)",
            "        else:",
            "            masters.remove(master_id)",
            "",
            "        while True:",
            "            yield master_id, self._syndics[master_id]",
            "            if not masters:",
            "                break",
            "            master_id = masters.pop(0)",
            "",
            "    def _reset_event_aggregation(self):",
            "        self.job_rets = {}",
            "        self.raw_events = []",
            "",
            "    def reconnect_event_bus(self, something):",
            "        future = self.local.event.set_event_handler(self._process_event)",
            "        self.io_loop.add_future(future, self.reconnect_event_bus)",
            "",
            "    # Syndic Tune In",
            "    def tune_in(self):",
            "        \"\"\"",
            "        Lock onto the publisher. This is the main event loop for the syndic",
            "        \"\"\"",
            "        self._spawn_syndics()",
            "        # Instantiate the local client",
            "        self.local = salt.client.get_local_client(",
            "            self.opts[\"_minion_conf_file\"], io_loop=self.io_loop",
            "        )",
            "        self.local.event.subscribe(\"\")",
            "",
            "        log.debug(\"SyndicManager '%s' trying to tune in\", self.opts[\"id\"])",
            "",
            "        # register the event sub to the poller",
            "        self.job_rets = {}",
            "        self.raw_events = []",
            "        self._reset_event_aggregation()",
            "        future = self.local.event.set_event_handler(self._process_event)",
            "        self.io_loop.add_future(future, self.reconnect_event_bus)",
            "",
            "        # forward events every syndic_event_forward_timeout",
            "        self.forward_events = salt.ext.tornado.ioloop.PeriodicCallback(",
            "            self._forward_events, self.opts[\"syndic_event_forward_timeout\"] * 1000,",
            "        )",
            "        self.forward_events.start()",
            "",
            "        # Make sure to gracefully handle SIGUSR1",
            "        enable_sigusr1_handler()",
            "",
            "        self.io_loop.start()",
            "",
            "    def _process_event(self, raw):",
            "        # TODO: cleanup: Move down into event class",
            "        mtag, data = self.local.event.unpack(raw, self.local.event.serial)",
            "        log.trace(\"Got event %s\", mtag)  # pylint: disable=no-member",
            "",
            "        tag_parts = mtag.split(\"/\")",
            "        if (",
            "            len(tag_parts) >= 4",
            "            and tag_parts[1] == \"job\"",
            "            and salt.utils.jid.is_jid(tag_parts[2])",
            "            and tag_parts[3] == \"ret\"",
            "            and \"return\" in data",
            "        ):",
            "            if \"jid\" not in data:",
            "                # Not a job return",
            "                return",
            "            if self.syndic_mode == \"cluster\" and data.get(",
            "                \"master_id\", 0",
            "            ) == self.opts.get(\"master_id\", 1):",
            "                log.debug(\"Return received with matching master_id, not forwarding\")",
            "                return",
            "",
            "            master = data.get(\"master_id\")",
            "            jdict = self.job_rets.setdefault(master, {}).setdefault(mtag, {})",
            "            if not jdict:",
            "                jdict[\"__fun__\"] = data.get(\"fun\")",
            "                jdict[\"__jid__\"] = data[\"jid\"]",
            "                jdict[\"__load__\"] = {}",
            "                fstr = \"{}.get_load\".format(self.opts[\"master_job_cache\"])",
            "                # Only need to forward each load once. Don't hit the disk",
            "                # for every minion return!",
            "                if data[\"jid\"] not in self.jid_forward_cache:",
            "                    jdict[\"__load__\"].update(self.mminion.returners[fstr](data[\"jid\"]))",
            "                    self.jid_forward_cache.add(data[\"jid\"])",
            "                    if (",
            "                        len(self.jid_forward_cache)",
            "                        > self.opts[\"syndic_jid_forward_cache_hwm\"]",
            "                    ):",
            "                        # Pop the oldest jid from the cache",
            "                        tmp = sorted(list(self.jid_forward_cache))",
            "                        tmp.pop(0)",
            "                        self.jid_forward_cache = set(tmp)",
            "            if master is not None:",
            "                # __'s to make sure it doesn't print out on the master cli",
            "                jdict[\"__master_id__\"] = master",
            "            ret = {}",
            "            for key in \"return\", \"retcode\", \"success\":",
            "                if key in data:",
            "                    ret[key] = data[key]",
            "            jdict[data[\"id\"]] = ret",
            "        else:",
            "            # TODO: config to forward these? If so we'll have to keep track of who",
            "            # has seen them",
            "            # if we are the top level masters-- don't forward all the minion events",
            "            if self.syndic_mode == \"sync\":",
            "                # Add generic event aggregation here",
            "                if \"retcode\" not in data:",
            "                    self.raw_events.append({\"data\": data, \"tag\": mtag})",
            "",
            "    def _forward_events(self):",
            "        log.trace(\"Forwarding events\")  # pylint: disable=no-member",
            "        if self.raw_events:",
            "            events = self.raw_events",
            "            self.raw_events = []",
            "            self._call_syndic(",
            "                \"_fire_master\",",
            "                kwargs={",
            "                    \"events\": events,",
            "                    \"pretag\": tagify(self.opts[\"id\"], base=\"syndic\"),",
            "                    \"timeout\": self._return_retry_timer(),",
            "                    \"sync\": False,",
            "                },",
            "            )",
            "        if self.delayed:",
            "            res = self._return_pub_syndic(self.delayed)",
            "            if res:",
            "                self.delayed = []",
            "        for master in list(self.job_rets.keys()):",
            "            values = list(self.job_rets[master].values())",
            "            res = self._return_pub_syndic(values, master_id=master)",
            "            if res:",
            "                del self.job_rets[master]",
            "",
            "",
            "class ProxyMinionManager(MinionManager):",
            "    \"\"\"",
            "    Create the multi-minion interface but for proxy minions",
            "    \"\"\"",
            "",
            "    def _create_minion_object(",
            "        self, opts, timeout, safe, io_loop=None, loaded_base_name=None, jid_queue=None",
            "    ):",
            "        \"\"\"",
            "        Helper function to return the correct type of object",
            "        \"\"\"",
            "        return ProxyMinion(",
            "            opts,",
            "            timeout,",
            "            safe,",
            "            io_loop=io_loop,",
            "            loaded_base_name=loaded_base_name,",
            "            jid_queue=jid_queue,",
            "        )",
            "",
            "",
            "def _metaproxy_call(opts, fn_name):",
            "    loaded_base_name = \"{}.{}\".format(opts[\"id\"], salt.loader.LOADED_BASE_NAME)",
            "    metaproxy = salt.loader.metaproxy(opts, loaded_base_name=loaded_base_name)",
            "    try:",
            "        metaproxy_name = opts[\"metaproxy\"]",
            "    except KeyError:",
            "        metaproxy_name = \"proxy\"",
            "        errmsg = (",
            "            \"No metaproxy key found in opts for id \"",
            "            + opts[\"id\"]",
            "            + \". \"",
            "            + \"Defaulting to standard proxy minion\"",
            "        )",
            "        log.error(errmsg)",
            "",
            "    metaproxy_fn = metaproxy_name + \".\" + fn_name",
            "    return metaproxy[metaproxy_fn]",
            "",
            "",
            "class ProxyMinion(Minion):",
            "    \"\"\"",
            "    This class instantiates a 'proxy' minion--a minion that does not manipulate",
            "    the host it runs on, but instead manipulates a device that cannot run a minion.",
            "    \"\"\"",
            "",
            "    # TODO: better name...",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _post_master_init(self, master):",
            "        \"\"\"",
            "        Function to finish init after connecting to a master",
            "",
            "        This is primarily loading modules, pillars, etc. (since they need",
            "        to know which master they connected to)",
            "",
            "        If this function is changed, please check Minion._post_master_init",
            "        to see if those changes need to be propagated.",
            "",
            "        ProxyMinions need a significantly different post master setup,",
            "        which is why the differences are not factored out into separate helper",
            "        functions.",
            "        \"\"\"",
            "        mp_call = _metaproxy_call(self.opts, \"post_master_init\")",
            "        return mp_call(self, master)",
            "",
            "    def tune_in(self, start=True):",
            "        \"\"\"",
            "        Lock onto the publisher. This is the main event loop for the minion",
            "        :rtype : None",
            "        \"\"\"",
            "        mp_call = _metaproxy_call(self.opts, \"tune_in\")",
            "        return mp_call(self, start)",
            "",
            "    def _target_load(self, load):",
            "        \"\"\"",
            "        Verify that the publication is valid and applies to this minion",
            "        \"\"\"",
            "        mp_call = _metaproxy_call(self.opts, \"target_load\")",
            "        return mp_call(self, load)",
            "",
            "    def _handle_payload(self, payload):",
            "        mp_call = _metaproxy_call(self.opts, \"handle_payload\")",
            "        return mp_call(self, payload)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _handle_decoded_payload(self, data):",
            "        mp_call = _metaproxy_call(self.opts, \"handle_decoded_payload\")",
            "        return mp_call(self, data)",
            "",
            "    @classmethod",
            "    def _target(cls, minion_instance, opts, data, connected):",
            "",
            "        mp_call = _metaproxy_call(opts, \"target\")",
            "        return mp_call(cls, minion_instance, opts, data, connected)",
            "",
            "    @classmethod",
            "    def _thread_return(cls, minion_instance, opts, data):",
            "        mp_call = _metaproxy_call(opts, \"thread_return\")",
            "        return mp_call(cls, minion_instance, opts, data)",
            "",
            "    @classmethod",
            "    def _thread_multi_return(cls, minion_instance, opts, data):",
            "        mp_call = _metaproxy_call(opts, \"thread_multi_return\")",
            "        return mp_call(cls, minion_instance, opts, data)",
            "",
            "",
            "class SProxyMinion(SMinion):",
            "    \"\"\"",
            "    Create an object that has loaded all of the minion module functions,",
            "    grains, modules, returners etc.  The SProxyMinion allows developers to",
            "    generate all of the salt minion functions and present them with these",
            "    functions for general use.",
            "    \"\"\"",
            "",
            "    def gen_modules(self, initial_load=False, context=None):",
            "        \"\"\"",
            "        Tell the minion to reload the execution modules",
            "",
            "        CLI Example:",
            "",
            "        .. code-block:: bash",
            "",
            "            salt '*' sys.reload_modules",
            "        \"\"\"",
            "        self.opts[\"grains\"] = salt.loader.grains(self.opts)",
            "        self.opts[\"pillar\"] = salt.pillar.get_pillar(",
            "            self.opts,",
            "            self.opts[\"grains\"],",
            "            self.opts[\"id\"],",
            "            saltenv=self.opts[\"saltenv\"],",
            "            pillarenv=self.opts.get(\"pillarenv\"),",
            "        ).compile_pillar()",
            "",
            "        if \"proxy\" not in self.opts[\"pillar\"] and \"proxy\" not in self.opts:",
            "            errmsg = (",
            "                'No \"proxy\" configuration key found in pillar or opts '",
            "                \"dictionaries for id {id}. Check your pillar/options \"",
            "                \"configuration and contents. Salt-proxy aborted.\"",
            "            ).format(id=self.opts[\"id\"])",
            "            log.error(errmsg)",
            "            self._running = False",
            "            raise SaltSystemExit(code=salt.defaults.exitcodes.EX_GENERIC, msg=errmsg)",
            "",
            "        if \"proxy\" not in self.opts:",
            "            self.opts[\"proxy\"] = self.opts[\"pillar\"][\"proxy\"]",
            "",
            "        # Then load the proxy module",
            "        self.proxy = salt.loader.proxy(self.opts)",
            "",
            "        self.utils = salt.loader.utils(self.opts, proxy=self.proxy, context=context)",
            "",
            "        self.functions = salt.loader.minion_mods(",
            "            self.opts, utils=self.utils, notify=False, proxy=self.proxy, context=context",
            "        )",
            "        self.returners = salt.loader.returners(",
            "            self.opts, functions=self.functions, proxy=self.proxy, context=context",
            "        )",
            "        self.matchers = salt.loader.matchers(self.opts)",
            "        self.functions[\"sys.reload_modules\"] = self.gen_modules",
            "        self.executors = salt.loader.executors(",
            "            self.opts, functions=self.functions, proxy=self.proxy, context=context,",
            "        )",
            "",
            "        fq_proxyname = self.opts[\"proxy\"][\"proxytype\"]",
            "",
            "        # we can then sync any proxymodules down from the master",
            "        # we do a sync_all here in case proxy code was installed by",
            "        # SPM or was manually placed in /srv/salt/_modules etc.",
            "        self.functions[\"saltutil.sync_all\"](saltenv=self.opts[\"saltenv\"])",
            "",
            "        self.functions.pack[\"__proxy__\"] = self.proxy",
            "        self.proxy.pack[\"__salt__\"] = self.functions",
            "        self.proxy.pack[\"__ret__\"] = self.returners",
            "        self.proxy.pack[\"__pillar__\"] = self.opts[\"pillar\"]",
            "",
            "        # Reload utils as well (chicken and egg, __utils__ needs __proxy__ and __proxy__ needs __utils__",
            "        self.utils = salt.loader.utils(self.opts, proxy=self.proxy, context=context)",
            "        self.proxy.pack[\"__utils__\"] = self.utils",
            "",
            "        # Reload all modules so all dunder variables are injected",
            "        self.proxy.reload_modules()",
            "",
            "        if (",
            "            \"{}.init\".format(fq_proxyname) not in self.proxy",
            "            or \"{}.shutdown\".format(fq_proxyname) not in self.proxy",
            "        ):",
            "            errmsg = (",
            "                \"Proxymodule {} is missing an init() or a shutdown() or both. \".format(",
            "                    fq_proxyname",
            "                )",
            "                + \"Check your proxymodule.  Salt-proxy aborted.\"",
            "            )",
            "            log.error(errmsg)",
            "            self._running = False",
            "            raise SaltSystemExit(code=salt.defaults.exitcodes.EX_GENERIC, msg=errmsg)",
            "",
            "        self.module_executors = self.proxy.get(",
            "            \"{}.module_executors\".format(fq_proxyname), lambda: []",
            "        )()",
            "        proxy_init_fn = self.proxy[fq_proxyname + \".init\"]",
            "        proxy_init_fn(self.opts)",
            "",
            "        self.opts[\"grains\"] = salt.loader.grains(self.opts, proxy=self.proxy)",
            "",
            "        #  Sync the grains here so the proxy can communicate them to the master",
            "        self.functions[\"saltutil.sync_grains\"](saltenv=\"base\")",
            "        self.grains_cache = self.opts[\"grains\"]",
            "        self.ready = True"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Routines to set up a minion",
            "\"\"\"",
            "",
            "import contextlib",
            "import copy",
            "import functools",
            "import logging",
            "import multiprocessing",
            "import os",
            "import random",
            "import signal",
            "import sys",
            "import threading",
            "import time",
            "import traceback",
            "import types",
            "from binascii import crc32",
            "from random import randint, shuffle",
            "from stat import S_IMODE",
            "",
            "import salt",
            "import salt.beacons",
            "import salt.cli.daemons",
            "import salt.client",
            "import salt.crypt",
            "import salt.defaults.events",
            "import salt.defaults.exitcodes",
            "import salt.engines",
            "",
            "# pylint: enable=no-name-in-module,redefined-builtin",
            "import salt.ext.tornado",
            "import salt.ext.tornado.gen  # pylint: disable=F0401",
            "import salt.ext.tornado.ioloop  # pylint: disable=F0401",
            "import salt.loader",
            "import salt.log.setup",
            "import salt.payload",
            "import salt.pillar",
            "import salt.serializers.msgpack",
            "import salt.syspaths",
            "import salt.transport.client",
            "import salt.utils.args",
            "import salt.utils.context",
            "import salt.utils.crypt",
            "import salt.utils.data",
            "import salt.utils.dictupdate",
            "import salt.utils.error",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.jid",
            "import salt.utils.minion",
            "import salt.utils.minions",
            "import salt.utils.network",
            "import salt.utils.platform",
            "import salt.utils.process",
            "import salt.utils.schedule",
            "import salt.utils.ssdp",
            "import salt.utils.user",
            "import salt.utils.zeromq",
            "from salt._compat import ipaddress",
            "from salt.config import DEFAULT_MINION_OPTS",
            "from salt.defaults import DEFAULT_TARGET_DELIM",
            "from salt.exceptions import (",
            "    CommandExecutionError,",
            "    CommandNotFoundError,",
            "    SaltClientError,",
            "    SaltDaemonNotRunning,",
            "    SaltException,",
            "    SaltInvocationError,",
            "    SaltMasterUnresolvableError,",
            "    SaltReqTimeoutError,",
            "    SaltSystemExit,",
            ")",
            "",
            "# pylint: disable=import-error,no-name-in-module,redefined-builtin",
            "from salt.ext import six",
            "from salt.ext.six.moves import range",
            "from salt.template import SLS_ENCODING",
            "from salt.utils.ctx import RequestContext",
            "from salt.utils.debug import enable_sigusr1_handler",
            "from salt.utils.event import tagify",
            "from salt.utils.network import parse_host_port",
            "from salt.utils.odict import OrderedDict",
            "from salt.utils.process import ProcessManager, SignalHandlingProcess, default_signals",
            "from salt.utils.zeromq import ZMQ_VERSION_INFO, ZMQDefaultLoop, install_zmq, zmq",
            "",
            "HAS_PSUTIL = False",
            "try:",
            "    import salt.utils.psutil_compat as psutil",
            "",
            "    HAS_PSUTIL = True",
            "except ImportError:",
            "    pass",
            "",
            "HAS_RESOURCE = False",
            "try:",
            "    import resource",
            "",
            "    HAS_RESOURCE = True",
            "except ImportError:",
            "    pass",
            "",
            "try:",
            "    import salt.utils.win_functions",
            "",
            "    HAS_WIN_FUNCTIONS = True",
            "except ImportError:",
            "    HAS_WIN_FUNCTIONS = False",
            "# pylint: enable=import-error",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "# To set up a minion:",
            "# 1. Read in the configuration",
            "# 2. Generate the function mapping dict",
            "# 3. Authenticate with the master",
            "# 4. Store the AES key",
            "# 5. Connect to the publisher",
            "# 6. Handle publications",
            "",
            "",
            "def resolve_dns(opts, fallback=True):",
            "    \"\"\"",
            "    Resolves the master_ip and master_uri options",
            "    \"\"\"",
            "    ret = {}",
            "    check_dns = True",
            "    if opts.get(\"file_client\", \"remote\") == \"local\" and not opts.get(",
            "        \"use_master_when_local\", False",
            "    ):",
            "        check_dns = False",
            "    # Since salt.log is imported below, salt.utils.network needs to be imported here as well",
            "    import salt.utils.network",
            "",
            "    if check_dns is True:",
            "        try:",
            "            if opts[\"master\"] == \"\":",
            "                raise SaltSystemExit",
            "            ret[\"master_ip\"] = salt.utils.network.dns_check(",
            "                opts[\"master\"], int(opts[\"master_port\"]), True, opts[\"ipv6\"]",
            "            )",
            "        except SaltClientError:",
            "            retry_dns_count = opts.get(\"retry_dns_count\", None)",
            "            if opts[\"retry_dns\"]:",
            "                while True:",
            "                    if retry_dns_count is not None:",
            "                        if retry_dns_count == 0:",
            "                            raise SaltMasterUnresolvableError",
            "                        retry_dns_count -= 1",
            "                    import salt.log",
            "",
            "                    msg = (",
            "                        \"Master hostname: '{}' not found or not responsive. \"",
            "                        \"Retrying in {} seconds\"",
            "                    ).format(opts[\"master\"], opts[\"retry_dns\"])",
            "                    if salt.log.setup.is_console_configured():",
            "                        log.error(msg)",
            "                    else:",
            "                        print(\"WARNING: {}\".format(msg))",
            "                    time.sleep(opts[\"retry_dns\"])",
            "                    try:",
            "                        ret[\"master_ip\"] = salt.utils.network.dns_check(",
            "                            opts[\"master\"], int(opts[\"master_port\"]), True, opts[\"ipv6\"]",
            "                        )",
            "                        break",
            "                    except SaltClientError:",
            "                        pass",
            "            else:",
            "                if fallback:",
            "                    ret[\"master_ip\"] = \"127.0.0.1\"",
            "                else:",
            "                    raise",
            "        except SaltSystemExit:",
            "            unknown_str = \"unknown address\"",
            "            master = opts.get(\"master\", unknown_str)",
            "            if master == \"\":",
            "                master = unknown_str",
            "            if opts.get(\"__role\") == \"syndic\":",
            "                err = (",
            "                    \"Master address: '{}' could not be resolved. Invalid or unresolveable address. \"",
            "                    \"Set 'syndic_master' value in minion config.\".format(master)",
            "                )",
            "            else:",
            "                err = (",
            "                    \"Master address: '{}' could not be resolved. Invalid or unresolveable address. \"",
            "                    \"Set 'master' value in minion config.\".format(master)",
            "                )",
            "            log.error(err)",
            "            raise SaltSystemExit(code=42, msg=err)",
            "    else:",
            "        ret[\"master_ip\"] = \"127.0.0.1\"",
            "",
            "    if \"master_ip\" in ret and \"master_ip\" in opts:",
            "        if ret[\"master_ip\"] != opts[\"master_ip\"]:",
            "            log.warning(",
            "                \"Master ip address changed from %s to %s\",",
            "                opts[\"master_ip\"],",
            "                ret[\"master_ip\"],",
            "            )",
            "    if opts[\"source_interface_name\"]:",
            "        log.trace(\"Custom source interface required: %s\", opts[\"source_interface_name\"])",
            "        interfaces = salt.utils.network.interfaces()",
            "        log.trace(\"The following interfaces are available on this Minion:\")",
            "        log.trace(interfaces)",
            "        if opts[\"source_interface_name\"] in interfaces:",
            "            if interfaces[opts[\"source_interface_name\"]][\"up\"]:",
            "                addrs = (",
            "                    interfaces[opts[\"source_interface_name\"]][\"inet\"]",
            "                    if not opts[\"ipv6\"]",
            "                    else interfaces[opts[\"source_interface_name\"]][\"inet6\"]",
            "                )",
            "                ret[\"source_ip\"] = addrs[0][\"address\"]",
            "                log.debug(\"Using %s as source IP address\", ret[\"source_ip\"])",
            "            else:",
            "                log.warning(",
            "                    \"The interface %s is down so it cannot be used as source to connect to the Master\",",
            "                    opts[\"source_interface_name\"],",
            "                )",
            "        else:",
            "            log.warning(",
            "                \"%s is not a valid interface. Ignoring.\", opts[\"source_interface_name\"]",
            "            )",
            "    elif opts[\"source_address\"]:",
            "        ret[\"source_ip\"] = salt.utils.network.dns_check(",
            "            opts[\"source_address\"], int(opts[\"source_ret_port\"]), True, opts[\"ipv6\"]",
            "        )",
            "        log.debug(\"Using %s as source IP address\", ret[\"source_ip\"])",
            "    if opts[\"source_ret_port\"]:",
            "        ret[\"source_ret_port\"] = int(opts[\"source_ret_port\"])",
            "        log.debug(\"Using %d as source port for the ret server\", ret[\"source_ret_port\"])",
            "    if opts[\"source_publish_port\"]:",
            "        ret[\"source_publish_port\"] = int(opts[\"source_publish_port\"])",
            "        log.debug(",
            "            \"Using %d as source port for the master pub\", ret[\"source_publish_port\"]",
            "        )",
            "    ret[\"master_uri\"] = \"tcp://{ip}:{port}\".format(",
            "        ip=ret[\"master_ip\"], port=opts[\"master_port\"]",
            "    )",
            "    log.debug(\"Master URI: %s\", ret[\"master_uri\"])",
            "",
            "    return ret",
            "",
            "",
            "def prep_ip_port(opts):",
            "    \"\"\"",
            "    parse host:port values from opts['master'] and return valid:",
            "        master: ip address or hostname as a string",
            "        master_port: (optional) master returner port as integer",
            "",
            "    e.g.:",
            "      - master: 'localhost:1234' -> {'master': 'localhost', 'master_port': 1234}",
            "      - master: '127.0.0.1:1234' -> {'master': '127.0.0.1', 'master_port' :1234}",
            "      - master: '[::1]:1234' -> {'master': '::1', 'master_port': 1234}",
            "      - master: 'fe80::a00:27ff:fedc:ba98' -> {'master': 'fe80::a00:27ff:fedc:ba98'}",
            "    \"\"\"",
            "    ret = {}",
            "    # Use given master IP if \"ip_only\" is set or if master_ip is an ipv6 address without",
            "    # a port specified. The is_ipv6 check returns False if brackets are used in the IP",
            "    # definition such as master: '[::1]:1234'.",
            "    if opts[\"master_uri_format\"] == \"ip_only\":",
            "        ret[\"master\"] = ipaddress.ip_address(opts[\"master\"])",
            "    else:",
            "        try:",
            "            host, port = parse_host_port(opts[\"master\"])",
            "        except ValueError as exc:",
            "            raise SaltClientError(exc)",
            "        ret = {\"master\": host}",
            "        if port:",
            "            ret.update({\"master_port\": port})",
            "",
            "    return ret",
            "",
            "",
            "def get_proc_dir(cachedir, **kwargs):",
            "    \"\"\"",
            "    Given the cache directory, return the directory that process data is",
            "    stored in, creating it if it doesn't exist.",
            "    The following optional Keyword Arguments are handled:",
            "",
            "    mode: which is anything os.makedir would accept as mode.",
            "",
            "    uid: the uid to set, if not set, or it is None or -1 no changes are",
            "         made. Same applies if the directory is already owned by this",
            "         uid. Must be int. Works only on unix/unix like systems.",
            "",
            "    gid: the gid to set, if not set, or it is None or -1 no changes are",
            "         made. Same applies if the directory is already owned by this",
            "         gid. Must be int. Works only on unix/unix like systems.",
            "    \"\"\"",
            "    fn_ = os.path.join(cachedir, \"proc\")",
            "    mode = kwargs.pop(\"mode\", None)",
            "",
            "    if mode is None:",
            "        mode = {}",
            "    else:",
            "        mode = {\"mode\": mode}",
            "",
            "    if not os.path.isdir(fn_):",
            "        # proc_dir is not present, create it with mode settings",
            "        os.makedirs(fn_, **mode)",
            "",
            "    d_stat = os.stat(fn_)",
            "",
            "    # if mode is not an empty dict then we have an explicit",
            "    # dir mode. So lets check if mode needs to be changed.",
            "    if mode:",
            "        mode_part = S_IMODE(d_stat.st_mode)",
            "        if mode_part != mode[\"mode\"]:",
            "            os.chmod(fn_, (d_stat.st_mode ^ mode_part) | mode[\"mode\"])",
            "",
            "    if hasattr(os, \"chown\"):",
            "        # only on unix/unix like systems",
            "        uid = kwargs.pop(\"uid\", -1)",
            "        gid = kwargs.pop(\"gid\", -1)",
            "",
            "        # if uid and gid are both -1 then go ahead with",
            "        # no changes at all",
            "        if (d_stat.st_uid != uid or d_stat.st_gid != gid) and [",
            "            i for i in (uid, gid) if i != -1",
            "        ]:",
            "            os.chown(fn_, uid, gid)",
            "",
            "    return fn_",
            "",
            "",
            "def load_args_and_kwargs(func, args, data=None, ignore_invalid=False):",
            "    \"\"\"",
            "    Detect the args and kwargs that need to be passed to a function call, and",
            "    check them against what was passed.",
            "    \"\"\"",
            "    argspec = salt.utils.args.get_function_argspec(func)",
            "    _args = []",
            "    _kwargs = {}",
            "    invalid_kwargs = []",
            "",
            "    for arg in args:",
            "        if isinstance(arg, dict) and arg.pop(\"__kwarg__\", False) is True:",
            "            # if the arg is a dict with __kwarg__ == True, then its a kwarg",
            "            for key, val in arg.items():",
            "                if argspec.keywords or key in argspec.args:",
            "                    # Function supports **kwargs or is a positional argument to",
            "                    # the function.",
            "                    _kwargs[key] = val",
            "                else:",
            "                    # **kwargs not in argspec and parsed argument name not in",
            "                    # list of positional arguments. This keyword argument is",
            "                    # invalid.",
            "                    invalid_kwargs.append(\"{}={}\".format(key, val))",
            "            continue",
            "",
            "        else:",
            "            string_kwarg = salt.utils.args.parse_input([arg], condition=False)[",
            "                1",
            "            ]  # pylint: disable=W0632",
            "            if string_kwarg:",
            "                if argspec.keywords or next(iter(string_kwarg.keys())) in argspec.args:",
            "                    # Function supports **kwargs or is a positional argument to",
            "                    # the function.",
            "                    _kwargs.update(string_kwarg)",
            "                else:",
            "                    # **kwargs not in argspec and parsed argument name not in",
            "                    # list of positional arguments. This keyword argument is",
            "                    # invalid.",
            "                    for key, val in string_kwarg.items():",
            "                        invalid_kwargs.append(\"{}={}\".format(key, val))",
            "            else:",
            "                _args.append(arg)",
            "",
            "    if invalid_kwargs and not ignore_invalid:",
            "        salt.utils.args.invalid_kwargs(invalid_kwargs)",
            "",
            "    if argspec.keywords and isinstance(data, dict):",
            "        # this function accepts **kwargs, pack in the publish data",
            "        for key, val in data.items():",
            "            _kwargs[\"__pub_{}\".format(key)] = val",
            "",
            "    return _args, _kwargs",
            "",
            "",
            "def eval_master_func(opts):",
            "    \"\"\"",
            "    Evaluate master function if master type is 'func'",
            "    and save it result in opts['master']",
            "    \"\"\"",
            "    if \"__master_func_evaluated\" not in opts:",
            "        # split module and function and try loading the module",
            "        mod_fun = opts[\"master\"]",
            "        mod, fun = mod_fun.split(\".\")",
            "        try:",
            "            master_mod = salt.loader.raw_mod(opts, mod, fun)",
            "            if not master_mod:",
            "                raise KeyError",
            "            # we take whatever the module returns as master address",
            "            opts[\"master\"] = master_mod[mod_fun]()",
            "            # Check for valid types",
            "            if not isinstance(opts[\"master\"], ((str,), list)):",
            "                raise TypeError",
            "            opts[\"__master_func_evaluated\"] = True",
            "        except KeyError:",
            "            log.error(\"Failed to load module %s\", mod_fun)",
            "            sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "        except TypeError:",
            "            log.error(\"%s returned from %s is not a string\", opts[\"master\"], mod_fun)",
            "            sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "        log.info(\"Evaluated master from module: %s\", mod_fun)",
            "",
            "",
            "def master_event(type, master=None):",
            "    \"\"\"",
            "    Centralized master event function which will return event type based on event_map",
            "    \"\"\"",
            "    event_map = {",
            "        \"connected\": \"__master_connected\",",
            "        \"disconnected\": \"__master_disconnected\",",
            "        \"failback\": \"__master_failback\",",
            "        \"alive\": \"__master_alive\",",
            "    }",
            "",
            "    if type == \"alive\" and master is not None:",
            "        return \"{}_{}\".format(event_map.get(type), master)",
            "",
            "    return event_map.get(type, None)",
            "",
            "",
            "def service_name():",
            "    \"\"\"",
            "    Return the proper service name based on platform",
            "    \"\"\"",
            "    return \"salt_minion\" if \"bsd\" in sys.platform else \"salt-minion\"",
            "",
            "",
            "class MinionBase:",
            "    def __init__(self, opts):",
            "        self.opts = opts",
            "        self.beacons_leader = opts.get(\"beacons_leader\", True)",
            "",
            "    def gen_modules(self, initial_load=False, context=None):",
            "        \"\"\"",
            "        Tell the minion to reload the execution modules",
            "",
            "        CLI Example:",
            "",
            "        .. code-block:: bash",
            "",
            "            salt '*' sys.reload_modules",
            "        \"\"\"",
            "        if initial_load:",
            "            self.opts[\"pillar\"] = salt.pillar.get_pillar(",
            "                self.opts,",
            "                self.opts[\"grains\"],",
            "                self.opts[\"id\"],",
            "                self.opts[\"saltenv\"],",
            "                pillarenv=self.opts.get(\"pillarenv\"),",
            "            ).compile_pillar()",
            "",
            "        self.utils = salt.loader.utils(self.opts, context=context)",
            "        self.functions = salt.loader.minion_mods(",
            "            self.opts, utils=self.utils, context=context",
            "        )",
            "        self.serializers = salt.loader.serializers(self.opts)",
            "        self.returners = salt.loader.returners(",
            "            self.opts, functions=self.functions, context=context",
            "        )",
            "        self.proxy = salt.loader.proxy(",
            "            self.opts, functions=self.functions, returners=self.returners",
            "        )",
            "        # TODO: remove",
            "        self.function_errors = {}  # Keep the funcs clean",
            "        self.states = salt.loader.states(",
            "            self.opts,",
            "            functions=self.functions,",
            "            utils=self.utils,",
            "            serializers=self.serializers,",
            "            context=context,",
            "        )",
            "        self.rend = salt.loader.render(",
            "            self.opts, functions=self.functions, context=context",
            "        )",
            "        #        self.matcher = Matcher(self.opts, self.functions)",
            "        self.matchers = salt.loader.matchers(self.opts)",
            "        self.functions[\"sys.reload_modules\"] = self.gen_modules",
            "        self.executors = salt.loader.executors(",
            "            self.opts, functions=self.functions, proxy=self.proxy, context=context",
            "        )",
            "",
            "    @staticmethod",
            "    def process_schedule(minion, loop_interval):",
            "        try:",
            "            if hasattr(minion, \"schedule\"):",
            "                minion.schedule.eval()",
            "            else:",
            "                log.error(",
            "                    \"Minion scheduler not initialized. Scheduled jobs will not be run.\"",
            "                )",
            "                return",
            "            # Check if scheduler requires lower loop interval than",
            "            # the loop_interval setting",
            "            if minion.schedule.loop_interval < loop_interval:",
            "                loop_interval = minion.schedule.loop_interval",
            "                log.debug(\"Overriding loop_interval because of scheduled jobs.\")",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            log.error(\"Exception %s occurred in scheduled job\", exc)",
            "        return loop_interval",
            "",
            "    def process_beacons(self, functions):",
            "        \"\"\"",
            "        Evaluate all of the configured beacons, grab the config again in case",
            "        the pillar or grains changed",
            "        \"\"\"",
            "        if \"config.merge\" in functions:",
            "            b_conf = functions[\"config.merge\"](",
            "                \"beacons\", self.opts[\"beacons\"], omit_opts=True",
            "            )",
            "            if b_conf:",
            "                return self.beacons.process(",
            "                    b_conf, self.opts[\"grains\"]",
            "                )  # pylint: disable=no-member",
            "        return []",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def eval_master(self, opts, timeout=60, safe=True, failed=False, failback=False):",
            "        \"\"\"",
            "        Evaluates and returns a tuple of the current master address and the pub_channel.",
            "",
            "        In standard mode, just creates a pub_channel with the given master address.",
            "",
            "        With master_type=func evaluates the current master address from the given",
            "        module and then creates a pub_channel.",
            "",
            "        With master_type=failover takes the list of masters and loops through them.",
            "        The first one that allows the minion to create a pub_channel is then",
            "        returned. If this function is called outside the minions initialization",
            "        phase (for example from the minions main event-loop when a master connection",
            "        loss was detected), 'failed' should be set to True. The current",
            "        (possibly failed) master will then be removed from the list of masters.",
            "        \"\"\"",
            "        # return early if we are not connecting to a master",
            "        if opts[\"master_type\"] == \"disable\":",
            "            log.warning(\"Master is set to disable, skipping connection\")",
            "            self.connected = False",
            "            raise salt.ext.tornado.gen.Return((None, None))",
            "",
            "        # Run masters discovery over SSDP. This may modify the whole configuration,",
            "        # depending of the networking and sets of masters.",
            "        self._discover_masters()",
            "",
            "        # check if master_type was altered from its default",
            "        if opts[\"master_type\"] != \"str\" and opts[\"__role\"] != \"syndic\":",
            "            # check for a valid keyword",
            "            if opts[\"master_type\"] == \"func\":",
            "                eval_master_func(opts)",
            "",
            "            # if failover or distributed is set, master has to be of type list",
            "            elif opts[\"master_type\"] in (\"failover\", \"distributed\"):",
            "                if isinstance(opts[\"master\"], list):",
            "                    log.info(",
            "                        \"Got list of available master addresses: %s\", opts[\"master\"]",
            "                    )",
            "",
            "                    if opts[\"master_type\"] == \"distributed\":",
            "                        master_len = len(opts[\"master\"])",
            "                        if master_len > 1:",
            "                            secondary_masters = opts[\"master\"][1:]",
            "                            master_idx = crc32(opts[\"id\"]) % master_len",
            "                            try:",
            "                                preferred_masters = opts[\"master\"]",
            "                                preferred_masters[0] = opts[\"master\"][master_idx]",
            "                                preferred_masters[1:] = [",
            "                                    m",
            "                                    for m in opts[\"master\"]",
            "                                    if m != preferred_masters[0]",
            "                                ]",
            "                                opts[\"master\"] = preferred_masters",
            "                                log.info(",
            "                                    \"Distributed to the master at '%s'.\",",
            "                                    opts[\"master\"][0],",
            "                                )",
            "                            except (KeyError, AttributeError, TypeError):",
            "                                log.warning(",
            "                                    \"Failed to distribute to a specific master.\"",
            "                                )",
            "                        else:",
            "                            log.warning(",
            "                                \"master_type = distributed needs more than 1 master.\"",
            "                            )",
            "",
            "                    if opts[\"master_shuffle\"]:",
            "                        log.warning(",
            "                            \"Use of 'master_shuffle' detected. 'master_shuffle' is deprecated in favor \"",
            "                            \"of 'random_master'. Please update your minion config file.\"",
            "                        )",
            "                        opts[\"random_master\"] = opts[\"master_shuffle\"]",
            "",
            "                    opts[\"auth_tries\"] = 0",
            "                    if (",
            "                        opts[\"master_failback\"]",
            "                        and opts[\"master_failback_interval\"] == 0",
            "                    ):",
            "                        opts[\"master_failback_interval\"] = opts[\"master_alive_interval\"]",
            "                # if opts['master'] is a str and we have never created opts['master_list']",
            "                elif isinstance(opts[\"master\"], str) and (\"master_list\" not in opts):",
            "                    # We have a string, but a list was what was intended. Convert.",
            "                    # See issue 23611 for details",
            "                    opts[\"master\"] = [opts[\"master\"]]",
            "                elif opts[\"__role\"] == \"syndic\":",
            "                    log.info(\"Syndic setting master_syndic to '%s'\", opts[\"master\"])",
            "",
            "                # if failed=True, the minion was previously connected",
            "                # we're probably called from the minions main-event-loop",
            "                # because a master connection loss was detected. remove",
            "                # the possibly failed master from the list of masters.",
            "                elif failed:",
            "                    if failback:",
            "                        # failback list of masters to original config",
            "                        opts[\"master\"] = opts[\"master_list\"]",
            "                    else:",
            "                        log.info(",
            "                            \"Moving possibly failed master %s to the end of \"",
            "                            \"the list of masters\",",
            "                            opts[\"master\"],",
            "                        )",
            "                        if opts[\"master\"] in opts[\"local_masters\"]:",
            "                            # create new list of master with the possibly failed",
            "                            # one moved to the end",
            "                            failed_master = opts[\"master\"]",
            "                            opts[\"master\"] = [",
            "                                x for x in opts[\"local_masters\"] if opts[\"master\"] != x",
            "                            ]",
            "                            opts[\"master\"].append(failed_master)",
            "                        else:",
            "                            opts[\"master\"] = opts[\"master_list\"]",
            "                else:",
            "                    msg = (",
            "                        \"master_type set to 'failover' but 'master' \"",
            "                        \"is not of type list but of type \"",
            "                        \"{}\".format(type(opts[\"master\"]))",
            "                    )",
            "                    log.error(msg)",
            "                    sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "                # If failover is set, minion have to failover on DNS errors instead of retry DNS resolve.",
            "                # See issue 21082 for details",
            "                if opts[\"retry_dns\"] and opts[\"master_type\"] == \"failover\":",
            "                    msg = (",
            "                        \"'master_type' set to 'failover' but 'retry_dns' is not 0. \"",
            "                        \"Setting 'retry_dns' to 0 to failover to the next master on DNS errors.\"",
            "                    )",
            "                    log.critical(msg)",
            "                    opts[\"retry_dns\"] = 0",
            "            else:",
            "                msg = \"Invalid keyword '{}' for variable \" \"'master_type'\".format(",
            "                    opts[\"master_type\"]",
            "                )",
            "                log.error(msg)",
            "                sys.exit(salt.defaults.exitcodes.EX_GENERIC)",
            "",
            "        # FIXME: if SMinion don't define io_loop, it can't switch master see #29088",
            "        # Specify kwargs for the channel factory so that SMinion doesn't need to define an io_loop",
            "        # (The channel factories will set a default if the kwarg isn't passed)",
            "        factory_kwargs = {\"timeout\": timeout, \"safe\": safe}",
            "        if getattr(self, \"io_loop\", None):",
            "            factory_kwargs[\"io_loop\"] = self.io_loop  # pylint: disable=no-member",
            "",
            "        tries = opts.get(\"master_tries\", 1)",
            "        attempts = 0",
            "",
            "        # if we have a list of masters, loop through them and be",
            "        # happy with the first one that allows us to connect",
            "        if isinstance(opts[\"master\"], list):",
            "            conn = False",
            "            last_exc = None",
            "            opts[\"master_uri_list\"] = []",
            "            opts[\"local_masters\"] = copy.copy(opts[\"master\"])",
            "",
            "            # shuffle the masters and then loop through them",
            "            if opts[\"random_master\"]:",
            "                # master_failback is only used when master_type is set to failover",
            "                if opts[\"master_type\"] == \"failover\" and opts[\"master_failback\"]:",
            "                    secondary_masters = opts[\"local_masters\"][1:]",
            "                    shuffle(secondary_masters)",
            "                    opts[\"local_masters\"][1:] = secondary_masters",
            "                else:",
            "                    shuffle(opts[\"local_masters\"])",
            "",
            "            # This sits outside of the connection loop below because it needs to set",
            "            # up a list of master URIs regardless of which masters are available",
            "            # to connect _to_. This is primarily used for masterless mode, when",
            "            # we need a list of master URIs to fire calls back to.",
            "            for master in opts[\"local_masters\"]:",
            "                opts[\"master\"] = master",
            "                opts.update(prep_ip_port(opts))",
            "                if opts[\"master_type\"] == \"failover\":",
            "                    try:",
            "                        opts[\"master_uri_list\"].append(",
            "                            resolve_dns(opts, False)[\"master_uri\"]",
            "                        )",
            "                    except SaltClientError:",
            "                        continue",
            "                else:",
            "                    opts[\"master_uri_list\"].append(resolve_dns(opts)[\"master_uri\"])",
            "",
            "            if not opts[\"master_uri_list\"]:",
            "                msg = \"No master could be resolved\"",
            "                log.error(msg)",
            "                raise SaltClientError(msg)",
            "",
            "            pub_channel = None",
            "            while True:",
            "                if attempts != 0:",
            "                    # Give up a little time between connection attempts",
            "                    # to allow the IOLoop to run any other scheduled tasks.",
            "                    yield salt.ext.tornado.gen.sleep(opts[\"acceptance_wait_time\"])",
            "                attempts += 1",
            "                if tries > 0:",
            "                    log.debug(\"Connecting to master. Attempt %s of %s\", attempts, tries)",
            "                else:",
            "                    log.debug(",
            "                        \"Connecting to master. Attempt %s (infinite attempts)\", attempts",
            "                    )",
            "                for master in opts[\"local_masters\"]:",
            "                    opts[\"master\"] = master",
            "                    opts.update(prep_ip_port(opts))",
            "                    if opts[\"master_type\"] == \"failover\":",
            "                        try:",
            "                            opts.update(resolve_dns(opts, False))",
            "                        except SaltClientError:",
            "                            continue",
            "                    else:",
            "                        opts.update(resolve_dns(opts))",
            "",
            "                    # on first run, update self.opts with the whole master list",
            "                    # to enable a minion to re-use old masters if they get fixed",
            "                    if \"master_list\" not in opts:",
            "                        opts[\"master_list\"] = copy.copy(opts[\"local_masters\"])",
            "",
            "                    self.opts = opts",
            "",
            "                    pub_channel = salt.transport.client.AsyncPubChannel.factory(",
            "                        opts, **factory_kwargs",
            "                    )",
            "                    try:",
            "                        yield pub_channel.connect()",
            "                        conn = True",
            "                        break",
            "                    except SaltClientError as exc:",
            "                        last_exc = exc",
            "                        if exc.strerror.startswith(\"Could not access\"):",
            "                            msg = (",
            "                                \"Failed to initiate connection with Master \"",
            "                                \"%s: check ownership/permissions. Error \"",
            "                                \"message: %s\",",
            "                                opts[\"master\"],",
            "                                exc,",
            "                            )",
            "                        else:",
            "                            msg = (",
            "                                \"Master %s could not be reached, trying next \"",
            "                                \"next master (if any)\",",
            "                                opts[\"master\"],",
            "                            )",
            "                        log.info(msg)",
            "                        pub_channel.close()",
            "                        pub_channel = None",
            "                        continue",
            "",
            "                if not conn:",
            "                    if attempts == tries:",
            "                        # Exhausted all attempts. Return exception.",
            "                        self.connected = False",
            "                        self.opts[\"master\"] = copy.copy(self.opts[\"local_masters\"])",
            "                        log.error(",
            "                            \"No master could be reached or all masters \"",
            "                            \"denied the minion's connection attempt.\"",
            "                        )",
            "                        if pub_channel:",
            "                            pub_channel.close()",
            "                        # If the code reaches this point, 'last_exc'",
            "                        # should already be set.",
            "                        raise last_exc  # pylint: disable=E0702",
            "                else:",
            "                    self.tok = pub_channel.auth.gen_token(b\"salt\")",
            "                    self.connected = True",
            "                    raise salt.ext.tornado.gen.Return((opts[\"master\"], pub_channel))",
            "",
            "        # single master sign in",
            "        else:",
            "            if opts[\"random_master\"]:",
            "                log.warning(",
            "                    \"random_master is True but there is only one master specified. Ignoring.\"",
            "                )",
            "            pub_channel = None",
            "            while True:",
            "                if attempts != 0:",
            "                    # Give up a little time between connection attempts",
            "                    # to allow the IOLoop to run any other scheduled tasks.",
            "                    yield salt.ext.tornado.gen.sleep(opts[\"acceptance_wait_time\"])",
            "                attempts += 1",
            "                if tries > 0:",
            "                    log.debug(\"Connecting to master. Attempt %s of %s\", attempts, tries)",
            "                else:",
            "                    log.debug(",
            "                        \"Connecting to master. Attempt %s (infinite attempts)\", attempts",
            "                    )",
            "                opts.update(prep_ip_port(opts))",
            "                opts.update(resolve_dns(opts))",
            "                try:",
            "                    if self.opts[\"transport\"] == \"detect\":",
            "                        self.opts[\"detect_mode\"] = True",
            "                        for trans in (\"zeromq\", \"tcp\"):",
            "                            if trans == \"zeromq\" and not zmq:",
            "                                continue",
            "                            self.opts[\"transport\"] = trans",
            "                            pub_channel = salt.transport.client.AsyncPubChannel.factory(",
            "                                self.opts, **factory_kwargs",
            "                            )",
            "                            yield pub_channel.connect()",
            "                            if not pub_channel.auth.authenticated:",
            "                                continue",
            "                            del self.opts[\"detect_mode\"]",
            "                            break",
            "                    else:",
            "                        pub_channel = salt.transport.client.AsyncPubChannel.factory(",
            "                            self.opts, **factory_kwargs",
            "                        )",
            "                        yield pub_channel.connect()",
            "                    self.tok = pub_channel.auth.gen_token(b\"salt\")",
            "                    self.connected = True",
            "                    raise salt.ext.tornado.gen.Return((opts[\"master\"], pub_channel))",
            "                except SaltClientError:",
            "                    if attempts == tries:",
            "                        # Exhausted all attempts. Return exception.",
            "                        self.connected = False",
            "                        if pub_channel:",
            "                            pub_channel.close()",
            "                        raise",
            "",
            "    def _discover_masters(self):",
            "        \"\"\"",
            "        Discover master(s) and decide where to connect, if SSDP is around.",
            "        This modifies the configuration on the fly.",
            "        :return:",
            "        \"\"\"",
            "        if (",
            "            self.opts[\"master\"] == DEFAULT_MINION_OPTS[\"master\"]",
            "            and self.opts[\"discovery\"] is not False",
            "        ):",
            "            master_discovery_client = salt.utils.ssdp.SSDPDiscoveryClient()",
            "            masters = {}",
            "            for att in range(self.opts[\"discovery\"].get(\"attempts\", 3)):",
            "                try:",
            "                    att += 1",
            "                    log.info(\"Attempting %s time(s) to discover masters\", att)",
            "                    masters.update(master_discovery_client.discover())",
            "                    if not masters:",
            "                        time.sleep(self.opts[\"discovery\"].get(\"pause\", 5))",
            "                    else:",
            "                        break",
            "                except Exception as err:  # pylint: disable=broad-except",
            "                    log.error(\"SSDP discovery failure: %s\", err)",
            "                    break",
            "",
            "            if masters:",
            "                policy = self.opts.get(\"discovery\", {}).get(\"match\", \"any\")",
            "                if policy not in [\"any\", \"all\"]:",
            "                    log.error(",
            "                        'SSDP configuration matcher failure: unknown value \"%s\". '",
            "                        'Should be \"any\" or \"all\"',",
            "                        policy,",
            "                    )",
            "                else:",
            "                    mapping = self.opts[\"discovery\"].get(\"mapping\", {})",
            "                    for addr, mappings in masters.items():",
            "                        for proto_data in mappings:",
            "                            cnt = len(",
            "                                [",
            "                                    key",
            "                                    for key, value in mapping.items()",
            "                                    if proto_data.get(\"mapping\", {}).get(key) == value",
            "                                ]",
            "                            )",
            "                            if policy == \"any\" and bool(cnt) or cnt == len(mapping):",
            "                                self.opts[\"master\"] = proto_data[\"master\"]",
            "                                return",
            "",
            "    def _return_retry_timer(self):",
            "        \"\"\"",
            "        Based on the minion configuration, either return a randomized timer or",
            "        just return the value of the return_retry_timer.",
            "        \"\"\"",
            "        msg = \"Minion return retry timer set to %s seconds\"",
            "        if self.opts.get(\"return_retry_timer_max\"):",
            "            try:",
            "                random_retry = randint(",
            "                    self.opts[\"return_retry_timer\"], self.opts[\"return_retry_timer_max\"]",
            "                )",
            "                retry_msg = msg % random_retry",
            "                log.debug(\"%s (randomized)\", msg % random_retry)",
            "                return random_retry",
            "            except ValueError:",
            "                # Catch wiseguys using negative integers here",
            "                log.error(",
            "                    \"Invalid value (return_retry_timer: %s or \"",
            "                    \"return_retry_timer_max: %s). Both must be positive \"",
            "                    \"integers.\",",
            "                    self.opts[\"return_retry_timer\"],",
            "                    self.opts[\"return_retry_timer_max\"],",
            "                )",
            "                log.debug(msg, DEFAULT_MINION_OPTS[\"return_retry_timer\"])",
            "                return DEFAULT_MINION_OPTS[\"return_retry_timer\"]",
            "        else:",
            "            log.debug(msg, self.opts.get(\"return_retry_timer\"))",
            "            return self.opts.get(\"return_retry_timer\")",
            "",
            "",
            "class SMinion(MinionBase):",
            "    \"\"\"",
            "    Create an object that has loaded all of the minion module functions,",
            "    grains, modules, returners etc.  The SMinion allows developers to",
            "    generate all of the salt minion functions and present them with these",
            "    functions for general use.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, context=None):",
            "        # Late setup of the opts grains, so we can log from the grains module",
            "        import salt.loader",
            "",
            "        opts[\"grains\"] = salt.loader.grains(opts)",
            "        super().__init__(opts)",
            "",
            "        # Clean out the proc directory (default /var/cache/salt/minion/proc)",
            "        if self.opts.get(\"file_client\", \"remote\") == \"remote\" or self.opts.get(",
            "            \"use_master_when_local\", False",
            "        ):",
            "            install_zmq()",
            "            io_loop = ZMQDefaultLoop.current()",
            "            io_loop.run_sync(lambda: self.eval_master(self.opts, failed=True))",
            "        self.gen_modules(initial_load=True, context=context or {})",
            "",
            "        # If configured, cache pillar data on the minion",
            "        if self.opts[\"file_client\"] == \"remote\" and self.opts.get(",
            "            \"minion_pillar_cache\", False",
            "        ):",
            "            import salt.utils.yaml",
            "",
            "            pdir = os.path.join(self.opts[\"cachedir\"], \"pillar\")",
            "            if not os.path.isdir(pdir):",
            "                os.makedirs(pdir, 0o700)",
            "            ptop = os.path.join(pdir, \"top.sls\")",
            "            if self.opts[\"saltenv\"] is not None:",
            "                penv = self.opts[\"saltenv\"]",
            "            else:",
            "                penv = \"base\"",
            "            cache_top = {penv: {self.opts[\"id\"]: [\"cache\"]}}",
            "            with salt.utils.files.fopen(ptop, \"wb\") as fp_:",
            "                salt.utils.yaml.safe_dump(cache_top, fp_, encoding=SLS_ENCODING)",
            "                os.chmod(ptop, 0o600)",
            "            cache_sls = os.path.join(pdir, \"cache.sls\")",
            "            with salt.utils.files.fopen(cache_sls, \"wb\") as fp_:",
            "                salt.utils.yaml.safe_dump(",
            "                    self.opts[\"pillar\"], fp_, encoding=SLS_ENCODING",
            "                )",
            "                os.chmod(cache_sls, 0o600)",
            "",
            "",
            "class MasterMinion:",
            "    \"\"\"",
            "    Create a fully loaded minion function object for generic use on the",
            "    master. What makes this class different is that the pillar is",
            "    omitted, otherwise everything else is loaded cleanly.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        opts,",
            "        returners=True,",
            "        states=True,",
            "        rend=True,",
            "        matcher=True,",
            "        whitelist=None,",
            "        ignore_config_errors=True,",
            "    ):",
            "        self.opts = salt.config.minion_config(",
            "            opts[\"conf_file\"], ignore_config_errors=ignore_config_errors, role=\"master\"",
            "        )",
            "        self.opts.update(opts)",
            "        self.whitelist = whitelist",
            "        self.opts[\"grains\"] = salt.loader.grains(opts)",
            "        self.opts[\"pillar\"] = {}",
            "        self.mk_returners = returners",
            "        self.mk_states = states",
            "        self.mk_rend = rend",
            "        self.mk_matcher = matcher",
            "        self.gen_modules(initial_load=True)",
            "",
            "    def gen_modules(self, initial_load=False):",
            "        \"\"\"",
            "        Tell the minion to reload the execution modules",
            "",
            "        CLI Example:",
            "",
            "        .. code-block:: bash",
            "",
            "            salt '*' sys.reload_modules",
            "        \"\"\"",
            "        self.utils = salt.loader.utils(self.opts)",
            "        self.functions = salt.loader.minion_mods(",
            "            self.opts,",
            "            utils=self.utils,",
            "            whitelist=self.whitelist,",
            "            initial_load=initial_load,",
            "        )",
            "        self.serializers = salt.loader.serializers(self.opts)",
            "        if self.mk_returners:",
            "            self.returners = salt.loader.returners(self.opts, self.functions)",
            "        if self.mk_states:",
            "            self.states = salt.loader.states(",
            "                self.opts, self.functions, self.utils, self.serializers",
            "            )",
            "        if self.mk_rend:",
            "            self.rend = salt.loader.render(self.opts, self.functions)",
            "        if self.mk_matcher:",
            "            self.matchers = salt.loader.matchers(self.opts)",
            "        self.functions[\"sys.reload_modules\"] = self.gen_modules",
            "",
            "",
            "class MinionManager(MinionBase):",
            "    \"\"\"",
            "    Create a multi minion interface, this creates as many minions as are",
            "    defined in the master option and binds each minion object to a respective",
            "    master.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts):",
            "        super().__init__(opts)",
            "        self.auth_wait = self.opts[\"acceptance_wait_time\"]",
            "        self.max_auth_wait = self.opts[\"acceptance_wait_time_max\"]",
            "        self.minions = []",
            "        self.jid_queue = []",
            "",
            "        install_zmq()",
            "        self.io_loop = ZMQDefaultLoop.current()",
            "        self.process_manager = ProcessManager(name=\"MultiMinionProcessManager\")",
            "        self.io_loop.spawn_callback(",
            "            self.process_manager.run, **{\"asynchronous\": True}",
            "        )  # Tornado backward compat",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.destroy()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _bind(self):",
            "        # start up the event publisher, so we can see events during startup",
            "        self.event_publisher = salt.utils.event.AsyncEventPublisher(",
            "            self.opts, io_loop=self.io_loop,",
            "        )",
            "        self.event = salt.utils.event.get_event(",
            "            \"minion\", opts=self.opts, io_loop=self.io_loop",
            "        )",
            "        self.event.subscribe(\"\")",
            "        self.event.set_event_handler(self.handle_event)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_event(self, package):",
            "        for minion in self.minions:",
            "            minion.handle_event(package)",
            "",
            "    def _create_minion_object(",
            "        self, opts, timeout, safe, io_loop=None, loaded_base_name=None, jid_queue=None",
            "    ):",
            "        \"\"\"",
            "        Helper function to return the correct type of object",
            "        \"\"\"",
            "        return Minion(",
            "            opts,",
            "            timeout,",
            "            safe,",
            "            io_loop=io_loop,",
            "            loaded_base_name=loaded_base_name,",
            "            jid_queue=jid_queue,",
            "        )",
            "",
            "    def _check_minions(self):",
            "        \"\"\"",
            "        Check the size of self.minions and raise an error if it's empty",
            "        \"\"\"",
            "        if not self.minions:",
            "            err = \"Minion unable to successfully connect to \" \"a Salt Master.\"",
            "            log.error(err)",
            "",
            "    def _spawn_minions(self, timeout=60):",
            "        \"\"\"",
            "        Spawn all the coroutines which will sign in to masters",
            "        \"\"\"",
            "        masters = self.opts[\"master\"]",
            "        if (self.opts[\"master_type\"] in (\"failover\", \"distributed\")) or not isinstance(",
            "            self.opts[\"master\"], list",
            "        ):",
            "            masters = [masters]",
            "",
            "        beacons_leader = True",
            "        for master in masters:",
            "            s_opts = copy.deepcopy(self.opts)",
            "            s_opts[\"master\"] = master",
            "            s_opts[\"multimaster\"] = True",
            "            s_opts[\"beacons_leader\"] = beacons_leader",
            "            if beacons_leader:",
            "                beacons_leader = False",
            "            minion = self._create_minion_object(",
            "                s_opts,",
            "                s_opts[\"auth_timeout\"],",
            "                False,",
            "                io_loop=self.io_loop,",
            "                loaded_base_name=\"salt.loader.{}\".format(s_opts[\"master\"]),",
            "                jid_queue=self.jid_queue,",
            "            )",
            "            self.io_loop.spawn_callback(self._connect_minion, minion)",
            "        self.io_loop.call_later(timeout, self._check_minions)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _connect_minion(self, minion):",
            "        \"\"\"",
            "        Create a minion, and asynchronously connect it to a master",
            "        \"\"\"",
            "        last = 0  # never have we signed in",
            "        auth_wait = minion.opts[\"acceptance_wait_time\"]",
            "        failed = False",
            "        while True:",
            "            try:",
            "                if minion.opts.get(\"beacons_before_connect\", False):",
            "                    minion.setup_beacons(before_connect=True)",
            "                if minion.opts.get(\"scheduler_before_connect\", False):",
            "                    minion.setup_scheduler(before_connect=True)",
            "                yield minion.connect_master(failed=failed)",
            "                minion.tune_in(start=False)",
            "                self.minions.append(minion)",
            "                break",
            "            except SaltClientError as exc:",
            "                failed = True",
            "                log.error(",
            "                    \"Error while bringing up minion for multi-master. Is \"",
            "                    \"master at %s responding?\",",
            "                    minion.opts[\"master\"],",
            "                )",
            "                last = time.time()",
            "                if auth_wait < self.max_auth_wait:",
            "                    auth_wait += self.auth_wait",
            "                yield salt.ext.tornado.gen.sleep(auth_wait)  # TODO: log?",
            "            except SaltMasterUnresolvableError:",
            "                err = (",
            "                    \"Master address: '{}' could not be resolved. Invalid or unresolveable address. \"",
            "                    \"Set 'master' value in minion config.\".format(minion.opts[\"master\"])",
            "                )",
            "                log.error(err)",
            "                break",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                failed = True",
            "                log.critical(",
            "                    \"Unexpected error while connecting to %s\",",
            "                    minion.opts[\"master\"],",
            "                    exc_info=True,",
            "                )",
            "",
            "    # Multi Master Tune In",
            "    def tune_in(self):",
            "        \"\"\"",
            "        Bind to the masters",
            "",
            "        This loop will attempt to create connections to masters it hasn't connected",
            "        to yet, but once the initial connection is made it is up to ZMQ to do the",
            "        reconnect (don't know of an API to get the state here in salt)",
            "        \"\"\"",
            "        self._bind()",
            "",
            "        # Fire off all the minion coroutines",
            "        self._spawn_minions()",
            "",
            "        # serve forever!",
            "        self.io_loop.start()",
            "",
            "    @property",
            "    def restart(self):",
            "        for minion in self.minions:",
            "            if minion.restart:",
            "                return True",
            "        return False",
            "",
            "    def stop(self, signum):",
            "        for minion in self.minions:",
            "            minion.process_manager.stop_restarting()",
            "            minion.process_manager.send_signal_to_processes(signum)",
            "            # kill any remaining processes",
            "            minion.process_manager.kill_children()",
            "            minion.destroy()",
            "",
            "    def destroy(self):",
            "        for minion in self.minions:",
            "            minion.destroy()",
            "",
            "",
            "class Minion(MinionBase):",
            "    \"\"\"",
            "    This class instantiates a minion, runs connections for a minion,",
            "    and loads all of the functions into the minion",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        opts,",
            "        timeout=60,",
            "        safe=True,",
            "        loaded_base_name=None,",
            "        io_loop=None,",
            "        jid_queue=None,",
            "    ):  # pylint: disable=W0231",
            "        \"\"\"",
            "        Pass in the options dict",
            "        \"\"\"",
            "        # this means that the parent class doesn't know *which* master we connect to",
            "        super().__init__(opts)",
            "        self.timeout = timeout",
            "        self.safe = safe",
            "",
            "        self._running = None",
            "        self.win_proc = []",
            "        self.subprocess_list = salt.utils.process.SubprocessList()",
            "        self.loaded_base_name = loaded_base_name",
            "        self.connected = False",
            "        self.restart = False",
            "        # Flag meaning minion has finished initialization including first connect to the master.",
            "        # True means the Minion is fully functional and ready to handle events.",
            "        self.ready = False",
            "        self.jid_queue = [] if jid_queue is None else jid_queue",
            "        self.periodic_callbacks = {}",
            "",
            "        if io_loop is None:",
            "            install_zmq()",
            "            self.io_loop = ZMQDefaultLoop.current()",
            "        else:",
            "            self.io_loop = io_loop",
            "",
            "        # Warn if ZMQ < 3.2",
            "        if zmq:",
            "            if ZMQ_VERSION_INFO < (3, 2):",
            "                log.warning(",
            "                    \"You have a version of ZMQ less than ZMQ 3.2! There are \"",
            "                    \"known connection keep-alive issues with ZMQ < 3.2 which \"",
            "                    \"may result in loss of contact with minions. Please \"",
            "                    \"upgrade your ZMQ!\"",
            "                )",
            "        # Late setup of the opts grains, so we can log from the grains",
            "        # module.  If this is a proxy, however, we need to init the proxymodule",
            "        # before we can get the grains.  We do this for proxies in the",
            "        # post_master_init",
            "        if not salt.utils.platform.is_proxy():",
            "            if not self.opts.get(\"grains\", {}):",
            "                self.opts[\"grains\"] = salt.loader.grains(opts)",
            "        else:",
            "            if self.opts.get(\"beacons_before_connect\", False):",
            "                log.warning(",
            "                    \"'beacons_before_connect' is not supported \"",
            "                    \"for proxy minions. Setting to False\"",
            "                )",
            "                self.opts[\"beacons_before_connect\"] = False",
            "            if self.opts.get(\"scheduler_before_connect\", False):",
            "                log.warning(",
            "                    \"'scheduler_before_connect' is not supported \"",
            "                    \"for proxy minions. Setting to False\"",
            "                )",
            "                self.opts[\"scheduler_before_connect\"] = False",
            "",
            "        log.info(\"Creating minion process manager\")",
            "",
            "        if self.opts[\"random_startup_delay\"]:",
            "            sleep_time = random.randint(0, self.opts[\"random_startup_delay\"])",
            "            log.info(",
            "                \"Minion sleeping for %s seconds due to configured \"",
            "                \"startup_delay between 0 and %s seconds\",",
            "                sleep_time,",
            "                self.opts[\"random_startup_delay\"],",
            "            )",
            "            time.sleep(sleep_time)",
            "",
            "        self.process_manager = ProcessManager(name=\"MinionProcessManager\")",
            "        self.io_loop.spawn_callback(self.process_manager.run, **{\"asynchronous\": True})",
            "        # We don't have the proxy setup yet, so we can't start engines",
            "        # Engines need to be able to access __proxy__",
            "        if not salt.utils.platform.is_proxy():",
            "            self.io_loop.spawn_callback(",
            "                salt.engines.start_engines, self.opts, self.process_manager",
            "            )",
            "",
            "        # Install the SIGINT/SIGTERM handlers if not done so far",
            "        if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGINT, self._handle_signals)",
            "",
            "        if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:",
            "            # No custom signal handling was added, install our own",
            "            signal.signal(signal.SIGTERM, self._handle_signals)",
            "",
            "    def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument",
            "        self._running = False",
            "        # escalate the signals to the process manager",
            "        self.process_manager.stop_restarting()",
            "        self.process_manager.send_signal_to_processes(signum)",
            "        # kill any remaining processes",
            "        self.process_manager.kill_children()",
            "        time.sleep(1)",
            "        sys.exit(0)",
            "",
            "    def sync_connect_master(self, timeout=None, failed=False):",
            "        \"\"\"",
            "        Block until we are connected to a master",
            "        \"\"\"",
            "        self._sync_connect_master_success = False",
            "        log.debug(\"sync_connect_master\")",
            "",
            "        def on_connect_master_future_done(future):",
            "            self._sync_connect_master_success = True",
            "            self.io_loop.stop()",
            "",
            "        self._connect_master_future = self.connect_master(failed=failed)",
            "        # finish connecting to master",
            "        self._connect_master_future.add_done_callback(on_connect_master_future_done)",
            "        if timeout:",
            "            self.io_loop.call_later(timeout, self.io_loop.stop)",
            "        try:",
            "            self.io_loop.start()",
            "        except KeyboardInterrupt:",
            "            self.destroy()",
            "        # I made the following 3 line oddity to preserve traceback.",
            "        # Please read PR #23978 before changing, hopefully avoiding regressions.",
            "        # Good luck, we're all counting on you.  Thanks.",
            "        if self._connect_master_future.done():",
            "            future_exception = self._connect_master_future.exception()",
            "            if future_exception:",
            "                # This needs to be re-raised to preserve restart_on_error behavior.",
            "                raise six.reraise(*future_exception)",
            "        if timeout and self._sync_connect_master_success is False:",
            "            raise SaltDaemonNotRunning(\"Failed to connect to the salt-master\")",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect_master(self, failed=False):",
            "        \"\"\"",
            "        Return a future which will complete when you are connected to a master",
            "        \"\"\"",
            "        master, self.pub_channel = yield self.eval_master(",
            "            self.opts, self.timeout, self.safe, failed",
            "        )",
            "        yield self._post_master_init(master)",
            "",
            "    # TODO: better name...",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _post_master_init(self, master):",
            "        \"\"\"",
            "        Function to finish init after connecting to a master",
            "",
            "        This is primarily loading modules, pillars, etc. (since they need",
            "        to know which master they connected to)",
            "",
            "        If this function is changed, please check ProxyMinion._post_master_init",
            "        to see if those changes need to be propagated.",
            "",
            "        Minions and ProxyMinions need significantly different post master setups,",
            "        which is why the differences are not factored out into separate helper",
            "        functions.",
            "        \"\"\"",
            "        if self.connected:",
            "            self.opts[\"master\"] = master",
            "",
            "            # Initialize pillar before loader to make pillar accessible in modules",
            "            async_pillar = salt.pillar.get_async_pillar(",
            "                self.opts,",
            "                self.opts[\"grains\"],",
            "                self.opts[\"id\"],",
            "                self.opts[\"saltenv\"],",
            "                pillarenv=self.opts.get(\"pillarenv\"),",
            "            )",
            "            self.opts[\"pillar\"] = yield async_pillar.compile_pillar()",
            "            async_pillar.destroy()",
            "",
            "        if not self.ready:",
            "            self._setup_core()",
            "        elif self.connected and self.opts[\"pillar\"]:",
            "            # The pillar has changed due to the connection to the master.",
            "            # Reload the functions so that they can use the new pillar data.",
            "            (",
            "                self.functions,",
            "                self.returners,",
            "                self.function_errors,",
            "                self.executors,",
            "            ) = self._load_modules()",
            "            if hasattr(self, \"schedule\"):",
            "                self.schedule.functions = self.functions",
            "                self.schedule.returners = self.returners",
            "",
            "        if not hasattr(self, \"schedule\"):",
            "            self.schedule = salt.utils.schedule.Schedule(",
            "                self.opts,",
            "                self.functions,",
            "                self.returners,",
            "                cleanup=[master_event(type=\"alive\")],",
            "            )",
            "",
            "        # add default scheduling jobs to the minions scheduler",
            "        if self.opts[\"mine_enabled\"] and \"mine.update\" in self.functions:",
            "            self.schedule.add_job(",
            "                {",
            "                    \"__mine_interval\": {",
            "                        \"function\": \"mine.update\",",
            "                        \"minutes\": self.opts[\"mine_interval\"],",
            "                        \"jid_include\": True,",
            "                        \"maxrunning\": 2,",
            "                        \"run_on_start\": True,",
            "                        \"return_job\": self.opts.get(\"mine_return_job\", False),",
            "                    }",
            "                },",
            "                persist=True,",
            "            )",
            "            log.info(\"Added mine.update to scheduler\")",
            "        else:",
            "            self.schedule.delete_job(\"__mine_interval\", persist=True)",
            "",
            "        # add master_alive job if enabled",
            "        if (",
            "            self.opts[\"transport\"] != \"tcp\"",
            "            and self.opts[\"master_alive_interval\"] > 0",
            "            and self.connected",
            "        ):",
            "            self.schedule.add_job(",
            "                {",
            "                    master_event(type=\"alive\", master=self.opts[\"master\"]): {",
            "                        \"function\": \"status.master\",",
            "                        \"seconds\": self.opts[\"master_alive_interval\"],",
            "                        \"jid_include\": True,",
            "                        \"maxrunning\": 1,",
            "                        \"return_job\": False,",
            "                        \"kwargs\": {\"master\": self.opts[\"master\"], \"connected\": True},",
            "                    }",
            "                },",
            "                persist=True,",
            "            )",
            "            if (",
            "                self.opts[\"master_failback\"]",
            "                and \"master_list\" in self.opts",
            "                and self.opts[\"master\"] != self.opts[\"master_list\"][0]",
            "            ):",
            "                self.schedule.add_job(",
            "                    {",
            "                        master_event(type=\"failback\"): {",
            "                            \"function\": \"status.ping_master\",",
            "                            \"seconds\": self.opts[\"master_failback_interval\"],",
            "                            \"jid_include\": True,",
            "                            \"maxrunning\": 1,",
            "                            \"return_job\": False,",
            "                            \"kwargs\": {\"master\": self.opts[\"master_list\"][0]},",
            "                        }",
            "                    },",
            "                    persist=True,",
            "                )",
            "            else:",
            "                self.schedule.delete_job(master_event(type=\"failback\"), persist=True)",
            "        else:",
            "            self.schedule.delete_job(",
            "                master_event(type=\"alive\", master=self.opts[\"master\"]), persist=True",
            "            )",
            "            self.schedule.delete_job(master_event(type=\"failback\"), persist=True)",
            "",
            "    def _prep_mod_opts(self):",
            "        \"\"\"",
            "        Returns a copy of the opts with key bits stripped out",
            "        \"\"\"",
            "        mod_opts = {}",
            "        for key, val in self.opts.items():",
            "            if key == \"logger\":",
            "                continue",
            "            mod_opts[key] = val",
            "        return mod_opts",
            "",
            "    def _load_modules(",
            "        self, force_refresh=False, notify=False, grains=None, opts=None, context=None",
            "    ):",
            "        \"\"\"",
            "        Return the functions and the returners loaded up from the loader",
            "        module",
            "        \"\"\"",
            "        opt_in = True",
            "        if not opts:",
            "            opts = self.opts",
            "            opt_in = False",
            "        # if this is a *nix system AND modules_max_memory is set, lets enforce",
            "        # a memory limit on module imports",
            "        # this feature ONLY works on *nix like OSs (resource module doesn't work on windows)",
            "        modules_max_memory = False",
            "        if opts.get(\"modules_max_memory\", -1) > 0 and HAS_PSUTIL and HAS_RESOURCE:",
            "            log.debug(",
            "                \"modules_max_memory set, enforcing a maximum of %s\",",
            "                opts[\"modules_max_memory\"],",
            "            )",
            "            modules_max_memory = True",
            "            old_mem_limit = resource.getrlimit(resource.RLIMIT_AS)",
            "            rss, vms = psutil.Process(os.getpid()).memory_info()[:2]",
            "            mem_limit = rss + vms + opts[\"modules_max_memory\"]",
            "            resource.setrlimit(resource.RLIMIT_AS, (mem_limit, mem_limit))",
            "        elif opts.get(\"modules_max_memory\", -1) > 0:",
            "            if not HAS_PSUTIL:",
            "                log.error(",
            "                    \"Unable to enforce modules_max_memory because psutil is missing\"",
            "                )",
            "            if not HAS_RESOURCE:",
            "                log.error(",
            "                    \"Unable to enforce modules_max_memory because resource is missing\"",
            "                )",
            "",
            "        # This might be a proxy minion",
            "        if hasattr(self, \"proxy\"):",
            "            proxy = self.proxy",
            "        else:",
            "            proxy = None",
            "",
            "        if context is None:",
            "            context = {}",
            "",
            "        if grains is None:",
            "            opts[\"grains\"] = salt.loader.grains(",
            "                opts, force_refresh, proxy=proxy, context=context",
            "            )",
            "        self.utils = salt.loader.utils(opts, proxy=proxy, context=context)",
            "",
            "        if opts.get(\"multimaster\", False):",
            "            s_opts = copy.deepcopy(opts)",
            "            functions = salt.loader.minion_mods(",
            "                s_opts,",
            "                utils=self.utils,",
            "                proxy=proxy,",
            "                loaded_base_name=self.loaded_base_name,",
            "                notify=notify,",
            "                context=context,",
            "            )",
            "        else:",
            "            functions = salt.loader.minion_mods(",
            "                opts, utils=self.utils, notify=notify, proxy=proxy, context=context,",
            "            )",
            "        returners = salt.loader.returners(opts, functions, proxy=proxy, context=context)",
            "        errors = {}",
            "        if \"_errors\" in functions:",
            "            errors = functions[\"_errors\"]",
            "            functions.pop(\"_errors\")",
            "",
            "        # we're done, reset the limits!",
            "        if modules_max_memory is True:",
            "            resource.setrlimit(resource.RLIMIT_AS, old_mem_limit)",
            "",
            "        executors = salt.loader.executors(opts, functions, proxy=proxy, context=context)",
            "",
            "        if opt_in:",
            "            self.opts = opts",
            "",
            "        return functions, returners, errors, executors",
            "",
            "    def _send_req_sync(self, load, timeout):",
            "",
            "        if self.opts[\"minion_sign_messages\"]:",
            "            log.trace(\"Signing event to be published onto the bus.\")",
            "            minion_privkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "            sig = salt.crypt.sign_message(",
            "                minion_privkey_path, salt.serializers.msgpack.serialize(load)",
            "            )",
            "            load[\"sig\"] = sig",
            "",
            "        with salt.transport.client.ReqChannel.factory(self.opts) as channel:",
            "            return channel.send(load, timeout=timeout)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _send_req_async(self, load, timeout):",
            "",
            "        if self.opts[\"minion_sign_messages\"]:",
            "            log.trace(\"Signing event to be published onto the bus.\")",
            "            minion_privkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion.pem\")",
            "            sig = salt.crypt.sign_message(",
            "                minion_privkey_path, salt.serializers.msgpack.serialize(load)",
            "            )",
            "            load[\"sig\"] = sig",
            "",
            "        with salt.transport.client.AsyncReqChannel.factory(self.opts) as channel:",
            "            ret = yield channel.send(load, timeout=timeout)",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    def _fire_master(",
            "        self,",
            "        data=None,",
            "        tag=None,",
            "        events=None,",
            "        pretag=None,",
            "        timeout=60,",
            "        sync=True,",
            "        timeout_handler=None,",
            "        include_startup_grains=False,",
            "    ):",
            "        \"\"\"",
            "        Fire an event on the master, or drop message if unable to send.",
            "        \"\"\"",
            "        load = {",
            "            \"id\": self.opts[\"id\"],",
            "            \"cmd\": \"_minion_event\",",
            "            \"pretag\": pretag,",
            "            \"tok\": self.tok,",
            "        }",
            "        if events:",
            "            load[\"events\"] = events",
            "        elif data and tag:",
            "            load[\"data\"] = data",
            "            load[\"tag\"] = tag",
            "        elif not data and tag:",
            "            load[\"data\"] = {}",
            "            load[\"tag\"] = tag",
            "        else:",
            "            return",
            "",
            "        if include_startup_grains:",
            "            grains_to_add = {",
            "                k: v",
            "                for k, v in self.opts.get(\"grains\", {}).items()",
            "                if k in self.opts[\"start_event_grains\"]",
            "            }",
            "            load[\"grains\"] = grains_to_add",
            "",
            "        if sync:",
            "            try:",
            "                self._send_req_sync(load, timeout)",
            "            except salt.exceptions.SaltReqTimeoutError:",
            "                log.info(",
            "                    \"fire_master failed: master could not be contacted. Request timed out.\"",
            "                )",
            "                return False",
            "            except Exception:  # pylint: disable=broad-except",
            "                log.info(\"fire_master failed: %s\", traceback.format_exc())",
            "                return False",
            "        else:",
            "            if timeout_handler is None:",
            "",
            "                def handle_timeout(*_):",
            "                    log.info(",
            "                        \"fire_master failed: master could not be contacted. Request timed out.\"",
            "                    )",
            "                    return True",
            "",
            "                timeout_handler = handle_timeout",
            "",
            "            with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "                # pylint: disable=unexpected-keyword-arg",
            "                self._send_req_async(load, timeout, callback=lambda f: None)",
            "                # pylint: enable=unexpected-keyword-arg",
            "        return True",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _handle_decoded_payload(self, data):",
            "        \"\"\"",
            "        Override this method if you wish to handle the decoded data",
            "        differently.",
            "        \"\"\"",
            "",
            "        # Ensure payload is unicode. Disregard failure to decode binary blobs.",
            "        if six.PY2:",
            "            data = salt.utils.data.decode(data, keep=True)",
            "        if \"user\" in data:",
            "            log.info(",
            "                \"User %s Executing command %s with jid %s\",",
            "                data[\"user\"],",
            "                data[\"fun\"],",
            "                data[\"jid\"],",
            "            )",
            "        else:",
            "            log.info(\"Executing command %s with jid %s\", data[\"fun\"], data[\"jid\"])",
            "        log.debug(\"Command details %s\", data)",
            "",
            "        # Don't duplicate jobs",
            "        log.trace(\"Started JIDs: %s\", self.jid_queue)",
            "        if self.jid_queue is not None:",
            "            if data[\"jid\"] in self.jid_queue:",
            "                return",
            "            else:",
            "                self.jid_queue.append(data[\"jid\"])",
            "                if len(self.jid_queue) > self.opts[\"minion_jid_queue_hwm\"]:",
            "                    self.jid_queue.pop(0)",
            "",
            "        if isinstance(data[\"fun\"], str):",
            "            if data[\"fun\"] == \"sys.reload_modules\":",
            "                (",
            "                    self.functions,",
            "                    self.returners,",
            "                    self.function_errors,",
            "                    self.executors,",
            "                ) = self._load_modules()",
            "                self.schedule.functions = self.functions",
            "                self.schedule.returners = self.returners",
            "",
            "        process_count_max = self.opts.get(\"process_count_max\")",
            "        if process_count_max > 0:",
            "            process_count = len(salt.utils.minion.running(self.opts))",
            "            while process_count >= process_count_max:",
            "                log.warning(",
            "                    \"Maximum number of processes reached while executing jid %s, waiting...\",",
            "                    data[\"jid\"],",
            "                )",
            "                yield salt.ext.tornado.gen.sleep(10)",
            "                process_count = len(salt.utils.minion.running(self.opts))",
            "",
            "        # We stash an instance references to allow for the socket",
            "        # communication in Windows. You can't pickle functions, and thus",
            "        # python needs to be able to reconstruct the reference on the other",
            "        # side.",
            "        instance = self",
            "        multiprocessing_enabled = self.opts.get(\"multiprocessing\", True)",
            "        if multiprocessing_enabled:",
            "            if sys.platform.startswith(\"win\"):",
            "                # let python reconstruct the minion on the other side if we're",
            "                # running on windows",
            "                instance = None",
            "            with default_signals(signal.SIGINT, signal.SIGTERM):",
            "                process = SignalHandlingProcess(",
            "                    target=self._target,",
            "                    name=\"ProcessPayload\",",
            "                    args=(instance, self.opts, data, self.connected),",
            "                )",
            "                process._after_fork_methods.append(",
            "                    (salt.utils.crypt.reinit_crypto, [], {})",
            "                )",
            "        else:",
            "            process = threading.Thread(",
            "                target=self._target,",
            "                args=(instance, self.opts, data, self.connected),",
            "                name=data[\"jid\"],",
            "            )",
            "",
            "        if multiprocessing_enabled:",
            "            with default_signals(signal.SIGINT, signal.SIGTERM):",
            "                # Reset current signals before starting the process in",
            "                # order not to inherit the current signal handlers",
            "                process.start()",
            "        else:",
            "            process.start()",
            "        process.name = \"{}-Job-{}\".format(process.name, data[\"jid\"])",
            "        self.subprocess_list.add(process)",
            "",
            "    def ctx(self):",
            "        \"\"\"",
            "        Return a single context manager for the minion's data",
            "        \"\"\"",
            "        exitstack = contextlib.ExitStack()",
            "        exitstack.enter_context(self.functions.context_dict.clone())",
            "        exitstack.enter_context(self.returners.context_dict.clone())",
            "        exitstack.enter_context(self.executors.context_dict.clone())",
            "        return exitstack",
            "",
            "    @classmethod",
            "    def _target(cls, minion_instance, opts, data, connected):",
            "        if not minion_instance:",
            "            minion_instance = cls(opts)",
            "            minion_instance.connected = connected",
            "            if not hasattr(minion_instance, \"functions\"):",
            "                (",
            "                    functions,",
            "                    returners,",
            "                    function_errors,",
            "                    executors,",
            "                ) = minion_instance._load_modules(grains=opts[\"grains\"])",
            "                minion_instance.functions = functions",
            "                minion_instance.returners = returners",
            "                minion_instance.function_errors = function_errors",
            "                minion_instance.executors = executors",
            "            if not hasattr(minion_instance, \"serial\"):",
            "                minion_instance.serial = salt.payload.Serial(opts)",
            "            if not hasattr(minion_instance, \"proc_dir\"):",
            "                uid = salt.utils.user.get_uid(user=opts.get(\"user\", None))",
            "                minion_instance.proc_dir = get_proc_dir(opts[\"cachedir\"], uid=uid)",
            "",
            "        def run_func(minion_instance, opts, data):",
            "            if isinstance(data[\"fun\"], tuple) or isinstance(data[\"fun\"], list):",
            "                return Minion._thread_multi_return(minion_instance, opts, data)",
            "            else:",
            "                return Minion._thread_return(minion_instance, opts, data)",
            "",
            "        with salt.ext.tornado.stack_context.StackContext(",
            "            functools.partial(RequestContext, {\"data\": data, \"opts\": opts})",
            "        ):",
            "            with salt.ext.tornado.stack_context.StackContext(minion_instance.ctx):",
            "                run_func(minion_instance, opts, data)",
            "",
            "    def _execute_job_function(",
            "        self, function_name, function_args, executors, opts, data",
            "    ):",
            "        \"\"\"",
            "        Executes a function within a job given it's name, the args and the executors.",
            "        It also checks if the function is allowed to run if 'blackout mode' is enabled.",
            "        \"\"\"",
            "        minion_blackout_violation = False",
            "        if self.connected and self.opts[\"pillar\"].get(\"minion_blackout\", False):",
            "            whitelist = self.opts[\"pillar\"].get(\"minion_blackout_whitelist\", [])",
            "            # this minion is blacked out. Only allow saltutil.refresh_pillar and the whitelist",
            "            if (",
            "                function_name != \"saltutil.refresh_pillar\"",
            "                and function_name not in whitelist",
            "            ):",
            "                minion_blackout_violation = True",
            "        # use minion_blackout_whitelist from grains if it exists",
            "        if self.opts[\"grains\"].get(\"minion_blackout\", False):",
            "            whitelist = self.opts[\"grains\"].get(\"minion_blackout_whitelist\", [])",
            "            if (",
            "                function_name != \"saltutil.refresh_pillar\"",
            "                and function_name not in whitelist",
            "            ):",
            "                minion_blackout_violation = True",
            "        if minion_blackout_violation:",
            "            raise SaltInvocationError(",
            "                \"Minion in blackout mode. Set 'minion_blackout' \"",
            "                \"to False in pillar or grains to resume operations. Only \"",
            "                \"saltutil.refresh_pillar allowed in blackout mode.\"",
            "            )",
            "",
            "        if function_name in self.functions:",
            "            func = self.functions[function_name]",
            "            args, kwargs = load_args_and_kwargs(func, function_args, data)",
            "        else:",
            "            # only run if function_name is not in minion_instance.functions and allow_missing_funcs is True",
            "            func = function_name",
            "            args, kwargs = function_args, data",
            "        self.functions.pack[\"__context__\"][\"retcode\"] = 0",
            "",
            "        if isinstance(executors, str):",
            "            executors = [executors]",
            "        elif not isinstance(executors, list) or not executors:",
            "            raise SaltInvocationError(",
            "                \"Wrong executors specification: {}. String or non-empty list expected\".format(",
            "                    executors",
            "                )",
            "            )",
            "        if opts.get(\"sudo_user\", \"\") and executors[-1] != \"sudo\":",
            "            executors[-1] = \"sudo\"  # replace the last one with sudo",
            "        log.trace(\"Executors list %s\", executors)  # pylint: disable=no-member",
            "",
            "        for name in executors:",
            "            fname = \"{}.execute\".format(name)",
            "            if fname not in self.executors:",
            "                raise SaltInvocationError(\"Executor '{}' is not available\".format(name))",
            "            return_data = self.executors[fname](opts, data, func, args, kwargs)",
            "            if return_data is not None:",
            "                return return_data",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def _thread_return(cls, minion_instance, opts, data):",
            "        \"\"\"",
            "        This method should be used as a threading target, start the actual",
            "        minion side execution.",
            "        \"\"\"",
            "        minion_instance.gen_modules()",
            "        fn_ = os.path.join(minion_instance.proc_dir, data[\"jid\"])",
            "",
            "        salt.utils.process.appendproctitle(",
            "            \"{}._thread_return {}\".format(cls.__name__, data[\"jid\"])",
            "        )",
            "",
            "        sdata = {\"pid\": os.getpid()}",
            "        sdata.update(data)",
            "        log.info(\"Starting a new job %s with PID %s\", data[\"jid\"], sdata[\"pid\"])",
            "        with salt.utils.files.fopen(fn_, \"w+b\") as fp_:",
            "            fp_.write(minion_instance.serial.dumps(sdata))",
            "        ret = {\"success\": False}",
            "        function_name = data[\"fun\"]",
            "        function_args = data[\"arg\"]",
            "        executors = (",
            "            data.get(\"module_executors\")",
            "            or getattr(minion_instance, \"module_executors\", [])",
            "            or opts.get(\"module_executors\", [\"direct_call\"])",
            "        )",
            "        allow_missing_funcs = any(",
            "            [",
            "                minion_instance.executors[\"{}.allow_missing_func\".format(executor)](",
            "                    function_name",
            "                )",
            "                for executor in executors",
            "                if \"{}.allow_missing_func\".format(executor) in minion_instance.executors",
            "            ]",
            "        )",
            "        if function_name in minion_instance.functions or allow_missing_funcs is True:",
            "            try:",
            "                return_data = minion_instance._execute_job_function(",
            "                    function_name, function_args, executors, opts, data",
            "                )",
            "",
            "                if isinstance(return_data, types.GeneratorType):",
            "                    ind = 0",
            "                    iret = {}",
            "                    for single in return_data:",
            "                        if isinstance(single, dict) and isinstance(iret, dict):",
            "                            iret.update(single)",
            "                        else:",
            "                            if not iret:",
            "                                iret = []",
            "                            iret.append(single)",
            "                        tag = tagify([data[\"jid\"], \"prog\", opts[\"id\"], str(ind)], \"job\")",
            "                        event_data = {\"return\": single}",
            "                        minion_instance._fire_master(event_data, tag)",
            "                        ind += 1",
            "                    ret[\"return\"] = iret",
            "                else:",
            "                    ret[\"return\"] = return_data",
            "",
            "                retcode = minion_instance.functions.pack[\"__context__\"].get(",
            "                    \"retcode\", salt.defaults.exitcodes.EX_OK",
            "                )",
            "                if retcode == salt.defaults.exitcodes.EX_OK:",
            "                    # No nonzero retcode in __context__ dunder. Check if return",
            "                    # is a dictionary with a \"result\" or \"success\" key.",
            "                    try:",
            "                        func_result = all(",
            "                            return_data.get(x, True) for x in (\"result\", \"success\")",
            "                        )",
            "                    except Exception:  # pylint: disable=broad-except",
            "                        # return data is not a dict",
            "                        func_result = True",
            "                    if not func_result:",
            "                        retcode = salt.defaults.exitcodes.EX_GENERIC",
            "",
            "                ret[\"retcode\"] = retcode",
            "                ret[\"success\"] = retcode == salt.defaults.exitcodes.EX_OK",
            "            except CommandNotFoundError as exc:",
            "                msg = \"Command required for '{}' not found\".format(function_name)",
            "                log.debug(msg, exc_info=True)",
            "                ret[\"return\"] = \"{}: {}\".format(msg, exc)",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except CommandExecutionError as exc:",
            "                log.error(",
            "                    \"A command in '%s' had a problem: %s\",",
            "                    function_name,",
            "                    exc,",
            "                    exc_info_on_loglevel=logging.DEBUG,",
            "                )",
            "                ret[\"return\"] = \"ERROR: {}\".format(exc)",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except SaltInvocationError as exc:",
            "                log.error(",
            "                    \"Problem executing '%s': %s\",",
            "                    function_name,",
            "                    exc,",
            "                    exc_info_on_loglevel=logging.DEBUG,",
            "                )",
            "                ret[\"return\"] = \"ERROR executing '{}': {}\".format(function_name, exc)",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except TypeError as exc:",
            "                msg = \"Passed invalid arguments to {}: {}\\n{}\".format(",
            "                    function_name,",
            "                    exc,",
            "                    minion_instance.functions[function_name].__doc__ or \"\",",
            "                )",
            "                log.warning(msg, exc_info_on_loglevel=logging.DEBUG)",
            "                ret[\"return\"] = msg",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            except Exception:  # pylint: disable=broad-except",
            "                msg = \"The minion function caused an exception\"",
            "                log.warning(msg, exc_info_on_loglevel=True)",
            "                salt.utils.error.fire_exception(",
            "                    salt.exceptions.MinionError(msg), opts, job=data",
            "                )",
            "                ret[\"return\"] = \"{}: {}\".format(msg, traceback.format_exc())",
            "                ret[\"out\"] = \"nested\"",
            "                ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "        else:",
            "            docs = minion_instance.functions[\"sys.doc\"](\"{}*\".format(function_name))",
            "            if docs:",
            "                docs[function_name] = minion_instance.functions.missing_fun_string(",
            "                    function_name",
            "                )",
            "                ret[\"return\"] = docs",
            "            else:",
            "                ret[\"return\"] = minion_instance.functions.missing_fun_string(",
            "                    function_name",
            "                )",
            "                mod_name = function_name.split(\".\")[0]",
            "                if mod_name in minion_instance.function_errors:",
            "                    ret[\"return\"] += \" Possible reasons: '{}'\".format(",
            "                        minion_instance.function_errors[mod_name]",
            "                    )",
            "            ret[\"success\"] = False",
            "            ret[\"retcode\"] = salt.defaults.exitcodes.EX_GENERIC",
            "            ret[\"out\"] = \"nested\"",
            "",
            "        ret[\"jid\"] = data[\"jid\"]",
            "        ret[\"fun\"] = data[\"fun\"]",
            "        ret[\"fun_args\"] = data[\"arg\"]",
            "        if \"master_id\" in data:",
            "            ret[\"master_id\"] = data[\"master_id\"]",
            "        if \"metadata\" in data:",
            "            if isinstance(data[\"metadata\"], dict):",
            "                ret[\"metadata\"] = data[\"metadata\"]",
            "            else:",
            "                log.warning(\"The metadata parameter must be a dictionary. Ignoring.\")",
            "        if minion_instance.connected:",
            "            minion_instance._return_pub(",
            "                ret, timeout=minion_instance._return_retry_timer()",
            "            )",
            "",
            "        # Add default returners from minion config",
            "        # Should have been coverted to comma-delimited string already",
            "        if isinstance(opts.get(\"return\"), str):",
            "            if data[\"ret\"]:",
            "                data[\"ret\"] = \",\".join((data[\"ret\"], opts[\"return\"]))",
            "            else:",
            "                data[\"ret\"] = opts[\"return\"]",
            "",
            "        log.debug(\"minion return: %s\", ret)",
            "        # TODO: make a list? Seems odd to split it this late :/",
            "        if data[\"ret\"] and isinstance(data[\"ret\"], str):",
            "            if \"ret_config\" in data:",
            "                ret[\"ret_config\"] = data[\"ret_config\"]",
            "            if \"ret_kwargs\" in data:",
            "                ret[\"ret_kwargs\"] = data[\"ret_kwargs\"]",
            "            ret[\"id\"] = opts[\"id\"]",
            "            for returner in set(data[\"ret\"].split(\",\")):",
            "                try:",
            "                    returner_str = \"{}.returner\".format(returner)",
            "                    if returner_str in minion_instance.returners:",
            "                        minion_instance.returners[returner_str](ret)",
            "                    else:",
            "                        returner_err = minion_instance.returners.missing_fun_string(",
            "                            returner_str",
            "                        )",
            "                        log.error(",
            "                            \"Returner %s could not be loaded: %s\",",
            "                            returner_str,",
            "                            returner_err,",
            "                        )",
            "                except Exception as exc:  # pylint: disable=broad-except",
            "                    log.exception(\"The return failed for job %s: %s\", data[\"jid\"], exc)",
            "",
            "    @classmethod",
            "    def _thread_multi_return(cls, minion_instance, opts, data):",
            "        \"\"\"",
            "        This method should be used as a threading target, start the actual",
            "        minion side execution.",
            "        \"\"\"",
            "        minion_instance.gen_modules()",
            "        fn_ = os.path.join(minion_instance.proc_dir, data[\"jid\"])",
            "",
            "        salt.utils.process.appendproctitle(",
            "            \"{}._thread_multi_return {}\".format(cls.__name__, data[\"jid\"])",
            "        )",
            "",
            "        sdata = {\"pid\": os.getpid()}",
            "        sdata.update(data)",
            "        log.info(\"Starting a new job with PID %s\", sdata[\"pid\"])",
            "        with salt.utils.files.fopen(fn_, \"w+b\") as fp_:",
            "            fp_.write(minion_instance.serial.dumps(sdata))",
            "",
            "        multifunc_ordered = opts.get(\"multifunc_ordered\", False)",
            "        num_funcs = len(data[\"fun\"])",
            "        if multifunc_ordered:",
            "            ret = {",
            "                \"return\": [None] * num_funcs,",
            "                \"retcode\": [None] * num_funcs,",
            "                \"success\": [False] * num_funcs,",
            "            }",
            "        else:",
            "            ret = {\"return\": {}, \"retcode\": {}, \"success\": {}}",
            "        executors = (",
            "            data.get(\"module_executors\")",
            "            or getattr(minion_instance, \"module_executors\", [])",
            "            or opts.get(\"module_executors\", [\"direct_call\"])",
            "        )",
            "",
            "        for ind in range(0, num_funcs):",
            "            function_name = data[\"fun\"][ind]",
            "            function_args = data[\"arg\"][ind]",
            "            if not multifunc_ordered:",
            "                ret[\"success\"][function_name] = False",
            "            try:",
            "                return_data = minion_instance._execute_job_function(",
            "                    function_name, function_args, executors, opts, data",
            "                )",
            "",
            "                key = ind if multifunc_ordered else data[\"fun\"][ind]",
            "                ret[\"return\"][key] = return_data",
            "                retcode = minion_instance.functions.pack[\"__context__\"].get(",
            "                    \"retcode\", 0",
            "                )",
            "                if retcode == 0:",
            "                    # No nonzero retcode in __context__ dunder. Check if return",
            "                    # is a dictionary with a \"result\" or \"success\" key.",
            "                    try:",
            "                        func_result = all(",
            "                            ret[\"return\"][key].get(x, True)",
            "                            for x in (\"result\", \"success\")",
            "                        )",
            "                    except Exception:  # pylint: disable=broad-except",
            "                        # return data is not a dict",
            "                        func_result = True",
            "                    if not func_result:",
            "                        retcode = 1",
            "",
            "                ret[\"retcode\"][key] = retcode",
            "                ret[\"success\"][key] = retcode == 0",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                trb = traceback.format_exc()",
            "                log.warning(\"The minion function caused an exception: %s\", exc)",
            "                if multifunc_ordered:",
            "                    ret[\"return\"][ind] = trb",
            "                else:",
            "                    ret[\"return\"][data[\"fun\"][ind]] = trb",
            "            ret[\"jid\"] = data[\"jid\"]",
            "            ret[\"fun\"] = data[\"fun\"]",
            "            ret[\"fun_args\"] = data[\"arg\"]",
            "        if \"metadata\" in data:",
            "            ret[\"metadata\"] = data[\"metadata\"]",
            "        if minion_instance.connected:",
            "            minion_instance._return_pub(",
            "                ret, timeout=minion_instance._return_retry_timer()",
            "            )",
            "        if data[\"ret\"]:",
            "            if \"ret_config\" in data:",
            "                ret[\"ret_config\"] = data[\"ret_config\"]",
            "            if \"ret_kwargs\" in data:",
            "                ret[\"ret_kwargs\"] = data[\"ret_kwargs\"]",
            "            for returner in set(data[\"ret\"].split(\",\")):",
            "                ret[\"id\"] = opts[\"id\"]",
            "                try:",
            "                    minion_instance.returners[\"{}.returner\".format(returner)](ret)",
            "                except Exception as exc:  # pylint: disable=broad-except",
            "                    log.error(\"The return failed for job %s: %s\", data[\"jid\"], exc)",
            "",
            "    def _return_pub(self, ret, ret_cmd=\"_return\", timeout=60, sync=True):",
            "        \"\"\"",
            "        Return the data from the executed command to the master server",
            "        \"\"\"",
            "        jid = ret.get(\"jid\", ret.get(\"__jid__\"))",
            "        fun = ret.get(\"fun\", ret.get(\"__fun__\"))",
            "        if self.opts[\"multiprocessing\"]:",
            "            fn_ = os.path.join(self.proc_dir, jid)",
            "            if os.path.isfile(fn_):",
            "                try:",
            "                    os.remove(fn_)",
            "                except OSError:",
            "                    # The file is gone already",
            "                    pass",
            "        log.info(\"Returning information for job: %s\", jid)",
            "        log.trace(\"Return data: %s\", ret)",
            "        if ret_cmd == \"_syndic_return\":",
            "            load = {",
            "                \"cmd\": ret_cmd,",
            "                \"id\": self.opts[\"uid\"],",
            "                \"jid\": jid,",
            "                \"fun\": fun,",
            "                \"arg\": ret.get(\"arg\"),",
            "                \"tgt\": ret.get(\"tgt\"),",
            "                \"tgt_type\": ret.get(\"tgt_type\"),",
            "                \"load\": ret.get(\"__load__\"),",
            "            }",
            "            if \"__master_id__\" in ret:",
            "                load[\"master_id\"] = ret[\"__master_id__\"]",
            "            load[\"return\"] = {}",
            "            for key, value in ret.items():",
            "                if key.startswith(\"__\"):",
            "                    continue",
            "                load[\"return\"][key] = value",
            "        else:",
            "            load = {\"cmd\": ret_cmd, \"id\": self.opts[\"id\"]}",
            "            for key, value in ret.items():",
            "                load[key] = value",
            "",
            "        if \"out\" in ret:",
            "            if isinstance(ret[\"out\"], str):",
            "                load[\"out\"] = ret[\"out\"]",
            "            else:",
            "                log.error(\"Invalid outputter %s. This is likely a bug.\", ret[\"out\"])",
            "        else:",
            "            try:",
            "                oput = self.functions[fun].__outputter__",
            "            except (KeyError, AttributeError, TypeError):",
            "                pass",
            "            else:",
            "                if isinstance(oput, str):",
            "                    load[\"out\"] = oput",
            "        if self.opts[\"cache_jobs\"]:",
            "            # Local job cache has been enabled",
            "            if ret[\"jid\"] == \"req\":",
            "                ret[\"jid\"] = salt.utils.jid.gen_jid(self.opts)",
            "            salt.utils.minion.cache_jobs(self.opts, ret[\"jid\"], ret)",
            "",
            "        if not self.opts[\"pub_ret\"]:",
            "            return \"\"",
            "",
            "        def timeout_handler(*_):",
            "            log.warning(",
            "                \"The minion failed to return the job information for job %s. \"",
            "                \"This is often due to the master being shut down or \"",
            "                \"overloaded. If the master is running, consider increasing \"",
            "                \"the worker_threads value.\",",
            "                jid,",
            "            )",
            "            return True",
            "",
            "        if sync:",
            "            try:",
            "                ret_val = self._send_req_sync(load, timeout=timeout)",
            "            except SaltReqTimeoutError:",
            "                timeout_handler()",
            "                return \"\"",
            "        else:",
            "            with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "                # pylint: disable=unexpected-keyword-arg",
            "                ret_val = self._send_req_async(",
            "                    load, timeout=timeout, callback=lambda f: None",
            "                )",
            "                # pylint: enable=unexpected-keyword-arg",
            "",
            "        log.trace(\"ret_val = %s\", ret_val)  # pylint: disable=no-member",
            "        return ret_val",
            "",
            "    def _return_pub_multi(self, rets, ret_cmd=\"_return\", timeout=60, sync=True):",
            "        \"\"\"",
            "        Return the data from the executed command to the master server",
            "        \"\"\"",
            "        if not isinstance(rets, list):",
            "            rets = [rets]",
            "        jids = {}",
            "        for ret in rets:",
            "            jid = ret.get(\"jid\", ret.get(\"__jid__\"))",
            "            fun = ret.get(\"fun\", ret.get(\"__fun__\"))",
            "            if self.opts[\"multiprocessing\"]:",
            "                fn_ = os.path.join(self.proc_dir, jid)",
            "                if os.path.isfile(fn_):",
            "                    try:",
            "                        os.remove(fn_)",
            "                    except OSError:",
            "                        # The file is gone already",
            "                        pass",
            "            log.info(\"Returning information for job: %s\", jid)",
            "            load = jids.setdefault(jid, {})",
            "            if ret_cmd == \"_syndic_return\":",
            "                if not load:",
            "                    load.update(",
            "                        {",
            "                            \"id\": self.opts[\"id\"],",
            "                            \"jid\": jid,",
            "                            \"fun\": fun,",
            "                            \"arg\": ret.get(\"arg\"),",
            "                            \"tgt\": ret.get(\"tgt\"),",
            "                            \"tgt_type\": ret.get(\"tgt_type\"),",
            "                            \"load\": ret.get(\"__load__\"),",
            "                            \"return\": {},",
            "                        }",
            "                    )",
            "                if \"__master_id__\" in ret:",
            "                    load[\"master_id\"] = ret[\"__master_id__\"]",
            "                for key, value in ret.items():",
            "                    if key.startswith(\"__\"):",
            "                        continue",
            "                    load[\"return\"][key] = value",
            "            else:",
            "                load.update({\"id\": self.opts[\"id\"]})",
            "                for key, value in ret.items():",
            "                    load[key] = value",
            "",
            "            if \"out\" in ret:",
            "                if isinstance(ret[\"out\"], str):",
            "                    load[\"out\"] = ret[\"out\"]",
            "                else:",
            "                    log.error(\"Invalid outputter %s. This is likely a bug.\", ret[\"out\"])",
            "            else:",
            "                try:",
            "                    oput = self.functions[fun].__outputter__",
            "                except (KeyError, AttributeError, TypeError):",
            "                    pass",
            "                else:",
            "                    if isinstance(oput, str):",
            "                        load[\"out\"] = oput",
            "            if self.opts[\"cache_jobs\"]:",
            "                # Local job cache has been enabled",
            "                salt.utils.minion.cache_jobs(self.opts, load[\"jid\"], ret)",
            "",
            "        load = {\"cmd\": ret_cmd, \"load\": list(jids.values())}",
            "",
            "        def timeout_handler(*_):",
            "            log.warning(",
            "                \"The minion failed to return the job information for job %s. \"",
            "                \"This is often due to the master being shut down or \"",
            "                \"overloaded. If the master is running, consider increasing \"",
            "                \"the worker_threads value.\",",
            "                jid,",
            "            )",
            "            return True",
            "",
            "        if sync:",
            "            try:",
            "                ret_val = self._send_req_sync(load, timeout=timeout)",
            "            except SaltReqTimeoutError:",
            "                timeout_handler()",
            "                return \"\"",
            "        else:",
            "            with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "                # pylint: disable=unexpected-keyword-arg",
            "                ret_val = self._send_req_async(",
            "                    load, timeout=timeout, callback=lambda f: None",
            "                )",
            "                # pylint: enable=unexpected-keyword-arg",
            "",
            "        log.trace(\"ret_val = %s\", ret_val)  # pylint: disable=no-member",
            "        return ret_val",
            "",
            "    def _state_run(self):",
            "        \"\"\"",
            "        Execute a state run based on information set in the minion config file",
            "        \"\"\"",
            "        if self.opts[\"startup_states\"]:",
            "            if (",
            "                self.opts.get(\"master_type\", \"str\") == \"disable\"",
            "                and self.opts.get(\"file_client\", \"remote\") == \"remote\"",
            "            ):",
            "                log.warning(",
            "                    \"Cannot run startup_states when 'master_type' is set \"",
            "                    \"to 'disable' and 'file_client' is set to \"",
            "                    \"'remote'. Skipping.\"",
            "                )",
            "            else:",
            "                data = {\"jid\": \"req\", \"ret\": self.opts.get(\"ext_job_cache\", \"\")}",
            "                if self.opts[\"startup_states\"] == \"sls\":",
            "                    data[\"fun\"] = \"state.sls\"",
            "                    data[\"arg\"] = [self.opts[\"sls_list\"]]",
            "                elif self.opts[\"startup_states\"] == \"top\":",
            "                    data[\"fun\"] = \"state.top\"",
            "                    data[\"arg\"] = [self.opts[\"top_file\"]]",
            "                else:",
            "                    data[\"fun\"] = \"state.highstate\"",
            "                    data[\"arg\"] = []",
            "                self._handle_decoded_payload(data)",
            "",
            "    def _refresh_grains_watcher(self, refresh_interval_in_minutes):",
            "        \"\"\"",
            "        Create a loop that will fire a pillar refresh to inform a master about a change in the grains of this minion",
            "        :param refresh_interval_in_minutes:",
            "        :return: None",
            "        \"\"\"",
            "        if \"__update_grains\" not in self.opts.get(\"schedule\", {}):",
            "            if \"schedule\" not in self.opts:",
            "                self.opts[\"schedule\"] = {}",
            "            self.opts[\"schedule\"].update(",
            "                {",
            "                    \"__update_grains\": {",
            "                        \"function\": \"event.fire\",",
            "                        \"args\": [{}, \"grains_refresh\"],",
            "                        \"minutes\": refresh_interval_in_minutes,",
            "                    }",
            "                }",
            "            )",
            "",
            "    def _fire_master_minion_start(self):",
            "        include_grains = False",
            "        if self.opts[\"start_event_grains\"]:",
            "            include_grains = True",
            "        # Send an event to the master that the minion is live",
            "        if self.opts[\"enable_legacy_startup_events\"]:",
            "            # Old style event. Defaults to False in 3001 release.",
            "            self._fire_master(",
            "                \"Minion {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "                \"minion_start\",",
            "                include_startup_grains=include_grains,",
            "            )",
            "        # send name spaced event",
            "        self._fire_master(",
            "            \"Minion {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "            tagify([self.opts[\"id\"], \"start\"], \"minion\"),",
            "            include_startup_grains=include_grains,",
            "        )",
            "",
            "    def module_refresh(self, force_refresh=False, notify=False):",
            "        \"\"\"",
            "        Refresh the functions and returners.",
            "        \"\"\"",
            "        log.debug(\"Refreshing modules. Notify=%s\", notify)",
            "        self.functions, self.returners, _, self.executors = self._load_modules(",
            "            force_refresh, notify=notify",
            "        )",
            "",
            "        self.schedule.functions = self.functions",
            "        self.schedule.returners = self.returners",
            "",
            "    def beacons_refresh(self):",
            "        \"\"\"",
            "        Refresh the functions and returners.",
            "        \"\"\"",
            "        if not self.beacons_leader:",
            "            return",
            "        log.debug(\"Refreshing beacons.\")",
            "        self.beacons = salt.beacons.Beacon(self.opts, self.functions)",
            "",
            "    def matchers_refresh(self):",
            "        \"\"\"",
            "        Refresh the matchers",
            "        \"\"\"",
            "        log.debug(\"Refreshing matchers.\")",
            "        self.matchers = salt.loader.matchers(self.opts)",
            "",
            "    # TODO: only allow one future in flight at a time?",
            "    @salt.ext.tornado.gen.coroutine",
            "    def pillar_refresh(self, force_refresh=False):",
            "        \"\"\"",
            "        Refresh the pillar",
            "        \"\"\"",
            "        self.module_refresh(force_refresh)",
            "",
            "        if self.connected:",
            "            log.debug(\"Refreshing pillar.\")",
            "            async_pillar = salt.pillar.get_async_pillar(",
            "                self.opts,",
            "                self.opts[\"grains\"],",
            "                self.opts[\"id\"],",
            "                self.opts[\"saltenv\"],",
            "                pillarenv=self.opts.get(\"pillarenv\"),",
            "            )",
            "            try:",
            "                self.opts[\"pillar\"] = yield async_pillar.compile_pillar()",
            "            except SaltClientError:",
            "                # Do not exit if a pillar refresh fails.",
            "                log.error(",
            "                    \"Pillar data could not be refreshed. \"",
            "                    \"One or more masters may be down!\"",
            "                )",
            "            finally:",
            "                async_pillar.destroy()",
            "        self.matchers_refresh()",
            "        self.beacons_refresh()",
            "        with salt.utils.event.get_event(\"minion\", opts=self.opts, listen=False) as evt:",
            "            evt.fire_event(",
            "                {\"complete\": True},",
            "                tag=salt.defaults.events.MINION_PILLAR_REFRESH_COMPLETE,",
            "            )",
            "",
            "    def manage_schedule(self, tag, data):",
            "        \"\"\"",
            "        Refresh the functions and returners.",
            "        \"\"\"",
            "        func = data.get(\"func\", None)",
            "        name = data.get(\"name\", None)",
            "        schedule = data.get(\"schedule\", None)",
            "        where = data.get(\"where\", None)",
            "        persist = data.get(\"persist\", None)",
            "",
            "        funcs = {",
            "            \"delete\": (\"delete_job\", (name, persist)),",
            "            \"add\": (\"add_job\", (schedule, persist)),",
            "            \"modify\": (\"modify_job\", (name, schedule, persist)),",
            "            \"enable\": (\"enable_schedule\", (persist,)),",
            "            \"disable\": (\"disable_schedule\", (persist,)),",
            "            \"enable_job\": (\"enable_job\", (name, persist)),",
            "            \"run_job\": (\"run_job\", (name,)),",
            "            \"disable_job\": (\"disable_job\", (name, persist)),",
            "            \"postpone_job\": (\"postpone_job\", (name, data)),",
            "            \"skip_job\": (\"skip_job\", (name, data)),",
            "            \"reload\": (\"reload\", (schedule,)),",
            "            \"list\": (\"list\", (where,)),",
            "            \"save_schedule\": (\"save_schedule\", ()),",
            "            \"get_next_fire_time\": (\"get_next_fire_time\", (name,)),",
            "        }",
            "",
            "        # Call the appropriate schedule function",
            "        try:",
            "            alias, params = funcs.get(func)",
            "            getattr(self.schedule, alias)(*params)",
            "        except TypeError:",
            "            log.error('Function \"%s\" is unavailable in salt.utils.scheduler', func)",
            "",
            "    def manage_beacons(self, tag, data):",
            "        \"\"\"",
            "        Manage Beacons",
            "        \"\"\"",
            "        if not self.beacons_leader:",
            "            return",
            "",
            "        func = data.get(\"func\", None)",
            "        name = data.get(\"name\", None)",
            "        beacon_data = data.get(\"beacon_data\", None)",
            "        include_pillar = data.get(\"include_pillar\", None)",
            "        include_opts = data.get(\"include_opts\", None)",
            "",
            "        funcs = {",
            "            \"add\": (\"add_beacon\", (name, beacon_data)),",
            "            \"modify\": (\"modify_beacon\", (name, beacon_data)),",
            "            \"delete\": (\"delete_beacon\", (name,)),",
            "            \"enable\": (\"enable_beacons\", ()),",
            "            \"disable\": (\"disable_beacons\", ()),",
            "            \"enable_beacon\": (\"enable_beacon\", (name,)),",
            "            \"disable_beacon\": (\"disable_beacon\", (name,)),",
            "            \"list\": (\"list_beacons\", (include_opts, include_pillar)),",
            "            \"list_available\": (\"list_available_beacons\", ()),",
            "            \"validate_beacon\": (\"validate_beacon\", (name, beacon_data)),",
            "            \"reset\": (\"reset\", ()),",
            "        }",
            "",
            "        # Call the appropriate beacon function",
            "        try:",
            "            alias, params = funcs.get(func)",
            "            getattr(self.beacons, alias)(*params)",
            "        except TypeError:",
            "            log.error('Function \"%s\" is unavailable in salt.utils.beacons', func)",
            "",
            "    def environ_setenv(self, tag, data):",
            "        \"\"\"",
            "        Set the salt-minion main process environment according to",
            "        the data contained in the minion event data",
            "        \"\"\"",
            "        environ = data.get(\"environ\", None)",
            "        if environ is None:",
            "            return False",
            "        false_unsets = data.get(\"false_unsets\", False)",
            "        clear_all = data.get(\"clear_all\", False)",
            "        import salt.modules.environ as mod_environ",
            "",
            "        return mod_environ.setenv(environ, false_unsets, clear_all)",
            "",
            "    def _pre_tune(self):",
            "        \"\"\"",
            "        Set the minion running flag and issue the appropriate warnings if",
            "        the minion cannot be started or is already running",
            "        \"\"\"",
            "        if self._running is None:",
            "            self._running = True",
            "        elif self._running is False:",
            "            log.error(",
            "                \"This %s was scheduled to stop. Not running %s.tune_in()\",",
            "                self.__class__.__name__,",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "        elif self._running is True:",
            "            log.error(",
            "                \"This %s is already running. Not running %s.tune_in()\",",
            "                self.__class__.__name__,",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "",
            "        try:",
            "            log.info(",
            "                \"%s is starting as user '%s'\",",
            "                self.__class__.__name__,",
            "                salt.utils.user.get_user(),",
            "            )",
            "        except Exception as err:  # pylint: disable=broad-except",
            "            # Only windows is allowed to fail here. See #3189. Log as debug in",
            "            # that case. Else, error.",
            "            log.log(",
            "                salt.utils.platform.is_windows() and logging.DEBUG or logging.ERROR,",
            "                \"Failed to get the user who is starting %s\",",
            "                self.__class__.__name__,",
            "                exc_info=err,",
            "            )",
            "",
            "    def _mine_send(self, tag, data):",
            "        \"\"\"",
            "        Send mine data to the master",
            "        \"\"\"",
            "        with salt.transport.client.ReqChannel.factory(self.opts) as channel:",
            "            data[\"tok\"] = self.tok",
            "            try:",
            "                ret = channel.send(data)",
            "                return ret",
            "            except SaltReqTimeoutError:",
            "                log.warning(\"Unable to send mine data to master.\")",
            "                return None",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_event(self, package):",
            "        \"\"\"",
            "        Handle an event from the epull_sock (all local minion events)",
            "        \"\"\"",
            "        if not self.ready:",
            "            raise salt.ext.tornado.gen.Return()",
            "        tag, data = salt.utils.event.SaltEvent.unpack(package)",
            "",
            "        if \"proxy_target\" in data and self.opts.get(\"metaproxy\") == \"deltaproxy\":",
            "            proxy_target = data[\"proxy_target\"]",
            "            _minion = self.deltaproxy_objs[proxy_target]",
            "        else:",
            "            _minion = self",
            "",
            "        log.debug(\"Minion of '%s' is handling event tag '%s'\", self.opts[\"master\"], tag)",
            "        if tag.startswith(\"module_refresh\"):",
            "            _minion.module_refresh(",
            "                force_refresh=data.get(\"force_refresh\", False),",
            "                notify=data.get(\"notify\", False),",
            "            )",
            "        elif tag.startswith(\"pillar_refresh\"):",
            "            yield _minion.pillar_refresh(force_refresh=data.get(\"force_refresh\", False))",
            "        elif tag.startswith(\"beacons_refresh\"):",
            "            _minion.beacons_refresh()",
            "        elif tag.startswith(\"matchers_refresh\"):",
            "            _minion.matchers_refresh()",
            "        elif tag.startswith(\"manage_schedule\"):",
            "            _minion.manage_schedule(tag, data)",
            "        elif tag.startswith(\"manage_beacons\"):",
            "            _minion.manage_beacons(tag, data)",
            "        elif tag.startswith(\"grains_refresh\"):",
            "            if (",
            "                data.get(\"force_refresh\", False)",
            "                or _minion.grains_cache != _minion.opts[\"grains\"]",
            "            ):",
            "                _minion.pillar_refresh(force_refresh=True)",
            "                _minion.grains_cache = _minion.opts[\"grains\"]",
            "        elif tag.startswith(\"environ_setenv\"):",
            "            self.environ_setenv(tag, data)",
            "        elif tag.startswith(\"_minion_mine\"):",
            "            self._mine_send(tag, data)",
            "        elif tag.startswith(\"fire_master\"):",
            "            if self.connected:",
            "                log.debug(\"Forwarding master event tag=%s\", data[\"tag\"])",
            "                self._fire_master(",
            "                    data[\"data\"],",
            "                    data[\"tag\"],",
            "                    data[\"events\"],",
            "                    data[\"pretag\"],",
            "                    sync=False,",
            "                )",
            "        elif tag.startswith(master_event(type=\"disconnected\")) or tag.startswith(",
            "            master_event(type=\"failback\")",
            "        ):",
            "            # if the master disconnect event is for a different master, raise an exception",
            "            if (",
            "                tag.startswith(master_event(type=\"disconnected\"))",
            "                and data[\"master\"] != self.opts[\"master\"]",
            "            ):",
            "                # not mine master, ignore",
            "                raise salt.ext.tornado.gen.Return()",
            "            if tag.startswith(master_event(type=\"failback\")):",
            "                # if the master failback event is not for the top master, raise an exception",
            "                if data[\"master\"] != self.opts[\"master_list\"][0]:",
            "                    raise SaltException(",
            "                        \"Bad master '{}' when mine failback is '{}'\".format(",
            "                            data[\"master\"], self.opts[\"master\"]",
            "                        )",
            "                    )",
            "                # if the master failback event is for the current master, raise an exception",
            "                elif data[\"master\"] == self.opts[\"master\"][0]:",
            "                    raise SaltException(",
            "                        \"Already connected to '{}'\".format(data[\"master\"])",
            "                    )",
            "",
            "            if self.connected:",
            "                # we are not connected anymore",
            "                self.connected = False",
            "                log.info(\"Connection to master %s lost\", self.opts[\"master\"])",
            "",
            "                if self.opts[\"master_type\"] != \"failover\":",
            "                    # modify the scheduled job to fire on reconnect",
            "                    if self.opts[\"transport\"] != \"tcp\":",
            "                        schedule = {",
            "                            \"function\": \"status.master\",",
            "                            \"seconds\": self.opts[\"master_alive_interval\"],",
            "                            \"jid_include\": True,",
            "                            \"maxrunning\": 1,",
            "                            \"return_job\": False,",
            "                            \"kwargs\": {",
            "                                \"master\": self.opts[\"master\"],",
            "                                \"connected\": False,",
            "                            },",
            "                        }",
            "                        self.schedule.modify_job(",
            "                            name=master_event(type=\"alive\", master=self.opts[\"master\"]),",
            "                            schedule=schedule,",
            "                        )",
            "                else:",
            "                    # delete the scheduled job to don't interfere with the failover process",
            "                    if self.opts[\"transport\"] != \"tcp\":",
            "                        self.schedule.delete_job(name=master_event(type=\"alive\"))",
            "",
            "                    log.info(\"Trying to tune in to next master from master-list\")",
            "",
            "                    if hasattr(self, \"pub_channel\"):",
            "                        self.pub_channel.on_recv(None)",
            "                        if hasattr(self.pub_channel, \"auth\"):",
            "                            self.pub_channel.auth.invalidate()",
            "                        if hasattr(self.pub_channel, \"close\"):",
            "                            self.pub_channel.close()",
            "                        del self.pub_channel",
            "",
            "                    # if eval_master finds a new master for us, self.connected",
            "                    # will be True again on successful master authentication",
            "                    try:",
            "                        master, self.pub_channel = yield self.eval_master(",
            "                            opts=self.opts,",
            "                            failed=True,",
            "                            failback=tag.startswith(master_event(type=\"failback\")),",
            "                        )",
            "                    except SaltClientError:",
            "                        pass",
            "",
            "                    if self.connected:",
            "                        self.opts[\"master\"] = master",
            "",
            "                        # re-init the subsystems to work with the new master",
            "                        log.info(",
            "                            \"Re-initialising subsystems for new master %s\",",
            "                            self.opts[\"master\"],",
            "                        )",
            "                        # put the current schedule into the new loaders",
            "                        self.opts[\"schedule\"] = self.schedule.option(\"schedule\")",
            "                        (",
            "                            self.functions,",
            "                            self.returners,",
            "                            self.function_errors,",
            "                            self.executors,",
            "                        ) = self._load_modules()",
            "                        # make the schedule to use the new 'functions' loader",
            "                        self.schedule.functions = self.functions",
            "                        self.pub_channel.on_recv(self._handle_payload)",
            "                        self._fire_master_minion_start()",
            "                        log.info(\"Minion is ready to receive requests!\")",
            "",
            "                        # update scheduled job to run with the new master addr",
            "                        if self.opts[\"transport\"] != \"tcp\":",
            "                            schedule = {",
            "                                \"function\": \"status.master\",",
            "                                \"seconds\": self.opts[\"master_alive_interval\"],",
            "                                \"jid_include\": True,",
            "                                \"maxrunning\": 1,",
            "                                \"return_job\": False,",
            "                                \"kwargs\": {",
            "                                    \"master\": self.opts[\"master\"],",
            "                                    \"connected\": True,",
            "                                },",
            "                            }",
            "                            self.schedule.modify_job(",
            "                                name=master_event(",
            "                                    type=\"alive\", master=self.opts[\"master\"]",
            "                                ),",
            "                                schedule=schedule,",
            "                            )",
            "",
            "                            if (",
            "                                self.opts[\"master_failback\"]",
            "                                and \"master_list\" in self.opts",
            "                            ):",
            "                                if self.opts[\"master\"] != self.opts[\"master_list\"][0]:",
            "                                    schedule = {",
            "                                        \"function\": \"status.ping_master\",",
            "                                        \"seconds\": self.opts[",
            "                                            \"master_failback_interval\"",
            "                                        ],",
            "                                        \"jid_include\": True,",
            "                                        \"maxrunning\": 1,",
            "                                        \"return_job\": False,",
            "                                        \"kwargs\": {",
            "                                            \"master\": self.opts[\"master_list\"][0]",
            "                                        },",
            "                                    }",
            "                                    self.schedule.modify_job(",
            "                                        name=master_event(type=\"failback\"),",
            "                                        schedule=schedule,",
            "                                    )",
            "                                else:",
            "                                    self.schedule.delete_job(",
            "                                        name=master_event(type=\"failback\"), persist=True",
            "                                    )",
            "                    else:",
            "                        self.restart = True",
            "                        self.io_loop.stop()",
            "",
            "        elif tag.startswith(master_event(type=\"connected\")):",
            "            # handle this event only once. otherwise it will pollute the log",
            "            # also if master type is failover all the reconnection work is done",
            "            # by `disconnected` event handler and this event must never happen,",
            "            # anyway check it to be sure",
            "            if not self.connected and self.opts[\"master_type\"] != \"failover\":",
            "                log.info(\"Connection to master %s re-established\", self.opts[\"master\"])",
            "                self.connected = True",
            "                # modify the __master_alive job to only fire,",
            "                # if the connection is lost again",
            "                if self.opts[\"transport\"] != \"tcp\":",
            "                    schedule = {",
            "                        \"function\": \"status.master\",",
            "                        \"seconds\": self.opts[\"master_alive_interval\"],",
            "                        \"jid_include\": True,",
            "                        \"maxrunning\": 1,",
            "                        \"return_job\": False,",
            "                        \"kwargs\": {\"master\": self.opts[\"master\"], \"connected\": True},",
            "                    }",
            "",
            "                    self.schedule.modify_job(",
            "                        name=master_event(type=\"alive\", master=self.opts[\"master\"]),",
            "                        schedule=schedule,",
            "                    )",
            "        elif tag.startswith(\"__schedule_return\"):",
            "            # reporting current connection with master",
            "            if data[\"schedule\"].startswith(master_event(type=\"alive\", master=\"\")):",
            "                if data[\"return\"]:",
            "                    log.debug(",
            "                        \"Connected to master %s\",",
            "                        data[\"schedule\"].split(master_event(type=\"alive\", master=\"\"))[",
            "                            1",
            "                        ],",
            "                    )",
            "            self._return_pub(data, ret_cmd=\"_return\", sync=False)",
            "        elif tag.startswith(\"_salt_error\"):",
            "            if self.connected:",
            "                log.debug(\"Forwarding salt error event tag=%s\", tag)",
            "                self._fire_master(data, tag, sync=False)",
            "        elif tag.startswith(\"salt/auth/creds\"):",
            "            key = tuple(data[\"key\"])",
            "            log.debug(",
            "                \"Updating auth data for %s: %s -> %s\",",
            "                key,",
            "                salt.crypt.AsyncAuth.creds_map.get(key),",
            "                data[\"creds\"],",
            "            )",
            "            salt.crypt.AsyncAuth.creds_map[tuple(data[\"key\"])] = data[\"creds\"]",
            "        elif tag.startswith(\"__beacons_return\"):",
            "            if self.connected:",
            "                log.debug(\"Firing beacons to master\")",
            "                self._fire_master(events=data[\"beacons\"])",
            "",
            "    def cleanup_subprocesses(self):",
            "        \"\"\"",
            "        Clean up subprocesses and spawned threads.",
            "        \"\"\"",
            "        # Add an extra fallback in case a forked process leaks through",
            "        multiprocessing.active_children()",
            "        self.subprocess_list.cleanup()",
            "        if self.schedule:",
            "            self.schedule.cleanup_subprocesses()",
            "",
            "    def _setup_core(self):",
            "        \"\"\"",
            "        Set up the core minion attributes.",
            "        This is safe to call multiple times.",
            "        \"\"\"",
            "        if not self.ready:",
            "            # First call. Initialize.",
            "            (",
            "                self.functions,",
            "                self.returners,",
            "                self.function_errors,",
            "                self.executors,",
            "            ) = self._load_modules()",
            "            self.serial = salt.payload.Serial(self.opts)",
            "            self.mod_opts = self._prep_mod_opts()",
            "            #            self.matcher = Matcher(self.opts, self.functions)",
            "            self.matchers = salt.loader.matchers(self.opts)",
            "            if self.beacons_leader:",
            "                self.beacons = salt.beacons.Beacon(self.opts, self.functions)",
            "            uid = salt.utils.user.get_uid(user=self.opts.get(\"user\", None))",
            "            self.proc_dir = get_proc_dir(self.opts[\"cachedir\"], uid=uid)",
            "            self.grains_cache = self.opts[\"grains\"]",
            "            self.ready = True",
            "",
            "    def setup_beacons(self, before_connect=False):",
            "        \"\"\"",
            "        Set up the beacons.",
            "        This is safe to call multiple times.",
            "        \"\"\"",
            "        # In multimaster configuration the only one minion shall execute beacons",
            "        if not self.beacons_leader:",
            "            return",
            "",
            "        self._setup_core()",
            "        loop_interval = self.opts[\"loop_interval\"]",
            "        if \"beacons\" not in self.periodic_callbacks:",
            "            self.beacons = salt.beacons.Beacon(self.opts, self.functions)",
            "",
            "            def handle_beacons():",
            "                # Process Beacons",
            "                beacons = None",
            "                try:",
            "                    beacons = self.process_beacons(self.functions)",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.critical(\"The beacon errored: \", exc_info=True)",
            "                if beacons:",
            "                    event = salt.utils.event.get_event(",
            "                        \"minion\", opts=self.opts, listen=False",
            "                    )",
            "                    event.fire_event({\"beacons\": beacons}, \"__beacons_return\")",
            "                    event.destroy()",
            "",
            "            if before_connect:",
            "                # Make sure there is a chance for one iteration to occur before connect",
            "                handle_beacons()",
            "",
            "            self.add_periodic_callback(\"beacons\", handle_beacons)",
            "",
            "    def setup_scheduler(self, before_connect=False):",
            "        \"\"\"",
            "        Set up the scheduler.",
            "        This is safe to call multiple times.",
            "        \"\"\"",
            "        self._setup_core()",
            "",
            "        loop_interval = self.opts[\"loop_interval\"]",
            "",
            "        if \"schedule\" not in self.periodic_callbacks:",
            "            if \"schedule\" not in self.opts:",
            "                self.opts[\"schedule\"] = {}",
            "            if not hasattr(self, \"schedule\"):",
            "                self.schedule = salt.utils.schedule.Schedule(",
            "                    self.opts,",
            "                    self.functions,",
            "                    self.returners,",
            "                    utils=self.utils,",
            "                    cleanup=[master_event(type=\"alive\")],",
            "                )",
            "",
            "            try:",
            "                if self.opts[\"grains_refresh_every\"]:  # In minutes, not seconds!",
            "                    log.debug(",
            "                        \"Enabling the grains refresher. Will run every %d minute(s).\",",
            "                        self.opts[\"grains_refresh_every\"],",
            "                    )",
            "                    self._refresh_grains_watcher(abs(self.opts[\"grains_refresh_every\"]))",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                log.error(",
            "                    \"Exception occurred in attempt to initialize grain refresh \"",
            "                    \"routine during minion tune-in: %s\",",
            "                    exc,",
            "                )",
            "",
            "            # TODO: actually listen to the return and change period",
            "            def handle_schedule():",
            "                self.process_schedule(self, loop_interval)",
            "",
            "            if before_connect:",
            "                # Make sure there is a chance for one iteration to occur before connect",
            "                handle_schedule()",
            "",
            "            self.add_periodic_callback(\"schedule\", handle_schedule)",
            "",
            "    def add_periodic_callback(self, name, method, interval=1):",
            "        \"\"\"",
            "        Add a periodic callback to the event loop and call its start method.",
            "        If a callback by the given name exists this method returns False",
            "        \"\"\"",
            "        if name in self.periodic_callbacks:",
            "            return False",
            "        self.periodic_callbacks[name] = salt.ext.tornado.ioloop.PeriodicCallback(",
            "            method, interval * 1000,",
            "        )",
            "        self.periodic_callbacks[name].start()",
            "        return True",
            "",
            "    def remove_periodic_callback(self, name):",
            "        \"\"\"",
            "        Remove a periodic callback.",
            "        If a callback by the given name does not exist this method returns False",
            "        \"\"\"",
            "        callback = self.periodic_callbacks.pop(name, None)",
            "        if callback is None:",
            "            return False",
            "        callback.stop()",
            "        return True",
            "",
            "    # Main Minion Tune In",
            "    def tune_in(self, start=True):",
            "        \"\"\"",
            "        Lock onto the publisher. This is the main event loop for the minion",
            "        :rtype : None",
            "        \"\"\"",
            "        self._pre_tune()",
            "",
            "        log.debug(\"Minion '%s' trying to tune in\", self.opts[\"id\"])",
            "",
            "        if start:",
            "            if self.opts.get(\"beacons_before_connect\", False):",
            "                self.setup_beacons(before_connect=True)",
            "            if self.opts.get(\"scheduler_before_connect\", False):",
            "                self.setup_scheduler(before_connect=True)",
            "            self.sync_connect_master()",
            "        if self.connected:",
            "            self._fire_master_minion_start()",
            "            log.info(\"Minion is ready to receive requests!\")",
            "",
            "        # Make sure to gracefully handle SIGUSR1",
            "        enable_sigusr1_handler()",
            "",
            "        # Make sure to gracefully handle CTRL_LOGOFF_EVENT",
            "        if HAS_WIN_FUNCTIONS:",
            "            salt.utils.win_functions.enable_ctrl_logoff_handler()",
            "",
            "        # On first startup execute a state run if configured to do so",
            "        self._state_run()",
            "",
            "        self.setup_beacons()",
            "        self.setup_scheduler()",
            "        self.add_periodic_callback(\"cleanup\", self.cleanup_subprocesses)",
            "",
            "        # schedule the stuff that runs every interval",
            "        ping_interval = self.opts.get(\"ping_interval\", 0) * 60",
            "        if ping_interval > 0 and self.connected:",
            "",
            "            def ping_master():",
            "                try:",
            "",
            "                    def ping_timeout_handler(*_):",
            "                        if self.opts.get(\"auth_safemode\", False):",
            "                            log.error(",
            "                                \"** Master Ping failed. Attempting to restart minion**\"",
            "                            )",
            "                            delay = self.opts.get(\"random_reauth_delay\", 5)",
            "                            log.info(\"delaying random_reauth_delay %ss\", delay)",
            "                            try:",
            "                                self.functions[\"service.restart\"](service_name())",
            "                            except KeyError:",
            "                                # Probably no init system (running in docker?)",
            "                                log.warning(",
            "                                    \"ping_interval reached without response \"",
            "                                    \"from the master, but service.restart \"",
            "                                    \"could not be run to restart the minion \"",
            "                                    \"daemon. ping_interval requires that the \"",
            "                                    \"minion is running under an init system.\"",
            "                                )",
            "",
            "                    self._fire_master(",
            "                        \"ping\",",
            "                        \"minion_ping\",",
            "                        sync=False,",
            "                        timeout_handler=ping_timeout_handler,",
            "                    )",
            "                except Exception:  # pylint: disable=broad-except",
            "                    log.warning(",
            "                        \"Attempt to ping master failed.\", exc_on_loglevel=logging.DEBUG",
            "                    )",
            "",
            "            self.remove_periodic_callback(\"ping\")",
            "            self.add_periodic_callback(\"ping\", ping_master, ping_interval)",
            "",
            "        # add handler to subscriber",
            "        if hasattr(self, \"pub_channel\") and self.pub_channel is not None:",
            "            self.pub_channel.on_recv(self._handle_payload)",
            "        elif self.opts.get(\"master_type\") != \"disable\":",
            "            log.error(\"No connection to master found. Scheduled jobs will not run.\")",
            "",
            "        if start:",
            "            try:",
            "                self.io_loop.start()",
            "                if self.restart:",
            "                    self.destroy()",
            "            except (",
            "                KeyboardInterrupt,",
            "                RuntimeError,",
            "            ):  # A RuntimeError can be re-raised by Tornado on shutdown",
            "                self.destroy()",
            "",
            "    def _handle_payload(self, payload):",
            "        if payload is not None and payload[\"enc\"] == \"aes\":",
            "            if self._target_load(payload[\"load\"]):",
            "                self._handle_decoded_payload(payload[\"load\"])",
            "            elif self.opts[\"zmq_filtering\"]:",
            "                # In the filtering enabled case, we'd like to know when minion sees something it shouldnt",
            "                log.trace(",
            "                    \"Broadcast message received not for this minion, Load: %s\",",
            "                    payload[\"load\"],",
            "                )",
            "        # If it's not AES, and thus has not been verified, we do nothing.",
            "        # In the future, we could add support for some clearfuncs, but",
            "        # the minion currently has no need.",
            "",
            "    def _target_load(self, load):",
            "        # Verify that the publication is valid",
            "        if (",
            "            \"tgt\" not in load",
            "            or \"jid\" not in load",
            "            or \"fun\" not in load",
            "            or \"arg\" not in load",
            "        ):",
            "            return False",
            "        # Verify that the publication applies to this minion",
            "",
            "        # It's important to note that the master does some pre-processing",
            "        # to determine which minions to send a request to. So for example,",
            "        # a \"salt -G 'grain_key:grain_val' test.ping\" will invoke some",
            "        # pre-processing on the master and this minion should not see the",
            "        # publication if the master does not determine that it should.",
            "",
            "        if \"tgt_type\" in load:",
            "            match_func = self.matchers.get(",
            "                \"{}_match.match\".format(load[\"tgt_type\"]), None",
            "            )",
            "            if match_func is None:",
            "                return False",
            "            if load[\"tgt_type\"] in (\"grain\", \"grain_pcre\", \"pillar\"):",
            "                delimiter = load.get(\"delimiter\", DEFAULT_TARGET_DELIM)",
            "                if not match_func(load[\"tgt\"], delimiter=delimiter):",
            "                    return False",
            "            elif not match_func(load[\"tgt\"]):",
            "                return False",
            "        else:",
            "            if not self.matchers[\"glob_match.match\"](load[\"tgt\"]):",
            "                return False",
            "",
            "        return True",
            "",
            "    def destroy(self):",
            "        \"\"\"",
            "        Tear down the minion",
            "        \"\"\"",
            "        if self._running is False:",
            "            return",
            "",
            "        self._running = False",
            "        if hasattr(self, \"schedule\"):",
            "            del self.schedule",
            "        if hasattr(self, \"pub_channel\") and self.pub_channel is not None:",
            "            self.pub_channel.on_recv(None)",
            "            if hasattr(self.pub_channel, \"close\"):",
            "                self.pub_channel.close()",
            "            del self.pub_channel",
            "        if hasattr(self, \"periodic_callbacks\"):",
            "            for cb in self.periodic_callbacks.values():",
            "                cb.stop()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.destroy()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "class Syndic(Minion):",
            "    \"\"\"",
            "    Make a Syndic minion, this minion will use the minion keys on the",
            "    master to authenticate with a higher level master.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        self._syndic_interface = opts.get(\"interface\")",
            "        self._syndic = True",
            "        # force auth_safemode True because Syndic don't support autorestart",
            "        opts[\"auth_safemode\"] = True",
            "        opts[\"loop_interval\"] = 1",
            "        super().__init__(opts, **kwargs)",
            "        self.mminion = salt.minion.MasterMinion(opts)",
            "        self.jid_forward_cache = set()",
            "        self.jids = {}",
            "        self.raw_events = []",
            "        self.pub_future = None",
            "",
            "    def _handle_decoded_payload(self, data):",
            "        \"\"\"",
            "        Override this method if you wish to handle the decoded data",
            "        differently.",
            "        \"\"\"",
            "        # TODO: even do this??",
            "        data[\"to\"] = int(data.get(\"to\", self.opts[\"timeout\"])) - 1",
            "        # Only forward the command if it didn't originate from ourselves",
            "        if data.get(\"master_id\", 0) != self.opts.get(\"master_id\", 1):",
            "            self.syndic_cmd(data)",
            "",
            "    def syndic_cmd(self, data):",
            "        \"\"\"",
            "        Take the now clear load and forward it on to the client cmd",
            "        \"\"\"",
            "        # Set up default tgt_type",
            "        if \"tgt_type\" not in data:",
            "            data[\"tgt_type\"] = \"glob\"",
            "        kwargs = {}",
            "",
            "        # optionally add a few fields to the publish data",
            "        for field in (",
            "            \"master_id\",  # which master the job came from",
            "            \"user\",  # which user ran the job",
            "        ):",
            "            if field in data:",
            "                kwargs[field] = data[field]",
            "",
            "        def timeout_handler(*args):",
            "            log.warning(\"Unable to forward pub data: %s\", args[1])",
            "            return True",
            "",
            "        with salt.ext.tornado.stack_context.ExceptionStackContext(timeout_handler):",
            "            self.local.pub_async(",
            "                data[\"tgt\"],",
            "                data[\"fun\"],",
            "                data[\"arg\"],",
            "                data[\"tgt_type\"],",
            "                data[\"ret\"],",
            "                data[\"jid\"],",
            "                data[\"to\"],",
            "                io_loop=self.io_loop,",
            "                callback=lambda _: None,",
            "                **kwargs",
            "            )",
            "",
            "    def fire_master_syndic_start(self):",
            "        # Send an event to the master that the minion is live",
            "        if self.opts[\"enable_legacy_startup_events\"]:",
            "            # Old style event. Defaults to false in 3001 release.",
            "            self._fire_master(",
            "                \"Syndic {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "                \"syndic_start\",",
            "                sync=False,",
            "            )",
            "        self._fire_master(",
            "            \"Syndic {} started at {}\".format(self.opts[\"id\"], time.asctime()),",
            "            tagify([self.opts[\"id\"], \"start\"], \"syndic\"),",
            "            sync=False,",
            "        )",
            "",
            "    # TODO: clean up docs",
            "    def tune_in_no_block(self):",
            "        \"\"\"",
            "        Executes the tune_in sequence but omits extra logging and the",
            "        management of the event bus assuming that these are handled outside",
            "        the tune_in sequence",
            "        \"\"\"",
            "        # Instantiate the local client",
            "        self.local = salt.client.get_local_client(",
            "            self.opts[\"_minion_conf_file\"], io_loop=self.io_loop",
            "        )",
            "",
            "        # add handler to subscriber",
            "        self.pub_channel.on_recv(self._process_cmd_socket)",
            "",
            "    def _process_cmd_socket(self, payload):",
            "        if payload is not None and payload[\"enc\"] == \"aes\":",
            "            log.trace(\"Handling payload\")",
            "            self._handle_decoded_payload(payload[\"load\"])",
            "        # If it's not AES, and thus has not been verified, we do nothing.",
            "        # In the future, we could add support for some clearfuncs, but",
            "        # the syndic currently has no need.",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def reconnect(self):",
            "        if hasattr(self, \"pub_channel\"):",
            "            self.pub_channel.on_recv(None)",
            "            if hasattr(self.pub_channel, \"close\"):",
            "                self.pub_channel.close()",
            "            del self.pub_channel",
            "",
            "        # if eval_master finds a new master for us, self.connected",
            "        # will be True again on successful master authentication",
            "        master, self.pub_channel = yield self.eval_master(opts=self.opts)",
            "",
            "        if self.connected:",
            "            self.opts[\"master\"] = master",
            "            self.pub_channel.on_recv(self._process_cmd_socket)",
            "            log.info(\"Minion is ready to receive requests!\")",
            "",
            "        raise salt.ext.tornado.gen.Return(self)",
            "",
            "    def destroy(self):",
            "        \"\"\"",
            "        Tear down the syndic minion",
            "        \"\"\"",
            "        # We borrowed the local clients poller so give it back before",
            "        # it's destroyed. Reset the local poller reference.",
            "        super().destroy()",
            "        if hasattr(self, \"local\"):",
            "            del self.local",
            "",
            "        if hasattr(self, \"forward_events\"):",
            "            self.forward_events.stop()",
            "",
            "",
            "# TODO: need a way of knowing if the syndic connection is busted",
            "class SyndicManager(MinionBase):",
            "    \"\"\"",
            "    Make a MultiMaster syndic minion, this minion will handle relaying jobs and returns from",
            "    all minions connected to it to the list of masters it is connected to.",
            "",
            "    Modes (controlled by `syndic_mode`:",
            "        sync: This mode will synchronize all events and publishes from higher level masters",
            "        cluster: This mode will only sync job publishes and returns",
            "",
            "    Note: jobs will be returned best-effort to the requesting master. This also means",
            "    (since we are using zmq) that if a job was fired and the master disconnects",
            "    between the publish and return, that the return will end up in a zmq buffer",
            "    in this Syndic headed to that original master.",
            "",
            "    In addition, since these classes all seem to use a mix of blocking and non-blocking",
            "    calls (with varying timeouts along the way) this daemon does not handle failure well,",
            "    it will (under most circumstances) stall the daemon for ~15s trying to forward events",
            "    to the down master",
            "    \"\"\"",
            "",
            "    # time to connect to upstream master",
            "    SYNDIC_CONNECT_TIMEOUT = 5",
            "    SYNDIC_EVENT_TIMEOUT = 5",
            "",
            "    def __init__(self, opts, io_loop=None):",
            "        opts[\"loop_interval\"] = 1",
            "        super().__init__(opts)",
            "        self.mminion = salt.minion.MasterMinion(opts)",
            "        # sync (old behavior), cluster (only returns and publishes)",
            "        self.syndic_mode = self.opts.get(\"syndic_mode\", \"sync\")",
            "        self.syndic_failover = self.opts.get(\"syndic_failover\", \"random\")",
            "",
            "        self.auth_wait = self.opts[\"acceptance_wait_time\"]",
            "        self.max_auth_wait = self.opts[\"acceptance_wait_time_max\"]",
            "",
            "        self._has_master = threading.Event()",
            "        self.jid_forward_cache = set()",
            "",
            "        if io_loop is None:",
            "            install_zmq()",
            "            self.io_loop = ZMQDefaultLoop.current()",
            "        else:",
            "            self.io_loop = io_loop",
            "",
            "        # List of events",
            "        self.raw_events = []",
            "        # Dict of rets: {master_id: {event_tag: job_ret, ...}, ...}",
            "        self.job_rets = {}",
            "        # List of delayed job_rets which was unable to send for some reason and will be resend to",
            "        # any available master",
            "        self.delayed = []",
            "        # Active pub futures: {master_id: (future, [job_ret, ...]), ...}",
            "        self.pub_futures = {}",
            "",
            "    def _spawn_syndics(self):",
            "        \"\"\"",
            "        Spawn all the coroutines which will sign in the syndics",
            "        \"\"\"",
            "        self._syndics = OrderedDict()  # mapping of opts['master'] -> syndic",
            "        masters = self.opts[\"master\"]",
            "        if not isinstance(masters, list):",
            "            masters = [masters]",
            "        for master in masters:",
            "            s_opts = copy.copy(self.opts)",
            "            s_opts[\"master\"] = master",
            "            self._syndics[master] = self._connect_syndic(s_opts)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _connect_syndic(self, opts):",
            "        \"\"\"",
            "        Create a syndic, and asynchronously connect it to a master",
            "        \"\"\"",
            "        last = 0  # never have we signed in",
            "        auth_wait = opts[\"acceptance_wait_time\"]",
            "        failed = False",
            "        while True:",
            "            log.debug(\"Syndic attempting to connect to %s\", opts[\"master\"])",
            "            try:",
            "                syndic = Syndic(",
            "                    opts,",
            "                    timeout=self.SYNDIC_CONNECT_TIMEOUT,",
            "                    safe=False,",
            "                    io_loop=self.io_loop,",
            "                )",
            "                yield syndic.connect_master(failed=failed)",
            "                # set up the syndic to handle publishes (specifically not event forwarding)",
            "                syndic.tune_in_no_block()",
            "",
            "                # Send an event to the master that the minion is live",
            "                syndic.fire_master_syndic_start()",
            "",
            "                log.info(\"Syndic successfully connected to %s\", opts[\"master\"])",
            "                break",
            "            except SaltClientError as exc:",
            "                failed = True",
            "                log.error(",
            "                    \"Error while bringing up syndic for multi-syndic. Is the \"",
            "                    \"master at %s responding?\",",
            "                    opts[\"master\"],",
            "                )",
            "                last = time.time()",
            "                if auth_wait < self.max_auth_wait:",
            "                    auth_wait += self.auth_wait",
            "                yield salt.ext.tornado.gen.sleep(auth_wait)  # TODO: log?",
            "            except (KeyboardInterrupt, SystemExit):  # pylint: disable=try-except-raise",
            "                raise",
            "            except Exception:  # pylint: disable=broad-except",
            "                failed = True",
            "                log.critical(",
            "                    \"Unexpected error while connecting to %s\",",
            "                    opts[\"master\"],",
            "                    exc_info=True,",
            "                )",
            "",
            "        raise salt.ext.tornado.gen.Return(syndic)",
            "",
            "    def _mark_master_dead(self, master):",
            "        \"\"\"",
            "        Mark a master as dead. This will start the sign-in routine",
            "        \"\"\"",
            "        # if its connected, mark it dead",
            "        if self._syndics[master].done():",
            "            syndic = self._syndics[master].result()  # pylint: disable=no-member",
            "            self._syndics[master] = syndic.reconnect()",
            "        else:",
            "            # TODO: debug?",
            "            log.info(",
            "                \"Attempting to mark %s as dead, although it is already \" \"marked dead\",",
            "                master,",
            "            )",
            "",
            "    def _call_syndic(self, func, args=(), kwargs=None, master_id=None):",
            "        \"\"\"",
            "        Wrapper to call a given func on a syndic, best effort to get the one you asked for",
            "        \"\"\"",
            "        if kwargs is None:",
            "            kwargs = {}",
            "        successful = False",
            "        # Call for each master",
            "        for master, syndic_future in self.iter_master_options(master_id):",
            "            if not syndic_future.done() or syndic_future.exception():",
            "                log.error(",
            "                    \"Unable to call %s on %s, that syndic is not connected\",",
            "                    func,",
            "                    master,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                getattr(syndic_future.result(), func)(*args, **kwargs)",
            "                successful = True",
            "            except SaltClientError:",
            "                log.error(\"Unable to call %s on %s, trying another...\", func, master)",
            "                self._mark_master_dead(master)",
            "        if not successful:",
            "            log.critical(\"Unable to call %s on any masters!\", func)",
            "",
            "    def _return_pub_syndic(self, values, master_id=None):",
            "        \"\"\"",
            "        Wrapper to call the '_return_pub_multi' a syndic, best effort to get the one you asked for",
            "        \"\"\"",
            "        func = \"_return_pub_multi\"",
            "        for master, syndic_future in self.iter_master_options(master_id):",
            "            if not syndic_future.done() or syndic_future.exception():",
            "                log.error(",
            "                    \"Unable to call %s on %s, that syndic is not connected\",",
            "                    func,",
            "                    master,",
            "                )",
            "                continue",
            "",
            "            future, data = self.pub_futures.get(master, (None, None))",
            "            if future is not None:",
            "                if not future.done():",
            "                    if master == master_id:",
            "                        # Targeted master previous send not done yet, call again later",
            "                        return False",
            "                    else:",
            "                        # Fallback master is busy, try the next one",
            "                        continue",
            "                elif future.exception():",
            "                    # Previous execution on this master returned an error",
            "                    log.error(",
            "                        \"Unable to call %s on %s, trying another...\", func, master",
            "                    )",
            "                    self._mark_master_dead(master)",
            "                    del self.pub_futures[master]",
            "                    # Add not sent data to the delayed list and try the next master",
            "                    self.delayed.extend(data)",
            "                    continue",
            "            future = getattr(syndic_future.result(), func)(",
            "                values, \"_syndic_return\", timeout=self._return_retry_timer(), sync=False",
            "            )",
            "            self.pub_futures[master] = (future, values)",
            "            return True",
            "        # Loop done and didn't exit: wasn't sent, try again later",
            "        return False",
            "",
            "    def iter_master_options(self, master_id=None):",
            "        \"\"\"",
            "        Iterate (in order) over your options for master",
            "        \"\"\"",
            "        masters = list(self._syndics.keys())",
            "        if self.opts[\"syndic_failover\"] == \"random\":",
            "            shuffle(masters)",
            "        if master_id not in self._syndics:",
            "            master_id = masters.pop(0)",
            "        else:",
            "            masters.remove(master_id)",
            "",
            "        while True:",
            "            yield master_id, self._syndics[master_id]",
            "            if not masters:",
            "                break",
            "            master_id = masters.pop(0)",
            "",
            "    def _reset_event_aggregation(self):",
            "        self.job_rets = {}",
            "        self.raw_events = []",
            "",
            "    def reconnect_event_bus(self, something):",
            "        future = self.local.event.set_event_handler(self._process_event)",
            "        self.io_loop.add_future(future, self.reconnect_event_bus)",
            "",
            "    # Syndic Tune In",
            "    def tune_in(self):",
            "        \"\"\"",
            "        Lock onto the publisher. This is the main event loop for the syndic",
            "        \"\"\"",
            "        self._spawn_syndics()",
            "        # Instantiate the local client",
            "        self.local = salt.client.get_local_client(",
            "            self.opts[\"_minion_conf_file\"], io_loop=self.io_loop",
            "        )",
            "        self.local.event.subscribe(\"\")",
            "",
            "        log.debug(\"SyndicManager '%s' trying to tune in\", self.opts[\"id\"])",
            "",
            "        # register the event sub to the poller",
            "        self.job_rets = {}",
            "        self.raw_events = []",
            "        self._reset_event_aggregation()",
            "        future = self.local.event.set_event_handler(self._process_event)",
            "        self.io_loop.add_future(future, self.reconnect_event_bus)",
            "",
            "        # forward events every syndic_event_forward_timeout",
            "        self.forward_events = salt.ext.tornado.ioloop.PeriodicCallback(",
            "            self._forward_events, self.opts[\"syndic_event_forward_timeout\"] * 1000,",
            "        )",
            "        self.forward_events.start()",
            "",
            "        # Make sure to gracefully handle SIGUSR1",
            "        enable_sigusr1_handler()",
            "",
            "        self.io_loop.start()",
            "",
            "    def _process_event(self, raw):",
            "        # TODO: cleanup: Move down into event class",
            "        mtag, data = self.local.event.unpack(raw, self.local.event.serial)",
            "        log.trace(\"Got event %s\", mtag)  # pylint: disable=no-member",
            "",
            "        tag_parts = mtag.split(\"/\")",
            "        if (",
            "            len(tag_parts) >= 4",
            "            and tag_parts[1] == \"job\"",
            "            and salt.utils.jid.is_jid(tag_parts[2])",
            "            and tag_parts[3] == \"ret\"",
            "            and \"return\" in data",
            "        ):",
            "            if \"jid\" not in data:",
            "                # Not a job return",
            "                return",
            "            if self.syndic_mode == \"cluster\" and data.get(",
            "                \"master_id\", 0",
            "            ) == self.opts.get(\"master_id\", 1):",
            "                log.debug(\"Return received with matching master_id, not forwarding\")",
            "                return",
            "",
            "            master = data.get(\"master_id\")",
            "            jdict = self.job_rets.setdefault(master, {}).setdefault(mtag, {})",
            "            if not jdict:",
            "                jdict[\"__fun__\"] = data.get(\"fun\")",
            "                jdict[\"__jid__\"] = data[\"jid\"]",
            "                jdict[\"__load__\"] = {}",
            "                fstr = \"{}.get_load\".format(self.opts[\"master_job_cache\"])",
            "                # Only need to forward each load once. Don't hit the disk",
            "                # for every minion return!",
            "                if data[\"jid\"] not in self.jid_forward_cache:",
            "                    jdict[\"__load__\"].update(self.mminion.returners[fstr](data[\"jid\"]))",
            "                    self.jid_forward_cache.add(data[\"jid\"])",
            "                    if (",
            "                        len(self.jid_forward_cache)",
            "                        > self.opts[\"syndic_jid_forward_cache_hwm\"]",
            "                    ):",
            "                        # Pop the oldest jid from the cache",
            "                        tmp = sorted(list(self.jid_forward_cache))",
            "                        tmp.pop(0)",
            "                        self.jid_forward_cache = set(tmp)",
            "            if master is not None:",
            "                # __'s to make sure it doesn't print out on the master cli",
            "                jdict[\"__master_id__\"] = master",
            "            ret = {}",
            "            for key in \"return\", \"retcode\", \"success\":",
            "                if key in data:",
            "                    ret[key] = data[key]",
            "            jdict[data[\"id\"]] = ret",
            "        else:",
            "            # TODO: config to forward these? If so we'll have to keep track of who",
            "            # has seen them",
            "            # if we are the top level masters-- don't forward all the minion events",
            "            if self.syndic_mode == \"sync\":",
            "                # Add generic event aggregation here",
            "                if \"retcode\" not in data:",
            "                    self.raw_events.append({\"data\": data, \"tag\": mtag})",
            "",
            "    def _forward_events(self):",
            "        log.trace(\"Forwarding events\")  # pylint: disable=no-member",
            "        if self.raw_events:",
            "            events = self.raw_events",
            "            self.raw_events = []",
            "            self._call_syndic(",
            "                \"_fire_master\",",
            "                kwargs={",
            "                    \"events\": events,",
            "                    \"pretag\": tagify(self.opts[\"id\"], base=\"syndic\"),",
            "                    \"timeout\": self._return_retry_timer(),",
            "                    \"sync\": False,",
            "                },",
            "            )",
            "        if self.delayed:",
            "            res = self._return_pub_syndic(self.delayed)",
            "            if res:",
            "                self.delayed = []",
            "        for master in list(self.job_rets.keys()):",
            "            values = list(self.job_rets[master].values())",
            "            res = self._return_pub_syndic(values, master_id=master)",
            "            if res:",
            "                del self.job_rets[master]",
            "",
            "",
            "class ProxyMinionManager(MinionManager):",
            "    \"\"\"",
            "    Create the multi-minion interface but for proxy minions",
            "    \"\"\"",
            "",
            "    def _create_minion_object(",
            "        self, opts, timeout, safe, io_loop=None, loaded_base_name=None, jid_queue=None",
            "    ):",
            "        \"\"\"",
            "        Helper function to return the correct type of object",
            "        \"\"\"",
            "        return ProxyMinion(",
            "            opts,",
            "            timeout,",
            "            safe,",
            "            io_loop=io_loop,",
            "            loaded_base_name=loaded_base_name,",
            "            jid_queue=jid_queue,",
            "        )",
            "",
            "",
            "def _metaproxy_call(opts, fn_name):",
            "    loaded_base_name = \"{}.{}\".format(opts[\"id\"], salt.loader.LOADED_BASE_NAME)",
            "    metaproxy = salt.loader.metaproxy(opts, loaded_base_name=loaded_base_name)",
            "    try:",
            "        metaproxy_name = opts[\"metaproxy\"]",
            "    except KeyError:",
            "        metaproxy_name = \"proxy\"",
            "        errmsg = (",
            "            \"No metaproxy key found in opts for id \"",
            "            + opts[\"id\"]",
            "            + \". \"",
            "            + \"Defaulting to standard proxy minion\"",
            "        )",
            "        log.error(errmsg)",
            "",
            "    metaproxy_fn = metaproxy_name + \".\" + fn_name",
            "    return metaproxy[metaproxy_fn]",
            "",
            "",
            "class ProxyMinion(Minion):",
            "    \"\"\"",
            "    This class instantiates a 'proxy' minion--a minion that does not manipulate",
            "    the host it runs on, but instead manipulates a device that cannot run a minion.",
            "    \"\"\"",
            "",
            "    # TODO: better name...",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _post_master_init(self, master):",
            "        \"\"\"",
            "        Function to finish init after connecting to a master",
            "",
            "        This is primarily loading modules, pillars, etc. (since they need",
            "        to know which master they connected to)",
            "",
            "        If this function is changed, please check Minion._post_master_init",
            "        to see if those changes need to be propagated.",
            "",
            "        ProxyMinions need a significantly different post master setup,",
            "        which is why the differences are not factored out into separate helper",
            "        functions.",
            "        \"\"\"",
            "        mp_call = _metaproxy_call(self.opts, \"post_master_init\")",
            "        return mp_call(self, master)",
            "",
            "    def tune_in(self, start=True):",
            "        \"\"\"",
            "        Lock onto the publisher. This is the main event loop for the minion",
            "        :rtype : None",
            "        \"\"\"",
            "        mp_call = _metaproxy_call(self.opts, \"tune_in\")",
            "        return mp_call(self, start)",
            "",
            "    def _target_load(self, load):",
            "        \"\"\"",
            "        Verify that the publication is valid and applies to this minion",
            "        \"\"\"",
            "        mp_call = _metaproxy_call(self.opts, \"target_load\")",
            "        return mp_call(self, load)",
            "",
            "    def _handle_payload(self, payload):",
            "        mp_call = _metaproxy_call(self.opts, \"handle_payload\")",
            "        return mp_call(self, payload)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _handle_decoded_payload(self, data):",
            "        mp_call = _metaproxy_call(self.opts, \"handle_decoded_payload\")",
            "        return mp_call(self, data)",
            "",
            "    @classmethod",
            "    def _target(cls, minion_instance, opts, data, connected):",
            "",
            "        mp_call = _metaproxy_call(opts, \"target\")",
            "        return mp_call(cls, minion_instance, opts, data, connected)",
            "",
            "    @classmethod",
            "    def _thread_return(cls, minion_instance, opts, data):",
            "        mp_call = _metaproxy_call(opts, \"thread_return\")",
            "        return mp_call(cls, minion_instance, opts, data)",
            "",
            "    @classmethod",
            "    def _thread_multi_return(cls, minion_instance, opts, data):",
            "        mp_call = _metaproxy_call(opts, \"thread_multi_return\")",
            "        return mp_call(cls, minion_instance, opts, data)",
            "",
            "",
            "class SProxyMinion(SMinion):",
            "    \"\"\"",
            "    Create an object that has loaded all of the minion module functions,",
            "    grains, modules, returners etc.  The SProxyMinion allows developers to",
            "    generate all of the salt minion functions and present them with these",
            "    functions for general use.",
            "    \"\"\"",
            "",
            "    def gen_modules(self, initial_load=False, context=None):",
            "        \"\"\"",
            "        Tell the minion to reload the execution modules",
            "",
            "        CLI Example:",
            "",
            "        .. code-block:: bash",
            "",
            "            salt '*' sys.reload_modules",
            "        \"\"\"",
            "        self.opts[\"grains\"] = salt.loader.grains(self.opts)",
            "        self.opts[\"pillar\"] = salt.pillar.get_pillar(",
            "            self.opts,",
            "            self.opts[\"grains\"],",
            "            self.opts[\"id\"],",
            "            saltenv=self.opts[\"saltenv\"],",
            "            pillarenv=self.opts.get(\"pillarenv\"),",
            "        ).compile_pillar()",
            "",
            "        if \"proxy\" not in self.opts[\"pillar\"] and \"proxy\" not in self.opts:",
            "            errmsg = (",
            "                'No \"proxy\" configuration key found in pillar or opts '",
            "                \"dictionaries for id {id}. Check your pillar/options \"",
            "                \"configuration and contents. Salt-proxy aborted.\"",
            "            ).format(id=self.opts[\"id\"])",
            "            log.error(errmsg)",
            "            self._running = False",
            "            raise SaltSystemExit(code=salt.defaults.exitcodes.EX_GENERIC, msg=errmsg)",
            "",
            "        if \"proxy\" not in self.opts:",
            "            self.opts[\"proxy\"] = self.opts[\"pillar\"][\"proxy\"]",
            "",
            "        # Then load the proxy module",
            "        self.proxy = salt.loader.proxy(self.opts)",
            "",
            "        self.utils = salt.loader.utils(self.opts, proxy=self.proxy, context=context)",
            "",
            "        self.functions = salt.loader.minion_mods(",
            "            self.opts, utils=self.utils, notify=False, proxy=self.proxy, context=context",
            "        )",
            "        self.returners = salt.loader.returners(",
            "            self.opts, functions=self.functions, proxy=self.proxy, context=context",
            "        )",
            "        self.matchers = salt.loader.matchers(self.opts)",
            "        self.functions[\"sys.reload_modules\"] = self.gen_modules",
            "        self.executors = salt.loader.executors(",
            "            self.opts, functions=self.functions, proxy=self.proxy, context=context,",
            "        )",
            "",
            "        fq_proxyname = self.opts[\"proxy\"][\"proxytype\"]",
            "",
            "        # we can then sync any proxymodules down from the master",
            "        # we do a sync_all here in case proxy code was installed by",
            "        # SPM or was manually placed in /srv/salt/_modules etc.",
            "        self.functions[\"saltutil.sync_all\"](saltenv=self.opts[\"saltenv\"])",
            "",
            "        self.functions.pack[\"__proxy__\"] = self.proxy",
            "        self.proxy.pack[\"__salt__\"] = self.functions",
            "        self.proxy.pack[\"__ret__\"] = self.returners",
            "        self.proxy.pack[\"__pillar__\"] = self.opts[\"pillar\"]",
            "",
            "        # Reload utils as well (chicken and egg, __utils__ needs __proxy__ and __proxy__ needs __utils__",
            "        self.utils = salt.loader.utils(self.opts, proxy=self.proxy, context=context)",
            "        self.proxy.pack[\"__utils__\"] = self.utils",
            "",
            "        # Reload all modules so all dunder variables are injected",
            "        self.proxy.reload_modules()",
            "",
            "        if (",
            "            \"{}.init\".format(fq_proxyname) not in self.proxy",
            "            or \"{}.shutdown\".format(fq_proxyname) not in self.proxy",
            "        ):",
            "            errmsg = (",
            "                \"Proxymodule {} is missing an init() or a shutdown() or both. \".format(",
            "                    fq_proxyname",
            "                )",
            "                + \"Check your proxymodule.  Salt-proxy aborted.\"",
            "            )",
            "            log.error(errmsg)",
            "            self._running = False",
            "            raise SaltSystemExit(code=salt.defaults.exitcodes.EX_GENERIC, msg=errmsg)",
            "",
            "        self.module_executors = self.proxy.get(",
            "            \"{}.module_executors\".format(fq_proxyname), lambda: []",
            "        )()",
            "        proxy_init_fn = self.proxy[fq_proxyname + \".init\"]",
            "        proxy_init_fn(self.opts)",
            "",
            "        self.opts[\"grains\"] = salt.loader.grains(self.opts, proxy=self.proxy)",
            "",
            "        #  Sync the grains here so the proxy can communicate them to the master",
            "        self.functions[\"saltutil.sync_grains\"](saltenv=\"base\")",
            "        self.grains_cache = self.opts[\"grains\"]",
            "        self.ready = True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "shuup.front.urls",
            "salt.minion.Minion._state_run",
            "salt.minion.Minion._handle_payload"
        ]
    },
    "salt/transport/mixins/auth.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 604,
                "afterPatchRowNumber": 604,
                "PatchRowcode": "                 ret[\"aes\"] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)"
            },
            "1": {
                "beforePatchRowNumber": 605,
                "afterPatchRowNumber": 605,
                "PatchRowcode": "             else:"
            },
            "2": {
                "beforePatchRowNumber": 606,
                "afterPatchRowNumber": 606,
                "PatchRowcode": "                 ret[\"aes\"] = cipher.encrypt(aes)"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 607,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": 607,
                "afterPatchRowNumber": 608,
                "PatchRowcode": "         # Be aggressive about the signature"
            },
            "5": {
                "beforePatchRowNumber": 608,
                "afterPatchRowNumber": 609,
                "PatchRowcode": "         digest = salt.utils.stringutils.to_bytes(hashlib.sha256(aes).hexdigest())"
            },
            "6": {
                "beforePatchRowNumber": 609,
                "afterPatchRowNumber": 610,
                "PatchRowcode": "         ret[\"sig\"] = salt.crypt.private_encrypt(self.master_key.key, digest)"
            }
        },
        "frontPatchFile": [
            "import binascii",
            "import ctypes",
            "import hashlib",
            "import logging",
            "import multiprocessing",
            "import os",
            "import shutil",
            "",
            "import salt.crypt",
            "import salt.ext.tornado.gen",
            "import salt.master",
            "import salt.payload",
            "import salt.transport.frame",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.minions",
            "import salt.utils.stringutils",
            "import salt.utils.verify",
            "from salt.utils.cache import CacheCli",
            "",
            "try:",
            "    from M2Crypto import RSA",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "    try:",
            "        from Cryptodome.Cipher import PKCS1_OAEP",
            "    except ImportError:",
            "        from Crypto.Cipher import PKCS1_OAEP  # nosec",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "# TODO: rename",
            "class AESPubClientMixin:",
            "    def _verify_master_signature(self, payload):",
            "        if self.opts.get(\"sign_pub_messages\"):",
            "            if not payload.get(\"sig\", False):",
            "                raise salt.crypt.AuthenticationError(",
            "                    \"Message signing is enabled but the payload has no signature.\"",
            "                )",
            "",
            "            # Verify that the signature is valid",
            "            master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion_master.pub\")",
            "            if not salt.crypt.verify_signature(",
            "                master_pubkey_path, payload[\"load\"], payload.get(\"sig\")",
            "            ):",
            "                raise salt.crypt.AuthenticationError(",
            "                    \"Message signature failed to validate.\"",
            "                )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _decode_payload(self, payload):",
            "        # we need to decrypt it",
            "        log.trace(\"Decoding payload: %s\", payload)",
            "        if payload[\"enc\"] == \"aes\":",
            "            self._verify_master_signature(payload)",
            "            try:",
            "                payload[\"load\"] = self.auth.crypticle.loads(payload[\"load\"])",
            "            except salt.crypt.AuthenticationError:",
            "                yield self.auth.authenticate()",
            "                payload[\"load\"] = self.auth.crypticle.loads(payload[\"load\"])",
            "",
            "        raise salt.ext.tornado.gen.Return(payload)",
            "",
            "",
            "# TODO: rename?",
            "class AESReqServerMixin:",
            "    \"\"\"",
            "    Mixin to house all of the master-side auth crypto",
            "    \"\"\"",
            "",
            "    def pre_fork(self, _):",
            "        \"\"\"",
            "        Pre-fork we need to create the zmq router device",
            "        \"\"\"",
            "        if \"aes\" not in salt.master.SMaster.secrets:",
            "            # TODO: This is still needed only for the unit tests",
            "            # 'tcp_test.py' and 'zeromq_test.py'. Fix that. In normal",
            "            # cases, 'aes' is already set in the secrets.",
            "            salt.master.SMaster.secrets[\"aes\"] = {",
            "                \"secret\": multiprocessing.Array(",
            "                    ctypes.c_char,",
            "                    salt.utils.stringutils.to_bytes(",
            "                        salt.crypt.Crypticle.generate_key_string()",
            "                    ),",
            "                ),",
            "                \"reload\": salt.crypt.Crypticle.generate_key_string,",
            "            }",
            "",
            "    def post_fork(self, _, __):",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.crypticle = salt.crypt.Crypticle(",
            "            self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "        )",
            "",
            "        # other things needed for _auth",
            "        # Create the event manager",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        self.auto_key = salt.daemons.masterapi.AutoKey(self.opts)",
            "",
            "        # only create a con_cache-client if the con_cache is active",
            "        if self.opts[\"con_cache\"]:",
            "            self.cache_cli = CacheCli(self.opts)",
            "        else:",
            "            self.cache_cli = False",
            "            # Make an minion checker object",
            "            self.ckminions = salt.utils.minions.CkMinions(self.opts)",
            "",
            "        self.master_key = salt.crypt.MasterKeys(self.opts)",
            "",
            "    def _encrypt_private(self, ret, dictkey, target, nonce=None, sign_messages=True):",
            "        \"\"\"",
            "        The server equivalent of ReqChannel.crypted_transfer_decode_dictentry",
            "        \"\"\"",
            "        # encrypt with a specific AES key",
            "        pubfn = os.path.join(self.opts[\"pki_dir\"], \"minions\", target)",
            "        key = salt.crypt.Crypticle.generate_key_string()",
            "        pcrypt = salt.crypt.Crypticle(self.opts, key)",
            "        try:",
            "            pub = salt.crypt.get_rsa_pub_key(pubfn)",
            "        except (ValueError, IndexError, TypeError):",
            "            return self.crypticle.dumps({})",
            "        except OSError:",
            "            log.error(\"AES key not found\")",
            "            return {\"error\": \"AES key not found\"}",
            "        pret = {}",
            "        key = salt.utils.stringutils.to_bytes(key)",
            "        if HAS_M2:",
            "            pret[\"key\"] = pub.public_encrypt(key, RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(pub)",
            "            pret[\"key\"] = cipher.encrypt(key)",
            "        if ret is False:",
            "            ret = {}",
            "        if sign_messages:",
            "            if nonce is None:",
            "                return {\"error\": \"Nonce not included in request\"}",
            "            tosign = salt.payload.Serial({}).dumps(",
            "                {\"key\": pret[\"key\"], \"pillar\": ret, \"nonce\": nonce}",
            "            )",
            "            master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "            signed_msg = {",
            "                \"data\": tosign,",
            "                \"sig\": salt.crypt.sign_message(master_pem_path, tosign),",
            "            }",
            "            pret[dictkey] = pcrypt.dumps(signed_msg)",
            "        else:",
            "            pret[dictkey] = pcrypt.dumps(ret)",
            "        return pret",
            "",
            "    def _clear_signed(self, load):",
            "        master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "        tosign = salt.payload.Serial({}).dumps(load)",
            "        return {",
            "            \"enc\": \"clear\",",
            "            \"load\": tosign,",
            "            \"sig\": salt.crypt.sign_message(master_pem_path, tosign),",
            "        }",
            "",
            "    def _update_aes(self):",
            "        \"\"\"",
            "        Check to see if a fresh AES key is available and update the components",
            "        of the worker",
            "        \"\"\"",
            "        if (",
            "            salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "            != self.crypticle.key_string",
            "        ):",
            "            self.crypticle = salt.crypt.Crypticle(",
            "                self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "            )",
            "            return True",
            "        return False",
            "",
            "    def _decode_payload(self, payload):",
            "        # we need to decrypt it",
            "        if payload[\"enc\"] == \"aes\":",
            "            try:",
            "                payload[\"load\"] = self.crypticle.loads(payload[\"load\"])",
            "            except salt.crypt.AuthenticationError:",
            "                if not self._update_aes():",
            "                    raise",
            "                payload[\"load\"] = self.crypticle.loads(payload[\"load\"])",
            "        return payload",
            "",
            "    def _auth(self, load, sign_messages=False):",
            "        \"\"\"",
            "        Authenticate the client, use the sent public key to encrypt the AES key",
            "        which was generated at start up.",
            "",
            "        This method fires an event over the master event manager. The event is",
            "        tagged \"auth\" and returns a dict with information about the auth",
            "        event",
            "",
            "        # Verify that the key we are receiving matches the stored key",
            "        # Store the key if it is not there",
            "        # Make an RSA key with the pub key",
            "        # Encrypt the AES key as an encrypted salt.payload",
            "        # Package the return and return it",
            "        \"\"\"",
            "",
            "        if not salt.utils.verify.valid_id(self.opts, load[\"id\"]):",
            "            log.info(\"Authentication request from invalid id %s\", load[\"id\"])",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "        log.info(\"Authentication request from %s\", load[\"id\"])",
            "",
            "        # 0 is default which should be 'unlimited'",
            "        if self.opts[\"max_minions\"] > 0:",
            "            # use the ConCache if enabled, else use the minion utils",
            "            if self.cache_cli:",
            "                minions = self.cache_cli.get_cached()",
            "            else:",
            "                minions = self.ckminions.connected_ids()",
            "                if len(minions) > 1000:",
            "                    log.info(",
            "                        \"With large numbers of minions it is advised \"",
            "                        \"to enable the ConCache with 'con_cache: True' \"",
            "                        \"in the masters configuration file.\"",
            "                    )",
            "",
            "            if not len(minions) <= self.opts[\"max_minions\"]:",
            "                # we reject new minions, minions that are already",
            "                # connected must be allowed for the mine, highstate, etc.",
            "                if load[\"id\"] not in minions:",
            "                    msg = (",
            "                        \"Too many minions connected (max_minions={}). \"",
            "                        \"Rejecting connection from id \"",
            "                        \"{}\".format(self.opts[\"max_minions\"], load[\"id\"])",
            "                    )",
            "                    log.info(msg)",
            "                    eload = {",
            "                        \"result\": False,",
            "                        \"act\": \"full\",",
            "                        \"id\": load[\"id\"],",
            "                        \"pub\": load[\"pub\"],",
            "                    }",
            "",
            "                    if self.opts.get(\"auth_events\") is True:",
            "                        self.event.fire_event(",
            "                            eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                        )",
            "                    if sign_messages:",
            "                        return self._clear_signed(",
            "                            {\"ret\": \"full\", \"nonce\": load[\"nonce\"]}",
            "                        )",
            "                    else:",
            "                        return {\"enc\": \"clear\", \"load\": {\"ret\": \"full\"}}",
            "",
            "        # Check if key is configured to be auto-rejected/signed",
            "        auto_reject = self.auto_key.check_autoreject(load[\"id\"])",
            "        auto_sign = self.auto_key.check_autosign(",
            "            load[\"id\"], load.get(\"autosign_grains\", None)",
            "        )",
            "",
            "        pubfn = os.path.join(self.opts[\"pki_dir\"], \"minions\", load[\"id\"])",
            "        pubfn_pend = os.path.join(self.opts[\"pki_dir\"], \"minions_pre\", load[\"id\"])",
            "        pubfn_rejected = os.path.join(",
            "            self.opts[\"pki_dir\"], \"minions_rejected\", load[\"id\"]",
            "        )",
            "        pubfn_denied = os.path.join(self.opts[\"pki_dir\"], \"minions_denied\", load[\"id\"])",
            "        if self.opts[\"open_mode\"]:",
            "            # open mode is turned on, nuts to checks and overwrite whatever",
            "            # is there",
            "            pass",
            "        elif os.path.isfile(pubfn_rejected):",
            "            # The key has been rejected, don't place it in pending",
            "            log.info(",
            "                \"Public key rejected for %s. Key is present in \" \"rejection key dir.\",",
            "                load[\"id\"],",
            "            )",
            "            eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "            if self.opts.get(\"auth_events\") is True:",
            "                self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "        elif os.path.isfile(pubfn):",
            "            # The key has been accepted, check it",
            "            with salt.utils.files.fopen(pubfn, \"r\") as pubfn_handle:",
            "                if pubfn_handle.read().strip() != load[\"pub\"].strip():",
            "                    log.error(",
            "                        \"Authentication attempt from %s failed, the public \"",
            "                        \"keys did not match. This may be an attempt to compromise \"",
            "                        \"the Salt cluster.\",",
            "                        load[\"id\"],",
            "                    )",
            "                    # put denied minion key into minions_denied",
            "                    with salt.utils.files.fopen(pubfn_denied, \"w+\") as fp_:",
            "                        fp_.write(load[\"pub\"])",
            "                    eload = {",
            "                        \"result\": False,",
            "                        \"id\": load[\"id\"],",
            "                        \"act\": \"denied\",",
            "                        \"pub\": load[\"pub\"],",
            "                    }",
            "                    if self.opts.get(\"auth_events\") is True:",
            "                        self.event.fire_event(",
            "                            eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                        )",
            "                    if sign_messages:",
            "                        return self._clear_signed(",
            "                            {\"ret\": False, \"nonce\": load[\"nonce\"]}",
            "                        )",
            "                    else:",
            "                        return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        elif not os.path.isfile(pubfn_pend):",
            "            # The key has not been accepted, this is a new minion",
            "            if os.path.isdir(pubfn_pend):",
            "                # The key path is a directory, error out",
            "                log.info(\"New public key %s is a directory\", load[\"id\"])",
            "                eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "                if sign_messages:",
            "                    return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "            if auto_reject:",
            "                key_path = pubfn_rejected",
            "                log.info(",
            "                    \"New public key for %s rejected via autoreject_file\", load[\"id\"]",
            "                )",
            "                key_act = \"reject\"",
            "                key_result = False",
            "            elif not auto_sign:",
            "                key_path = pubfn_pend",
            "                log.info(\"New public key for %s placed in pending\", load[\"id\"])",
            "                key_act = \"pend\"",
            "                key_result = True",
            "            else:",
            "                # The key is being automatically accepted, don't do anything",
            "                # here and let the auto accept logic below handle it.",
            "                key_path = None",
            "",
            "            if key_path is not None:",
            "                # Write the key to the appropriate location",
            "                with salt.utils.files.fopen(key_path, \"w+\") as fp_:",
            "                    fp_.write(load[\"pub\"])",
            "                eload = {",
            "                    \"result\": key_result,",
            "                    \"act\": key_act,",
            "                    \"id\": load[\"id\"],",
            "                    \"pub\": load[\"pub\"],",
            "                }",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "                if sign_messages:",
            "                    return self._clear_signed(",
            "                        {\"ret\": key_result, \"nonce\": load[\"nonce\"]}",
            "                    )",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": key_result}}",
            "",
            "        elif os.path.isfile(pubfn_pend):",
            "            # This key is in the pending dir and is awaiting acceptance",
            "            if auto_reject:",
            "                # We don't care if the keys match, this minion is being",
            "                # auto-rejected. Move the key file from the pending dir to the",
            "                # rejected dir.",
            "                try:",
            "                    shutil.move(pubfn_pend, pubfn_rejected)",
            "                except OSError:",
            "                    pass",
            "                log.info(",
            "                    \"Pending public key for %s rejected via \" \"autoreject_file\",",
            "                    load[\"id\"],",
            "                )",
            "                eload = {",
            "                    \"result\": False,",
            "                    \"act\": \"reject\",",
            "                    \"id\": load[\"id\"],",
            "                    \"pub\": load[\"pub\"],",
            "                }",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "                if sign_messages:",
            "                    return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "            elif not auto_sign:",
            "                # This key is in the pending dir and is not being auto-signed.",
            "                # Check if the keys are the same and error out if this is the",
            "                # case. Otherwise log the fact that the minion is still",
            "                # pending.",
            "                with salt.utils.files.fopen(pubfn_pend, \"r\") as pubfn_handle:",
            "                    if pubfn_handle.read() != load[\"pub\"]:",
            "                        log.error(",
            "                            \"Authentication attempt from %s failed, the public \"",
            "                            \"key in pending did not match. This may be an \"",
            "                            \"attempt to compromise the Salt cluster.\",",
            "                            load[\"id\"],",
            "                        )",
            "                        # put denied minion key into minions_denied",
            "                        with salt.utils.files.fopen(pubfn_denied, \"w+\") as fp_:",
            "                            fp_.write(load[\"pub\"])",
            "                        eload = {",
            "                            \"result\": False,",
            "                            \"id\": load[\"id\"],",
            "                            \"act\": \"denied\",",
            "                            \"pub\": load[\"pub\"],",
            "                        }",
            "                        if self.opts.get(\"auth_events\") is True:",
            "                            self.event.fire_event(",
            "                                eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                            )",
            "                        if sign_messages:",
            "                            return self._clear_signed(",
            "                                {\"ret\": False, \"nonce\": load[\"nonce\"]}",
            "                            )",
            "                        else:",
            "                            return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "                    else:",
            "                        log.info(",
            "                            \"Authentication failed from host %s, the key is in \"",
            "                            \"pending and needs to be accepted with salt-key \"",
            "                            \"-a %s\",",
            "                            load[\"id\"],",
            "                            load[\"id\"],",
            "                        )",
            "                        eload = {",
            "                            \"result\": True,",
            "                            \"act\": \"pend\",",
            "                            \"id\": load[\"id\"],",
            "                            \"pub\": load[\"pub\"],",
            "                        }",
            "                        if self.opts.get(\"auth_events\") is True:",
            "                            self.event.fire_event(",
            "                                eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                            )",
            "                        if sign_messages:",
            "                            return self._clear_signed(",
            "                                {\"ret\": True, \"nonce\": load[\"nonce\"]}",
            "                            )",
            "                        else:",
            "                            return {\"enc\": \"clear\", \"load\": {\"ret\": True}}",
            "            else:",
            "                # This key is in pending and has been configured to be",
            "                # auto-signed. Check to see if it is the same key, and if",
            "                # so, pass on doing anything here, and let it get automatically",
            "                # accepted below.",
            "                with salt.utils.files.fopen(pubfn_pend, \"r\") as pubfn_handle:",
            "                    if pubfn_handle.read() != load[\"pub\"]:",
            "                        log.error(",
            "                            \"Authentication attempt from %s failed, the public \"",
            "                            \"keys in pending did not match. This may be an \"",
            "                            \"attempt to compromise the Salt cluster.\",",
            "                            load[\"id\"],",
            "                        )",
            "                        # put denied minion key into minions_denied",
            "                        with salt.utils.files.fopen(pubfn_denied, \"w+\") as fp_:",
            "                            fp_.write(load[\"pub\"])",
            "                        eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "                        if self.opts.get(\"auth_events\") is True:",
            "                            self.event.fire_event(",
            "                                eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                            )",
            "                        if sign_messages:",
            "                            return self._clear_signed(",
            "                                {\"ret\": False, \"nonce\": load[\"nonce\"]}",
            "                            )",
            "                        else:",
            "                            return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "                    else:",
            "                        os.remove(pubfn_pend)",
            "",
            "        else:",
            "            # Something happened that I have not accounted for, FAIL!",
            "            log.warning(\"Unaccounted for authentication failure\")",
            "            eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "            if self.opts.get(\"auth_events\") is True:",
            "                self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        log.info(\"Authentication accepted from %s\", load[\"id\"])",
            "        # only write to disk if you are adding the file, and in open mode,",
            "        # which implies we accept any key from a minion.",
            "        if not os.path.isfile(pubfn) and not self.opts[\"open_mode\"]:",
            "            with salt.utils.files.fopen(pubfn, \"w+\") as fp_:",
            "                fp_.write(load[\"pub\"])",
            "        elif self.opts[\"open_mode\"]:",
            "            disk_key = \"\"",
            "            if os.path.isfile(pubfn):",
            "                with salt.utils.files.fopen(pubfn, \"r\") as fp_:",
            "                    disk_key = fp_.read()",
            "            if load[\"pub\"] and load[\"pub\"] != disk_key:",
            "                log.debug(\"Host key change detected in open mode.\")",
            "                with salt.utils.files.fopen(pubfn, \"w+\") as fp_:",
            "                    fp_.write(load[\"pub\"])",
            "            elif not load[\"pub\"]:",
            "                log.error(\"Public key is empty: %s\", load[\"id\"])",
            "                if sign_messages:",
            "                    return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        pub = None",
            "",
            "        # the con_cache is enabled, send the minion id to the cache",
            "        if self.cache_cli:",
            "            self.cache_cli.put_cache([load[\"id\"]])",
            "",
            "        # The key payload may sometimes be corrupt when using auto-accept",
            "        # and an empty request comes in",
            "        try:",
            "            pub = salt.crypt.get_rsa_pub_key(pubfn)",
            "        except (ValueError, IndexError, TypeError) as err:",
            "            log.error('Corrupt public key \"%s\": %s', pubfn, err)",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        if not HAS_M2:",
            "            cipher = PKCS1_OAEP.new(pub)",
            "        ret = {",
            "            \"enc\": \"pub\",",
            "            \"pub_key\": self.master_key.get_pub_str(),",
            "            \"publish_port\": self.opts[\"publish_port\"],",
            "        }",
            "",
            "        # sign the master's pubkey (if enabled) before it is",
            "        # sent to the minion that was just authenticated",
            "        if self.opts[\"master_sign_pubkey\"]:",
            "            # append the pre-computed signature to the auth-reply",
            "            if self.master_key.pubkey_signature():",
            "                log.debug(\"Adding pubkey signature to auth-reply\")",
            "                log.debug(self.master_key.pubkey_signature())",
            "                ret.update({\"pub_sig\": self.master_key.pubkey_signature()})",
            "            else:",
            "                # the master has its own signing-keypair, compute the master.pub's",
            "                # signature and append that to the auth-reply",
            "",
            "                # get the key_pass for the signing key",
            "                key_pass = salt.utils.sdb.sdb_get(",
            "                    self.opts[\"signing_key_pass\"], self.opts",
            "                )",
            "",
            "                log.debug(\"Signing master public key before sending\")",
            "                pub_sign = salt.crypt.sign_message(",
            "                    self.master_key.get_sign_paths()[1], ret[\"pub_key\"], key_pass",
            "                )",
            "                ret.update({\"pub_sig\": binascii.b2a_base64(pub_sign)})",
            "",
            "        if not HAS_M2:",
            "            mcipher = PKCS1_OAEP.new(self.master_key.key)",
            "        if self.opts[\"auth_mode\"] >= 2:",
            "            if \"token\" in load:",
            "                try:",
            "                    if HAS_M2:",
            "                        mtoken = self.master_key.key.private_decrypt(",
            "                            load[\"token\"], RSA.pkcs1_oaep_padding",
            "                        )",
            "                    else:",
            "                        mtoken = mcipher.decrypt(load[\"token\"])",
            "                    aes = \"{}_|-{}\".format(",
            "                        salt.master.SMaster.secrets[\"aes\"][\"secret\"].value, mtoken",
            "                    )",
            "                except Exception:  # pylint: disable=broad-except",
            "                    # Token failed to decrypt, send back the salty bacon to",
            "                    # support older minions",
            "                    pass",
            "            else:",
            "                aes = salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "",
            "            if HAS_M2:",
            "                ret[\"aes\"] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)",
            "            else:",
            "                ret[\"aes\"] = cipher.encrypt(aes)",
            "        else:",
            "            if \"token\" in load:",
            "                try:",
            "                    if HAS_M2:",
            "                        mtoken = self.master_key.key.private_decrypt(",
            "                            load[\"token\"], RSA.pkcs1_oaep_padding",
            "                        )",
            "                        ret[\"token\"] = pub.public_encrypt(",
            "                            mtoken, RSA.pkcs1_oaep_padding",
            "                        )",
            "                    else:",
            "                        mtoken = mcipher.decrypt(load[\"token\"])",
            "                        ret[\"token\"] = cipher.encrypt(mtoken)",
            "                except Exception:  # pylint: disable=broad-except",
            "                    # Token failed to decrypt, send back the salty bacon to",
            "                    # support older minions",
            "                    pass",
            "",
            "            aes = salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "            if HAS_M2:",
            "                ret[\"aes\"] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)",
            "            else:",
            "                ret[\"aes\"] = cipher.encrypt(aes)",
            "        # Be aggressive about the signature",
            "        digest = salt.utils.stringutils.to_bytes(hashlib.sha256(aes).hexdigest())",
            "        ret[\"sig\"] = salt.crypt.private_encrypt(self.master_key.key, digest)",
            "        eload = {\"result\": True, \"act\": \"accept\", \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "        if self.opts.get(\"auth_events\") is True:",
            "            self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "        if sign_messages:",
            "            ret[\"nonce\"] = load[\"nonce\"]",
            "            return self._clear_signed(ret)",
            "        return ret"
        ],
        "afterPatchFile": [
            "import binascii",
            "import ctypes",
            "import hashlib",
            "import logging",
            "import multiprocessing",
            "import os",
            "import shutil",
            "",
            "import salt.crypt",
            "import salt.ext.tornado.gen",
            "import salt.master",
            "import salt.payload",
            "import salt.transport.frame",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.minions",
            "import salt.utils.stringutils",
            "import salt.utils.verify",
            "from salt.utils.cache import CacheCli",
            "",
            "try:",
            "    from M2Crypto import RSA",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "    try:",
            "        from Cryptodome.Cipher import PKCS1_OAEP",
            "    except ImportError:",
            "        from Crypto.Cipher import PKCS1_OAEP  # nosec",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "# TODO: rename",
            "class AESPubClientMixin:",
            "    def _verify_master_signature(self, payload):",
            "        if self.opts.get(\"sign_pub_messages\"):",
            "            if not payload.get(\"sig\", False):",
            "                raise salt.crypt.AuthenticationError(",
            "                    \"Message signing is enabled but the payload has no signature.\"",
            "                )",
            "",
            "            # Verify that the signature is valid",
            "            master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion_master.pub\")",
            "            if not salt.crypt.verify_signature(",
            "                master_pubkey_path, payload[\"load\"], payload.get(\"sig\")",
            "            ):",
            "                raise salt.crypt.AuthenticationError(",
            "                    \"Message signature failed to validate.\"",
            "                )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _decode_payload(self, payload):",
            "        # we need to decrypt it",
            "        log.trace(\"Decoding payload: %s\", payload)",
            "        if payload[\"enc\"] == \"aes\":",
            "            self._verify_master_signature(payload)",
            "            try:",
            "                payload[\"load\"] = self.auth.crypticle.loads(payload[\"load\"])",
            "            except salt.crypt.AuthenticationError:",
            "                yield self.auth.authenticate()",
            "                payload[\"load\"] = self.auth.crypticle.loads(payload[\"load\"])",
            "",
            "        raise salt.ext.tornado.gen.Return(payload)",
            "",
            "",
            "# TODO: rename?",
            "class AESReqServerMixin:",
            "    \"\"\"",
            "    Mixin to house all of the master-side auth crypto",
            "    \"\"\"",
            "",
            "    def pre_fork(self, _):",
            "        \"\"\"",
            "        Pre-fork we need to create the zmq router device",
            "        \"\"\"",
            "        if \"aes\" not in salt.master.SMaster.secrets:",
            "            # TODO: This is still needed only for the unit tests",
            "            # 'tcp_test.py' and 'zeromq_test.py'. Fix that. In normal",
            "            # cases, 'aes' is already set in the secrets.",
            "            salt.master.SMaster.secrets[\"aes\"] = {",
            "                \"secret\": multiprocessing.Array(",
            "                    ctypes.c_char,",
            "                    salt.utils.stringutils.to_bytes(",
            "                        salt.crypt.Crypticle.generate_key_string()",
            "                    ),",
            "                ),",
            "                \"reload\": salt.crypt.Crypticle.generate_key_string,",
            "            }",
            "",
            "    def post_fork(self, _, __):",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.crypticle = salt.crypt.Crypticle(",
            "            self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "        )",
            "",
            "        # other things needed for _auth",
            "        # Create the event manager",
            "        self.event = salt.utils.event.get_master_event(",
            "            self.opts, self.opts[\"sock_dir\"], listen=False",
            "        )",
            "        self.auto_key = salt.daemons.masterapi.AutoKey(self.opts)",
            "",
            "        # only create a con_cache-client if the con_cache is active",
            "        if self.opts[\"con_cache\"]:",
            "            self.cache_cli = CacheCli(self.opts)",
            "        else:",
            "            self.cache_cli = False",
            "            # Make an minion checker object",
            "            self.ckminions = salt.utils.minions.CkMinions(self.opts)",
            "",
            "        self.master_key = salt.crypt.MasterKeys(self.opts)",
            "",
            "    def _encrypt_private(self, ret, dictkey, target, nonce=None, sign_messages=True):",
            "        \"\"\"",
            "        The server equivalent of ReqChannel.crypted_transfer_decode_dictentry",
            "        \"\"\"",
            "        # encrypt with a specific AES key",
            "        pubfn = os.path.join(self.opts[\"pki_dir\"], \"minions\", target)",
            "        key = salt.crypt.Crypticle.generate_key_string()",
            "        pcrypt = salt.crypt.Crypticle(self.opts, key)",
            "        try:",
            "            pub = salt.crypt.get_rsa_pub_key(pubfn)",
            "        except (ValueError, IndexError, TypeError):",
            "            return self.crypticle.dumps({})",
            "        except OSError:",
            "            log.error(\"AES key not found\")",
            "            return {\"error\": \"AES key not found\"}",
            "        pret = {}",
            "        key = salt.utils.stringutils.to_bytes(key)",
            "        if HAS_M2:",
            "            pret[\"key\"] = pub.public_encrypt(key, RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(pub)",
            "            pret[\"key\"] = cipher.encrypt(key)",
            "        if ret is False:",
            "            ret = {}",
            "        if sign_messages:",
            "            if nonce is None:",
            "                return {\"error\": \"Nonce not included in request\"}",
            "            tosign = salt.payload.Serial({}).dumps(",
            "                {\"key\": pret[\"key\"], \"pillar\": ret, \"nonce\": nonce}",
            "            )",
            "            master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "            signed_msg = {",
            "                \"data\": tosign,",
            "                \"sig\": salt.crypt.sign_message(master_pem_path, tosign),",
            "            }",
            "            pret[dictkey] = pcrypt.dumps(signed_msg)",
            "        else:",
            "            pret[dictkey] = pcrypt.dumps(ret)",
            "        return pret",
            "",
            "    def _clear_signed(self, load):",
            "        master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "        tosign = salt.payload.Serial({}).dumps(load)",
            "        return {",
            "            \"enc\": \"clear\",",
            "            \"load\": tosign,",
            "            \"sig\": salt.crypt.sign_message(master_pem_path, tosign),",
            "        }",
            "",
            "    def _update_aes(self):",
            "        \"\"\"",
            "        Check to see if a fresh AES key is available and update the components",
            "        of the worker",
            "        \"\"\"",
            "        if (",
            "            salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "            != self.crypticle.key_string",
            "        ):",
            "            self.crypticle = salt.crypt.Crypticle(",
            "                self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "            )",
            "            return True",
            "        return False",
            "",
            "    def _decode_payload(self, payload):",
            "        # we need to decrypt it",
            "        if payload[\"enc\"] == \"aes\":",
            "            try:",
            "                payload[\"load\"] = self.crypticle.loads(payload[\"load\"])",
            "            except salt.crypt.AuthenticationError:",
            "                if not self._update_aes():",
            "                    raise",
            "                payload[\"load\"] = self.crypticle.loads(payload[\"load\"])",
            "        return payload",
            "",
            "    def _auth(self, load, sign_messages=False):",
            "        \"\"\"",
            "        Authenticate the client, use the sent public key to encrypt the AES key",
            "        which was generated at start up.",
            "",
            "        This method fires an event over the master event manager. The event is",
            "        tagged \"auth\" and returns a dict with information about the auth",
            "        event",
            "",
            "        # Verify that the key we are receiving matches the stored key",
            "        # Store the key if it is not there",
            "        # Make an RSA key with the pub key",
            "        # Encrypt the AES key as an encrypted salt.payload",
            "        # Package the return and return it",
            "        \"\"\"",
            "",
            "        if not salt.utils.verify.valid_id(self.opts, load[\"id\"]):",
            "            log.info(\"Authentication request from invalid id %s\", load[\"id\"])",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "        log.info(\"Authentication request from %s\", load[\"id\"])",
            "",
            "        # 0 is default which should be 'unlimited'",
            "        if self.opts[\"max_minions\"] > 0:",
            "            # use the ConCache if enabled, else use the minion utils",
            "            if self.cache_cli:",
            "                minions = self.cache_cli.get_cached()",
            "            else:",
            "                minions = self.ckminions.connected_ids()",
            "                if len(minions) > 1000:",
            "                    log.info(",
            "                        \"With large numbers of minions it is advised \"",
            "                        \"to enable the ConCache with 'con_cache: True' \"",
            "                        \"in the masters configuration file.\"",
            "                    )",
            "",
            "            if not len(minions) <= self.opts[\"max_minions\"]:",
            "                # we reject new minions, minions that are already",
            "                # connected must be allowed for the mine, highstate, etc.",
            "                if load[\"id\"] not in minions:",
            "                    msg = (",
            "                        \"Too many minions connected (max_minions={}). \"",
            "                        \"Rejecting connection from id \"",
            "                        \"{}\".format(self.opts[\"max_minions\"], load[\"id\"])",
            "                    )",
            "                    log.info(msg)",
            "                    eload = {",
            "                        \"result\": False,",
            "                        \"act\": \"full\",",
            "                        \"id\": load[\"id\"],",
            "                        \"pub\": load[\"pub\"],",
            "                    }",
            "",
            "                    if self.opts.get(\"auth_events\") is True:",
            "                        self.event.fire_event(",
            "                            eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                        )",
            "                    if sign_messages:",
            "                        return self._clear_signed(",
            "                            {\"ret\": \"full\", \"nonce\": load[\"nonce\"]}",
            "                        )",
            "                    else:",
            "                        return {\"enc\": \"clear\", \"load\": {\"ret\": \"full\"}}",
            "",
            "        # Check if key is configured to be auto-rejected/signed",
            "        auto_reject = self.auto_key.check_autoreject(load[\"id\"])",
            "        auto_sign = self.auto_key.check_autosign(",
            "            load[\"id\"], load.get(\"autosign_grains\", None)",
            "        )",
            "",
            "        pubfn = os.path.join(self.opts[\"pki_dir\"], \"minions\", load[\"id\"])",
            "        pubfn_pend = os.path.join(self.opts[\"pki_dir\"], \"minions_pre\", load[\"id\"])",
            "        pubfn_rejected = os.path.join(",
            "            self.opts[\"pki_dir\"], \"minions_rejected\", load[\"id\"]",
            "        )",
            "        pubfn_denied = os.path.join(self.opts[\"pki_dir\"], \"minions_denied\", load[\"id\"])",
            "        if self.opts[\"open_mode\"]:",
            "            # open mode is turned on, nuts to checks and overwrite whatever",
            "            # is there",
            "            pass",
            "        elif os.path.isfile(pubfn_rejected):",
            "            # The key has been rejected, don't place it in pending",
            "            log.info(",
            "                \"Public key rejected for %s. Key is present in \" \"rejection key dir.\",",
            "                load[\"id\"],",
            "            )",
            "            eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "            if self.opts.get(\"auth_events\") is True:",
            "                self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "        elif os.path.isfile(pubfn):",
            "            # The key has been accepted, check it",
            "            with salt.utils.files.fopen(pubfn, \"r\") as pubfn_handle:",
            "                if pubfn_handle.read().strip() != load[\"pub\"].strip():",
            "                    log.error(",
            "                        \"Authentication attempt from %s failed, the public \"",
            "                        \"keys did not match. This may be an attempt to compromise \"",
            "                        \"the Salt cluster.\",",
            "                        load[\"id\"],",
            "                    )",
            "                    # put denied minion key into minions_denied",
            "                    with salt.utils.files.fopen(pubfn_denied, \"w+\") as fp_:",
            "                        fp_.write(load[\"pub\"])",
            "                    eload = {",
            "                        \"result\": False,",
            "                        \"id\": load[\"id\"],",
            "                        \"act\": \"denied\",",
            "                        \"pub\": load[\"pub\"],",
            "                    }",
            "                    if self.opts.get(\"auth_events\") is True:",
            "                        self.event.fire_event(",
            "                            eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                        )",
            "                    if sign_messages:",
            "                        return self._clear_signed(",
            "                            {\"ret\": False, \"nonce\": load[\"nonce\"]}",
            "                        )",
            "                    else:",
            "                        return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        elif not os.path.isfile(pubfn_pend):",
            "            # The key has not been accepted, this is a new minion",
            "            if os.path.isdir(pubfn_pend):",
            "                # The key path is a directory, error out",
            "                log.info(\"New public key %s is a directory\", load[\"id\"])",
            "                eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "                if sign_messages:",
            "                    return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "            if auto_reject:",
            "                key_path = pubfn_rejected",
            "                log.info(",
            "                    \"New public key for %s rejected via autoreject_file\", load[\"id\"]",
            "                )",
            "                key_act = \"reject\"",
            "                key_result = False",
            "            elif not auto_sign:",
            "                key_path = pubfn_pend",
            "                log.info(\"New public key for %s placed in pending\", load[\"id\"])",
            "                key_act = \"pend\"",
            "                key_result = True",
            "            else:",
            "                # The key is being automatically accepted, don't do anything",
            "                # here and let the auto accept logic below handle it.",
            "                key_path = None",
            "",
            "            if key_path is not None:",
            "                # Write the key to the appropriate location",
            "                with salt.utils.files.fopen(key_path, \"w+\") as fp_:",
            "                    fp_.write(load[\"pub\"])",
            "                eload = {",
            "                    \"result\": key_result,",
            "                    \"act\": key_act,",
            "                    \"id\": load[\"id\"],",
            "                    \"pub\": load[\"pub\"],",
            "                }",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "                if sign_messages:",
            "                    return self._clear_signed(",
            "                        {\"ret\": key_result, \"nonce\": load[\"nonce\"]}",
            "                    )",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": key_result}}",
            "",
            "        elif os.path.isfile(pubfn_pend):",
            "            # This key is in the pending dir and is awaiting acceptance",
            "            if auto_reject:",
            "                # We don't care if the keys match, this minion is being",
            "                # auto-rejected. Move the key file from the pending dir to the",
            "                # rejected dir.",
            "                try:",
            "                    shutil.move(pubfn_pend, pubfn_rejected)",
            "                except OSError:",
            "                    pass",
            "                log.info(",
            "                    \"Pending public key for %s rejected via \" \"autoreject_file\",",
            "                    load[\"id\"],",
            "                )",
            "                eload = {",
            "                    \"result\": False,",
            "                    \"act\": \"reject\",",
            "                    \"id\": load[\"id\"],",
            "                    \"pub\": load[\"pub\"],",
            "                }",
            "                if self.opts.get(\"auth_events\") is True:",
            "                    self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "                if sign_messages:",
            "                    return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "            elif not auto_sign:",
            "                # This key is in the pending dir and is not being auto-signed.",
            "                # Check if the keys are the same and error out if this is the",
            "                # case. Otherwise log the fact that the minion is still",
            "                # pending.",
            "                with salt.utils.files.fopen(pubfn_pend, \"r\") as pubfn_handle:",
            "                    if pubfn_handle.read() != load[\"pub\"]:",
            "                        log.error(",
            "                            \"Authentication attempt from %s failed, the public \"",
            "                            \"key in pending did not match. This may be an \"",
            "                            \"attempt to compromise the Salt cluster.\",",
            "                            load[\"id\"],",
            "                        )",
            "                        # put denied minion key into minions_denied",
            "                        with salt.utils.files.fopen(pubfn_denied, \"w+\") as fp_:",
            "                            fp_.write(load[\"pub\"])",
            "                        eload = {",
            "                            \"result\": False,",
            "                            \"id\": load[\"id\"],",
            "                            \"act\": \"denied\",",
            "                            \"pub\": load[\"pub\"],",
            "                        }",
            "                        if self.opts.get(\"auth_events\") is True:",
            "                            self.event.fire_event(",
            "                                eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                            )",
            "                        if sign_messages:",
            "                            return self._clear_signed(",
            "                                {\"ret\": False, \"nonce\": load[\"nonce\"]}",
            "                            )",
            "                        else:",
            "                            return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "                    else:",
            "                        log.info(",
            "                            \"Authentication failed from host %s, the key is in \"",
            "                            \"pending and needs to be accepted with salt-key \"",
            "                            \"-a %s\",",
            "                            load[\"id\"],",
            "                            load[\"id\"],",
            "                        )",
            "                        eload = {",
            "                            \"result\": True,",
            "                            \"act\": \"pend\",",
            "                            \"id\": load[\"id\"],",
            "                            \"pub\": load[\"pub\"],",
            "                        }",
            "                        if self.opts.get(\"auth_events\") is True:",
            "                            self.event.fire_event(",
            "                                eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                            )",
            "                        if sign_messages:",
            "                            return self._clear_signed(",
            "                                {\"ret\": True, \"nonce\": load[\"nonce\"]}",
            "                            )",
            "                        else:",
            "                            return {\"enc\": \"clear\", \"load\": {\"ret\": True}}",
            "            else:",
            "                # This key is in pending and has been configured to be",
            "                # auto-signed. Check to see if it is the same key, and if",
            "                # so, pass on doing anything here, and let it get automatically",
            "                # accepted below.",
            "                with salt.utils.files.fopen(pubfn_pend, \"r\") as pubfn_handle:",
            "                    if pubfn_handle.read() != load[\"pub\"]:",
            "                        log.error(",
            "                            \"Authentication attempt from %s failed, the public \"",
            "                            \"keys in pending did not match. This may be an \"",
            "                            \"attempt to compromise the Salt cluster.\",",
            "                            load[\"id\"],",
            "                        )",
            "                        # put denied minion key into minions_denied",
            "                        with salt.utils.files.fopen(pubfn_denied, \"w+\") as fp_:",
            "                            fp_.write(load[\"pub\"])",
            "                        eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "                        if self.opts.get(\"auth_events\") is True:",
            "                            self.event.fire_event(",
            "                                eload, salt.utils.event.tagify(prefix=\"auth\")",
            "                            )",
            "                        if sign_messages:",
            "                            return self._clear_signed(",
            "                                {\"ret\": False, \"nonce\": load[\"nonce\"]}",
            "                            )",
            "                        else:",
            "                            return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "                    else:",
            "                        os.remove(pubfn_pend)",
            "",
            "        else:",
            "            # Something happened that I have not accounted for, FAIL!",
            "            log.warning(\"Unaccounted for authentication failure\")",
            "            eload = {\"result\": False, \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "            if self.opts.get(\"auth_events\") is True:",
            "                self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        log.info(\"Authentication accepted from %s\", load[\"id\"])",
            "        # only write to disk if you are adding the file, and in open mode,",
            "        # which implies we accept any key from a minion.",
            "        if not os.path.isfile(pubfn) and not self.opts[\"open_mode\"]:",
            "            with salt.utils.files.fopen(pubfn, \"w+\") as fp_:",
            "                fp_.write(load[\"pub\"])",
            "        elif self.opts[\"open_mode\"]:",
            "            disk_key = \"\"",
            "            if os.path.isfile(pubfn):",
            "                with salt.utils.files.fopen(pubfn, \"r\") as fp_:",
            "                    disk_key = fp_.read()",
            "            if load[\"pub\"] and load[\"pub\"] != disk_key:",
            "                log.debug(\"Host key change detected in open mode.\")",
            "                with salt.utils.files.fopen(pubfn, \"w+\") as fp_:",
            "                    fp_.write(load[\"pub\"])",
            "            elif not load[\"pub\"]:",
            "                log.error(\"Public key is empty: %s\", load[\"id\"])",
            "                if sign_messages:",
            "                    return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "                else:",
            "                    return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        pub = None",
            "",
            "        # the con_cache is enabled, send the minion id to the cache",
            "        if self.cache_cli:",
            "            self.cache_cli.put_cache([load[\"id\"]])",
            "",
            "        # The key payload may sometimes be corrupt when using auto-accept",
            "        # and an empty request comes in",
            "        try:",
            "            pub = salt.crypt.get_rsa_pub_key(pubfn)",
            "        except (ValueError, IndexError, TypeError) as err:",
            "            log.error('Corrupt public key \"%s\": %s', pubfn, err)",
            "            if sign_messages:",
            "                return self._clear_signed({\"ret\": False, \"nonce\": load[\"nonce\"]})",
            "            else:",
            "                return {\"enc\": \"clear\", \"load\": {\"ret\": False}}",
            "",
            "        if not HAS_M2:",
            "            cipher = PKCS1_OAEP.new(pub)",
            "        ret = {",
            "            \"enc\": \"pub\",",
            "            \"pub_key\": self.master_key.get_pub_str(),",
            "            \"publish_port\": self.opts[\"publish_port\"],",
            "        }",
            "",
            "        # sign the master's pubkey (if enabled) before it is",
            "        # sent to the minion that was just authenticated",
            "        if self.opts[\"master_sign_pubkey\"]:",
            "            # append the pre-computed signature to the auth-reply",
            "            if self.master_key.pubkey_signature():",
            "                log.debug(\"Adding pubkey signature to auth-reply\")",
            "                log.debug(self.master_key.pubkey_signature())",
            "                ret.update({\"pub_sig\": self.master_key.pubkey_signature()})",
            "            else:",
            "                # the master has its own signing-keypair, compute the master.pub's",
            "                # signature and append that to the auth-reply",
            "",
            "                # get the key_pass for the signing key",
            "                key_pass = salt.utils.sdb.sdb_get(",
            "                    self.opts[\"signing_key_pass\"], self.opts",
            "                )",
            "",
            "                log.debug(\"Signing master public key before sending\")",
            "                pub_sign = salt.crypt.sign_message(",
            "                    self.master_key.get_sign_paths()[1], ret[\"pub_key\"], key_pass",
            "                )",
            "                ret.update({\"pub_sig\": binascii.b2a_base64(pub_sign)})",
            "",
            "        if not HAS_M2:",
            "            mcipher = PKCS1_OAEP.new(self.master_key.key)",
            "        if self.opts[\"auth_mode\"] >= 2:",
            "            if \"token\" in load:",
            "                try:",
            "                    if HAS_M2:",
            "                        mtoken = self.master_key.key.private_decrypt(",
            "                            load[\"token\"], RSA.pkcs1_oaep_padding",
            "                        )",
            "                    else:",
            "                        mtoken = mcipher.decrypt(load[\"token\"])",
            "                    aes = \"{}_|-{}\".format(",
            "                        salt.master.SMaster.secrets[\"aes\"][\"secret\"].value, mtoken",
            "                    )",
            "                except Exception:  # pylint: disable=broad-except",
            "                    # Token failed to decrypt, send back the salty bacon to",
            "                    # support older minions",
            "                    pass",
            "            else:",
            "                aes = salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "",
            "            if HAS_M2:",
            "                ret[\"aes\"] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)",
            "            else:",
            "                ret[\"aes\"] = cipher.encrypt(aes)",
            "        else:",
            "            if \"token\" in load:",
            "                try:",
            "                    if HAS_M2:",
            "                        mtoken = self.master_key.key.private_decrypt(",
            "                            load[\"token\"], RSA.pkcs1_oaep_padding",
            "                        )",
            "                        ret[\"token\"] = pub.public_encrypt(",
            "                            mtoken, RSA.pkcs1_oaep_padding",
            "                        )",
            "                    else:",
            "                        mtoken = mcipher.decrypt(load[\"token\"])",
            "                        ret[\"token\"] = cipher.encrypt(mtoken)",
            "                except Exception:  # pylint: disable=broad-except",
            "                    # Token failed to decrypt, send back the salty bacon to",
            "                    # support older minions",
            "                    pass",
            "",
            "            aes = salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "            if HAS_M2:",
            "                ret[\"aes\"] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)",
            "            else:",
            "                ret[\"aes\"] = cipher.encrypt(aes)",
            "",
            "        # Be aggressive about the signature",
            "        digest = salt.utils.stringutils.to_bytes(hashlib.sha256(aes).hexdigest())",
            "        ret[\"sig\"] = salt.crypt.private_encrypt(self.master_key.key, digest)",
            "        eload = {\"result\": True, \"act\": \"accept\", \"id\": load[\"id\"], \"pub\": load[\"pub\"]}",
            "        if self.opts.get(\"auth_events\") is True:",
            "            self.event.fire_event(eload, salt.utils.event.tagify(prefix=\"auth\"))",
            "        if sign_messages:",
            "            ret[\"nonce\"] = load[\"nonce\"]",
            "            return self._clear_signed(ret)",
            "        return ret"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "salt.transport.mixins.auth.AESReqServerMixin._auth.eload",
            "shuup.front.urls",
            "salt.transport.mixins.auth.AESReqServerMixin._clear_signed.load"
        ]
    },
    "salt/transport/tcp.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 410,
                "afterPatchRowNumber": 410,
                "PatchRowcode": "         Indeed, we can fail too early in case of a master restart during a"
            },
            "1": {
                "beforePatchRowNumber": 411,
                "afterPatchRowNumber": 411,
                "PatchRowcode": "         minion state execution call"
            },
            "2": {
                "beforePatchRowNumber": 412,
                "afterPatchRowNumber": 412,
                "PatchRowcode": "         \"\"\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 413,
                "PatchRowcode": "+        nonce = uuid.uuid4().hex"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 414,
                "PatchRowcode": "+        if load and isinstance(load, dict):"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 415,
                "PatchRowcode": "+            load[\"nonce\"] = nonce"
            },
            "6": {
                "beforePatchRowNumber": 413,
                "afterPatchRowNumber": 416,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 414,
                "afterPatchRowNumber": 417,
                "PatchRowcode": "         @salt.ext.tornado.gen.coroutine"
            },
            "8": {
                "beforePatchRowNumber": 415,
                "afterPatchRowNumber": 418,
                "PatchRowcode": "         def _do_transfer():"
            },
            "9": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 424,
                "PatchRowcode": "             # communication, we do not subscribe to return events, we just"
            },
            "10": {
                "beforePatchRowNumber": 422,
                "afterPatchRowNumber": 425,
                "PatchRowcode": "             # upload the results to the master"
            },
            "11": {
                "beforePatchRowNumber": 423,
                "afterPatchRowNumber": 426,
                "PatchRowcode": "             if data:"
            },
            "12": {
                "beforePatchRowNumber": 424,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                data = self.auth.crypticle.loads(data)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 427,
                "PatchRowcode": "+                data = self.auth.crypticle.loads(data, nonce=nonce)"
            },
            "14": {
                "beforePatchRowNumber": 425,
                "afterPatchRowNumber": 428,
                "PatchRowcode": "                 data = salt.transport.frame.decode_embedded_strs(data)"
            },
            "15": {
                "beforePatchRowNumber": 426,
                "afterPatchRowNumber": 429,
                "PatchRowcode": "             raise salt.ext.tornado.gen.Return(data)"
            },
            "16": {
                "beforePatchRowNumber": 427,
                "afterPatchRowNumber": 430,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 834,
                "afterPatchRowNumber": 837,
                "PatchRowcode": "             elif req_fun == \"send\":"
            },
            "18": {
                "beforePatchRowNumber": 835,
                "afterPatchRowNumber": 838,
                "PatchRowcode": "                 stream.write("
            },
            "19": {
                "beforePatchRowNumber": 836,
                "afterPatchRowNumber": 839,
                "PatchRowcode": "                     salt.transport.frame.frame_msg("
            },
            "20": {
                "beforePatchRowNumber": 837,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        self.crypticle.dumps(ret), header=header"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 840,
                "PatchRowcode": "+                        self.crypticle.dumps(ret, nonce), header=header"
            },
            "22": {
                "beforePatchRowNumber": 838,
                "afterPatchRowNumber": 841,
                "PatchRowcode": "                     )"
            },
            "23": {
                "beforePatchRowNumber": 839,
                "afterPatchRowNumber": 842,
                "PatchRowcode": "                 )"
            },
            "24": {
                "beforePatchRowNumber": 840,
                "afterPatchRowNumber": 843,
                "PatchRowcode": "             elif req_fun == \"send_private\":"
            },
            "25": {
                "beforePatchRowNumber": 1672,
                "afterPatchRowNumber": 1675,
                "PatchRowcode": "         Publish \"load\" to minions"
            },
            "26": {
                "beforePatchRowNumber": 1673,
                "afterPatchRowNumber": 1676,
                "PatchRowcode": "         \"\"\""
            },
            "27": {
                "beforePatchRowNumber": 1674,
                "afterPatchRowNumber": 1677,
                "PatchRowcode": "         payload = {\"enc\": \"aes\"}"
            },
            "28": {
                "beforePatchRowNumber": 1675,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1678,
                "PatchRowcode": "+        load[\"serial\"] = salt.master.SMaster.get_serial()"
            },
            "30": {
                "beforePatchRowNumber": 1676,
                "afterPatchRowNumber": 1679,
                "PatchRowcode": "         crypticle = salt.crypt.Crypticle("
            },
            "31": {
                "beforePatchRowNumber": 1677,
                "afterPatchRowNumber": 1680,
                "PatchRowcode": "             self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value"
            },
            "32": {
                "beforePatchRowNumber": 1678,
                "afterPatchRowNumber": 1681,
                "PatchRowcode": "         )"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "TCP transport classes",
            "",
            "Wire protocol: \"len(payload) msgpack({'head': SOMEHEADER, 'body': SOMEBODY})\"",
            "",
            "\"\"\"",
            "",
            "import errno",
            "import logging",
            "import os",
            "import queue",
            "import socket",
            "import threading",
            "import time",
            "import traceback",
            "import urllib.parse as urlparse",
            "import uuid",
            "import weakref",
            "",
            "import salt.crypt",
            "import salt.exceptions",
            "import salt.ext.tornado",
            "import salt.ext.tornado.concurrent",
            "import salt.ext.tornado.gen",
            "import salt.ext.tornado.iostream",
            "import salt.ext.tornado.netutil",
            "import salt.ext.tornado.tcpclient",
            "import salt.ext.tornado.tcpserver",
            "import salt.payload",
            "import salt.transport.client",
            "import salt.transport.frame",
            "import salt.transport.ipc",
            "import salt.transport.mixins.auth",
            "import salt.transport.server",
            "import salt.utils.asynchronous",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.msgpack",
            "import salt.utils.platform",
            "import salt.utils.process",
            "import salt.utils.verify",
            "from salt.exceptions import SaltClientError, SaltReqTimeoutError",
            "from salt.transport import iter_transport_opts",
            "",
            "try:",
            "    from M2Crypto import RSA",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "    try:",
            "        from Cryptodome.Cipher import PKCS1_OAEP",
            "    except ImportError:",
            "        from Crypto.Cipher import PKCS1_OAEP  # nosec",
            "",
            "if salt.utils.platform.is_windows():",
            "    USE_LOAD_BALANCER = True",
            "else:",
            "    USE_LOAD_BALANCER = False",
            "",
            "if USE_LOAD_BALANCER:",
            "    import threading",
            "    import multiprocessing",
            "    import salt.ext.tornado.util",
            "    from salt.utils.process import SignalHandlingProcess",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def _set_tcp_keepalive(sock, opts):",
            "    \"\"\"",
            "    Ensure that TCP keepalives are set for the socket.",
            "    \"\"\"",
            "    if hasattr(socket, \"SO_KEEPALIVE\"):",
            "        if opts.get(\"tcp_keepalive\", False):",
            "            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)",
            "            if hasattr(socket, \"SOL_TCP\"):",
            "                if hasattr(socket, \"TCP_KEEPIDLE\"):",
            "                    tcp_keepalive_idle = opts.get(\"tcp_keepalive_idle\", -1)",
            "                    if tcp_keepalive_idle > 0:",
            "                        sock.setsockopt(",
            "                            socket.SOL_TCP, socket.TCP_KEEPIDLE, int(tcp_keepalive_idle)",
            "                        )",
            "                if hasattr(socket, \"TCP_KEEPCNT\"):",
            "                    tcp_keepalive_cnt = opts.get(\"tcp_keepalive_cnt\", -1)",
            "                    if tcp_keepalive_cnt > 0:",
            "                        sock.setsockopt(",
            "                            socket.SOL_TCP, socket.TCP_KEEPCNT, int(tcp_keepalive_cnt)",
            "                        )",
            "                if hasattr(socket, \"TCP_KEEPINTVL\"):",
            "                    tcp_keepalive_intvl = opts.get(\"tcp_keepalive_intvl\", -1)",
            "                    if tcp_keepalive_intvl > 0:",
            "                        sock.setsockopt(",
            "                            socket.SOL_TCP,",
            "                            socket.TCP_KEEPINTVL,",
            "                            int(tcp_keepalive_intvl),",
            "                        )",
            "            if hasattr(socket, \"SIO_KEEPALIVE_VALS\"):",
            "                # Windows doesn't support TCP_KEEPIDLE, TCP_KEEPCNT, nor",
            "                # TCP_KEEPINTVL. Instead, it has its own proprietary",
            "                # SIO_KEEPALIVE_VALS.",
            "                tcp_keepalive_idle = opts.get(\"tcp_keepalive_idle\", -1)",
            "                tcp_keepalive_intvl = opts.get(\"tcp_keepalive_intvl\", -1)",
            "                # Windows doesn't support changing something equivalent to",
            "                # TCP_KEEPCNT.",
            "                if tcp_keepalive_idle > 0 or tcp_keepalive_intvl > 0:",
            "                    # Windows defaults may be found by using the link below.",
            "                    # Search for 'KeepAliveTime' and 'KeepAliveInterval'.",
            "                    # https://technet.microsoft.com/en-us/library/bb726981.aspx#EDAA",
            "                    # If one value is set and the other isn't, we still need",
            "                    # to send both values to SIO_KEEPALIVE_VALS and they both",
            "                    # need to be valid. So in that case, use the Windows",
            "                    # default.",
            "                    if tcp_keepalive_idle <= 0:",
            "                        tcp_keepalive_idle = 7200",
            "                    if tcp_keepalive_intvl <= 0:",
            "                        tcp_keepalive_intvl = 1",
            "                    # The values expected are in milliseconds, so multiply by",
            "                    # 1000.",
            "                    sock.ioctl(",
            "                        socket.SIO_KEEPALIVE_VALS,",
            "                        (",
            "                            1,",
            "                            int(tcp_keepalive_idle * 1000),",
            "                            int(tcp_keepalive_intvl * 1000),",
            "                        ),",
            "                    )",
            "        else:",
            "            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 0)",
            "",
            "",
            "if USE_LOAD_BALANCER:",
            "",
            "    class LoadBalancerServer(SignalHandlingProcess):",
            "        \"\"\"",
            "        Raw TCP server which runs in its own process and will listen",
            "        for incoming connections. Each incoming connection will be",
            "        sent via multiprocessing queue to the workers.",
            "        Since the queue is shared amongst workers, only one worker will",
            "        handle a given connection.",
            "        \"\"\"",
            "",
            "        # TODO: opts!",
            "        # Based on default used in salt.ext.tornado.netutil.bind_sockets()",
            "        backlog = 128",
            "",
            "        def __init__(self, opts, socket_queue, **kwargs):",
            "            super().__init__(**kwargs)",
            "            self.opts = opts",
            "            self.socket_queue = socket_queue",
            "            self._socket = None",
            "",
            "        # __setstate__ and __getstate__ are only used on Windows.",
            "        # We do this so that __init__ will be invoked on Windows in the child",
            "        # process so that a register_after_fork() equivalent will work on",
            "        # Windows.",
            "        def __setstate__(self, state):",
            "            self.__init__(",
            "                state[\"opts\"],",
            "                state[\"socket_queue\"],",
            "                log_queue=state[\"log_queue\"],",
            "                log_queue_level=state[\"log_queue_level\"],",
            "            )",
            "",
            "        def __getstate__(self):",
            "            return {",
            "                \"opts\": self.opts,",
            "                \"socket_queue\": self.socket_queue,",
            "                \"log_queue\": self.log_queue,",
            "                \"log_queue_level\": self.log_queue_level,",
            "            }",
            "",
            "        def close(self):",
            "            if self._socket is not None:",
            "                self._socket.shutdown(socket.SHUT_RDWR)",
            "                self._socket.close()",
            "                self._socket = None",
            "",
            "        # pylint: disable=W1701",
            "        def __del__(self):",
            "            self.close()",
            "",
            "        # pylint: enable=W1701",
            "",
            "        def run(self):",
            "            \"\"\"",
            "            Start the load balancer",
            "            \"\"\"",
            "            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "            _set_tcp_keepalive(self._socket, self.opts)",
            "            self._socket.setblocking(1)",
            "            self._socket.bind((self.opts[\"interface\"], int(self.opts[\"ret_port\"])))",
            "            self._socket.listen(self.backlog)",
            "",
            "            while True:",
            "                try:",
            "                    # Wait for a connection to occur since the socket is",
            "                    # blocking.",
            "                    connection, address = self._socket.accept()",
            "                    # Wait for a free slot to be available to put",
            "                    # the connection into.",
            "                    # Sockets are picklable on Windows in Python 3.",
            "                    self.socket_queue.put((connection, address), True, None)",
            "                except OSError as e:",
            "                    # ECONNABORTED indicates that there was a connection",
            "                    # but it was closed while still in the accept queue.",
            "                    # (observed on FreeBSD).",
            "                    if (",
            "                        salt.ext.tornado.util.errno_from_exception(e)",
            "                        == errno.ECONNABORTED",
            "                    ):",
            "                        continue",
            "                    raise",
            "",
            "",
            "# TODO: move serial down into message library",
            "class AsyncTCPReqChannel(salt.transport.client.ReqChannel):",
            "    \"\"\"",
            "    Encapsulate sending routines to tcp.",
            "",
            "    Note: this class returns a singleton",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    # mapping of io_loop -> {key -> channel}",
            "    instance_map = weakref.WeakKeyDictionary()",
            "    async_methods = [",
            "        \"crypted_transfer_decode_dictentry\",",
            "        \"_crypted_transfer\",",
            "        \"_uncrypted_transfer\",",
            "        \"send\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __new__(cls, opts, **kwargs):",
            "        \"\"\"",
            "        Only create one instance of channel per __key()",
            "        \"\"\"",
            "        # do we have any mapping for this io_loop",
            "        io_loop = kwargs.get(\"io_loop\") or salt.ext.tornado.ioloop.IOLoop.current()",
            "        if io_loop not in cls.instance_map:",
            "            cls.instance_map[io_loop] = weakref.WeakValueDictionary()",
            "        loop_instance_map = cls.instance_map[io_loop]",
            "",
            "        key = cls.__key(opts, **kwargs)",
            "        obj = loop_instance_map.get(key)",
            "        if obj is None:",
            "            log.debug(\"Initializing new AsyncTCPReqChannel for %s\", key)",
            "            # we need to make a local variable for this, as we are going to store",
            "            # it in a WeakValueDictionary-- which will remove the item if no one",
            "            # references it-- this forces a reference while we return to the caller",
            "            obj = object.__new__(cls)",
            "            obj.__singleton_init__(opts, **kwargs)",
            "            obj._instance_key = key",
            "            loop_instance_map[key] = obj",
            "            obj._refcount = 1",
            "            obj._refcount_lock = threading.RLock()",
            "        else:",
            "            with obj._refcount_lock:",
            "                obj._refcount += 1",
            "            log.debug(\"Re-using AsyncTCPReqChannel for %s\", key)",
            "        return obj",
            "",
            "    @classmethod",
            "    def __key(cls, opts, **kwargs):",
            "        if \"master_uri\" in kwargs:",
            "            opts[\"master_uri\"] = kwargs[\"master_uri\"]",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            opts[\"master_uri\"],",
            "            kwargs.get(\"crypt\", \"aes\"),  # TODO: use the same channel for crypt",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, **kwargs):",
            "        pass",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, **kwargs):",
            "        self.opts = dict(opts)",
            "",
            "        self.serial = salt.payload.Serial(self.opts)",
            "",
            "        # crypt defaults to 'aes'",
            "        self.crypt = kwargs.get(\"crypt\", \"aes\")",
            "",
            "        self.io_loop = kwargs.get(\"io_loop\") or salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        if self.crypt != \"clear\":",
            "            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)",
            "",
            "        resolver = kwargs.get(\"resolver\")",
            "",
            "        parse = urlparse.urlparse(self.opts[\"master_uri\"])",
            "        master_host, master_port = parse.netloc.rsplit(\":\", 1)",
            "        self.master_addr = (master_host, int(master_port))",
            "        self._closing = False",
            "        self.message_client = SaltMessageClientPool(",
            "            self.opts,",
            "            args=(self.opts, master_host, int(master_port),),",
            "            kwargs={",
            "                \"io_loop\": self.io_loop,",
            "                \"resolver\": resolver,",
            "                \"source_ip\": self.opts.get(\"source_ip\"),",
            "                \"source_port\": self.opts.get(\"source_ret_port\"),",
            "            },",
            "        )",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "",
            "        if self._refcount > 1:",
            "            # Decrease refcount",
            "            with self._refcount_lock:",
            "                self._refcount -= 1",
            "            log.debug(",
            "                \"This is not the last %s instance. Not closing yet.\",",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "",
            "        log.debug(\"Closing %s instance\", self.__class__.__name__)",
            "        self._closing = True",
            "        self.message_client.close()",
            "",
            "        # Remove the entry from the instance map so that a closed entry may not",
            "        # be reused.",
            "        # This forces this operation even if the reference count of the entry",
            "        # has not yet gone to zero.",
            "        if self.io_loop in self.__class__.instance_map:",
            "            loop_instance_map = self.__class__.instance_map[self.io_loop]",
            "            if self._instance_key in loop_instance_map:",
            "                del loop_instance_map[self._instance_key]",
            "            if not loop_instance_map:",
            "                del self.__class__.instance_map[self.io_loop]",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        with self._refcount_lock:",
            "            # Make sure we actually close no matter if something",
            "            # went wrong with our ref counting",
            "            self._refcount = 1",
            "        try:",
            "            self.close()",
            "        except OSError as exc:",
            "            if exc.errno != errno.EBADF:",
            "                # If its not a bad file descriptor error, raise",
            "                raise",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _package_load(self, load):",
            "        return {",
            "            \"enc\": self.crypt,",
            "            \"load\": load,",
            "            \"version\": 2,",
            "        }",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def crypted_transfer_decode_dictentry(",
            "        self, load, dictkey=None, tries=3, timeout=60",
            "    ):",
            "        nonce = uuid.uuid4().hex",
            "        load[\"nonce\"] = nonce",
            "        if not self.auth.authenticated:",
            "            yield self.auth.authenticate()",
            "        ret = yield self.message_client.send(",
            "            self._package_load(self.auth.crypticle.dumps(load)), timeout=timeout",
            "        )",
            "        key = self.auth.get_keys()",
            "        if HAS_M2:",
            "            aes = key.private_decrypt(ret[\"key\"], RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(key)",
            "            aes = cipher.decrypt(ret[\"key\"])",
            "",
            "        # Decrypt using the public key.",
            "        pcrypt = salt.crypt.Crypticle(self.opts, aes)",
            "        signed_msg = pcrypt.loads(ret[dictkey])",
            "",
            "        # Validate the master's signature.",
            "        master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion_master.pub\")",
            "        if not salt.crypt.verify_signature(",
            "            master_pubkey_path, signed_msg[\"data\"], signed_msg[\"sig\"]",
            "        ):",
            "            raise salt.crypt.AuthenticationError(",
            "                \"Pillar payload signature failed to validate.\"",
            "            )",
            "",
            "        # Make sure the signed key matches the key we used to decrypt the data.",
            "        data = salt.payload.Serial({}).loads(signed_msg[\"data\"])",
            "        if data[\"key\"] != ret[\"key\"]:",
            "            raise salt.crypt.AuthenticationError(\"Key verification failed.\")",
            "",
            "        # Validate the nonce.",
            "        if data[\"nonce\"] != nonce:",
            "            raise salt.crypt.AuthenticationError(\"Pillar nonce verification failed.\")",
            "        raise salt.ext.tornado.gen.Return(data[\"pillar\"])",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _crypted_transfer(self, load, tries=3, timeout=60):",
            "        \"\"\"",
            "        In case of authentication errors, try to renegotiate authentication",
            "        and retry the method.",
            "        Indeed, we can fail too early in case of a master restart during a",
            "        minion state execution call",
            "        \"\"\"",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def _do_transfer():",
            "            data = yield self.message_client.send(",
            "                self._package_load(self.auth.crypticle.dumps(load)), timeout=timeout,",
            "            )",
            "            # we may not have always data",
            "            # as for example for saltcall ret submission, this is a blind",
            "            # communication, we do not subscribe to return events, we just",
            "            # upload the results to the master",
            "            if data:",
            "                data = self.auth.crypticle.loads(data)",
            "                data = salt.transport.frame.decode_embedded_strs(data)",
            "            raise salt.ext.tornado.gen.Return(data)",
            "",
            "        if not self.auth.authenticated:",
            "            yield self.auth.authenticate()",
            "        try:",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "        except salt.crypt.AuthenticationError:",
            "            yield self.auth.authenticate()",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _uncrypted_transfer(self, load, tries=3, timeout=60):",
            "        ret = yield self.message_client.send(self._package_load(load), timeout=timeout)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def send(self, load, tries=3, timeout=60, raw=False):",
            "        \"\"\"",
            "        Send a request, return a future which will complete when we send the message",
            "        \"\"\"",
            "        try:",
            "            if self.crypt == \"clear\":",
            "                ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)",
            "            else:",
            "                ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout)",
            "        except salt.ext.tornado.iostream.StreamClosedError:",
            "            # Convert to 'SaltClientError' so that clients can handle this",
            "            # exception more appropriately.",
            "            raise SaltClientError(\"Connection to master lost\")",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "",
            "class AsyncTCPPubChannel(",
            "    salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel",
            "):",
            "    async_methods = [",
            "        \"send_id\",",
            "        \"connect_callback\",",
            "        \"connect\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        self.opts = opts",
            "",
            "        self.serial = salt.payload.Serial(self.opts)",
            "",
            "        self.crypt = kwargs.get(\"crypt\", \"aes\")",
            "        self.io_loop = kwargs.get(\"io_loop\") or salt.ext.tornado.ioloop.IOLoop.current()",
            "        self.connected = False",
            "        self._closing = False",
            "        self._reconnected = False",
            "        self.event = salt.utils.event.get_event(\"minion\", opts=self.opts, listen=False)",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "        if hasattr(self, \"message_client\"):",
            "            self.message_client.close()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _package_load(self, load):",
            "        return {",
            "            \"enc\": self.crypt,",
            "            \"load\": load,",
            "            \"version\": 2,",
            "        }",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def send_id(self, tok, force_auth):",
            "        \"\"\"",
            "        Send the minion id to the master so that the master may better",
            "        track the connection state of the minion.",
            "        In case of authentication errors, try to renegotiate authentication",
            "        and retry the method.",
            "        \"\"\"",
            "        load = {\"id\": self.opts[\"id\"], \"tok\": tok}",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def _do_transfer():",
            "            msg = self._package_load(self.auth.crypticle.dumps(load))",
            "            package = salt.transport.frame.frame_msg(msg, header=None)",
            "            yield self.message_client.write_to_stream(package)",
            "            raise salt.ext.tornado.gen.Return(True)",
            "",
            "        if force_auth or not self.auth.authenticated:",
            "            count = 0",
            "            while (",
            "                count <= self.opts[\"tcp_authentication_retries\"]",
            "                or self.opts[\"tcp_authentication_retries\"] < 0",
            "            ):",
            "                try:",
            "                    yield self.auth.authenticate()",
            "                    break",
            "                except SaltClientError as exc:",
            "                    log.debug(exc)",
            "                    count += 1",
            "        try:",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "        except salt.crypt.AuthenticationError:",
            "            yield self.auth.authenticate()",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect_callback(self, result):",
            "        if self._closing:",
            "            return",
            "        # Force re-auth on reconnect since the master",
            "        # may have been restarted",
            "        yield self.send_id(self.tok, self._reconnected)",
            "        self.connected = True",
            "        self.event.fire_event({\"master\": self.opts[\"master\"]}, \"__master_connected\")",
            "        if self._reconnected:",
            "            # On reconnects, fire a master event to notify that the minion is",
            "            # available.",
            "            if self.opts.get(\"__role\") == \"syndic\":",
            "                data = \"Syndic {} started at {}\".format(self.opts[\"id\"], time.asctime())",
            "                tag = salt.utils.event.tagify([self.opts[\"id\"], \"start\"], \"syndic\")",
            "            else:",
            "                data = \"Minion {} started at {}\".format(self.opts[\"id\"], time.asctime())",
            "                tag = salt.utils.event.tagify([self.opts[\"id\"], \"start\"], \"minion\")",
            "            load = {",
            "                \"id\": self.opts[\"id\"],",
            "                \"cmd\": \"_minion_event\",",
            "                \"pretag\": None,",
            "                \"tok\": self.tok,",
            "                \"data\": data,",
            "                \"tag\": tag,",
            "            }",
            "            req_channel = salt.utils.asynchronous.SyncWrapper(",
            "                AsyncTCPReqChannel, (self.opts,), loop_kwarg=\"io_loop\",",
            "            )",
            "            try:",
            "                req_channel.send(load, timeout=60)",
            "            except salt.exceptions.SaltReqTimeoutError:",
            "                log.info(",
            "                    \"fire_master failed: master could not be contacted. Request timed out.\"",
            "                )",
            "            except Exception:  # pylint: disable=broad-except",
            "                log.info(\"fire_master failed: %s\", traceback.format_exc())",
            "            finally:",
            "                # SyncWrapper will call either close() or destroy(), whichever is available",
            "                del req_channel",
            "        else:",
            "            self._reconnected = True",
            "",
            "    def disconnect_callback(self):",
            "        if self._closing:",
            "            return",
            "        self.connected = False",
            "        self.event.fire_event({\"master\": self.opts[\"master\"]}, \"__master_disconnected\")",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect(self):",
            "        try:",
            "            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)",
            "            self.tok = self.auth.gen_token(b\"salt\")",
            "            if not self.auth.authenticated:",
            "                yield self.auth.authenticate()",
            "            if self.auth.authenticated:",
            "                # if this is changed from the default, we assume it was intentional",
            "                if int(self.opts.get(\"publish_port\", 4505)) != 4505:",
            "                    self.publish_port = self.opts.get(\"publish_port\")",
            "                # else take the relayed publish_port master reports",
            "                else:",
            "                    self.publish_port = self.auth.creds[\"publish_port\"]",
            "",
            "                self.message_client = SaltMessageClientPool(",
            "                    self.opts,",
            "                    args=(self.opts, self.opts[\"master_ip\"], int(self.publish_port),),",
            "                    kwargs={",
            "                        \"io_loop\": self.io_loop,",
            "                        \"connect_callback\": self.connect_callback,",
            "                        \"disconnect_callback\": self.disconnect_callback,",
            "                        \"source_ip\": self.opts.get(\"source_ip\"),",
            "                        \"source_port\": self.opts.get(\"source_publish_port\"),",
            "                    },",
            "                )",
            "                yield self.message_client.connect()  # wait for the client to be connected",
            "                self.connected = True",
            "        # TODO: better exception handling...",
            "        except KeyboardInterrupt:  # pylint: disable=try-except-raise",
            "            raise",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            if \"-|RETRY|-\" not in str(exc):",
            "                raise SaltClientError(",
            "                    \"Unable to sign_in to master: {}\".format(exc)",
            "                )  # TODO: better error message",
            "",
            "    def on_recv(self, callback):",
            "        \"\"\"",
            "        Register an on_recv callback",
            "        \"\"\"",
            "        if callback is None:",
            "            return self.message_client.on_recv(callback)",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def wrap_callback(body):",
            "            if not isinstance(body, dict):",
            "                # TODO: For some reason we need to decode here for things",
            "                #       to work. Fix this.",
            "                body = salt.utils.msgpack.loads(body)",
            "                body = salt.transport.frame.decode_embedded_strs(body)",
            "            ret = yield self._decode_payload(body)",
            "            callback(ret)",
            "",
            "        return self.message_client.on_recv(wrap_callback)",
            "",
            "",
            "class TCPReqServerChannel(",
            "    salt.transport.mixins.auth.AESReqServerMixin, salt.transport.server.ReqServerChannel",
            "):",
            "    # TODO: opts!",
            "    backlog = 5",
            "",
            "    def __init__(self, opts):",
            "        salt.transport.server.ReqServerChannel.__init__(self, opts)",
            "        self._socket = None",
            "",
            "    @property",
            "    def socket(self):",
            "        return self._socket",
            "",
            "    def close(self):",
            "        if self._socket is not None:",
            "            try:",
            "                self._socket.shutdown(socket.SHUT_RDWR)",
            "            except OSError as exc:",
            "                if exc.errno == errno.ENOTCONN:",
            "                    # We may try to shutdown a socket which is already disconnected.",
            "                    # Ignore this condition and continue.",
            "                    pass",
            "                else:",
            "                    raise",
            "            self._socket.close()",
            "            self._socket = None",
            "        if hasattr(self.req_server, \"shutdown\"):",
            "            try:",
            "                self.req_server.shutdown()",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                log.exception(",
            "                    \"TCPReqServerChannel close generated an exception: %s\", str(exc)",
            "                )",
            "        elif hasattr(self.req_server, \"stop\"):",
            "            try:",
            "                self.req_server.stop()",
            "            except OSError as exc:",
            "                if exc.errno != 9:",
            "                    raise",
            "                log.exception(",
            "                    \"TCPReqServerChannel close generated an exception: %s\", str(exc)",
            "                )",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def pre_fork(self, process_manager):",
            "        \"\"\"",
            "        Pre-fork we need to create the zmq router device",
            "        \"\"\"",
            "        salt.transport.mixins.auth.AESReqServerMixin.pre_fork(self, process_manager)",
            "        if USE_LOAD_BALANCER:",
            "            self.socket_queue = multiprocessing.Queue()",
            "            process_manager.add_process(",
            "                LoadBalancerServer, args=(self.opts, self.socket_queue)",
            "            )",
            "        elif not salt.utils.platform.is_windows():",
            "            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "            _set_tcp_keepalive(self._socket, self.opts)",
            "            self._socket.setblocking(0)",
            "            self._socket.bind((self.opts[\"interface\"], int(self.opts[\"ret_port\"])))",
            "",
            "    def post_fork(self, payload_handler, io_loop):",
            "        \"\"\"",
            "        After forking we need to create all of the local sockets to listen to the",
            "        router",
            "",
            "        payload_handler: function to call with your payloads",
            "        \"\"\"",
            "        if self.opts[\"pub_server_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting Publish daemon niceness to %i\",",
            "                self.opts[\"pub_server_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"pub_server_niceness\"])",
            "",
            "        self.payload_handler = payload_handler",
            "        self.io_loop = io_loop",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        with salt.utils.asynchronous.current_ioloop(self.io_loop):",
            "            if USE_LOAD_BALANCER:",
            "                self.req_server = LoadBalancerWorker(",
            "                    self.socket_queue,",
            "                    self.handle_message,",
            "                    ssl_options=self.opts.get(\"ssl\"),",
            "                )",
            "            else:",
            "                if salt.utils.platform.is_windows():",
            "                    self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "                    self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "                    _set_tcp_keepalive(self._socket, self.opts)",
            "                    self._socket.setblocking(0)",
            "                    self._socket.bind(",
            "                        (self.opts[\"interface\"], int(self.opts[\"ret_port\"]))",
            "                    )",
            "                self.req_server = SaltMessageServer(",
            "                    self.handle_message,",
            "                    ssl_options=self.opts.get(\"ssl\"),",
            "                    io_loop=self.io_loop,",
            "                )",
            "                self.req_server.add_socket(self._socket)",
            "                self._socket.listen(self.backlog)",
            "        salt.transport.mixins.auth.AESReqServerMixin.post_fork(",
            "            self, payload_handler, io_loop",
            "        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_message(self, stream, header, payload):",
            "        \"\"\"",
            "        Handle incoming messages from underlying tcp streams",
            "        \"\"\"",
            "        try:",
            "            try:",
            "                payload = self._decode_payload(payload)",
            "            except Exception:  # pylint: disable=broad-except",
            "                stream.write(salt.transport.frame.frame_msg(\"bad load\", header=header))",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            # TODO helper functions to normalize payload?",
            "            if not isinstance(payload, dict) or not isinstance(",
            "                payload.get(\"load\"), dict",
            "            ):",
            "                yield stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        \"payload and load must be a dict\", header=header",
            "                    )",
            "                )",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            try:",
            "                id_ = payload[\"load\"].get(\"id\", \"\")",
            "                if \"\\0\" in id_:",
            "                    log.error(\"Payload contains an id with a null byte: %s\", payload)",
            "                    stream.send(self.serial.dumps(\"bad load: id contains a null byte\"))",
            "                    raise salt.ext.tornado.gen.Return()",
            "            except TypeError:",
            "                log.error(\"Payload contains non-string id: %s\", payload)",
            "                stream.send(",
            "                    self.serial.dumps(\"bad load: id {} is not a string\".format(id_))",
            "                )",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            version = 0",
            "            if \"version\" in payload:",
            "                version = payload[\"version\"]",
            "",
            "            sign_messages = False",
            "            if version > 1:",
            "                sign_messages = True",
            "",
            "            # intercept the \"_auth\" commands, since the main daemon shouldn't know",
            "            # anything about our key auth",
            "            if (",
            "                payload[\"enc\"] == \"clear\"",
            "                and payload.get(\"load\", {}).get(\"cmd\") == \"_auth\"",
            "            ):",
            "                yield stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        self._auth(payload[\"load\"], sign_messages), header=header",
            "                    )",
            "                )",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            nonce = None",
            "            if version > 1:",
            "                nonce = payload[\"load\"].pop(\"nonce\", None)",
            "",
            "            # TODO: test",
            "            try:",
            "                ret, req_opts = yield self.payload_handler(payload)",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                # always attempt to return an error to the minion",
            "                stream.write(\"Some exception handling minion payload\")",
            "                log.error(",
            "                    \"Some exception handling a payload from minion\", exc_info=True",
            "                )",
            "                stream.close()",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            req_fun = req_opts.get(\"fun\", \"send\")",
            "            if req_fun == \"send_clear\":",
            "                stream.write(salt.transport.frame.frame_msg(ret, header=header))",
            "            elif req_fun == \"send\":",
            "                stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        self.crypticle.dumps(ret), header=header",
            "                    )",
            "                )",
            "            elif req_fun == \"send_private\":",
            "                stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        self._encrypt_private(",
            "                            ret, req_opts[\"key\"], req_opts[\"tgt\"], nonce, sign_messages,",
            "                        ),",
            "                        header=header,",
            "                    )",
            "                )",
            "            else:",
            "                log.error(\"Unknown req_fun %s\", req_fun)",
            "                # always attempt to return an error to the minion",
            "                stream.write(\"Server-side exception handling payload\")",
            "                stream.close()",
            "        except salt.ext.tornado.gen.Return:",
            "            raise",
            "        except salt.ext.tornado.iostream.StreamClosedError:",
            "            # Stream was closed. This could happen if the remote side",
            "            # closed the connection on its end (eg in a timeout or shutdown",
            "            # situation).",
            "            log.error(\"Connection was unexpectedly closed\", exc_info=True)",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            # Absorb any other exceptions",
            "            log.error(\"Unexpected exception occurred: %s\", exc, exc_info=True)",
            "",
            "        raise salt.ext.tornado.gen.Return()",
            "",
            "",
            "class SaltMessageServer(salt.ext.tornado.tcpserver.TCPServer):",
            "    \"\"\"",
            "    Raw TCP server which will receive all of the TCP streams and re-assemble",
            "    messages that are sent through to us",
            "    \"\"\"",
            "",
            "    def __init__(self, message_handler, *args, **kwargs):",
            "        io_loop = (",
            "            kwargs.pop(\"io_loop\", None) or salt.ext.tornado.ioloop.IOLoop.current()",
            "        )",
            "        super().__init__(*args, **kwargs)",
            "        self.io_loop = io_loop",
            "        self.clients = []",
            "        self.message_handler = message_handler",
            "        self._shutting_down = False",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_stream(self, stream, address):",
            "        \"\"\"",
            "        Handle incoming streams and add messages to the incoming queue",
            "        \"\"\"",
            "        log.trace(\"Req client %s connected\", address)",
            "        self.clients.append((stream, address))",
            "        unpacker = salt.utils.msgpack.Unpacker()",
            "        try:",
            "            while True:",
            "                wire_bytes = yield stream.read_bytes(4096, partial=True)",
            "                unpacker.feed(wire_bytes)",
            "                for framed_msg in unpacker:",
            "                    framed_msg = salt.transport.frame.decode_embedded_strs(framed_msg)",
            "                    header = framed_msg[\"head\"]",
            "                    self.io_loop.spawn_callback(",
            "                        self.message_handler, stream, header, framed_msg[\"body\"]",
            "                    )",
            "",
            "        except salt.ext.tornado.iostream.StreamClosedError:",
            "            log.trace(\"req client disconnected %s\", address)",
            "            self.remove_client((stream, address))",
            "        except Exception as e:  # pylint: disable=broad-except",
            "            log.trace(\"other master-side exception: %s\", e)",
            "            self.remove_client((stream, address))",
            "            stream.close()",
            "",
            "    def remove_client(self, client):",
            "        try:",
            "            self.clients.remove(client)",
            "        except ValueError:",
            "            log.trace(\"Message server client was not in list to remove\")",
            "",
            "    def shutdown(self):",
            "        \"\"\"",
            "        Shutdown the whole server",
            "        \"\"\"",
            "        if self._shutting_down:",
            "            return",
            "        self._shutting_down = True",
            "        for item in self.clients:",
            "            client, address = item",
            "            client.close()",
            "            self.remove_client(item)",
            "        try:",
            "            self.stop()",
            "        except OSError as exc:",
            "            if exc.errno != 9:",
            "                raise",
            "",
            "",
            "if USE_LOAD_BALANCER:",
            "",
            "    class LoadBalancerWorker(SaltMessageServer):",
            "        \"\"\"",
            "        This will receive TCP connections from 'LoadBalancerServer' via",
            "        a multiprocessing queue.",
            "        Since the queue is shared amongst workers, only one worker will handle",
            "        a given connection.",
            "        \"\"\"",
            "",
            "        def __init__(self, socket_queue, message_handler, *args, **kwargs):",
            "            super().__init__(message_handler, *args, **kwargs)",
            "            self.socket_queue = socket_queue",
            "            self._stop = threading.Event()",
            "            self.thread = threading.Thread(target=self.socket_queue_thread)",
            "            self.thread.start()",
            "",
            "        def stop(self):",
            "            self._stop.set()",
            "            self.thread.join()",
            "",
            "        def socket_queue_thread(self):",
            "            try:",
            "                while True:",
            "                    try:",
            "                        client_socket, address = self.socket_queue.get(True, 1)",
            "                    except queue.Empty:",
            "                        if self._stop.is_set():",
            "                            break",
            "                        continue",
            "                    # 'self.io_loop' initialized in super class",
            "                    # 'salt.ext.tornado.tcpserver.TCPServer'.",
            "                    # 'self._handle_connection' defined in same super class.",
            "                    self.io_loop.spawn_callback(",
            "                        self._handle_connection, client_socket, address",
            "                    )",
            "            except (KeyboardInterrupt, SystemExit):",
            "                pass",
            "",
            "",
            "class TCPClientKeepAlive(salt.ext.tornado.tcpclient.TCPClient):",
            "    \"\"\"",
            "    Override _create_stream() in TCPClient to enable keep alive support.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, resolver=None):",
            "        self.opts = opts",
            "        super().__init__(resolver=resolver)",
            "",
            "    def _create_stream(",
            "        self, max_buffer_size, af, addr, **kwargs",
            "    ):  # pylint: disable=unused-argument,arguments-differ",
            "        \"\"\"",
            "        Override _create_stream() in TCPClient.",
            "",
            "        Tornado 4.5 added the kwargs 'source_ip' and 'source_port'.",
            "        Due to this, use **kwargs to swallow these and any future",
            "        kwargs to maintain compatibility.",
            "        \"\"\"",
            "        # Always connect in plaintext; we'll convert to ssl if necessary",
            "        # after one connection has completed.",
            "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "        _set_tcp_keepalive(sock, self.opts)",
            "        stream = salt.ext.tornado.iostream.IOStream(",
            "            sock, max_buffer_size=max_buffer_size",
            "        )",
            "        if salt.ext.tornado.version_info < (5,):",
            "            return stream.connect(addr)",
            "        return stream, stream.connect(addr)",
            "",
            "",
            "class SaltMessageClientPool(salt.transport.MessageClientPool):",
            "    \"\"\"",
            "    Wrapper class of SaltMessageClient to avoid blocking waiting while writing data to socket.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, args=None, kwargs=None):",
            "        super().__init__(SaltMessageClient, opts, args=args, kwargs=kwargs)",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def close(self):",
            "        for message_client in self.message_clients:",
            "            message_client.close()",
            "        self.message_clients = []",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect(self):",
            "        futures = []",
            "        for message_client in self.message_clients:",
            "            futures.append(message_client.connect())",
            "        yield futures",
            "        raise salt.ext.tornado.gen.Return(None)",
            "",
            "    def on_recv(self, *args, **kwargs):",
            "        for message_client in self.message_clients:",
            "            message_client.on_recv(*args, **kwargs)",
            "",
            "    def send(self, *args, **kwargs):",
            "        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))",
            "        return message_clients[0].send(*args, **kwargs)",
            "",
            "    def write_to_stream(self, *args, **kwargs):",
            "        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))",
            "        return message_clients[0]._stream.write(*args, **kwargs)",
            "",
            "",
            "# TODO consolidate with IPCClient",
            "# TODO: limit in-flight messages.",
            "# TODO: singleton? Something to not re-create the tcp connection so much",
            "class SaltMessageClient:",
            "    \"\"\"",
            "    Low-level message sending client",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        opts,",
            "        host,",
            "        port,",
            "        io_loop=None,",
            "        resolver=None,",
            "        connect_callback=None,",
            "        disconnect_callback=None,",
            "        source_ip=None,",
            "        source_port=None,",
            "    ):",
            "        self.opts = opts",
            "        self.host = host",
            "        self.port = port",
            "        self.source_ip = source_ip",
            "        self.source_port = source_port",
            "        self.connect_callback = connect_callback",
            "        self.disconnect_callback = disconnect_callback",
            "",
            "        self.io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        with salt.utils.asynchronous.current_ioloop(self.io_loop):",
            "            self._tcp_client = TCPClientKeepAlive(opts, resolver=resolver)",
            "",
            "        self._mid = 1",
            "        self._max_messages = int((1 << 31) - 2)  # number of IDs before we wrap",
            "",
            "        # TODO: max queue size",
            "        self.send_queue = []  # queue of messages to be sent",
            "        self.send_future_map = {}  # mapping of request_id -> Future",
            "        self.send_timeout_map = {}  # request_id -> timeout_callback",
            "",
            "        self._read_until_future = None",
            "        self._on_recv = None",
            "        self._closing = False",
            "        self._connecting_future = self.connect()",
            "        self._stream_return_future = salt.ext.tornado.concurrent.Future()",
            "        self.io_loop.spawn_callback(self._stream_return)",
            "",
            "    def _stop_io_loop(self):",
            "        if self.io_loop is not None:",
            "            self.io_loop.stop()",
            "",
            "    # TODO: timeout inflight sessions",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "        if hasattr(self, \"_stream\") and not self._stream.closed():",
            "            # If _stream_return() hasn't completed, it means the IO",
            "            # Loop is stopped (such as when using",
            "            # 'salt.utils.asynchronous.SyncWrapper'). Ensure that",
            "            # _stream_return() completes by restarting the IO Loop.",
            "            # This will prevent potential errors on shutdown.",
            "            try:",
            "                orig_loop = salt.ext.tornado.ioloop.IOLoop.current()",
            "                self.io_loop.make_current()",
            "                self._stream.close()",
            "                if self._read_until_future is not None:",
            "                    # This will prevent this message from showing up:",
            "                    # '[ERROR   ] Future exception was never retrieved:",
            "                    # StreamClosedError'",
            "                    # This happens because the logic is always waiting to read",
            "                    # the next message and the associated read future is marked",
            "                    # 'StreamClosedError' when the stream is closed.",
            "                    if self._read_until_future.done():",
            "                        self._read_until_future.exception()",
            "                    if (",
            "                        self.io_loop",
            "                        != salt.ext.tornado.ioloop.IOLoop.current(instance=False)",
            "                        or not self._stream_return_future.done()",
            "                    ):",
            "                        self.io_loop.add_future(",
            "                            self._stream_return_future,",
            "                            lambda future: self._stop_io_loop(),",
            "                        )",
            "                        self.io_loop.start()",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                log.info(\"Exception caught in SaltMessageClient.close: %s\", str(e))",
            "            finally:",
            "                orig_loop.make_current()",
            "        self._tcp_client.close()",
            "        self.io_loop = None",
            "        self._read_until_future = None",
            "        # Clear callback references to allow the object that they belong to",
            "        # to be deleted.",
            "        self.connect_callback = None",
            "        self.disconnect_callback = None",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def connect(self):",
            "        \"\"\"",
            "        Ask for this client to reconnect to the origin",
            "        \"\"\"",
            "        if hasattr(self, \"_connecting_future\") and not self._connecting_future.done():",
            "            future = self._connecting_future",
            "        else:",
            "            future = salt.ext.tornado.concurrent.Future()",
            "            self._connecting_future = future",
            "            self.io_loop.add_callback(self._connect)",
            "",
            "            # Add the callback only when a new future is created",
            "            if self.connect_callback is not None:",
            "",
            "                def handle_future(future):",
            "                    response = future.result()",
            "                    self.io_loop.add_callback(self.connect_callback, response)",
            "",
            "                future.add_done_callback(handle_future)",
            "",
            "        return future",
            "",
            "    # TODO: tcp backoff opts",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _connect(self):",
            "        \"\"\"",
            "        Try to connect for the rest of time!",
            "        \"\"\"",
            "        while True:",
            "            if self._closing:",
            "                break",
            "            try:",
            "                kwargs = {}",
            "                if self.source_ip or self.source_port:",
            "                    if salt.ext.tornado.version_info >= (4, 5):",
            "                        ### source_ip and source_port are supported only in Tornado >= 4.5",
            "                        # See http://www.tornadoweb.org/en/stable/releases/v4.5.0.html",
            "                        # Otherwise will just ignore these args",
            "                        kwargs = {",
            "                            \"source_ip\": self.source_ip,",
            "                            \"source_port\": self.source_port,",
            "                        }",
            "                    else:",
            "                        log.warning(",
            "                            \"If you need a certain source IP/port, consider upgrading Tornado >= 4.5\"",
            "                        )",
            "                with salt.utils.asynchronous.current_ioloop(self.io_loop):",
            "                    self._stream = yield self._tcp_client.connect(",
            "                        self.host, self.port, ssl_options=self.opts.get(\"ssl\"), **kwargs",
            "                    )",
            "                self._connecting_future.set_result(True)",
            "                break",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                log.warning(\"TCP Message Client encountered an exception %r\", exc)",
            "                yield salt.ext.tornado.gen.sleep(1)  # TODO: backoff",
            "                # self._connecting_future.set_exception(e)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _stream_return(self):",
            "        try:",
            "            while not self._closing and (",
            "                not self._connecting_future.done()",
            "                or self._connecting_future.result() is not True",
            "            ):",
            "                yield self._connecting_future",
            "            unpacker = salt.utils.msgpack.Unpacker()",
            "            while not self._closing:",
            "                try:",
            "                    self._read_until_future = self._stream.read_bytes(",
            "                        4096, partial=True",
            "                    )",
            "                    wire_bytes = yield self._read_until_future",
            "                    unpacker.feed(wire_bytes)",
            "                    for framed_msg in unpacker:",
            "                        framed_msg = salt.transport.frame.decode_embedded_strs(",
            "                            framed_msg",
            "                        )",
            "                        header = framed_msg[\"head\"]",
            "                        body = framed_msg[\"body\"]",
            "                        message_id = header.get(\"mid\")",
            "",
            "                        if message_id in self.send_future_map:",
            "                            self.send_future_map.pop(message_id).set_result(body)",
            "                            self.remove_message_timeout(message_id)",
            "                        else:",
            "                            if self._on_recv is not None:",
            "                                self.io_loop.spawn_callback(self._on_recv, header, body)",
            "                            else:",
            "                                log.error(",
            "                                    \"Got response for message_id %s that we are not tracking\",",
            "                                    message_id,",
            "                                )",
            "                except salt.ext.tornado.iostream.StreamClosedError as e:",
            "                    log.debug(",
            "                        \"tcp stream to %s:%s closed, unable to recv\",",
            "                        self.host,",
            "                        self.port,",
            "                    )",
            "                    for future in self.send_future_map.values():",
            "                        future.set_exception(e)",
            "                    self.send_future_map = {}",
            "                    if self._closing:",
            "                        return",
            "                    if self.disconnect_callback:",
            "                        self.disconnect_callback()",
            "                    # if the last connect finished, then we need to make a new one",
            "                    if self._connecting_future.done():",
            "                        self._connecting_future = self.connect()",
            "                    yield self._connecting_future",
            "                except TypeError:",
            "                    # This is an invalid transport",
            "                    if \"detect_mode\" in self.opts:",
            "                        log.info(",
            "                            \"There was an error trying to use TCP transport; \"",
            "                            \"attempting to fallback to another transport\"",
            "                        )",
            "                    else:",
            "                        raise SaltClientError",
            "                except Exception as e:  # pylint: disable=broad-except",
            "                    log.error(\"Exception parsing response\", exc_info=True)",
            "                    for future in self.send_future_map.values():",
            "                        future.set_exception(e)",
            "                    self.send_future_map = {}",
            "                    if self._closing:",
            "                        return",
            "                    if self.disconnect_callback:",
            "                        self.disconnect_callback()",
            "                    # if the last connect finished, then we need to make a new one",
            "                    if self._connecting_future.done():",
            "                        self._connecting_future = self.connect()",
            "                    yield self._connecting_future",
            "        finally:",
            "            self._stream_return_future.set_result(True)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _stream_send(self):",
            "        while (",
            "            not self._connecting_future.done()",
            "            or self._connecting_future.result() is not True",
            "        ):",
            "            yield self._connecting_future",
            "        while len(self.send_queue) > 0:",
            "            message_id, item = self.send_queue[0]",
            "            try:",
            "                yield self._stream.write(item)",
            "                del self.send_queue[0]",
            "            # if the connection is dead, lets fail this send, and make sure we",
            "            # attempt to reconnect",
            "            except salt.ext.tornado.iostream.StreamClosedError as e:",
            "                if message_id in self.send_future_map:",
            "                    self.send_future_map.pop(message_id).set_exception(e)",
            "                self.remove_message_timeout(message_id)",
            "                del self.send_queue[0]",
            "                if self._closing:",
            "                    return",
            "                if self.disconnect_callback:",
            "                    self.disconnect_callback()",
            "                # if the last connect finished, then we need to make a new one",
            "                if self._connecting_future.done():",
            "                    self._connecting_future = self.connect()",
            "                yield self._connecting_future",
            "",
            "    def _message_id(self):",
            "        wrap = False",
            "        while self._mid in self.send_future_map:",
            "            if self._mid >= self._max_messages:",
            "                if wrap:",
            "                    # this shouldn't ever happen, but just in case",
            "                    raise Exception(\"Unable to find available messageid\")",
            "                self._mid = 1",
            "                wrap = True",
            "            else:",
            "                self._mid += 1",
            "",
            "        return self._mid",
            "",
            "    # TODO: return a message object which takes care of multiplexing?",
            "    def on_recv(self, callback):",
            "        \"\"\"",
            "        Register a callback for received messages (that we didn't initiate)",
            "        \"\"\"",
            "        if callback is None:",
            "            self._on_recv = callback",
            "        else:",
            "",
            "            def wrap_recv(header, body):",
            "                callback(body)",
            "",
            "            self._on_recv = wrap_recv",
            "",
            "    def remove_message_timeout(self, message_id):",
            "        if message_id not in self.send_timeout_map:",
            "            return",
            "        timeout = self.send_timeout_map.pop(message_id)",
            "        self.io_loop.remove_timeout(timeout)",
            "",
            "    def timeout_message(self, message_id):",
            "        if message_id in self.send_timeout_map:",
            "            del self.send_timeout_map[message_id]",
            "        if message_id in self.send_future_map:",
            "            self.send_future_map.pop(message_id).set_exception(",
            "                SaltReqTimeoutError(\"Message timed out\")",
            "            )",
            "",
            "    def send(self, msg, timeout=None, callback=None, raw=False):",
            "        \"\"\"",
            "        Send given message, and return a future",
            "        \"\"\"",
            "        message_id = self._message_id()",
            "        header = {\"mid\": message_id}",
            "",
            "        future = salt.ext.tornado.concurrent.Future()",
            "        if callback is not None:",
            "",
            "            def handle_future(future):",
            "                response = future.result()",
            "                self.io_loop.add_callback(callback, response)",
            "",
            "            future.add_done_callback(handle_future)",
            "        # Add this future to the mapping",
            "        self.send_future_map[message_id] = future",
            "",
            "        if self.opts.get(\"detect_mode\") is True:",
            "            timeout = 1",
            "",
            "        if timeout is not None:",
            "            send_timeout = self.io_loop.call_later(",
            "                timeout, self.timeout_message, message_id",
            "            )",
            "            self.send_timeout_map[message_id] = send_timeout",
            "",
            "        # if we don't have a send queue, we need to spawn the callback to do the sending",
            "        if len(self.send_queue) == 0:",
            "            self.io_loop.spawn_callback(self._stream_send)",
            "        self.send_queue.append(",
            "            (message_id, salt.transport.frame.frame_msg(msg, header=header))",
            "        )",
            "        return future",
            "",
            "",
            "class Subscriber:",
            "    \"\"\"",
            "    Client object for use with the TCP publisher server",
            "    \"\"\"",
            "",
            "    def __init__(self, stream, address):",
            "        self.stream = stream",
            "        self.address = address",
            "        self._closing = False",
            "        self._read_until_future = None",
            "        self.id_ = None",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "        if not self.stream.closed():",
            "            self.stream.close()",
            "            if self._read_until_future is not None and self._read_until_future.done():",
            "                # This will prevent this message from showing up:",
            "                # '[ERROR   ] Future exception was never retrieved:",
            "                # StreamClosedError'",
            "                # This happens because the logic is always waiting to read",
            "                # the next message and the associated read future is marked",
            "                # 'StreamClosedError' when the stream is closed.",
            "                self._read_until_future.exception()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "class PubServer(salt.ext.tornado.tcpserver.TCPServer):",
            "    \"\"\"",
            "    TCP publisher",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, io_loop=None):",
            "        super().__init__(ssl_options=opts.get(\"ssl\"))",
            "        self.io_loop = io_loop",
            "        self.opts = opts",
            "        self._closing = False",
            "        self.clients = set()",
            "        self.aes_funcs = salt.master.AESFuncs(self.opts)",
            "        self.present = {}",
            "        self.presence_events = False",
            "        if self.opts.get(\"presence_events\", False):",
            "            tcp_only = True",
            "            for transport, _ in iter_transport_opts(self.opts):",
            "                if transport != \"tcp\":",
            "                    tcp_only = False",
            "            if tcp_only:",
            "                # Only when the transport is TCP only, the presence events will",
            "                # be handled here. Otherwise, it will be handled in the",
            "                # 'Maintenance' process.",
            "                self.presence_events = True",
            "",
            "        if self.presence_events:",
            "            self.event = salt.utils.event.get_event(",
            "                \"master\", opts=self.opts, listen=False",
            "            )",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _add_client_present(self, client):",
            "        id_ = client.id_",
            "        if id_ in self.present:",
            "            clients = self.present[id_]",
            "            clients.add(client)",
            "        else:",
            "            self.present[id_] = {client}",
            "            if self.presence_events:",
            "                data = {\"new\": [id_], \"lost\": []}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"change\", \"presence\")",
            "                )",
            "                data = {\"present\": list(self.present.keys())}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"present\", \"presence\")",
            "                )",
            "",
            "    def _remove_client_present(self, client):",
            "        id_ = client.id_",
            "        if id_ is None or id_ not in self.present:",
            "            # This is possible if _remove_client_present() is invoked",
            "            # before the minion's id is validated.",
            "            return",
            "",
            "        clients = self.present[id_]",
            "        if client not in clients:",
            "            # Since _remove_client_present() is potentially called from",
            "            # _stream_read() and/or publish_payload(), it is possible for",
            "            # it to be called twice, in which case we will get here.",
            "            # This is not an abnormal case, so no logging is required.",
            "            return",
            "",
            "        clients.remove(client)",
            "        if len(clients) == 0:",
            "            del self.present[id_]",
            "            if self.presence_events:",
            "                data = {\"new\": [], \"lost\": [id_]}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"change\", \"presence\")",
            "                )",
            "                data = {\"present\": list(self.present.keys())}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"present\", \"presence\")",
            "                )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _stream_read(self, client):",
            "        unpacker = salt.utils.msgpack.Unpacker()",
            "        while not self._closing:",
            "            try:",
            "                client._read_until_future = client.stream.read_bytes(4096, partial=True)",
            "                wire_bytes = yield client._read_until_future",
            "                unpacker.feed(wire_bytes)",
            "                for framed_msg in unpacker:",
            "                    framed_msg = salt.transport.frame.decode_embedded_strs(framed_msg)",
            "                    body = framed_msg[\"body\"]",
            "                    if body[\"enc\"] != \"aes\":",
            "                        # We only accept 'aes' encoded messages for 'id'",
            "                        continue",
            "                    crypticle = salt.crypt.Crypticle(",
            "                        self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "                    )",
            "                    load = crypticle.loads(body[\"load\"])",
            "                    load = salt.transport.frame.decode_embedded_strs(load)",
            "                    if not self.aes_funcs.verify_minion(load[\"id\"], load[\"tok\"]):",
            "                        continue",
            "                    client.id_ = load[\"id\"]",
            "                    self._add_client_present(client)",
            "            except salt.ext.tornado.iostream.StreamClosedError as e:",
            "                log.debug(\"tcp stream to %s closed, unable to recv\", client.address)",
            "                client.close()",
            "                self._remove_client_present(client)",
            "                self.clients.discard(client)",
            "                break",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                log.error(",
            "                    \"Exception parsing response from %s\", client.address, exc_info=True",
            "                )",
            "                continue",
            "",
            "    def handle_stream(self, stream, address):",
            "        log.trace(\"Subscriber at %s connected\", address)",
            "        client = Subscriber(stream, address)",
            "        self.clients.add(client)",
            "        self.io_loop.spawn_callback(self._stream_read, client)",
            "",
            "    # TODO: ACK the publish through IPC",
            "    @salt.ext.tornado.gen.coroutine",
            "    def publish_payload(self, package, _):",
            "        log.debug(\"TCP PubServer sending payload: %s\", package)",
            "        payload = salt.transport.frame.frame_msg(package[\"payload\"])",
            "",
            "        to_remove = []",
            "        if \"topic_lst\" in package:",
            "            topic_lst = package[\"topic_lst\"]",
            "            for topic in topic_lst:",
            "                if topic in self.present:",
            "                    # This will rarely be a list of more than 1 item. It will",
            "                    # be more than 1 item if the minion disconnects from the",
            "                    # master in an unclean manner (eg cable yank), then",
            "                    # restarts and the master is yet to detect the disconnect",
            "                    # via TCP keep-alive.",
            "                    for client in self.present[topic]:",
            "                        try:",
            "                            # Write the packed str",
            "                            f = client.stream.write(payload)",
            "                            self.io_loop.add_future(f, lambda f: True)",
            "                        except salt.ext.tornado.iostream.StreamClosedError:",
            "                            to_remove.append(client)",
            "                else:",
            "                    log.debug(\"Publish target %s not connected\", topic)",
            "        else:",
            "            for client in self.clients:",
            "                try:",
            "                    # Write the packed str",
            "                    f = client.stream.write(payload)",
            "                    self.io_loop.add_future(f, lambda f: True)",
            "                except salt.ext.tornado.iostream.StreamClosedError:",
            "                    to_remove.append(client)",
            "        for client in to_remove:",
            "            log.debug(",
            "                \"Subscriber at %s has disconnected from publisher\", client.address",
            "            )",
            "            client.close()",
            "            self._remove_client_present(client)",
            "            self.clients.discard(client)",
            "        log.trace(\"TCP PubServer finished publishing payload\")",
            "",
            "",
            "class TCPPubServerChannel(salt.transport.server.PubServerChannel):",
            "    # TODO: opts!",
            "    # Based on default used in salt.ext.tornado.netutil.bind_sockets()",
            "    backlog = 128",
            "",
            "    def __init__(self, opts):",
            "        self.opts = opts",
            "        self.serial = salt.payload.Serial(self.opts)  # TODO: in init?",
            "        self.ckminions = salt.utils.minions.CkMinions(opts)",
            "        self.io_loop = None",
            "",
            "    def __setstate__(self, state):",
            "        salt.master.SMaster.secrets = state[\"secrets\"]",
            "        self.__init__(state[\"opts\"])",
            "",
            "    def __getstate__(self):",
            "        return {\"opts\": self.opts, \"secrets\": salt.master.SMaster.secrets}",
            "",
            "    def _publish_daemon(self, **kwargs):",
            "        \"\"\"",
            "        Bind to the interface specified in the configuration file",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        log_queue = kwargs.get(\"log_queue\")",
            "        if log_queue is not None:",
            "            salt.log.setup.set_multiprocessing_logging_queue(log_queue)",
            "        log_queue_level = kwargs.get(\"log_queue_level\")",
            "        if log_queue_level is not None:",
            "            salt.log.setup.set_multiprocessing_logging_level(log_queue_level)",
            "        salt.log.setup.setup_multiprocessing_logging(log_queue)",
            "",
            "        # Check if io_loop was set outside",
            "        if self.io_loop is None:",
            "            self.io_loop = salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        # Spin up the publisher",
            "        pub_server = PubServer(self.opts, io_loop=self.io_loop)",
            "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "        _set_tcp_keepalive(sock, self.opts)",
            "        sock.setblocking(0)",
            "        sock.bind((self.opts[\"interface\"], int(self.opts[\"publish_port\"])))",
            "        sock.listen(self.backlog)",
            "        # pub_server will take ownership of the socket",
            "        pub_server.add_socket(sock)",
            "",
            "        # Set up Salt IPC server",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = int(self.opts.get(\"tcp_master_publish_pull\", 4514))",
            "        else:",
            "            pull_uri = os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "",
            "        pull_sock = salt.transport.ipc.IPCMessageServer(",
            "            pull_uri, io_loop=self.io_loop, payload_handler=pub_server.publish_payload,",
            "        )",
            "",
            "        # Securely create socket",
            "        log.info(\"Starting the Salt Puller on %s\", pull_uri)",
            "        with salt.utils.files.set_umask(0o177):",
            "            pull_sock.start()",
            "",
            "        # run forever",
            "        try:",
            "            self.io_loop.start()",
            "        except (KeyboardInterrupt, SystemExit):",
            "            salt.log.setup.shutdown_multiprocessing_logging()",
            "",
            "    def pre_fork(self, process_manager, kwargs=None):",
            "        \"\"\"",
            "        Do anything necessary pre-fork. Since this is on the master side this will",
            "        primarily be used to create IPC channels and create our daemon process to",
            "        do the actual publishing",
            "        \"\"\"",
            "        process_manager.add_process(self._publish_daemon, kwargs=kwargs)",
            "",
            "    def publish(self, load):",
            "        \"\"\"",
            "        Publish \"load\" to minions",
            "        \"\"\"",
            "        payload = {\"enc\": \"aes\"}",
            "",
            "        crypticle = salt.crypt.Crypticle(",
            "            self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "        )",
            "        payload[\"load\"] = crypticle.dumps(load)",
            "        if self.opts[\"sign_pub_messages\"]:",
            "            master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "            log.debug(\"Signing data packet\")",
            "            payload[\"sig\"] = salt.crypt.sign_message(master_pem_path, payload[\"load\"])",
            "        # Use the Salt IPC server",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = int(self.opts.get(\"tcp_master_publish_pull\", 4514))",
            "        else:",
            "            pull_uri = os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "        # TODO: switch to the actual asynchronous interface",
            "        # pub_sock = salt.transport.ipc.IPCMessageClient(self.opts, io_loop=self.io_loop)",
            "        pub_sock = salt.utils.asynchronous.SyncWrapper(",
            "            salt.transport.ipc.IPCMessageClient, (pull_uri,), loop_kwarg=\"io_loop\",",
            "        )",
            "        pub_sock.connect()",
            "",
            "        int_payload = {\"payload\": self.serial.dumps(payload)}",
            "",
            "        # add some targeting stuff for lists only (for now)",
            "        if load[\"tgt_type\"] == \"list\" and not self.opts.get(\"order_masters\", False):",
            "            if isinstance(load[\"tgt\"], str):",
            "                # Fetch a list of minions that match",
            "                _res = self.ckminions.check_minions(",
            "                    load[\"tgt\"], tgt_type=load[\"tgt_type\"]",
            "                )",
            "                match_ids = _res[\"minions\"]",
            "",
            "                log.debug(\"Publish Side Match: %s\", match_ids)",
            "                # Send list of miions thru so zmq can target them",
            "                int_payload[\"topic_lst\"] = match_ids",
            "            else:",
            "                int_payload[\"topic_lst\"] = load[\"tgt\"]",
            "        # Send it over IPC!",
            "        pub_sock.send(int_payload)"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "TCP transport classes",
            "",
            "Wire protocol: \"len(payload) msgpack({'head': SOMEHEADER, 'body': SOMEBODY})\"",
            "",
            "\"\"\"",
            "",
            "import errno",
            "import logging",
            "import os",
            "import queue",
            "import socket",
            "import threading",
            "import time",
            "import traceback",
            "import urllib.parse as urlparse",
            "import uuid",
            "import weakref",
            "",
            "import salt.crypt",
            "import salt.exceptions",
            "import salt.ext.tornado",
            "import salt.ext.tornado.concurrent",
            "import salt.ext.tornado.gen",
            "import salt.ext.tornado.iostream",
            "import salt.ext.tornado.netutil",
            "import salt.ext.tornado.tcpclient",
            "import salt.ext.tornado.tcpserver",
            "import salt.payload",
            "import salt.transport.client",
            "import salt.transport.frame",
            "import salt.transport.ipc",
            "import salt.transport.mixins.auth",
            "import salt.transport.server",
            "import salt.utils.asynchronous",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.msgpack",
            "import salt.utils.platform",
            "import salt.utils.process",
            "import salt.utils.verify",
            "from salt.exceptions import SaltClientError, SaltReqTimeoutError",
            "from salt.transport import iter_transport_opts",
            "",
            "try:",
            "    from M2Crypto import RSA",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "    try:",
            "        from Cryptodome.Cipher import PKCS1_OAEP",
            "    except ImportError:",
            "        from Crypto.Cipher import PKCS1_OAEP  # nosec",
            "",
            "if salt.utils.platform.is_windows():",
            "    USE_LOAD_BALANCER = True",
            "else:",
            "    USE_LOAD_BALANCER = False",
            "",
            "if USE_LOAD_BALANCER:",
            "    import threading",
            "    import multiprocessing",
            "    import salt.ext.tornado.util",
            "    from salt.utils.process import SignalHandlingProcess",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def _set_tcp_keepalive(sock, opts):",
            "    \"\"\"",
            "    Ensure that TCP keepalives are set for the socket.",
            "    \"\"\"",
            "    if hasattr(socket, \"SO_KEEPALIVE\"):",
            "        if opts.get(\"tcp_keepalive\", False):",
            "            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)",
            "            if hasattr(socket, \"SOL_TCP\"):",
            "                if hasattr(socket, \"TCP_KEEPIDLE\"):",
            "                    tcp_keepalive_idle = opts.get(\"tcp_keepalive_idle\", -1)",
            "                    if tcp_keepalive_idle > 0:",
            "                        sock.setsockopt(",
            "                            socket.SOL_TCP, socket.TCP_KEEPIDLE, int(tcp_keepalive_idle)",
            "                        )",
            "                if hasattr(socket, \"TCP_KEEPCNT\"):",
            "                    tcp_keepalive_cnt = opts.get(\"tcp_keepalive_cnt\", -1)",
            "                    if tcp_keepalive_cnt > 0:",
            "                        sock.setsockopt(",
            "                            socket.SOL_TCP, socket.TCP_KEEPCNT, int(tcp_keepalive_cnt)",
            "                        )",
            "                if hasattr(socket, \"TCP_KEEPINTVL\"):",
            "                    tcp_keepalive_intvl = opts.get(\"tcp_keepalive_intvl\", -1)",
            "                    if tcp_keepalive_intvl > 0:",
            "                        sock.setsockopt(",
            "                            socket.SOL_TCP,",
            "                            socket.TCP_KEEPINTVL,",
            "                            int(tcp_keepalive_intvl),",
            "                        )",
            "            if hasattr(socket, \"SIO_KEEPALIVE_VALS\"):",
            "                # Windows doesn't support TCP_KEEPIDLE, TCP_KEEPCNT, nor",
            "                # TCP_KEEPINTVL. Instead, it has its own proprietary",
            "                # SIO_KEEPALIVE_VALS.",
            "                tcp_keepalive_idle = opts.get(\"tcp_keepalive_idle\", -1)",
            "                tcp_keepalive_intvl = opts.get(\"tcp_keepalive_intvl\", -1)",
            "                # Windows doesn't support changing something equivalent to",
            "                # TCP_KEEPCNT.",
            "                if tcp_keepalive_idle > 0 or tcp_keepalive_intvl > 0:",
            "                    # Windows defaults may be found by using the link below.",
            "                    # Search for 'KeepAliveTime' and 'KeepAliveInterval'.",
            "                    # https://technet.microsoft.com/en-us/library/bb726981.aspx#EDAA",
            "                    # If one value is set and the other isn't, we still need",
            "                    # to send both values to SIO_KEEPALIVE_VALS and they both",
            "                    # need to be valid. So in that case, use the Windows",
            "                    # default.",
            "                    if tcp_keepalive_idle <= 0:",
            "                        tcp_keepalive_idle = 7200",
            "                    if tcp_keepalive_intvl <= 0:",
            "                        tcp_keepalive_intvl = 1",
            "                    # The values expected are in milliseconds, so multiply by",
            "                    # 1000.",
            "                    sock.ioctl(",
            "                        socket.SIO_KEEPALIVE_VALS,",
            "                        (",
            "                            1,",
            "                            int(tcp_keepalive_idle * 1000),",
            "                            int(tcp_keepalive_intvl * 1000),",
            "                        ),",
            "                    )",
            "        else:",
            "            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 0)",
            "",
            "",
            "if USE_LOAD_BALANCER:",
            "",
            "    class LoadBalancerServer(SignalHandlingProcess):",
            "        \"\"\"",
            "        Raw TCP server which runs in its own process and will listen",
            "        for incoming connections. Each incoming connection will be",
            "        sent via multiprocessing queue to the workers.",
            "        Since the queue is shared amongst workers, only one worker will",
            "        handle a given connection.",
            "        \"\"\"",
            "",
            "        # TODO: opts!",
            "        # Based on default used in salt.ext.tornado.netutil.bind_sockets()",
            "        backlog = 128",
            "",
            "        def __init__(self, opts, socket_queue, **kwargs):",
            "            super().__init__(**kwargs)",
            "            self.opts = opts",
            "            self.socket_queue = socket_queue",
            "            self._socket = None",
            "",
            "        # __setstate__ and __getstate__ are only used on Windows.",
            "        # We do this so that __init__ will be invoked on Windows in the child",
            "        # process so that a register_after_fork() equivalent will work on",
            "        # Windows.",
            "        def __setstate__(self, state):",
            "            self.__init__(",
            "                state[\"opts\"],",
            "                state[\"socket_queue\"],",
            "                log_queue=state[\"log_queue\"],",
            "                log_queue_level=state[\"log_queue_level\"],",
            "            )",
            "",
            "        def __getstate__(self):",
            "            return {",
            "                \"opts\": self.opts,",
            "                \"socket_queue\": self.socket_queue,",
            "                \"log_queue\": self.log_queue,",
            "                \"log_queue_level\": self.log_queue_level,",
            "            }",
            "",
            "        def close(self):",
            "            if self._socket is not None:",
            "                self._socket.shutdown(socket.SHUT_RDWR)",
            "                self._socket.close()",
            "                self._socket = None",
            "",
            "        # pylint: disable=W1701",
            "        def __del__(self):",
            "            self.close()",
            "",
            "        # pylint: enable=W1701",
            "",
            "        def run(self):",
            "            \"\"\"",
            "            Start the load balancer",
            "            \"\"\"",
            "            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "            _set_tcp_keepalive(self._socket, self.opts)",
            "            self._socket.setblocking(1)",
            "            self._socket.bind((self.opts[\"interface\"], int(self.opts[\"ret_port\"])))",
            "            self._socket.listen(self.backlog)",
            "",
            "            while True:",
            "                try:",
            "                    # Wait for a connection to occur since the socket is",
            "                    # blocking.",
            "                    connection, address = self._socket.accept()",
            "                    # Wait for a free slot to be available to put",
            "                    # the connection into.",
            "                    # Sockets are picklable on Windows in Python 3.",
            "                    self.socket_queue.put((connection, address), True, None)",
            "                except OSError as e:",
            "                    # ECONNABORTED indicates that there was a connection",
            "                    # but it was closed while still in the accept queue.",
            "                    # (observed on FreeBSD).",
            "                    if (",
            "                        salt.ext.tornado.util.errno_from_exception(e)",
            "                        == errno.ECONNABORTED",
            "                    ):",
            "                        continue",
            "                    raise",
            "",
            "",
            "# TODO: move serial down into message library",
            "class AsyncTCPReqChannel(salt.transport.client.ReqChannel):",
            "    \"\"\"",
            "    Encapsulate sending routines to tcp.",
            "",
            "    Note: this class returns a singleton",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    # mapping of io_loop -> {key -> channel}",
            "    instance_map = weakref.WeakKeyDictionary()",
            "    async_methods = [",
            "        \"crypted_transfer_decode_dictentry\",",
            "        \"_crypted_transfer\",",
            "        \"_uncrypted_transfer\",",
            "        \"send\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __new__(cls, opts, **kwargs):",
            "        \"\"\"",
            "        Only create one instance of channel per __key()",
            "        \"\"\"",
            "        # do we have any mapping for this io_loop",
            "        io_loop = kwargs.get(\"io_loop\") or salt.ext.tornado.ioloop.IOLoop.current()",
            "        if io_loop not in cls.instance_map:",
            "            cls.instance_map[io_loop] = weakref.WeakValueDictionary()",
            "        loop_instance_map = cls.instance_map[io_loop]",
            "",
            "        key = cls.__key(opts, **kwargs)",
            "        obj = loop_instance_map.get(key)",
            "        if obj is None:",
            "            log.debug(\"Initializing new AsyncTCPReqChannel for %s\", key)",
            "            # we need to make a local variable for this, as we are going to store",
            "            # it in a WeakValueDictionary-- which will remove the item if no one",
            "            # references it-- this forces a reference while we return to the caller",
            "            obj = object.__new__(cls)",
            "            obj.__singleton_init__(opts, **kwargs)",
            "            obj._instance_key = key",
            "            loop_instance_map[key] = obj",
            "            obj._refcount = 1",
            "            obj._refcount_lock = threading.RLock()",
            "        else:",
            "            with obj._refcount_lock:",
            "                obj._refcount += 1",
            "            log.debug(\"Re-using AsyncTCPReqChannel for %s\", key)",
            "        return obj",
            "",
            "    @classmethod",
            "    def __key(cls, opts, **kwargs):",
            "        if \"master_uri\" in kwargs:",
            "            opts[\"master_uri\"] = kwargs[\"master_uri\"]",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            opts[\"master_uri\"],",
            "            kwargs.get(\"crypt\", \"aes\"),  # TODO: use the same channel for crypt",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, **kwargs):",
            "        pass",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, **kwargs):",
            "        self.opts = dict(opts)",
            "",
            "        self.serial = salt.payload.Serial(self.opts)",
            "",
            "        # crypt defaults to 'aes'",
            "        self.crypt = kwargs.get(\"crypt\", \"aes\")",
            "",
            "        self.io_loop = kwargs.get(\"io_loop\") or salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        if self.crypt != \"clear\":",
            "            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)",
            "",
            "        resolver = kwargs.get(\"resolver\")",
            "",
            "        parse = urlparse.urlparse(self.opts[\"master_uri\"])",
            "        master_host, master_port = parse.netloc.rsplit(\":\", 1)",
            "        self.master_addr = (master_host, int(master_port))",
            "        self._closing = False",
            "        self.message_client = SaltMessageClientPool(",
            "            self.opts,",
            "            args=(self.opts, master_host, int(master_port),),",
            "            kwargs={",
            "                \"io_loop\": self.io_loop,",
            "                \"resolver\": resolver,",
            "                \"source_ip\": self.opts.get(\"source_ip\"),",
            "                \"source_port\": self.opts.get(\"source_ret_port\"),",
            "            },",
            "        )",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "",
            "        if self._refcount > 1:",
            "            # Decrease refcount",
            "            with self._refcount_lock:",
            "                self._refcount -= 1",
            "            log.debug(",
            "                \"This is not the last %s instance. Not closing yet.\",",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "",
            "        log.debug(\"Closing %s instance\", self.__class__.__name__)",
            "        self._closing = True",
            "        self.message_client.close()",
            "",
            "        # Remove the entry from the instance map so that a closed entry may not",
            "        # be reused.",
            "        # This forces this operation even if the reference count of the entry",
            "        # has not yet gone to zero.",
            "        if self.io_loop in self.__class__.instance_map:",
            "            loop_instance_map = self.__class__.instance_map[self.io_loop]",
            "            if self._instance_key in loop_instance_map:",
            "                del loop_instance_map[self._instance_key]",
            "            if not loop_instance_map:",
            "                del self.__class__.instance_map[self.io_loop]",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        with self._refcount_lock:",
            "            # Make sure we actually close no matter if something",
            "            # went wrong with our ref counting",
            "            self._refcount = 1",
            "        try:",
            "            self.close()",
            "        except OSError as exc:",
            "            if exc.errno != errno.EBADF:",
            "                # If its not a bad file descriptor error, raise",
            "                raise",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _package_load(self, load):",
            "        return {",
            "            \"enc\": self.crypt,",
            "            \"load\": load,",
            "            \"version\": 2,",
            "        }",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def crypted_transfer_decode_dictentry(",
            "        self, load, dictkey=None, tries=3, timeout=60",
            "    ):",
            "        nonce = uuid.uuid4().hex",
            "        load[\"nonce\"] = nonce",
            "        if not self.auth.authenticated:",
            "            yield self.auth.authenticate()",
            "        ret = yield self.message_client.send(",
            "            self._package_load(self.auth.crypticle.dumps(load)), timeout=timeout",
            "        )",
            "        key = self.auth.get_keys()",
            "        if HAS_M2:",
            "            aes = key.private_decrypt(ret[\"key\"], RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(key)",
            "            aes = cipher.decrypt(ret[\"key\"])",
            "",
            "        # Decrypt using the public key.",
            "        pcrypt = salt.crypt.Crypticle(self.opts, aes)",
            "        signed_msg = pcrypt.loads(ret[dictkey])",
            "",
            "        # Validate the master's signature.",
            "        master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion_master.pub\")",
            "        if not salt.crypt.verify_signature(",
            "            master_pubkey_path, signed_msg[\"data\"], signed_msg[\"sig\"]",
            "        ):",
            "            raise salt.crypt.AuthenticationError(",
            "                \"Pillar payload signature failed to validate.\"",
            "            )",
            "",
            "        # Make sure the signed key matches the key we used to decrypt the data.",
            "        data = salt.payload.Serial({}).loads(signed_msg[\"data\"])",
            "        if data[\"key\"] != ret[\"key\"]:",
            "            raise salt.crypt.AuthenticationError(\"Key verification failed.\")",
            "",
            "        # Validate the nonce.",
            "        if data[\"nonce\"] != nonce:",
            "            raise salt.crypt.AuthenticationError(\"Pillar nonce verification failed.\")",
            "        raise salt.ext.tornado.gen.Return(data[\"pillar\"])",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _crypted_transfer(self, load, tries=3, timeout=60):",
            "        \"\"\"",
            "        In case of authentication errors, try to renegotiate authentication",
            "        and retry the method.",
            "        Indeed, we can fail too early in case of a master restart during a",
            "        minion state execution call",
            "        \"\"\"",
            "        nonce = uuid.uuid4().hex",
            "        if load and isinstance(load, dict):",
            "            load[\"nonce\"] = nonce",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def _do_transfer():",
            "            data = yield self.message_client.send(",
            "                self._package_load(self.auth.crypticle.dumps(load)), timeout=timeout,",
            "            )",
            "            # we may not have always data",
            "            # as for example for saltcall ret submission, this is a blind",
            "            # communication, we do not subscribe to return events, we just",
            "            # upload the results to the master",
            "            if data:",
            "                data = self.auth.crypticle.loads(data, nonce=nonce)",
            "                data = salt.transport.frame.decode_embedded_strs(data)",
            "            raise salt.ext.tornado.gen.Return(data)",
            "",
            "        if not self.auth.authenticated:",
            "            yield self.auth.authenticate()",
            "        try:",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "        except salt.crypt.AuthenticationError:",
            "            yield self.auth.authenticate()",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _uncrypted_transfer(self, load, tries=3, timeout=60):",
            "        ret = yield self.message_client.send(self._package_load(load), timeout=timeout)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def send(self, load, tries=3, timeout=60, raw=False):",
            "        \"\"\"",
            "        Send a request, return a future which will complete when we send the message",
            "        \"\"\"",
            "        try:",
            "            if self.crypt == \"clear\":",
            "                ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)",
            "            else:",
            "                ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout)",
            "        except salt.ext.tornado.iostream.StreamClosedError:",
            "            # Convert to 'SaltClientError' so that clients can handle this",
            "            # exception more appropriately.",
            "            raise SaltClientError(\"Connection to master lost\")",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "",
            "class AsyncTCPPubChannel(",
            "    salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel",
            "):",
            "    async_methods = [",
            "        \"send_id\",",
            "        \"connect_callback\",",
            "        \"connect\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        self.opts = opts",
            "",
            "        self.serial = salt.payload.Serial(self.opts)",
            "",
            "        self.crypt = kwargs.get(\"crypt\", \"aes\")",
            "        self.io_loop = kwargs.get(\"io_loop\") or salt.ext.tornado.ioloop.IOLoop.current()",
            "        self.connected = False",
            "        self._closing = False",
            "        self._reconnected = False",
            "        self.event = salt.utils.event.get_event(\"minion\", opts=self.opts, listen=False)",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "        if hasattr(self, \"message_client\"):",
            "            self.message_client.close()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _package_load(self, load):",
            "        return {",
            "            \"enc\": self.crypt,",
            "            \"load\": load,",
            "            \"version\": 2,",
            "        }",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def send_id(self, tok, force_auth):",
            "        \"\"\"",
            "        Send the minion id to the master so that the master may better",
            "        track the connection state of the minion.",
            "        In case of authentication errors, try to renegotiate authentication",
            "        and retry the method.",
            "        \"\"\"",
            "        load = {\"id\": self.opts[\"id\"], \"tok\": tok}",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def _do_transfer():",
            "            msg = self._package_load(self.auth.crypticle.dumps(load))",
            "            package = salt.transport.frame.frame_msg(msg, header=None)",
            "            yield self.message_client.write_to_stream(package)",
            "            raise salt.ext.tornado.gen.Return(True)",
            "",
            "        if force_auth or not self.auth.authenticated:",
            "            count = 0",
            "            while (",
            "                count <= self.opts[\"tcp_authentication_retries\"]",
            "                or self.opts[\"tcp_authentication_retries\"] < 0",
            "            ):",
            "                try:",
            "                    yield self.auth.authenticate()",
            "                    break",
            "                except SaltClientError as exc:",
            "                    log.debug(exc)",
            "                    count += 1",
            "        try:",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "        except salt.crypt.AuthenticationError:",
            "            yield self.auth.authenticate()",
            "            ret = yield _do_transfer()",
            "            raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect_callback(self, result):",
            "        if self._closing:",
            "            return",
            "        # Force re-auth on reconnect since the master",
            "        # may have been restarted",
            "        yield self.send_id(self.tok, self._reconnected)",
            "        self.connected = True",
            "        self.event.fire_event({\"master\": self.opts[\"master\"]}, \"__master_connected\")",
            "        if self._reconnected:",
            "            # On reconnects, fire a master event to notify that the minion is",
            "            # available.",
            "            if self.opts.get(\"__role\") == \"syndic\":",
            "                data = \"Syndic {} started at {}\".format(self.opts[\"id\"], time.asctime())",
            "                tag = salt.utils.event.tagify([self.opts[\"id\"], \"start\"], \"syndic\")",
            "            else:",
            "                data = \"Minion {} started at {}\".format(self.opts[\"id\"], time.asctime())",
            "                tag = salt.utils.event.tagify([self.opts[\"id\"], \"start\"], \"minion\")",
            "            load = {",
            "                \"id\": self.opts[\"id\"],",
            "                \"cmd\": \"_minion_event\",",
            "                \"pretag\": None,",
            "                \"tok\": self.tok,",
            "                \"data\": data,",
            "                \"tag\": tag,",
            "            }",
            "            req_channel = salt.utils.asynchronous.SyncWrapper(",
            "                AsyncTCPReqChannel, (self.opts,), loop_kwarg=\"io_loop\",",
            "            )",
            "            try:",
            "                req_channel.send(load, timeout=60)",
            "            except salt.exceptions.SaltReqTimeoutError:",
            "                log.info(",
            "                    \"fire_master failed: master could not be contacted. Request timed out.\"",
            "                )",
            "            except Exception:  # pylint: disable=broad-except",
            "                log.info(\"fire_master failed: %s\", traceback.format_exc())",
            "            finally:",
            "                # SyncWrapper will call either close() or destroy(), whichever is available",
            "                del req_channel",
            "        else:",
            "            self._reconnected = True",
            "",
            "    def disconnect_callback(self):",
            "        if self._closing:",
            "            return",
            "        self.connected = False",
            "        self.event.fire_event({\"master\": self.opts[\"master\"]}, \"__master_disconnected\")",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect(self):",
            "        try:",
            "            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)",
            "            self.tok = self.auth.gen_token(b\"salt\")",
            "            if not self.auth.authenticated:",
            "                yield self.auth.authenticate()",
            "            if self.auth.authenticated:",
            "                # if this is changed from the default, we assume it was intentional",
            "                if int(self.opts.get(\"publish_port\", 4505)) != 4505:",
            "                    self.publish_port = self.opts.get(\"publish_port\")",
            "                # else take the relayed publish_port master reports",
            "                else:",
            "                    self.publish_port = self.auth.creds[\"publish_port\"]",
            "",
            "                self.message_client = SaltMessageClientPool(",
            "                    self.opts,",
            "                    args=(self.opts, self.opts[\"master_ip\"], int(self.publish_port),),",
            "                    kwargs={",
            "                        \"io_loop\": self.io_loop,",
            "                        \"connect_callback\": self.connect_callback,",
            "                        \"disconnect_callback\": self.disconnect_callback,",
            "                        \"source_ip\": self.opts.get(\"source_ip\"),",
            "                        \"source_port\": self.opts.get(\"source_publish_port\"),",
            "                    },",
            "                )",
            "                yield self.message_client.connect()  # wait for the client to be connected",
            "                self.connected = True",
            "        # TODO: better exception handling...",
            "        except KeyboardInterrupt:  # pylint: disable=try-except-raise",
            "            raise",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            if \"-|RETRY|-\" not in str(exc):",
            "                raise SaltClientError(",
            "                    \"Unable to sign_in to master: {}\".format(exc)",
            "                )  # TODO: better error message",
            "",
            "    def on_recv(self, callback):",
            "        \"\"\"",
            "        Register an on_recv callback",
            "        \"\"\"",
            "        if callback is None:",
            "            return self.message_client.on_recv(callback)",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def wrap_callback(body):",
            "            if not isinstance(body, dict):",
            "                # TODO: For some reason we need to decode here for things",
            "                #       to work. Fix this.",
            "                body = salt.utils.msgpack.loads(body)",
            "                body = salt.transport.frame.decode_embedded_strs(body)",
            "            ret = yield self._decode_payload(body)",
            "            callback(ret)",
            "",
            "        return self.message_client.on_recv(wrap_callback)",
            "",
            "",
            "class TCPReqServerChannel(",
            "    salt.transport.mixins.auth.AESReqServerMixin, salt.transport.server.ReqServerChannel",
            "):",
            "    # TODO: opts!",
            "    backlog = 5",
            "",
            "    def __init__(self, opts):",
            "        salt.transport.server.ReqServerChannel.__init__(self, opts)",
            "        self._socket = None",
            "",
            "    @property",
            "    def socket(self):",
            "        return self._socket",
            "",
            "    def close(self):",
            "        if self._socket is not None:",
            "            try:",
            "                self._socket.shutdown(socket.SHUT_RDWR)",
            "            except OSError as exc:",
            "                if exc.errno == errno.ENOTCONN:",
            "                    # We may try to shutdown a socket which is already disconnected.",
            "                    # Ignore this condition and continue.",
            "                    pass",
            "                else:",
            "                    raise",
            "            self._socket.close()",
            "            self._socket = None",
            "        if hasattr(self.req_server, \"shutdown\"):",
            "            try:",
            "                self.req_server.shutdown()",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                log.exception(",
            "                    \"TCPReqServerChannel close generated an exception: %s\", str(exc)",
            "                )",
            "        elif hasattr(self.req_server, \"stop\"):",
            "            try:",
            "                self.req_server.stop()",
            "            except OSError as exc:",
            "                if exc.errno != 9:",
            "                    raise",
            "                log.exception(",
            "                    \"TCPReqServerChannel close generated an exception: %s\", str(exc)",
            "                )",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def pre_fork(self, process_manager):",
            "        \"\"\"",
            "        Pre-fork we need to create the zmq router device",
            "        \"\"\"",
            "        salt.transport.mixins.auth.AESReqServerMixin.pre_fork(self, process_manager)",
            "        if USE_LOAD_BALANCER:",
            "            self.socket_queue = multiprocessing.Queue()",
            "            process_manager.add_process(",
            "                LoadBalancerServer, args=(self.opts, self.socket_queue)",
            "            )",
            "        elif not salt.utils.platform.is_windows():",
            "            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "            _set_tcp_keepalive(self._socket, self.opts)",
            "            self._socket.setblocking(0)",
            "            self._socket.bind((self.opts[\"interface\"], int(self.opts[\"ret_port\"])))",
            "",
            "    def post_fork(self, payload_handler, io_loop):",
            "        \"\"\"",
            "        After forking we need to create all of the local sockets to listen to the",
            "        router",
            "",
            "        payload_handler: function to call with your payloads",
            "        \"\"\"",
            "        if self.opts[\"pub_server_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting Publish daemon niceness to %i\",",
            "                self.opts[\"pub_server_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"pub_server_niceness\"])",
            "",
            "        self.payload_handler = payload_handler",
            "        self.io_loop = io_loop",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        with salt.utils.asynchronous.current_ioloop(self.io_loop):",
            "            if USE_LOAD_BALANCER:",
            "                self.req_server = LoadBalancerWorker(",
            "                    self.socket_queue,",
            "                    self.handle_message,",
            "                    ssl_options=self.opts.get(\"ssl\"),",
            "                )",
            "            else:",
            "                if salt.utils.platform.is_windows():",
            "                    self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "                    self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "                    _set_tcp_keepalive(self._socket, self.opts)",
            "                    self._socket.setblocking(0)",
            "                    self._socket.bind(",
            "                        (self.opts[\"interface\"], int(self.opts[\"ret_port\"]))",
            "                    )",
            "                self.req_server = SaltMessageServer(",
            "                    self.handle_message,",
            "                    ssl_options=self.opts.get(\"ssl\"),",
            "                    io_loop=self.io_loop,",
            "                )",
            "                self.req_server.add_socket(self._socket)",
            "                self._socket.listen(self.backlog)",
            "        salt.transport.mixins.auth.AESReqServerMixin.post_fork(",
            "            self, payload_handler, io_loop",
            "        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_message(self, stream, header, payload):",
            "        \"\"\"",
            "        Handle incoming messages from underlying tcp streams",
            "        \"\"\"",
            "        try:",
            "            try:",
            "                payload = self._decode_payload(payload)",
            "            except Exception:  # pylint: disable=broad-except",
            "                stream.write(salt.transport.frame.frame_msg(\"bad load\", header=header))",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            # TODO helper functions to normalize payload?",
            "            if not isinstance(payload, dict) or not isinstance(",
            "                payload.get(\"load\"), dict",
            "            ):",
            "                yield stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        \"payload and load must be a dict\", header=header",
            "                    )",
            "                )",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            try:",
            "                id_ = payload[\"load\"].get(\"id\", \"\")",
            "                if \"\\0\" in id_:",
            "                    log.error(\"Payload contains an id with a null byte: %s\", payload)",
            "                    stream.send(self.serial.dumps(\"bad load: id contains a null byte\"))",
            "                    raise salt.ext.tornado.gen.Return()",
            "            except TypeError:",
            "                log.error(\"Payload contains non-string id: %s\", payload)",
            "                stream.send(",
            "                    self.serial.dumps(\"bad load: id {} is not a string\".format(id_))",
            "                )",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            version = 0",
            "            if \"version\" in payload:",
            "                version = payload[\"version\"]",
            "",
            "            sign_messages = False",
            "            if version > 1:",
            "                sign_messages = True",
            "",
            "            # intercept the \"_auth\" commands, since the main daemon shouldn't know",
            "            # anything about our key auth",
            "            if (",
            "                payload[\"enc\"] == \"clear\"",
            "                and payload.get(\"load\", {}).get(\"cmd\") == \"_auth\"",
            "            ):",
            "                yield stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        self._auth(payload[\"load\"], sign_messages), header=header",
            "                    )",
            "                )",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            nonce = None",
            "            if version > 1:",
            "                nonce = payload[\"load\"].pop(\"nonce\", None)",
            "",
            "            # TODO: test",
            "            try:",
            "                ret, req_opts = yield self.payload_handler(payload)",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                # always attempt to return an error to the minion",
            "                stream.write(\"Some exception handling minion payload\")",
            "                log.error(",
            "                    \"Some exception handling a payload from minion\", exc_info=True",
            "                )",
            "                stream.close()",
            "                raise salt.ext.tornado.gen.Return()",
            "",
            "            req_fun = req_opts.get(\"fun\", \"send\")",
            "            if req_fun == \"send_clear\":",
            "                stream.write(salt.transport.frame.frame_msg(ret, header=header))",
            "            elif req_fun == \"send\":",
            "                stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        self.crypticle.dumps(ret, nonce), header=header",
            "                    )",
            "                )",
            "            elif req_fun == \"send_private\":",
            "                stream.write(",
            "                    salt.transport.frame.frame_msg(",
            "                        self._encrypt_private(",
            "                            ret, req_opts[\"key\"], req_opts[\"tgt\"], nonce, sign_messages,",
            "                        ),",
            "                        header=header,",
            "                    )",
            "                )",
            "            else:",
            "                log.error(\"Unknown req_fun %s\", req_fun)",
            "                # always attempt to return an error to the minion",
            "                stream.write(\"Server-side exception handling payload\")",
            "                stream.close()",
            "        except salt.ext.tornado.gen.Return:",
            "            raise",
            "        except salt.ext.tornado.iostream.StreamClosedError:",
            "            # Stream was closed. This could happen if the remote side",
            "            # closed the connection on its end (eg in a timeout or shutdown",
            "            # situation).",
            "            log.error(\"Connection was unexpectedly closed\", exc_info=True)",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            # Absorb any other exceptions",
            "            log.error(\"Unexpected exception occurred: %s\", exc, exc_info=True)",
            "",
            "        raise salt.ext.tornado.gen.Return()",
            "",
            "",
            "class SaltMessageServer(salt.ext.tornado.tcpserver.TCPServer):",
            "    \"\"\"",
            "    Raw TCP server which will receive all of the TCP streams and re-assemble",
            "    messages that are sent through to us",
            "    \"\"\"",
            "",
            "    def __init__(self, message_handler, *args, **kwargs):",
            "        io_loop = (",
            "            kwargs.pop(\"io_loop\", None) or salt.ext.tornado.ioloop.IOLoop.current()",
            "        )",
            "        super().__init__(*args, **kwargs)",
            "        self.io_loop = io_loop",
            "        self.clients = []",
            "        self.message_handler = message_handler",
            "        self._shutting_down = False",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_stream(self, stream, address):",
            "        \"\"\"",
            "        Handle incoming streams and add messages to the incoming queue",
            "        \"\"\"",
            "        log.trace(\"Req client %s connected\", address)",
            "        self.clients.append((stream, address))",
            "        unpacker = salt.utils.msgpack.Unpacker()",
            "        try:",
            "            while True:",
            "                wire_bytes = yield stream.read_bytes(4096, partial=True)",
            "                unpacker.feed(wire_bytes)",
            "                for framed_msg in unpacker:",
            "                    framed_msg = salt.transport.frame.decode_embedded_strs(framed_msg)",
            "                    header = framed_msg[\"head\"]",
            "                    self.io_loop.spawn_callback(",
            "                        self.message_handler, stream, header, framed_msg[\"body\"]",
            "                    )",
            "",
            "        except salt.ext.tornado.iostream.StreamClosedError:",
            "            log.trace(\"req client disconnected %s\", address)",
            "            self.remove_client((stream, address))",
            "        except Exception as e:  # pylint: disable=broad-except",
            "            log.trace(\"other master-side exception: %s\", e)",
            "            self.remove_client((stream, address))",
            "            stream.close()",
            "",
            "    def remove_client(self, client):",
            "        try:",
            "            self.clients.remove(client)",
            "        except ValueError:",
            "            log.trace(\"Message server client was not in list to remove\")",
            "",
            "    def shutdown(self):",
            "        \"\"\"",
            "        Shutdown the whole server",
            "        \"\"\"",
            "        if self._shutting_down:",
            "            return",
            "        self._shutting_down = True",
            "        for item in self.clients:",
            "            client, address = item",
            "            client.close()",
            "            self.remove_client(item)",
            "        try:",
            "            self.stop()",
            "        except OSError as exc:",
            "            if exc.errno != 9:",
            "                raise",
            "",
            "",
            "if USE_LOAD_BALANCER:",
            "",
            "    class LoadBalancerWorker(SaltMessageServer):",
            "        \"\"\"",
            "        This will receive TCP connections from 'LoadBalancerServer' via",
            "        a multiprocessing queue.",
            "        Since the queue is shared amongst workers, only one worker will handle",
            "        a given connection.",
            "        \"\"\"",
            "",
            "        def __init__(self, socket_queue, message_handler, *args, **kwargs):",
            "            super().__init__(message_handler, *args, **kwargs)",
            "            self.socket_queue = socket_queue",
            "            self._stop = threading.Event()",
            "            self.thread = threading.Thread(target=self.socket_queue_thread)",
            "            self.thread.start()",
            "",
            "        def stop(self):",
            "            self._stop.set()",
            "            self.thread.join()",
            "",
            "        def socket_queue_thread(self):",
            "            try:",
            "                while True:",
            "                    try:",
            "                        client_socket, address = self.socket_queue.get(True, 1)",
            "                    except queue.Empty:",
            "                        if self._stop.is_set():",
            "                            break",
            "                        continue",
            "                    # 'self.io_loop' initialized in super class",
            "                    # 'salt.ext.tornado.tcpserver.TCPServer'.",
            "                    # 'self._handle_connection' defined in same super class.",
            "                    self.io_loop.spawn_callback(",
            "                        self._handle_connection, client_socket, address",
            "                    )",
            "            except (KeyboardInterrupt, SystemExit):",
            "                pass",
            "",
            "",
            "class TCPClientKeepAlive(salt.ext.tornado.tcpclient.TCPClient):",
            "    \"\"\"",
            "    Override _create_stream() in TCPClient to enable keep alive support.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, resolver=None):",
            "        self.opts = opts",
            "        super().__init__(resolver=resolver)",
            "",
            "    def _create_stream(",
            "        self, max_buffer_size, af, addr, **kwargs",
            "    ):  # pylint: disable=unused-argument,arguments-differ",
            "        \"\"\"",
            "        Override _create_stream() in TCPClient.",
            "",
            "        Tornado 4.5 added the kwargs 'source_ip' and 'source_port'.",
            "        Due to this, use **kwargs to swallow these and any future",
            "        kwargs to maintain compatibility.",
            "        \"\"\"",
            "        # Always connect in plaintext; we'll convert to ssl if necessary",
            "        # after one connection has completed.",
            "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "        _set_tcp_keepalive(sock, self.opts)",
            "        stream = salt.ext.tornado.iostream.IOStream(",
            "            sock, max_buffer_size=max_buffer_size",
            "        )",
            "        if salt.ext.tornado.version_info < (5,):",
            "            return stream.connect(addr)",
            "        return stream, stream.connect(addr)",
            "",
            "",
            "class SaltMessageClientPool(salt.transport.MessageClientPool):",
            "    \"\"\"",
            "    Wrapper class of SaltMessageClient to avoid blocking waiting while writing data to socket.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, args=None, kwargs=None):",
            "        super().__init__(SaltMessageClient, opts, args=args, kwargs=kwargs)",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def close(self):",
            "        for message_client in self.message_clients:",
            "            message_client.close()",
            "        self.message_clients = []",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect(self):",
            "        futures = []",
            "        for message_client in self.message_clients:",
            "            futures.append(message_client.connect())",
            "        yield futures",
            "        raise salt.ext.tornado.gen.Return(None)",
            "",
            "    def on_recv(self, *args, **kwargs):",
            "        for message_client in self.message_clients:",
            "            message_client.on_recv(*args, **kwargs)",
            "",
            "    def send(self, *args, **kwargs):",
            "        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))",
            "        return message_clients[0].send(*args, **kwargs)",
            "",
            "    def write_to_stream(self, *args, **kwargs):",
            "        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))",
            "        return message_clients[0]._stream.write(*args, **kwargs)",
            "",
            "",
            "# TODO consolidate with IPCClient",
            "# TODO: limit in-flight messages.",
            "# TODO: singleton? Something to not re-create the tcp connection so much",
            "class SaltMessageClient:",
            "    \"\"\"",
            "    Low-level message sending client",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        opts,",
            "        host,",
            "        port,",
            "        io_loop=None,",
            "        resolver=None,",
            "        connect_callback=None,",
            "        disconnect_callback=None,",
            "        source_ip=None,",
            "        source_port=None,",
            "    ):",
            "        self.opts = opts",
            "        self.host = host",
            "        self.port = port",
            "        self.source_ip = source_ip",
            "        self.source_port = source_port",
            "        self.connect_callback = connect_callback",
            "        self.disconnect_callback = disconnect_callback",
            "",
            "        self.io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        with salt.utils.asynchronous.current_ioloop(self.io_loop):",
            "            self._tcp_client = TCPClientKeepAlive(opts, resolver=resolver)",
            "",
            "        self._mid = 1",
            "        self._max_messages = int((1 << 31) - 2)  # number of IDs before we wrap",
            "",
            "        # TODO: max queue size",
            "        self.send_queue = []  # queue of messages to be sent",
            "        self.send_future_map = {}  # mapping of request_id -> Future",
            "        self.send_timeout_map = {}  # request_id -> timeout_callback",
            "",
            "        self._read_until_future = None",
            "        self._on_recv = None",
            "        self._closing = False",
            "        self._connecting_future = self.connect()",
            "        self._stream_return_future = salt.ext.tornado.concurrent.Future()",
            "        self.io_loop.spawn_callback(self._stream_return)",
            "",
            "    def _stop_io_loop(self):",
            "        if self.io_loop is not None:",
            "            self.io_loop.stop()",
            "",
            "    # TODO: timeout inflight sessions",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "        if hasattr(self, \"_stream\") and not self._stream.closed():",
            "            # If _stream_return() hasn't completed, it means the IO",
            "            # Loop is stopped (such as when using",
            "            # 'salt.utils.asynchronous.SyncWrapper'). Ensure that",
            "            # _stream_return() completes by restarting the IO Loop.",
            "            # This will prevent potential errors on shutdown.",
            "            try:",
            "                orig_loop = salt.ext.tornado.ioloop.IOLoop.current()",
            "                self.io_loop.make_current()",
            "                self._stream.close()",
            "                if self._read_until_future is not None:",
            "                    # This will prevent this message from showing up:",
            "                    # '[ERROR   ] Future exception was never retrieved:",
            "                    # StreamClosedError'",
            "                    # This happens because the logic is always waiting to read",
            "                    # the next message and the associated read future is marked",
            "                    # 'StreamClosedError' when the stream is closed.",
            "                    if self._read_until_future.done():",
            "                        self._read_until_future.exception()",
            "                    if (",
            "                        self.io_loop",
            "                        != salt.ext.tornado.ioloop.IOLoop.current(instance=False)",
            "                        or not self._stream_return_future.done()",
            "                    ):",
            "                        self.io_loop.add_future(",
            "                            self._stream_return_future,",
            "                            lambda future: self._stop_io_loop(),",
            "                        )",
            "                        self.io_loop.start()",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                log.info(\"Exception caught in SaltMessageClient.close: %s\", str(e))",
            "            finally:",
            "                orig_loop.make_current()",
            "        self._tcp_client.close()",
            "        self.io_loop = None",
            "        self._read_until_future = None",
            "        # Clear callback references to allow the object that they belong to",
            "        # to be deleted.",
            "        self.connect_callback = None",
            "        self.disconnect_callback = None",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def connect(self):",
            "        \"\"\"",
            "        Ask for this client to reconnect to the origin",
            "        \"\"\"",
            "        if hasattr(self, \"_connecting_future\") and not self._connecting_future.done():",
            "            future = self._connecting_future",
            "        else:",
            "            future = salt.ext.tornado.concurrent.Future()",
            "            self._connecting_future = future",
            "            self.io_loop.add_callback(self._connect)",
            "",
            "            # Add the callback only when a new future is created",
            "            if self.connect_callback is not None:",
            "",
            "                def handle_future(future):",
            "                    response = future.result()",
            "                    self.io_loop.add_callback(self.connect_callback, response)",
            "",
            "                future.add_done_callback(handle_future)",
            "",
            "        return future",
            "",
            "    # TODO: tcp backoff opts",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _connect(self):",
            "        \"\"\"",
            "        Try to connect for the rest of time!",
            "        \"\"\"",
            "        while True:",
            "            if self._closing:",
            "                break",
            "            try:",
            "                kwargs = {}",
            "                if self.source_ip or self.source_port:",
            "                    if salt.ext.tornado.version_info >= (4, 5):",
            "                        ### source_ip and source_port are supported only in Tornado >= 4.5",
            "                        # See http://www.tornadoweb.org/en/stable/releases/v4.5.0.html",
            "                        # Otherwise will just ignore these args",
            "                        kwargs = {",
            "                            \"source_ip\": self.source_ip,",
            "                            \"source_port\": self.source_port,",
            "                        }",
            "                    else:",
            "                        log.warning(",
            "                            \"If you need a certain source IP/port, consider upgrading Tornado >= 4.5\"",
            "                        )",
            "                with salt.utils.asynchronous.current_ioloop(self.io_loop):",
            "                    self._stream = yield self._tcp_client.connect(",
            "                        self.host, self.port, ssl_options=self.opts.get(\"ssl\"), **kwargs",
            "                    )",
            "                self._connecting_future.set_result(True)",
            "                break",
            "            except Exception as exc:  # pylint: disable=broad-except",
            "                log.warning(\"TCP Message Client encountered an exception %r\", exc)",
            "                yield salt.ext.tornado.gen.sleep(1)  # TODO: backoff",
            "                # self._connecting_future.set_exception(e)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _stream_return(self):",
            "        try:",
            "            while not self._closing and (",
            "                not self._connecting_future.done()",
            "                or self._connecting_future.result() is not True",
            "            ):",
            "                yield self._connecting_future",
            "            unpacker = salt.utils.msgpack.Unpacker()",
            "            while not self._closing:",
            "                try:",
            "                    self._read_until_future = self._stream.read_bytes(",
            "                        4096, partial=True",
            "                    )",
            "                    wire_bytes = yield self._read_until_future",
            "                    unpacker.feed(wire_bytes)",
            "                    for framed_msg in unpacker:",
            "                        framed_msg = salt.transport.frame.decode_embedded_strs(",
            "                            framed_msg",
            "                        )",
            "                        header = framed_msg[\"head\"]",
            "                        body = framed_msg[\"body\"]",
            "                        message_id = header.get(\"mid\")",
            "",
            "                        if message_id in self.send_future_map:",
            "                            self.send_future_map.pop(message_id).set_result(body)",
            "                            self.remove_message_timeout(message_id)",
            "                        else:",
            "                            if self._on_recv is not None:",
            "                                self.io_loop.spawn_callback(self._on_recv, header, body)",
            "                            else:",
            "                                log.error(",
            "                                    \"Got response for message_id %s that we are not tracking\",",
            "                                    message_id,",
            "                                )",
            "                except salt.ext.tornado.iostream.StreamClosedError as e:",
            "                    log.debug(",
            "                        \"tcp stream to %s:%s closed, unable to recv\",",
            "                        self.host,",
            "                        self.port,",
            "                    )",
            "                    for future in self.send_future_map.values():",
            "                        future.set_exception(e)",
            "                    self.send_future_map = {}",
            "                    if self._closing:",
            "                        return",
            "                    if self.disconnect_callback:",
            "                        self.disconnect_callback()",
            "                    # if the last connect finished, then we need to make a new one",
            "                    if self._connecting_future.done():",
            "                        self._connecting_future = self.connect()",
            "                    yield self._connecting_future",
            "                except TypeError:",
            "                    # This is an invalid transport",
            "                    if \"detect_mode\" in self.opts:",
            "                        log.info(",
            "                            \"There was an error trying to use TCP transport; \"",
            "                            \"attempting to fallback to another transport\"",
            "                        )",
            "                    else:",
            "                        raise SaltClientError",
            "                except Exception as e:  # pylint: disable=broad-except",
            "                    log.error(\"Exception parsing response\", exc_info=True)",
            "                    for future in self.send_future_map.values():",
            "                        future.set_exception(e)",
            "                    self.send_future_map = {}",
            "                    if self._closing:",
            "                        return",
            "                    if self.disconnect_callback:",
            "                        self.disconnect_callback()",
            "                    # if the last connect finished, then we need to make a new one",
            "                    if self._connecting_future.done():",
            "                        self._connecting_future = self.connect()",
            "                    yield self._connecting_future",
            "        finally:",
            "            self._stream_return_future.set_result(True)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _stream_send(self):",
            "        while (",
            "            not self._connecting_future.done()",
            "            or self._connecting_future.result() is not True",
            "        ):",
            "            yield self._connecting_future",
            "        while len(self.send_queue) > 0:",
            "            message_id, item = self.send_queue[0]",
            "            try:",
            "                yield self._stream.write(item)",
            "                del self.send_queue[0]",
            "            # if the connection is dead, lets fail this send, and make sure we",
            "            # attempt to reconnect",
            "            except salt.ext.tornado.iostream.StreamClosedError as e:",
            "                if message_id in self.send_future_map:",
            "                    self.send_future_map.pop(message_id).set_exception(e)",
            "                self.remove_message_timeout(message_id)",
            "                del self.send_queue[0]",
            "                if self._closing:",
            "                    return",
            "                if self.disconnect_callback:",
            "                    self.disconnect_callback()",
            "                # if the last connect finished, then we need to make a new one",
            "                if self._connecting_future.done():",
            "                    self._connecting_future = self.connect()",
            "                yield self._connecting_future",
            "",
            "    def _message_id(self):",
            "        wrap = False",
            "        while self._mid in self.send_future_map:",
            "            if self._mid >= self._max_messages:",
            "                if wrap:",
            "                    # this shouldn't ever happen, but just in case",
            "                    raise Exception(\"Unable to find available messageid\")",
            "                self._mid = 1",
            "                wrap = True",
            "            else:",
            "                self._mid += 1",
            "",
            "        return self._mid",
            "",
            "    # TODO: return a message object which takes care of multiplexing?",
            "    def on_recv(self, callback):",
            "        \"\"\"",
            "        Register a callback for received messages (that we didn't initiate)",
            "        \"\"\"",
            "        if callback is None:",
            "            self._on_recv = callback",
            "        else:",
            "",
            "            def wrap_recv(header, body):",
            "                callback(body)",
            "",
            "            self._on_recv = wrap_recv",
            "",
            "    def remove_message_timeout(self, message_id):",
            "        if message_id not in self.send_timeout_map:",
            "            return",
            "        timeout = self.send_timeout_map.pop(message_id)",
            "        self.io_loop.remove_timeout(timeout)",
            "",
            "    def timeout_message(self, message_id):",
            "        if message_id in self.send_timeout_map:",
            "            del self.send_timeout_map[message_id]",
            "        if message_id in self.send_future_map:",
            "            self.send_future_map.pop(message_id).set_exception(",
            "                SaltReqTimeoutError(\"Message timed out\")",
            "            )",
            "",
            "    def send(self, msg, timeout=None, callback=None, raw=False):",
            "        \"\"\"",
            "        Send given message, and return a future",
            "        \"\"\"",
            "        message_id = self._message_id()",
            "        header = {\"mid\": message_id}",
            "",
            "        future = salt.ext.tornado.concurrent.Future()",
            "        if callback is not None:",
            "",
            "            def handle_future(future):",
            "                response = future.result()",
            "                self.io_loop.add_callback(callback, response)",
            "",
            "            future.add_done_callback(handle_future)",
            "        # Add this future to the mapping",
            "        self.send_future_map[message_id] = future",
            "",
            "        if self.opts.get(\"detect_mode\") is True:",
            "            timeout = 1",
            "",
            "        if timeout is not None:",
            "            send_timeout = self.io_loop.call_later(",
            "                timeout, self.timeout_message, message_id",
            "            )",
            "            self.send_timeout_map[message_id] = send_timeout",
            "",
            "        # if we don't have a send queue, we need to spawn the callback to do the sending",
            "        if len(self.send_queue) == 0:",
            "            self.io_loop.spawn_callback(self._stream_send)",
            "        self.send_queue.append(",
            "            (message_id, salt.transport.frame.frame_msg(msg, header=header))",
            "        )",
            "        return future",
            "",
            "",
            "class Subscriber:",
            "    \"\"\"",
            "    Client object for use with the TCP publisher server",
            "    \"\"\"",
            "",
            "    def __init__(self, stream, address):",
            "        self.stream = stream",
            "        self.address = address",
            "        self._closing = False",
            "        self._read_until_future = None",
            "        self.id_ = None",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "        if not self.stream.closed():",
            "            self.stream.close()",
            "            if self._read_until_future is not None and self._read_until_future.done():",
            "                # This will prevent this message from showing up:",
            "                # '[ERROR   ] Future exception was never retrieved:",
            "                # StreamClosedError'",
            "                # This happens because the logic is always waiting to read",
            "                # the next message and the associated read future is marked",
            "                # 'StreamClosedError' when the stream is closed.",
            "                self._read_until_future.exception()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "class PubServer(salt.ext.tornado.tcpserver.TCPServer):",
            "    \"\"\"",
            "    TCP publisher",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, io_loop=None):",
            "        super().__init__(ssl_options=opts.get(\"ssl\"))",
            "        self.io_loop = io_loop",
            "        self.opts = opts",
            "        self._closing = False",
            "        self.clients = set()",
            "        self.aes_funcs = salt.master.AESFuncs(self.opts)",
            "        self.present = {}",
            "        self.presence_events = False",
            "        if self.opts.get(\"presence_events\", False):",
            "            tcp_only = True",
            "            for transport, _ in iter_transport_opts(self.opts):",
            "                if transport != \"tcp\":",
            "                    tcp_only = False",
            "            if tcp_only:",
            "                # Only when the transport is TCP only, the presence events will",
            "                # be handled here. Otherwise, it will be handled in the",
            "                # 'Maintenance' process.",
            "                self.presence_events = True",
            "",
            "        if self.presence_events:",
            "            self.event = salt.utils.event.get_event(",
            "                \"master\", opts=self.opts, listen=False",
            "            )",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "        self._closing = True",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _add_client_present(self, client):",
            "        id_ = client.id_",
            "        if id_ in self.present:",
            "            clients = self.present[id_]",
            "            clients.add(client)",
            "        else:",
            "            self.present[id_] = {client}",
            "            if self.presence_events:",
            "                data = {\"new\": [id_], \"lost\": []}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"change\", \"presence\")",
            "                )",
            "                data = {\"present\": list(self.present.keys())}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"present\", \"presence\")",
            "                )",
            "",
            "    def _remove_client_present(self, client):",
            "        id_ = client.id_",
            "        if id_ is None or id_ not in self.present:",
            "            # This is possible if _remove_client_present() is invoked",
            "            # before the minion's id is validated.",
            "            return",
            "",
            "        clients = self.present[id_]",
            "        if client not in clients:",
            "            # Since _remove_client_present() is potentially called from",
            "            # _stream_read() and/or publish_payload(), it is possible for",
            "            # it to be called twice, in which case we will get here.",
            "            # This is not an abnormal case, so no logging is required.",
            "            return",
            "",
            "        clients.remove(client)",
            "        if len(clients) == 0:",
            "            del self.present[id_]",
            "            if self.presence_events:",
            "                data = {\"new\": [], \"lost\": [id_]}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"change\", \"presence\")",
            "                )",
            "                data = {\"present\": list(self.present.keys())}",
            "                self.event.fire_event(",
            "                    data, salt.utils.event.tagify(\"present\", \"presence\")",
            "                )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _stream_read(self, client):",
            "        unpacker = salt.utils.msgpack.Unpacker()",
            "        while not self._closing:",
            "            try:",
            "                client._read_until_future = client.stream.read_bytes(4096, partial=True)",
            "                wire_bytes = yield client._read_until_future",
            "                unpacker.feed(wire_bytes)",
            "                for framed_msg in unpacker:",
            "                    framed_msg = salt.transport.frame.decode_embedded_strs(framed_msg)",
            "                    body = framed_msg[\"body\"]",
            "                    if body[\"enc\"] != \"aes\":",
            "                        # We only accept 'aes' encoded messages for 'id'",
            "                        continue",
            "                    crypticle = salt.crypt.Crypticle(",
            "                        self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "                    )",
            "                    load = crypticle.loads(body[\"load\"])",
            "                    load = salt.transport.frame.decode_embedded_strs(load)",
            "                    if not self.aes_funcs.verify_minion(load[\"id\"], load[\"tok\"]):",
            "                        continue",
            "                    client.id_ = load[\"id\"]",
            "                    self._add_client_present(client)",
            "            except salt.ext.tornado.iostream.StreamClosedError as e:",
            "                log.debug(\"tcp stream to %s closed, unable to recv\", client.address)",
            "                client.close()",
            "                self._remove_client_present(client)",
            "                self.clients.discard(client)",
            "                break",
            "            except Exception as e:  # pylint: disable=broad-except",
            "                log.error(",
            "                    \"Exception parsing response from %s\", client.address, exc_info=True",
            "                )",
            "                continue",
            "",
            "    def handle_stream(self, stream, address):",
            "        log.trace(\"Subscriber at %s connected\", address)",
            "        client = Subscriber(stream, address)",
            "        self.clients.add(client)",
            "        self.io_loop.spawn_callback(self._stream_read, client)",
            "",
            "    # TODO: ACK the publish through IPC",
            "    @salt.ext.tornado.gen.coroutine",
            "    def publish_payload(self, package, _):",
            "        log.debug(\"TCP PubServer sending payload: %s\", package)",
            "        payload = salt.transport.frame.frame_msg(package[\"payload\"])",
            "",
            "        to_remove = []",
            "        if \"topic_lst\" in package:",
            "            topic_lst = package[\"topic_lst\"]",
            "            for topic in topic_lst:",
            "                if topic in self.present:",
            "                    # This will rarely be a list of more than 1 item. It will",
            "                    # be more than 1 item if the minion disconnects from the",
            "                    # master in an unclean manner (eg cable yank), then",
            "                    # restarts and the master is yet to detect the disconnect",
            "                    # via TCP keep-alive.",
            "                    for client in self.present[topic]:",
            "                        try:",
            "                            # Write the packed str",
            "                            f = client.stream.write(payload)",
            "                            self.io_loop.add_future(f, lambda f: True)",
            "                        except salt.ext.tornado.iostream.StreamClosedError:",
            "                            to_remove.append(client)",
            "                else:",
            "                    log.debug(\"Publish target %s not connected\", topic)",
            "        else:",
            "            for client in self.clients:",
            "                try:",
            "                    # Write the packed str",
            "                    f = client.stream.write(payload)",
            "                    self.io_loop.add_future(f, lambda f: True)",
            "                except salt.ext.tornado.iostream.StreamClosedError:",
            "                    to_remove.append(client)",
            "        for client in to_remove:",
            "            log.debug(",
            "                \"Subscriber at %s has disconnected from publisher\", client.address",
            "            )",
            "            client.close()",
            "            self._remove_client_present(client)",
            "            self.clients.discard(client)",
            "        log.trace(\"TCP PubServer finished publishing payload\")",
            "",
            "",
            "class TCPPubServerChannel(salt.transport.server.PubServerChannel):",
            "    # TODO: opts!",
            "    # Based on default used in salt.ext.tornado.netutil.bind_sockets()",
            "    backlog = 128",
            "",
            "    def __init__(self, opts):",
            "        self.opts = opts",
            "        self.serial = salt.payload.Serial(self.opts)  # TODO: in init?",
            "        self.ckminions = salt.utils.minions.CkMinions(opts)",
            "        self.io_loop = None",
            "",
            "    def __setstate__(self, state):",
            "        salt.master.SMaster.secrets = state[\"secrets\"]",
            "        self.__init__(state[\"opts\"])",
            "",
            "    def __getstate__(self):",
            "        return {\"opts\": self.opts, \"secrets\": salt.master.SMaster.secrets}",
            "",
            "    def _publish_daemon(self, **kwargs):",
            "        \"\"\"",
            "        Bind to the interface specified in the configuration file",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        log_queue = kwargs.get(\"log_queue\")",
            "        if log_queue is not None:",
            "            salt.log.setup.set_multiprocessing_logging_queue(log_queue)",
            "        log_queue_level = kwargs.get(\"log_queue_level\")",
            "        if log_queue_level is not None:",
            "            salt.log.setup.set_multiprocessing_logging_level(log_queue_level)",
            "        salt.log.setup.setup_multiprocessing_logging(log_queue)",
            "",
            "        # Check if io_loop was set outside",
            "        if self.io_loop is None:",
            "            self.io_loop = salt.ext.tornado.ioloop.IOLoop.current()",
            "",
            "        # Spin up the publisher",
            "        pub_server = PubServer(self.opts, io_loop=self.io_loop)",
            "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)",
            "        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)",
            "        _set_tcp_keepalive(sock, self.opts)",
            "        sock.setblocking(0)",
            "        sock.bind((self.opts[\"interface\"], int(self.opts[\"publish_port\"])))",
            "        sock.listen(self.backlog)",
            "        # pub_server will take ownership of the socket",
            "        pub_server.add_socket(sock)",
            "",
            "        # Set up Salt IPC server",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = int(self.opts.get(\"tcp_master_publish_pull\", 4514))",
            "        else:",
            "            pull_uri = os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "",
            "        pull_sock = salt.transport.ipc.IPCMessageServer(",
            "            pull_uri, io_loop=self.io_loop, payload_handler=pub_server.publish_payload,",
            "        )",
            "",
            "        # Securely create socket",
            "        log.info(\"Starting the Salt Puller on %s\", pull_uri)",
            "        with salt.utils.files.set_umask(0o177):",
            "            pull_sock.start()",
            "",
            "        # run forever",
            "        try:",
            "            self.io_loop.start()",
            "        except (KeyboardInterrupt, SystemExit):",
            "            salt.log.setup.shutdown_multiprocessing_logging()",
            "",
            "    def pre_fork(self, process_manager, kwargs=None):",
            "        \"\"\"",
            "        Do anything necessary pre-fork. Since this is on the master side this will",
            "        primarily be used to create IPC channels and create our daemon process to",
            "        do the actual publishing",
            "        \"\"\"",
            "        process_manager.add_process(self._publish_daemon, kwargs=kwargs)",
            "",
            "    def publish(self, load):",
            "        \"\"\"",
            "        Publish \"load\" to minions",
            "        \"\"\"",
            "        payload = {\"enc\": \"aes\"}",
            "        load[\"serial\"] = salt.master.SMaster.get_serial()",
            "        crypticle = salt.crypt.Crypticle(",
            "            self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "        )",
            "        payload[\"load\"] = crypticle.dumps(load)",
            "        if self.opts[\"sign_pub_messages\"]:",
            "            master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "            log.debug(\"Signing data packet\")",
            "            payload[\"sig\"] = salt.crypt.sign_message(master_pem_path, payload[\"load\"])",
            "        # Use the Salt IPC server",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = int(self.opts.get(\"tcp_master_publish_pull\", 4514))",
            "        else:",
            "            pull_uri = os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "        # TODO: switch to the actual asynchronous interface",
            "        # pub_sock = salt.transport.ipc.IPCMessageClient(self.opts, io_loop=self.io_loop)",
            "        pub_sock = salt.utils.asynchronous.SyncWrapper(",
            "            salt.transport.ipc.IPCMessageClient, (pull_uri,), loop_kwarg=\"io_loop\",",
            "        )",
            "        pub_sock.connect()",
            "",
            "        int_payload = {\"payload\": self.serial.dumps(payload)}",
            "",
            "        # add some targeting stuff for lists only (for now)",
            "        if load[\"tgt_type\"] == \"list\" and not self.opts.get(\"order_masters\", False):",
            "            if isinstance(load[\"tgt\"], str):",
            "                # Fetch a list of minions that match",
            "                _res = self.ckminions.check_minions(",
            "                    load[\"tgt\"], tgt_type=load[\"tgt_type\"]",
            "                )",
            "                match_ids = _res[\"minions\"]",
            "",
            "                log.debug(\"Publish Side Match: %s\", match_ids)",
            "                # Send list of miions thru so zmq can target them",
            "                int_payload[\"topic_lst\"] = match_ids",
            "            else:",
            "                int_payload[\"topic_lst\"] = load[\"tgt\"]",
            "        # Send it over IPC!",
            "        pub_sock.send(int_payload)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "424": [
                "AsyncTCPReqChannel",
                "_crypted_transfer",
                "_do_transfer"
            ],
            "837": [
                "TCPReqServerChannel",
                "handle_message"
            ],
            "1675": [
                "TCPPubServerChannel",
                "publish"
            ]
        },
        "addLocation": [
            "shuup.front.urls",
            "salt.transport.tcp.AsyncTCPReqChannel.send"
        ]
    },
    "salt/transport/zeromq.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "     except ImportError:"
            },
            "1": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "         from Crypto.Cipher import PKCS1_OAEP  # nosec"
            },
            "2": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 69,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " log = logging.getLogger(__name__)"
            },
            "5": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 72,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "     rc = zmq_connect(socket, \"tcp://192.168.1.17:5555;192.168.1.1:5555\"); assert (rc == 0);"
            },
            "8": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 80,
                "PatchRowcode": "     Source: http://api.zeromq.org/4-1:zmq-tcp"
            },
            "9": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 81,
                "PatchRowcode": "     \"\"\""
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "     from salt.utils.zeromq import ip_bracket"
            },
            "12": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "     master_uri = \"tcp://{master_ip}:{master_port}\".format("
            },
            "14": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "         master_ip=ip_bracket(master_ip), master_port=master_port"
            },
            "15": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 87,
                "PatchRowcode": "     )"
            },
            "16": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "17": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "     if source_ip or source_port:"
            },
            "18": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "         if LIBZMQ_VERSION_INFO >= (4, 1, 6) and ZMQ_VERSION_INFO >= (16, 0, 1):"
            },
            "19": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "             # The source:port syntax for ZeroMQ has been added in libzmq 4.1.6"
            },
            "20": {
                "beforePatchRowNumber": 416,
                "afterPatchRowNumber": 417,
                "PatchRowcode": "         :param int tries: The number of times to make before failure"
            },
            "21": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 418,
                "PatchRowcode": "         :param int timeout: The number of seconds on a response before failing"
            },
            "22": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 419,
                "PatchRowcode": "         \"\"\""
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 420,
                "PatchRowcode": "+        nonce = uuid.uuid4().hex"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 421,
                "PatchRowcode": "+        if load and isinstance(load, dict):"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 422,
                "PatchRowcode": "+            load[\"nonce\"] = nonce"
            },
            "26": {
                "beforePatchRowNumber": 419,
                "afterPatchRowNumber": 423,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 420,
                "afterPatchRowNumber": 424,
                "PatchRowcode": "         @salt.ext.tornado.gen.coroutine"
            },
            "28": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 425,
                "PatchRowcode": "         def _do_transfer():"
            },
            "29": {
                "beforePatchRowNumber": 430,
                "afterPatchRowNumber": 434,
                "PatchRowcode": "             # communication, we do not subscribe to return events, we just"
            },
            "30": {
                "beforePatchRowNumber": 431,
                "afterPatchRowNumber": 435,
                "PatchRowcode": "             # upload the results to the master"
            },
            "31": {
                "beforePatchRowNumber": 432,
                "afterPatchRowNumber": 436,
                "PatchRowcode": "             if data:"
            },
            "32": {
                "beforePatchRowNumber": 433,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                data = self.auth.crypticle.loads(data, raw)"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 437,
                "PatchRowcode": "+                data = self.auth.crypticle.loads(data, raw, nonce)"
            },
            "34": {
                "beforePatchRowNumber": 434,
                "afterPatchRowNumber": 438,
                "PatchRowcode": "             if six.PY3 and not raw:"
            },
            "35": {
                "beforePatchRowNumber": 435,
                "afterPatchRowNumber": 439,
                "PatchRowcode": "                 data = salt.transport.frame.decode_embedded_strs(data)"
            },
            "36": {
                "beforePatchRowNumber": 436,
                "afterPatchRowNumber": 440,
                "PatchRowcode": "             raise salt.ext.tornado.gen.Return(data)"
            },
            "37": {
                "beforePatchRowNumber": 919,
                "afterPatchRowNumber": 923,
                "PatchRowcode": "         if req_fun == \"send_clear\":"
            },
            "38": {
                "beforePatchRowNumber": 920,
                "afterPatchRowNumber": 924,
                "PatchRowcode": "             stream.send(self.serial.dumps(ret))"
            },
            "39": {
                "beforePatchRowNumber": 921,
                "afterPatchRowNumber": 925,
                "PatchRowcode": "         elif req_fun == \"send\":"
            },
            "40": {
                "beforePatchRowNumber": 922,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            stream.send(self.serial.dumps(self.crypticle.dumps(ret)))"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 926,
                "PatchRowcode": "+            stream.send(self.serial.dumps(self.crypticle.dumps(ret, nonce)))"
            },
            "42": {
                "beforePatchRowNumber": 923,
                "afterPatchRowNumber": 927,
                "PatchRowcode": "         elif req_fun == \"send_private\":"
            },
            "43": {
                "beforePatchRowNumber": 924,
                "afterPatchRowNumber": 928,
                "PatchRowcode": "             stream.send("
            },
            "44": {
                "beforePatchRowNumber": 925,
                "afterPatchRowNumber": 929,
                "PatchRowcode": "                 self.serial.dumps("
            },
            "45": {
                "beforePatchRowNumber": 1180,
                "afterPatchRowNumber": 1184,
                "PatchRowcode": "         :param dict load: A load to be sent across the wire to minions"
            },
            "46": {
                "beforePatchRowNumber": 1181,
                "afterPatchRowNumber": 1185,
                "PatchRowcode": "         \"\"\""
            },
            "47": {
                "beforePatchRowNumber": 1182,
                "afterPatchRowNumber": 1186,
                "PatchRowcode": "         payload = {\"enc\": \"aes\"}"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1187,
                "PatchRowcode": "+        load[\"serial\"] = salt.master.SMaster.get_serial()"
            },
            "49": {
                "beforePatchRowNumber": 1183,
                "afterPatchRowNumber": 1188,
                "PatchRowcode": "         crypticle = salt.crypt.Crypticle("
            },
            "50": {
                "beforePatchRowNumber": 1184,
                "afterPatchRowNumber": 1189,
                "PatchRowcode": "             self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value"
            },
            "51": {
                "beforePatchRowNumber": 1185,
                "afterPatchRowNumber": 1190,
                "PatchRowcode": "         )"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Zeromq transport classes",
            "\"\"\"",
            "",
            "",
            "import copy",
            "import errno",
            "import hashlib",
            "import logging",
            "import os",
            "import signal",
            "import sys",
            "import threading",
            "import uuid",
            "import weakref",
            "from random import randint",
            "",
            "import salt.auth",
            "import salt.crypt",
            "import salt.ext.tornado",
            "import salt.ext.tornado.concurrent",
            "import salt.ext.tornado.gen",
            "import salt.ext.tornado.ioloop",
            "import salt.log.setup",
            "import salt.payload",
            "import salt.transport.client",
            "import salt.transport.mixins.auth",
            "import salt.transport.server",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.minions",
            "import salt.utils.process",
            "import salt.utils.stringutils",
            "import salt.utils.verify",
            "import salt.utils.versions",
            "import salt.utils.zeromq",
            "import zmq.error",
            "import zmq.eventloop.ioloop",
            "import zmq.eventloop.zmqstream",
            "from salt._compat import ipaddress",
            "from salt.exceptions import SaltException, SaltReqTimeoutError",
            "from salt.ext import six",
            "from salt.utils.zeromq import (",
            "    LIBZMQ_VERSION_INFO,",
            "    ZMQ_VERSION_INFO,",
            "    ZMQDefaultLoop,",
            "    install_zmq,",
            "    zmq,",
            ")",
            "",
            "try:",
            "    import zmq.utils.monitor",
            "",
            "    HAS_ZMQ_MONITOR = True",
            "except ImportError:",
            "    HAS_ZMQ_MONITOR = False",
            "",
            "",
            "try:",
            "    from M2Crypto import RSA",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "    try:",
            "        from Cryptodome.Cipher import PKCS1_OAEP",
            "    except ImportError:",
            "        from Crypto.Cipher import PKCS1_OAEP  # nosec",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def _get_master_uri(master_ip, master_port, source_ip=None, source_port=None):",
            "    \"\"\"",
            "    Return the ZeroMQ URI to connect the Minion to the Master.",
            "    It supports different source IP / port, given the ZeroMQ syntax:",
            "    // Connecting using a IP address and bind to an IP address",
            "    rc = zmq_connect(socket, \"tcp://192.168.1.17:5555;192.168.1.1:5555\"); assert (rc == 0);",
            "    Source: http://api.zeromq.org/4-1:zmq-tcp",
            "    \"\"\"",
            "    from salt.utils.zeromq import ip_bracket",
            "",
            "    master_uri = \"tcp://{master_ip}:{master_port}\".format(",
            "        master_ip=ip_bracket(master_ip), master_port=master_port",
            "    )",
            "",
            "    if source_ip or source_port:",
            "        if LIBZMQ_VERSION_INFO >= (4, 1, 6) and ZMQ_VERSION_INFO >= (16, 0, 1):",
            "            # The source:port syntax for ZeroMQ has been added in libzmq 4.1.6",
            "            # which is included in the pyzmq wheels starting with 16.0.1.",
            "            if source_ip and source_port:",
            "                master_uri = \"tcp://{source_ip}:{source_port};{master_ip}:{master_port}\".format(",
            "                    source_ip=ip_bracket(source_ip),",
            "                    source_port=source_port,",
            "                    master_ip=ip_bracket(master_ip),",
            "                    master_port=master_port,",
            "                )",
            "            elif source_ip and not source_port:",
            "                master_uri = \"tcp://{source_ip}:0;{master_ip}:{master_port}\".format(",
            "                    source_ip=ip_bracket(source_ip),",
            "                    master_ip=ip_bracket(master_ip),",
            "                    master_port=master_port,",
            "                )",
            "            elif source_port and not source_ip:",
            "                ip_any = (",
            "                    \"0.0.0.0\"",
            "                    if ipaddress.ip_address(master_ip).version == 4",
            "                    else ip_bracket(\"::\")",
            "                )",
            "                master_uri = \"tcp://{ip_any}:{source_port};{master_ip}:{master_port}\".format(",
            "                    ip_any=ip_any,",
            "                    source_port=source_port,",
            "                    master_ip=ip_bracket(master_ip),",
            "                    master_port=master_port,",
            "                )",
            "        else:",
            "            log.warning(",
            "                \"Unable to connect to the Master using a specific source IP / port\"",
            "            )",
            "            log.warning(\"Consider upgrading to pyzmq >= 16.0.1 and libzmq >= 4.1.6\")",
            "            log.warning(",
            "                \"Specific source IP / port for connecting to master returner port: configuraion ignored\"",
            "            )",
            "",
            "    return master_uri",
            "",
            "",
            "class AsyncZeroMQReqChannel(salt.transport.client.ReqChannel):",
            "    \"\"\"",
            "    Encapsulate sending routines to ZeroMQ.",
            "",
            "    ZMQ Channels default to 'crypt=aes'",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    # mapping of io_loop -> {key -> channel}",
            "    instance_map = weakref.WeakKeyDictionary()",
            "    async_methods = [",
            "        \"crypted_transfer_decode_dictentry\",",
            "        \"_crypted_transfer\",",
            "        \"_do_transfer\",",
            "        \"_uncrypted_transfer\",",
            "        \"send\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __new__(cls, opts, **kwargs):",
            "        \"\"\"",
            "        Only create one instance of channel per __key()",
            "        \"\"\"",
            "",
            "        # do we have any mapping for this io_loop",
            "        io_loop = kwargs.get(\"io_loop\")",
            "        if io_loop is None:",
            "            install_zmq()",
            "            io_loop = ZMQDefaultLoop.current()",
            "        if io_loop not in cls.instance_map:",
            "            cls.instance_map[io_loop] = weakref.WeakValueDictionary()",
            "        loop_instance_map = cls.instance_map[io_loop]",
            "",
            "        key = cls.__key(opts, **kwargs)",
            "        obj = loop_instance_map.get(key)",
            "        if obj is None:",
            "            log.debug(\"Initializing new AsyncZeroMQReqChannel for %s\", key)",
            "            # we need to make a local variable for this, as we are going to store",
            "            # it in a WeakValueDictionary-- which will remove the item if no one",
            "            # references it-- this forces a reference while we return to the caller",
            "            obj = object.__new__(cls)",
            "            obj.__singleton_init__(opts, **kwargs)",
            "            obj._instance_key = key",
            "            loop_instance_map[key] = obj",
            "            obj._refcount = 1",
            "            obj._refcount_lock = threading.RLock()",
            "            log.trace(",
            "                \"Inserted key into loop_instance_map id %s for key %s and process %s\",",
            "                id(loop_instance_map),",
            "                key,",
            "                os.getpid(),",
            "            )",
            "        else:",
            "            with obj._refcount_lock:",
            "                obj._refcount += 1",
            "            log.debug(\"Re-using AsyncZeroMQReqChannel for %s\", key)",
            "        return obj",
            "",
            "    def __deepcopy__(self, memo):",
            "        cls = self.__class__",
            "        # pylint: disable=too-many-function-args",
            "        result = cls.__new__(cls, copy.deepcopy(self.opts, memo))",
            "        # pylint: enable=too-many-function-args",
            "        memo[id(self)] = result",
            "        for key in self.__dict__:",
            "            if key in (\"_io_loop\", \"_refcount\", \"_refcount_lock\"):",
            "                continue",
            "                # The _io_loop has a thread Lock which will fail to be deep",
            "                # copied. Skip it because it will just be recreated on the",
            "                # new copy.",
            "            if key == \"message_client\":",
            "                # Recreate the message client because it will fail to be deep",
            "                # copied. The reason is the same as the io_loop skip above.",
            "                setattr(",
            "                    result,",
            "                    key,",
            "                    AsyncReqMessageClientPool(",
            "                        result.opts,",
            "                        args=(result.opts, self.master_uri,),",
            "                        kwargs={\"io_loop\": self._io_loop},",
            "                    ),",
            "                )",
            "",
            "                continue",
            "            setattr(result, key, copy.deepcopy(self.__dict__[key], memo))",
            "        return result",
            "",
            "    @classmethod",
            "    def force_close_all_instances(cls):",
            "        \"\"\"",
            "        Will force close all instances",
            "",
            "        ZMQ can hang on quit if left to deconstruct on its own.",
            "        This because is deconstructs out of order.",
            "",
            "        :return: None",
            "        \"\"\"",
            "        for weak_dict in list(cls.instance_map.values()):",
            "            for instance in list(weak_dict.values()):",
            "                instance.close()",
            "",
            "    @classmethod",
            "    def __key(cls, opts, **kwargs):",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            kwargs.get(\"master_uri\", opts.get(\"master_uri\")),  # master ID",
            "            kwargs.get(\"crypt\", \"aes\"),  # TODO: use the same channel for crypt",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, **kwargs):",
            "        pass",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, **kwargs):",
            "        self.opts = dict(opts)",
            "        self.ttype = \"zeromq\"",
            "",
            "        # crypt defaults to 'aes'",
            "        self.crypt = kwargs.get(\"crypt\", \"aes\")",
            "",
            "        if \"master_uri\" in kwargs:",
            "            self.opts[\"master_uri\"] = kwargs[\"master_uri\"]",
            "",
            "        self._io_loop = kwargs.get(\"io_loop\")",
            "        if self._io_loop is None:",
            "            install_zmq()",
            "            self._io_loop = ZMQDefaultLoop.current()",
            "",
            "        if self.crypt != \"clear\":",
            "            # we don't need to worry about auth as a kwarg, since its a singleton",
            "            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self._io_loop)",
            "        log.debug(",
            "            \"Connecting the Minion to the Master URI (for the return server): %s\",",
            "            self.master_uri,",
            "        )",
            "        self.message_client = AsyncReqMessageClientPool(",
            "            self.opts,",
            "            args=(self.opts, self.master_uri,),",
            "            kwargs={\"io_loop\": self._io_loop},",
            "        )",
            "        self._closing = False",
            "",
            "    def close(self):",
            "        \"\"\"",
            "        Since the message_client creates sockets and assigns them to the IOLoop we have to",
            "        specifically destroy them, since we aren't the only ones with references to the FDs",
            "        \"\"\"",
            "        if self._closing:",
            "            return",
            "",
            "        if self._refcount > 1:",
            "            # Decrease refcount",
            "            with self._refcount_lock:",
            "                self._refcount -= 1",
            "            log.debug(",
            "                \"This is not the last %s instance. Not closing yet.\",",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "",
            "        log.debug(\"Closing %s instance\", self.__class__.__name__)",
            "        self._closing = True",
            "        if hasattr(self, \"message_client\"):",
            "            self.message_client.close()",
            "",
            "        # Remove the entry from the instance map so that a closed entry may not",
            "        # be reused.",
            "        # This forces this operation even if the reference count of the entry",
            "        # has not yet gone to zero.",
            "        if self._io_loop in self.__class__.instance_map:",
            "            loop_instance_map = self.__class__.instance_map[self._io_loop]",
            "            if self._instance_key in loop_instance_map:",
            "                del loop_instance_map[self._instance_key]",
            "            if not loop_instance_map:",
            "                del self.__class__.instance_map[self._io_loop]",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        with self._refcount_lock:",
            "            # Make sure we actually close no matter if something",
            "            # went wrong with our ref counting",
            "            self._refcount = 1",
            "        try:",
            "            self.close()",
            "        except OSError as exc:",
            "            if exc.errno != errno.EBADF:",
            "                # If its not a bad file descriptor error, raise",
            "                raise",
            "",
            "    # pylint: enable=W1701",
            "",
            "    @property",
            "    def master_uri(self):",
            "        if \"master_uri\" in self.opts:",
            "            return self.opts[\"master_uri\"]",
            "",
            "        # if by chance master_uri is not there..",
            "        if \"master_ip\" in self.opts:",
            "            return _get_master_uri(",
            "                self.opts[\"master_ip\"],",
            "                self.opts[\"master_port\"],",
            "                source_ip=self.opts.get(\"source_ip\"),",
            "                source_port=self.opts.get(\"source_ret_port\"),",
            "            )",
            "",
            "        # if we've reached here something is very abnormal",
            "        raise SaltException(\"ReqChannel: missing master_uri/master_ip in self.opts\")",
            "",
            "    def _package_load(self, load):",
            "        return {",
            "            \"enc\": self.crypt,",
            "            \"load\": load,",
            "            \"version\": 2,",
            "        }",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def crypted_transfer_decode_dictentry(",
            "        self, load, dictkey=None, tries=3, timeout=60",
            "    ):",
            "        nonce = uuid.uuid4().hex",
            "        load[\"nonce\"] = nonce",
            "        if not self.auth.authenticated:",
            "            # Return control back to the caller, continue when authentication succeeds",
            "            yield self.auth.authenticate()",
            "",
            "        # Return control to the caller. When send() completes, resume by",
            "        # populating ret with the Future.result",
            "        ret = yield self.message_client.send(",
            "            self._package_load(self.auth.crypticle.dumps(load)),",
            "            timeout=timeout,",
            "            tries=tries,",
            "        )",
            "",
            "        if \"key\" not in ret:",
            "            # Reauth in the case our key is deleted on the master side.",
            "            yield self.auth.authenticate()",
            "            ret = yield self.message_client.send(",
            "                self._package_load(self.auth.crypticle.dumps(load)),",
            "                timeout=timeout,",
            "                tries=tries,",
            "            )",
            "",
            "        key = self.auth.get_keys()",
            "        if HAS_M2:",
            "            aes = key.private_decrypt(ret[\"key\"], RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(key)",
            "            aes = cipher.decrypt(ret[\"key\"])",
            "",
            "        # Decrypt using the public key.",
            "        pcrypt = salt.crypt.Crypticle(self.opts, aes)",
            "        signed_msg = pcrypt.loads(ret[dictkey])",
            "",
            "        # Validate the master's signature.",
            "        master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion_master.pub\")",
            "        if not salt.crypt.verify_signature(",
            "            master_pubkey_path, signed_msg[\"data\"], signed_msg[\"sig\"]",
            "        ):",
            "            raise salt.crypt.AuthenticationError(",
            "                \"Pillar payload signature failed to validate.\"",
            "            )",
            "",
            "        # Make sure the signed key matches the key we used to decrypt the data.",
            "        data = salt.payload.Serial({}).loads(signed_msg[\"data\"])",
            "        if data[\"key\"] != ret[\"key\"]:",
            "            raise salt.crypt.AuthenticationError(\"Key verification failed.\")",
            "",
            "        # Validate the nonce.",
            "        if data[\"nonce\"] != nonce:",
            "            raise salt.crypt.AuthenticationError(\"Pillar nonce verification failed.\")",
            "        raise salt.ext.tornado.gen.Return(data[\"pillar\"])",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _crypted_transfer(self, load, tries=3, timeout=60, raw=False):",
            "        \"\"\"",
            "        Send a load across the wire, with encryption",
            "",
            "        In case of authentication errors, try to renegotiate authentication",
            "        and retry the method.",
            "",
            "        Indeed, we can fail too early in case of a master restart during a",
            "        minion state execution call",
            "",
            "        :param dict load: A load to send across the wire",
            "        :param int tries: The number of times to make before failure",
            "        :param int timeout: The number of seconds on a response before failing",
            "        \"\"\"",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def _do_transfer():",
            "            # Yield control to the caller. When send() completes, resume by populating data with the Future.result",
            "            data = yield self.message_client.send(",
            "                self._package_load(self.auth.crypticle.dumps(load)),",
            "                timeout=timeout,",
            "                tries=tries,",
            "            )",
            "            # we may not have always data",
            "            # as for example for saltcall ret submission, this is a blind",
            "            # communication, we do not subscribe to return events, we just",
            "            # upload the results to the master",
            "            if data:",
            "                data = self.auth.crypticle.loads(data, raw)",
            "            if six.PY3 and not raw:",
            "                data = salt.transport.frame.decode_embedded_strs(data)",
            "            raise salt.ext.tornado.gen.Return(data)",
            "",
            "        if not self.auth.authenticated:",
            "            # Return control back to the caller, resume when authentication succeeds",
            "            yield self.auth.authenticate()",
            "        try:",
            "            # We did not get data back the first time. Retry.",
            "            ret = yield _do_transfer()",
            "        except salt.crypt.AuthenticationError:",
            "            # If auth error, return control back to the caller, continue when authentication succeeds",
            "            yield self.auth.authenticate()",
            "            ret = yield _do_transfer()",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _uncrypted_transfer(self, load, tries=3, timeout=60):",
            "        \"\"\"",
            "        Send a load across the wire in cleartext",
            "",
            "        :param dict load: A load to send across the wire",
            "        :param int tries: The number of times to make before failure",
            "        :param int timeout: The number of seconds on a response before failing",
            "        \"\"\"",
            "        ret = yield self.message_client.send(",
            "            self._package_load(load), timeout=timeout, tries=tries,",
            "        )",
            "",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def send(self, load, tries=3, timeout=60, raw=False):",
            "        \"\"\"",
            "        Send a request, return a future which will complete when we send the message",
            "        \"\"\"",
            "        if self.crypt == \"clear\":",
            "            ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)",
            "        else:",
            "            ret = yield self._crypted_transfer(",
            "                load, tries=tries, timeout=timeout, raw=raw",
            "            )",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "",
            "class AsyncZeroMQPubChannel(",
            "    salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel",
            "):",
            "    \"\"\"",
            "    A transport channel backed by ZeroMQ for a Salt Publisher to use to",
            "    publish commands to connected minions",
            "    \"\"\"",
            "",
            "    async_methods = [",
            "        \"connect\",",
            "        \"_decode_messages\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        self.opts = opts",
            "        self.ttype = \"zeromq\"",
            "        self.io_loop = kwargs.get(\"io_loop\")",
            "",
            "        if self.io_loop is None:",
            "            install_zmq()",
            "            self.io_loop = ZMQDefaultLoop.current()",
            "",
            "        self.hexid = hashlib.sha1(",
            "            salt.utils.stringutils.to_bytes(self.opts[\"id\"])",
            "        ).hexdigest()",
            "        self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.context = zmq.Context()",
            "        self._socket = self.context.socket(zmq.SUB)",
            "",
            "        if self.opts[\"zmq_filtering\"]:",
            "            # TODO: constants file for \"broadcast\"",
            "            self._socket.setsockopt(zmq.SUBSCRIBE, b\"broadcast\")",
            "            if self.opts.get(\"__role\") == \"syndic\":",
            "                self._socket.setsockopt(zmq.SUBSCRIBE, b\"syndic\")",
            "            else:",
            "                self._socket.setsockopt(",
            "                    zmq.SUBSCRIBE, salt.utils.stringutils.to_bytes(self.hexid)",
            "                )",
            "        else:",
            "            self._socket.setsockopt(zmq.SUBSCRIBE, b\"\")",
            "",
            "        self._socket.setsockopt(",
            "            zmq.IDENTITY, salt.utils.stringutils.to_bytes(self.opts[\"id\"])",
            "        )",
            "",
            "        # TODO: cleanup all the socket opts stuff",
            "        if hasattr(zmq, \"TCP_KEEPALIVE\"):",
            "            self._socket.setsockopt(zmq.TCP_KEEPALIVE, self.opts[\"tcp_keepalive\"])",
            "            self._socket.setsockopt(",
            "                zmq.TCP_KEEPALIVE_IDLE, self.opts[\"tcp_keepalive_idle\"]",
            "            )",
            "            self._socket.setsockopt(",
            "                zmq.TCP_KEEPALIVE_CNT, self.opts[\"tcp_keepalive_cnt\"]",
            "            )",
            "            self._socket.setsockopt(",
            "                zmq.TCP_KEEPALIVE_INTVL, self.opts[\"tcp_keepalive_intvl\"]",
            "            )",
            "",
            "        recon_delay = self.opts[\"recon_default\"]",
            "",
            "        if self.opts[\"recon_randomize\"]:",
            "            recon_delay = randint(",
            "                self.opts[\"recon_default\"],",
            "                self.opts[\"recon_default\"] + self.opts[\"recon_max\"],",
            "            )",
            "",
            "            log.debug(",
            "                \"Generated random reconnect delay between '%sms' and '%sms' (%s)\",",
            "                self.opts[\"recon_default\"],",
            "                self.opts[\"recon_default\"] + self.opts[\"recon_max\"],",
            "                recon_delay,",
            "            )",
            "",
            "        log.debug(\"Setting zmq_reconnect_ivl to '%sms'\", recon_delay)",
            "        self._socket.setsockopt(zmq.RECONNECT_IVL, recon_delay)",
            "",
            "        if hasattr(zmq, \"RECONNECT_IVL_MAX\"):",
            "            log.debug(",
            "                \"Setting zmq_reconnect_ivl_max to '%sms'\",",
            "                self.opts[\"recon_default\"] + self.opts[\"recon_max\"],",
            "            )",
            "",
            "            self._socket.setsockopt(zmq.RECONNECT_IVL_MAX, self.opts[\"recon_max\"])",
            "",
            "        if (self.opts[\"ipv6\"] is True or \":\" in self.opts[\"master_ip\"]) and hasattr(",
            "            zmq, \"IPV4ONLY\"",
            "        ):",
            "            # IPv6 sockets work for both IPv6 and IPv4 addresses",
            "            self._socket.setsockopt(zmq.IPV4ONLY, 0)",
            "",
            "        if HAS_ZMQ_MONITOR and self.opts[\"zmq_monitor\"]:",
            "            self._monitor = ZeroMQSocketMonitor(self._socket)",
            "            self._monitor.start_io_loop(self.io_loop)",
            "",
            "    def close(self):",
            "        if hasattr(self, \"_monitor\") and self._monitor is not None:",
            "            self._monitor.stop()",
            "            self._monitor = None",
            "        if hasattr(self, \"_stream\"):",
            "            if ZMQ_VERSION_INFO < (14, 3, 0):",
            "                # stream.close() doesn't work properly on pyzmq < 14.3.0",
            "                self._stream.io_loop.remove_handler(self._stream.socket)",
            "                self._stream.socket.close(0)",
            "            else:",
            "                self._stream.close(0)",
            "        elif hasattr(self, \"_socket\"):",
            "            self._socket.close(0)",
            "        if hasattr(self, \"context\") and self.context.closed is False:",
            "            self.context.term()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    # TODO: this is the time to see if we are connected, maybe use the req channel to guess?",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect(self):",
            "        if not self.auth.authenticated:",
            "            yield self.auth.authenticate()",
            "",
            "        # if this is changed from the default, we assume it was intentional",
            "        if int(self.opts.get(\"publish_port\", 4506)) != 4506:",
            "            self.publish_port = self.opts.get(\"publish_port\")",
            "        # else take the relayed publish_port master reports",
            "        else:",
            "            self.publish_port = self.auth.creds[\"publish_port\"]",
            "",
            "        log.debug(",
            "            \"Connecting the Minion to the Master publish port, using the URI: %s\",",
            "            self.master_pub,",
            "        )",
            "        self._socket.connect(self.master_pub)",
            "",
            "    @property",
            "    def master_pub(self):",
            "        \"\"\"",
            "        Return the master publish port",
            "        \"\"\"",
            "        return _get_master_uri(",
            "            self.opts[\"master_ip\"],",
            "            self.publish_port,",
            "            source_ip=self.opts.get(\"source_ip\"),",
            "            source_port=self.opts.get(\"source_publish_port\"),",
            "        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _decode_messages(self, messages):",
            "        \"\"\"",
            "        Take the zmq messages, decrypt/decode them into a payload",
            "",
            "        :param list messages: A list of messages to be decoded",
            "        \"\"\"",
            "        messages_len = len(messages)",
            "        # if it was one message, then its old style",
            "        if messages_len == 1:",
            "            payload = self.serial.loads(messages[0])",
            "        # 2 includes a header which says who should do it",
            "        elif messages_len == 2:",
            "            message_target = salt.utils.stringutils.to_str(messages[0])",
            "            if (",
            "                self.opts.get(\"__role\") != \"syndic\"",
            "                and message_target not in (\"broadcast\", self.hexid)",
            "            ) or (",
            "                self.opts.get(\"__role\") == \"syndic\"",
            "                and message_target not in (\"broadcast\", \"syndic\")",
            "            ):",
            "                log.debug(\"Publish received for not this minion: %s\", message_target)",
            "                raise salt.ext.tornado.gen.Return(None)",
            "            payload = self.serial.loads(messages[1])",
            "        else:",
            "            raise Exception(",
            "                (",
            "                    \"Invalid number of messages ({}) in zeromq pub\"",
            "                    \"message from master\"",
            "                ).format(len(messages_len))",
            "            )",
            "        # Yield control back to the caller. When the payload has been decoded, assign",
            "        # the decoded payload to 'ret' and resume operation",
            "        ret = yield self._decode_payload(payload)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @property",
            "    def stream(self):",
            "        \"\"\"",
            "        Return the current zmqstream, creating one if necessary",
            "        \"\"\"",
            "        if not hasattr(self, \"_stream\"):",
            "            self._stream = zmq.eventloop.zmqstream.ZMQStream(",
            "                self._socket, io_loop=self.io_loop",
            "            )",
            "        return self._stream",
            "",
            "    def on_recv(self, callback):",
            "        \"\"\"",
            "        Register a callback for received messages (that we didn't initiate)",
            "",
            "        :param func callback: A function which should be called when data is received",
            "        \"\"\"",
            "        if callback is None:",
            "            return self.stream.on_recv(None)",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def wrap_callback(messages):",
            "            payload = yield self._decode_messages(messages)",
            "            if payload is not None:",
            "                callback(payload)",
            "",
            "        return self.stream.on_recv(wrap_callback)",
            "",
            "",
            "class ZeroMQReqServerChannel(",
            "    salt.transport.mixins.auth.AESReqServerMixin, salt.transport.server.ReqServerChannel",
            "):",
            "    def __init__(self, opts):",
            "        salt.transport.server.ReqServerChannel.__init__(self, opts)",
            "        self._closing = False",
            "",
            "    def zmq_device(self):",
            "        \"\"\"",
            "        Multiprocessing target for the zmq queue device",
            "        \"\"\"",
            "        self.__setup_signals()",
            "        salt.utils.process.appendproctitle(\"MWorkerQueue\")",
            "        self.context = zmq.Context(self.opts[\"worker_threads\"])",
            "        # Prepare the zeromq sockets",
            "        self.uri = \"tcp://{interface}:{ret_port}\".format(**self.opts)",
            "        self.clients = self.context.socket(zmq.ROUTER)",
            "        if self.opts[\"ipv6\"] is True and hasattr(zmq, \"IPV4ONLY\"):",
            "            # IPv6 sockets work for both IPv6 and IPv4 addresses",
            "            self.clients.setsockopt(zmq.IPV4ONLY, 0)",
            "        self.clients.setsockopt(zmq.BACKLOG, self.opts.get(\"zmq_backlog\", 1000))",
            "        self._start_zmq_monitor()",
            "        self.workers = self.context.socket(zmq.DEALER)",
            "",
            "        if self.opts[\"mworker_queue_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting mworker_queue niceness to %d\",",
            "                self.opts[\"mworker_queue_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"mworker_queue_niceness\"])",
            "",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            self.w_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_workers\", 4515)",
            "            )",
            "        else:",
            "            self.w_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"workers.ipc\")",
            "            )",
            "",
            "        log.info(\"Setting up the master communication server\")",
            "        self.clients.bind(self.uri)",
            "        self.workers.bind(self.w_uri)",
            "",
            "        while True:",
            "            if self.clients.closed or self.workers.closed:",
            "                break",
            "            try:",
            "                zmq.device(zmq.QUEUE, self.clients, self.workers)",
            "            except zmq.ZMQError as exc:",
            "                if exc.errno == errno.EINTR:",
            "                    continue",
            "                raise",
            "            except (KeyboardInterrupt, SystemExit):",
            "                break",
            "",
            "    def close(self):",
            "        \"\"\"",
            "        Cleanly shutdown the router socket",
            "        \"\"\"",
            "        if self._closing:",
            "            return",
            "        log.info(\"MWorkerQueue under PID %s is closing\", os.getpid())",
            "        self._closing = True",
            "        # pylint: disable=E0203",
            "        if getattr(self, \"_monitor\", None) is not None:",
            "            self._monitor.stop()",
            "            self._monitor = None",
            "        if getattr(self, \"_w_monitor\", None) is not None:",
            "            self._w_monitor.stop()",
            "            self._w_monitor = None",
            "        if hasattr(self, \"clients\") and self.clients.closed is False:",
            "            self.clients.close()",
            "        if hasattr(self, \"workers\") and self.workers.closed is False:",
            "            self.workers.close()",
            "        if hasattr(self, \"stream\"):",
            "            self.stream.close()",
            "        if hasattr(self, \"_socket\") and self._socket.closed is False:",
            "            self._socket.close()",
            "        if hasattr(self, \"context\") and self.context.closed is False:",
            "            self.context.term()",
            "        # pylint: enable=E0203",
            "",
            "    def pre_fork(self, process_manager):",
            "        \"\"\"",
            "        Pre-fork we need to create the zmq router device",
            "",
            "        :param func process_manager: An instance of salt.utils.process.ProcessManager",
            "        \"\"\"",
            "        salt.transport.mixins.auth.AESReqServerMixin.pre_fork(self, process_manager)",
            "        process_manager.add_process(self.zmq_device)",
            "",
            "    def _start_zmq_monitor(self):",
            "        \"\"\"",
            "        Starts ZMQ monitor for debugging purposes.",
            "        :return:",
            "        \"\"\"",
            "        # Socket monitor shall be used the only for debug",
            "        # purposes so using threading doesn't look too bad here",
            "",
            "        if HAS_ZMQ_MONITOR and self.opts[\"zmq_monitor\"]:",
            "            log.debug(\"Starting ZMQ monitor\")",
            "            import threading",
            "",
            "            self._w_monitor = ZeroMQSocketMonitor(self._socket)",
            "            threading.Thread(target=self._w_monitor.start_poll).start()",
            "            log.debug(\"ZMQ monitor has been started started\")",
            "",
            "    def post_fork(self, payload_handler, io_loop):",
            "        \"\"\"",
            "        After forking we need to create all of the local sockets to listen to the",
            "        router",
            "",
            "        :param func payload_handler: A function to called to handle incoming payloads as",
            "                                     they are picked up off the wire",
            "        :param IOLoop io_loop: An instance of a Tornado IOLoop, to handle event scheduling",
            "        \"\"\"",
            "        self.payload_handler = payload_handler",
            "        self.io_loop = io_loop",
            "",
            "        self.context = zmq.Context(1)",
            "        self._socket = self.context.socket(zmq.REP)",
            "        self._start_zmq_monitor()",
            "",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            self.w_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_workers\", 4515)",
            "            )",
            "        else:",
            "            self.w_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"workers.ipc\")",
            "            )",
            "        log.info(\"Worker binding to socket %s\", self.w_uri)",
            "        self._socket.connect(self.w_uri)",
            "",
            "        salt.transport.mixins.auth.AESReqServerMixin.post_fork(",
            "            self, payload_handler, io_loop",
            "        )",
            "",
            "        self.stream = zmq.eventloop.zmqstream.ZMQStream(",
            "            self._socket, io_loop=self.io_loop",
            "        )",
            "        self.stream.on_recv_stream(self.handle_message)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_message(self, stream, payload):",
            "        \"\"\"",
            "        Handle incoming messages from underlying TCP streams",
            "",
            "        :stream ZMQStream stream: A ZeroMQ stream.",
            "        See http://zeromq.github.io/pyzmq/api/generated/zmq.eventloop.zmqstream.html",
            "",
            "        :param dict payload: A payload to process",
            "        \"\"\"",
            "        try:",
            "            payload = self.serial.loads(payload[0])",
            "            payload = self._decode_payload(payload)",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            exc_type = type(exc).__name__",
            "            if exc_type == \"AuthenticationError\":",
            "                log.debug(",
            "                    \"Minion failed to auth to master. Since the payload is \"",
            "                    \"encrypted, it is not known which minion failed to \"",
            "                    \"authenticate. It is likely that this is a transient \"",
            "                    \"failure due to the master rotating its public key.\"",
            "                )",
            "            else:",
            "                log.error(\"Bad load from minion: %s: %s\", exc_type, exc)",
            "            stream.send(self.serial.dumps(\"bad load\"))",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        # TODO helper functions to normalize payload?",
            "        if not isinstance(payload, dict) or not isinstance(payload.get(\"load\"), dict):",
            "            log.error(",
            "                \"payload and load must be a dict. Payload was: %s and load was %s\",",
            "                payload,",
            "                payload.get(\"load\"),",
            "            )",
            "            stream.send(self.serial.dumps(\"payload and load must be a dict\"))",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        try:",
            "            id_ = payload[\"load\"].get(\"id\", \"\")",
            "            if \"\\0\" in id_:",
            "                log.error(\"Payload contains an id with a null byte: %s\", payload)",
            "                stream.send(self.serial.dumps(\"bad load: id contains a null byte\"))",
            "                raise salt.ext.tornado.gen.Return()",
            "        except TypeError:",
            "            log.error(\"Payload contains non-string id: %s\", payload)",
            "            stream.send(",
            "                self.serial.dumps(\"bad load: id {} is not a string\".format(id_))",
            "            )",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        version = 0",
            "        if \"version\" in payload:",
            "            version = payload[\"version\"]",
            "",
            "        sign_messages = False",
            "        if version > 1:",
            "            sign_messages = True",
            "",
            "        # intercept the \"_auth\" commands, since the main daemon shouldn't know",
            "        # anything about our key auth",
            "        if payload[\"enc\"] == \"clear\" and payload.get(\"load\", {}).get(\"cmd\") == \"_auth\":",
            "            stream.send(self.serial.dumps(self._auth(payload[\"load\"], sign_messages)))",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        nonce = None",
            "        if version > 1:",
            "            nonce = payload[\"load\"].pop(\"nonce\", None)",
            "",
            "        # TODO: test",
            "        try:",
            "            # Take the payload_handler function that was registered when we created the channel",
            "            # and call it, returning control to the caller until it completes",
            "            ret, req_opts = yield self.payload_handler(payload)",
            "        except Exception as e:  # pylint: disable=broad-except",
            "            # always attempt to return an error to the minion",
            "            stream.send(\"Some exception handling minion payload\")",
            "            log.error(\"Some exception handling a payload from minion\", exc_info=True)",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        req_fun = req_opts.get(\"fun\", \"send\")",
            "        if req_fun == \"send_clear\":",
            "            stream.send(self.serial.dumps(ret))",
            "        elif req_fun == \"send\":",
            "            stream.send(self.serial.dumps(self.crypticle.dumps(ret)))",
            "        elif req_fun == \"send_private\":",
            "            stream.send(",
            "                self.serial.dumps(",
            "                    self._encrypt_private(",
            "                        ret, req_opts[\"key\"], req_opts[\"tgt\"], nonce, sign_messages,",
            "                    )",
            "                )",
            "            )",
            "        else:",
            "            log.error(\"Unknown req_fun %s\", req_fun)",
            "            # always attempt to return an error to the minion",
            "            stream.send(\"Server-side exception handling payload\")",
            "        raise salt.ext.tornado.gen.Return()",
            "",
            "    def __setup_signals(self):",
            "        signal.signal(signal.SIGINT, self._handle_signals)",
            "        signal.signal(signal.SIGTERM, self._handle_signals)",
            "",
            "    def _handle_signals(self, signum, sigframe):",
            "        msg = \"{} received a \".format(self.__class__.__name__)",
            "        if signum == signal.SIGINT:",
            "            msg += \"SIGINT\"",
            "        elif signum == signal.SIGTERM:",
            "            msg += \"SIGTERM\"",
            "        msg += \". Exiting\"",
            "        log.debug(msg)",
            "        self.close()",
            "        sys.exit(salt.defaults.exitcodes.EX_OK)",
            "",
            "",
            "def _set_tcp_keepalive(zmq_socket, opts):",
            "    \"\"\"",
            "    Ensure that TCP keepalives are set as specified in \"opts\".",
            "",
            "    Warning: Failure to set TCP keepalives on the salt-master can result in",
            "    not detecting the loss of a minion when the connection is lost or when",
            "    its host has been terminated without first closing the socket.",
            "    Salt's Presence System depends on this connection status to know if a minion",
            "    is \"present\".",
            "",
            "    Warning: Failure to set TCP keepalives on minions can result in frequent or",
            "    unexpected disconnects!",
            "    \"\"\"",
            "    if hasattr(zmq, \"TCP_KEEPALIVE\") and opts:",
            "        if \"tcp_keepalive\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE, opts[\"tcp_keepalive\"])",
            "        if \"tcp_keepalive_idle\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE_IDLE, opts[\"tcp_keepalive_idle\"])",
            "        if \"tcp_keepalive_cnt\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE_CNT, opts[\"tcp_keepalive_cnt\"])",
            "        if \"tcp_keepalive_intvl\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE_INTVL, opts[\"tcp_keepalive_intvl\"])",
            "",
            "",
            "class ZeroMQPubServerChannel(salt.transport.server.PubServerChannel):",
            "    \"\"\"",
            "    Encapsulate synchronous operations for a publisher channel",
            "    \"\"\"",
            "",
            "    _sock_data = threading.local()",
            "",
            "    def __init__(self, opts):",
            "        self.opts = opts",
            "        self.serial = salt.payload.Serial(self.opts)  # TODO: in init?",
            "        self.ckminions = salt.utils.minions.CkMinions(self.opts)",
            "",
            "    def connect(self):",
            "        return salt.ext.tornado.gen.sleep(5)",
            "",
            "    def _publish_daemon(self, log_queue=None):",
            "        \"\"\"",
            "        Bind to the interface specified in the configuration file",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        if self.opts[\"pub_server_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting Publish daemon niceness to %i\",",
            "                self.opts[\"pub_server_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"pub_server_niceness\"])",
            "",
            "        if log_queue:",
            "            salt.log.setup.set_multiprocessing_logging_queue(log_queue)",
            "            salt.log.setup.setup_multiprocessing_logging(log_queue)",
            "",
            "        # Set up the context",
            "        context = zmq.Context(1)",
            "        # Prepare minion publish socket",
            "        pub_sock = context.socket(zmq.PUB)",
            "        _set_tcp_keepalive(pub_sock, self.opts)",
            "        # if 2.1 >= zmq < 3.0, we only have one HWM setting",
            "        try:",
            "            pub_sock.setsockopt(zmq.HWM, self.opts.get(\"pub_hwm\", 1000))",
            "        # in zmq >= 3.0, there are separate send and receive HWM settings",
            "        except AttributeError:",
            "            # Set the High Water Marks. For more information on HWM, see:",
            "            # http://api.zeromq.org/4-1:zmq-setsockopt",
            "            pub_sock.setsockopt(zmq.SNDHWM, self.opts.get(\"pub_hwm\", 1000))",
            "            pub_sock.setsockopt(zmq.RCVHWM, self.opts.get(\"pub_hwm\", 1000))",
            "        if self.opts[\"ipv6\"] is True and hasattr(zmq, \"IPV4ONLY\"):",
            "            # IPv6 sockets work for both IPv6 and IPv4 addresses",
            "            pub_sock.setsockopt(zmq.IPV4ONLY, 0)",
            "        pub_sock.setsockopt(zmq.BACKLOG, self.opts.get(\"zmq_backlog\", 1000))",
            "        pub_sock.setsockopt(zmq.LINGER, -1)",
            "        pub_uri = \"tcp://{interface}:{publish_port}\".format(**self.opts)",
            "        # Prepare minion pull socket",
            "        pull_sock = context.socket(zmq.PULL)",
            "        pull_sock.setsockopt(zmq.LINGER, -1)",
            "",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_publish_pull\", 4514)",
            "            )",
            "        else:",
            "            pull_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "            )",
            "        salt.utils.zeromq.check_ipc_path_max_len(pull_uri)",
            "",
            "        # Start the minion command publisher",
            "        log.info(\"Starting the Salt Publisher on %s\", pub_uri)",
            "        pub_sock.bind(pub_uri)",
            "",
            "        # Securely create socket",
            "        log.info(\"Starting the Salt Puller on %s\", pull_uri)",
            "        with salt.utils.files.set_umask(0o177):",
            "            pull_sock.bind(pull_uri)",
            "",
            "        try:",
            "            while True:",
            "                # Catch and handle EINTR from when this process is sent",
            "                # SIGUSR1 gracefully so we don't choke and die horribly",
            "                try:",
            "                    log.debug(\"Publish daemon getting data from puller %s\", pull_uri)",
            "                    package = pull_sock.recv()",
            "                    log.debug(\"Publish daemon received payload. size=%d\", len(package))",
            "",
            "                    unpacked_package = salt.payload.unpackage(package)",
            "                    unpacked_package = salt.transport.frame.decode_embedded_strs(",
            "                        unpacked_package",
            "                    )",
            "                    payload = unpacked_package[\"payload\"]",
            "                    log.trace(\"Accepted unpacked package from puller\")",
            "                    if self.opts[\"zmq_filtering\"]:",
            "                        # if you have a specific topic list, use that",
            "                        if \"topic_lst\" in unpacked_package:",
            "                            for topic in unpacked_package[\"topic_lst\"]:",
            "                                log.trace(",
            "                                    \"Sending filtered data over publisher %s\", pub_uri",
            "                                )",
            "                                # zmq filters are substring match, hash the topic",
            "                                # to avoid collisions",
            "                                htopic = salt.utils.stringutils.to_bytes(",
            "                                    hashlib.sha1(",
            "                                        salt.utils.stringutils.to_bytes(topic)",
            "                                    ).hexdigest()",
            "                                )",
            "                                pub_sock.send(htopic, flags=zmq.SNDMORE)",
            "                                pub_sock.send(payload)",
            "                                log.trace(\"Filtered data has been sent\")",
            "",
            "                            # Syndic broadcast",
            "                            if self.opts.get(\"order_masters\"):",
            "                                log.trace(\"Sending filtered data to syndic\")",
            "                                pub_sock.send(b\"syndic\", flags=zmq.SNDMORE)",
            "                                pub_sock.send(payload)",
            "                                log.trace(\"Filtered data has been sent to syndic\")",
            "                        # otherwise its a broadcast",
            "                        else:",
            "                            # TODO: constants file for \"broadcast\"",
            "                            log.trace(",
            "                                \"Sending broadcasted data over publisher %s\", pub_uri",
            "                            )",
            "                            pub_sock.send(b\"broadcast\", flags=zmq.SNDMORE)",
            "                            pub_sock.send(payload)",
            "                            log.trace(\"Broadcasted data has been sent\")",
            "                    else:",
            "                        log.trace(",
            "                            \"Sending ZMQ-unfiltered data over publisher %s\", pub_uri",
            "                        )",
            "                        pub_sock.send(payload)",
            "                        log.trace(\"Unfiltered data has been sent\")",
            "                except zmq.ZMQError as exc:",
            "                    if exc.errno == errno.EINTR:",
            "                        continue",
            "                    raise",
            "",
            "        except KeyboardInterrupt:",
            "            log.trace(\"Publish daemon caught Keyboard interupt, tearing down\")",
            "        # Cleanly close the sockets if we're shutting down",
            "        if pub_sock.closed is False:",
            "            pub_sock.close()",
            "        if pull_sock.closed is False:",
            "            pull_sock.close()",
            "        if context.closed is False:",
            "            context.term()",
            "",
            "    def pre_fork(self, process_manager, kwargs=None):",
            "        \"\"\"",
            "        Do anything necessary pre-fork. Since this is on the master side this will",
            "        primarily be used to create IPC channels and create our daemon process to",
            "        do the actual publishing",
            "",
            "        :param func process_manager: A ProcessManager, from salt.utils.process.ProcessManager",
            "        \"\"\"",
            "        process_manager.add_process(self._publish_daemon, kwargs=kwargs)",
            "",
            "    @property",
            "    def pub_sock(self):",
            "        \"\"\"",
            "        This thread's zmq publisher socket. This socket is stored on the class",
            "        so that multiple instantiations in the same thread will re-use a single",
            "        zmq socket.",
            "        \"\"\"",
            "        try:",
            "            return self._sock_data.sock",
            "        except AttributeError:",
            "            pass",
            "",
            "    def pub_connect(self):",
            "        \"\"\"",
            "        Create and connect this thread's zmq socket. If a publisher socket",
            "        already exists \"pub_close\" is called before creating and connecting a",
            "        new socket.",
            "        \"\"\"",
            "        if self.pub_sock:",
            "            self.pub_close()",
            "        ctx = zmq.Context.instance()",
            "        self._sock_data.sock = ctx.socket(zmq.PUSH)",
            "        self.pub_sock.setsockopt(zmq.LINGER, -1)",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_publish_pull\", 4514)",
            "            )",
            "        else:",
            "            pull_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "            )",
            "        log.debug(\"Connecting to pub server: %s\", pull_uri)",
            "        self.pub_sock.connect(pull_uri)",
            "        return self._sock_data.sock",
            "",
            "    def pub_close(self):",
            "        \"\"\"",
            "        Disconnect an existing publisher socket and remove it from the local",
            "        thread's cache.",
            "        \"\"\"",
            "        if hasattr(self._sock_data, \"sock\"):",
            "            self._sock_data.sock.close()",
            "            delattr(self._sock_data, \"sock\")",
            "",
            "    def publish(self, load):",
            "        \"\"\"",
            "        Publish \"load\" to minions. This send the load to the publisher daemon",
            "        process with does the actual sending to minions.",
            "",
            "        :param dict load: A load to be sent across the wire to minions",
            "        \"\"\"",
            "        payload = {\"enc\": \"aes\"}",
            "        crypticle = salt.crypt.Crypticle(",
            "            self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "        )",
            "        payload[\"load\"] = crypticle.dumps(load)",
            "        if self.opts[\"sign_pub_messages\"]:",
            "            master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "            log.debug(\"Signing data packet\")",
            "            payload[\"sig\"] = salt.crypt.sign_message(master_pem_path, payload[\"load\"])",
            "        int_payload = {\"payload\": self.serial.dumps(payload)}",
            "",
            "        # add some targeting stuff for lists only (for now)",
            "        if load[\"tgt_type\"] == \"list\":",
            "            int_payload[\"topic_lst\"] = load[\"tgt\"]",
            "",
            "        # If zmq_filtering is enabled, target matching has to happen master side",
            "        match_targets = [\"pcre\", \"glob\", \"list\"]",
            "        if self.opts[\"zmq_filtering\"] and load[\"tgt_type\"] in match_targets:",
            "            # Fetch a list of minions that match",
            "            _res = self.ckminions.check_minions(load[\"tgt\"], tgt_type=load[\"tgt_type\"])",
            "            match_ids = _res[\"minions\"]",
            "",
            "            log.debug(\"Publish Side Match: %s\", match_ids)",
            "            # Send list of miions thru so zmq can target them",
            "            int_payload[\"topic_lst\"] = match_ids",
            "        payload = self.serial.dumps(int_payload)",
            "        log.debug(",
            "            \"Sending payload to publish daemon. jid=%s size=%d\",",
            "            load.get(\"jid\", None),",
            "            len(payload),",
            "        )",
            "        if not self.pub_sock:",
            "            self.pub_connect()",
            "        self.pub_sock.send(payload)",
            "        log.debug(\"Sent payload to publish daemon.\")",
            "",
            "",
            "class AsyncReqMessageClientPool(salt.transport.MessageClientPool):",
            "    \"\"\"",
            "    Wrapper class of AsyncReqMessageClientPool to avoid blocking waiting while writing data to socket.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, args=None, kwargs=None):",
            "        super().__init__(AsyncReqMessageClient, opts, args=args, kwargs=kwargs)",
            "        self._closing = False",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "",
            "        self._closing = True",
            "        for message_client in self.message_clients:",
            "            message_client.close()",
            "        self.message_clients = []",
            "",
            "    def send(self, *args, **kwargs):",
            "        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))",
            "        return message_clients[0].send(*args, **kwargs)",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "# TODO: unit tests!",
            "class AsyncReqMessageClient:",
            "    \"\"\"",
            "    This class wraps the underlying zeromq REQ socket and gives a future-based",
            "    interface to sending and recieving messages. This works around the primary",
            "    limitation of serialized send/recv on the underlying socket by queueing the",
            "    message sends in this class. In the future if we decide to attempt to multiplex",
            "    we can manage a pool of REQ/REP sockets-- but for now we'll just do them in serial",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, addr, linger=0, io_loop=None):",
            "        \"\"\"",
            "        Create an asynchronous message client",
            "",
            "        :param dict opts: The salt opts dictionary",
            "        :param str addr: The interface IP address to bind to",
            "        :param int linger: The number of seconds to linger on a ZMQ socket. See",
            "                           http://api.zeromq.org/2-1:zmq-setsockopt [ZMQ_LINGER]",
            "        :param IOLoop io_loop: A Tornado IOLoop event scheduler [tornado.ioloop.IOLoop]",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.addr = addr",
            "        self.linger = linger",
            "        if io_loop is None:",
            "            self.io_loop = salt.ext.tornado.ioloop.IOLoop.current()",
            "        else:",
            "            self.io_loop = io_loop",
            "",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.context = zmq.Context()",
            "",
            "        # wire up sockets",
            "        self._init_socket()",
            "",
            "        self.send_queue = []",
            "        # mapping of message -> future",
            "        self.send_future_map = {}",
            "",
            "        self.send_timeout_map = {}  # message -> timeout",
            "        self._closing = False",
            "",
            "    # TODO: timeout all in-flight sessions, or error",
            "    def close(self):",
            "        try:",
            "            if self._closing:",
            "                return",
            "        except AttributeError:",
            "            # We must have been called from __del__",
            "            # The python interpreter has nuked most attributes already",
            "            return",
            "        else:",
            "            self._closing = True",
            "            if hasattr(self, \"stream\") and self.stream is not None:",
            "                if ZMQ_VERSION_INFO < (14, 3, 0):",
            "                    # stream.close() doesn't work properly on pyzmq < 14.3.0",
            "                    if self.stream.socket:",
            "                        self.stream.socket.close()",
            "                    self.stream.io_loop.remove_handler(self.stream.socket)",
            "                    # set this to None, more hacks for messed up pyzmq",
            "                    self.stream.socket = None",
            "                    self.socket.close()",
            "                else:",
            "                    self.stream.close()",
            "                    self.socket = None",
            "                self.stream = None",
            "            if self.context.closed is False:",
            "                self.context.term()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _init_socket(self):",
            "        if hasattr(self, \"stream\"):",
            "            self.stream.close()  # pylint: disable=E0203",
            "            self.socket.close()  # pylint: disable=E0203",
            "            del self.stream",
            "            del self.socket",
            "",
            "        self.socket = self.context.socket(zmq.REQ)",
            "",
            "        # socket options",
            "        if hasattr(zmq, \"RECONNECT_IVL_MAX\"):",
            "            self.socket.setsockopt(zmq.RECONNECT_IVL_MAX, 5000)",
            "",
            "        _set_tcp_keepalive(self.socket, self.opts)",
            "        if self.addr.startswith(\"tcp://[\"):",
            "            # Hint PF type if bracket enclosed IPv6 address",
            "            if hasattr(zmq, \"IPV6\"):",
            "                self.socket.setsockopt(zmq.IPV6, 1)",
            "            elif hasattr(zmq, \"IPV4ONLY\"):",
            "                self.socket.setsockopt(zmq.IPV4ONLY, 0)",
            "        self.socket.linger = self.linger",
            "        log.debug(\"Trying to connect to: %s\", self.addr)",
            "        self.socket.connect(self.addr)",
            "        self.stream = zmq.eventloop.zmqstream.ZMQStream(",
            "            self.socket, io_loop=self.io_loop",
            "        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _internal_send_recv(self):",
            "        while len(self.send_queue) > 0:",
            "            message = self.send_queue[0]",
            "            future = self.send_future_map.get(message, None)",
            "            if future is None:",
            "                # Timedout",
            "                del self.send_queue[0]",
            "                continue",
            "",
            "            # send",
            "            def mark_future(msg):",
            "                if not future.done():",
            "                    data = self.serial.loads(msg[0])",
            "                    future.set_result(data)",
            "",
            "            self.stream.on_recv(mark_future)",
            "            self.stream.send(message)",
            "",
            "            try:",
            "                ret = yield future",
            "            except Exception as err:  # pylint: disable=broad-except",
            "                log.debug(\"Re-init ZMQ socket: %s\", err)",
            "                self._init_socket()  # re-init the zmq socket (no other way in zmq)",
            "                del self.send_queue[0]",
            "                continue",
            "            del self.send_queue[0]",
            "            self.send_future_map.pop(message, None)",
            "            self.remove_message_timeout(message)",
            "",
            "    def remove_message_timeout(self, message):",
            "        if message not in self.send_timeout_map:",
            "            return",
            "        timeout = self.send_timeout_map.pop(message, None)",
            "        if timeout is not None:",
            "            # Hasn't been already timedout",
            "            self.io_loop.remove_timeout(timeout)",
            "",
            "    def timeout_message(self, message):",
            "        \"\"\"",
            "        Handle a message timeout by removing it from the sending queue",
            "        and informing the caller",
            "",
            "        :raises: SaltReqTimeoutError",
            "        \"\"\"",
            "        future = self.send_future_map.pop(message, None)",
            "        # In a race condition the message might have been sent by the time",
            "        # we're timing it out. Make sure the future is not None",
            "        if future is not None:",
            "            del self.send_timeout_map[message]",
            "            if future.attempts < future.tries:",
            "                future.attempts += 1",
            "                log.debug(",
            "                    \"SaltReqTimeoutError, retrying. (%s/%s)\",",
            "                    future.attempts,",
            "                    future.tries,",
            "                )",
            "                self.send(",
            "                    message, timeout=future.timeout, tries=future.tries, future=future,",
            "                )",
            "",
            "            else:",
            "                future.set_exception(SaltReqTimeoutError(\"Message timed out\"))",
            "",
            "    def send(",
            "        self, message, timeout=None, tries=3, future=None, callback=None, raw=False",
            "    ):",
            "        \"\"\"",
            "        Return a future which will be completed when the message has a response",
            "        \"\"\"",
            "        if future is None:",
            "            future = salt.ext.tornado.concurrent.Future()",
            "            future.tries = tries",
            "            future.attempts = 0",
            "            future.timeout = timeout",
            "            # if a future wasn't passed in, we need to serialize the message",
            "            message = self.serial.dumps(message)",
            "        if callback is not None:",
            "",
            "            def handle_future(future):",
            "                response = future.result()",
            "                self.io_loop.add_callback(callback, response)",
            "",
            "            future.add_done_callback(handle_future)",
            "        # Add this future to the mapping",
            "        self.send_future_map[message] = future",
            "",
            "        if self.opts.get(\"detect_mode\") is True:",
            "            timeout = 1",
            "",
            "        if timeout is not None:",
            "            send_timeout = self.io_loop.call_later(",
            "                timeout, self.timeout_message, message",
            "            )",
            "            self.send_timeout_map[message] = send_timeout",
            "",
            "        if len(self.send_queue) == 0:",
            "            self.io_loop.spawn_callback(self._internal_send_recv)",
            "",
            "        self.send_queue.append(message)",
            "",
            "        return future",
            "",
            "",
            "class ZeroMQSocketMonitor:",
            "    __EVENT_MAP = None",
            "",
            "    def __init__(self, socket):",
            "        \"\"\"",
            "        Create ZMQ monitor sockets",
            "",
            "        More information:",
            "            http://api.zeromq.org/4-0:zmq-socket-monitor",
            "        \"\"\"",
            "        self._socket = socket",
            "        self._monitor_socket = self._socket.get_monitor_socket()",
            "        self._monitor_stream = None",
            "",
            "    def start_io_loop(self, io_loop):",
            "        log.trace(\"Event monitor start!\")",
            "        self._monitor_stream = zmq.eventloop.zmqstream.ZMQStream(",
            "            self._monitor_socket, io_loop=io_loop",
            "        )",
            "        self._monitor_stream.on_recv(self.monitor_callback)",
            "",
            "    def start_poll(self):",
            "        log.trace(\"Event monitor start!\")",
            "        try:",
            "            while self._monitor_socket is not None and self._monitor_socket.poll():",
            "                msg = self._monitor_socket.recv_multipart()",
            "                self.monitor_callback(msg)",
            "        except (AttributeError, zmq.error.ContextTerminated):",
            "            # We cannot log here because we'll get an interrupted system call in trying",
            "            # to flush the logging buffer as we terminate",
            "            pass",
            "",
            "    @property",
            "    def event_map(self):",
            "        if ZeroMQSocketMonitor.__EVENT_MAP is None:",
            "            event_map = {}",
            "            for name in dir(zmq):",
            "                if name.startswith(\"EVENT_\"):",
            "                    value = getattr(zmq, name)",
            "                    event_map[value] = name",
            "            ZeroMQSocketMonitor.__EVENT_MAP = event_map",
            "        return ZeroMQSocketMonitor.__EVENT_MAP",
            "",
            "    def monitor_callback(self, msg):",
            "        evt = zmq.utils.monitor.parse_monitor_message(msg)",
            "        evt[\"description\"] = self.event_map[evt[\"event\"]]",
            "        log.debug(\"ZeroMQ event: %s\", evt)",
            "        if evt[\"event\"] == zmq.EVENT_MONITOR_STOPPED:",
            "            self.stop()",
            "",
            "    def stop(self):",
            "        if self._socket is None:",
            "            return",
            "        self._socket.disable_monitor()",
            "        self._socket = None",
            "        self._monitor_socket = None",
            "        if self._monitor_stream is not None:",
            "            self._monitor_stream.close()",
            "            self._monitor_stream = None",
            "        log.trace(\"Event monitor done!\")"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Zeromq transport classes",
            "\"\"\"",
            "",
            "",
            "import copy",
            "import errno",
            "import hashlib",
            "import logging",
            "import os",
            "import signal",
            "import sys",
            "import threading",
            "import uuid",
            "import weakref",
            "from random import randint",
            "",
            "import salt.auth",
            "import salt.crypt",
            "import salt.ext.tornado",
            "import salt.ext.tornado.concurrent",
            "import salt.ext.tornado.gen",
            "import salt.ext.tornado.ioloop",
            "import salt.log.setup",
            "import salt.payload",
            "import salt.transport.client",
            "import salt.transport.mixins.auth",
            "import salt.transport.server",
            "import salt.utils.event",
            "import salt.utils.files",
            "import salt.utils.minions",
            "import salt.utils.process",
            "import salt.utils.stringutils",
            "import salt.utils.verify",
            "import salt.utils.versions",
            "import salt.utils.zeromq",
            "import zmq.error",
            "import zmq.eventloop.ioloop",
            "import zmq.eventloop.zmqstream",
            "from salt._compat import ipaddress",
            "from salt.exceptions import SaltException, SaltReqTimeoutError",
            "from salt.ext import six",
            "from salt.utils.zeromq import (",
            "    LIBZMQ_VERSION_INFO,",
            "    ZMQ_VERSION_INFO,",
            "    ZMQDefaultLoop,",
            "    install_zmq,",
            "    zmq,",
            ")",
            "",
            "try:",
            "    import zmq.utils.monitor",
            "",
            "    HAS_ZMQ_MONITOR = True",
            "except ImportError:",
            "    HAS_ZMQ_MONITOR = False",
            "",
            "",
            "try:",
            "    from M2Crypto import RSA",
            "",
            "    HAS_M2 = True",
            "except ImportError:",
            "    HAS_M2 = False",
            "    try:",
            "        from Cryptodome.Cipher import PKCS1_OAEP",
            "    except ImportError:",
            "        from Crypto.Cipher import PKCS1_OAEP  # nosec",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def _get_master_uri(master_ip, master_port, source_ip=None, source_port=None):",
            "    \"\"\"",
            "    Return the ZeroMQ URI to connect the Minion to the Master.",
            "    It supports different source IP / port, given the ZeroMQ syntax:",
            "    // Connecting using a IP address and bind to an IP address",
            "    rc = zmq_connect(socket, \"tcp://192.168.1.17:5555;192.168.1.1:5555\"); assert (rc == 0);",
            "    Source: http://api.zeromq.org/4-1:zmq-tcp",
            "    \"\"\"",
            "",
            "    from salt.utils.zeromq import ip_bracket",
            "",
            "    master_uri = \"tcp://{master_ip}:{master_port}\".format(",
            "        master_ip=ip_bracket(master_ip), master_port=master_port",
            "    )",
            "    if source_ip or source_port:",
            "        if LIBZMQ_VERSION_INFO >= (4, 1, 6) and ZMQ_VERSION_INFO >= (16, 0, 1):",
            "            # The source:port syntax for ZeroMQ has been added in libzmq 4.1.6",
            "            # which is included in the pyzmq wheels starting with 16.0.1.",
            "            if source_ip and source_port:",
            "                master_uri = \"tcp://{source_ip}:{source_port};{master_ip}:{master_port}\".format(",
            "                    source_ip=ip_bracket(source_ip),",
            "                    source_port=source_port,",
            "                    master_ip=ip_bracket(master_ip),",
            "                    master_port=master_port,",
            "                )",
            "            elif source_ip and not source_port:",
            "                master_uri = \"tcp://{source_ip}:0;{master_ip}:{master_port}\".format(",
            "                    source_ip=ip_bracket(source_ip),",
            "                    master_ip=ip_bracket(master_ip),",
            "                    master_port=master_port,",
            "                )",
            "            elif source_port and not source_ip:",
            "                ip_any = (",
            "                    \"0.0.0.0\"",
            "                    if ipaddress.ip_address(master_ip).version == 4",
            "                    else ip_bracket(\"::\")",
            "                )",
            "                master_uri = \"tcp://{ip_any}:{source_port};{master_ip}:{master_port}\".format(",
            "                    ip_any=ip_any,",
            "                    source_port=source_port,",
            "                    master_ip=ip_bracket(master_ip),",
            "                    master_port=master_port,",
            "                )",
            "        else:",
            "            log.warning(",
            "                \"Unable to connect to the Master using a specific source IP / port\"",
            "            )",
            "            log.warning(\"Consider upgrading to pyzmq >= 16.0.1 and libzmq >= 4.1.6\")",
            "            log.warning(",
            "                \"Specific source IP / port for connecting to master returner port: configuraion ignored\"",
            "            )",
            "",
            "    return master_uri",
            "",
            "",
            "class AsyncZeroMQReqChannel(salt.transport.client.ReqChannel):",
            "    \"\"\"",
            "    Encapsulate sending routines to ZeroMQ.",
            "",
            "    ZMQ Channels default to 'crypt=aes'",
            "    \"\"\"",
            "",
            "    # This class is only a singleton per minion/master pair",
            "    # mapping of io_loop -> {key -> channel}",
            "    instance_map = weakref.WeakKeyDictionary()",
            "    async_methods = [",
            "        \"crypted_transfer_decode_dictentry\",",
            "        \"_crypted_transfer\",",
            "        \"_do_transfer\",",
            "        \"_uncrypted_transfer\",",
            "        \"send\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __new__(cls, opts, **kwargs):",
            "        \"\"\"",
            "        Only create one instance of channel per __key()",
            "        \"\"\"",
            "",
            "        # do we have any mapping for this io_loop",
            "        io_loop = kwargs.get(\"io_loop\")",
            "        if io_loop is None:",
            "            install_zmq()",
            "            io_loop = ZMQDefaultLoop.current()",
            "        if io_loop not in cls.instance_map:",
            "            cls.instance_map[io_loop] = weakref.WeakValueDictionary()",
            "        loop_instance_map = cls.instance_map[io_loop]",
            "",
            "        key = cls.__key(opts, **kwargs)",
            "        obj = loop_instance_map.get(key)",
            "        if obj is None:",
            "            log.debug(\"Initializing new AsyncZeroMQReqChannel for %s\", key)",
            "            # we need to make a local variable for this, as we are going to store",
            "            # it in a WeakValueDictionary-- which will remove the item if no one",
            "            # references it-- this forces a reference while we return to the caller",
            "            obj = object.__new__(cls)",
            "            obj.__singleton_init__(opts, **kwargs)",
            "            obj._instance_key = key",
            "            loop_instance_map[key] = obj",
            "            obj._refcount = 1",
            "            obj._refcount_lock = threading.RLock()",
            "            log.trace(",
            "                \"Inserted key into loop_instance_map id %s for key %s and process %s\",",
            "                id(loop_instance_map),",
            "                key,",
            "                os.getpid(),",
            "            )",
            "        else:",
            "            with obj._refcount_lock:",
            "                obj._refcount += 1",
            "            log.debug(\"Re-using AsyncZeroMQReqChannel for %s\", key)",
            "        return obj",
            "",
            "    def __deepcopy__(self, memo):",
            "        cls = self.__class__",
            "        # pylint: disable=too-many-function-args",
            "        result = cls.__new__(cls, copy.deepcopy(self.opts, memo))",
            "        # pylint: enable=too-many-function-args",
            "        memo[id(self)] = result",
            "        for key in self.__dict__:",
            "            if key in (\"_io_loop\", \"_refcount\", \"_refcount_lock\"):",
            "                continue",
            "                # The _io_loop has a thread Lock which will fail to be deep",
            "                # copied. Skip it because it will just be recreated on the",
            "                # new copy.",
            "            if key == \"message_client\":",
            "                # Recreate the message client because it will fail to be deep",
            "                # copied. The reason is the same as the io_loop skip above.",
            "                setattr(",
            "                    result,",
            "                    key,",
            "                    AsyncReqMessageClientPool(",
            "                        result.opts,",
            "                        args=(result.opts, self.master_uri,),",
            "                        kwargs={\"io_loop\": self._io_loop},",
            "                    ),",
            "                )",
            "",
            "                continue",
            "            setattr(result, key, copy.deepcopy(self.__dict__[key], memo))",
            "        return result",
            "",
            "    @classmethod",
            "    def force_close_all_instances(cls):",
            "        \"\"\"",
            "        Will force close all instances",
            "",
            "        ZMQ can hang on quit if left to deconstruct on its own.",
            "        This because is deconstructs out of order.",
            "",
            "        :return: None",
            "        \"\"\"",
            "        for weak_dict in list(cls.instance_map.values()):",
            "            for instance in list(weak_dict.values()):",
            "                instance.close()",
            "",
            "    @classmethod",
            "    def __key(cls, opts, **kwargs):",
            "        return (",
            "            opts[\"pki_dir\"],  # where the keys are stored",
            "            opts[\"id\"],  # minion ID",
            "            kwargs.get(\"master_uri\", opts.get(\"master_uri\")),  # master ID",
            "            kwargs.get(\"crypt\", \"aes\"),  # TODO: use the same channel for crypt",
            "        )",
            "",
            "    # has to remain empty for singletons, since __init__ will *always* be called",
            "    def __init__(self, opts, **kwargs):",
            "        pass",
            "",
            "    # an init for the singleton instance to call",
            "    def __singleton_init__(self, opts, **kwargs):",
            "        self.opts = dict(opts)",
            "        self.ttype = \"zeromq\"",
            "",
            "        # crypt defaults to 'aes'",
            "        self.crypt = kwargs.get(\"crypt\", \"aes\")",
            "",
            "        if \"master_uri\" in kwargs:",
            "            self.opts[\"master_uri\"] = kwargs[\"master_uri\"]",
            "",
            "        self._io_loop = kwargs.get(\"io_loop\")",
            "        if self._io_loop is None:",
            "            install_zmq()",
            "            self._io_loop = ZMQDefaultLoop.current()",
            "",
            "        if self.crypt != \"clear\":",
            "            # we don't need to worry about auth as a kwarg, since its a singleton",
            "            self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self._io_loop)",
            "        log.debug(",
            "            \"Connecting the Minion to the Master URI (for the return server): %s\",",
            "            self.master_uri,",
            "        )",
            "        self.message_client = AsyncReqMessageClientPool(",
            "            self.opts,",
            "            args=(self.opts, self.master_uri,),",
            "            kwargs={\"io_loop\": self._io_loop},",
            "        )",
            "        self._closing = False",
            "",
            "    def close(self):",
            "        \"\"\"",
            "        Since the message_client creates sockets and assigns them to the IOLoop we have to",
            "        specifically destroy them, since we aren't the only ones with references to the FDs",
            "        \"\"\"",
            "        if self._closing:",
            "            return",
            "",
            "        if self._refcount > 1:",
            "            # Decrease refcount",
            "            with self._refcount_lock:",
            "                self._refcount -= 1",
            "            log.debug(",
            "                \"This is not the last %s instance. Not closing yet.\",",
            "                self.__class__.__name__,",
            "            )",
            "            return",
            "",
            "        log.debug(\"Closing %s instance\", self.__class__.__name__)",
            "        self._closing = True",
            "        if hasattr(self, \"message_client\"):",
            "            self.message_client.close()",
            "",
            "        # Remove the entry from the instance map so that a closed entry may not",
            "        # be reused.",
            "        # This forces this operation even if the reference count of the entry",
            "        # has not yet gone to zero.",
            "        if self._io_loop in self.__class__.instance_map:",
            "            loop_instance_map = self.__class__.instance_map[self._io_loop]",
            "            if self._instance_key in loop_instance_map:",
            "                del loop_instance_map[self._instance_key]",
            "            if not loop_instance_map:",
            "                del self.__class__.instance_map[self._io_loop]",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        with self._refcount_lock:",
            "            # Make sure we actually close no matter if something",
            "            # went wrong with our ref counting",
            "            self._refcount = 1",
            "        try:",
            "            self.close()",
            "        except OSError as exc:",
            "            if exc.errno != errno.EBADF:",
            "                # If its not a bad file descriptor error, raise",
            "                raise",
            "",
            "    # pylint: enable=W1701",
            "",
            "    @property",
            "    def master_uri(self):",
            "        if \"master_uri\" in self.opts:",
            "            return self.opts[\"master_uri\"]",
            "",
            "        # if by chance master_uri is not there..",
            "        if \"master_ip\" in self.opts:",
            "            return _get_master_uri(",
            "                self.opts[\"master_ip\"],",
            "                self.opts[\"master_port\"],",
            "                source_ip=self.opts.get(\"source_ip\"),",
            "                source_port=self.opts.get(\"source_ret_port\"),",
            "            )",
            "",
            "        # if we've reached here something is very abnormal",
            "        raise SaltException(\"ReqChannel: missing master_uri/master_ip in self.opts\")",
            "",
            "    def _package_load(self, load):",
            "        return {",
            "            \"enc\": self.crypt,",
            "            \"load\": load,",
            "            \"version\": 2,",
            "        }",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def crypted_transfer_decode_dictentry(",
            "        self, load, dictkey=None, tries=3, timeout=60",
            "    ):",
            "        nonce = uuid.uuid4().hex",
            "        load[\"nonce\"] = nonce",
            "        if not self.auth.authenticated:",
            "            # Return control back to the caller, continue when authentication succeeds",
            "            yield self.auth.authenticate()",
            "",
            "        # Return control to the caller. When send() completes, resume by",
            "        # populating ret with the Future.result",
            "        ret = yield self.message_client.send(",
            "            self._package_load(self.auth.crypticle.dumps(load)),",
            "            timeout=timeout,",
            "            tries=tries,",
            "        )",
            "",
            "        if \"key\" not in ret:",
            "            # Reauth in the case our key is deleted on the master side.",
            "            yield self.auth.authenticate()",
            "            ret = yield self.message_client.send(",
            "                self._package_load(self.auth.crypticle.dumps(load)),",
            "                timeout=timeout,",
            "                tries=tries,",
            "            )",
            "",
            "        key = self.auth.get_keys()",
            "        if HAS_M2:",
            "            aes = key.private_decrypt(ret[\"key\"], RSA.pkcs1_oaep_padding)",
            "        else:",
            "            cipher = PKCS1_OAEP.new(key)",
            "            aes = cipher.decrypt(ret[\"key\"])",
            "",
            "        # Decrypt using the public key.",
            "        pcrypt = salt.crypt.Crypticle(self.opts, aes)",
            "        signed_msg = pcrypt.loads(ret[dictkey])",
            "",
            "        # Validate the master's signature.",
            "        master_pubkey_path = os.path.join(self.opts[\"pki_dir\"], \"minion_master.pub\")",
            "        if not salt.crypt.verify_signature(",
            "            master_pubkey_path, signed_msg[\"data\"], signed_msg[\"sig\"]",
            "        ):",
            "            raise salt.crypt.AuthenticationError(",
            "                \"Pillar payload signature failed to validate.\"",
            "            )",
            "",
            "        # Make sure the signed key matches the key we used to decrypt the data.",
            "        data = salt.payload.Serial({}).loads(signed_msg[\"data\"])",
            "        if data[\"key\"] != ret[\"key\"]:",
            "            raise salt.crypt.AuthenticationError(\"Key verification failed.\")",
            "",
            "        # Validate the nonce.",
            "        if data[\"nonce\"] != nonce:",
            "            raise salt.crypt.AuthenticationError(\"Pillar nonce verification failed.\")",
            "        raise salt.ext.tornado.gen.Return(data[\"pillar\"])",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _crypted_transfer(self, load, tries=3, timeout=60, raw=False):",
            "        \"\"\"",
            "        Send a load across the wire, with encryption",
            "",
            "        In case of authentication errors, try to renegotiate authentication",
            "        and retry the method.",
            "",
            "        Indeed, we can fail too early in case of a master restart during a",
            "        minion state execution call",
            "",
            "        :param dict load: A load to send across the wire",
            "        :param int tries: The number of times to make before failure",
            "        :param int timeout: The number of seconds on a response before failing",
            "        \"\"\"",
            "        nonce = uuid.uuid4().hex",
            "        if load and isinstance(load, dict):",
            "            load[\"nonce\"] = nonce",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def _do_transfer():",
            "            # Yield control to the caller. When send() completes, resume by populating data with the Future.result",
            "            data = yield self.message_client.send(",
            "                self._package_load(self.auth.crypticle.dumps(load)),",
            "                timeout=timeout,",
            "                tries=tries,",
            "            )",
            "            # we may not have always data",
            "            # as for example for saltcall ret submission, this is a blind",
            "            # communication, we do not subscribe to return events, we just",
            "            # upload the results to the master",
            "            if data:",
            "                data = self.auth.crypticle.loads(data, raw, nonce)",
            "            if six.PY3 and not raw:",
            "                data = salt.transport.frame.decode_embedded_strs(data)",
            "            raise salt.ext.tornado.gen.Return(data)",
            "",
            "        if not self.auth.authenticated:",
            "            # Return control back to the caller, resume when authentication succeeds",
            "            yield self.auth.authenticate()",
            "        try:",
            "            # We did not get data back the first time. Retry.",
            "            ret = yield _do_transfer()",
            "        except salt.crypt.AuthenticationError:",
            "            # If auth error, return control back to the caller, continue when authentication succeeds",
            "            yield self.auth.authenticate()",
            "            ret = yield _do_transfer()",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _uncrypted_transfer(self, load, tries=3, timeout=60):",
            "        \"\"\"",
            "        Send a load across the wire in cleartext",
            "",
            "        :param dict load: A load to send across the wire",
            "        :param int tries: The number of times to make before failure",
            "        :param int timeout: The number of seconds on a response before failing",
            "        \"\"\"",
            "        ret = yield self.message_client.send(",
            "            self._package_load(load), timeout=timeout, tries=tries,",
            "        )",
            "",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def send(self, load, tries=3, timeout=60, raw=False):",
            "        \"\"\"",
            "        Send a request, return a future which will complete when we send the message",
            "        \"\"\"",
            "        if self.crypt == \"clear\":",
            "            ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)",
            "        else:",
            "            ret = yield self._crypted_transfer(",
            "                load, tries=tries, timeout=timeout, raw=raw",
            "            )",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "",
            "class AsyncZeroMQPubChannel(",
            "    salt.transport.mixins.auth.AESPubClientMixin, salt.transport.client.AsyncPubChannel",
            "):",
            "    \"\"\"",
            "    A transport channel backed by ZeroMQ for a Salt Publisher to use to",
            "    publish commands to connected minions",
            "    \"\"\"",
            "",
            "    async_methods = [",
            "        \"connect\",",
            "        \"_decode_messages\",",
            "    ]",
            "    close_methods = [",
            "        \"close\",",
            "    ]",
            "",
            "    def __init__(self, opts, **kwargs):",
            "        self.opts = opts",
            "        self.ttype = \"zeromq\"",
            "        self.io_loop = kwargs.get(\"io_loop\")",
            "",
            "        if self.io_loop is None:",
            "            install_zmq()",
            "            self.io_loop = ZMQDefaultLoop.current()",
            "",
            "        self.hexid = hashlib.sha1(",
            "            salt.utils.stringutils.to_bytes(self.opts[\"id\"])",
            "        ).hexdigest()",
            "        self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self.io_loop)",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.context = zmq.Context()",
            "        self._socket = self.context.socket(zmq.SUB)",
            "",
            "        if self.opts[\"zmq_filtering\"]:",
            "            # TODO: constants file for \"broadcast\"",
            "            self._socket.setsockopt(zmq.SUBSCRIBE, b\"broadcast\")",
            "            if self.opts.get(\"__role\") == \"syndic\":",
            "                self._socket.setsockopt(zmq.SUBSCRIBE, b\"syndic\")",
            "            else:",
            "                self._socket.setsockopt(",
            "                    zmq.SUBSCRIBE, salt.utils.stringutils.to_bytes(self.hexid)",
            "                )",
            "        else:",
            "            self._socket.setsockopt(zmq.SUBSCRIBE, b\"\")",
            "",
            "        self._socket.setsockopt(",
            "            zmq.IDENTITY, salt.utils.stringutils.to_bytes(self.opts[\"id\"])",
            "        )",
            "",
            "        # TODO: cleanup all the socket opts stuff",
            "        if hasattr(zmq, \"TCP_KEEPALIVE\"):",
            "            self._socket.setsockopt(zmq.TCP_KEEPALIVE, self.opts[\"tcp_keepalive\"])",
            "            self._socket.setsockopt(",
            "                zmq.TCP_KEEPALIVE_IDLE, self.opts[\"tcp_keepalive_idle\"]",
            "            )",
            "            self._socket.setsockopt(",
            "                zmq.TCP_KEEPALIVE_CNT, self.opts[\"tcp_keepalive_cnt\"]",
            "            )",
            "            self._socket.setsockopt(",
            "                zmq.TCP_KEEPALIVE_INTVL, self.opts[\"tcp_keepalive_intvl\"]",
            "            )",
            "",
            "        recon_delay = self.opts[\"recon_default\"]",
            "",
            "        if self.opts[\"recon_randomize\"]:",
            "            recon_delay = randint(",
            "                self.opts[\"recon_default\"],",
            "                self.opts[\"recon_default\"] + self.opts[\"recon_max\"],",
            "            )",
            "",
            "            log.debug(",
            "                \"Generated random reconnect delay between '%sms' and '%sms' (%s)\",",
            "                self.opts[\"recon_default\"],",
            "                self.opts[\"recon_default\"] + self.opts[\"recon_max\"],",
            "                recon_delay,",
            "            )",
            "",
            "        log.debug(\"Setting zmq_reconnect_ivl to '%sms'\", recon_delay)",
            "        self._socket.setsockopt(zmq.RECONNECT_IVL, recon_delay)",
            "",
            "        if hasattr(zmq, \"RECONNECT_IVL_MAX\"):",
            "            log.debug(",
            "                \"Setting zmq_reconnect_ivl_max to '%sms'\",",
            "                self.opts[\"recon_default\"] + self.opts[\"recon_max\"],",
            "            )",
            "",
            "            self._socket.setsockopt(zmq.RECONNECT_IVL_MAX, self.opts[\"recon_max\"])",
            "",
            "        if (self.opts[\"ipv6\"] is True or \":\" in self.opts[\"master_ip\"]) and hasattr(",
            "            zmq, \"IPV4ONLY\"",
            "        ):",
            "            # IPv6 sockets work for both IPv6 and IPv4 addresses",
            "            self._socket.setsockopt(zmq.IPV4ONLY, 0)",
            "",
            "        if HAS_ZMQ_MONITOR and self.opts[\"zmq_monitor\"]:",
            "            self._monitor = ZeroMQSocketMonitor(self._socket)",
            "            self._monitor.start_io_loop(self.io_loop)",
            "",
            "    def close(self):",
            "        if hasattr(self, \"_monitor\") and self._monitor is not None:",
            "            self._monitor.stop()",
            "            self._monitor = None",
            "        if hasattr(self, \"_stream\"):",
            "            if ZMQ_VERSION_INFO < (14, 3, 0):",
            "                # stream.close() doesn't work properly on pyzmq < 14.3.0",
            "                self._stream.io_loop.remove_handler(self._stream.socket)",
            "                self._stream.socket.close(0)",
            "            else:",
            "                self._stream.close(0)",
            "        elif hasattr(self, \"_socket\"):",
            "            self._socket.close(0)",
            "        if hasattr(self, \"context\") and self.context.closed is False:",
            "            self.context.term()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    # TODO: this is the time to see if we are connected, maybe use the req channel to guess?",
            "    @salt.ext.tornado.gen.coroutine",
            "    def connect(self):",
            "        if not self.auth.authenticated:",
            "            yield self.auth.authenticate()",
            "",
            "        # if this is changed from the default, we assume it was intentional",
            "        if int(self.opts.get(\"publish_port\", 4506)) != 4506:",
            "            self.publish_port = self.opts.get(\"publish_port\")",
            "        # else take the relayed publish_port master reports",
            "        else:",
            "            self.publish_port = self.auth.creds[\"publish_port\"]",
            "",
            "        log.debug(",
            "            \"Connecting the Minion to the Master publish port, using the URI: %s\",",
            "            self.master_pub,",
            "        )",
            "        self._socket.connect(self.master_pub)",
            "",
            "    @property",
            "    def master_pub(self):",
            "        \"\"\"",
            "        Return the master publish port",
            "        \"\"\"",
            "        return _get_master_uri(",
            "            self.opts[\"master_ip\"],",
            "            self.publish_port,",
            "            source_ip=self.opts.get(\"source_ip\"),",
            "            source_port=self.opts.get(\"source_publish_port\"),",
            "        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _decode_messages(self, messages):",
            "        \"\"\"",
            "        Take the zmq messages, decrypt/decode them into a payload",
            "",
            "        :param list messages: A list of messages to be decoded",
            "        \"\"\"",
            "        messages_len = len(messages)",
            "        # if it was one message, then its old style",
            "        if messages_len == 1:",
            "            payload = self.serial.loads(messages[0])",
            "        # 2 includes a header which says who should do it",
            "        elif messages_len == 2:",
            "            message_target = salt.utils.stringutils.to_str(messages[0])",
            "            if (",
            "                self.opts.get(\"__role\") != \"syndic\"",
            "                and message_target not in (\"broadcast\", self.hexid)",
            "            ) or (",
            "                self.opts.get(\"__role\") == \"syndic\"",
            "                and message_target not in (\"broadcast\", \"syndic\")",
            "            ):",
            "                log.debug(\"Publish received for not this minion: %s\", message_target)",
            "                raise salt.ext.tornado.gen.Return(None)",
            "            payload = self.serial.loads(messages[1])",
            "        else:",
            "            raise Exception(",
            "                (",
            "                    \"Invalid number of messages ({}) in zeromq pub\"",
            "                    \"message from master\"",
            "                ).format(len(messages_len))",
            "            )",
            "        # Yield control back to the caller. When the payload has been decoded, assign",
            "        # the decoded payload to 'ret' and resume operation",
            "        ret = yield self._decode_payload(payload)",
            "        raise salt.ext.tornado.gen.Return(ret)",
            "",
            "    @property",
            "    def stream(self):",
            "        \"\"\"",
            "        Return the current zmqstream, creating one if necessary",
            "        \"\"\"",
            "        if not hasattr(self, \"_stream\"):",
            "            self._stream = zmq.eventloop.zmqstream.ZMQStream(",
            "                self._socket, io_loop=self.io_loop",
            "            )",
            "        return self._stream",
            "",
            "    def on_recv(self, callback):",
            "        \"\"\"",
            "        Register a callback for received messages (that we didn't initiate)",
            "",
            "        :param func callback: A function which should be called when data is received",
            "        \"\"\"",
            "        if callback is None:",
            "            return self.stream.on_recv(None)",
            "",
            "        @salt.ext.tornado.gen.coroutine",
            "        def wrap_callback(messages):",
            "            payload = yield self._decode_messages(messages)",
            "            if payload is not None:",
            "                callback(payload)",
            "",
            "        return self.stream.on_recv(wrap_callback)",
            "",
            "",
            "class ZeroMQReqServerChannel(",
            "    salt.transport.mixins.auth.AESReqServerMixin, salt.transport.server.ReqServerChannel",
            "):",
            "    def __init__(self, opts):",
            "        salt.transport.server.ReqServerChannel.__init__(self, opts)",
            "        self._closing = False",
            "",
            "    def zmq_device(self):",
            "        \"\"\"",
            "        Multiprocessing target for the zmq queue device",
            "        \"\"\"",
            "        self.__setup_signals()",
            "        salt.utils.process.appendproctitle(\"MWorkerQueue\")",
            "        self.context = zmq.Context(self.opts[\"worker_threads\"])",
            "        # Prepare the zeromq sockets",
            "        self.uri = \"tcp://{interface}:{ret_port}\".format(**self.opts)",
            "        self.clients = self.context.socket(zmq.ROUTER)",
            "        if self.opts[\"ipv6\"] is True and hasattr(zmq, \"IPV4ONLY\"):",
            "            # IPv6 sockets work for both IPv6 and IPv4 addresses",
            "            self.clients.setsockopt(zmq.IPV4ONLY, 0)",
            "        self.clients.setsockopt(zmq.BACKLOG, self.opts.get(\"zmq_backlog\", 1000))",
            "        self._start_zmq_monitor()",
            "        self.workers = self.context.socket(zmq.DEALER)",
            "",
            "        if self.opts[\"mworker_queue_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting mworker_queue niceness to %d\",",
            "                self.opts[\"mworker_queue_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"mworker_queue_niceness\"])",
            "",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            self.w_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_workers\", 4515)",
            "            )",
            "        else:",
            "            self.w_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"workers.ipc\")",
            "            )",
            "",
            "        log.info(\"Setting up the master communication server\")",
            "        self.clients.bind(self.uri)",
            "        self.workers.bind(self.w_uri)",
            "",
            "        while True:",
            "            if self.clients.closed or self.workers.closed:",
            "                break",
            "            try:",
            "                zmq.device(zmq.QUEUE, self.clients, self.workers)",
            "            except zmq.ZMQError as exc:",
            "                if exc.errno == errno.EINTR:",
            "                    continue",
            "                raise",
            "            except (KeyboardInterrupt, SystemExit):",
            "                break",
            "",
            "    def close(self):",
            "        \"\"\"",
            "        Cleanly shutdown the router socket",
            "        \"\"\"",
            "        if self._closing:",
            "            return",
            "        log.info(\"MWorkerQueue under PID %s is closing\", os.getpid())",
            "        self._closing = True",
            "        # pylint: disable=E0203",
            "        if getattr(self, \"_monitor\", None) is not None:",
            "            self._monitor.stop()",
            "            self._monitor = None",
            "        if getattr(self, \"_w_monitor\", None) is not None:",
            "            self._w_monitor.stop()",
            "            self._w_monitor = None",
            "        if hasattr(self, \"clients\") and self.clients.closed is False:",
            "            self.clients.close()",
            "        if hasattr(self, \"workers\") and self.workers.closed is False:",
            "            self.workers.close()",
            "        if hasattr(self, \"stream\"):",
            "            self.stream.close()",
            "        if hasattr(self, \"_socket\") and self._socket.closed is False:",
            "            self._socket.close()",
            "        if hasattr(self, \"context\") and self.context.closed is False:",
            "            self.context.term()",
            "        # pylint: enable=E0203",
            "",
            "    def pre_fork(self, process_manager):",
            "        \"\"\"",
            "        Pre-fork we need to create the zmq router device",
            "",
            "        :param func process_manager: An instance of salt.utils.process.ProcessManager",
            "        \"\"\"",
            "        salt.transport.mixins.auth.AESReqServerMixin.pre_fork(self, process_manager)",
            "        process_manager.add_process(self.zmq_device)",
            "",
            "    def _start_zmq_monitor(self):",
            "        \"\"\"",
            "        Starts ZMQ monitor for debugging purposes.",
            "        :return:",
            "        \"\"\"",
            "        # Socket monitor shall be used the only for debug",
            "        # purposes so using threading doesn't look too bad here",
            "",
            "        if HAS_ZMQ_MONITOR and self.opts[\"zmq_monitor\"]:",
            "            log.debug(\"Starting ZMQ monitor\")",
            "            import threading",
            "",
            "            self._w_monitor = ZeroMQSocketMonitor(self._socket)",
            "            threading.Thread(target=self._w_monitor.start_poll).start()",
            "            log.debug(\"ZMQ monitor has been started started\")",
            "",
            "    def post_fork(self, payload_handler, io_loop):",
            "        \"\"\"",
            "        After forking we need to create all of the local sockets to listen to the",
            "        router",
            "",
            "        :param func payload_handler: A function to called to handle incoming payloads as",
            "                                     they are picked up off the wire",
            "        :param IOLoop io_loop: An instance of a Tornado IOLoop, to handle event scheduling",
            "        \"\"\"",
            "        self.payload_handler = payload_handler",
            "        self.io_loop = io_loop",
            "",
            "        self.context = zmq.Context(1)",
            "        self._socket = self.context.socket(zmq.REP)",
            "        self._start_zmq_monitor()",
            "",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            self.w_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_workers\", 4515)",
            "            )",
            "        else:",
            "            self.w_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"workers.ipc\")",
            "            )",
            "        log.info(\"Worker binding to socket %s\", self.w_uri)",
            "        self._socket.connect(self.w_uri)",
            "",
            "        salt.transport.mixins.auth.AESReqServerMixin.post_fork(",
            "            self, payload_handler, io_loop",
            "        )",
            "",
            "        self.stream = zmq.eventloop.zmqstream.ZMQStream(",
            "            self._socket, io_loop=self.io_loop",
            "        )",
            "        self.stream.on_recv_stream(self.handle_message)",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def handle_message(self, stream, payload):",
            "        \"\"\"",
            "        Handle incoming messages from underlying TCP streams",
            "",
            "        :stream ZMQStream stream: A ZeroMQ stream.",
            "        See http://zeromq.github.io/pyzmq/api/generated/zmq.eventloop.zmqstream.html",
            "",
            "        :param dict payload: A payload to process",
            "        \"\"\"",
            "        try:",
            "            payload = self.serial.loads(payload[0])",
            "            payload = self._decode_payload(payload)",
            "        except Exception as exc:  # pylint: disable=broad-except",
            "            exc_type = type(exc).__name__",
            "            if exc_type == \"AuthenticationError\":",
            "                log.debug(",
            "                    \"Minion failed to auth to master. Since the payload is \"",
            "                    \"encrypted, it is not known which minion failed to \"",
            "                    \"authenticate. It is likely that this is a transient \"",
            "                    \"failure due to the master rotating its public key.\"",
            "                )",
            "            else:",
            "                log.error(\"Bad load from minion: %s: %s\", exc_type, exc)",
            "            stream.send(self.serial.dumps(\"bad load\"))",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        # TODO helper functions to normalize payload?",
            "        if not isinstance(payload, dict) or not isinstance(payload.get(\"load\"), dict):",
            "            log.error(",
            "                \"payload and load must be a dict. Payload was: %s and load was %s\",",
            "                payload,",
            "                payload.get(\"load\"),",
            "            )",
            "            stream.send(self.serial.dumps(\"payload and load must be a dict\"))",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        try:",
            "            id_ = payload[\"load\"].get(\"id\", \"\")",
            "            if \"\\0\" in id_:",
            "                log.error(\"Payload contains an id with a null byte: %s\", payload)",
            "                stream.send(self.serial.dumps(\"bad load: id contains a null byte\"))",
            "                raise salt.ext.tornado.gen.Return()",
            "        except TypeError:",
            "            log.error(\"Payload contains non-string id: %s\", payload)",
            "            stream.send(",
            "                self.serial.dumps(\"bad load: id {} is not a string\".format(id_))",
            "            )",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        version = 0",
            "        if \"version\" in payload:",
            "            version = payload[\"version\"]",
            "",
            "        sign_messages = False",
            "        if version > 1:",
            "            sign_messages = True",
            "",
            "        # intercept the \"_auth\" commands, since the main daemon shouldn't know",
            "        # anything about our key auth",
            "        if payload[\"enc\"] == \"clear\" and payload.get(\"load\", {}).get(\"cmd\") == \"_auth\":",
            "            stream.send(self.serial.dumps(self._auth(payload[\"load\"], sign_messages)))",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        nonce = None",
            "        if version > 1:",
            "            nonce = payload[\"load\"].pop(\"nonce\", None)",
            "",
            "        # TODO: test",
            "        try:",
            "            # Take the payload_handler function that was registered when we created the channel",
            "            # and call it, returning control to the caller until it completes",
            "            ret, req_opts = yield self.payload_handler(payload)",
            "        except Exception as e:  # pylint: disable=broad-except",
            "            # always attempt to return an error to the minion",
            "            stream.send(\"Some exception handling minion payload\")",
            "            log.error(\"Some exception handling a payload from minion\", exc_info=True)",
            "            raise salt.ext.tornado.gen.Return()",
            "",
            "        req_fun = req_opts.get(\"fun\", \"send\")",
            "        if req_fun == \"send_clear\":",
            "            stream.send(self.serial.dumps(ret))",
            "        elif req_fun == \"send\":",
            "            stream.send(self.serial.dumps(self.crypticle.dumps(ret, nonce)))",
            "        elif req_fun == \"send_private\":",
            "            stream.send(",
            "                self.serial.dumps(",
            "                    self._encrypt_private(",
            "                        ret, req_opts[\"key\"], req_opts[\"tgt\"], nonce, sign_messages,",
            "                    )",
            "                )",
            "            )",
            "        else:",
            "            log.error(\"Unknown req_fun %s\", req_fun)",
            "            # always attempt to return an error to the minion",
            "            stream.send(\"Server-side exception handling payload\")",
            "        raise salt.ext.tornado.gen.Return()",
            "",
            "    def __setup_signals(self):",
            "        signal.signal(signal.SIGINT, self._handle_signals)",
            "        signal.signal(signal.SIGTERM, self._handle_signals)",
            "",
            "    def _handle_signals(self, signum, sigframe):",
            "        msg = \"{} received a \".format(self.__class__.__name__)",
            "        if signum == signal.SIGINT:",
            "            msg += \"SIGINT\"",
            "        elif signum == signal.SIGTERM:",
            "            msg += \"SIGTERM\"",
            "        msg += \". Exiting\"",
            "        log.debug(msg)",
            "        self.close()",
            "        sys.exit(salt.defaults.exitcodes.EX_OK)",
            "",
            "",
            "def _set_tcp_keepalive(zmq_socket, opts):",
            "    \"\"\"",
            "    Ensure that TCP keepalives are set as specified in \"opts\".",
            "",
            "    Warning: Failure to set TCP keepalives on the salt-master can result in",
            "    not detecting the loss of a minion when the connection is lost or when",
            "    its host has been terminated without first closing the socket.",
            "    Salt's Presence System depends on this connection status to know if a minion",
            "    is \"present\".",
            "",
            "    Warning: Failure to set TCP keepalives on minions can result in frequent or",
            "    unexpected disconnects!",
            "    \"\"\"",
            "    if hasattr(zmq, \"TCP_KEEPALIVE\") and opts:",
            "        if \"tcp_keepalive\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE, opts[\"tcp_keepalive\"])",
            "        if \"tcp_keepalive_idle\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE_IDLE, opts[\"tcp_keepalive_idle\"])",
            "        if \"tcp_keepalive_cnt\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE_CNT, opts[\"tcp_keepalive_cnt\"])",
            "        if \"tcp_keepalive_intvl\" in opts:",
            "            zmq_socket.setsockopt(zmq.TCP_KEEPALIVE_INTVL, opts[\"tcp_keepalive_intvl\"])",
            "",
            "",
            "class ZeroMQPubServerChannel(salt.transport.server.PubServerChannel):",
            "    \"\"\"",
            "    Encapsulate synchronous operations for a publisher channel",
            "    \"\"\"",
            "",
            "    _sock_data = threading.local()",
            "",
            "    def __init__(self, opts):",
            "        self.opts = opts",
            "        self.serial = salt.payload.Serial(self.opts)  # TODO: in init?",
            "        self.ckminions = salt.utils.minions.CkMinions(self.opts)",
            "",
            "    def connect(self):",
            "        return salt.ext.tornado.gen.sleep(5)",
            "",
            "    def _publish_daemon(self, log_queue=None):",
            "        \"\"\"",
            "        Bind to the interface specified in the configuration file",
            "        \"\"\"",
            "        salt.utils.process.appendproctitle(self.__class__.__name__)",
            "",
            "        if self.opts[\"pub_server_niceness\"] and not salt.utils.platform.is_windows():",
            "            log.info(",
            "                \"setting Publish daemon niceness to %i\",",
            "                self.opts[\"pub_server_niceness\"],",
            "            )",
            "            os.nice(self.opts[\"pub_server_niceness\"])",
            "",
            "        if log_queue:",
            "            salt.log.setup.set_multiprocessing_logging_queue(log_queue)",
            "            salt.log.setup.setup_multiprocessing_logging(log_queue)",
            "",
            "        # Set up the context",
            "        context = zmq.Context(1)",
            "        # Prepare minion publish socket",
            "        pub_sock = context.socket(zmq.PUB)",
            "        _set_tcp_keepalive(pub_sock, self.opts)",
            "        # if 2.1 >= zmq < 3.0, we only have one HWM setting",
            "        try:",
            "            pub_sock.setsockopt(zmq.HWM, self.opts.get(\"pub_hwm\", 1000))",
            "        # in zmq >= 3.0, there are separate send and receive HWM settings",
            "        except AttributeError:",
            "            # Set the High Water Marks. For more information on HWM, see:",
            "            # http://api.zeromq.org/4-1:zmq-setsockopt",
            "            pub_sock.setsockopt(zmq.SNDHWM, self.opts.get(\"pub_hwm\", 1000))",
            "            pub_sock.setsockopt(zmq.RCVHWM, self.opts.get(\"pub_hwm\", 1000))",
            "        if self.opts[\"ipv6\"] is True and hasattr(zmq, \"IPV4ONLY\"):",
            "            # IPv6 sockets work for both IPv6 and IPv4 addresses",
            "            pub_sock.setsockopt(zmq.IPV4ONLY, 0)",
            "        pub_sock.setsockopt(zmq.BACKLOG, self.opts.get(\"zmq_backlog\", 1000))",
            "        pub_sock.setsockopt(zmq.LINGER, -1)",
            "        pub_uri = \"tcp://{interface}:{publish_port}\".format(**self.opts)",
            "        # Prepare minion pull socket",
            "        pull_sock = context.socket(zmq.PULL)",
            "        pull_sock.setsockopt(zmq.LINGER, -1)",
            "",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_publish_pull\", 4514)",
            "            )",
            "        else:",
            "            pull_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "            )",
            "        salt.utils.zeromq.check_ipc_path_max_len(pull_uri)",
            "",
            "        # Start the minion command publisher",
            "        log.info(\"Starting the Salt Publisher on %s\", pub_uri)",
            "        pub_sock.bind(pub_uri)",
            "",
            "        # Securely create socket",
            "        log.info(\"Starting the Salt Puller on %s\", pull_uri)",
            "        with salt.utils.files.set_umask(0o177):",
            "            pull_sock.bind(pull_uri)",
            "",
            "        try:",
            "            while True:",
            "                # Catch and handle EINTR from when this process is sent",
            "                # SIGUSR1 gracefully so we don't choke and die horribly",
            "                try:",
            "                    log.debug(\"Publish daemon getting data from puller %s\", pull_uri)",
            "                    package = pull_sock.recv()",
            "                    log.debug(\"Publish daemon received payload. size=%d\", len(package))",
            "",
            "                    unpacked_package = salt.payload.unpackage(package)",
            "                    unpacked_package = salt.transport.frame.decode_embedded_strs(",
            "                        unpacked_package",
            "                    )",
            "                    payload = unpacked_package[\"payload\"]",
            "                    log.trace(\"Accepted unpacked package from puller\")",
            "                    if self.opts[\"zmq_filtering\"]:",
            "                        # if you have a specific topic list, use that",
            "                        if \"topic_lst\" in unpacked_package:",
            "                            for topic in unpacked_package[\"topic_lst\"]:",
            "                                log.trace(",
            "                                    \"Sending filtered data over publisher %s\", pub_uri",
            "                                )",
            "                                # zmq filters are substring match, hash the topic",
            "                                # to avoid collisions",
            "                                htopic = salt.utils.stringutils.to_bytes(",
            "                                    hashlib.sha1(",
            "                                        salt.utils.stringutils.to_bytes(topic)",
            "                                    ).hexdigest()",
            "                                )",
            "                                pub_sock.send(htopic, flags=zmq.SNDMORE)",
            "                                pub_sock.send(payload)",
            "                                log.trace(\"Filtered data has been sent\")",
            "",
            "                            # Syndic broadcast",
            "                            if self.opts.get(\"order_masters\"):",
            "                                log.trace(\"Sending filtered data to syndic\")",
            "                                pub_sock.send(b\"syndic\", flags=zmq.SNDMORE)",
            "                                pub_sock.send(payload)",
            "                                log.trace(\"Filtered data has been sent to syndic\")",
            "                        # otherwise its a broadcast",
            "                        else:",
            "                            # TODO: constants file for \"broadcast\"",
            "                            log.trace(",
            "                                \"Sending broadcasted data over publisher %s\", pub_uri",
            "                            )",
            "                            pub_sock.send(b\"broadcast\", flags=zmq.SNDMORE)",
            "                            pub_sock.send(payload)",
            "                            log.trace(\"Broadcasted data has been sent\")",
            "                    else:",
            "                        log.trace(",
            "                            \"Sending ZMQ-unfiltered data over publisher %s\", pub_uri",
            "                        )",
            "                        pub_sock.send(payload)",
            "                        log.trace(\"Unfiltered data has been sent\")",
            "                except zmq.ZMQError as exc:",
            "                    if exc.errno == errno.EINTR:",
            "                        continue",
            "                    raise",
            "",
            "        except KeyboardInterrupt:",
            "            log.trace(\"Publish daemon caught Keyboard interupt, tearing down\")",
            "        # Cleanly close the sockets if we're shutting down",
            "        if pub_sock.closed is False:",
            "            pub_sock.close()",
            "        if pull_sock.closed is False:",
            "            pull_sock.close()",
            "        if context.closed is False:",
            "            context.term()",
            "",
            "    def pre_fork(self, process_manager, kwargs=None):",
            "        \"\"\"",
            "        Do anything necessary pre-fork. Since this is on the master side this will",
            "        primarily be used to create IPC channels and create our daemon process to",
            "        do the actual publishing",
            "",
            "        :param func process_manager: A ProcessManager, from salt.utils.process.ProcessManager",
            "        \"\"\"",
            "        process_manager.add_process(self._publish_daemon, kwargs=kwargs)",
            "",
            "    @property",
            "    def pub_sock(self):",
            "        \"\"\"",
            "        This thread's zmq publisher socket. This socket is stored on the class",
            "        so that multiple instantiations in the same thread will re-use a single",
            "        zmq socket.",
            "        \"\"\"",
            "        try:",
            "            return self._sock_data.sock",
            "        except AttributeError:",
            "            pass",
            "",
            "    def pub_connect(self):",
            "        \"\"\"",
            "        Create and connect this thread's zmq socket. If a publisher socket",
            "        already exists \"pub_close\" is called before creating and connecting a",
            "        new socket.",
            "        \"\"\"",
            "        if self.pub_sock:",
            "            self.pub_close()",
            "        ctx = zmq.Context.instance()",
            "        self._sock_data.sock = ctx.socket(zmq.PUSH)",
            "        self.pub_sock.setsockopt(zmq.LINGER, -1)",
            "        if self.opts.get(\"ipc_mode\", \"\") == \"tcp\":",
            "            pull_uri = \"tcp://127.0.0.1:{}\".format(",
            "                self.opts.get(\"tcp_master_publish_pull\", 4514)",
            "            )",
            "        else:",
            "            pull_uri = \"ipc://{}\".format(",
            "                os.path.join(self.opts[\"sock_dir\"], \"publish_pull.ipc\")",
            "            )",
            "        log.debug(\"Connecting to pub server: %s\", pull_uri)",
            "        self.pub_sock.connect(pull_uri)",
            "        return self._sock_data.sock",
            "",
            "    def pub_close(self):",
            "        \"\"\"",
            "        Disconnect an existing publisher socket and remove it from the local",
            "        thread's cache.",
            "        \"\"\"",
            "        if hasattr(self._sock_data, \"sock\"):",
            "            self._sock_data.sock.close()",
            "            delattr(self._sock_data, \"sock\")",
            "",
            "    def publish(self, load):",
            "        \"\"\"",
            "        Publish \"load\" to minions. This send the load to the publisher daemon",
            "        process with does the actual sending to minions.",
            "",
            "        :param dict load: A load to be sent across the wire to minions",
            "        \"\"\"",
            "        payload = {\"enc\": \"aes\"}",
            "        load[\"serial\"] = salt.master.SMaster.get_serial()",
            "        crypticle = salt.crypt.Crypticle(",
            "            self.opts, salt.master.SMaster.secrets[\"aes\"][\"secret\"].value",
            "        )",
            "        payload[\"load\"] = crypticle.dumps(load)",
            "        if self.opts[\"sign_pub_messages\"]:",
            "            master_pem_path = os.path.join(self.opts[\"pki_dir\"], \"master.pem\")",
            "            log.debug(\"Signing data packet\")",
            "            payload[\"sig\"] = salt.crypt.sign_message(master_pem_path, payload[\"load\"])",
            "        int_payload = {\"payload\": self.serial.dumps(payload)}",
            "",
            "        # add some targeting stuff for lists only (for now)",
            "        if load[\"tgt_type\"] == \"list\":",
            "            int_payload[\"topic_lst\"] = load[\"tgt\"]",
            "",
            "        # If zmq_filtering is enabled, target matching has to happen master side",
            "        match_targets = [\"pcre\", \"glob\", \"list\"]",
            "        if self.opts[\"zmq_filtering\"] and load[\"tgt_type\"] in match_targets:",
            "            # Fetch a list of minions that match",
            "            _res = self.ckminions.check_minions(load[\"tgt\"], tgt_type=load[\"tgt_type\"])",
            "            match_ids = _res[\"minions\"]",
            "",
            "            log.debug(\"Publish Side Match: %s\", match_ids)",
            "            # Send list of miions thru so zmq can target them",
            "            int_payload[\"topic_lst\"] = match_ids",
            "        payload = self.serial.dumps(int_payload)",
            "        log.debug(",
            "            \"Sending payload to publish daemon. jid=%s size=%d\",",
            "            load.get(\"jid\", None),",
            "            len(payload),",
            "        )",
            "        if not self.pub_sock:",
            "            self.pub_connect()",
            "        self.pub_sock.send(payload)",
            "        log.debug(\"Sent payload to publish daemon.\")",
            "",
            "",
            "class AsyncReqMessageClientPool(salt.transport.MessageClientPool):",
            "    \"\"\"",
            "    Wrapper class of AsyncReqMessageClientPool to avoid blocking waiting while writing data to socket.",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, args=None, kwargs=None):",
            "        super().__init__(AsyncReqMessageClient, opts, args=args, kwargs=kwargs)",
            "        self._closing = False",
            "",
            "    def close(self):",
            "        if self._closing:",
            "            return",
            "",
            "        self._closing = True",
            "        for message_client in self.message_clients:",
            "            message_client.close()",
            "        self.message_clients = []",
            "",
            "    def send(self, *args, **kwargs):",
            "        message_clients = sorted(self.message_clients, key=lambda x: len(x.send_queue))",
            "        return message_clients[0].send(*args, **kwargs)",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "",
            "# TODO: unit tests!",
            "class AsyncReqMessageClient:",
            "    \"\"\"",
            "    This class wraps the underlying zeromq REQ socket and gives a future-based",
            "    interface to sending and recieving messages. This works around the primary",
            "    limitation of serialized send/recv on the underlying socket by queueing the",
            "    message sends in this class. In the future if we decide to attempt to multiplex",
            "    we can manage a pool of REQ/REP sockets-- but for now we'll just do them in serial",
            "    \"\"\"",
            "",
            "    def __init__(self, opts, addr, linger=0, io_loop=None):",
            "        \"\"\"",
            "        Create an asynchronous message client",
            "",
            "        :param dict opts: The salt opts dictionary",
            "        :param str addr: The interface IP address to bind to",
            "        :param int linger: The number of seconds to linger on a ZMQ socket. See",
            "                           http://api.zeromq.org/2-1:zmq-setsockopt [ZMQ_LINGER]",
            "        :param IOLoop io_loop: A Tornado IOLoop event scheduler [tornado.ioloop.IOLoop]",
            "        \"\"\"",
            "        self.opts = opts",
            "        self.addr = addr",
            "        self.linger = linger",
            "        if io_loop is None:",
            "            self.io_loop = salt.ext.tornado.ioloop.IOLoop.current()",
            "        else:",
            "            self.io_loop = io_loop",
            "",
            "        self.serial = salt.payload.Serial(self.opts)",
            "        self.context = zmq.Context()",
            "",
            "        # wire up sockets",
            "        self._init_socket()",
            "",
            "        self.send_queue = []",
            "        # mapping of message -> future",
            "        self.send_future_map = {}",
            "",
            "        self.send_timeout_map = {}  # message -> timeout",
            "        self._closing = False",
            "",
            "    # TODO: timeout all in-flight sessions, or error",
            "    def close(self):",
            "        try:",
            "            if self._closing:",
            "                return",
            "        except AttributeError:",
            "            # We must have been called from __del__",
            "            # The python interpreter has nuked most attributes already",
            "            return",
            "        else:",
            "            self._closing = True",
            "            if hasattr(self, \"stream\") and self.stream is not None:",
            "                if ZMQ_VERSION_INFO < (14, 3, 0):",
            "                    # stream.close() doesn't work properly on pyzmq < 14.3.0",
            "                    if self.stream.socket:",
            "                        self.stream.socket.close()",
            "                    self.stream.io_loop.remove_handler(self.stream.socket)",
            "                    # set this to None, more hacks for messed up pyzmq",
            "                    self.stream.socket = None",
            "                    self.socket.close()",
            "                else:",
            "                    self.stream.close()",
            "                    self.socket = None",
            "                self.stream = None",
            "            if self.context.closed is False:",
            "                self.context.term()",
            "",
            "    # pylint: disable=W1701",
            "    def __del__(self):",
            "        self.close()",
            "",
            "    # pylint: enable=W1701",
            "",
            "    def _init_socket(self):",
            "        if hasattr(self, \"stream\"):",
            "            self.stream.close()  # pylint: disable=E0203",
            "            self.socket.close()  # pylint: disable=E0203",
            "            del self.stream",
            "            del self.socket",
            "",
            "        self.socket = self.context.socket(zmq.REQ)",
            "",
            "        # socket options",
            "        if hasattr(zmq, \"RECONNECT_IVL_MAX\"):",
            "            self.socket.setsockopt(zmq.RECONNECT_IVL_MAX, 5000)",
            "",
            "        _set_tcp_keepalive(self.socket, self.opts)",
            "        if self.addr.startswith(\"tcp://[\"):",
            "            # Hint PF type if bracket enclosed IPv6 address",
            "            if hasattr(zmq, \"IPV6\"):",
            "                self.socket.setsockopt(zmq.IPV6, 1)",
            "            elif hasattr(zmq, \"IPV4ONLY\"):",
            "                self.socket.setsockopt(zmq.IPV4ONLY, 0)",
            "        self.socket.linger = self.linger",
            "        log.debug(\"Trying to connect to: %s\", self.addr)",
            "        self.socket.connect(self.addr)",
            "        self.stream = zmq.eventloop.zmqstream.ZMQStream(",
            "            self.socket, io_loop=self.io_loop",
            "        )",
            "",
            "    @salt.ext.tornado.gen.coroutine",
            "    def _internal_send_recv(self):",
            "        while len(self.send_queue) > 0:",
            "            message = self.send_queue[0]",
            "            future = self.send_future_map.get(message, None)",
            "            if future is None:",
            "                # Timedout",
            "                del self.send_queue[0]",
            "                continue",
            "",
            "            # send",
            "            def mark_future(msg):",
            "                if not future.done():",
            "                    data = self.serial.loads(msg[0])",
            "                    future.set_result(data)",
            "",
            "            self.stream.on_recv(mark_future)",
            "            self.stream.send(message)",
            "",
            "            try:",
            "                ret = yield future",
            "            except Exception as err:  # pylint: disable=broad-except",
            "                log.debug(\"Re-init ZMQ socket: %s\", err)",
            "                self._init_socket()  # re-init the zmq socket (no other way in zmq)",
            "                del self.send_queue[0]",
            "                continue",
            "            del self.send_queue[0]",
            "            self.send_future_map.pop(message, None)",
            "            self.remove_message_timeout(message)",
            "",
            "    def remove_message_timeout(self, message):",
            "        if message not in self.send_timeout_map:",
            "            return",
            "        timeout = self.send_timeout_map.pop(message, None)",
            "        if timeout is not None:",
            "            # Hasn't been already timedout",
            "            self.io_loop.remove_timeout(timeout)",
            "",
            "    def timeout_message(self, message):",
            "        \"\"\"",
            "        Handle a message timeout by removing it from the sending queue",
            "        and informing the caller",
            "",
            "        :raises: SaltReqTimeoutError",
            "        \"\"\"",
            "        future = self.send_future_map.pop(message, None)",
            "        # In a race condition the message might have been sent by the time",
            "        # we're timing it out. Make sure the future is not None",
            "        if future is not None:",
            "            del self.send_timeout_map[message]",
            "            if future.attempts < future.tries:",
            "                future.attempts += 1",
            "                log.debug(",
            "                    \"SaltReqTimeoutError, retrying. (%s/%s)\",",
            "                    future.attempts,",
            "                    future.tries,",
            "                )",
            "                self.send(",
            "                    message, timeout=future.timeout, tries=future.tries, future=future,",
            "                )",
            "",
            "            else:",
            "                future.set_exception(SaltReqTimeoutError(\"Message timed out\"))",
            "",
            "    def send(",
            "        self, message, timeout=None, tries=3, future=None, callback=None, raw=False",
            "    ):",
            "        \"\"\"",
            "        Return a future which will be completed when the message has a response",
            "        \"\"\"",
            "        if future is None:",
            "            future = salt.ext.tornado.concurrent.Future()",
            "            future.tries = tries",
            "            future.attempts = 0",
            "            future.timeout = timeout",
            "            # if a future wasn't passed in, we need to serialize the message",
            "            message = self.serial.dumps(message)",
            "        if callback is not None:",
            "",
            "            def handle_future(future):",
            "                response = future.result()",
            "                self.io_loop.add_callback(callback, response)",
            "",
            "            future.add_done_callback(handle_future)",
            "        # Add this future to the mapping",
            "        self.send_future_map[message] = future",
            "",
            "        if self.opts.get(\"detect_mode\") is True:",
            "            timeout = 1",
            "",
            "        if timeout is not None:",
            "            send_timeout = self.io_loop.call_later(",
            "                timeout, self.timeout_message, message",
            "            )",
            "            self.send_timeout_map[message] = send_timeout",
            "",
            "        if len(self.send_queue) == 0:",
            "            self.io_loop.spawn_callback(self._internal_send_recv)",
            "",
            "        self.send_queue.append(message)",
            "",
            "        return future",
            "",
            "",
            "class ZeroMQSocketMonitor:",
            "    __EVENT_MAP = None",
            "",
            "    def __init__(self, socket):",
            "        \"\"\"",
            "        Create ZMQ monitor sockets",
            "",
            "        More information:",
            "            http://api.zeromq.org/4-0:zmq-socket-monitor",
            "        \"\"\"",
            "        self._socket = socket",
            "        self._monitor_socket = self._socket.get_monitor_socket()",
            "        self._monitor_stream = None",
            "",
            "    def start_io_loop(self, io_loop):",
            "        log.trace(\"Event monitor start!\")",
            "        self._monitor_stream = zmq.eventloop.zmqstream.ZMQStream(",
            "            self._monitor_socket, io_loop=io_loop",
            "        )",
            "        self._monitor_stream.on_recv(self.monitor_callback)",
            "",
            "    def start_poll(self):",
            "        log.trace(\"Event monitor start!\")",
            "        try:",
            "            while self._monitor_socket is not None and self._monitor_socket.poll():",
            "                msg = self._monitor_socket.recv_multipart()",
            "                self.monitor_callback(msg)",
            "        except (AttributeError, zmq.error.ContextTerminated):",
            "            # We cannot log here because we'll get an interrupted system call in trying",
            "            # to flush the logging buffer as we terminate",
            "            pass",
            "",
            "    @property",
            "    def event_map(self):",
            "        if ZeroMQSocketMonitor.__EVENT_MAP is None:",
            "            event_map = {}",
            "            for name in dir(zmq):",
            "                if name.startswith(\"EVENT_\"):",
            "                    value = getattr(zmq, name)",
            "                    event_map[value] = name",
            "            ZeroMQSocketMonitor.__EVENT_MAP = event_map",
            "        return ZeroMQSocketMonitor.__EVENT_MAP",
            "",
            "    def monitor_callback(self, msg):",
            "        evt = zmq.utils.monitor.parse_monitor_message(msg)",
            "        evt[\"description\"] = self.event_map[evt[\"event\"]]",
            "        log.debug(\"ZeroMQ event: %s\", evt)",
            "        if evt[\"event\"] == zmq.EVENT_MONITOR_STOPPED:",
            "            self.stop()",
            "",
            "    def stop(self):",
            "        if self._socket is None:",
            "            return",
            "        self._socket.disable_monitor()",
            "        self._socket = None",
            "        self._monitor_socket = None",
            "        if self._monitor_stream is not None:",
            "            self._monitor_stream.close()",
            "            self._monitor_stream = None",
            "        log.trace(\"Event monitor done!\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "86": [
                "_get_master_uri"
            ],
            "433": [
                "AsyncZeroMQReqChannel",
                "_crypted_transfer",
                "_do_transfer"
            ],
            "922": [
                "ZeroMQReqServerChannel",
                "handle_message"
            ]
        },
        "addLocation": [
            "salt.transport.zeromq.AsyncZeroMQReqChannel.master_uri",
            "shuup.front.urls",
            "salt.transport.zeromq.AsyncZeroMQPubChannel.master_pub"
        ]
    }
}