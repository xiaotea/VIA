{
    "tornado/httputil.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1057,
                "afterPatchRowNumber": 1057,
                "PatchRowcode": "             yield (k, v)"
            },
            "1": {
                "beforePatchRowNumber": 1058,
                "afterPatchRowNumber": 1058,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 1059,
                "afterPatchRowNumber": 1059,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 1060,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-_OctalPatt = re.compile(r\"\\\\[0-3][0-7][0-7]\")"
            },
            "4": {
                "beforePatchRowNumber": 1061,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-_QuotePatt = re.compile(r\"[\\\\].\")"
            },
            "5": {
                "beforePatchRowNumber": 1062,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-_nulljoin = \"\".join"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1060,
                "PatchRowcode": "+_unquote_sub = re.compile(r\"\\\\(?:([0-3][0-7][0-7])|(.))\").sub"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1061,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1062,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1063,
                "PatchRowcode": "+def _unquote_replace(m: re.Match) -> str:"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1064,
                "PatchRowcode": "+    if m[1]:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1065,
                "PatchRowcode": "+        return chr(int(m[1], 8))"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1066,
                "PatchRowcode": "+    else:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1067,
                "PatchRowcode": "+        return m[2]"
            },
            "14": {
                "beforePatchRowNumber": 1063,
                "afterPatchRowNumber": 1068,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 1064,
                "afterPatchRowNumber": 1069,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 1065,
                "afterPatchRowNumber": 1070,
                "PatchRowcode": " def _unquote_cookie(s: str) -> str:"
            },
            "17": {
                "beforePatchRowNumber": 1066,
                "afterPatchRowNumber": 1071,
                "PatchRowcode": "     \"\"\"Handle double quotes and escaping in cookie values."
            },
            "18": {
                "beforePatchRowNumber": 1067,
                "afterPatchRowNumber": 1072,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": 1068,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    This method is copied verbatim from the Python 3.5 standard"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1073,
                "PatchRowcode": "+    This method is copied verbatim from the Python 3.13 standard"
            },
            "21": {
                "beforePatchRowNumber": 1069,
                "afterPatchRowNumber": 1074,
                "PatchRowcode": "     library (http.cookies._unquote) so we don't have to depend on"
            },
            "22": {
                "beforePatchRowNumber": 1070,
                "afterPatchRowNumber": 1075,
                "PatchRowcode": "     non-public interfaces."
            },
            "23": {
                "beforePatchRowNumber": 1071,
                "afterPatchRowNumber": 1076,
                "PatchRowcode": "     \"\"\""
            },
            "24": {
                "beforePatchRowNumber": 1086,
                "afterPatchRowNumber": 1091,
                "PatchRowcode": "     #    \\012 --> \\n"
            },
            "25": {
                "beforePatchRowNumber": 1087,
                "afterPatchRowNumber": 1092,
                "PatchRowcode": "     #    \\\"   --> \""
            },
            "26": {
                "beforePatchRowNumber": 1088,
                "afterPatchRowNumber": 1093,
                "PatchRowcode": "     #"
            },
            "27": {
                "beforePatchRowNumber": 1089,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    i = 0"
            },
            "28": {
                "beforePatchRowNumber": 1090,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    n = len(s)"
            },
            "29": {
                "beforePatchRowNumber": 1091,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    res = []"
            },
            "30": {
                "beforePatchRowNumber": 1092,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    while 0 <= i < n:"
            },
            "31": {
                "beforePatchRowNumber": 1093,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        o_match = _OctalPatt.search(s, i)"
            },
            "32": {
                "beforePatchRowNumber": 1094,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        q_match = _QuotePatt.search(s, i)"
            },
            "33": {
                "beforePatchRowNumber": 1095,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not o_match and not q_match:  # Neither matched"
            },
            "34": {
                "beforePatchRowNumber": 1096,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            res.append(s[i:])"
            },
            "35": {
                "beforePatchRowNumber": 1097,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            break"
            },
            "36": {
                "beforePatchRowNumber": 1098,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # else:"
            },
            "37": {
                "beforePatchRowNumber": 1099,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        j = k = -1"
            },
            "38": {
                "beforePatchRowNumber": 1100,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if o_match:"
            },
            "39": {
                "beforePatchRowNumber": 1101,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            j = o_match.start(0)"
            },
            "40": {
                "beforePatchRowNumber": 1102,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if q_match:"
            },
            "41": {
                "beforePatchRowNumber": 1103,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            k = q_match.start(0)"
            },
            "42": {
                "beforePatchRowNumber": 1104,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if q_match and (not o_match or k < j):  # QuotePatt matched"
            },
            "43": {
                "beforePatchRowNumber": 1105,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            res.append(s[i:k])"
            },
            "44": {
                "beforePatchRowNumber": 1106,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            res.append(s[k + 1])"
            },
            "45": {
                "beforePatchRowNumber": 1107,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            i = k + 2"
            },
            "46": {
                "beforePatchRowNumber": 1108,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:  # OctalPatt matched"
            },
            "47": {
                "beforePatchRowNumber": 1109,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            res.append(s[i:j])"
            },
            "48": {
                "beforePatchRowNumber": 1110,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            res.append(chr(int(s[j + 1 : j + 4], 8)))"
            },
            "49": {
                "beforePatchRowNumber": 1111,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            i = j + 4"
            },
            "50": {
                "beforePatchRowNumber": 1112,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return _nulljoin(res)"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1094,
                "PatchRowcode": "+    return _unquote_sub(_unquote_replace, s)"
            },
            "52": {
                "beforePatchRowNumber": 1113,
                "afterPatchRowNumber": 1095,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 1114,
                "afterPatchRowNumber": 1096,
                "PatchRowcode": " "
            },
            "54": {
                "beforePatchRowNumber": 1115,
                "afterPatchRowNumber": 1097,
                "PatchRowcode": " def parse_cookie(cookie: str) -> Dict[str, str]:"
            }
        },
        "frontPatchFile": [
            "#",
            "# Copyright 2009 Facebook",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"HTTP utility code shared by clients and servers.",
            "",
            "This module also defines the `HTTPServerRequest` class which is exposed",
            "via `tornado.web.RequestHandler.request`.",
            "\"\"\"",
            "",
            "import calendar",
            "import collections.abc",
            "import copy",
            "import datetime",
            "import email.utils",
            "from functools import lru_cache",
            "from http.client import responses",
            "import http.cookies",
            "import re",
            "from ssl import SSLError",
            "import time",
            "import unicodedata",
            "from urllib.parse import urlencode, urlparse, urlunparse, parse_qsl",
            "",
            "from tornado.escape import native_str, parse_qs_bytes, utf8",
            "from tornado.log import gen_log",
            "from tornado.util import ObjectDict, unicode_type",
            "",
            "",
            "# responses is unused in this file, but we re-export it to other files.",
            "# Reference it so pyflakes doesn't complain.",
            "responses",
            "",
            "import typing",
            "from typing import (",
            "    Tuple,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Iterator,",
            "    Dict,",
            "    Union,",
            "    Optional,",
            "    Awaitable,",
            "    Generator,",
            "    AnyStr,",
            ")",
            "",
            "if typing.TYPE_CHECKING:",
            "    from typing import Deque  # noqa: F401",
            "    from asyncio import Future  # noqa: F401",
            "    import unittest  # noqa: F401",
            "",
            "# To be used with str.strip() and related methods.",
            "HTTP_WHITESPACE = \" \\t\"",
            "",
            "",
            "@lru_cache(1000)",
            "def _normalize_header(name: str) -> str:",
            "    \"\"\"Map a header name to Http-Header-Case.",
            "",
            "    >>> _normalize_header(\"coNtent-TYPE\")",
            "    'Content-Type'",
            "    \"\"\"",
            "    return \"-\".join([w.capitalize() for w in name.split(\"-\")])",
            "",
            "",
            "class HTTPHeaders(collections.abc.MutableMapping):",
            "    \"\"\"A dictionary that maintains ``Http-Header-Case`` for all keys.",
            "",
            "    Supports multiple values per key via a pair of new methods,",
            "    `add()` and `get_list()`.  The regular dictionary interface",
            "    returns a single value per key, with multiple values joined by a",
            "    comma.",
            "",
            "    >>> h = HTTPHeaders({\"content-type\": \"text/html\"})",
            "    >>> list(h.keys())",
            "    ['Content-Type']",
            "    >>> h[\"Content-Type\"]",
            "    'text/html'",
            "",
            "    >>> h.add(\"Set-Cookie\", \"A=B\")",
            "    >>> h.add(\"Set-Cookie\", \"C=D\")",
            "    >>> h[\"set-cookie\"]",
            "    'A=B,C=D'",
            "    >>> h.get_list(\"set-cookie\")",
            "    ['A=B', 'C=D']",
            "",
            "    >>> for (k,v) in sorted(h.get_all()):",
            "    ...    print('%s: %s' % (k,v))",
            "    ...",
            "    Content-Type: text/html",
            "    Set-Cookie: A=B",
            "    Set-Cookie: C=D",
            "    \"\"\"",
            "",
            "    @typing.overload",
            "    def __init__(self, __arg: Mapping[str, List[str]]) -> None:",
            "        pass",
            "",
            "    @typing.overload  # noqa: F811",
            "    def __init__(self, __arg: Mapping[str, str]) -> None:",
            "        pass",
            "",
            "    @typing.overload  # noqa: F811",
            "    def __init__(self, *args: Tuple[str, str]) -> None:",
            "        pass",
            "",
            "    @typing.overload  # noqa: F811",
            "    def __init__(self, **kwargs: str) -> None:",
            "        pass",
            "",
            "    def __init__(self, *args: typing.Any, **kwargs: str) -> None:  # noqa: F811",
            "        self._dict = {}  # type: typing.Dict[str, str]",
            "        self._as_list = {}  # type: typing.Dict[str, typing.List[str]]",
            "        self._last_key = None  # type: Optional[str]",
            "        if len(args) == 1 and len(kwargs) == 0 and isinstance(args[0], HTTPHeaders):",
            "            # Copy constructor",
            "            for k, v in args[0].get_all():",
            "                self.add(k, v)",
            "        else:",
            "            # Dict-style initialization",
            "            self.update(*args, **kwargs)",
            "",
            "    # new public methods",
            "",
            "    def add(self, name: str, value: str) -> None:",
            "        \"\"\"Adds a new value for the given key.\"\"\"",
            "        norm_name = _normalize_header(name)",
            "        self._last_key = norm_name",
            "        if norm_name in self:",
            "            self._dict[norm_name] = (",
            "                native_str(self[norm_name]) + \",\" + native_str(value)",
            "            )",
            "            self._as_list[norm_name].append(value)",
            "        else:",
            "            self[norm_name] = value",
            "",
            "    def get_list(self, name: str) -> List[str]:",
            "        \"\"\"Returns all values for the given header as a list.\"\"\"",
            "        norm_name = _normalize_header(name)",
            "        return self._as_list.get(norm_name, [])",
            "",
            "    def get_all(self) -> Iterable[Tuple[str, str]]:",
            "        \"\"\"Returns an iterable of all (name, value) pairs.",
            "",
            "        If a header has multiple values, multiple pairs will be",
            "        returned with the same name.",
            "        \"\"\"",
            "        for name, values in self._as_list.items():",
            "            for value in values:",
            "                yield (name, value)",
            "",
            "    def parse_line(self, line: str) -> None:",
            "        \"\"\"Updates the dictionary with a single header line.",
            "",
            "        >>> h = HTTPHeaders()",
            "        >>> h.parse_line(\"Content-Type: text/html\")",
            "        >>> h.get('content-type')",
            "        'text/html'",
            "        \"\"\"",
            "        if line[0].isspace():",
            "            # continuation of a multi-line header",
            "            if self._last_key is None:",
            "                raise HTTPInputError(\"first header line cannot start with whitespace\")",
            "            new_part = \" \" + line.lstrip(HTTP_WHITESPACE)",
            "            self._as_list[self._last_key][-1] += new_part",
            "            self._dict[self._last_key] += new_part",
            "        else:",
            "            try:",
            "                name, value = line.split(\":\", 1)",
            "            except ValueError:",
            "                raise HTTPInputError(\"no colon in header line\")",
            "            self.add(name, value.strip(HTTP_WHITESPACE))",
            "",
            "    @classmethod",
            "    def parse(cls, headers: str) -> \"HTTPHeaders\":",
            "        \"\"\"Returns a dictionary from HTTP header text.",
            "",
            "        >>> h = HTTPHeaders.parse(\"Content-Type: text/html\\\\r\\\\nContent-Length: 42\\\\r\\\\n\")",
            "        >>> sorted(h.items())",
            "        [('Content-Length', '42'), ('Content-Type', 'text/html')]",
            "",
            "        .. versionchanged:: 5.1",
            "",
            "           Raises `HTTPInputError` on malformed headers instead of a",
            "           mix of `KeyError`, and `ValueError`.",
            "",
            "        \"\"\"",
            "        h = cls()",
            "        # RFC 7230 section 3.5: a recipient MAY recognize a single LF as a line",
            "        # terminator and ignore any preceding CR.",
            "        for line in headers.split(\"\\n\"):",
            "            if line.endswith(\"\\r\"):",
            "                line = line[:-1]",
            "            if line:",
            "                h.parse_line(line)",
            "        return h",
            "",
            "    # MutableMapping abstract method implementations.",
            "",
            "    def __setitem__(self, name: str, value: str) -> None:",
            "        norm_name = _normalize_header(name)",
            "        self._dict[norm_name] = value",
            "        self._as_list[norm_name] = [value]",
            "",
            "    def __getitem__(self, name: str) -> str:",
            "        return self._dict[_normalize_header(name)]",
            "",
            "    def __delitem__(self, name: str) -> None:",
            "        norm_name = _normalize_header(name)",
            "        del self._dict[norm_name]",
            "        del self._as_list[norm_name]",
            "",
            "    def __len__(self) -> int:",
            "        return len(self._dict)",
            "",
            "    def __iter__(self) -> Iterator[typing.Any]:",
            "        return iter(self._dict)",
            "",
            "    def copy(self) -> \"HTTPHeaders\":",
            "        # defined in dict but not in MutableMapping.",
            "        return HTTPHeaders(self)",
            "",
            "    # Use our overridden copy method for the copy.copy module.",
            "    # This makes shallow copies one level deeper, but preserves",
            "    # the appearance that HTTPHeaders is a single container.",
            "    __copy__ = copy",
            "",
            "    def __str__(self) -> str:",
            "        lines = []",
            "        for name, value in self.get_all():",
            "            lines.append(\"%s: %s\\n\" % (name, value))",
            "        return \"\".join(lines)",
            "",
            "    __unicode__ = __str__",
            "",
            "",
            "class HTTPServerRequest(object):",
            "    \"\"\"A single HTTP request.",
            "",
            "    All attributes are type `str` unless otherwise noted.",
            "",
            "    .. attribute:: method",
            "",
            "       HTTP request method, e.g. \"GET\" or \"POST\"",
            "",
            "    .. attribute:: uri",
            "",
            "       The requested uri.",
            "",
            "    .. attribute:: path",
            "",
            "       The path portion of `uri`",
            "",
            "    .. attribute:: query",
            "",
            "       The query portion of `uri`",
            "",
            "    .. attribute:: version",
            "",
            "       HTTP version specified in request, e.g. \"HTTP/1.1\"",
            "",
            "    .. attribute:: headers",
            "",
            "       `.HTTPHeaders` dictionary-like object for request headers.  Acts like",
            "       a case-insensitive dictionary with additional methods for repeated",
            "       headers.",
            "",
            "    .. attribute:: body",
            "",
            "       Request body, if present, as a byte string.",
            "",
            "    .. attribute:: remote_ip",
            "",
            "       Client's IP address as a string.  If ``HTTPServer.xheaders`` is set,",
            "       will pass along the real IP address provided by a load balancer",
            "       in the ``X-Real-Ip`` or ``X-Forwarded-For`` header.",
            "",
            "    .. versionchanged:: 3.1",
            "       The list format of ``X-Forwarded-For`` is now supported.",
            "",
            "    .. attribute:: protocol",
            "",
            "       The protocol used, either \"http\" or \"https\".  If ``HTTPServer.xheaders``",
            "       is set, will pass along the protocol used by a load balancer if",
            "       reported via an ``X-Scheme`` header.",
            "",
            "    .. attribute:: host",
            "",
            "       The requested hostname, usually taken from the ``Host`` header.",
            "",
            "    .. attribute:: arguments",
            "",
            "       GET/POST arguments are available in the arguments property, which",
            "       maps arguments names to lists of values (to support multiple values",
            "       for individual names). Names are of type `str`, while arguments",
            "       are byte strings.  Note that this is different from",
            "       `.RequestHandler.get_argument`, which returns argument values as",
            "       unicode strings.",
            "",
            "    .. attribute:: query_arguments",
            "",
            "       Same format as ``arguments``, but contains only arguments extracted",
            "       from the query string.",
            "",
            "       .. versionadded:: 3.2",
            "",
            "    .. attribute:: body_arguments",
            "",
            "       Same format as ``arguments``, but contains only arguments extracted",
            "       from the request body.",
            "",
            "       .. versionadded:: 3.2",
            "",
            "    .. attribute:: files",
            "",
            "       File uploads are available in the files property, which maps file",
            "       names to lists of `.HTTPFile`.",
            "",
            "    .. attribute:: connection",
            "",
            "       An HTTP request is attached to a single HTTP connection, which can",
            "       be accessed through the \"connection\" attribute. Since connections",
            "       are typically kept open in HTTP/1.1, multiple requests can be handled",
            "       sequentially on a single connection.",
            "",
            "    .. versionchanged:: 4.0",
            "       Moved from ``tornado.httpserver.HTTPRequest``.",
            "    \"\"\"",
            "",
            "    path = None  # type: str",
            "    query = None  # type: str",
            "",
            "    # HACK: Used for stream_request_body",
            "    _body_future = None  # type: Future[None]",
            "",
            "    def __init__(",
            "        self,",
            "        method: Optional[str] = None,",
            "        uri: Optional[str] = None,",
            "        version: str = \"HTTP/1.0\",",
            "        headers: Optional[HTTPHeaders] = None,",
            "        body: Optional[bytes] = None,",
            "        host: Optional[str] = None,",
            "        files: Optional[Dict[str, List[\"HTTPFile\"]]] = None,",
            "        connection: Optional[\"HTTPConnection\"] = None,",
            "        start_line: Optional[\"RequestStartLine\"] = None,",
            "        server_connection: Optional[object] = None,",
            "    ) -> None:",
            "        if start_line is not None:",
            "            method, uri, version = start_line",
            "        self.method = method",
            "        self.uri = uri",
            "        self.version = version",
            "        self.headers = headers or HTTPHeaders()",
            "        self.body = body or b\"\"",
            "",
            "        # set remote IP and protocol",
            "        context = getattr(connection, \"context\", None)",
            "        self.remote_ip = getattr(context, \"remote_ip\", None)",
            "        self.protocol = getattr(context, \"protocol\", \"http\")",
            "",
            "        self.host = host or self.headers.get(\"Host\") or \"127.0.0.1\"",
            "        self.host_name = split_host_and_port(self.host.lower())[0]",
            "        self.files = files or {}",
            "        self.connection = connection",
            "        self.server_connection = server_connection",
            "        self._start_time = time.time()",
            "        self._finish_time = None",
            "",
            "        if uri is not None:",
            "            self.path, sep, self.query = uri.partition(\"?\")",
            "        self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)",
            "        self.query_arguments = copy.deepcopy(self.arguments)",
            "        self.body_arguments = {}  # type: Dict[str, List[bytes]]",
            "",
            "    @property",
            "    def cookies(self) -> Dict[str, http.cookies.Morsel]:",
            "        \"\"\"A dictionary of ``http.cookies.Morsel`` objects.\"\"\"",
            "        if not hasattr(self, \"_cookies\"):",
            "            self._cookies = (",
            "                http.cookies.SimpleCookie()",
            "            )  # type: http.cookies.SimpleCookie",
            "            if \"Cookie\" in self.headers:",
            "                try:",
            "                    parsed = parse_cookie(self.headers[\"Cookie\"])",
            "                except Exception:",
            "                    pass",
            "                else:",
            "                    for k, v in parsed.items():",
            "                        try:",
            "                            self._cookies[k] = v",
            "                        except Exception:",
            "                            # SimpleCookie imposes some restrictions on keys;",
            "                            # parse_cookie does not. Discard any cookies",
            "                            # with disallowed keys.",
            "                            pass",
            "        return self._cookies",
            "",
            "    def full_url(self) -> str:",
            "        \"\"\"Reconstructs the full URL for this request.\"\"\"",
            "        return self.protocol + \"://\" + self.host + self.uri  # type: ignore[operator]",
            "",
            "    def request_time(self) -> float:",
            "        \"\"\"Returns the amount of time it took for this request to execute.\"\"\"",
            "        if self._finish_time is None:",
            "            return time.time() - self._start_time",
            "        else:",
            "            return self._finish_time - self._start_time",
            "",
            "    def get_ssl_certificate(",
            "        self, binary_form: bool = False",
            "    ) -> Union[None, Dict, bytes]:",
            "        \"\"\"Returns the client's SSL certificate, if any.",
            "",
            "        To use client certificates, the HTTPServer's",
            "        `ssl.SSLContext.verify_mode` field must be set, e.g.::",
            "",
            "            ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)",
            "            ssl_ctx.load_cert_chain(\"foo.crt\", \"foo.key\")",
            "            ssl_ctx.load_verify_locations(\"cacerts.pem\")",
            "            ssl_ctx.verify_mode = ssl.CERT_REQUIRED",
            "            server = HTTPServer(app, ssl_options=ssl_ctx)",
            "",
            "        By default, the return value is a dictionary (or None, if no",
            "        client certificate is present).  If ``binary_form`` is true, a",
            "        DER-encoded form of the certificate is returned instead.  See",
            "        SSLSocket.getpeercert() in the standard library for more",
            "        details.",
            "        http://docs.python.org/library/ssl.html#sslsocket-objects",
            "        \"\"\"",
            "        try:",
            "            if self.connection is None:",
            "                return None",
            "            # TODO: add a method to HTTPConnection for this so it can work with HTTP/2",
            "            return self.connection.stream.socket.getpeercert(  # type: ignore",
            "                binary_form=binary_form",
            "            )",
            "        except SSLError:",
            "            return None",
            "",
            "    def _parse_body(self) -> None:",
            "        parse_body_arguments(",
            "            self.headers.get(\"Content-Type\", \"\"),",
            "            self.body,",
            "            self.body_arguments,",
            "            self.files,",
            "            self.headers,",
            "        )",
            "",
            "        for k, v in self.body_arguments.items():",
            "            self.arguments.setdefault(k, []).extend(v)",
            "",
            "    def __repr__(self) -> str:",
            "        attrs = (\"protocol\", \"host\", \"method\", \"uri\", \"version\", \"remote_ip\")",
            "        args = \", \".join([\"%s=%r\" % (n, getattr(self, n)) for n in attrs])",
            "        return \"%s(%s)\" % (self.__class__.__name__, args)",
            "",
            "",
            "class HTTPInputError(Exception):",
            "    \"\"\"Exception class for malformed HTTP requests or responses",
            "    from remote sources.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class HTTPOutputError(Exception):",
            "    \"\"\"Exception class for errors in HTTP output.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class HTTPServerConnectionDelegate(object):",
            "    \"\"\"Implement this interface to handle requests from `.HTTPServer`.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    def start_request(",
            "        self, server_conn: object, request_conn: \"HTTPConnection\"",
            "    ) -> \"HTTPMessageDelegate\":",
            "        \"\"\"This method is called by the server when a new request has started.",
            "",
            "        :arg server_conn: is an opaque object representing the long-lived",
            "            (e.g. tcp-level) connection.",
            "        :arg request_conn: is a `.HTTPConnection` object for a single",
            "            request/response exchange.",
            "",
            "        This method should return a `.HTTPMessageDelegate`.",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def on_close(self, server_conn: object) -> None:",
            "        \"\"\"This method is called when a connection has been closed.",
            "",
            "        :arg server_conn: is a server connection that has previously been",
            "            passed to ``start_request``.",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "class HTTPMessageDelegate(object):",
            "    \"\"\"Implement this interface to handle an HTTP request or response.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    # TODO: genericize this class to avoid exposing the Union.",
            "    def headers_received(",
            "        self,",
            "        start_line: Union[\"RequestStartLine\", \"ResponseStartLine\"],",
            "        headers: HTTPHeaders,",
            "    ) -> Optional[Awaitable[None]]:",
            "        \"\"\"Called when the HTTP headers have been received and parsed.",
            "",
            "        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`",
            "            depending on whether this is a client or server message.",
            "        :arg headers: a `.HTTPHeaders` instance.",
            "",
            "        Some `.HTTPConnection` methods can only be called during",
            "        ``headers_received``.",
            "",
            "        May return a `.Future`; if it does the body will not be read",
            "        until it is done.",
            "        \"\"\"",
            "        pass",
            "",
            "    def data_received(self, chunk: bytes) -> Optional[Awaitable[None]]:",
            "        \"\"\"Called when a chunk of data has been received.",
            "",
            "        May return a `.Future` for flow control.",
            "        \"\"\"",
            "        pass",
            "",
            "    def finish(self) -> None:",
            "        \"\"\"Called after the last chunk of data has been received.\"\"\"",
            "        pass",
            "",
            "    def on_connection_close(self) -> None:",
            "        \"\"\"Called if the connection is closed without finishing the request.",
            "",
            "        If ``headers_received`` is called, either ``finish`` or",
            "        ``on_connection_close`` will be called, but not both.",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "class HTTPConnection(object):",
            "    \"\"\"Applications use this interface to write their responses.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    def write_headers(",
            "        self,",
            "        start_line: Union[\"RequestStartLine\", \"ResponseStartLine\"],",
            "        headers: HTTPHeaders,",
            "        chunk: Optional[bytes] = None,",
            "    ) -> \"Future[None]\":",
            "        \"\"\"Write an HTTP header block.",
            "",
            "        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`.",
            "        :arg headers: a `.HTTPHeaders` instance.",
            "        :arg chunk: the first (optional) chunk of data.  This is an optimization",
            "            so that small responses can be written in the same call as their",
            "            headers.",
            "",
            "        The ``version`` field of ``start_line`` is ignored.",
            "",
            "        Returns a future for flow control.",
            "",
            "        .. versionchanged:: 6.0",
            "",
            "           The ``callback`` argument was removed.",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def write(self, chunk: bytes) -> \"Future[None]\":",
            "        \"\"\"Writes a chunk of body data.",
            "",
            "        Returns a future for flow control.",
            "",
            "        .. versionchanged:: 6.0",
            "",
            "           The ``callback`` argument was removed.",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def finish(self) -> None:",
            "        \"\"\"Indicates that the last body data has been written.\"\"\"",
            "        raise NotImplementedError()",
            "",
            "",
            "def url_concat(",
            "    url: str,",
            "    args: Union[",
            "        None, Dict[str, str], List[Tuple[str, str]], Tuple[Tuple[str, str], ...]",
            "    ],",
            ") -> str:",
            "    \"\"\"Concatenate url and arguments regardless of whether",
            "    url has existing query parameters.",
            "",
            "    ``args`` may be either a dictionary or a list of key-value pairs",
            "    (the latter allows for multiple values with the same key.",
            "",
            "    >>> url_concat(\"http://example.com/foo\", dict(c=\"d\"))",
            "    'http://example.com/foo?c=d'",
            "    >>> url_concat(\"http://example.com/foo?a=b\", dict(c=\"d\"))",
            "    'http://example.com/foo?a=b&c=d'",
            "    >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])",
            "    'http://example.com/foo?a=b&c=d&c=d2'",
            "    \"\"\"",
            "    if args is None:",
            "        return url",
            "    parsed_url = urlparse(url)",
            "    if isinstance(args, dict):",
            "        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)",
            "        parsed_query.extend(args.items())",
            "    elif isinstance(args, list) or isinstance(args, tuple):",
            "        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)",
            "        parsed_query.extend(args)",
            "    else:",
            "        err = \"'args' parameter should be dict, list or tuple. Not {0}\".format(",
            "            type(args)",
            "        )",
            "        raise TypeError(err)",
            "    final_query = urlencode(parsed_query)",
            "    url = urlunparse(",
            "        (",
            "            parsed_url[0],",
            "            parsed_url[1],",
            "            parsed_url[2],",
            "            parsed_url[3],",
            "            final_query,",
            "            parsed_url[5],",
            "        )",
            "    )",
            "    return url",
            "",
            "",
            "class HTTPFile(ObjectDict):",
            "    \"\"\"Represents a file uploaded via a form.",
            "",
            "    For backwards compatibility, its instance attributes are also",
            "    accessible as dictionary keys.",
            "",
            "    * ``filename``",
            "    * ``body``",
            "    * ``content_type``",
            "    \"\"\"",
            "",
            "    filename: str",
            "    body: bytes",
            "    content_type: str",
            "",
            "",
            "def _parse_request_range(",
            "    range_header: str,",
            ") -> Optional[Tuple[Optional[int], Optional[int]]]:",
            "    \"\"\"Parses a Range header.",
            "",
            "    Returns either ``None`` or tuple ``(start, end)``.",
            "    Note that while the HTTP headers use inclusive byte positions,",
            "    this method returns indexes suitable for use in slices.",
            "",
            "    >>> start, end = _parse_request_range(\"bytes=1-2\")",
            "    >>> start, end",
            "    (1, 3)",
            "    >>> [0, 1, 2, 3, 4][start:end]",
            "    [1, 2]",
            "    >>> _parse_request_range(\"bytes=6-\")",
            "    (6, None)",
            "    >>> _parse_request_range(\"bytes=-6\")",
            "    (-6, None)",
            "    >>> _parse_request_range(\"bytes=-0\")",
            "    (None, 0)",
            "    >>> _parse_request_range(\"bytes=\")",
            "    (None, None)",
            "    >>> _parse_request_range(\"foo=42\")",
            "    >>> _parse_request_range(\"bytes=1-2,6-10\")",
            "",
            "    Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).",
            "",
            "    See [0] for the details of the range header.",
            "",
            "    [0]: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html#byte.ranges",
            "    \"\"\"",
            "    unit, _, value = range_header.partition(\"=\")",
            "    unit, value = unit.strip(), value.strip()",
            "    if unit != \"bytes\":",
            "        return None",
            "    start_b, _, end_b = value.partition(\"-\")",
            "    try:",
            "        start = _int_or_none(start_b)",
            "        end = _int_or_none(end_b)",
            "    except ValueError:",
            "        return None",
            "    if end is not None:",
            "        if start is None:",
            "            if end != 0:",
            "                start = -end",
            "                end = None",
            "        else:",
            "            end += 1",
            "    return (start, end)",
            "",
            "",
            "def _get_content_range(start: Optional[int], end: Optional[int], total: int) -> str:",
            "    \"\"\"Returns a suitable Content-Range header:",
            "",
            "    >>> print(_get_content_range(None, 1, 4))",
            "    bytes 0-0/4",
            "    >>> print(_get_content_range(1, 3, 4))",
            "    bytes 1-2/4",
            "    >>> print(_get_content_range(None, None, 4))",
            "    bytes 0-3/4",
            "    \"\"\"",
            "    start = start or 0",
            "    end = (end or total) - 1",
            "    return \"bytes %s-%s/%s\" % (start, end, total)",
            "",
            "",
            "def _int_or_none(val: str) -> Optional[int]:",
            "    val = val.strip()",
            "    if val == \"\":",
            "        return None",
            "    return int(val)",
            "",
            "",
            "def parse_body_arguments(",
            "    content_type: str,",
            "    body: bytes,",
            "    arguments: Dict[str, List[bytes]],",
            "    files: Dict[str, List[HTTPFile]],",
            "    headers: Optional[HTTPHeaders] = None,",
            ") -> None:",
            "    \"\"\"Parses a form request body.",
            "",
            "    Supports ``application/x-www-form-urlencoded`` and",
            "    ``multipart/form-data``.  The ``content_type`` parameter should be",
            "    a string and ``body`` should be a byte string.  The ``arguments``",
            "    and ``files`` parameters are dictionaries that will be updated",
            "    with the parsed contents.",
            "    \"\"\"",
            "    if content_type.startswith(\"application/x-www-form-urlencoded\"):",
            "        if headers and \"Content-Encoding\" in headers:",
            "            gen_log.warning(",
            "                \"Unsupported Content-Encoding: %s\", headers[\"Content-Encoding\"]",
            "            )",
            "            return",
            "        try:",
            "            # real charset decoding will happen in RequestHandler.decode_argument()",
            "            uri_arguments = parse_qs_bytes(body, keep_blank_values=True)",
            "        except Exception as e:",
            "            gen_log.warning(\"Invalid x-www-form-urlencoded body: %s\", e)",
            "            uri_arguments = {}",
            "        for name, values in uri_arguments.items():",
            "            if values:",
            "                arguments.setdefault(name, []).extend(values)",
            "    elif content_type.startswith(\"multipart/form-data\"):",
            "        if headers and \"Content-Encoding\" in headers:",
            "            gen_log.warning(",
            "                \"Unsupported Content-Encoding: %s\", headers[\"Content-Encoding\"]",
            "            )",
            "            return",
            "        try:",
            "            fields = content_type.split(\";\")",
            "            for field in fields:",
            "                k, sep, v = field.strip().partition(\"=\")",
            "                if k == \"boundary\" and v:",
            "                    parse_multipart_form_data(utf8(v), body, arguments, files)",
            "                    break",
            "            else:",
            "                raise ValueError(\"multipart boundary not found\")",
            "        except Exception as e:",
            "            gen_log.warning(\"Invalid multipart/form-data: %s\", e)",
            "",
            "",
            "def parse_multipart_form_data(",
            "    boundary: bytes,",
            "    data: bytes,",
            "    arguments: Dict[str, List[bytes]],",
            "    files: Dict[str, List[HTTPFile]],",
            ") -> None:",
            "    \"\"\"Parses a ``multipart/form-data`` body.",
            "",
            "    The ``boundary`` and ``data`` parameters are both byte strings.",
            "    The dictionaries given in the arguments and files parameters",
            "    will be updated with the contents of the body.",
            "",
            "    .. versionchanged:: 5.1",
            "",
            "       Now recognizes non-ASCII filenames in RFC 2231/5987",
            "       (``filename*=``) format.",
            "    \"\"\"",
            "    # The standard allows for the boundary to be quoted in the header,",
            "    # although it's rare (it happens at least for google app engine",
            "    # xmpp).  I think we're also supposed to handle backslash-escapes",
            "    # here but I'll save that until we see a client that uses them",
            "    # in the wild.",
            "    if boundary.startswith(b'\"') and boundary.endswith(b'\"'):",
            "        boundary = boundary[1:-1]",
            "    final_boundary_index = data.rfind(b\"--\" + boundary + b\"--\")",
            "    if final_boundary_index == -1:",
            "        gen_log.warning(\"Invalid multipart/form-data: no final boundary\")",
            "        return",
            "    parts = data[:final_boundary_index].split(b\"--\" + boundary + b\"\\r\\n\")",
            "    for part in parts:",
            "        if not part:",
            "            continue",
            "        eoh = part.find(b\"\\r\\n\\r\\n\")",
            "        if eoh == -1:",
            "            gen_log.warning(\"multipart/form-data missing headers\")",
            "            continue",
            "        headers = HTTPHeaders.parse(part[:eoh].decode(\"utf-8\"))",
            "        disp_header = headers.get(\"Content-Disposition\", \"\")",
            "        disposition, disp_params = _parse_header(disp_header)",
            "        if disposition != \"form-data\" or not part.endswith(b\"\\r\\n\"):",
            "            gen_log.warning(\"Invalid multipart/form-data\")",
            "            continue",
            "        value = part[eoh + 4 : -2]",
            "        if not disp_params.get(\"name\"):",
            "            gen_log.warning(\"multipart/form-data value missing name\")",
            "            continue",
            "        name = disp_params[\"name\"]",
            "        if disp_params.get(\"filename\"):",
            "            ctype = headers.get(\"Content-Type\", \"application/unknown\")",
            "            files.setdefault(name, []).append(",
            "                HTTPFile(",
            "                    filename=disp_params[\"filename\"], body=value, content_type=ctype",
            "                )",
            "            )",
            "        else:",
            "            arguments.setdefault(name, []).append(value)",
            "",
            "",
            "def format_timestamp(",
            "    ts: Union[int, float, tuple, time.struct_time, datetime.datetime]",
            ") -> str:",
            "    \"\"\"Formats a timestamp in the format used by HTTP.",
            "",
            "    The argument may be a numeric timestamp as returned by `time.time`,",
            "    a time tuple as returned by `time.gmtime`, or a `datetime.datetime`",
            "    object. Naive `datetime.datetime` objects are assumed to represent",
            "    UTC; aware objects are converted to UTC before formatting.",
            "",
            "    >>> format_timestamp(1359312200)",
            "    'Sun, 27 Jan 2013 18:43:20 GMT'",
            "    \"\"\"",
            "    if isinstance(ts, (int, float)):",
            "        time_num = ts",
            "    elif isinstance(ts, (tuple, time.struct_time)):",
            "        time_num = calendar.timegm(ts)",
            "    elif isinstance(ts, datetime.datetime):",
            "        time_num = calendar.timegm(ts.utctimetuple())",
            "    else:",
            "        raise TypeError(\"unknown timestamp type: %r\" % ts)",
            "    return email.utils.formatdate(time_num, usegmt=True)",
            "",
            "",
            "RequestStartLine = collections.namedtuple(",
            "    \"RequestStartLine\", [\"method\", \"path\", \"version\"]",
            ")",
            "",
            "",
            "_http_version_re = re.compile(r\"^HTTP/1\\.[0-9]$\")",
            "",
            "",
            "def parse_request_start_line(line: str) -> RequestStartLine:",
            "    \"\"\"Returns a (method, path, version) tuple for an HTTP 1.x request line.",
            "",
            "    The response is a `collections.namedtuple`.",
            "",
            "    >>> parse_request_start_line(\"GET /foo HTTP/1.1\")",
            "    RequestStartLine(method='GET', path='/foo', version='HTTP/1.1')",
            "    \"\"\"",
            "    try:",
            "        method, path, version = line.split(\" \")",
            "    except ValueError:",
            "        # https://tools.ietf.org/html/rfc7230#section-3.1.1",
            "        # invalid request-line SHOULD respond with a 400 (Bad Request)",
            "        raise HTTPInputError(\"Malformed HTTP request line\")",
            "    if not _http_version_re.match(version):",
            "        raise HTTPInputError(",
            "            \"Malformed HTTP version in HTTP Request-Line: %r\" % version",
            "        )",
            "    return RequestStartLine(method, path, version)",
            "",
            "",
            "ResponseStartLine = collections.namedtuple(",
            "    \"ResponseStartLine\", [\"version\", \"code\", \"reason\"]",
            ")",
            "",
            "",
            "_http_response_line_re = re.compile(r\"(HTTP/1.[0-9]) ([0-9]+) ([^\\r]*)\")",
            "",
            "",
            "def parse_response_start_line(line: str) -> ResponseStartLine:",
            "    \"\"\"Returns a (version, code, reason) tuple for an HTTP 1.x response line.",
            "",
            "    The response is a `collections.namedtuple`.",
            "",
            "    >>> parse_response_start_line(\"HTTP/1.1 200 OK\")",
            "    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')",
            "    \"\"\"",
            "    line = native_str(line)",
            "    match = _http_response_line_re.match(line)",
            "    if not match:",
            "        raise HTTPInputError(\"Error parsing response start line\")",
            "    return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))",
            "",
            "",
            "# _parseparam and _parse_header are copied and modified from python2.7's cgi.py",
            "# The original 2.7 version of this code did not correctly support some",
            "# combinations of semicolons and double quotes.",
            "# It has also been modified to support valueless parameters as seen in",
            "# websocket extension negotiations, and to support non-ascii values in",
            "# RFC 2231/5987 format.",
            "",
            "",
            "def _parseparam(s: str) -> Generator[str, None, None]:",
            "    while s[:1] == \";\":",
            "        s = s[1:]",
            "        end = s.find(\";\")",
            "        while end > 0 and (s.count('\"', 0, end) - s.count('\\\\\"', 0, end)) % 2:",
            "            end = s.find(\";\", end + 1)",
            "        if end < 0:",
            "            end = len(s)",
            "        f = s[:end]",
            "        yield f.strip()",
            "        s = s[end:]",
            "",
            "",
            "def _parse_header(line: str) -> Tuple[str, Dict[str, str]]:",
            "    r\"\"\"Parse a Content-type like header.",
            "",
            "    Return the main content-type and a dictionary of options.",
            "",
            "    >>> d = \"form-data; foo=\\\"b\\\\\\\\a\\\\\\\"r\\\"; file*=utf-8''T%C3%A4st\"",
            "    >>> ct, d = _parse_header(d)",
            "    >>> ct",
            "    'form-data'",
            "    >>> d['file'] == r'T\\u00e4st'.encode('ascii').decode('unicode_escape')",
            "    True",
            "    >>> d['foo']",
            "    'b\\\\a\"r'",
            "    \"\"\"",
            "    parts = _parseparam(\";\" + line)",
            "    key = next(parts)",
            "    # decode_params treats first argument special, but we already stripped key",
            "    params = [(\"Dummy\", \"value\")]",
            "    for p in parts:",
            "        i = p.find(\"=\")",
            "        if i >= 0:",
            "            name = p[:i].strip().lower()",
            "            value = p[i + 1 :].strip()",
            "            params.append((name, native_str(value)))",
            "    decoded_params = email.utils.decode_params(params)",
            "    decoded_params.pop(0)  # get rid of the dummy again",
            "    pdict = {}",
            "    for name, decoded_value in decoded_params:",
            "        value = email.utils.collapse_rfc2231_value(decoded_value)",
            "        if len(value) >= 2 and value[0] == '\"' and value[-1] == '\"':",
            "            value = value[1:-1]",
            "        pdict[name] = value",
            "    return key, pdict",
            "",
            "",
            "def _encode_header(key: str, pdict: Dict[str, str]) -> str:",
            "    \"\"\"Inverse of _parse_header.",
            "",
            "    >>> _encode_header('permessage-deflate',",
            "    ...     {'client_max_window_bits': 15, 'client_no_context_takeover': None})",
            "    'permessage-deflate; client_max_window_bits=15; client_no_context_takeover'",
            "    \"\"\"",
            "    if not pdict:",
            "        return key",
            "    out = [key]",
            "    # Sort the parameters just to make it easy to test.",
            "    for k, v in sorted(pdict.items()):",
            "        if v is None:",
            "            out.append(k)",
            "        else:",
            "            # TODO: quote if necessary.",
            "            out.append(\"%s=%s\" % (k, v))",
            "    return \"; \".join(out)",
            "",
            "",
            "def encode_username_password(",
            "    username: Union[str, bytes], password: Union[str, bytes]",
            ") -> bytes:",
            "    \"\"\"Encodes a username/password pair in the format used by HTTP auth.",
            "",
            "    The return value is a byte string in the form ``username:password``.",
            "",
            "    .. versionadded:: 5.1",
            "    \"\"\"",
            "    if isinstance(username, unicode_type):",
            "        username = unicodedata.normalize(\"NFC\", username)",
            "    if isinstance(password, unicode_type):",
            "        password = unicodedata.normalize(\"NFC\", password)",
            "    return utf8(username) + b\":\" + utf8(password)",
            "",
            "",
            "def doctests():",
            "    # type: () -> unittest.TestSuite",
            "    import doctest",
            "",
            "    return doctest.DocTestSuite()",
            "",
            "",
            "_netloc_re = re.compile(r\"^(.+):(\\d+)$\")",
            "",
            "",
            "def split_host_and_port(netloc: str) -> Tuple[str, Optional[int]]:",
            "    \"\"\"Returns ``(host, port)`` tuple from ``netloc``.",
            "",
            "    Returned ``port`` will be ``None`` if not present.",
            "",
            "    .. versionadded:: 4.1",
            "    \"\"\"",
            "    match = _netloc_re.match(netloc)",
            "    if match:",
            "        host = match.group(1)",
            "        port = int(match.group(2))  # type: Optional[int]",
            "    else:",
            "        host = netloc",
            "        port = None",
            "    return (host, port)",
            "",
            "",
            "def qs_to_qsl(qs: Dict[str, List[AnyStr]]) -> Iterable[Tuple[str, AnyStr]]:",
            "    \"\"\"Generator converting a result of ``parse_qs`` back to name-value pairs.",
            "",
            "    .. versionadded:: 5.0",
            "    \"\"\"",
            "    for k, vs in qs.items():",
            "        for v in vs:",
            "            yield (k, v)",
            "",
            "",
            "_OctalPatt = re.compile(r\"\\\\[0-3][0-7][0-7]\")",
            "_QuotePatt = re.compile(r\"[\\\\].\")",
            "_nulljoin = \"\".join",
            "",
            "",
            "def _unquote_cookie(s: str) -> str:",
            "    \"\"\"Handle double quotes and escaping in cookie values.",
            "",
            "    This method is copied verbatim from the Python 3.5 standard",
            "    library (http.cookies._unquote) so we don't have to depend on",
            "    non-public interfaces.",
            "    \"\"\"",
            "    # If there aren't any doublequotes,",
            "    # then there can't be any special characters.  See RFC 2109.",
            "    if s is None or len(s) < 2:",
            "        return s",
            "    if s[0] != '\"' or s[-1] != '\"':",
            "        return s",
            "",
            "    # We have to assume that we must decode this string.",
            "    # Down to work.",
            "",
            "    # Remove the \"s",
            "    s = s[1:-1]",
            "",
            "    # Check for special sequences.  Examples:",
            "    #    \\012 --> \\n",
            "    #    \\\"   --> \"",
            "    #",
            "    i = 0",
            "    n = len(s)",
            "    res = []",
            "    while 0 <= i < n:",
            "        o_match = _OctalPatt.search(s, i)",
            "        q_match = _QuotePatt.search(s, i)",
            "        if not o_match and not q_match:  # Neither matched",
            "            res.append(s[i:])",
            "            break",
            "        # else:",
            "        j = k = -1",
            "        if o_match:",
            "            j = o_match.start(0)",
            "        if q_match:",
            "            k = q_match.start(0)",
            "        if q_match and (not o_match or k < j):  # QuotePatt matched",
            "            res.append(s[i:k])",
            "            res.append(s[k + 1])",
            "            i = k + 2",
            "        else:  # OctalPatt matched",
            "            res.append(s[i:j])",
            "            res.append(chr(int(s[j + 1 : j + 4], 8)))",
            "            i = j + 4",
            "    return _nulljoin(res)",
            "",
            "",
            "def parse_cookie(cookie: str) -> Dict[str, str]:",
            "    \"\"\"Parse a ``Cookie`` HTTP header into a dict of name/value pairs.",
            "",
            "    This function attempts to mimic browser cookie parsing behavior;",
            "    it specifically does not follow any of the cookie-related RFCs",
            "    (because browsers don't either).",
            "",
            "    The algorithm used is identical to that used by Django version 1.9.10.",
            "",
            "    .. versionadded:: 4.4.2",
            "    \"\"\"",
            "    cookiedict = {}",
            "    for chunk in cookie.split(str(\";\")):",
            "        if str(\"=\") in chunk:",
            "            key, val = chunk.split(str(\"=\"), 1)",
            "        else:",
            "            # Assume an empty name per",
            "            # https://bugzilla.mozilla.org/show_bug.cgi?id=169091",
            "            key, val = str(\"\"), chunk",
            "        key, val = key.strip(), val.strip()",
            "        if key or val:",
            "            # unquote using Python's algorithm.",
            "            cookiedict[key] = _unquote_cookie(val)",
            "    return cookiedict"
        ],
        "afterPatchFile": [
            "#",
            "# Copyright 2009 Facebook",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "# not use this file except in compliance with the License. You may obtain",
            "# a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "# License for the specific language governing permissions and limitations",
            "# under the License.",
            "",
            "\"\"\"HTTP utility code shared by clients and servers.",
            "",
            "This module also defines the `HTTPServerRequest` class which is exposed",
            "via `tornado.web.RequestHandler.request`.",
            "\"\"\"",
            "",
            "import calendar",
            "import collections.abc",
            "import copy",
            "import datetime",
            "import email.utils",
            "from functools import lru_cache",
            "from http.client import responses",
            "import http.cookies",
            "import re",
            "from ssl import SSLError",
            "import time",
            "import unicodedata",
            "from urllib.parse import urlencode, urlparse, urlunparse, parse_qsl",
            "",
            "from tornado.escape import native_str, parse_qs_bytes, utf8",
            "from tornado.log import gen_log",
            "from tornado.util import ObjectDict, unicode_type",
            "",
            "",
            "# responses is unused in this file, but we re-export it to other files.",
            "# Reference it so pyflakes doesn't complain.",
            "responses",
            "",
            "import typing",
            "from typing import (",
            "    Tuple,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Iterator,",
            "    Dict,",
            "    Union,",
            "    Optional,",
            "    Awaitable,",
            "    Generator,",
            "    AnyStr,",
            ")",
            "",
            "if typing.TYPE_CHECKING:",
            "    from typing import Deque  # noqa: F401",
            "    from asyncio import Future  # noqa: F401",
            "    import unittest  # noqa: F401",
            "",
            "# To be used with str.strip() and related methods.",
            "HTTP_WHITESPACE = \" \\t\"",
            "",
            "",
            "@lru_cache(1000)",
            "def _normalize_header(name: str) -> str:",
            "    \"\"\"Map a header name to Http-Header-Case.",
            "",
            "    >>> _normalize_header(\"coNtent-TYPE\")",
            "    'Content-Type'",
            "    \"\"\"",
            "    return \"-\".join([w.capitalize() for w in name.split(\"-\")])",
            "",
            "",
            "class HTTPHeaders(collections.abc.MutableMapping):",
            "    \"\"\"A dictionary that maintains ``Http-Header-Case`` for all keys.",
            "",
            "    Supports multiple values per key via a pair of new methods,",
            "    `add()` and `get_list()`.  The regular dictionary interface",
            "    returns a single value per key, with multiple values joined by a",
            "    comma.",
            "",
            "    >>> h = HTTPHeaders({\"content-type\": \"text/html\"})",
            "    >>> list(h.keys())",
            "    ['Content-Type']",
            "    >>> h[\"Content-Type\"]",
            "    'text/html'",
            "",
            "    >>> h.add(\"Set-Cookie\", \"A=B\")",
            "    >>> h.add(\"Set-Cookie\", \"C=D\")",
            "    >>> h[\"set-cookie\"]",
            "    'A=B,C=D'",
            "    >>> h.get_list(\"set-cookie\")",
            "    ['A=B', 'C=D']",
            "",
            "    >>> for (k,v) in sorted(h.get_all()):",
            "    ...    print('%s: %s' % (k,v))",
            "    ...",
            "    Content-Type: text/html",
            "    Set-Cookie: A=B",
            "    Set-Cookie: C=D",
            "    \"\"\"",
            "",
            "    @typing.overload",
            "    def __init__(self, __arg: Mapping[str, List[str]]) -> None:",
            "        pass",
            "",
            "    @typing.overload  # noqa: F811",
            "    def __init__(self, __arg: Mapping[str, str]) -> None:",
            "        pass",
            "",
            "    @typing.overload  # noqa: F811",
            "    def __init__(self, *args: Tuple[str, str]) -> None:",
            "        pass",
            "",
            "    @typing.overload  # noqa: F811",
            "    def __init__(self, **kwargs: str) -> None:",
            "        pass",
            "",
            "    def __init__(self, *args: typing.Any, **kwargs: str) -> None:  # noqa: F811",
            "        self._dict = {}  # type: typing.Dict[str, str]",
            "        self._as_list = {}  # type: typing.Dict[str, typing.List[str]]",
            "        self._last_key = None  # type: Optional[str]",
            "        if len(args) == 1 and len(kwargs) == 0 and isinstance(args[0], HTTPHeaders):",
            "            # Copy constructor",
            "            for k, v in args[0].get_all():",
            "                self.add(k, v)",
            "        else:",
            "            # Dict-style initialization",
            "            self.update(*args, **kwargs)",
            "",
            "    # new public methods",
            "",
            "    def add(self, name: str, value: str) -> None:",
            "        \"\"\"Adds a new value for the given key.\"\"\"",
            "        norm_name = _normalize_header(name)",
            "        self._last_key = norm_name",
            "        if norm_name in self:",
            "            self._dict[norm_name] = (",
            "                native_str(self[norm_name]) + \",\" + native_str(value)",
            "            )",
            "            self._as_list[norm_name].append(value)",
            "        else:",
            "            self[norm_name] = value",
            "",
            "    def get_list(self, name: str) -> List[str]:",
            "        \"\"\"Returns all values for the given header as a list.\"\"\"",
            "        norm_name = _normalize_header(name)",
            "        return self._as_list.get(norm_name, [])",
            "",
            "    def get_all(self) -> Iterable[Tuple[str, str]]:",
            "        \"\"\"Returns an iterable of all (name, value) pairs.",
            "",
            "        If a header has multiple values, multiple pairs will be",
            "        returned with the same name.",
            "        \"\"\"",
            "        for name, values in self._as_list.items():",
            "            for value in values:",
            "                yield (name, value)",
            "",
            "    def parse_line(self, line: str) -> None:",
            "        \"\"\"Updates the dictionary with a single header line.",
            "",
            "        >>> h = HTTPHeaders()",
            "        >>> h.parse_line(\"Content-Type: text/html\")",
            "        >>> h.get('content-type')",
            "        'text/html'",
            "        \"\"\"",
            "        if line[0].isspace():",
            "            # continuation of a multi-line header",
            "            if self._last_key is None:",
            "                raise HTTPInputError(\"first header line cannot start with whitespace\")",
            "            new_part = \" \" + line.lstrip(HTTP_WHITESPACE)",
            "            self._as_list[self._last_key][-1] += new_part",
            "            self._dict[self._last_key] += new_part",
            "        else:",
            "            try:",
            "                name, value = line.split(\":\", 1)",
            "            except ValueError:",
            "                raise HTTPInputError(\"no colon in header line\")",
            "            self.add(name, value.strip(HTTP_WHITESPACE))",
            "",
            "    @classmethod",
            "    def parse(cls, headers: str) -> \"HTTPHeaders\":",
            "        \"\"\"Returns a dictionary from HTTP header text.",
            "",
            "        >>> h = HTTPHeaders.parse(\"Content-Type: text/html\\\\r\\\\nContent-Length: 42\\\\r\\\\n\")",
            "        >>> sorted(h.items())",
            "        [('Content-Length', '42'), ('Content-Type', 'text/html')]",
            "",
            "        .. versionchanged:: 5.1",
            "",
            "           Raises `HTTPInputError` on malformed headers instead of a",
            "           mix of `KeyError`, and `ValueError`.",
            "",
            "        \"\"\"",
            "        h = cls()",
            "        # RFC 7230 section 3.5: a recipient MAY recognize a single LF as a line",
            "        # terminator and ignore any preceding CR.",
            "        for line in headers.split(\"\\n\"):",
            "            if line.endswith(\"\\r\"):",
            "                line = line[:-1]",
            "            if line:",
            "                h.parse_line(line)",
            "        return h",
            "",
            "    # MutableMapping abstract method implementations.",
            "",
            "    def __setitem__(self, name: str, value: str) -> None:",
            "        norm_name = _normalize_header(name)",
            "        self._dict[norm_name] = value",
            "        self._as_list[norm_name] = [value]",
            "",
            "    def __getitem__(self, name: str) -> str:",
            "        return self._dict[_normalize_header(name)]",
            "",
            "    def __delitem__(self, name: str) -> None:",
            "        norm_name = _normalize_header(name)",
            "        del self._dict[norm_name]",
            "        del self._as_list[norm_name]",
            "",
            "    def __len__(self) -> int:",
            "        return len(self._dict)",
            "",
            "    def __iter__(self) -> Iterator[typing.Any]:",
            "        return iter(self._dict)",
            "",
            "    def copy(self) -> \"HTTPHeaders\":",
            "        # defined in dict but not in MutableMapping.",
            "        return HTTPHeaders(self)",
            "",
            "    # Use our overridden copy method for the copy.copy module.",
            "    # This makes shallow copies one level deeper, but preserves",
            "    # the appearance that HTTPHeaders is a single container.",
            "    __copy__ = copy",
            "",
            "    def __str__(self) -> str:",
            "        lines = []",
            "        for name, value in self.get_all():",
            "            lines.append(\"%s: %s\\n\" % (name, value))",
            "        return \"\".join(lines)",
            "",
            "    __unicode__ = __str__",
            "",
            "",
            "class HTTPServerRequest(object):",
            "    \"\"\"A single HTTP request.",
            "",
            "    All attributes are type `str` unless otherwise noted.",
            "",
            "    .. attribute:: method",
            "",
            "       HTTP request method, e.g. \"GET\" or \"POST\"",
            "",
            "    .. attribute:: uri",
            "",
            "       The requested uri.",
            "",
            "    .. attribute:: path",
            "",
            "       The path portion of `uri`",
            "",
            "    .. attribute:: query",
            "",
            "       The query portion of `uri`",
            "",
            "    .. attribute:: version",
            "",
            "       HTTP version specified in request, e.g. \"HTTP/1.1\"",
            "",
            "    .. attribute:: headers",
            "",
            "       `.HTTPHeaders` dictionary-like object for request headers.  Acts like",
            "       a case-insensitive dictionary with additional methods for repeated",
            "       headers.",
            "",
            "    .. attribute:: body",
            "",
            "       Request body, if present, as a byte string.",
            "",
            "    .. attribute:: remote_ip",
            "",
            "       Client's IP address as a string.  If ``HTTPServer.xheaders`` is set,",
            "       will pass along the real IP address provided by a load balancer",
            "       in the ``X-Real-Ip`` or ``X-Forwarded-For`` header.",
            "",
            "    .. versionchanged:: 3.1",
            "       The list format of ``X-Forwarded-For`` is now supported.",
            "",
            "    .. attribute:: protocol",
            "",
            "       The protocol used, either \"http\" or \"https\".  If ``HTTPServer.xheaders``",
            "       is set, will pass along the protocol used by a load balancer if",
            "       reported via an ``X-Scheme`` header.",
            "",
            "    .. attribute:: host",
            "",
            "       The requested hostname, usually taken from the ``Host`` header.",
            "",
            "    .. attribute:: arguments",
            "",
            "       GET/POST arguments are available in the arguments property, which",
            "       maps arguments names to lists of values (to support multiple values",
            "       for individual names). Names are of type `str`, while arguments",
            "       are byte strings.  Note that this is different from",
            "       `.RequestHandler.get_argument`, which returns argument values as",
            "       unicode strings.",
            "",
            "    .. attribute:: query_arguments",
            "",
            "       Same format as ``arguments``, but contains only arguments extracted",
            "       from the query string.",
            "",
            "       .. versionadded:: 3.2",
            "",
            "    .. attribute:: body_arguments",
            "",
            "       Same format as ``arguments``, but contains only arguments extracted",
            "       from the request body.",
            "",
            "       .. versionadded:: 3.2",
            "",
            "    .. attribute:: files",
            "",
            "       File uploads are available in the files property, which maps file",
            "       names to lists of `.HTTPFile`.",
            "",
            "    .. attribute:: connection",
            "",
            "       An HTTP request is attached to a single HTTP connection, which can",
            "       be accessed through the \"connection\" attribute. Since connections",
            "       are typically kept open in HTTP/1.1, multiple requests can be handled",
            "       sequentially on a single connection.",
            "",
            "    .. versionchanged:: 4.0",
            "       Moved from ``tornado.httpserver.HTTPRequest``.",
            "    \"\"\"",
            "",
            "    path = None  # type: str",
            "    query = None  # type: str",
            "",
            "    # HACK: Used for stream_request_body",
            "    _body_future = None  # type: Future[None]",
            "",
            "    def __init__(",
            "        self,",
            "        method: Optional[str] = None,",
            "        uri: Optional[str] = None,",
            "        version: str = \"HTTP/1.0\",",
            "        headers: Optional[HTTPHeaders] = None,",
            "        body: Optional[bytes] = None,",
            "        host: Optional[str] = None,",
            "        files: Optional[Dict[str, List[\"HTTPFile\"]]] = None,",
            "        connection: Optional[\"HTTPConnection\"] = None,",
            "        start_line: Optional[\"RequestStartLine\"] = None,",
            "        server_connection: Optional[object] = None,",
            "    ) -> None:",
            "        if start_line is not None:",
            "            method, uri, version = start_line",
            "        self.method = method",
            "        self.uri = uri",
            "        self.version = version",
            "        self.headers = headers or HTTPHeaders()",
            "        self.body = body or b\"\"",
            "",
            "        # set remote IP and protocol",
            "        context = getattr(connection, \"context\", None)",
            "        self.remote_ip = getattr(context, \"remote_ip\", None)",
            "        self.protocol = getattr(context, \"protocol\", \"http\")",
            "",
            "        self.host = host or self.headers.get(\"Host\") or \"127.0.0.1\"",
            "        self.host_name = split_host_and_port(self.host.lower())[0]",
            "        self.files = files or {}",
            "        self.connection = connection",
            "        self.server_connection = server_connection",
            "        self._start_time = time.time()",
            "        self._finish_time = None",
            "",
            "        if uri is not None:",
            "            self.path, sep, self.query = uri.partition(\"?\")",
            "        self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)",
            "        self.query_arguments = copy.deepcopy(self.arguments)",
            "        self.body_arguments = {}  # type: Dict[str, List[bytes]]",
            "",
            "    @property",
            "    def cookies(self) -> Dict[str, http.cookies.Morsel]:",
            "        \"\"\"A dictionary of ``http.cookies.Morsel`` objects.\"\"\"",
            "        if not hasattr(self, \"_cookies\"):",
            "            self._cookies = (",
            "                http.cookies.SimpleCookie()",
            "            )  # type: http.cookies.SimpleCookie",
            "            if \"Cookie\" in self.headers:",
            "                try:",
            "                    parsed = parse_cookie(self.headers[\"Cookie\"])",
            "                except Exception:",
            "                    pass",
            "                else:",
            "                    for k, v in parsed.items():",
            "                        try:",
            "                            self._cookies[k] = v",
            "                        except Exception:",
            "                            # SimpleCookie imposes some restrictions on keys;",
            "                            # parse_cookie does not. Discard any cookies",
            "                            # with disallowed keys.",
            "                            pass",
            "        return self._cookies",
            "",
            "    def full_url(self) -> str:",
            "        \"\"\"Reconstructs the full URL for this request.\"\"\"",
            "        return self.protocol + \"://\" + self.host + self.uri  # type: ignore[operator]",
            "",
            "    def request_time(self) -> float:",
            "        \"\"\"Returns the amount of time it took for this request to execute.\"\"\"",
            "        if self._finish_time is None:",
            "            return time.time() - self._start_time",
            "        else:",
            "            return self._finish_time - self._start_time",
            "",
            "    def get_ssl_certificate(",
            "        self, binary_form: bool = False",
            "    ) -> Union[None, Dict, bytes]:",
            "        \"\"\"Returns the client's SSL certificate, if any.",
            "",
            "        To use client certificates, the HTTPServer's",
            "        `ssl.SSLContext.verify_mode` field must be set, e.g.::",
            "",
            "            ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)",
            "            ssl_ctx.load_cert_chain(\"foo.crt\", \"foo.key\")",
            "            ssl_ctx.load_verify_locations(\"cacerts.pem\")",
            "            ssl_ctx.verify_mode = ssl.CERT_REQUIRED",
            "            server = HTTPServer(app, ssl_options=ssl_ctx)",
            "",
            "        By default, the return value is a dictionary (or None, if no",
            "        client certificate is present).  If ``binary_form`` is true, a",
            "        DER-encoded form of the certificate is returned instead.  See",
            "        SSLSocket.getpeercert() in the standard library for more",
            "        details.",
            "        http://docs.python.org/library/ssl.html#sslsocket-objects",
            "        \"\"\"",
            "        try:",
            "            if self.connection is None:",
            "                return None",
            "            # TODO: add a method to HTTPConnection for this so it can work with HTTP/2",
            "            return self.connection.stream.socket.getpeercert(  # type: ignore",
            "                binary_form=binary_form",
            "            )",
            "        except SSLError:",
            "            return None",
            "",
            "    def _parse_body(self) -> None:",
            "        parse_body_arguments(",
            "            self.headers.get(\"Content-Type\", \"\"),",
            "            self.body,",
            "            self.body_arguments,",
            "            self.files,",
            "            self.headers,",
            "        )",
            "",
            "        for k, v in self.body_arguments.items():",
            "            self.arguments.setdefault(k, []).extend(v)",
            "",
            "    def __repr__(self) -> str:",
            "        attrs = (\"protocol\", \"host\", \"method\", \"uri\", \"version\", \"remote_ip\")",
            "        args = \", \".join([\"%s=%r\" % (n, getattr(self, n)) for n in attrs])",
            "        return \"%s(%s)\" % (self.__class__.__name__, args)",
            "",
            "",
            "class HTTPInputError(Exception):",
            "    \"\"\"Exception class for malformed HTTP requests or responses",
            "    from remote sources.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class HTTPOutputError(Exception):",
            "    \"\"\"Exception class for errors in HTTP output.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    pass",
            "",
            "",
            "class HTTPServerConnectionDelegate(object):",
            "    \"\"\"Implement this interface to handle requests from `.HTTPServer`.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    def start_request(",
            "        self, server_conn: object, request_conn: \"HTTPConnection\"",
            "    ) -> \"HTTPMessageDelegate\":",
            "        \"\"\"This method is called by the server when a new request has started.",
            "",
            "        :arg server_conn: is an opaque object representing the long-lived",
            "            (e.g. tcp-level) connection.",
            "        :arg request_conn: is a `.HTTPConnection` object for a single",
            "            request/response exchange.",
            "",
            "        This method should return a `.HTTPMessageDelegate`.",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def on_close(self, server_conn: object) -> None:",
            "        \"\"\"This method is called when a connection has been closed.",
            "",
            "        :arg server_conn: is a server connection that has previously been",
            "            passed to ``start_request``.",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "class HTTPMessageDelegate(object):",
            "    \"\"\"Implement this interface to handle an HTTP request or response.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    # TODO: genericize this class to avoid exposing the Union.",
            "    def headers_received(",
            "        self,",
            "        start_line: Union[\"RequestStartLine\", \"ResponseStartLine\"],",
            "        headers: HTTPHeaders,",
            "    ) -> Optional[Awaitable[None]]:",
            "        \"\"\"Called when the HTTP headers have been received and parsed.",
            "",
            "        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`",
            "            depending on whether this is a client or server message.",
            "        :arg headers: a `.HTTPHeaders` instance.",
            "",
            "        Some `.HTTPConnection` methods can only be called during",
            "        ``headers_received``.",
            "",
            "        May return a `.Future`; if it does the body will not be read",
            "        until it is done.",
            "        \"\"\"",
            "        pass",
            "",
            "    def data_received(self, chunk: bytes) -> Optional[Awaitable[None]]:",
            "        \"\"\"Called when a chunk of data has been received.",
            "",
            "        May return a `.Future` for flow control.",
            "        \"\"\"",
            "        pass",
            "",
            "    def finish(self) -> None:",
            "        \"\"\"Called after the last chunk of data has been received.\"\"\"",
            "        pass",
            "",
            "    def on_connection_close(self) -> None:",
            "        \"\"\"Called if the connection is closed without finishing the request.",
            "",
            "        If ``headers_received`` is called, either ``finish`` or",
            "        ``on_connection_close`` will be called, but not both.",
            "        \"\"\"",
            "        pass",
            "",
            "",
            "class HTTPConnection(object):",
            "    \"\"\"Applications use this interface to write their responses.",
            "",
            "    .. versionadded:: 4.0",
            "    \"\"\"",
            "",
            "    def write_headers(",
            "        self,",
            "        start_line: Union[\"RequestStartLine\", \"ResponseStartLine\"],",
            "        headers: HTTPHeaders,",
            "        chunk: Optional[bytes] = None,",
            "    ) -> \"Future[None]\":",
            "        \"\"\"Write an HTTP header block.",
            "",
            "        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`.",
            "        :arg headers: a `.HTTPHeaders` instance.",
            "        :arg chunk: the first (optional) chunk of data.  This is an optimization",
            "            so that small responses can be written in the same call as their",
            "            headers.",
            "",
            "        The ``version`` field of ``start_line`` is ignored.",
            "",
            "        Returns a future for flow control.",
            "",
            "        .. versionchanged:: 6.0",
            "",
            "           The ``callback`` argument was removed.",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def write(self, chunk: bytes) -> \"Future[None]\":",
            "        \"\"\"Writes a chunk of body data.",
            "",
            "        Returns a future for flow control.",
            "",
            "        .. versionchanged:: 6.0",
            "",
            "           The ``callback`` argument was removed.",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def finish(self) -> None:",
            "        \"\"\"Indicates that the last body data has been written.\"\"\"",
            "        raise NotImplementedError()",
            "",
            "",
            "def url_concat(",
            "    url: str,",
            "    args: Union[",
            "        None, Dict[str, str], List[Tuple[str, str]], Tuple[Tuple[str, str], ...]",
            "    ],",
            ") -> str:",
            "    \"\"\"Concatenate url and arguments regardless of whether",
            "    url has existing query parameters.",
            "",
            "    ``args`` may be either a dictionary or a list of key-value pairs",
            "    (the latter allows for multiple values with the same key.",
            "",
            "    >>> url_concat(\"http://example.com/foo\", dict(c=\"d\"))",
            "    'http://example.com/foo?c=d'",
            "    >>> url_concat(\"http://example.com/foo?a=b\", dict(c=\"d\"))",
            "    'http://example.com/foo?a=b&c=d'",
            "    >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])",
            "    'http://example.com/foo?a=b&c=d&c=d2'",
            "    \"\"\"",
            "    if args is None:",
            "        return url",
            "    parsed_url = urlparse(url)",
            "    if isinstance(args, dict):",
            "        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)",
            "        parsed_query.extend(args.items())",
            "    elif isinstance(args, list) or isinstance(args, tuple):",
            "        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)",
            "        parsed_query.extend(args)",
            "    else:",
            "        err = \"'args' parameter should be dict, list or tuple. Not {0}\".format(",
            "            type(args)",
            "        )",
            "        raise TypeError(err)",
            "    final_query = urlencode(parsed_query)",
            "    url = urlunparse(",
            "        (",
            "            parsed_url[0],",
            "            parsed_url[1],",
            "            parsed_url[2],",
            "            parsed_url[3],",
            "            final_query,",
            "            parsed_url[5],",
            "        )",
            "    )",
            "    return url",
            "",
            "",
            "class HTTPFile(ObjectDict):",
            "    \"\"\"Represents a file uploaded via a form.",
            "",
            "    For backwards compatibility, its instance attributes are also",
            "    accessible as dictionary keys.",
            "",
            "    * ``filename``",
            "    * ``body``",
            "    * ``content_type``",
            "    \"\"\"",
            "",
            "    filename: str",
            "    body: bytes",
            "    content_type: str",
            "",
            "",
            "def _parse_request_range(",
            "    range_header: str,",
            ") -> Optional[Tuple[Optional[int], Optional[int]]]:",
            "    \"\"\"Parses a Range header.",
            "",
            "    Returns either ``None`` or tuple ``(start, end)``.",
            "    Note that while the HTTP headers use inclusive byte positions,",
            "    this method returns indexes suitable for use in slices.",
            "",
            "    >>> start, end = _parse_request_range(\"bytes=1-2\")",
            "    >>> start, end",
            "    (1, 3)",
            "    >>> [0, 1, 2, 3, 4][start:end]",
            "    [1, 2]",
            "    >>> _parse_request_range(\"bytes=6-\")",
            "    (6, None)",
            "    >>> _parse_request_range(\"bytes=-6\")",
            "    (-6, None)",
            "    >>> _parse_request_range(\"bytes=-0\")",
            "    (None, 0)",
            "    >>> _parse_request_range(\"bytes=\")",
            "    (None, None)",
            "    >>> _parse_request_range(\"foo=42\")",
            "    >>> _parse_request_range(\"bytes=1-2,6-10\")",
            "",
            "    Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).",
            "",
            "    See [0] for the details of the range header.",
            "",
            "    [0]: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html#byte.ranges",
            "    \"\"\"",
            "    unit, _, value = range_header.partition(\"=\")",
            "    unit, value = unit.strip(), value.strip()",
            "    if unit != \"bytes\":",
            "        return None",
            "    start_b, _, end_b = value.partition(\"-\")",
            "    try:",
            "        start = _int_or_none(start_b)",
            "        end = _int_or_none(end_b)",
            "    except ValueError:",
            "        return None",
            "    if end is not None:",
            "        if start is None:",
            "            if end != 0:",
            "                start = -end",
            "                end = None",
            "        else:",
            "            end += 1",
            "    return (start, end)",
            "",
            "",
            "def _get_content_range(start: Optional[int], end: Optional[int], total: int) -> str:",
            "    \"\"\"Returns a suitable Content-Range header:",
            "",
            "    >>> print(_get_content_range(None, 1, 4))",
            "    bytes 0-0/4",
            "    >>> print(_get_content_range(1, 3, 4))",
            "    bytes 1-2/4",
            "    >>> print(_get_content_range(None, None, 4))",
            "    bytes 0-3/4",
            "    \"\"\"",
            "    start = start or 0",
            "    end = (end or total) - 1",
            "    return \"bytes %s-%s/%s\" % (start, end, total)",
            "",
            "",
            "def _int_or_none(val: str) -> Optional[int]:",
            "    val = val.strip()",
            "    if val == \"\":",
            "        return None",
            "    return int(val)",
            "",
            "",
            "def parse_body_arguments(",
            "    content_type: str,",
            "    body: bytes,",
            "    arguments: Dict[str, List[bytes]],",
            "    files: Dict[str, List[HTTPFile]],",
            "    headers: Optional[HTTPHeaders] = None,",
            ") -> None:",
            "    \"\"\"Parses a form request body.",
            "",
            "    Supports ``application/x-www-form-urlencoded`` and",
            "    ``multipart/form-data``.  The ``content_type`` parameter should be",
            "    a string and ``body`` should be a byte string.  The ``arguments``",
            "    and ``files`` parameters are dictionaries that will be updated",
            "    with the parsed contents.",
            "    \"\"\"",
            "    if content_type.startswith(\"application/x-www-form-urlencoded\"):",
            "        if headers and \"Content-Encoding\" in headers:",
            "            gen_log.warning(",
            "                \"Unsupported Content-Encoding: %s\", headers[\"Content-Encoding\"]",
            "            )",
            "            return",
            "        try:",
            "            # real charset decoding will happen in RequestHandler.decode_argument()",
            "            uri_arguments = parse_qs_bytes(body, keep_blank_values=True)",
            "        except Exception as e:",
            "            gen_log.warning(\"Invalid x-www-form-urlencoded body: %s\", e)",
            "            uri_arguments = {}",
            "        for name, values in uri_arguments.items():",
            "            if values:",
            "                arguments.setdefault(name, []).extend(values)",
            "    elif content_type.startswith(\"multipart/form-data\"):",
            "        if headers and \"Content-Encoding\" in headers:",
            "            gen_log.warning(",
            "                \"Unsupported Content-Encoding: %s\", headers[\"Content-Encoding\"]",
            "            )",
            "            return",
            "        try:",
            "            fields = content_type.split(\";\")",
            "            for field in fields:",
            "                k, sep, v = field.strip().partition(\"=\")",
            "                if k == \"boundary\" and v:",
            "                    parse_multipart_form_data(utf8(v), body, arguments, files)",
            "                    break",
            "            else:",
            "                raise ValueError(\"multipart boundary not found\")",
            "        except Exception as e:",
            "            gen_log.warning(\"Invalid multipart/form-data: %s\", e)",
            "",
            "",
            "def parse_multipart_form_data(",
            "    boundary: bytes,",
            "    data: bytes,",
            "    arguments: Dict[str, List[bytes]],",
            "    files: Dict[str, List[HTTPFile]],",
            ") -> None:",
            "    \"\"\"Parses a ``multipart/form-data`` body.",
            "",
            "    The ``boundary`` and ``data`` parameters are both byte strings.",
            "    The dictionaries given in the arguments and files parameters",
            "    will be updated with the contents of the body.",
            "",
            "    .. versionchanged:: 5.1",
            "",
            "       Now recognizes non-ASCII filenames in RFC 2231/5987",
            "       (``filename*=``) format.",
            "    \"\"\"",
            "    # The standard allows for the boundary to be quoted in the header,",
            "    # although it's rare (it happens at least for google app engine",
            "    # xmpp).  I think we're also supposed to handle backslash-escapes",
            "    # here but I'll save that until we see a client that uses them",
            "    # in the wild.",
            "    if boundary.startswith(b'\"') and boundary.endswith(b'\"'):",
            "        boundary = boundary[1:-1]",
            "    final_boundary_index = data.rfind(b\"--\" + boundary + b\"--\")",
            "    if final_boundary_index == -1:",
            "        gen_log.warning(\"Invalid multipart/form-data: no final boundary\")",
            "        return",
            "    parts = data[:final_boundary_index].split(b\"--\" + boundary + b\"\\r\\n\")",
            "    for part in parts:",
            "        if not part:",
            "            continue",
            "        eoh = part.find(b\"\\r\\n\\r\\n\")",
            "        if eoh == -1:",
            "            gen_log.warning(\"multipart/form-data missing headers\")",
            "            continue",
            "        headers = HTTPHeaders.parse(part[:eoh].decode(\"utf-8\"))",
            "        disp_header = headers.get(\"Content-Disposition\", \"\")",
            "        disposition, disp_params = _parse_header(disp_header)",
            "        if disposition != \"form-data\" or not part.endswith(b\"\\r\\n\"):",
            "            gen_log.warning(\"Invalid multipart/form-data\")",
            "            continue",
            "        value = part[eoh + 4 : -2]",
            "        if not disp_params.get(\"name\"):",
            "            gen_log.warning(\"multipart/form-data value missing name\")",
            "            continue",
            "        name = disp_params[\"name\"]",
            "        if disp_params.get(\"filename\"):",
            "            ctype = headers.get(\"Content-Type\", \"application/unknown\")",
            "            files.setdefault(name, []).append(",
            "                HTTPFile(",
            "                    filename=disp_params[\"filename\"], body=value, content_type=ctype",
            "                )",
            "            )",
            "        else:",
            "            arguments.setdefault(name, []).append(value)",
            "",
            "",
            "def format_timestamp(",
            "    ts: Union[int, float, tuple, time.struct_time, datetime.datetime]",
            ") -> str:",
            "    \"\"\"Formats a timestamp in the format used by HTTP.",
            "",
            "    The argument may be a numeric timestamp as returned by `time.time`,",
            "    a time tuple as returned by `time.gmtime`, or a `datetime.datetime`",
            "    object. Naive `datetime.datetime` objects are assumed to represent",
            "    UTC; aware objects are converted to UTC before formatting.",
            "",
            "    >>> format_timestamp(1359312200)",
            "    'Sun, 27 Jan 2013 18:43:20 GMT'",
            "    \"\"\"",
            "    if isinstance(ts, (int, float)):",
            "        time_num = ts",
            "    elif isinstance(ts, (tuple, time.struct_time)):",
            "        time_num = calendar.timegm(ts)",
            "    elif isinstance(ts, datetime.datetime):",
            "        time_num = calendar.timegm(ts.utctimetuple())",
            "    else:",
            "        raise TypeError(\"unknown timestamp type: %r\" % ts)",
            "    return email.utils.formatdate(time_num, usegmt=True)",
            "",
            "",
            "RequestStartLine = collections.namedtuple(",
            "    \"RequestStartLine\", [\"method\", \"path\", \"version\"]",
            ")",
            "",
            "",
            "_http_version_re = re.compile(r\"^HTTP/1\\.[0-9]$\")",
            "",
            "",
            "def parse_request_start_line(line: str) -> RequestStartLine:",
            "    \"\"\"Returns a (method, path, version) tuple for an HTTP 1.x request line.",
            "",
            "    The response is a `collections.namedtuple`.",
            "",
            "    >>> parse_request_start_line(\"GET /foo HTTP/1.1\")",
            "    RequestStartLine(method='GET', path='/foo', version='HTTP/1.1')",
            "    \"\"\"",
            "    try:",
            "        method, path, version = line.split(\" \")",
            "    except ValueError:",
            "        # https://tools.ietf.org/html/rfc7230#section-3.1.1",
            "        # invalid request-line SHOULD respond with a 400 (Bad Request)",
            "        raise HTTPInputError(\"Malformed HTTP request line\")",
            "    if not _http_version_re.match(version):",
            "        raise HTTPInputError(",
            "            \"Malformed HTTP version in HTTP Request-Line: %r\" % version",
            "        )",
            "    return RequestStartLine(method, path, version)",
            "",
            "",
            "ResponseStartLine = collections.namedtuple(",
            "    \"ResponseStartLine\", [\"version\", \"code\", \"reason\"]",
            ")",
            "",
            "",
            "_http_response_line_re = re.compile(r\"(HTTP/1.[0-9]) ([0-9]+) ([^\\r]*)\")",
            "",
            "",
            "def parse_response_start_line(line: str) -> ResponseStartLine:",
            "    \"\"\"Returns a (version, code, reason) tuple for an HTTP 1.x response line.",
            "",
            "    The response is a `collections.namedtuple`.",
            "",
            "    >>> parse_response_start_line(\"HTTP/1.1 200 OK\")",
            "    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')",
            "    \"\"\"",
            "    line = native_str(line)",
            "    match = _http_response_line_re.match(line)",
            "    if not match:",
            "        raise HTTPInputError(\"Error parsing response start line\")",
            "    return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))",
            "",
            "",
            "# _parseparam and _parse_header are copied and modified from python2.7's cgi.py",
            "# The original 2.7 version of this code did not correctly support some",
            "# combinations of semicolons and double quotes.",
            "# It has also been modified to support valueless parameters as seen in",
            "# websocket extension negotiations, and to support non-ascii values in",
            "# RFC 2231/5987 format.",
            "",
            "",
            "def _parseparam(s: str) -> Generator[str, None, None]:",
            "    while s[:1] == \";\":",
            "        s = s[1:]",
            "        end = s.find(\";\")",
            "        while end > 0 and (s.count('\"', 0, end) - s.count('\\\\\"', 0, end)) % 2:",
            "            end = s.find(\";\", end + 1)",
            "        if end < 0:",
            "            end = len(s)",
            "        f = s[:end]",
            "        yield f.strip()",
            "        s = s[end:]",
            "",
            "",
            "def _parse_header(line: str) -> Tuple[str, Dict[str, str]]:",
            "    r\"\"\"Parse a Content-type like header.",
            "",
            "    Return the main content-type and a dictionary of options.",
            "",
            "    >>> d = \"form-data; foo=\\\"b\\\\\\\\a\\\\\\\"r\\\"; file*=utf-8''T%C3%A4st\"",
            "    >>> ct, d = _parse_header(d)",
            "    >>> ct",
            "    'form-data'",
            "    >>> d['file'] == r'T\\u00e4st'.encode('ascii').decode('unicode_escape')",
            "    True",
            "    >>> d['foo']",
            "    'b\\\\a\"r'",
            "    \"\"\"",
            "    parts = _parseparam(\";\" + line)",
            "    key = next(parts)",
            "    # decode_params treats first argument special, but we already stripped key",
            "    params = [(\"Dummy\", \"value\")]",
            "    for p in parts:",
            "        i = p.find(\"=\")",
            "        if i >= 0:",
            "            name = p[:i].strip().lower()",
            "            value = p[i + 1 :].strip()",
            "            params.append((name, native_str(value)))",
            "    decoded_params = email.utils.decode_params(params)",
            "    decoded_params.pop(0)  # get rid of the dummy again",
            "    pdict = {}",
            "    for name, decoded_value in decoded_params:",
            "        value = email.utils.collapse_rfc2231_value(decoded_value)",
            "        if len(value) >= 2 and value[0] == '\"' and value[-1] == '\"':",
            "            value = value[1:-1]",
            "        pdict[name] = value",
            "    return key, pdict",
            "",
            "",
            "def _encode_header(key: str, pdict: Dict[str, str]) -> str:",
            "    \"\"\"Inverse of _parse_header.",
            "",
            "    >>> _encode_header('permessage-deflate',",
            "    ...     {'client_max_window_bits': 15, 'client_no_context_takeover': None})",
            "    'permessage-deflate; client_max_window_bits=15; client_no_context_takeover'",
            "    \"\"\"",
            "    if not pdict:",
            "        return key",
            "    out = [key]",
            "    # Sort the parameters just to make it easy to test.",
            "    for k, v in sorted(pdict.items()):",
            "        if v is None:",
            "            out.append(k)",
            "        else:",
            "            # TODO: quote if necessary.",
            "            out.append(\"%s=%s\" % (k, v))",
            "    return \"; \".join(out)",
            "",
            "",
            "def encode_username_password(",
            "    username: Union[str, bytes], password: Union[str, bytes]",
            ") -> bytes:",
            "    \"\"\"Encodes a username/password pair in the format used by HTTP auth.",
            "",
            "    The return value is a byte string in the form ``username:password``.",
            "",
            "    .. versionadded:: 5.1",
            "    \"\"\"",
            "    if isinstance(username, unicode_type):",
            "        username = unicodedata.normalize(\"NFC\", username)",
            "    if isinstance(password, unicode_type):",
            "        password = unicodedata.normalize(\"NFC\", password)",
            "    return utf8(username) + b\":\" + utf8(password)",
            "",
            "",
            "def doctests():",
            "    # type: () -> unittest.TestSuite",
            "    import doctest",
            "",
            "    return doctest.DocTestSuite()",
            "",
            "",
            "_netloc_re = re.compile(r\"^(.+):(\\d+)$\")",
            "",
            "",
            "def split_host_and_port(netloc: str) -> Tuple[str, Optional[int]]:",
            "    \"\"\"Returns ``(host, port)`` tuple from ``netloc``.",
            "",
            "    Returned ``port`` will be ``None`` if not present.",
            "",
            "    .. versionadded:: 4.1",
            "    \"\"\"",
            "    match = _netloc_re.match(netloc)",
            "    if match:",
            "        host = match.group(1)",
            "        port = int(match.group(2))  # type: Optional[int]",
            "    else:",
            "        host = netloc",
            "        port = None",
            "    return (host, port)",
            "",
            "",
            "def qs_to_qsl(qs: Dict[str, List[AnyStr]]) -> Iterable[Tuple[str, AnyStr]]:",
            "    \"\"\"Generator converting a result of ``parse_qs`` back to name-value pairs.",
            "",
            "    .. versionadded:: 5.0",
            "    \"\"\"",
            "    for k, vs in qs.items():",
            "        for v in vs:",
            "            yield (k, v)",
            "",
            "",
            "_unquote_sub = re.compile(r\"\\\\(?:([0-3][0-7][0-7])|(.))\").sub",
            "",
            "",
            "def _unquote_replace(m: re.Match) -> str:",
            "    if m[1]:",
            "        return chr(int(m[1], 8))",
            "    else:",
            "        return m[2]",
            "",
            "",
            "def _unquote_cookie(s: str) -> str:",
            "    \"\"\"Handle double quotes and escaping in cookie values.",
            "",
            "    This method is copied verbatim from the Python 3.13 standard",
            "    library (http.cookies._unquote) so we don't have to depend on",
            "    non-public interfaces.",
            "    \"\"\"",
            "    # If there aren't any doublequotes,",
            "    # then there can't be any special characters.  See RFC 2109.",
            "    if s is None or len(s) < 2:",
            "        return s",
            "    if s[0] != '\"' or s[-1] != '\"':",
            "        return s",
            "",
            "    # We have to assume that we must decode this string.",
            "    # Down to work.",
            "",
            "    # Remove the \"s",
            "    s = s[1:-1]",
            "",
            "    # Check for special sequences.  Examples:",
            "    #    \\012 --> \\n",
            "    #    \\\"   --> \"",
            "    #",
            "    return _unquote_sub(_unquote_replace, s)",
            "",
            "",
            "def parse_cookie(cookie: str) -> Dict[str, str]:",
            "    \"\"\"Parse a ``Cookie`` HTTP header into a dict of name/value pairs.",
            "",
            "    This function attempts to mimic browser cookie parsing behavior;",
            "    it specifically does not follow any of the cookie-related RFCs",
            "    (because browsers don't either).",
            "",
            "    The algorithm used is identical to that used by Django version 1.9.10.",
            "",
            "    .. versionadded:: 4.4.2",
            "    \"\"\"",
            "    cookiedict = {}",
            "    for chunk in cookie.split(str(\";\")):",
            "        if str(\"=\") in chunk:",
            "            key, val = chunk.split(str(\"=\"), 1)",
            "        else:",
            "            # Assume an empty name per",
            "            # https://bugzilla.mozilla.org/show_bug.cgi?id=169091",
            "            key, val = str(\"\"), chunk",
            "        key, val = key.strip(), val.strip()",
            "        if key or val:",
            "            # unquote using Python's algorithm.",
            "            cookiedict[key] = _unquote_cookie(val)",
            "    return cookiedict"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1060": [
                "_OctalPatt"
            ],
            "1061": [
                "_QuotePatt"
            ],
            "1062": [
                "_nulljoin"
            ],
            "1068": [
                "_unquote_cookie"
            ],
            "1089": [
                "_unquote_cookie"
            ],
            "1090": [
                "_unquote_cookie"
            ],
            "1091": [
                "_unquote_cookie"
            ],
            "1092": [
                "_unquote_cookie"
            ],
            "1093": [
                "_unquote_cookie"
            ],
            "1094": [
                "_unquote_cookie"
            ],
            "1095": [
                "_unquote_cookie"
            ],
            "1096": [
                "_unquote_cookie"
            ],
            "1097": [
                "_unquote_cookie"
            ],
            "1098": [
                "_unquote_cookie"
            ],
            "1099": [
                "_unquote_cookie"
            ],
            "1100": [
                "_unquote_cookie"
            ],
            "1101": [
                "_unquote_cookie"
            ],
            "1102": [
                "_unquote_cookie"
            ],
            "1103": [
                "_unquote_cookie"
            ],
            "1104": [
                "_unquote_cookie"
            ],
            "1105": [
                "_unquote_cookie"
            ],
            "1106": [
                "_unquote_cookie"
            ],
            "1107": [
                "_unquote_cookie"
            ],
            "1108": [
                "_unquote_cookie"
            ],
            "1109": [
                "_unquote_cookie"
            ],
            "1110": [
                "_unquote_cookie"
            ],
            "1111": [
                "_unquote_cookie"
            ],
            "1112": [
                "_unquote_cookie"
            ]
        },
        "addLocation": []
    },
    "tornado/test/httputil_test.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 560,
                "afterPatchRowNumber": 560,
                "PatchRowcode": "         self.assertEqual("
            },
            "1": {
                "beforePatchRowNumber": 561,
                "afterPatchRowNumber": 561,
                "PatchRowcode": "             parse_cookie(\"  =  b  ;  ;  =  ;   c  =  ;  \"), {\"\": \"b\", \"c\": \"\"}"
            },
            "2": {
                "beforePatchRowNumber": 562,
                "afterPatchRowNumber": 562,
                "PatchRowcode": "         )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 563,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 564,
                "PatchRowcode": "+    def test_unquote(self):"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 565,
                "PatchRowcode": "+        # Copied from"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 566,
                "PatchRowcode": "+        # https://github.com/python/cpython/blob/dc7a2b6522ec7af41282bc34f405bee9b306d611/Lib/test/test_http_cookies.py#L62"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 567,
                "PatchRowcode": "+        cases = ["
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 568,
                "PatchRowcode": "+            (r'a=\"b=\\\"\"', 'b=\"'),"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 569,
                "PatchRowcode": "+            (r'a=\"b=\\\\\"', \"b=\\\\\"),"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 570,
                "PatchRowcode": "+            (r'a=\"b=\\=\"', \"b==\"),"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 571,
                "PatchRowcode": "+            (r'a=\"b=\\n\"', \"b=n\"),"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 572,
                "PatchRowcode": "+            (r'a=\"b=\\042\"', 'b=\"'),"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 573,
                "PatchRowcode": "+            (r'a=\"b=\\134\"', \"b=\\\\\"),"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 574,
                "PatchRowcode": "+            (r'a=\"b=\\377\"', \"b=\\xff\"),"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 575,
                "PatchRowcode": "+            (r'a=\"b=\\400\"', \"b=400\"),"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 576,
                "PatchRowcode": "+            (r'a=\"b=\\42\"', \"b=42\"),"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 577,
                "PatchRowcode": "+            (r'a=\"b=\\\\042\"', \"b=\\\\042\"),"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 578,
                "PatchRowcode": "+            (r'a=\"b=\\\\134\"', \"b=\\\\134\"),"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 579,
                "PatchRowcode": "+            (r'a=\"b=\\\\\\\"\"', 'b=\\\\\"'),"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 580,
                "PatchRowcode": "+            (r'a=\"b=\\\\\\042\"', 'b=\\\\\"'),"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 581,
                "PatchRowcode": "+            (r'a=\"b=\\134\\\"\"', 'b=\\\\\"'),"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 582,
                "PatchRowcode": "+            (r'a=\"b=\\134\\042\"', 'b=\\\\\"'),"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 583,
                "PatchRowcode": "+        ]"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 584,
                "PatchRowcode": "+        for encoded, decoded in cases:"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 585,
                "PatchRowcode": "+            with self.subTest(encoded):"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 586,
                "PatchRowcode": "+                c = parse_cookie(encoded)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 587,
                "PatchRowcode": "+                self.assertEqual(c[\"a\"], decoded)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 588,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 589,
                "PatchRowcode": "+    def test_unquote_large(self):"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 590,
                "PatchRowcode": "+        # Adapted from"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 591,
                "PatchRowcode": "+        # https://github.com/python/cpython/blob/dc7a2b6522ec7af41282bc34f405bee9b306d611/Lib/test/test_http_cookies.py#L87"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 592,
                "PatchRowcode": "+        # Modified from that test because we handle semicolons differently from the stdlib."
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 593,
                "PatchRowcode": "+        #"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 594,
                "PatchRowcode": "+        # This is a performance regression test: prior to improvements in Tornado 6.4.2, this test"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 595,
                "PatchRowcode": "+        # would take over a minute with n= 100k. Now it runs in tens of milliseconds."
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 596,
                "PatchRowcode": "+        n = 100000"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 597,
                "PatchRowcode": "+        for encoded in r\"\\\\\", r\"\\134\":"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 598,
                "PatchRowcode": "+            with self.subTest(encoded):"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 599,
                "PatchRowcode": "+                start = time.time()"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 600,
                "PatchRowcode": "+                data = 'a=\"b=' + encoded * n + '\"'"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 601,
                "PatchRowcode": "+                value = parse_cookie(data)[\"a\"]"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 602,
                "PatchRowcode": "+                end = time.time()"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 603,
                "PatchRowcode": "+                self.assertEqual(value[:3], \"b=\\\\\")"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 604,
                "PatchRowcode": "+                self.assertEqual(value[-3:], \"\\\\\\\\\\\\\")"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 605,
                "PatchRowcode": "+                self.assertEqual(len(value), n + 2)"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 606,
                "PatchRowcode": "+"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 607,
                "PatchRowcode": "+                # Very loose performance check to avoid false positives"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 608,
                "PatchRowcode": "+                self.assertLess(end - start, 1, \"Test took too long\")"
            }
        },
        "frontPatchFile": [
            "from tornado.httputil import (",
            "    url_concat,",
            "    parse_multipart_form_data,",
            "    HTTPHeaders,",
            "    format_timestamp,",
            "    HTTPServerRequest,",
            "    parse_request_start_line,",
            "    parse_cookie,",
            "    qs_to_qsl,",
            "    HTTPInputError,",
            "    HTTPFile,",
            ")",
            "from tornado.escape import utf8, native_str",
            "from tornado.log import gen_log",
            "from tornado.testing import ExpectLog",
            "from tornado.test.util import ignore_deprecation",
            "",
            "import copy",
            "import datetime",
            "import logging",
            "import pickle",
            "import time",
            "import urllib.parse",
            "import unittest",
            "",
            "from typing import Tuple, Dict, List",
            "",
            "",
            "def form_data_args() -> Tuple[Dict[str, List[bytes]], Dict[str, List[HTTPFile]]]:",
            "    \"\"\"Return two empty dicts suitable for use with parse_multipart_form_data.",
            "",
            "    mypy insists on type annotations for dict literals, so this lets us avoid",
            "    the verbose types throughout this test.",
            "    \"\"\"",
            "    return {}, {}",
            "",
            "",
            "class TestUrlConcat(unittest.TestCase):",
            "    def test_url_concat_no_query_params(self):",
            "        url = url_concat(\"https://localhost/path\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y&z=z\")",
            "",
            "    def test_url_concat_encode_args(self):",
            "        url = url_concat(\"https://localhost/path\", [(\"y\", \"/y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=%2Fy&z=z\")",
            "",
            "    def test_url_concat_trailing_q(self):",
            "        url = url_concat(\"https://localhost/path?\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y&z=z\")",
            "",
            "    def test_url_concat_q_with_no_trailing_amp(self):",
            "        url = url_concat(\"https://localhost/path?x\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?x=&y=y&z=z\")",
            "",
            "    def test_url_concat_trailing_amp(self):",
            "        url = url_concat(\"https://localhost/path?x&\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?x=&y=y&z=z\")",
            "",
            "    def test_url_concat_mult_params(self):",
            "        url = url_concat(\"https://localhost/path?a=1&b=2\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?a=1&b=2&y=y&z=z\")",
            "",
            "    def test_url_concat_no_params(self):",
            "        url = url_concat(\"https://localhost/path?r=1&t=2\", [])",
            "        self.assertEqual(url, \"https://localhost/path?r=1&t=2\")",
            "",
            "    def test_url_concat_none_params(self):",
            "        url = url_concat(\"https://localhost/path?r=1&t=2\", None)",
            "        self.assertEqual(url, \"https://localhost/path?r=1&t=2\")",
            "",
            "    def test_url_concat_with_frag(self):",
            "        url = url_concat(\"https://localhost/path#tab\", [(\"y\", \"y\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y#tab\")",
            "",
            "    def test_url_concat_multi_same_params(self):",
            "        url = url_concat(\"https://localhost/path\", [(\"y\", \"y1\"), (\"y\", \"y2\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y1&y=y2\")",
            "",
            "    def test_url_concat_multi_same_query_params(self):",
            "        url = url_concat(\"https://localhost/path?r=1&r=2\", [(\"y\", \"y\")])",
            "        self.assertEqual(url, \"https://localhost/path?r=1&r=2&y=y\")",
            "",
            "    def test_url_concat_dict_params(self):",
            "        url = url_concat(\"https://localhost/path\", dict(y=\"y\"))",
            "        self.assertEqual(url, \"https://localhost/path?y=y\")",
            "",
            "",
            "class QsParseTest(unittest.TestCase):",
            "    def test_parsing(self):",
            "        qsstring = \"a=1&b=2&a=3\"",
            "        qs = urllib.parse.parse_qs(qsstring)",
            "        qsl = list(qs_to_qsl(qs))",
            "        self.assertIn((\"a\", \"1\"), qsl)",
            "        self.assertIn((\"a\", \"3\"), qsl)",
            "        self.assertIn((\"b\", \"2\"), qsl)",
            "",
            "",
            "class MultipartFormDataTest(unittest.TestCase):",
            "    def test_file_upload(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_unquoted_names(self):",
            "        # quotes are optional unless special characters are present",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=files; filename=ab.txt",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_special_filenames(self):",
            "        filenames = [",
            "            \"a;b.txt\",",
            "            'a\"b.txt',",
            "            'a\";b.txt',",
            "            'a;\"b.txt',",
            "            'a\";\";.txt',",
            "            'a\\\\\"b.txt',",
            "            \"a\\\\b.txt\",",
            "        ]",
            "        for filename in filenames:",
            "            logging.debug(\"trying filename %r\", filename)",
            "            str_data = \"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"%s\"",
            "",
            "Foo",
            "--1234--\"\"\" % filename.replace(",
            "                \"\\\\\", \"\\\\\\\\\"",
            "            ).replace(",
            "                '\"', '\\\\\"'",
            "            )",
            "            data = utf8(str_data.replace(\"\\n\", \"\\r\\n\"))",
            "            args, files = form_data_args()",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "            file = files[\"files\"][0]",
            "            self.assertEqual(file[\"filename\"], filename)",
            "            self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_non_ascii_filename(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"; filename*=UTF-8''%C3%A1b.txt",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"\u00e1b.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_boundary_starts_and_ends_with_quotes(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b'\"1234\"', data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_missing_headers(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"multipart/form-data missing headers\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_invalid_content_disposition(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: invalid; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"Invalid multipart/form-data\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_line_does_not_end_with_correct_line_break(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"Invalid multipart/form-data\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_content_disposition_header_without_name_parameter(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"multipart/form-data value missing name\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_data_after_final_boundary(self):",
            "        # The spec requires that data after the final boundary be ignored.",
            "        # http://www.w3.org/Protocols/rfc1341/7_2_Multipart.html",
            "        # In practice, some libraries include an extra CRLF after the boundary.",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--",
            "\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "",
            "class HTTPHeadersTest(unittest.TestCase):",
            "    def test_multi_line(self):",
            "        # Lines beginning with whitespace are appended to the previous line",
            "        # with any leading whitespace replaced by a single space.",
            "        # Note that while multi-line headers are a part of the HTTP spec,",
            "        # their use is strongly discouraged.",
            "        data = \"\"\"\\",
            "Foo: bar",
            " baz",
            "Asdf: qwer",
            "\\tzxcv",
            "Foo: even",
            "     more",
            "     lines",
            "\"\"\".replace(",
            "            \"\\n\", \"\\r\\n\"",
            "        )",
            "        headers = HTTPHeaders.parse(data)",
            "        self.assertEqual(headers[\"asdf\"], \"qwer zxcv\")",
            "        self.assertEqual(headers.get_list(\"asdf\"), [\"qwer zxcv\"])",
            "        self.assertEqual(headers[\"Foo\"], \"bar baz,even more lines\")",
            "        self.assertEqual(headers.get_list(\"foo\"), [\"bar baz\", \"even more lines\"])",
            "        self.assertEqual(",
            "            sorted(list(headers.get_all())),",
            "            [(\"Asdf\", \"qwer zxcv\"), (\"Foo\", \"bar baz\"), (\"Foo\", \"even more lines\")],",
            "        )",
            "",
            "    def test_malformed_continuation(self):",
            "        # If the first line starts with whitespace, it's a",
            "        # continuation line with nothing to continue, so reject it",
            "        # (with a proper error).",
            "        data = \" Foo: bar\"",
            "        self.assertRaises(HTTPInputError, HTTPHeaders.parse, data)",
            "",
            "    def test_unicode_newlines(self):",
            "        # Ensure that only \\r\\n is recognized as a header separator, and not",
            "        # the other newline-like unicode characters.",
            "        # Characters that are likely to be problematic can be found in",
            "        # http://unicode.org/standard/reports/tr13/tr13-5.html",
            "        # and cpython's unicodeobject.c (which defines the implementation",
            "        # of unicode_type.splitlines(), and uses a different list than TR13).",
            "        newlines = [",
            "            \"\\u001b\",  # VERTICAL TAB",
            "            \"\\u001c\",  # FILE SEPARATOR",
            "            \"\\u001d\",  # GROUP SEPARATOR",
            "            \"\\u001e\",  # RECORD SEPARATOR",
            "            \"\\u0085\",  # NEXT LINE",
            "            \"\\u2028\",  # LINE SEPARATOR",
            "            \"\\u2029\",  # PARAGRAPH SEPARATOR",
            "        ]",
            "        for newline in newlines:",
            "            # Try the utf8 and latin1 representations of each newline",
            "            for encoding in [\"utf8\", \"latin1\"]:",
            "                try:",
            "                    try:",
            "                        encoded = newline.encode(encoding)",
            "                    except UnicodeEncodeError:",
            "                        # Some chars cannot be represented in latin1",
            "                        continue",
            "                    data = b\"Cookie: foo=\" + encoded + b\"bar\"",
            "                    # parse() wants a native_str, so decode through latin1",
            "                    # in the same way the real parser does.",
            "                    headers = HTTPHeaders.parse(native_str(data.decode(\"latin1\")))",
            "                    expected = [",
            "                        (",
            "                            \"Cookie\",",
            "                            \"foo=\" + native_str(encoded.decode(\"latin1\")) + \"bar\",",
            "                        )",
            "                    ]",
            "                    self.assertEqual(expected, list(headers.get_all()))",
            "                except Exception:",
            "                    gen_log.warning(\"failed while trying %r in %s\", newline, encoding)",
            "                    raise",
            "",
            "    def test_unicode_whitespace(self):",
            "        # Only tabs and spaces are to be stripped according to the HTTP standard.",
            "        # Other unicode whitespace is to be left as-is. In the context of headers,",
            "        # this specifically means the whitespace characters falling within the",
            "        # latin1 charset.",
            "        whitespace = [",
            "            (\" \", True),  # SPACE",
            "            (\"\\t\", True),  # TAB",
            "            (\"\\u00a0\", False),  # NON-BREAKING SPACE",
            "            (\"\\u0085\", False),  # NEXT LINE",
            "        ]",
            "        for c, stripped in whitespace:",
            "            headers = HTTPHeaders.parse(\"Transfer-Encoding: %schunked\" % c)",
            "            if stripped:",
            "                expected = [(\"Transfer-Encoding\", \"chunked\")]",
            "            else:",
            "                expected = [(\"Transfer-Encoding\", \"%schunked\" % c)]",
            "            self.assertEqual(expected, list(headers.get_all()))",
            "",
            "    def test_optional_cr(self):",
            "        # Both CRLF and LF should be accepted as separators. CR should not be",
            "        # part of the data when followed by LF, but it is a normal char",
            "        # otherwise (or should bare CR be an error?)",
            "        headers = HTTPHeaders.parse(\"CRLF: crlf\\r\\nLF: lf\\nCR: cr\\rMore: more\\r\\n\")",
            "        self.assertEqual(",
            "            sorted(headers.get_all()),",
            "            [(\"Cr\", \"cr\\rMore: more\"), (\"Crlf\", \"crlf\"), (\"Lf\", \"lf\")],",
            "        )",
            "",
            "    def test_copy(self):",
            "        all_pairs = [(\"A\", \"1\"), (\"A\", \"2\"), (\"B\", \"c\")]",
            "        h1 = HTTPHeaders()",
            "        for k, v in all_pairs:",
            "            h1.add(k, v)",
            "        h2 = h1.copy()",
            "        h3 = copy.copy(h1)",
            "        h4 = copy.deepcopy(h1)",
            "        for headers in [h1, h2, h3, h4]:",
            "            # All the copies are identical, no matter how they were",
            "            # constructed.",
            "            self.assertEqual(list(sorted(headers.get_all())), all_pairs)",
            "        for headers in [h2, h3, h4]:",
            "            # Neither the dict or its member lists are reused.",
            "            self.assertIsNot(headers, h1)",
            "            self.assertIsNot(headers.get_list(\"A\"), h1.get_list(\"A\"))",
            "",
            "    def test_pickle_roundtrip(self):",
            "        headers = HTTPHeaders()",
            "        headers.add(\"Set-Cookie\", \"a=b\")",
            "        headers.add(\"Set-Cookie\", \"c=d\")",
            "        headers.add(\"Content-Type\", \"text/html\")",
            "        pickled = pickle.dumps(headers)",
            "        unpickled = pickle.loads(pickled)",
            "        self.assertEqual(sorted(headers.get_all()), sorted(unpickled.get_all()))",
            "        self.assertEqual(sorted(headers.items()), sorted(unpickled.items()))",
            "",
            "    def test_setdefault(self):",
            "        headers = HTTPHeaders()",
            "        headers[\"foo\"] = \"bar\"",
            "        # If a value is present, setdefault returns it without changes.",
            "        self.assertEqual(headers.setdefault(\"foo\", \"baz\"), \"bar\")",
            "        self.assertEqual(headers[\"foo\"], \"bar\")",
            "        # If a value is not present, setdefault sets it for future use.",
            "        self.assertEqual(headers.setdefault(\"quux\", \"xyzzy\"), \"xyzzy\")",
            "        self.assertEqual(headers[\"quux\"], \"xyzzy\")",
            "        self.assertEqual(sorted(headers.get_all()), [(\"Foo\", \"bar\"), (\"Quux\", \"xyzzy\")])",
            "",
            "    def test_string(self):",
            "        headers = HTTPHeaders()",
            "        headers.add(\"Foo\", \"1\")",
            "        headers.add(\"Foo\", \"2\")",
            "        headers.add(\"Foo\", \"3\")",
            "        headers2 = HTTPHeaders.parse(str(headers))",
            "        self.assertEqual(headers, headers2)",
            "",
            "",
            "class FormatTimestampTest(unittest.TestCase):",
            "    # Make sure that all the input types are supported.",
            "    TIMESTAMP = 1359312200.503611",
            "    EXPECTED = \"Sun, 27 Jan 2013 18:43:20 GMT\"",
            "",
            "    def check(self, value):",
            "        self.assertEqual(format_timestamp(value), self.EXPECTED)",
            "",
            "    def test_unix_time_float(self):",
            "        self.check(self.TIMESTAMP)",
            "",
            "    def test_unix_time_int(self):",
            "        self.check(int(self.TIMESTAMP))",
            "",
            "    def test_struct_time(self):",
            "        self.check(time.gmtime(self.TIMESTAMP))",
            "",
            "    def test_time_tuple(self):",
            "        tup = tuple(time.gmtime(self.TIMESTAMP))",
            "        self.assertEqual(9, len(tup))",
            "        self.check(tup)",
            "",
            "    def test_utc_naive_datetime(self):",
            "        self.check(",
            "            datetime.datetime.fromtimestamp(",
            "                self.TIMESTAMP, datetime.timezone.utc",
            "            ).replace(tzinfo=None)",
            "        )",
            "",
            "    def test_utc_naive_datetime_deprecated(self):",
            "        with ignore_deprecation():",
            "            self.check(datetime.datetime.utcfromtimestamp(self.TIMESTAMP))",
            "",
            "    def test_utc_aware_datetime(self):",
            "        self.check(",
            "            datetime.datetime.fromtimestamp(self.TIMESTAMP, datetime.timezone.utc)",
            "        )",
            "",
            "    def test_other_aware_datetime(self):",
            "        # Other timezones are ignored; the timezone is always printed as GMT",
            "        self.check(",
            "            datetime.datetime.fromtimestamp(",
            "                self.TIMESTAMP, datetime.timezone(datetime.timedelta(hours=-4))",
            "            )",
            "        )",
            "",
            "",
            "# HTTPServerRequest is mainly tested incidentally to the server itself,",
            "# but this tests the parts of the class that can be tested in isolation.",
            "class HTTPServerRequestTest(unittest.TestCase):",
            "    def test_default_constructor(self):",
            "        # All parameters are formally optional, but uri is required",
            "        # (and has been for some time).  This test ensures that no",
            "        # more required parameters slip in.",
            "        HTTPServerRequest(uri=\"/\")",
            "",
            "    def test_body_is_a_byte_string(self):",
            "        requets = HTTPServerRequest(uri=\"/\")",
            "        self.assertIsInstance(requets.body, bytes)",
            "",
            "    def test_repr_does_not_contain_headers(self):",
            "        request = HTTPServerRequest(",
            "            uri=\"/\", headers=HTTPHeaders({\"Canary\": [\"Coal Mine\"]})",
            "        )",
            "        self.assertTrue(\"Canary\" not in repr(request))",
            "",
            "",
            "class ParseRequestStartLineTest(unittest.TestCase):",
            "    METHOD = \"GET\"",
            "    PATH = \"/foo\"",
            "    VERSION = \"HTTP/1.1\"",
            "",
            "    def test_parse_request_start_line(self):",
            "        start_line = \" \".join([self.METHOD, self.PATH, self.VERSION])",
            "        parsed_start_line = parse_request_start_line(start_line)",
            "        self.assertEqual(parsed_start_line.method, self.METHOD)",
            "        self.assertEqual(parsed_start_line.path, self.PATH)",
            "        self.assertEqual(parsed_start_line.version, self.VERSION)",
            "",
            "",
            "class ParseCookieTest(unittest.TestCase):",
            "    # These tests copied from Django:",
            "    # https://github.com/django/django/pull/6277/commits/da810901ada1cae9fc1f018f879f11a7fb467b28",
            "    def test_python_cookies(self):",
            "        \"\"\"",
            "        Test cases copied from Python's Lib/test/test_http_cookies.py",
            "        \"\"\"",
            "        self.assertEqual(",
            "            parse_cookie(\"chips=ahoy; vienna=finger\"),",
            "            {\"chips\": \"ahoy\", \"vienna\": \"finger\"},",
            "        )",
            "        # Here parse_cookie() differs from Python's cookie parsing in that it",
            "        # treats all semicolons as delimiters, even within quotes.",
            "        self.assertEqual(",
            "            parse_cookie('keebler=\"E=mc2; L=\\\\\"Loves\\\\\"; fudge=\\\\012;\"'),",
            "            {\"keebler\": '\"E=mc2', \"L\": '\\\\\"Loves\\\\\"', \"fudge\": \"\\\\012\", \"\": '\"'},",
            "        )",
            "        # Illegal cookies that have an '=' char in an unquoted value.",
            "        self.assertEqual(parse_cookie(\"keebler=E=mc2\"), {\"keebler\": \"E=mc2\"})",
            "        # Cookies with ':' character in their name.",
            "        self.assertEqual(",
            "            parse_cookie(\"key:term=value:term\"), {\"key:term\": \"value:term\"}",
            "        )",
            "        # Cookies with '[' and ']'.",
            "        self.assertEqual(",
            "            parse_cookie(\"a=b; c=[; d=r; f=h\"), {\"a\": \"b\", \"c\": \"[\", \"d\": \"r\", \"f\": \"h\"}",
            "        )",
            "",
            "    def test_cookie_edgecases(self):",
            "        # Cookies that RFC6265 allows.",
            "        self.assertEqual(",
            "            parse_cookie(\"a=b; Domain=example.com\"), {\"a\": \"b\", \"Domain\": \"example.com\"}",
            "        )",
            "        # parse_cookie() has historically kept only the last cookie with the",
            "        # same name.",
            "        self.assertEqual(parse_cookie(\"a=b; h=i; a=c\"), {\"a\": \"c\", \"h\": \"i\"})",
            "",
            "    def test_invalid_cookies(self):",
            "        \"\"\"",
            "        Cookie strings that go against RFC6265 but browsers will send if set",
            "        via document.cookie.",
            "        \"\"\"",
            "        # Chunks without an equals sign appear as unnamed values per",
            "        # https://bugzilla.mozilla.org/show_bug.cgi?id=169091",
            "        self.assertIn(",
            "            \"django_language\",",
            "            parse_cookie(\"abc=def; unnamed; django_language=en\").keys(),",
            "        )",
            "        # Even a double quote may be an unamed value.",
            "        self.assertEqual(parse_cookie('a=b; \"; c=d'), {\"a\": \"b\", \"\": '\"', \"c\": \"d\"})",
            "        # Spaces in names and values, and an equals sign in values.",
            "        self.assertEqual(",
            "            parse_cookie(\"a b c=d e = f; gh=i\"), {\"a b c\": \"d e = f\", \"gh\": \"i\"}",
            "        )",
            "        # More characters the spec forbids.",
            "        self.assertEqual(",
            "            parse_cookie('a   b,c<>@:/[]?{}=d  \"  =e,f g'),",
            "            {\"a   b,c<>@:/[]?{}\": 'd  \"  =e,f g'},",
            "        )",
            "        # Unicode characters. The spec only allows ASCII.",
            "        self.assertEqual(",
            "            parse_cookie(\"saint=Andr\u00e9 Bessette\"),",
            "            {\"saint\": native_str(\"Andr\u00e9 Bessette\")},",
            "        )",
            "        # Browsers don't send extra whitespace or semicolons in Cookie headers,",
            "        # but parse_cookie() should parse whitespace the same way",
            "        # document.cookie parses whitespace.",
            "        self.assertEqual(",
            "            parse_cookie(\"  =  b  ;  ;  =  ;   c  =  ;  \"), {\"\": \"b\", \"c\": \"\"}",
            "        )"
        ],
        "afterPatchFile": [
            "from tornado.httputil import (",
            "    url_concat,",
            "    parse_multipart_form_data,",
            "    HTTPHeaders,",
            "    format_timestamp,",
            "    HTTPServerRequest,",
            "    parse_request_start_line,",
            "    parse_cookie,",
            "    qs_to_qsl,",
            "    HTTPInputError,",
            "    HTTPFile,",
            ")",
            "from tornado.escape import utf8, native_str",
            "from tornado.log import gen_log",
            "from tornado.testing import ExpectLog",
            "from tornado.test.util import ignore_deprecation",
            "",
            "import copy",
            "import datetime",
            "import logging",
            "import pickle",
            "import time",
            "import urllib.parse",
            "import unittest",
            "",
            "from typing import Tuple, Dict, List",
            "",
            "",
            "def form_data_args() -> Tuple[Dict[str, List[bytes]], Dict[str, List[HTTPFile]]]:",
            "    \"\"\"Return two empty dicts suitable for use with parse_multipart_form_data.",
            "",
            "    mypy insists on type annotations for dict literals, so this lets us avoid",
            "    the verbose types throughout this test.",
            "    \"\"\"",
            "    return {}, {}",
            "",
            "",
            "class TestUrlConcat(unittest.TestCase):",
            "    def test_url_concat_no_query_params(self):",
            "        url = url_concat(\"https://localhost/path\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y&z=z\")",
            "",
            "    def test_url_concat_encode_args(self):",
            "        url = url_concat(\"https://localhost/path\", [(\"y\", \"/y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=%2Fy&z=z\")",
            "",
            "    def test_url_concat_trailing_q(self):",
            "        url = url_concat(\"https://localhost/path?\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y&z=z\")",
            "",
            "    def test_url_concat_q_with_no_trailing_amp(self):",
            "        url = url_concat(\"https://localhost/path?x\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?x=&y=y&z=z\")",
            "",
            "    def test_url_concat_trailing_amp(self):",
            "        url = url_concat(\"https://localhost/path?x&\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?x=&y=y&z=z\")",
            "",
            "    def test_url_concat_mult_params(self):",
            "        url = url_concat(\"https://localhost/path?a=1&b=2\", [(\"y\", \"y\"), (\"z\", \"z\")])",
            "        self.assertEqual(url, \"https://localhost/path?a=1&b=2&y=y&z=z\")",
            "",
            "    def test_url_concat_no_params(self):",
            "        url = url_concat(\"https://localhost/path?r=1&t=2\", [])",
            "        self.assertEqual(url, \"https://localhost/path?r=1&t=2\")",
            "",
            "    def test_url_concat_none_params(self):",
            "        url = url_concat(\"https://localhost/path?r=1&t=2\", None)",
            "        self.assertEqual(url, \"https://localhost/path?r=1&t=2\")",
            "",
            "    def test_url_concat_with_frag(self):",
            "        url = url_concat(\"https://localhost/path#tab\", [(\"y\", \"y\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y#tab\")",
            "",
            "    def test_url_concat_multi_same_params(self):",
            "        url = url_concat(\"https://localhost/path\", [(\"y\", \"y1\"), (\"y\", \"y2\")])",
            "        self.assertEqual(url, \"https://localhost/path?y=y1&y=y2\")",
            "",
            "    def test_url_concat_multi_same_query_params(self):",
            "        url = url_concat(\"https://localhost/path?r=1&r=2\", [(\"y\", \"y\")])",
            "        self.assertEqual(url, \"https://localhost/path?r=1&r=2&y=y\")",
            "",
            "    def test_url_concat_dict_params(self):",
            "        url = url_concat(\"https://localhost/path\", dict(y=\"y\"))",
            "        self.assertEqual(url, \"https://localhost/path?y=y\")",
            "",
            "",
            "class QsParseTest(unittest.TestCase):",
            "    def test_parsing(self):",
            "        qsstring = \"a=1&b=2&a=3\"",
            "        qs = urllib.parse.parse_qs(qsstring)",
            "        qsl = list(qs_to_qsl(qs))",
            "        self.assertIn((\"a\", \"1\"), qsl)",
            "        self.assertIn((\"a\", \"3\"), qsl)",
            "        self.assertIn((\"b\", \"2\"), qsl)",
            "",
            "",
            "class MultipartFormDataTest(unittest.TestCase):",
            "    def test_file_upload(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_unquoted_names(self):",
            "        # quotes are optional unless special characters are present",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=files; filename=ab.txt",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_special_filenames(self):",
            "        filenames = [",
            "            \"a;b.txt\",",
            "            'a\"b.txt',",
            "            'a\";b.txt',",
            "            'a;\"b.txt',",
            "            'a\";\";.txt',",
            "            'a\\\\\"b.txt',",
            "            \"a\\\\b.txt\",",
            "        ]",
            "        for filename in filenames:",
            "            logging.debug(\"trying filename %r\", filename)",
            "            str_data = \"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"%s\"",
            "",
            "Foo",
            "--1234--\"\"\" % filename.replace(",
            "                \"\\\\\", \"\\\\\\\\\"",
            "            ).replace(",
            "                '\"', '\\\\\"'",
            "            )",
            "            data = utf8(str_data.replace(\"\\n\", \"\\r\\n\"))",
            "            args, files = form_data_args()",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "            file = files[\"files\"][0]",
            "            self.assertEqual(file[\"filename\"], filename)",
            "            self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_non_ascii_filename(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"; filename*=UTF-8''%C3%A1b.txt",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"\u00e1b.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_boundary_starts_and_ends_with_quotes(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b'\"1234\"', data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "    def test_missing_headers(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"multipart/form-data missing headers\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_invalid_content_disposition(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: invalid; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"Invalid multipart/form-data\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_line_does_not_end_with_correct_line_break(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"Invalid multipart/form-data\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_content_disposition_header_without_name_parameter(self):",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        with ExpectLog(gen_log, \"multipart/form-data value missing name\"):",
            "            parse_multipart_form_data(b\"1234\", data, args, files)",
            "        self.assertEqual(files, {})",
            "",
            "    def test_data_after_final_boundary(self):",
            "        # The spec requires that data after the final boundary be ignored.",
            "        # http://www.w3.org/Protocols/rfc1341/7_2_Multipart.html",
            "        # In practice, some libraries include an extra CRLF after the boundary.",
            "        data = b\"\"\"\\",
            "--1234",
            "Content-Disposition: form-data; name=\"files\"; filename=\"ab.txt\"",
            "",
            "Foo",
            "--1234--",
            "\"\"\".replace(",
            "            b\"\\n\", b\"\\r\\n\"",
            "        )",
            "        args, files = form_data_args()",
            "        parse_multipart_form_data(b\"1234\", data, args, files)",
            "        file = files[\"files\"][0]",
            "        self.assertEqual(file[\"filename\"], \"ab.txt\")",
            "        self.assertEqual(file[\"body\"], b\"Foo\")",
            "",
            "",
            "class HTTPHeadersTest(unittest.TestCase):",
            "    def test_multi_line(self):",
            "        # Lines beginning with whitespace are appended to the previous line",
            "        # with any leading whitespace replaced by a single space.",
            "        # Note that while multi-line headers are a part of the HTTP spec,",
            "        # their use is strongly discouraged.",
            "        data = \"\"\"\\",
            "Foo: bar",
            " baz",
            "Asdf: qwer",
            "\\tzxcv",
            "Foo: even",
            "     more",
            "     lines",
            "\"\"\".replace(",
            "            \"\\n\", \"\\r\\n\"",
            "        )",
            "        headers = HTTPHeaders.parse(data)",
            "        self.assertEqual(headers[\"asdf\"], \"qwer zxcv\")",
            "        self.assertEqual(headers.get_list(\"asdf\"), [\"qwer zxcv\"])",
            "        self.assertEqual(headers[\"Foo\"], \"bar baz,even more lines\")",
            "        self.assertEqual(headers.get_list(\"foo\"), [\"bar baz\", \"even more lines\"])",
            "        self.assertEqual(",
            "            sorted(list(headers.get_all())),",
            "            [(\"Asdf\", \"qwer zxcv\"), (\"Foo\", \"bar baz\"), (\"Foo\", \"even more lines\")],",
            "        )",
            "",
            "    def test_malformed_continuation(self):",
            "        # If the first line starts with whitespace, it's a",
            "        # continuation line with nothing to continue, so reject it",
            "        # (with a proper error).",
            "        data = \" Foo: bar\"",
            "        self.assertRaises(HTTPInputError, HTTPHeaders.parse, data)",
            "",
            "    def test_unicode_newlines(self):",
            "        # Ensure that only \\r\\n is recognized as a header separator, and not",
            "        # the other newline-like unicode characters.",
            "        # Characters that are likely to be problematic can be found in",
            "        # http://unicode.org/standard/reports/tr13/tr13-5.html",
            "        # and cpython's unicodeobject.c (which defines the implementation",
            "        # of unicode_type.splitlines(), and uses a different list than TR13).",
            "        newlines = [",
            "            \"\\u001b\",  # VERTICAL TAB",
            "            \"\\u001c\",  # FILE SEPARATOR",
            "            \"\\u001d\",  # GROUP SEPARATOR",
            "            \"\\u001e\",  # RECORD SEPARATOR",
            "            \"\\u0085\",  # NEXT LINE",
            "            \"\\u2028\",  # LINE SEPARATOR",
            "            \"\\u2029\",  # PARAGRAPH SEPARATOR",
            "        ]",
            "        for newline in newlines:",
            "            # Try the utf8 and latin1 representations of each newline",
            "            for encoding in [\"utf8\", \"latin1\"]:",
            "                try:",
            "                    try:",
            "                        encoded = newline.encode(encoding)",
            "                    except UnicodeEncodeError:",
            "                        # Some chars cannot be represented in latin1",
            "                        continue",
            "                    data = b\"Cookie: foo=\" + encoded + b\"bar\"",
            "                    # parse() wants a native_str, so decode through latin1",
            "                    # in the same way the real parser does.",
            "                    headers = HTTPHeaders.parse(native_str(data.decode(\"latin1\")))",
            "                    expected = [",
            "                        (",
            "                            \"Cookie\",",
            "                            \"foo=\" + native_str(encoded.decode(\"latin1\")) + \"bar\",",
            "                        )",
            "                    ]",
            "                    self.assertEqual(expected, list(headers.get_all()))",
            "                except Exception:",
            "                    gen_log.warning(\"failed while trying %r in %s\", newline, encoding)",
            "                    raise",
            "",
            "    def test_unicode_whitespace(self):",
            "        # Only tabs and spaces are to be stripped according to the HTTP standard.",
            "        # Other unicode whitespace is to be left as-is. In the context of headers,",
            "        # this specifically means the whitespace characters falling within the",
            "        # latin1 charset.",
            "        whitespace = [",
            "            (\" \", True),  # SPACE",
            "            (\"\\t\", True),  # TAB",
            "            (\"\\u00a0\", False),  # NON-BREAKING SPACE",
            "            (\"\\u0085\", False),  # NEXT LINE",
            "        ]",
            "        for c, stripped in whitespace:",
            "            headers = HTTPHeaders.parse(\"Transfer-Encoding: %schunked\" % c)",
            "            if stripped:",
            "                expected = [(\"Transfer-Encoding\", \"chunked\")]",
            "            else:",
            "                expected = [(\"Transfer-Encoding\", \"%schunked\" % c)]",
            "            self.assertEqual(expected, list(headers.get_all()))",
            "",
            "    def test_optional_cr(self):",
            "        # Both CRLF and LF should be accepted as separators. CR should not be",
            "        # part of the data when followed by LF, but it is a normal char",
            "        # otherwise (or should bare CR be an error?)",
            "        headers = HTTPHeaders.parse(\"CRLF: crlf\\r\\nLF: lf\\nCR: cr\\rMore: more\\r\\n\")",
            "        self.assertEqual(",
            "            sorted(headers.get_all()),",
            "            [(\"Cr\", \"cr\\rMore: more\"), (\"Crlf\", \"crlf\"), (\"Lf\", \"lf\")],",
            "        )",
            "",
            "    def test_copy(self):",
            "        all_pairs = [(\"A\", \"1\"), (\"A\", \"2\"), (\"B\", \"c\")]",
            "        h1 = HTTPHeaders()",
            "        for k, v in all_pairs:",
            "            h1.add(k, v)",
            "        h2 = h1.copy()",
            "        h3 = copy.copy(h1)",
            "        h4 = copy.deepcopy(h1)",
            "        for headers in [h1, h2, h3, h4]:",
            "            # All the copies are identical, no matter how they were",
            "            # constructed.",
            "            self.assertEqual(list(sorted(headers.get_all())), all_pairs)",
            "        for headers in [h2, h3, h4]:",
            "            # Neither the dict or its member lists are reused.",
            "            self.assertIsNot(headers, h1)",
            "            self.assertIsNot(headers.get_list(\"A\"), h1.get_list(\"A\"))",
            "",
            "    def test_pickle_roundtrip(self):",
            "        headers = HTTPHeaders()",
            "        headers.add(\"Set-Cookie\", \"a=b\")",
            "        headers.add(\"Set-Cookie\", \"c=d\")",
            "        headers.add(\"Content-Type\", \"text/html\")",
            "        pickled = pickle.dumps(headers)",
            "        unpickled = pickle.loads(pickled)",
            "        self.assertEqual(sorted(headers.get_all()), sorted(unpickled.get_all()))",
            "        self.assertEqual(sorted(headers.items()), sorted(unpickled.items()))",
            "",
            "    def test_setdefault(self):",
            "        headers = HTTPHeaders()",
            "        headers[\"foo\"] = \"bar\"",
            "        # If a value is present, setdefault returns it without changes.",
            "        self.assertEqual(headers.setdefault(\"foo\", \"baz\"), \"bar\")",
            "        self.assertEqual(headers[\"foo\"], \"bar\")",
            "        # If a value is not present, setdefault sets it for future use.",
            "        self.assertEqual(headers.setdefault(\"quux\", \"xyzzy\"), \"xyzzy\")",
            "        self.assertEqual(headers[\"quux\"], \"xyzzy\")",
            "        self.assertEqual(sorted(headers.get_all()), [(\"Foo\", \"bar\"), (\"Quux\", \"xyzzy\")])",
            "",
            "    def test_string(self):",
            "        headers = HTTPHeaders()",
            "        headers.add(\"Foo\", \"1\")",
            "        headers.add(\"Foo\", \"2\")",
            "        headers.add(\"Foo\", \"3\")",
            "        headers2 = HTTPHeaders.parse(str(headers))",
            "        self.assertEqual(headers, headers2)",
            "",
            "",
            "class FormatTimestampTest(unittest.TestCase):",
            "    # Make sure that all the input types are supported.",
            "    TIMESTAMP = 1359312200.503611",
            "    EXPECTED = \"Sun, 27 Jan 2013 18:43:20 GMT\"",
            "",
            "    def check(self, value):",
            "        self.assertEqual(format_timestamp(value), self.EXPECTED)",
            "",
            "    def test_unix_time_float(self):",
            "        self.check(self.TIMESTAMP)",
            "",
            "    def test_unix_time_int(self):",
            "        self.check(int(self.TIMESTAMP))",
            "",
            "    def test_struct_time(self):",
            "        self.check(time.gmtime(self.TIMESTAMP))",
            "",
            "    def test_time_tuple(self):",
            "        tup = tuple(time.gmtime(self.TIMESTAMP))",
            "        self.assertEqual(9, len(tup))",
            "        self.check(tup)",
            "",
            "    def test_utc_naive_datetime(self):",
            "        self.check(",
            "            datetime.datetime.fromtimestamp(",
            "                self.TIMESTAMP, datetime.timezone.utc",
            "            ).replace(tzinfo=None)",
            "        )",
            "",
            "    def test_utc_naive_datetime_deprecated(self):",
            "        with ignore_deprecation():",
            "            self.check(datetime.datetime.utcfromtimestamp(self.TIMESTAMP))",
            "",
            "    def test_utc_aware_datetime(self):",
            "        self.check(",
            "            datetime.datetime.fromtimestamp(self.TIMESTAMP, datetime.timezone.utc)",
            "        )",
            "",
            "    def test_other_aware_datetime(self):",
            "        # Other timezones are ignored; the timezone is always printed as GMT",
            "        self.check(",
            "            datetime.datetime.fromtimestamp(",
            "                self.TIMESTAMP, datetime.timezone(datetime.timedelta(hours=-4))",
            "            )",
            "        )",
            "",
            "",
            "# HTTPServerRequest is mainly tested incidentally to the server itself,",
            "# but this tests the parts of the class that can be tested in isolation.",
            "class HTTPServerRequestTest(unittest.TestCase):",
            "    def test_default_constructor(self):",
            "        # All parameters are formally optional, but uri is required",
            "        # (and has been for some time).  This test ensures that no",
            "        # more required parameters slip in.",
            "        HTTPServerRequest(uri=\"/\")",
            "",
            "    def test_body_is_a_byte_string(self):",
            "        requets = HTTPServerRequest(uri=\"/\")",
            "        self.assertIsInstance(requets.body, bytes)",
            "",
            "    def test_repr_does_not_contain_headers(self):",
            "        request = HTTPServerRequest(",
            "            uri=\"/\", headers=HTTPHeaders({\"Canary\": [\"Coal Mine\"]})",
            "        )",
            "        self.assertTrue(\"Canary\" not in repr(request))",
            "",
            "",
            "class ParseRequestStartLineTest(unittest.TestCase):",
            "    METHOD = \"GET\"",
            "    PATH = \"/foo\"",
            "    VERSION = \"HTTP/1.1\"",
            "",
            "    def test_parse_request_start_line(self):",
            "        start_line = \" \".join([self.METHOD, self.PATH, self.VERSION])",
            "        parsed_start_line = parse_request_start_line(start_line)",
            "        self.assertEqual(parsed_start_line.method, self.METHOD)",
            "        self.assertEqual(parsed_start_line.path, self.PATH)",
            "        self.assertEqual(parsed_start_line.version, self.VERSION)",
            "",
            "",
            "class ParseCookieTest(unittest.TestCase):",
            "    # These tests copied from Django:",
            "    # https://github.com/django/django/pull/6277/commits/da810901ada1cae9fc1f018f879f11a7fb467b28",
            "    def test_python_cookies(self):",
            "        \"\"\"",
            "        Test cases copied from Python's Lib/test/test_http_cookies.py",
            "        \"\"\"",
            "        self.assertEqual(",
            "            parse_cookie(\"chips=ahoy; vienna=finger\"),",
            "            {\"chips\": \"ahoy\", \"vienna\": \"finger\"},",
            "        )",
            "        # Here parse_cookie() differs from Python's cookie parsing in that it",
            "        # treats all semicolons as delimiters, even within quotes.",
            "        self.assertEqual(",
            "            parse_cookie('keebler=\"E=mc2; L=\\\\\"Loves\\\\\"; fudge=\\\\012;\"'),",
            "            {\"keebler\": '\"E=mc2', \"L\": '\\\\\"Loves\\\\\"', \"fudge\": \"\\\\012\", \"\": '\"'},",
            "        )",
            "        # Illegal cookies that have an '=' char in an unquoted value.",
            "        self.assertEqual(parse_cookie(\"keebler=E=mc2\"), {\"keebler\": \"E=mc2\"})",
            "        # Cookies with ':' character in their name.",
            "        self.assertEqual(",
            "            parse_cookie(\"key:term=value:term\"), {\"key:term\": \"value:term\"}",
            "        )",
            "        # Cookies with '[' and ']'.",
            "        self.assertEqual(",
            "            parse_cookie(\"a=b; c=[; d=r; f=h\"), {\"a\": \"b\", \"c\": \"[\", \"d\": \"r\", \"f\": \"h\"}",
            "        )",
            "",
            "    def test_cookie_edgecases(self):",
            "        # Cookies that RFC6265 allows.",
            "        self.assertEqual(",
            "            parse_cookie(\"a=b; Domain=example.com\"), {\"a\": \"b\", \"Domain\": \"example.com\"}",
            "        )",
            "        # parse_cookie() has historically kept only the last cookie with the",
            "        # same name.",
            "        self.assertEqual(parse_cookie(\"a=b; h=i; a=c\"), {\"a\": \"c\", \"h\": \"i\"})",
            "",
            "    def test_invalid_cookies(self):",
            "        \"\"\"",
            "        Cookie strings that go against RFC6265 but browsers will send if set",
            "        via document.cookie.",
            "        \"\"\"",
            "        # Chunks without an equals sign appear as unnamed values per",
            "        # https://bugzilla.mozilla.org/show_bug.cgi?id=169091",
            "        self.assertIn(",
            "            \"django_language\",",
            "            parse_cookie(\"abc=def; unnamed; django_language=en\").keys(),",
            "        )",
            "        # Even a double quote may be an unamed value.",
            "        self.assertEqual(parse_cookie('a=b; \"; c=d'), {\"a\": \"b\", \"\": '\"', \"c\": \"d\"})",
            "        # Spaces in names and values, and an equals sign in values.",
            "        self.assertEqual(",
            "            parse_cookie(\"a b c=d e = f; gh=i\"), {\"a b c\": \"d e = f\", \"gh\": \"i\"}",
            "        )",
            "        # More characters the spec forbids.",
            "        self.assertEqual(",
            "            parse_cookie('a   b,c<>@:/[]?{}=d  \"  =e,f g'),",
            "            {\"a   b,c<>@:/[]?{}\": 'd  \"  =e,f g'},",
            "        )",
            "        # Unicode characters. The spec only allows ASCII.",
            "        self.assertEqual(",
            "            parse_cookie(\"saint=Andr\u00e9 Bessette\"),",
            "            {\"saint\": native_str(\"Andr\u00e9 Bessette\")},",
            "        )",
            "        # Browsers don't send extra whitespace or semicolons in Cookie headers,",
            "        # but parse_cookie() should parse whitespace the same way",
            "        # document.cookie parses whitespace.",
            "        self.assertEqual(",
            "            parse_cookie(\"  =  b  ;  ;  =  ;   c  =  ;  \"), {\"\": \"b\", \"c\": \"\"}",
            "        )",
            "",
            "    def test_unquote(self):",
            "        # Copied from",
            "        # https://github.com/python/cpython/blob/dc7a2b6522ec7af41282bc34f405bee9b306d611/Lib/test/test_http_cookies.py#L62",
            "        cases = [",
            "            (r'a=\"b=\\\"\"', 'b=\"'),",
            "            (r'a=\"b=\\\\\"', \"b=\\\\\"),",
            "            (r'a=\"b=\\=\"', \"b==\"),",
            "            (r'a=\"b=\\n\"', \"b=n\"),",
            "            (r'a=\"b=\\042\"', 'b=\"'),",
            "            (r'a=\"b=\\134\"', \"b=\\\\\"),",
            "            (r'a=\"b=\\377\"', \"b=\\xff\"),",
            "            (r'a=\"b=\\400\"', \"b=400\"),",
            "            (r'a=\"b=\\42\"', \"b=42\"),",
            "            (r'a=\"b=\\\\042\"', \"b=\\\\042\"),",
            "            (r'a=\"b=\\\\134\"', \"b=\\\\134\"),",
            "            (r'a=\"b=\\\\\\\"\"', 'b=\\\\\"'),",
            "            (r'a=\"b=\\\\\\042\"', 'b=\\\\\"'),",
            "            (r'a=\"b=\\134\\\"\"', 'b=\\\\\"'),",
            "            (r'a=\"b=\\134\\042\"', 'b=\\\\\"'),",
            "        ]",
            "        for encoded, decoded in cases:",
            "            with self.subTest(encoded):",
            "                c = parse_cookie(encoded)",
            "                self.assertEqual(c[\"a\"], decoded)",
            "",
            "    def test_unquote_large(self):",
            "        # Adapted from",
            "        # https://github.com/python/cpython/blob/dc7a2b6522ec7af41282bc34f405bee9b306d611/Lib/test/test_http_cookies.py#L87",
            "        # Modified from that test because we handle semicolons differently from the stdlib.",
            "        #",
            "        # This is a performance regression test: prior to improvements in Tornado 6.4.2, this test",
            "        # would take over a minute with n= 100k. Now it runs in tens of milliseconds.",
            "        n = 100000",
            "        for encoded in r\"\\\\\", r\"\\134\":",
            "            with self.subTest(encoded):",
            "                start = time.time()",
            "                data = 'a=\"b=' + encoded * n + '\"'",
            "                value = parse_cookie(data)[\"a\"]",
            "                end = time.time()",
            "                self.assertEqual(value[:3], \"b=\\\\\")",
            "                self.assertEqual(value[-3:], \"\\\\\\\\\\\\\")",
            "                self.assertEqual(len(value), n + 2)",
            "",
            "                # Very loose performance check to avoid false positives",
            "                self.assertLess(end - start, 1, \"Test took too long\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "tornado.test.httputil_test.ParseCookieTest.self",
            "ecdsa.der"
        ]
    }
}