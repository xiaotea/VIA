{
    "feldman_vss.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 546,
                "afterPatchRowNumber": 546,
                "PatchRowcode": "     # Convert to bytes for consistent handling\r"
            },
            "1": {
                "beforePatchRowNumber": 547,
                "afterPatchRowNumber": 547,
                "PatchRowcode": "     if isinstance(a, int) and isinstance(b, int):\r"
            },
            "2": {
                "beforePatchRowNumber": 548,
                "afterPatchRowNumber": 548,
                "PatchRowcode": "         # For integers, ensure same bit length with padding\r"
            },
            "3": {
                "beforePatchRowNumber": 549,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        bit_length: int = max(a.bit_length(), b.bit_length(), 8)  # Minimum 8 bits\r"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 549,
                "PatchRowcode": "+        # Handle the case where a or b might be 0 (which doesn't have bit_length directly applicable)\r"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 550,
                "PatchRowcode": "+        a_bits = a.bit_length() if a != 0 and hasattr(a, 'bit_length') else 0\r"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 551,
                "PatchRowcode": "+        b_bits = b.bit_length() if b != 0 and hasattr(b, 'bit_length') else 0\r"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 552,
                "PatchRowcode": "+        bit_length: int = max(a_bits, b_bits, 8)  # Minimum 8 bits\r"
            },
            "8": {
                "beforePatchRowNumber": 550,
                "afterPatchRowNumber": 553,
                "PatchRowcode": "         byte_length: int = (bit_length + 7) // 8\r"
            },
            "9": {
                "beforePatchRowNumber": 551,
                "afterPatchRowNumber": 554,
                "PatchRowcode": "         a_bytes: bytes = a.to_bytes(byte_length, byteorder=\"big\")\r"
            },
            "10": {
                "beforePatchRowNumber": 552,
                "afterPatchRowNumber": 555,
                "PatchRowcode": "         b_bytes: bytes = b.to_bytes(byte_length, byteorder=\"big\")\r"
            },
            "11": {
                "beforePatchRowNumber": 588,
                "afterPatchRowNumber": 591,
                "PatchRowcode": "     \"\"\"\r"
            },
            "12": {
                "beforePatchRowNumber": 589,
                "afterPatchRowNumber": 592,
                "PatchRowcode": "     if isinstance(n, (int, gmpy2.mpz)):\r"
            },
            "13": {
                "beforePatchRowNumber": 590,
                "afterPatchRowNumber": 593,
                "PatchRowcode": "         bit_length: int = (\r"
            },
            "14": {
                "beforePatchRowNumber": 591,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            n.bit_length() if hasattr(n, \"bit_length\") else gmpy2.mpz(n).bit_length()\r"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 594,
                "PatchRowcode": "+            n.bit_length() if hasattr(n, \"bit_length\") and n != 0 else \r"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 595,
                "PatchRowcode": "+            gmpy2.mpz(n).bit_length() if n != 0 else 0\r"
            },
            "17": {
                "beforePatchRowNumber": 592,
                "afterPatchRowNumber": 596,
                "PatchRowcode": "         )\r"
            },
            "18": {
                "beforePatchRowNumber": 593,
                "afterPatchRowNumber": 597,
                "PatchRowcode": "     else:\r"
            },
            "19": {
                "beforePatchRowNumber": 594,
                "afterPatchRowNumber": 598,
                "PatchRowcode": "         bit_length = n  # Assume n is already a bit length\r"
            },
            "20": {
                "beforePatchRowNumber": 775,
                "afterPatchRowNumber": 779,
                "PatchRowcode": "             b: Any\r"
            },
            "21": {
                "beforePatchRowNumber": 776,
                "afterPatchRowNumber": 780,
                "PatchRowcode": "             a, b = args\r"
            },
            "22": {
                "beforePatchRowNumber": 777,
                "afterPatchRowNumber": 781,
                "PatchRowcode": "             a_bits: int = (\r"
            },
            "23": {
                "beforePatchRowNumber": 778,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                a.bit_length()\r"
            },
            "24": {
                "beforePatchRowNumber": 779,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                if hasattr(a, \"bit_length\")\r"
            },
            "25": {
                "beforePatchRowNumber": 780,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                else gmpy2.mpz(a).bit_length()\r"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 782,
                "PatchRowcode": "+                a.bit_length() if hasattr(a, \"bit_length\") and a != 0 \r"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 783,
                "PatchRowcode": "+                else gmpy2.mpz(a).bit_length() if a != 0 \r"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 784,
                "PatchRowcode": "+                else 0\r"
            },
            "29": {
                "beforePatchRowNumber": 781,
                "afterPatchRowNumber": 785,
                "PatchRowcode": "             )\r"
            },
            "30": {
                "beforePatchRowNumber": 782,
                "afterPatchRowNumber": 786,
                "PatchRowcode": "             b_bits: int = (\r"
            },
            "31": {
                "beforePatchRowNumber": 783,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                b.bit_length()\r"
            },
            "32": {
                "beforePatchRowNumber": 784,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                if hasattr(b, \"bit_length\")\r"
            },
            "33": {
                "beforePatchRowNumber": 785,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                else gmpy2.mpz(b).bit_length()\r"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 787,
                "PatchRowcode": "+                b.bit_length() if hasattr(b, \"bit_length\") and b != 0\r"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 788,
                "PatchRowcode": "+                else gmpy2.mpz(b).bit_length() if b != 0\r"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 789,
                "PatchRowcode": "+                else 0\r"
            },
            "37": {
                "beforePatchRowNumber": 786,
                "afterPatchRowNumber": 790,
                "PatchRowcode": "             )\r"
            },
            "38": {
                "beforePatchRowNumber": 787,
                "afterPatchRowNumber": 791,
                "PatchRowcode": "             result_bits = a_bits + b_bits  # Multiplication roughly adds bit lengths\r"
            },
            "39": {
                "beforePatchRowNumber": 788,
                "afterPatchRowNumber": 792,
                "PatchRowcode": "             estimated_bytes = estimate_mpz_size(result_bits)\r"
            },
            "40": {
                "beforePatchRowNumber": 1016,
                "afterPatchRowNumber": 1020,
                "PatchRowcode": "             logger.error(detailed_message)\r"
            },
            "41": {
                "beforePatchRowNumber": 1017,
                "afterPatchRowNumber": 1021,
                "PatchRowcode": " \r"
            },
            "42": {
                "beforePatchRowNumber": 1018,
                "afterPatchRowNumber": 1022,
                "PatchRowcode": "             # Use sanitization function if provided\r"
            },
            "43": {
                "beforePatchRowNumber": 1019,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if callable(sanitize_error_func):\r"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1023,
                "PatchRowcode": "+            if sanitize_error_func is not None and callable(sanitize_error_func):\r"
            },
            "45": {
                "beforePatchRowNumber": 1020,
                "afterPatchRowNumber": 1024,
                "PatchRowcode": "                 sanitized_message: str = sanitize_error_func(message, detailed_message)\r"
            },
            "46": {
                "beforePatchRowNumber": 1021,
                "afterPatchRowNumber": 1025,
                "PatchRowcode": "                 raise SecurityError(sanitized_message)\r"
            },
            "47": {
                "beforePatchRowNumber": 1022,
                "afterPatchRowNumber": 1026,
                "PatchRowcode": "             else:\r"
            },
            "48": {
                "beforePatchRowNumber": 1083,
                "afterPatchRowNumber": 1087,
                "PatchRowcode": "                 logger.error(detailed_message)\r"
            },
            "49": {
                "beforePatchRowNumber": 1084,
                "afterPatchRowNumber": 1088,
                "PatchRowcode": " \r"
            },
            "50": {
                "beforePatchRowNumber": 1085,
                "afterPatchRowNumber": 1089,
                "PatchRowcode": "                 # Use sanitization function if provided, otherwise use the generic message\r"
            },
            "51": {
                "beforePatchRowNumber": 1086,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                if callable(sanitize_error_func):\r"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1090,
                "PatchRowcode": "+                if sanitize_error_func is not None and callable(sanitize_error_func):\r"
            },
            "53": {
                "beforePatchRowNumber": 1087,
                "afterPatchRowNumber": 1091,
                "PatchRowcode": "                     sanitized_message = sanitize_error_func(message, detailed_message)\r"
            },
            "54": {
                "beforePatchRowNumber": 1088,
                "afterPatchRowNumber": 1092,
                "PatchRowcode": "                     raise SecurityError(sanitized_message)\r"
            },
            "55": {
                "beforePatchRowNumber": 1089,
                "afterPatchRowNumber": 1093,
                "PatchRowcode": "                 else:\r"
            },
            "56": {
                "beforePatchRowNumber": 1103,
                "afterPatchRowNumber": 1107,
                "PatchRowcode": "         message: str = \"Security validation process failed\"\r"
            },
            "57": {
                "beforePatchRowNumber": 1104,
                "afterPatchRowNumber": 1108,
                "PatchRowcode": "         logger.error(detailed_message)\r"
            },
            "58": {
                "beforePatchRowNumber": 1105,
                "afterPatchRowNumber": 1109,
                "PatchRowcode": " \r"
            },
            "59": {
                "beforePatchRowNumber": 1106,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if callable(sanitize_error_func):\r"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1110,
                "PatchRowcode": "+        if sanitize_error_func is not None and callable(sanitize_error_func):\r"
            },
            "61": {
                "beforePatchRowNumber": 1107,
                "afterPatchRowNumber": 1111,
                "PatchRowcode": "             sanitized_message: str = sanitize_error_func(message, detailed_message)\r"
            },
            "62": {
                "beforePatchRowNumber": 1108,
                "afterPatchRowNumber": 1112,
                "PatchRowcode": "             raise SecurityError(sanitized_message) from e\r"
            },
            "63": {
                "beforePatchRowNumber": 1109,
                "afterPatchRowNumber": 1113,
                "PatchRowcode": "         else:\r"
            }
        },
        "frontPatchFile": [
            "\"\"\"\r",
            "Post-Quantum Secure Feldman's Verifiable Secret Sharing (VSS) Implementation\r",
            "\r",
            "Version 0.8.0b2\r",
            "Developed in 2025 by David Osipov\r",
            "Licensed under the MIT License\r",
            "\r",
            "This module provides a secure, production-ready implementation of Feldman's VSS scheme\r",
            "with post-quantum security by design. It enhances Shamir's Secret Sharing with\r",
            "mathematical verification capabilities while remaining resistant to quantum attacks\r",
            "through hash-based commitments.\r",
            "\r",
            "Key Features:\r",
            "\r",
            "1.  **Post-Quantum Security:** Exclusively uses hash-based commitments (BLAKE3 or SHA3-256)\r",
            "    for proven resistance to quantum computer attacks. No reliance on discrete logarithm\r",
            "    problems.\r",
            "2.  **Secure Group Operations:** Employs the `CyclicGroup` class, which uses `gmpy2` for\r",
            "    efficient and secure modular arithmetic. Includes optimized exponentiation\r",
            "    (with precomputation and a thread-safe LRU cache) and multi-exponentiation.\r",
            "3.  **Efficient Batch Verification:** `batch_verify_shares` provides optimized verification\r",
            "    of multiple shares against the same commitments, significantly improving performance\r",
            "    for large numbers of shares.\r",
            "4.  **Serialization and Deserialization:** `serialize_commitments` and\r",
            "    `deserialize_commitments` methods provide secure serialization and deserialization of\r",
            "    commitment data, including checksums for integrity verification and handling of\r",
            "    extra entropy for low-entropy secrets.\r",
            "5.  **Comprehensive Validation and Error Handling:** Extensive input validation and error\r",
            "    handling throughout the code to prevent misuse and ensure robustness.  Includes\r",
            "    detailed error messages with forensic data collection for debugging and security\r",
            "    analysis. Sanitized errors are used by default to prevent information leakage.\r",
            "6.  **Fault Injection Countermeasures:** Uses redundant computation (`secure_redundant_execution`)\r",
            "    and constant-time comparisons (`constant_time_compare`) to mitigate fault injection attacks.\r",
            "7.  **Zero-Knowledge Proofs:** Supports the creation and verification of zero-knowledge\r",
            "    proofs of polynomial knowledge, allowing a prover to demonstrate knowledge of the\r",
            "    secret polynomial without revealing the coefficients.\r",
            "8.  **Share Refreshing:** Implements an enhanced version of Chen & Lindell's Protocol 5\r",
            "    for secure share refreshing, with improved Byzantine fault tolerance, adaptive\r",
            "    quorum-based Byzantine detection, and optimized verification.\r",
            "9.  **Integration with Pedersen VSS:** Includes helper functions (`integrate_with_pedersen`,\r",
            "    `create_dual_commitment_proof`, `verify_dual_commitments`) for combining Feldman VSS\r",
            "    with Pedersen VSS, providing both binding and hiding properties.\r",
            "10. **Configurable Parameters:** The `VSSConfig` class allows customization of security\r",
            "    parameters, including the prime bit length, safe prime usage, hash algorithm\r",
            "    (BLAKE3 or SHA3-256), and LRU cache size.\r",
            "11. **Deterministic Hashing:** Guarantees deterministic commitment generation across different\r",
            "    platforms and execution environments by using fixed-length byte representations for\r",
            "    integers in hash calculations.\r",
            "12. **Thread-Safe LRU Cache:** Employs a `SafeLRUCache` for efficient and thread-safe caching\r",
            "    of exponentiation results, with bounded memory usage.\r",
            "13. **Memory Safety:**  Includes a `MemoryMonitor` and `check_memory_safety` to prevent\r",
            "    excessive memory allocation, mitigating potential denial-of-service vulnerabilities.\r",
            "\r",
            "System Requirements:\r",
            "\r",
            "-   For threshold (t) = 50 with 4096-bit values: At least 2GB RAM\r",
            "-   For threshold (t) = 100 with 4096-bit values: At least 4GB RAM\r",
            "-   For threshold (t) > 100 with 4096-bit values: Consider increasing RAM or reducing parameters\r",
            "\r",
            "The memory requirements scale approximately as O(t\u00b2 * bit_length).\r",
            "\r",
            "**Python Version Compatibility:**\r",
            "\r",
            "-   **Minimum Supported Version: Python 3.8**\r",
            "-   **Recommended Version: Python 3.13.2 (or later)**\r",
            "\r",
            "While the library is designed to be compatible with Python 3.8 and above, using\r",
            "the latest stable release (currently 3.13.2) is highly recommended for optimal\r",
            "performance, security, and access to the latest language features.\r",
            "\r",
            "This library takes advantage of features introduced in various Python versions:\r",
            "\r",
            "-   **Python 3.6:** The `secrets` module, used for cryptographically secure random\r",
            "    number generation, was introduced in Python 3.6.\r",
            "-   **Python 3.7:** Data classes (`@dataclass` decorator), used in `VSSConfig`, were\r",
            "    introduced, simplifying class creation and reducing boilerplate code. Dictionaries\r",
            "    became ordered by insertion, providing more predictable behavior.\r",
            "-   **Python 3.8:**  `typing.TypedDict`, used for defining the structure of dictionaries\r",
            "    holding proof and verification data, became available. This version also introduced\r",
            "    positional-only parameters (though not currently used in this library, they represent\r",
            "    good practice for future development).  Audit hooks (PEP 578) were introduced,\r",
            "    allowing for better security monitoring (though not directly used by the library's core logic).\r",
            "-   **Python 3.9 and later:**  Continue to offer improvements in performance, type hinting,\r",
            "    and general security.  While not *strictly* required, using the newest Python\r",
            "    versions is generally beneficial. Python 3.13 specifically removed crypt module and\r",
            "    improved SSL, but does not have new cryptographic features.\r",
            "\r",
            "Security Considerations:\r",
            "\r",
            "-   Always uses at least 4096-bit prime fields for post-quantum security (configurable).\r",
            "-   Strongly recommends using safe primes (where (p-1)/2 is also prime) for enhanced security.\r",
            "-   Defaults to BLAKE3 for cryptographic hashing (faster and more secure than SHA3-256),\r",
            "    but falls back to SHA3-256 if BLAKE3 is not available.\r",
            "-   Designed for seamless integration with Shamir's Secret Sharing implementation.\r",
            "-   Implements countermeasures against timing attacks, fault injection attacks, and\r",
            "    Byzantine behavior.\r",
            "-   Uses cryptographically secure random number generation (secrets module) where needed.\r",
            "-   Provides detailed error messages for debugging and security analysis\r",
            "    (`sanitize_errors: bool = True` needs to be turned to `False`).\r",
            "\r",
            "Known Security Vulnerabilities:\r",
            "\r",
            "This library contains several timing side-channel and fault injection vulnerabilities that cannot be adequately addressed in pure Python:\r",
            "\r",
            "1.  **Timing Side-Channels in Matrix Operations**: Functions like `_find_secure_pivot` and `_secure_matrix_solve` cannot guarantee constant-time execution in Python, potentially leaking secret information.\r",
            "\r",
            "2.  **Non-Constant-Time Comparison**: The `constant_time_compare` function does not provide true constant-time guarantees due to Python's execution model.\r",
            "\r",
            "**Status**: These vulnerabilities require implementation in a lower-level language like Rust to fix properly. The library should be considered experimental until these issues are addressed.\r",
            "\r",
            "**Planned Resolution**: Future versions will integrate with Rust components for security-critical operations.\r",
            "\r",
            "**False-Positive Vulnerabilities:**\r",
            "\r",
            "1.  **Use of `random.Random()` in `_refresh_shares_additive`:** The code uses `random.Random()` seeded with cryptographically strong material (derived from a master secret and a party ID) within the `_refresh_shares_additive` function. While `random.Random()` is *not* generally suitable for cryptographic purposes, its use *here* is intentional and secure. The purpose is to generate *deterministic* but *unpredictable* values for the zero-sharing polynomials. The security comes from the cryptographically strong seed, *not* from the `random.Random()` algorithm itself. This is a deliberate design choice to enable verification and reduce communication overhead in the share refreshing protocol. It is *not* a source of cryptographic weakness.\r",
            "\r",
            "Note: This implementation is fully compatible with the ShamirSecretSharing class in\r",
            "the main module and is optimized to work in synergy with Pedersen VSS.\r",
            "\r",
            "Repository: https://github.com/DavidOsipov/PostQuantum-Feldman-VSS\r",
            "PyPI: https://pypi.org/project/PostQuantum-Feldman-VSS/\r",
            "\r",
            "Developer: David Osipov\r",
            "    Github Profile: https://github.com/DavidOsipov\r",
            "    Email: personal@david-osipov.vision\r",
            "    PGP key: https://openpgpkey.david-osipov.vision/.well-known/openpgpkey/david-osipov.vision/D3FC4983E500AC3F7F136EB80E55C4A47454E82E.asc\r",
            "    PGP fingerprint: D3FC 4983 E500 AC3F 7F13 6EB8 0E55 C4A4 7454 E82E\r",
            "    Website: https://david-osipov.vision\r",
            "    LinkedIn: https://www.linkedin.com/in/david-osipov/\r",
            "    \"\"\"\r",
            "\r",
            "# /// script\r",
            "# requires-python = \">=3.8\"\r",
            "# dependencies = [\r",
            "#   \"gmpy2 == 2.2.1\",\r",
            "#   \"msgpack == 1.1.0\",\r",
            "#   \"blake3 == 1.0.4; platform_system != 'Emscripten'\",\r",
            "#   \"psutil == 7.0.0; os_name != 'Emscripten'\" # Optional dependency\r",
            "# ]\r",
            "# ///\r",
            "\r",
            "# mypy: disallow-untyped-defs=False\r",
            "# mypy: disallow-incomplete-defs=False\r",
            "# pyright: reportOptionalMemberAccess=false\r",
            "\r",
            "import hashlib\r",
            "import logging\r",
            "import random\r",
            "import secrets\r",
            "import threading\r",
            "import time\r",
            "import traceback\r",
            "import warnings\r",
            "from base64 import urlsafe_b64decode, urlsafe_b64encode\r",
            "from collections import OrderedDict\r",
            "from typing import (\r",
            "    Any, Dict, List, Tuple, Optional, Union,\r",
            "    Callable, TypeVar, Generic, NoReturn, Type, Set, TypedDict,\r",
            ")\r",
            "from dataclasses import dataclass\r",
            "import msgpack\r",
            "\r",
            "# Import BLAKE3 for cryptographic hashing (faster and more secure than SHA3-256)\r",
            "import importlib.util\r",
            "\r",
            "HAS_BLAKE3 = importlib.util.find_spec(\"blake3\") is not None\r",
            "if HAS_BLAKE3:\r",
            "    import blake3\r",
            "else:\r",
            "    warnings.warn(\r",
            "        \"BLAKE3 library not found. Falling back to SHA3-256. \"\r",
            "        \"Install BLAKE3 with: pip install blake3\",\r",
            "        ImportWarning,\r",
            "    )\r",
            "\r",
            "# Import gmpy2 - now a strict requirement\r",
            "try:\r",
            "    import gmpy2\r",
            "except ImportError as exc:\r",
            "    raise ImportError(\r",
            "        \"gmpy2 library is required for this module. \"\r",
            "        \"Install gmpy2 with: pip install gmpy2\"\r",
            "    ) from exc\r",
            "\r",
            "logging.basicConfig(\r",
            "    level=logging.WARNING,\r",
            "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r",
            "    handlers=[logging.FileHandler(\"feldman_vss.log\"), logging.StreamHandler()],\r",
            ")\r",
            "logger = logging.getLogger(\"feldman_vss\")\r",
            "\r",
            "# Security parameters\r",
            "VSS_VERSION = \"VSS-0.8.0b2\"\r",
            "# Minimum size for secure prime fields for post-quantum security\r",
            "MIN_PRIME_BITS = 4096\r",
            "\r",
            "# Safe primes cache - these are primes p where (p-1)/2 is also prime\r",
            "# Using larger primes for post-quantum security\r",
            "SAFE_PRIMES = {\r",
            "    # Mimimal safe prime for 5 years is 3072. The recommended is 4096. These primes are from RFC 3526. More Modular Exponential (MODP) Diffie-Hellman groups for Internet Key Exchange (IKE).\r",
            "    3072: int(\r",
            "        \"FFFFFFFFFFFFFFFFC90FDAA22168C234C4C6628B80DC1CD129024E088A67CC74020BBEA63B139B22514A08798E3404DDEF9519B3CD3A431B302B0A6DF25F14374FE1356D6D51C245E485B576625E7EC6F44C42E9A637ED6B0BFF5CB6F406B7EDEE386BFB5A899FA5AE9F24117C4B1FE649286651ECE45B3DC2007CB8A163BF0598DA48361C55D39A69163FA8FD24CF5F83655D23DCA3AD961C62F356208552BB9ED529077096966D670C354E4ABC9804F1746C08CA18217C32905E462E36CE3BE39E772C180E86039B2783A2EC07A28FB5C55DF06F4C52C9DE2BCBF6955817183995497CEA956AE515D2261898FA051015728E5A8AAAC42DAD33170D04507A33A85521ABDF1CBA64ECFB850458DBEF0A8AEA71575D060C7DB3970F85A6E1E4C7ABF5AE8CDB0933D71E8C94E04A25619DCEE3D2261AD2EE6BF12FFA06D98A0864D87602733EC86A64521F2B18177B200CBBE117577A615D6C770988C0BAD946E208E24FA074E5AB3143DB5BFCE0FD108E4B82D120A93AD2CAFFFFFFFFFFFFFFFF\",\r",
            "        16,\r",
            "    ),\r",
            "    4096: int(\r",
            "        \"FFFFFFFFFFFFFFFFC90FDAA22168C234C4C6628B80DC1CD129024E088A67CC74020BBEA63B139B22514A08798E3404DDEF9519B3CD3A431B302B0A6DF25F14374FE1356D6D51C245E485B576625E7EC6F44C42E9A637ED6B0BFF5CB6F406B7EDEE386BFB5A899FA5AE9F24117C4B1FE649286651ECE45B3DC2007CB8A163BF0598DA48361C55D39A69163FA8FD24CF5F83655D23DCA3AD961C62F356208552BB9ED529077096966D670C354E4ABC9804F1746C08CA18217C32905E462E36CE3BE39E772C180E86039B2783A2EC07A28FB5C55DF06F4C52C9DE2BCBF6955817183995497CEA956AE515D2261898FA051015728E5A8AAAC42DAD33170D04507A33A85521ABDF1CBA64ECFB850458DBEF0A8AEA71575D060C7DB3970F85A6E1E4C7ABF5AE8CDB0933D71E8C94E04A25619DCEE3D2261AD2EE6BF12FFA06D98A0864D87602733EC86A64521F2B18177B200CBBE117577A615D6C770988C0BAD946E208E24FA074E5AB3143DB5BFCE0FD108E4B82D120A92108011A723C12A787E6D788719A10BDBA5B2699C327186AF4E23C1A946834B6150BDA2583E9CA2AD44CE8DBBBC2DB04DE8EF92E8EFC141FBECAA6287C59474E6BC05D99B2964FA090C3A2233BA186515BE7ED1F612970CEE2D7AFB81BDD762170481CD0069127D5B05AA993B4EA988D8FDDC186FFB7DC90A6C08F4DF435C934063199FFFFFFFFFFFFFFFF\",\r",
            "        16,\r",
            "    ),\r",
            "    6144: int(\r",
            "        \"FFFFFFFFFFFFFFFFC90FDAA22168C234C4C6628B80DC1CD129024E088A67CC74020BBEA63B139B22514A08798E3404DDEF9519B3CD3A431B302B0A6DF25F14374FE1356D6D51C245E485B576625E7EC6F44C42E9A637ED6B0BFF5CB6F406B7EDEE386BFB5A899FA5AE9F24117C4B1FE649286651ECE45B3DC2007CB8A163BF0598DA48361C55D39A69163FA8FD24CF5F83655D23DCA3AD961C62F356208552BB9ED529077096966D670C354E4ABC9804F1746C08CA18217C32905E462E36CE3BE39E772C180E86039B2783A2EC07A28FB5C55DF06F4C52C9DE2BCBF6955817183995497CEA956AE515D2261898FA051015728E5A8AAAC42DAD33170D04507A33A85521ABDF1CBA64ECFB850458DBEF0A8AEA71575D060C7DB3970F85A6E1E4C7ABF5AE8CDB0933D71E8C94E04A25619DCEE3D2261AD2EE6BF12FFA06D98A0864D87602733EC86A64521F2B18177B200CBBE117577A615D6C770988C0BAD946E208E24FA074E5AB3143DB5BFCE0FD108E4B82D120A92108011A723C12A787E6D788719A10BDBA5B2699C327186AF4E23C1A946834B6150BDA2583E9CA2AD44CE8DBBBC2DB04DE8EF92E8EFC141FBECAA6287C59474E6BC05D99B2964FA090C3A2233BA186515BE7ED1F612970CEE2D7AFB81BDD762170481CD0069127D5B05AA993B4EA988D8FDDC186FFB7DC90A6C08F4DF435C93402849236C3FAB4D27C7026C1D4DCB2602646DEC9751E763DBA37BDF8FF9406AD9E530EE5DB382F413001AEB06A53ED9027D831179727B0865A8918DA3EDBEBCF9B14ED44CE6CBACED4BB1BDB7F1447E6CC254B332051512BD7AF426FB8F401378CD2BF5983CA01C64B92ECF032EA15D1721D03F482D7CE6E74FEF6D55E702F46980C82B5A84031900B1C9E59E7C97FBEC7E8F323A97A7E36CC88BE0F1D45B7FF585AC54BD407B22B4154AACCC8F6D7EBF48E1D814CC5ED20F8037E0A79715EEF29BE32806A1D58BB7C5DA76F550AA3D8A1FBFF0EB19CCB1A313D55CDA56C9EC2EF29632387FE8D76E3C0468043E8F663F4860EE12BF2D5B0B7474D6E694F91E6DCC4024FFFFFFFFFFFFFFFF\",\r",
            "        16,\r",
            "    ),\r",
            "    8192: int(\r",
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r",
            "        16,\r",
            "    ),\r",
            "}\r",
            "\r",
            "\r",
            "# Type definitions\r",
            "ByzantineEvidenceDict = TypedDict('ByzantineEvidenceDict', {\r",
            "    'type': str,\r",
            "    'evidence': List[Dict[str, Any]],\r",
            "    'timestamp': int,\r",
            "    'signature': str\r",
            "})\r",
            "FieldElement = Union[int, \"gmpy2.mpz\"]  # Integer field elements\r",
            "SharePoint = Tuple[FieldElement, FieldElement]  # (x, y) coordinate\r",
            "ShareDict = Dict[int, SharePoint]  # Maps participant ID to share\r",
            "Randomizer = FieldElement  # Randomizer values for commitments\r",
            "InvalidityProofDict = TypedDict('InvalidityProofDict', {\r",
            "    'party_id': int,\r",
            "    'participant_id': int,\r",
            "    'share_x': FieldElement,\r",
            "    'share_y': FieldElement,\r",
            "    'expected_commitment': FieldElement,\r",
            "    'actual_commitment': FieldElement,\r",
            "    'combined_randomizer': FieldElement,\r",
            "    'timestamp': int,\r",
            "    'signature': str\r",
            "})\r",
            "VerificationDataDict = TypedDict('VerificationDataDict', {\r",
            "    'original_shares_count': int,\r",
            "    'threshold': int,\r",
            "    'zero_commitment_count': int,\r",
            "    'timestamp': int,\r",
            "    'protocol': str,\r",
            "    'verification_method': str,\r",
            "    'hash_based': bool,\r",
            "    'verification_summary': Dict[str, Any],\r",
            "    'seed_fingerprint': str,\r",
            "    'verification_proofs': Dict[int, Dict[int, Any]]\r",
            "})\r",
            "HashCommitment = Tuple[FieldElement, Randomizer, Optional[bytes]]  # (hash, randomizer, entropy)\r",
            "CommitmentList = List[HashCommitment]  # List of commitments\r",
            "ProofDict = TypedDict('ProofDict', {\r",
            "    'blinding_commitments': List[Tuple[FieldElement, FieldElement]],\r",
            "    'challenge': FieldElement,\r",
            "    'responses': List[FieldElement],\r",
            "    'commitment_randomizers': List[FieldElement],\r",
            "    'blinding_randomizers': List[FieldElement],\r",
            "    'timestamp': int\r",
            "})\r",
            "VerificationResult = Tuple[bool, Dict[int, bool]]\r",
            "RefreshingResult = Tuple[ShareDict, CommitmentList, Dict[str, Any]]\r",
            "\r",
            "# Type Aliases for Complex Types\r",
            "HashFunc = Callable[[bytes], Any]\r",
            "RedundantExecutorFunc = Callable[..., Any]\r",
            "\r",
            "# Type Aliases for Complex Types\r",
            "HashFunc = Callable[[bytes], Any]\r",
            "RedundantExecutorFunc = Callable[..., Any]\r",
            "\r",
            "# Custom warning for security issues\r",
            "class SecurityWarning(Warning):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Warning for potentially insecure configurations or operations\r",
            "    \"\"\"\r",
            "\r",
            "# Other exception classes\r",
            "class SecurityError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised for security-related issues in VSS\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"critical\", timestamp: Optional[int] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        super().__init__(message)\r",
            "class SerializationError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised for serialization or deserialization errors with enhanced \r",
            "        forensic data collection.\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"critical\", timestamp: Optional[int] = None,\r",
            "                 data_format: Optional[str] = None, checksum_info: Optional[Dict[str, Any]] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        self.data_format = data_format  # Stores format information about the serialized data\r",
            "        self.checksum_info = checksum_info  # Stores checksum validation details if applicable\r",
            "        super().__init__(message)\r",
            "        \r",
            "    def get_forensic_data(self) -> Dict[str, Any]:\r",
            "        \"\"\"Return all forensic information as a dictionary for logging or analysis\"\"\"\r",
            "        return {\r",
            "            \"message\": self.message,\r",
            "            \"detailed_info\": self.detailed_info,\r",
            "            \"severity\": self.severity,\r",
            "            \"timestamp\": self.timestamp,\r",
            "            \"data_format\": self.data_format,\r",
            "            \"checksum_info\": self.checksum_info,\r",
            "            \"error_type\": \"SerializationError\"\r",
            "        }\r",
            "\r",
            "class VerificationError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised when share verification fails with \r",
            "        comprehensive evidence collection.\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"critical\", timestamp: Optional[int] = None,\r",
            "                 share_info: Optional[Dict[str, Any]] = None, \r",
            "                 commitment_info: Optional[Dict[str, Any]] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        self.share_info = share_info  # Information about the share that failed verification\r",
            "        self.commitment_info = commitment_info  # Information about the commitments used\r",
            "        super().__init__(message)\r",
            "        \r",
            "    def get_forensic_data(self) -> Dict[str, Any]:\r",
            "        \"\"\"Return all forensic information as a dictionary for logging or analysis\"\"\"\r",
            "        return {\r",
            "            \"message\": self.message,\r",
            "            \"detailed_info\": self.detailed_info,\r",
            "            \"severity\": self.severity,\r",
            "            \"timestamp\": self.timestamp,\r",
            "            \"share_info\": self.share_info,\r",
            "            \"commitment_info\": self.commitment_info,\r",
            "            \"error_type\": \"VerificationError\"\r",
            "        }\r",
            "\r",
            "\r",
            "class ParameterError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised for invalid parameters in VSS with enhanced\r",
            "        parameter validation data.\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"error\", timestamp: Optional[int] = None,\r",
            "                 parameter_name: Optional[str] = None, \r",
            "                 parameter_value: Optional[Any] = None,\r",
            "                 expected_type: Optional[str] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        self.parameter_name = parameter_name  # Name of the invalid parameter\r",
            "        self.parameter_value = parameter_value  # Value of the invalid parameter\r",
            "        self.expected_type = expected_type  # Expected type or value range\r",
            "        super().__init__(message)\r",
            "        \r",
            "    def get_forensic_data(self) -> Dict[str, Any]:\r",
            "        \"\"\"Return all forensic information as a dictionary for logging or analysis\"\"\"\r",
            "        return {\r",
            "            \"message\": self.message,\r",
            "            \"detailed_info\": self.detailed_info,\r",
            "            \"severity\": self.severity,\r",
            "            \"timestamp\": self.timestamp,\r",
            "            \"parameter_name\": self.parameter_name,\r",
            "            \"parameter_value\": str(self.parameter_value),  # Convert to string to ensure serialization\r",
            "            \"expected_type\": self.expected_type,\r",
            "            \"error_type\": \"ParameterError\"\r",
            "        }\r",
            "\r",
            "@dataclass\r",
            "class VSSConfig:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Configuration parameters for Post-Quantum Secure Feldman VSS\r",
            "\r",
            "    Arguments:\r",
            "        prime_bits (int): Number of bits for the prime modulus. Default is 4096 for post-quantum security.\r",
            "        safe_prime (bool): Whether to use a safe prime (where (p-1)/2 is also prime). Default is True.\r",
            "        secure_serialization (bool): Whether to use a secure serialization format. Default is True.\r",
            "        use_blake3 (bool): Whether to use BLAKE3 for hashing (falls back to SHA3-256 if unavailable). Default is True.\r",
            "        cache_size (int): The size of the LRU cache for exponentiation. Default is 128.\r",
            "        sanitize_errors (bool): Whether to sanitize error messages. Default is True.\r",
            "\r",
            "    Inputs:\r",
            "        None\r",
            "\r",
            "    Outputs:\r",
            "        None\r",
            "    \"\"\"\r",
            "\r",
            "    prime_bits: int = 4096  # Post-quantum security default\r",
            "    safe_prime: bool = True  # Always use safe primes for better security\r",
            "    secure_serialization: bool = True\r",
            "    use_blake3: bool = (\r",
            "        True  # Whether to use BLAKE3 (falls back to SHA3-256 if unavailable)\r",
            "    )\r",
            "    cache_size: int = 128  # Default cache size for exponentiation results\r",
            "    sanitize_errors: bool = True  # Set to False in debug env for detailed errors\r",
            "\r",
            "    def __post_init__(self) -> None:\r",
            "        # Security check - enforce minimum prime size for post-quantum security\r",
            "        if self.prime_bits < MIN_PRIME_BITS:\r",
            "            warnings.warn(\r",
            "                f\"Using prime size less than {MIN_PRIME_BITS} bits is insecure against quantum attacks. \"\r",
            "                f\"Increasing to {MIN_PRIME_BITS} bits for post-quantum security.\",\r",
            "                SecurityWarning,\r",
            "            )\r",
            "            self.prime_bits = MIN_PRIME_BITS\r",
            "\r",
            "        if self.use_blake3 and not HAS_BLAKE3:\r",
            "            warnings.warn(\r",
            "                \"BLAKE3 requested but not installed. Falling back to SHA3-256. \"\r",
            "                \"Install BLAKE3 with: pip install blake3\",\r",
            "                RuntimeWarning,\r",
            "            )\r",
            "\r",
            "\r",
            "# Define type variables for our SafeLRUCache\r",
            "K = TypeVar('K')  # Key type\r",
            "V = TypeVar('V')  # Value type\r",
            "\r",
            "class SafeLRUCache(Generic[K, V]):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Thread-safe LRU cache implementation for efficient caching with memory constraints.\r",
            "\r",
            "    Arguments:\r",
            "        capacity (int): Maximum number of items to store in the cache.\r",
            "    \r",
            "    Type Parameters:\r",
            "        K: Type of the keys in the cache\r",
            "        V: Type of the values in the cache\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(self, capacity: int) -> None:\r",
            "        self.capacity: int = capacity\r",
            "        self.cache: OrderedDict[K, V] = OrderedDict()\r",
            "        self.lock: threading.RLock = threading.RLock()  # Use RLock for compatibility with existing code\r",
            "\r",
            "    def get(self, key: K) -> Optional[V]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Get an item from the cache, moving it to most recently used position.\r",
            "\r",
            "        Arguments:\r",
            "            key (K): The key to retrieve.\r",
            "\r",
            "        Returns:\r",
            "            Optional[V]: The value associated with the key, or None if not found.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            if key in self.cache:\r",
            "                # Move to the end (most recently used)\r",
            "                value: V = self.cache.pop(key)\r",
            "                self.cache[key] = value\r",
            "                return value\r",
            "            return None\r",
            "\r",
            "    def put(self, key: K, value: V) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Add an item to the cache, evicting least recently used item if necessary.\r",
            "\r",
            "        Arguments:\r",
            "            key (K): The key to store.\r",
            "            value (V): The value to associate with the key.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            if key in self.cache:\r",
            "                # Remove existing item first\r",
            "                self.cache.pop(key)\r",
            "            elif len(self.cache) >= self.capacity:\r",
            "                # Remove the first item (least recently used)\r",
            "                self.cache.popitem(last=False)\r",
            "            # Add new item\r",
            "            self.cache[key] = value\r",
            "\r",
            "    def clear(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Clear the cache.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            self.cache.clear()\r",
            "\r",
            "    def __len__(self) -> int:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Return number of items in the cache.\r",
            "\r",
            "        Outputs:\r",
            "            int: The number of items in the cache.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            return len(self.cache)\r",
            "\r",
            "\r",
            "# --- HELPER FUNCTIONS ---\r",
            "\r",
            "HashFunc = Callable[[bytes], Any]\r",
            "RedundantExecutorFunc = Callable[..., Any]\r",
            "\r",
            "def constant_time_compare(a: Union[int, str, bytes], b: Union[int, str, bytes]) -> bool:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Compare two values in constant time to prevent timing attacks.\r",
            "\r",
            "        This implementation handles integers, strings, and bytes with consistent\r",
            "        processing time regardless of where differences occur.\r",
            "\r",
            "    Arguments:\r",
            "        a (int, str, or bytes): First value to compare.\r",
            "        b (int, str, or bytes): Second value to compare.\r",
            "\r",
            "    Inputs:\r",
            "        a: First value to compare (int, str, or bytes)\r",
            "        b: Second value to compare (int, str, or bytes)\r",
            "\r",
            "    Outputs:\r",
            "        bool: True if values are equal, False otherwise.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if isinstance(a, gmpy2.mpz):\r",
            "        a = int(a)\r",
            "    if isinstance(b, gmpy2.mpz):\r",
            "        b = int(b)\r",
            "        \r",
            "    # Convert to bytes for consistent handling\r",
            "    if isinstance(a, int) and isinstance(b, int):\r",
            "        # For integers, ensure same bit length with padding\r",
            "        bit_length: int = max(a.bit_length(), b.bit_length(), 8)  # Minimum 8 bits\r",
            "        byte_length: int = (bit_length + 7) // 8\r",
            "        a_bytes: bytes = a.to_bytes(byte_length, byteorder=\"big\")\r",
            "        b_bytes: bytes = b.to_bytes(byte_length, byteorder=\"big\")\r",
            "    elif isinstance(a, str) and isinstance(b, str):\r",
            "        a_bytes = a.encode(\"utf-8\")\r",
            "        b_bytes = b.encode(\"utf-8\")\r",
            "    elif isinstance(a, bytes) and isinstance(b, bytes):\r",
            "        a_bytes = a\r",
            "        b_bytes = b\r",
            "    else:\r",
            "        # For mixed types, use a consistent conversion approach\r",
            "        a_bytes = str(a).encode(\"utf-8\")\r",
            "        b_bytes = str(b).encode(\"utf-8\")\r",
            "\r",
            "    # Handle different lengths with a padded comparison\r",
            "    # to maintain constant time behavior\r",
            "    max_len: int = max(len(a_bytes), len(b_bytes))\r",
            "    a_bytes = a_bytes.ljust(max_len, b\"\\0\")\r",
            "    b_bytes = b_bytes.ljust(max_len, b\"\\0\")\r",
            "\r",
            "    # Constant-time comparison with the full length\r",
            "    result: int = 0\r",
            "    for x, y in zip(a_bytes, b_bytes):\r",
            "        result |= x ^ y\r",
            "\r",
            "    # Final result is 0 only if all bytes matched\r",
            "    return result == 0\r",
            "\r",
            "\r",
            "def estimate_mpz_size(n: Union[int, \"gmpy2.mpz\"]) -> int:\r",
            "    \"\"\"\r",
            "    Estimate memory required for a gmpy2.mpz number of given bit length.\r",
            "\r",
            "    Arguments:\r",
            "        n (int or gmpy2.mpz): Number to estimate size for, or its bit length\r",
            "\r",
            "    Returns:\r",
            "        int: Estimated memory size in bytes\r",
            "    \"\"\"\r",
            "    if isinstance(n, (int, gmpy2.mpz)):\r",
            "        bit_length: int = (\r",
            "            n.bit_length() if hasattr(n, \"bit_length\") else gmpy2.mpz(n).bit_length()\r",
            "        )\r",
            "    else:\r",
            "        bit_length = n  # Assume n is already a bit length\r",
            "\r",
            "    # GMP internally uses limbs (detect size if possible, default to 8 bytes on 64-bit systems)\r",
            "    limb_size: int = 8  # bytes\r",
            "    try:\r",
            "        # Try to detect actual limb size from system architecture\r",
            "        import platform\r",
            "\r",
            "        if platform.architecture()[0] == \"32bit\":\r",
            "            limb_size = 4\r",
            "    except ImportError:\r",
            "        pass\r",
            "\r",
            "    num_limbs: int = (bit_length + 63) // 64\r",
            "\r",
            "    # GMP object overhead (improved estimate with scaling factor)\r",
            "    base_overhead: int = 32  # base overhead\r",
            "    scaling_factor: float = 1 + (num_limbs // 1000) * 0.1  # Add 10% for every 1000 limbs\r",
            "    overhead: int = int(base_overhead * scaling_factor)\r",
            "\r",
            "    return (num_limbs * limb_size) + overhead\r",
            "\r",
            "\r",
            "def estimate_mpz_operation_memory(op_type: str, a_bits: int, b_bits: Optional[int] = None) -> int:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Estimate memory requirements for gmpy2 mpz operations.\r",
            "\r",
            "    Arguments:\r",
            "        op_type (str): Operation type ('add', 'mul', 'pow', etc.)\r",
            "        a_bits (int): Bit length of first operand.\r",
            "        b_bits (int, optional): Bit length of second operand.\r",
            "\r",
            "    Inputs:\r",
            "        op_type (str): Operation to estimate.\r",
            "        a_bits (int): Size of first number.\r",
            "        b_bits (int, optional): Size of second number.\r",
            "\r",
            "    Outputs:\r",
            "        int: Estimated memory requirement in bytes.\r",
            "\r",
            "    Raises:\r",
            "        ValueError: If operation type is unknown or inputs are invalid.\r",
            "    \"\"\"\r",
            "    if not isinstance(a_bits, int) or a_bits <= 0:\r",
            "        raise ValueError(\"a_bits must be a positive integer\")\r",
            "\r",
            "    if op_type in (\"add\", \"sub\"):\r",
            "        # For addition/subtraction, result is at most 1 bit larger\r",
            "        result_bits: int = max(a_bits, b_bits or 0) + 1\r",
            "    elif op_type == \"mul\":\r",
            "        if not isinstance(b_bits, int) or b_bits <= 0:\r",
            "            raise ValueError(\"b_bits must be a positive integer for multiplication\")\r",
            "        # For multiplication, result is sum of bit lengths\r",
            "        result_bits = a_bits + b_bits\r",
            "    elif op_type == \"pow\":\r",
            "        if not isinstance(b_bits, int) or b_bits <= 0:\r",
            "            raise ValueError(\"b_bits must be a positive integer for exponentiation\")\r",
            "        # For exponentiation a^b, result is approximately a_bits * b\r",
            "        if b_bits > 64:  # If exponent is very large\r",
            "            raise ValueError(\"Exponent too large for safe memory estimation\")\r",
            "        # Convert b_bits to approximate value of b\r",
            "        b_approx: int = min(2**b_bits - 1, 2**32)  # Cap to avoid overflow\r",
            "        result_bits = a_bits * b_approx\r",
            "    elif op_type == \"mod\":\r",
            "        # For modulo, result is at most the size of the modulus\r",
            "        result_bits = b_bits if b_bits else a_bits\r",
            "    else:\r",
            "        raise ValueError(f\"Unknown operation type: {op_type}\")\r",
            "\r",
            "    # Convert bits to bytes with ceiling division and add overhead factor\r",
            "    overhead_factor: float = 1.5  # Allow 50% extra for gmpy2 internal overhead\r",
            "    result_bytes: float = ((result_bits + 7) // 8) * overhead_factor\r",
            "\r",
            "    return int(result_bytes)\r",
            "\r",
            "\r",
            "def estimate_exp_result_size(base_bits: int, exponent: Union[int, \"gmpy2.mpz\"]) -> int:\r",
            "    \"\"\"\r",
            "    Estimate the bit length of base^exponent.\r",
            "\r",
            "    Arguments:\r",
            "        base_bits (int): Bit length of base\r",
            "        exponent (int): Exponent value\r",
            "\r",
            "    Returns:\r",
            "        int: Estimated bit length of result\r",
            "    \"\"\"\r",
            "    # For modular exponentiation, result won't exceed modulus size\r",
            "    if isinstance(exponent, (int, gmpy2.mpz)) and exponent <= 2**30:\r",
            "        # For reasonable exponents, we can estimate more precisely\r",
            "        return base_bits * min(exponent, 2**30)\r",
            "    else:\r",
            "        # For very large exponents, return a reasonable maximum\r",
            "        return base_bits * 2**30  # This would likely exceed memory anyway\r",
            "\r",
            "\r",
            "def get_system_memory() -> int:\r",
            "    \"\"\"\r",
            "    Get available system memory in bytes.\r",
            "\r",
            "    Returns:\r",
            "        int: Available memory in bytes, or a conservative estimate if detection fails\r",
            "    \"\"\"\r",
            "    try:\r",
            "        import psutil\r",
            "\r",
            "        return int(psutil.virtual_memory().available)\r",
            "    except ImportError:\r",
            "        # If psutil not available, use a conservative default\r",
            "        return 1 * 1024 * 1024 * 1024  # 1GB conservative estimate\r",
            "\r",
            "\r",
            "def check_memory_safety(operation: str, *args: Any, max_size_mb: int = 1024, reject_unknown: bool = False) -> bool:\r",
            "    \"\"\"\r",
            "    Check if operation can be performed safely without exceeding memory limits.\r",
            "\r",
            "    Arguments:\r",
            "        operation (str): Operation type ('exp', 'mul', etc.)\r",
            "        *args: Arguments to the operation\r",
            "        max_size_mb (int): Maximum allowed memory in MB\r",
            "        reject_unknown (bool): If True, rejects all unknown operations\r",
            "\r",
            "    Returns:\r",
            "        bool: True if operation is likely safe, False otherwise\r",
            "    \"\"\"\r",
            "    max_bytes: int = max_size_mb * 1024 * 1024\r",
            "\r",
            "    try:\r",
            "        if operation == \"exp\":\r",
            "            base: Any\r",
            "            exponent: Any\r",
            "            base, exponent = args[:2]  # Get first two arguments\r",
            "            # Get bit length of base\r",
            "            base_bits: int = (\r",
            "                base.bit_length()\r",
            "                if hasattr(base, \"bit_length\")\r",
            "                else gmpy2.mpz(base).bit_length()\r",
            "            )\r",
            "\r",
            "            # Modular exponentiation won't exceed modulus size\r",
            "            if len(args) >= 3 and args[2] is not None:  # If modulus provided\r",
            "                modulus: Any = args[2]\r",
            "                mod_bits: int = (\r",
            "                    modulus.bit_length()\r",
            "                    if hasattr(modulus, \"bit_length\")\r",
            "                    else gmpy2.mpz(modulus).bit_length()\r",
            "                )\r",
            "                result_bits: int = mod_bits\r",
            "            else:\r",
            "                # Estimate memory for non-modular exponentiation\r",
            "                # Handle both int and gmpy2.mpz exponents safely without conversion\r",
            "                if isinstance(exponent, (int, gmpy2.mpz)) and not isinstance(\r",
            "                    exponent, bool\r",
            "                ):\r",
            "                    # For very large exponents, use the exponent's bit length to estimate\r",
            "                    exp_bit_length: int = (\r",
            "                        exponent.bit_length()\r",
            "                        if hasattr(exponent, \"bit_length\")\r",
            "                        else gmpy2.mpz(exponent).bit_length()\r",
            "                    )\r",
            "\r",
            "                    # If exponent is small enough, use direct multiplication\r",
            "                    if exp_bit_length < 20:  # Exponents up to ~1 million\r",
            "                        result_bits = base_bits * min(int(exponent), 1_000_000)\r",
            "                    else:\r",
            "                        # For larger exponents, use a logarithmic estimation\r",
            "                        # log2(base^exp) = exp * log2(base)\r",
            "                        result_bits = min(\r",
            "                            exp_bit_length * base_bits, base_bits * 1_000_000\r",
            "                        )\r",
            "                else:\r",
            "                    # Default for non-numeric exponents\r",
            "                    result_bits = base_bits * 1000  # Very conservative\r",
            "\r",
            "            estimated_bytes: int = estimate_mpz_size(result_bits)\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        # Other operations remain unchanged\r",
            "        elif operation == \"mul\":\r",
            "            a: Any\r",
            "            b: Any\r",
            "            a, b = args\r",
            "            a_bits: int = (\r",
            "                a.bit_length()\r",
            "                if hasattr(a, \"bit_length\")\r",
            "                else gmpy2.mpz(a).bit_length()\r",
            "            )\r",
            "            b_bits: int = (\r",
            "                b.bit_length()\r",
            "                if hasattr(b, \"bit_length\")\r",
            "                else gmpy2.mpz(b).bit_length()\r",
            "            )\r",
            "            result_bits = a_bits + b_bits  # Multiplication roughly adds bit lengths\r",
            "            estimated_bytes = estimate_mpz_size(result_bits)\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        # Add polynomial operation specifics\r",
            "        elif operation == \"polynomial\":\r",
            "            degree: int\r",
            "            max_coeff_bits: int\r",
            "            degree, max_coeff_bits = args[:2]\r",
            "            # Estimate size based on degree and coefficient size\r",
            "            estimated_bytes: int = degree * estimate_mpz_size(max_coeff_bits)\r",
            "            # Add overhead for intermediate calculations\r",
            "            estimated_bytes *= 3  # Conservative factor\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        elif operation == \"matrix\":\r",
            "            # For matrix operations\r",
            "            n: int\r",
            "            bit_length: int\r",
            "            n, bit_length = args[0], args[1]\r",
            "            estimated_bytes = (n * n * bit_length) // 8\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        elif operation == \"polynomial_eval\":\r",
            "            # For polynomial evaluation\r",
            "            degree: int\r",
            "            coeff_bits: int\r",
            "            degree, coeff_bits = args[0], args[1]\r",
            "            estimated_bytes = degree * estimate_mpz_size(coeff_bits)\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        else:\r",
            "            # Reject unknown operations if policy dictates\r",
            "            if reject_unknown:\r",
            "                logger.warning(f\"Rejecting unknown operation '{operation}' due to safety policy\")\r",
            "                return False\r",
            "                \r",
            "            # Generic fallback for unknown operations with enhanced safety margins\r",
            "            logger.warning(\r",
            "                f\"Unknown operation '{operation}' in memory safety check. \"\r",
            "                f\"Using conservative estimation, but consider adding specific handling.\"\r",
            "            )\r",
            "            \r",
            "            # Estimate based on argument sizes with increased conservatism\r",
            "            total_bits: int = 0\r",
            "            unknown_arg_count: int = 0\r",
            "            collection_size: int = 0\r",
            "            max_bit_length: int = 0\r",
            "            \r",
            "            for arg in args:\r",
            "                if hasattr(arg, \"bit_length\"):\r",
            "                    # For integers and objects with bit_length method\r",
            "                    bit_len: int = arg.bit_length()\r",
            "                    total_bits += bit_len\r",
            "                    max_bit_length = max(max_bit_length, bit_len)\r",
            "                elif isinstance(arg, (int, float)):\r",
            "                    # For numeric types without bit_length\r",
            "                    total_bits += 64  # Conservative estimate\r",
            "                elif isinstance(arg, (list, tuple)):\r",
            "                    # For collections, track total size and count\r",
            "                    arg_len: int = len(arg)\r",
            "                    collection_size += arg_len\r",
            "                    total_bits += arg_len * 64  # Conservative estimate for each element\r",
            "                else:\r",
            "                    # For unknown types, add a larger conservative buffer\r",
            "                    unknown_arg_count += 1\r",
            "                    total_bits += 2048  # 2KB buffer per unknown argument\r",
            "            \r",
            "            # Apply a more aggressive scaling factor based on complexity indicators\r",
            "            scaling_factor: float = 3.0\r",
            "            \r",
            "            # Increase scaling for operations with multiple unknown args\r",
            "            if unknown_arg_count > 1:\r",
            "                scaling_factor *= (1 + (unknown_arg_count * 0.5))\r",
            "                \r",
            "            # Increase scaling for operations with large collections\r",
            "            if collection_size > 100:\r",
            "                scaling_factor *= (1 + (min(collection_size, 10000) / 1000))\r",
            "                \r",
            "            # Increase scaling based on max bit length\r",
            "            if max_bit_length > 1024:\r",
            "                scaling_factor *= (1 + (max_bit_length / 4096))\r",
            "            \r",
            "            # Calculate final estimate with the adaptive scaling factor\r",
            "            estimated_bytes: int = int(estimate_mpz_size(total_bits) * scaling_factor)\r",
            "            \r",
            "            # Set a minimum reasonable estimate (1/4 of max) for unknown operations\r",
            "            min_safe_bytes: int = max_bytes // 4\r",
            "            if estimated_bytes < min_safe_bytes:\r",
            "                logger.warning(\r",
            "                    f\"Increasing estimated memory for unknown operation '{operation}' \"\r",
            "                    f\"from {estimated_bytes} to {min_safe_bytes} bytes for safety\"\r",
            "                )\r",
            "                estimated_bytes = min_safe_bytes\r",
            "            \r",
            "            # Log detailed information about the estimation\r",
            "            logger.debug(\r",
            "                f\"Memory safety estimation for unknown operation '{operation}': \"\r",
            "                f\"{estimated_bytes} bytes (scaling factor: {scaling_factor:.2f}, \"\r",
            "                f\"{estimated_bytes/(1024*1024):.2f}MB/{max_size_mb}MB)\"\r",
            "            )\r",
            "            \r",
            "            # For completely unknown operations with many args, reject the operation\r",
            "            if unknown_arg_count > 3 and len(args) > 5:\r",
            "                logger.error(\r",
            "                    f\"Rejecting complex unknown operation '{operation}' with too many \"\r",
            "                    f\"unrecognized arguments for reliable memory safety estimation\"\r",
            "                )\r",
            "                return False\r",
            "                \r",
            "            return estimated_bytes <= max_bytes\r",
            "    except Exception as e:\r",
            "        # If estimation fails, reject the operation for safety\r",
            "        logger.error(f\"Error during memory safety check for '{operation}': {str(e)}\")\r",
            "        return False\r",
            "\r",
            "\r",
            "def compute_checksum(data: bytes) -> int:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Compute checksum of data using xxhash3_128 with cryptographic fallback.\r",
            "\r",
            "        This provides tamper-evidence for serialized data with excellent performance\r",
            "        when xxhash is available, falling back to cryptographic hashes when it's not.\r",
            "\r",
            "    Arguments:\r",
            "        data (bytes): The data for which to compute the checksum.\r",
            "\r",
            "    Inputs:\r",
            "        data: The data for which to compute the checksum.\r",
            "\r",
            "    Outputs:\r",
            "        int: The computed checksum.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not isinstance(data, bytes):\r",
            "        raise TypeError(\"data must be bytes\")\r",
            "\r",
            "    if HAS_BLAKE3:\r",
            "        # trunk-ignore(pyright/reportPossiblyUnboundVariable)\r",
            "        return int.from_bytes(blake3.blake3(data).digest()[:16], \"big\")\r",
            "    return int.from_bytes(hashlib.sha3_256(data).digest()[:16], \"big\")\r",
            "\r",
            "\r",
            "def secure_redundant_execution(\r",
            "    func: RedundantExecutorFunc,\r",
            "    *args: Any,\r",
            "    sanitize_error_func: Optional[Callable[[str, Optional[str]], str]] = None,\r",
            "    function_name: Optional[str] = None,\r",
            "    context: Optional[str] = None,\r",
            "    **kwargs: Any,\r",
            ") -> Any:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Execute a function multiple times with additional safeguards to detect fault injection.\r",
            "\r",
            "        Uses improved constant-time comparison techniques and increased redundancy. Adds\r",
            "        random execution ordering and timing variation to further harden against\r",
            "        sophisticated fault injection attacks.\r",
            "\r",
            "    Arguments:\r",
            "        func (Callable): Function to execute redundantly.\r",
            "        *args: Arguments to pass to the function.\r",
            "        sanitize_error_func (Callable, optional): Function to sanitize error messages.\r",
            "        function_name (str, optional): Name of the function for error context.\r",
            "        context (str, optional): Additional context information for error messages.\r",
            "        **kwargs: Keyword arguments to pass to the function.\r",
            "\r",
            "    Outputs:\r",
            "        Any: Result of computation if all checks pass.\r",
            "\r",
            "    Raises:\r",
            "        SecurityError: If any computation results don't match.\r",
            "        TypeError: If func is not callable.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not callable(func):\r",
            "        raise TypeError(\"func must be callable\")\r",
            "\r",
            "    # Use function name for better error reporting\r",
            "    if function_name is None and hasattr(func, \"__name__\"):\r",
            "        function_name = func.__name__\r",
            "    else:\r",
            "        function_name = function_name or \"unknown function\"\r",
            "\r",
            "    # Increase executions from 3 to 5 for better statistical reliability\r",
            "    num_executions: int = 5\r",
            "\r",
            "    # Introduce randomly-ordered execution to prevent predictable timing patterns\r",
            "    execution_order: List[int] = list(range(num_executions))\r",
            "    try:\r",
            "        # Use existing random module\r",
            "        random.shuffle(execution_order)\r",
            "    except Exception as e:\r",
            "        # Fall back to deterministic if shuffle fails\r",
            "        logger.debug(f\"Random shuffle failed, using deterministic order: {str(e)}\")\r",
            "\r",
            "    # Execute function multiple times with randomized ordering\r",
            "    results: List[Any] = []\r",
            "    failures: List[Tuple[int, str]] = []\r",
            "\r",
            "    try:\r",
            "        for idx in execution_order:\r",
            "            # Small random delay to decorrelate execution timing\r",
            "            try:\r",
            "                time.sleep(secrets.randbelow(10) / 1000)  # 0-9ms random delay\r",
            "            except Exception as e:\r",
            "                logger.debug(f\"Random delay failed, continuing without delay: {str(e)}\")\r",
            "\r",
            "            try:\r",
            "                results.append(func(*args, **kwargs))\r",
            "            except Exception as e:\r",
            "                # Track failures for better diagnostics\r",
            "                failures.append((idx, str(e)))\r",
            "                # Continue with other executions to prevent timing attacks\r",
            "                results.append(None)\r",
            "\r",
            "        # If we have failures, raise an appropriate error\r",
            "        if failures:\r",
            "            failure_details: str = \", \".join(\r",
            "                [f\"attempt {idx}: {err}\" for idx, err in failures]\r",
            "            )\r",
            "            detailed_message: str = (\r",
            "                f\"Function {function_name} failed during redundant execution: \"\r",
            "                f\"{failure_details}\"\r",
            "            )\r",
            "            message: str = \"Computation failed during security validation\"\r",
            "\r",
            "            # Log the detailed message\r",
            "            logger.error(detailed_message)\r",
            "\r",
            "            # Use sanitization function if provided\r",
            "            if callable(sanitize_error_func):\r",
            "                sanitized_message: str = sanitize_error_func(message, detailed_message)\r",
            "                raise SecurityError(sanitized_message)\r",
            "            else:\r",
            "                raise SecurityError(message)\r",
            "\r",
            "        # Handle the case where all executions succeeded but results don't match\r",
            "        if not all(result == results[0] for result in results):\r",
            "            # Improved constant-time comparison for all permutations\r",
            "            valid: bool = True\r",
            "            mismatch_details: List[str] = []\r",
            "\r",
            "            for i in range(len(results)):\r",
            "                for j in range(i + 1, len(results)):  # Only check unique pairs\r",
            "                    if isinstance(results[i], int) and isinstance(results[j], int):\r",
            "                        # For integers, use constant-time comparison\r",
            "                        result_match: bool = constant_time_compare(results[i], results[j])\r",
            "                        valid &= result_match\r",
            "                        if not result_match:\r",
            "                            mismatch_details.append(f\"Results {i} and {j} differ\")\r",
            "                    elif isinstance(results[i], bytes) and isinstance(\r",
            "                        results[j], bytes\r",
            "                    ):\r",
            "                        # For bytes, use constant-time comparison directly\r",
            "                        result_match = constant_time_compare(results[i], results[j])\r",
            "                        valid &= result_match\r",
            "                        if not result_match:\r",
            "                            mismatch_details.append(f\"Results {i} and {j} differ\")\r",
            "                    else:\r",
            "                        # For complex objects, use serialization with fallbacks\r",
            "                        try:\r",
            "                            # Use the already-imported msgpack\r",
            "                            serialized_i: bytes = msgpack.packb(results[i], use_bin_type=True)\r",
            "                            serialized_j: bytes = msgpack.packb(results[j], use_bin_type=True)\r",
            "                            result_match = constant_time_compare(\r",
            "                                serialized_i, serialized_j\r",
            "                            )\r",
            "                            valid &= result_match\r",
            "                            if not result_match:\r",
            "                                mismatch_details.append(f\"Results {i} and {j} differ\")\r",
            "                        except (TypeError, ValueError):\r",
            "                            # Fall back to string representation as last resort\r",
            "                            result_match = constant_time_compare(\r",
            "                                str(results[i]), str(results[j])\r",
            "                            )\r",
            "                            valid &= result_match\r",
            "                            if not result_match:\r",
            "                                mismatch_details.append(\r",
            "                                    f\"Results {i} and {j} differ (string comparison)\"\r",
            "                                )\r",
            "\r",
            "            # Apply final check with more detailed error for debugging\r",
            "            if not valid:\r",
            "                # For detailed logging but not user-facing\r",
            "                context_info: str = f\" in {context}\" if context else \"\"\r",
            "                detailed_message = (\r",
            "                    f\"Redundant computation mismatch detected in function: \"\r",
            "                    f\"{function_name}{context_info}. Mismatches: {mismatch_details}\"\r",
            "                )\r",
            "\r",
            "                # Generic message for user-facing errors but with better categorization\r",
            "                message = \"Computation result mismatch - potential fault injection attack detected\"\r",
            "\r",
            "                # Log the detailed message\r",
            "                logger.error(detailed_message)\r",
            "\r",
            "                # Use sanitization function if provided, otherwise use the generic message\r",
            "                if callable(sanitize_error_func):\r",
            "                    sanitized_message = sanitize_error_func(message, detailed_message)\r",
            "                    raise SecurityError(sanitized_message)\r",
            "                else:\r",
            "                    # Default behavior if no sanitization function provided\r",
            "                    raise SecurityError(message)\r",
            "\r",
            "        # Return a deterministically selected result to prevent timing side-channels\r",
            "        result_index: int = hash(str(results[0])) % len(results)\r",
            "        return results[result_index]\r",
            "\r",
            "    except Exception as e:\r",
            "        # Handle unexpected exceptions during processing\r",
            "        if isinstance(e, SecurityError):\r",
            "            raise  # Re-raise already processed security errors\r",
            "\r",
            "        detailed_message: str = f\"Unexpected error in secure redundant execution of {function_name}: {str(e)}\"\r",
            "        message: str = \"Security validation process failed\"\r",
            "        logger.error(detailed_message)\r",
            "\r",
            "        if callable(sanitize_error_func):\r",
            "            sanitized_message: str = sanitize_error_func(message, detailed_message)\r",
            "            raise SecurityError(sanitized_message) from e\r",
            "        else:\r",
            "            raise SecurityError(message) from e\r",
            "\r",
            "\r",
            "class MemoryMonitor:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Track estimated memory usage across operations to prevent gmpy2 memory allocation failures.\r",
            "\r",
            "    Attributes:\r",
            "        max_memory_mb (int): Maximum allowed memory usage in megabytes.\r",
            "        current_usage (int): Current estimated memory usage in bytes.\r",
            "        peak_usage (int): Peak memory usage recorded in bytes.\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(self, max_memory_mb: int = 1024) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Initialize memory monitor with specified memory limits.\r",
            "\r",
            "        Arguments:\r",
            "            max_memory_mb (int, optional): Maximum allowed memory in megabytes. Defaults to 1024.\r",
            "\r",
            "        Inputs:\r",
            "            max_memory_mb (int): Memory limit in megabytes.\r",
            "\r",
            "        Raises:\r",
            "            ValueError: If max_memory_mb is not positive.\r",
            "        \"\"\"\r",
            "        if not isinstance(max_memory_mb, (int, float)) or max_memory_mb <= 0:\r",
            "            raise ValueError(\"max_memory_mb must be a positive number\")\r",
            "\r",
            "        self.max_memory_mb: int = max_memory_mb\r",
            "        self.current_usage: int = 0\r",
            "        self.peak_usage: int = 0\r",
            "\r",
            "    def check_allocation(self, size_bytes: int) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if an allocation would exceed memory limits without modifying usage tracker.\r",
            "\r",
            "        Arguments:\r",
            "            size_bytes (int): Size of proposed allocation in bytes.\r",
            "\r",
            "        Inputs:\r",
            "            size_bytes (int): Memory size to check.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if allocation is safe, False if it would exceed limits.\r",
            "\r",
            "        Raises:\r",
            "            ValueError: If size_bytes is negative.\r",
            "            TypeError: If size_bytes is not an integer.\r",
            "        \"\"\"\r",
            "        if not isinstance(size_bytes, int):\r",
            "            raise TypeError(\"size_bytes must be an integer\")\r",
            "        if size_bytes < 0:\r",
            "            raise ValueError(\"size_bytes cannot be negative\")\r",
            "\r",
            "        max_bytes: int = self.max_memory_mb * 1024 * 1024\r",
            "        return self.current_usage + size_bytes <= max_bytes\r",
            "\r",
            "    def allocate(self, size_bytes: int) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Track a memory allocation, raising exception if it would exceed limits.\r",
            "\r",
            "        Arguments:\r",
            "            size_bytes (int): Size of allocation in bytes.\r",
            "\r",
            "        Inputs:\r",
            "            size_bytes (int): Memory size to allocate.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if allocation succeeded.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If allocation would exceed memory limit.\r",
            "            ValueError: If size_bytes is negative.\r",
            "            TypeError: If size_bytes is not an integer.\r",
            "        \"\"\"\r",
            "        if not isinstance(size_bytes, int):\r",
            "            raise TypeError(\"size_bytes must be an integer\")\r",
            "        if size_bytes < 0:\r",
            "            raise ValueError(\"size_bytes cannot be negative\")\r",
            "\r",
            "        if not self.check_allocation(size_bytes):\r",
            "            raise MemoryError(\r",
            "                f\"Operation would exceed memory limit of {self.max_memory_mb}MB\"\r",
            "            )\r",
            "\r",
            "        self.current_usage += size_bytes\r",
            "        self.peak_usage = max(self.peak_usage, self.current_usage)\r",
            "        return True\r",
            "\r",
            "    def release(self, size_bytes: int) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Track memory release after operation is complete.\r",
            "\r",
            "        Arguments:\r",
            "            size_bytes (int): Size of memory to release in bytes.\r",
            "\r",
            "        Inputs:\r",
            "            size_bytes (int): Memory size to release.\r",
            "\r",
            "        Raises:\r",
            "            ValueError: If size_bytes is negative or exceeds current usage.\r",
            "            TypeError: If size_bytes is not an integer.\r",
            "        \"\"\"\r",
            "        if not isinstance(size_bytes, int):\r",
            "            raise TypeError(\"size_bytes must be an integer\")\r",
            "        if size_bytes < 0:\r",
            "            raise ValueError(\"size_bytes cannot be negative\")\r",
            "        if size_bytes > self.current_usage:\r",
            "            raise ValueError(\"Cannot release more memory than currently allocated\")\r",
            "\r",
            "        self.current_usage -= size_bytes\r",
            "\r",
            "    def get_usage_stats(self) -> Dict[str, Union[int, float]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Get current memory usage statistics.\r",
            "\r",
            "        Outputs:\r",
            "            dict: Dictionary containing current and peak memory usage information.\r",
            "        \"\"\"\r",
            "        return {\r",
            "            \"current_bytes\": self.current_usage,\r",
            "            \"current_mb\": self.current_usage / (1024 * 1024),\r",
            "            \"peak_bytes\": self.peak_usage,\r",
            "            \"peak_mb\": self.peak_usage / (1024 * 1024),\r",
            "            \"max_mb\": self.max_memory_mb,\r",
            "            \"usage_percent\": (self.current_usage / (self.max_memory_mb * 1024 * 1024))\r",
            "            * 100,\r",
            "            \"peak_percent\": (self.peak_usage / (self.max_memory_mb * 1024 * 1024))\r",
            "            * 100,\r",
            "        }\r",
            "\r",
            "\r",
            "class CyclicGroup:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Enhanced cyclic group implementation for cryptographic operations with optimizations,\r",
            "        strictly using gmpy2 for all arithmetic.\r",
            "\r",
            "    Arguments:\r",
            "        prime (int, optional): Prime modulus. If None, a safe prime will be selected or generated.\r",
            "        generator (int, optional): Generator of the group. If None, a generator will be found.\r",
            "        prime_bits (int): Bit size for the prime if generating one (default 3072 for PQ security).\r",
            "        use_safe_prime (bool): Whether to use a safe prime (p where (p-1)/2 is also prime).\r",
            "        cache_size (int): The size of the LRU cache for exponentiation.\r",
            "\r",
            "    Inputs:\r",
            "        None\r",
            "\r",
            "    Outputs:\r",
            "        None\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(\r",
            "        self,\r",
            "        prime: Optional[int] = None,\r",
            "        generator: Optional[int] = None,\r",
            "        prime_bits: int = 4096,\r",
            "        use_safe_prime: bool = True,\r",
            "        cache_size: int = 128,\r",
            "        _precompute_window_size: Optional[int] = None,\r",
            "    ) -> None:\r",
            "        # For post-quantum security, we recommend at least 3072-bit primes\r",
            "        if prime_bits < 3072:\r",
            "            warnings.warn(\r",
            "                \"For post-quantum security, consider using prime_bits >= 3072\",\r",
            "                SecurityWarning,\r",
            "            )\r",
            "\r",
            "        # Use provided prime or select one\r",
            "        if prime is not None:\r",
            "            self.prime: \"gmpy2.mpz\" = gmpy2.mpz(prime)\r",
            "            # Verify primality if not using a known safe prime\r",
            "            if self.prime not in SAFE_PRIMES.values() and use_safe_prime:\r",
            "                if not CyclicGroup._is_probable_prime(self.prime):\r",
            "                    raise ParameterError(\"Provided value is not a prime\")\r",
            "                if use_safe_prime and not CyclicGroup._is_safe_prime(self.prime):\r",
            "                    raise ParameterError(\"Provided prime is not a safe prime\")\r",
            "        else:\r",
            "            # Use cached safe prime if available and requested\r",
            "            if use_safe_prime and prime_bits in SAFE_PRIMES:\r",
            "                self.prime = gmpy2.mpz(SAFE_PRIMES[prime_bits])\r",
            "            else:\r",
            "                # Generate a prime of appropriate size\r",
            "                # Note: For production, generating safe primes is very slow\r",
            "                # and should be done offline or use precomputed values\r",
            "                if use_safe_prime:\r",
            "                    warnings.warn(\r",
            "                        \"Generating a safe prime is computationally expensive. \"\r",
            "                        \"Consider using precomputed safe primes for better performance.\",\r",
            "                        RuntimeWarning,\r",
            "                    )\r",
            "                    self.prime = self._generate_safe_prime(prime_bits)\r",
            "                else:\r",
            "                    self.prime = self._generate_prime(prime_bits)\r",
            "\r",
            "        # Set or find generator\r",
            "        if generator is not None:\r",
            "            self.generator: \"gmpy2.mpz\" = gmpy2.mpz(generator % self.prime)\r",
            "            if not self._is_generator(self.generator):\r",
            "                raise ParameterError(\"Provided value is not a generator of the group\")\r",
            "        else:\r",
            "            self.generator = self._find_generator()\r",
            "\r",
            "        # Cache initialization with SafeLRUCache\r",
            "        self.cached_powers: SafeLRUCache = SafeLRUCache(capacity=cache_size)\r",
            "\r",
            "        # Pre-compute fixed-base exponentiations for common operations\r",
            "        self._precompute_exponent_length: int = self.prime.bit_length()\r",
            "        self._precompute_window_size: Optional[int] = _precompute_window_size\r",
            "        self._precomputed_powers: Dict[Union[int, str], Any] = self._precompute_powers()\r",
            "\r",
            "    @staticmethod\r",
            "    def _is_probable_prime(n: Union[int, \"gmpy2.mpz\"], k: int = 40) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if n is probably prime using Miller-Rabin test.\r",
            "\r",
            "        Arguments:\r",
            "            n (int): Number to test.\r",
            "            k (int): Number of rounds (higher is more accurate).\r",
            "\r",
            "        Inputs:\r",
            "            n (int): Number to test.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if n is probably prime, False otherwise.\r",
            "        \"\"\"\r",
            "        if n <= 1:\r",
            "            return False\r",
            "        if n <= 3:\r",
            "            return True\r",
            "        if n % 2 == 0:\r",
            "            return False\r",
            "\r",
            "        # Write n as 2^r * d + 1\r",
            "        r: int\r",
            "        d: Union[int, \"gmpy2.mpz\"]\r",
            "        r, d = 0, n - 1\r",
            "        while d % 2 == 0:\r",
            "            r += 1\r",
            "            d //= 2\r",
            "\r",
            "        # Witness loop\r",
            "        a: int\r",
            "        x: \"gmpy2.mpz\"\r",
            "        for _ in range(k):\r",
            "            a = secrets.randbelow(n - 3) + 2\r",
            "            x = gmpy2.powmod(a, d, n)\r",
            "            if x == 1 or x == n - 1:\r",
            "                continue\r",
            "            for _ in range(r - 1):\r",
            "                x = gmpy2.powmod(x, 2, n)\r",
            "                if x == n - 1:\r",
            "                    break\r",
            "            else:\r",
            "                return False\r",
            "        return True\r",
            "\r",
            "    @staticmethod\r",
            "    def _is_safe_prime(p: Union[int, \"gmpy2.mpz\"]) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if p is a safe prime (p=2q+1 where q is prime).\r",
            "\r",
            "        Arguments:\r",
            "            p (int): Number to check.\r",
            "\r",
            "        Inputs:\r",
            "            p (int): Number to check.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if p is a safe prime, False otherwise.\r",
            "        \"\"\"\r",
            "        return CyclicGroup._is_probable_prime((p - 1) // 2)\r",
            "\r",
            "    def _generate_prime(self, bits: int) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a random prime of specified bits.\r",
            "\r",
            "        Arguments:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Inputs:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Outputs:\r",
            "            int: Generated prime number.\r",
            "        \"\"\"\r",
            "        p: int\r",
            "        while True:\r",
            "            # Generate random odd number of requested bit size\r",
            "            p = secrets.randbits(bits) | (1 << (bits - 1)) | 1\r",
            "            if self._is_probable_prime(p):\r",
            "                return gmpy2.mpz(p)\r",
            "\r",
            "    def _generate_safe_prime(self, bits: int) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a safe prime p where (p-1)/2 is also prime.\r",
            "\r",
            "        Arguments:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Inputs:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Outputs:\r",
            "            int: Generated safe prime number.\r",
            "        \"\"\"\r",
            "        # This is very slow for large bit sizes - should be done offline\r",
            "        q: \"gmpy2.mpz\"\r",
            "        p: \"gmpy2.mpz\"\r",
            "        while True:\r",
            "            # Generate candidate q\r",
            "            q = self._generate_prime(bits - 1)\r",
            "            # Compute p = 2q + 1\r",
            "            p = 2 * q + 1\r",
            "            if self._is_probable_prime(p):\r",
            "                return gmpy2.mpz(p)\r",
            "\r",
            "    def _is_generator(self, g: Union[int, \"gmpy2.mpz\"]) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if g is a generator of the group.\r",
            "            For a safe prime p = 2q + 1, we need to check:\r",
            "            1. g \u2260 0, 1, p-1\r",
            "            2. g^q \u2260 1 mod p\r",
            "\r",
            "        Arguments:\r",
            "            g (int): Element to check.\r",
            "\r",
            "        Inputs:\r",
            "            g (int): Element to check.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if g is a generator, False otherwise.\r",
            "        \"\"\"\r",
            "        if g <= 1 or g >= self.prime - 1:\r",
            "            return False\r",
            "\r",
            "        # For a safe prime p=2q+1, we check if g^q != 1 mod p\r",
            "        # This confirms g generates a subgroup of order q\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        return gmpy2.powmod(g, q, self.prime) != 1\r",
            "\r",
            "    def _find_generator(self) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Find a generator for the q-order subgroup of the cyclic group.\r",
            "            For a safe prime p=2q+1, this finds an element of order q.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            int: Generator of the group.\r",
            "        \"\"\"\r",
            "        # For a safe prime p=2q+1, we want a generator of the q-order subgroup\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "\r",
            "        # Try quadratic residues: for g in Z_p*, g^2 generates the q-order subgroup\r",
            "        h: int\r",
            "        g: \"gmpy2.mpz\"\r",
            "        for _ in range(10000):  # Try multiple times with different values\r",
            "            h = secrets.randbelow(self.prime - 3) + 2  # Random value in [2, p-2]\r",
            "            g = gmpy2.powmod(h, 2, self.prime)  # Square to get quadratic residue\r",
            "\r",
            "            # Skip if g=1, which doesn't generate anything interesting\r",
            "            if g == 1:\r",
            "                continue\r",
            "\r",
            "            # Verify g^q = 1 mod p (ensuring it's in the q-order subgroup)\r",
            "            if gmpy2.powmod(g, q, self.prime) == 1 and g != 1:\r",
            "                return g\r",
            "\r",
            "        # Fallback to standard values that are often generators\r",
            "        standard_candidates: List[int] = [2, 3, 5, 7, 11, 13, 17]\r",
            "        for g in standard_candidates:\r",
            "            if g < self.prime and self._is_generator(g):\r",
            "                return gmpy2.mpz(g)\r",
            "\r",
            "        raise RuntimeError(\"Failed to find a generator for the group\")\r",
            "\r",
            "    def _precompute_powers(self) -> Dict[Union[int, str], Any]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Pre-compute powers of the generator for faster exponentiation with multi-level windows.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            dict: Precomputed powers of the generator.\r",
            "        \"\"\"\r",
            "        bits: int = self.prime.bit_length()\r",
            "\r",
            "        # Dynamic window sizing based on prime size\r",
            "        small_window: int\r",
            "        if self._precompute_window_size is not None:\r",
            "            small_window = self._precompute_window_size\r",
            "        else:\r",
            "            # Enhanced adaptive logic with better scaling\r",
            "            if bits > 8192:\r",
            "                small_window = 8  # Conservative for very large primes\r",
            "            elif bits > 6144:\r",
            "                small_window = 7\r",
            "            elif bits > 4096:\r",
            "                small_window = 6\r",
            "            elif bits > 3072:\r",
            "                small_window = 5\r",
            "            else:\r",
            "                small_window = 4  # Minimum size for good performance\r",
            "\r",
            "        # Large window remains at 8 for consistent big jumps\r",
            "        large_window: int = 8\r",
            "        large_step: int = 2**small_window\r",
            "\r",
            "        # Rest of the method remains unchanged\r",
            "        precomputed: Dict[Union[int, str], Any] = {}\r",
            "\r",
            "        # Small window exponents for fine-grained values\r",
            "        j: int\r",
            "        for j in range(2**small_window):\r",
            "            precomputed[j] = gmpy2.powmod(self.generator, j, self.prime)\r",
            "\r",
            "        # Large window exponents for bigger jumps\r",
            "        large_exponents: Dict[int, \"gmpy2.mpz\"] = {}\r",
            "        k: int\r",
            "        for k in range(1, 2 ** (large_window - small_window)):\r",
            "            large_exponents[k] = gmpy2.powmod(\r",
            "                self.generator, k * large_step, self.prime\r",
            "            )\r",
            "\r",
            "        # Add to precomputed dict\r",
            "        precomputed.update(\r",
            "            {\r",
            "                \"large_window\": large_exponents,\r",
            "                \"small_bits\": small_window,\r",
            "                \"large_step\": large_step,\r",
            "            }\r",
            "        )\r",
            "\r",
            "        return precomputed\r",
            "\r",
            "    def exp(self, base: Union[int, \"gmpy2.mpz\"], exponent: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Thread-safe exponentiation in the group: base^exponent mod prime with optimizations.\r",
            "            NOT suitable for secret exponents - use secure_exp() instead for sensitive values.\r",
            "\r",
            "        Arguments:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Inputs:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the exponentiation.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If the operation would likely exceed available memory\r",
            "        \"\"\"\r",
            "        # Use precomputation for generator base if available\r",
            "        if base == self.generator and self._precomputed_powers:\r",
            "            return self._exp_with_precomputation(exponent)\r",
            "\r",
            "        # Normalize inputs\r",
            "        base_mpz: \"gmpy2.mpz\" = gmpy2.mpz(base % self.prime)\r",
            "        \r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        exponent_mpz: \"gmpy2.mpz\" = gmpy2.mpz(exponent % q)  # More efficient than % (self.prime - 1)\r",
            "\r",
            "        # Check memory safety before proceeding\r",
            "        if not check_memory_safety(\"exp\", base_mpz, exponent_mpz, self.prime):\r",
            "            raise MemoryError(\r",
            "                \"Attempted exponentiation would exceed memory limits. \"\r",
            "                \"Consider using a smaller exponent or larger system memory.\"\r",
            "            )\r",
            "\r",
            "        # Check cache for common operations\r",
            "        cache_key: Tuple[\"gmpy2.mpz\", \"gmpy2.mpz\"] = (base_mpz, exponent_mpz)\r",
            "\r",
            "        # Thread-safe cache access using SafeLRUCache methods\r",
            "        result: Optional[\"gmpy2.mpz\"] = self.cached_powers.get(cache_key)\r",
            "        if result is not None:\r",
            "            return result\r",
            "\r",
            "        # Use efficient binary exponentiation for large numbers\r",
            "        result = gmpy2.powmod(base_mpz, exponent_mpz, self.prime)\r",
            "\r",
            "        # Cache the result using SafeLRUCache's put method (no need to check size)\r",
            "        self.cached_powers.put(cache_key, result)\r",
            "        return result\r",
            "\r",
            "    def _exp_with_precomputation(self, exponent: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Exponentiation using multi-level window technique with precomputed values.\r",
            "\r",
            "        Arguments:\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Inputs:\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the exponentiation.\r",
            "        \"\"\"\r",
            "\r",
            "        if exponent == 0:\r",
            "            return gmpy2.mpz(1)\r",
            "\r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2 \r",
            "        exponent_mpz: \"gmpy2.mpz\" = gmpy2.mpz(exponent) % q\r",
            "\r",
            "        # Extract window parameters\r",
            "        small_bits: int = self._precomputed_powers[\"small_bits\"]\r",
            "        large_step: int = self._precomputed_powers[\"large_step\"]\r",
            "        large_window: Dict[int, \"gmpy2.mpz\"] = self._precomputed_powers.get(\"large_window\", {})\r",
            "\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "        remaining: \"gmpy2.mpz\" = exponent_mpz\r",
            "\r",
            "        # Process large steps first\r",
            "        large_count: int\r",
            "        max_step: int\r",
            "        while remaining >= large_step:\r",
            "            # Extract how many large steps to take\r",
            "            large_count = remaining // large_step\r",
            "            if large_count in large_window:\r",
            "                # Use precomputed large step\r",
            "                result = (result * large_window[large_count]) % self.prime\r",
            "                remaining -= large_count * large_step\r",
            "            else:\r",
            "                # Take the largest available step\r",
            "                max_step = max(\r",
            "                    (k for k in large_window.keys() if k <= large_count), default=0\r",
            "                )\r",
            "                if max_step > 0:\r",
            "                    result = (result * large_window[max_step]) % self.prime\r",
            "                    remaining -= max_step * large_step\r",
            "                else:\r",
            "                    # Fall back to small steps\r",
            "                    break\r",
            "\r",
            "        # Process remaining small steps\r",
            "        small_val: int\r",
            "        while remaining > 0:\r",
            "            # Extract small window bits\r",
            "            small_val = min(remaining, 2**small_bits - 1)\r",
            "            if small_val in self._precomputed_powers:\r",
            "                result = (result * self._precomputed_powers[small_val]) % self.prime\r",
            "                remaining -= small_val\r",
            "            else:\r",
            "                # This case shouldn't happen with full precomputation, but just in case\r",
            "                result = (\r",
            "                    result * gmpy2.powmod(self.generator, small_val, self.prime)\r",
            "                ) % self.prime\r",
            "                remaining -= small_val\r",
            "\r",
            "        return result\r",
            "\r",
            "    def mul(self, a: Union[int, \"gmpy2.mpz\"], b: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Multiply two elements in the group: (a * b) mod prime.\r",
            "\r",
            "        Arguments:\r",
            "            a (int): First element.\r",
            "            b (int): Second element.\r",
            "\r",
            "        Inputs:\r",
            "            a (int): First element.\r",
            "            b (int): Second element.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the multiplication.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If the operation would likely exceed available memory\r",
            "        \"\"\"\r",
            "        a_mpz: \"gmpy2.mpz\" = gmpy2.mpz(a)\r",
            "        b_mpz: \"gmpy2.mpz\" = gmpy2.mpz(b)\r",
            "\r",
            "        # Check memory safety before proceeding\r",
            "        if not check_memory_safety(\"mul\", a_mpz, b_mpz):\r",
            "            raise MemoryError(\r",
            "                \"Multiplication operation would exceed memory limits. \"\r",
            "                \"The operands are too large for available system memory.\"\r",
            "            )\r",
            "\r",
            "        return (a_mpz * b_mpz) % self.prime\r",
            "\r",
            "    def secure_random_element(self) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a secure random element in the group Z_p*.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            int: A random element in the range [1, prime-1].\r",
            "        \"\"\"\r",
            "        return gmpy2.mpz(secrets.randbelow(self.prime - 1) + 1)\r",
            "\r",
            "    def clear_cache(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Thread-safe clearing of exponentiation cache to free memory.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        # Use SafeLRUCache's clear method\r",
            "        self.cached_powers.clear()\r",
            "\r",
            "    def hash_to_group(self, data: bytes) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Hash arbitrary data to an element in the group with uniform distribution.\r",
            "            Uses strict rejection sampling with no fallback to biased methods, ensuring\r",
            "            perfect uniformity across the group range [1, prime-1].\r",
            "\r",
            "        Arguments:\r",
            "            data (bytes): The data to hash.\r",
            "\r",
            "        Inputs:\r",
            "            data (bytes): The data to hash.\r",
            "\r",
            "        Outputs:\r",
            "            int: An element in the range [1, prime-1] with uniform distribution.\r",
            "\r",
            "        Raises:\r",
            "            SecurityError: If unable to generate a uniformly distributed value after\r",
            "                        exhausting all attempts (extremely unlikely).\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(data, bytes):\r",
            "            raise TypeError(\"data must be bytes\")\r",
            "\r",
            "        # Calculate required bytes based on prime size with extra bytes to minimize bias\r",
            "        prime_bits: int = self.prime.bit_length()\r",
            "        required_bytes: int = (prime_bits + 7) // 8\r",
            "        extra_security_bytes: int = 32  # Increased from 16 for better security margin\r",
            "        total_bytes: int = required_bytes + extra_security_bytes\r",
            "\r",
            "        # Increase max attempts to reduce failure probability\r",
            "        max_attempts: int = 50000  # Increased from 10000\r",
            "        original_data: bytes = data\r",
            "\r",
            "        # Make multiple attempts with domain separation\r",
            "        attempt_round: int\r",
            "        for attempt_round in range(5):  # Increased from 3 rounds\r",
            "            counter: int = 0\r",
            "            while counter < max_attempts:\r",
            "                # Generate hash blocks with proper domain separation\r",
            "                hash_blocks: bytearray = bytearray()\r",
            "                block_counter: int = 0\r",
            "\r",
            "                # Domain separation prefix with version and attempt round\r",
            "                domain_prefix: bytes = f\"HTCG_PQS_v{VSS_VERSION}_r{attempt_round}_\".encode()\r",
            "\r",
            "                h: bytes\r",
            "                while len(hash_blocks) < total_bytes:\r",
            "                    block_data: bytes = (\r",
            "                        domain_prefix\r",
            "                        + original_data\r",
            "                        + counter.to_bytes(8, \"big\")\r",
            "                        + block_counter.to_bytes(8, \"big\")\r",
            "                    )\r",
            "\r",
            "                    if HAS_BLAKE3:\r",
            "                        h = blake3.blake3(block_data).digest(\r",
            "                            min(32, total_bytes - len(hash_blocks))\r",
            "                        )\r",
            "                    else:\r",
            "                        h = hashlib.sha3_256(block_data).digest()\r",
            "                    hash_blocks.extend(h)\r",
            "                    block_counter += 1\r",
            "\r",
            "                # Convert to integer, using only the necessary bytes\r",
            "                value: int = int.from_bytes(hash_blocks[:required_bytes], \"big\")\r",
            "\r",
            "                # Pure rejection sampling - accept ONLY if in valid range\r",
            "                if 1 <= value < self.prime:\r",
            "                    return gmpy2.mpz(value)\r",
            "\r",
            "                # If not in range, try again with a different hash input\r",
            "                counter += 1\r",
            "\r",
            "        # If we've exhausted all attempts across multiple rounds,\r",
            "        # this is an exceptional condition that should be treated as a security error\r",
            "        # We do NOT fall back to biased modular reduction\r",
            "        raise SecurityError(\r",
            "            f\"Failed to generate a uniform group element after {5 * max_attempts} attempts. \"\r",
            "            f\"This could indicate an implementation issue or an extraordinarily unlikely \"\r",
            "            f\"statistical event (probability approximately 2^-{30 + extra_security_bytes*8}).\"\r",
            "        )\r",
            "\r",
            "    def _enhanced_encode_for_hash(self, *args: Any, context: str = \"FeldmanVSS\") -> bytes:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Securely encode multiple values for hashing with enhanced domain separation.\r",
            "            Uses both type tagging and length-prefixing to prevent collision attacks.\r",
            "\r",
            "        Arguments:\r",
            "            *args: Values to encode for hashing.\r",
            "            context (str): Optional context string for domain separation (default: \"FeldmanVSS\").\r",
            "\r",
            "        Outputs:\r",
            "            bytes: Bytes ready for hashing.\r",
            "        \"\"\"\r",
            "        # Initialize encoded data\r",
            "        encoded: bytes = b\"\"\r",
            "\r",
            "        # Add protocol version identifier\r",
            "        encoded += VSS_VERSION.encode(\"utf-8\")\r",
            "\r",
            "        # Add context string with type tag and length prefixing for domain separation\r",
            "        context_bytes: bytes = context.encode(\"utf-8\")\r",
            "        encoded += b\"\\x01\"  # Type tag for context string\r",
            "        encoded += len(context_bytes).to_bytes(4, \"big\")\r",
            "        encoded += context_bytes\r",
            "\r",
            "        # Calculate byte length for integer serialization once\r",
            "        prime_bit_length: int = self.prime.bit_length()  # Changed from self.group.prime\r",
            "        byte_length: int = (prime_bit_length + 7) // 8\r",
            "\r",
            "        # Add each value with type tagging and length prefixing\r",
            "        arg: Any\r",
            "        arg_bytes: bytes\r",
            "        for arg in args:\r",
            "            # Convert to bytes with type-specific handling and tagging\r",
            "            if isinstance(arg, bytes):\r",
            "                encoded += b\"\\x00\"  # Tag for bytes\r",
            "                arg_bytes = arg\r",
            "            elif isinstance(arg, str):\r",
            "                encoded += b\"\\x01\"  # Tag for string\r",
            "                arg_bytes = arg.encode(\"utf-8\")\r",
            "            elif isinstance(arg, int) or isinstance(arg, gmpy2.mpz):\r",
            "                encoded += b\"\\x02\"  # Tag for int/mpz\r",
            "                arg_bytes = int(arg).to_bytes(byte_length, \"big\")\r",
            "            else:\r",
            "                encoded += b\"\\x03\"  # Tag for other types\r",
            "                arg_bytes = str(arg).encode(\"utf-8\")\r",
            "\r",
            "            # Add 4-byte length followed by the data itself\r",
            "            encoded += len(arg_bytes).to_bytes(4, \"big\")\r",
            "            encoded += arg_bytes\r",
            "\r",
            "        return encoded\r",
            "\r",
            "    def efficient_multi_exp(self, bases: List[Union[int, \"gmpy2.mpz\"]], exponents: List[Union[int, \"gmpy2.mpz\"]]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Efficient multi-exponentiation using simultaneous method.\r",
            "            Computes \u03a0(bases[i]^exponents[i]) mod prime.\r",
            "\r",
            "        Arguments:\r",
            "            bases (list): List of base values.\r",
            "            exponents (list): List of corresponding exponent values.\r",
            "\r",
            "        Inputs:\r",
            "            bases (list): List of base values.\r",
            "            exponents (list): List of corresponding exponent values.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the multi-exponentiation.\r",
            "        \"\"\"\r",
            "        if len(bases) != len(exponents):\r",
            "            raise ValueError(\"Number of bases must equal number of exponents\")\r",
            "\r",
            "        if len(bases) <= 1:\r",
            "            if not bases:\r",
            "                return gmpy2.mpz(1)\r",
            "            return self.exp(bases[0], exponents[0])\r",
            "\r",
            "        # Normalize inputs\r",
            "        prime: \"gmpy2.mpz\" = self.prime\r",
            "        bases_mpz: List[\"gmpy2.mpz\"] = [gmpy2.mpz(b) % prime for b in bases]\r",
            "        \r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        exponents_mpz: List[\"gmpy2.mpz\"] = [gmpy2.mpz(e) % q for e in exponents]  # More efficient\r",
            "\r",
            "        # Estimate memory requirements\r",
            "        max_base_bits: int = max(b.bit_length() for b in bases_mpz)\r",
            "        max_exp_bits: int = max(e.bit_length() for e in exponents_mpz)\r",
            "        total_ops: int = len(bases_mpz)\r",
            "\r",
            "        # Check if this operation would be safe\r",
            "        if not check_memory_safety(\r",
            "            \"exp\", max_base_bits, max_exp_bits, prime.bit_length()\r",
            "        ):\r",
            "            raise MemoryError(\r",
            "                f\"Multi-exponentiation with {total_ops} operations of size {max_base_bits} bits \"\r",
            "                f\"would exceed memory limits. Consider reducing parameters.\"\r",
            "            )\r",
            "\r",
            "        # Choose window size based on number of bases\r",
            "        n: int = len(bases_mpz)\r",
            "        window_size: int = 2 if n <= 4 else 3 if n <= 16 else 4\r",
            "        max_bits: int = max((e.bit_length() for e in exponents_mpz), default=0)\r",
            "\r",
            "        # For small exponents, reduce window size\r",
            "        if max_bits < 128:\r",
            "            window_size = max(1, window_size - 1)\r",
            "\r",
            "        # Optimize precomputation strategy based on number of bases\r",
            "        precomp: Dict[int, \"gmpy2.mpz\"]\r",
            "        if n <= 8:\r",
            "            # For small n, precompute all possible combinations\r",
            "            precomp = {}\r",
            "            i: int\r",
            "            for i in range(1, 2**n):\r",
            "                product: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "                j: int\r",
            "                for j in range(n):\r",
            "                    if (i >> j) & 1:\r",
            "                        product = (product * bases_mpz[j]) % prime\r",
            "                precomp[i] = product\r",
            "        else:\r",
            "            # For larger n, use selective precomputation\r",
            "            precomp = {1 << j: bases_mpz[j] for j in range(n)}\r",
            "\r",
            "        # Main exponentiation loop using the precomputation\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "        i: int\r",
            "        idx: int\r",
            "        for i in range(max_bits - 1, -1, -1):\r",
            "            result = (result * result) % prime\r",
            "\r",
            "            # Determine which bases to include in this step\r",
            "            idx = 0\r",
            "            j: int\r",
            "            for j in range(n):\r",
            "                if (exponents_mpz[j] >> i) & 1:\r",
            "                    idx |= 1 << j\r",
            "\r",
            "            if idx > 0:\r",
            "                if n <= 8:\r",
            "                    # Use fully precomputed value\r",
            "                    result = (result * precomp[idx]) % prime\r",
            "                else:\r",
            "                    # Selectively multiply by needed bases\r",
            "                    for j in range(n):\r",
            "                        if (idx >> j) & 1:\r",
            "                            result = (result * bases_mpz[j]) % prime\r",
            "\r",
            "        return result\r",
            "\r",
            "    def secure_exp(self, base: Union[int, \"gmpy2.mpz\"], exponent: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Constant-time exponentiation for sensitive cryptographic operations.\r",
            "            Avoids all caching and timing side-channels to prevent exponent leakage.\r",
            "\r",
            "        Arguments:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value (sensitive).\r",
            "\r",
            "        Inputs:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Outputs:\r",
            "            int: base^exponent mod prime.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If the operation would likely exceed available memory\r",
            "        \"\"\"\r",
            "        # Normalize inputs in a predictable way to avoid timing variations\r",
            "        int_base: \"gmpy2.mpz\" = gmpy2.mpz(base) % self.prime\r",
            "        \r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        int_exponent: \"gmpy2.mpz\" = gmpy2.mpz(exponent) % q  # More efficient\r",
            "\r",
            "        # Check memory safety before proceeding\r",
            "        if not check_memory_safety(\"exp\", int_base, int_exponent, self.prime):\r",
            "            raise MemoryError(\r",
            "                \"Attempted exponentiation would exceed memory limits. \"\r",
            "                \"Consider using a smaller exponent or larger system memory.\"\r",
            "            )\r",
            "\r",
            "        # Use gmpy2's powmod which implements constant-time modular exponentiation\r",
            "        return gmpy2.powmod(int_base, int_exponent, self.prime)\r",
            "\r",
            "\r",
            "# --- END OF HELPER FUNCTIONS ---\r",
            "\r",
            "\r",
            "class FeldmanVSS:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Post-Quantum Secure Feldman Verifiable Secret Sharing implementation.\r",
            "\r",
            "    Arguments:\r",
            "        field: Object with a prime attribute representing the field for polynomial operations.\r",
            "        config (VSSConfig, optional): VSSConfig object with configuration parameters. Defaults to a post-quantum secure configuration.\r",
            "        group (CyclicGroup, optional): Pre-configured CyclicGroup instance. If None, a new instance will be created.\r",
            "\r",
            "    Inputs:\r",
            "        None\r",
            "\r",
            "    Outputs:\r",
            "        None\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(self, field: Any, config: Optional[VSSConfig] = None, group: Optional[CyclicGroup] = None) -> None:\r",
            "        if not hasattr(field, \"prime\") or not isinstance(field.prime, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\r",
            "                \"Field must have a 'prime' attribute that is an integer or gmpy2.mpz.\"\r",
            "            )\r",
            "\r",
            "        self.field: Any = field\r",
            "        self.config: VSSConfig = config or VSSConfig()  # Always post-quantum secure by default\r",
            "        self._byzantine_evidence: Dict[int, Dict[str, Any]] = {}\r",
            "\r",
            "        # Initialize the cyclic group for commitments\r",
            "        if group is None:\r",
            "            # Use the enhanced CyclicGroup with appropriate security parameters\r",
            "            self.group: CyclicGroup = CyclicGroup(\r",
            "                prime_bits=self.config.prime_bits,\r",
            "                use_safe_prime=self.config.safe_prime,\r",
            "                cache_size=self.config.cache_size,\r",
            "            )\r",
            "        else:\r",
            "            self.group = group\r",
            "\r",
            "        # Store generator for commitments\r",
            "        self.generator: FieldElement = self.group.generator\r",
            "\r",
            "        # Initialize hash algorithm for use in various methods\r",
            "        self.hash_algorithm: HashFunc = (\r",
            "            blake3.blake3 if HAS_BLAKE3 and self.config.use_blake3 else hashlib.sha3_256\r",
            "        )\r",
            "\r",
            "    def _sanitize_error(self, message: str, detailed_message: Optional[str] = None) -> str:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Sanitize error messages based on configuration.\r",
            "\r",
            "        Arguments:\r",
            "            message (str): The original error message.\r",
            "            detailed_message (str, optional): Detailed information to log but not expose.\r",
            "\r",
            "        Outputs:\r",
            "            str: The sanitized message for external use.\r",
            "        \"\"\"\r",
            "        if detailed_message:\r",
            "            logger.error(detailed_message)\r",
            "\r",
            "        if self.config.sanitize_errors:\r",
            "            # Generic messages for different error categories\r",
            "            message_lower: str = message.lower()\r",
            "\r",
            "            # Enhanced categories for better coverage\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"insufficient\", \"quorum\", \"threshold\", \"not enough\"]\r",
            "            ):\r",
            "                return \"Security verification failed - share refresh aborted\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\r",
            "                    \"deserialized\",\r",
            "                    \"unpacked\",\r",
            "                    \"decode\",\r",
            "                    \"format\",\r",
            "                    \"structure\",\r",
            "                ]\r",
            "            ):\r",
            "                return \"Verification of cryptographic parameters failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\r",
            "                    \"tampering\",\r",
            "                    \"checksum\",\r",
            "                    \"integrity\",\r",
            "                    \"modified\",\r",
            "                    \"corrupted\",\r",
            "                ]\r",
            "            ):\r",
            "                return \"Data integrity check failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"byzan\", \"fault\", \"malicious\", \"attack\", \"adversary\"]\r",
            "            ):\r",
            "                return \"Protocol security violation detected\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"verify\", \"verif\", \"commit\", \"invalid\", \"mismatch\"]\r",
            "            ):\r",
            "                return \"Cryptographic verification failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"prime\", \"generator\", \"arithmetic\", \"computation\"]\r",
            "            ):\r",
            "                return \"Cryptographic parameter validation failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower for keyword in [\"timeout\", \"expired\", \"future\"]\r",
            "            ):\r",
            "                return \"Security timestamp verification failed\"\r",
            "\r",
            "            # Additional categories for better coverage\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"singular\", \"solve\", \"matrix\", \"gauss\"]\r",
            "            ):\r",
            "                return \"Matrix operation failed during cryptographic computation\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"party\", \"participant\", \"diagnostics\"]\r",
            "            ):\r",
            "                return \"Participant verification failed\"\r",
            "\r",
            "            if any(keyword in message_lower for keyword in [\"hash\", \"blake3\", \"sha3\"]):\r",
            "                return \"Hash operation failed\"\r",
            "\r",
            "            # Default generic message\r",
            "            return \"Cryptographic operation failed\"\r",
            "        else:\r",
            "            return message\r",
            "\r",
            "    def _raise_sanitized_error(self, error_class: Type[Exception], message: str, detailed_message: Optional[str] = None) -> NoReturn:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Raise an error with a sanitized message based on configuration.\r",
            "\r",
            "        Arguments:\r",
            "            error_class: Exception class to raise.\r",
            "            message (str): The original error message.\r",
            "            detailed_message (str, optional): Detailed information to log but not expose.\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        sanitized: str = self._sanitize_error(message, detailed_message)\r",
            "        raise error_class(sanitized)\r",
            "\r",
            "    def _compute_hash_commitment_single(\r",
            "        self, \r",
            "        value: FieldElement, \r",
            "        randomizer: FieldElement, \r",
            "        index: int,\r",
            "        context: Optional[str] = None, \r",
            "        extra_entropy: Optional[bytes] = None\r",
            "    ) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Single-instance hash commitment computation (internal use).\r",
            "\r",
            "            Uses deterministic byte encoding for integers to ensure consistent commitment\r",
            "            values regardless of platform or execution environment, which is critical\r",
            "            for cryptographic security.\r",
            "\r",
            "        Arguments:\r",
            "            value (int): The value to commit to.\r",
            "            randomizer (int): The randomizer value.\r",
            "            index (int): The position index (not used in hash calculation, kept for API compatibility).\r",
            "            context (str, optional): Context string for domain separation. Defaults to \"polynomial\".\r",
            "            extra_entropy (bytes, optional): Extra entropy for low-entropy secrets.\r",
            "\r",
            "        Inputs:\r",
            "            value: The value to commit to.\r",
            "            randomizer: Randomizer.\r",
            "            index: Index (not used in hash computation)\r",
            "            context: Context string\r",
            "            extra_entropy: extra_entropy bytes\r",
            "\r",
            "        Outputs:\r",
            "            int: The computed hash commitment.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If index is negative.\r",
            "        \"\"\"\r",
            "\r",
            "        # Add input validation\r",
            "        if not isinstance(value, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"value must be an integer\")\r",
            "        if not isinstance(randomizer, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"randomizer must be an integer\")\r",
            "        if not isinstance(index, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"index must be an integer\")\r",
            "        if index < 0:\r",
            "            raise ValueError(\"index must be non-negative\")\r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "        if extra_entropy is not None and not isinstance(extra_entropy, bytes):\r",
            "            raise TypeError(\"extra_entropy must be bytes if provided\")\r",
            "\r",
            "        # Convert inputs to mpz to ensure consistent handling\r",
            "        value_mpz: \"gmpy2.mpz\" = gmpy2.mpz(value)\r",
            "        randomizer_mpz: \"gmpy2.mpz\" = gmpy2.mpz(randomizer)\r",
            "\r",
            "        # Calculate byte length based on prime size\r",
            "        prime_bit_length: int = self.group.prime.bit_length()\r",
            "        byte_length: int = (prime_bit_length + 7) // 8\r",
            "\r",
            "        # Prepare elements with proper byte encoding\r",
            "        elements: List[Any] = [\r",
            "            VSS_VERSION,  # Protocol version\r",
            "            \"COMMIT\",  # Fixed domain separator\r",
            "            context or \"polynomial\",  # Context with default\r",
            "            value_mpz.to_bytes(byte_length, \"big\"),  # Value to commit to\r",
            "            randomizer_mpz.to_bytes(byte_length, \"big\"),  # Randomizer value\r",
            "        ]\r",
            "\r",
            "        # Add extra entropy if provided for low-entropy secrets\r",
            "        if extra_entropy:\r",
            "            if isinstance(extra_entropy, bytes):\r",
            "                elements.append(extra_entropy)\r",
            "            else:\r",
            "                elements.append(str(extra_entropy).encode(\"utf-8\"))\r",
            "\r",
            "        # Use the consistent encoding method from the group class\r",
            "        encoded: bytes = self.group._enhanced_encode_for_hash(*elements)\r",
            "\r",
            "        # Use preferred hash algorithm\r",
            "        hash_output: bytes\r",
            "        if HAS_BLAKE3 and self.config.use_blake3:\r",
            "            hash_output = blake3.blake3(encoded).digest(32)\r",
            "        else:\r",
            "            hash_output = hashlib.sha3_256(encoded).digest()\r",
            "\r",
            "        return int.from_bytes(hash_output, \"big\") % self.group.prime\r",
            "\r",
            "    def _compute_hash_commitment(\r",
            "        self, \r",
            "        value: FieldElement, \r",
            "        randomizer: FieldElement, \r",
            "        index: int, \r",
            "        context: Optional[str] = None, \r",
            "        extra_entropy: Optional[bytes] = None\r",
            "    ) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced hash commitment function with redundant execution for fault resistance.\r",
            "\r",
            "            This function protects against fault injection attacks by computing the hash\r",
            "            commitment multiple times and verifying the results match.\r",
            "\r",
            "        Arguments:\r",
            "            value (int): The value to commit to.\r",
            "            randomizer (int): The randomizer value.\r",
            "            index (int): The position index.\r",
            "            context (str, optional): Context string for domain separation. Defaults to \"polynomial\".\r",
            "            extra_entropy (bytes, optional): Extra entropy for low-entropy secrets.\r",
            "\r",
            "        Inputs:\r",
            "          value: value\r",
            "          randomizer: randomizer\r",
            "          index: index\r",
            "          context: context\r",
            "          extra_entropy: extra entropy\r",
            "\r",
            "        Outputs:\r",
            "            int: The computed hash commitment.\r",
            "        \"\"\"\r",
            "        return secure_redundant_execution(\r",
            "            self._compute_hash_commitment_single,\r",
            "            value,\r",
            "            randomizer,\r",
            "            index,\r",
            "            context,\r",
            "            extra_entropy,\r",
            "            sanitize_error_func=self._sanitize_error,\r",
            "            function_name=\"_compute_hash_commitment\",\r",
            "        )\r",
            "\r",
            "    def _compute_combined_randomizer(self, randomizers: List[FieldElement], x: FieldElement) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Compute the combined randomizer for evaluating a polynomial at point x.\r",
            "\r",
            "        Arguments:\r",
            "            randomizers (list): List of randomizers for each coefficient.\r",
            "            x (int): Point at which to evaluate.\r",
            "\r",
            "        Inputs:\r",
            "            randomizers: List of randomizers.\r",
            "            x: Point at which to evaluate\r",
            "\r",
            "        Outputs:\r",
            "            int: Combined randomizer value for point x.\r",
            "        \"\"\"\r",
            "        r_combined: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        x_power: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "\r",
            "        r_i: FieldElement\r",
            "        for r_i in randomizers:\r",
            "            r_combined = (r_combined + gmpy2.mpz(r_i) * x_power) % self.group.prime\r",
            "            x_power = (x_power * gmpy2.mpz(x)) % self.group.prime\r",
            "\r",
            "        return r_combined\r",
            "\r",
            "    def _compute_expected_commitment(self, commitments: List[Union[Tuple[FieldElement, ...], FieldElement]], x: FieldElement) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Compute the expected commitment value for a polynomial at point x.\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of commitments for each coefficient.\r",
            "            x (int): Point at which to evaluate.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "            x: x\r",
            "\r",
            "        Outputs:\r",
            "            int: Expected commitment value at point x.\r",
            "        \"\"\"\r",
            "        expected: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        x_power: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "\r",
            "        c_i: Union[Tuple[FieldElement, ...], FieldElement]\r",
            "        for c_i in commitments:\r",
            "            # Extract commitment value from tuple if hash-based\r",
            "            commitment_value: \"gmpy2.mpz\" = gmpy2.mpz(c_i[0] if isinstance(c_i, tuple) else c_i)\r",
            "            expected = (expected + commitment_value * x_power) % self.group.prime\r",
            "            x_power = (x_power * gmpy2.mpz(x)) % self.group.prime\r",
            "\r",
            "        return expected\r",
            "\r",
            "    def _verify_hash_based_commitment(\r",
            "        self,\r",
            "        value: Union[int, \"gmpy2.mpz\"],  # Improved type annotation\r",
            "        combined_randomizer: Union[int, \"gmpy2.mpz\"],\r",
            "        x: Union[int, \"gmpy2.mpz\"],\r",
            "        expected_commitment: Union[int, \"gmpy2.mpz\"],\r",
            "        context: Optional[str] = None,\r",
            "        extra_entropy: Optional[bytes] = None,\r",
            "    ) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verify a hash-based commitment for a value at point x.\r",
            "\r",
            "        Arguments:\r",
            "            value (int): The value to verify.\r",
            "            combined_randomizer (int): Combined randomizer for this point.\r",
            "            x (int): The x-coordinate or index.\r",
            "            expected_commitment (int): The expected commitment value.\r",
            "            context (str, optional): Optional context string.\r",
            "            extra_entropy (bytes, optional): Extra entropy for low-entropy secrets.\r",
            "\r",
            "        Inputs:\r",
            "          value: value\r",
            "          combined_randomizer: combined randomizer\r",
            "          x: x\r",
            "          expected_commitment: expected commitment\r",
            "          context: context\r",
            "          extra_entropy: extra_entropy\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if verification succeeds, False otherwise.\r",
            "        \"\"\"\r",
            "        # Compute the hash commitment\r",
            "        computed_commitment: Union[int, \"gmpy2.mpz\"] = self._compute_hash_commitment(\r",
            "            value, combined_randomizer, x, context, extra_entropy\r",
            "        )\r",
            "\r",
            "        # Compare with expected commitment using constant-time comparison\r",
            "        return constant_time_compare(computed_commitment, expected_commitment)\r",
            "\r",
            "    def create_commitments(self, coefficients: List[FieldElement], context: Optional[str] = None) -> CommitmentList:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create post-quantum secure hash-based commitments to polynomial coefficients.\r",
            "    \r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081] where a\u2080 is the secret.\r",
            "            context (str, optional): Optional context string for domain separation.\r",
            "    \r",
            "        Inputs:\r",
            "            coefficients: List of coefficients\r",
            "            context: Context string\r",
            "    \r",
            "        Outputs:\r",
            "            list: List of (hash, randomizer) tuples representing hash-based commitments.\r",
            "    \r",
            "        Raises:\r",
            "            TypeError: If coefficients is not a list.\r",
            "            ValueError: If coefficients list is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(coefficients, list):\r",
            "            raise TypeError(\"coefficients must be a list\")\r",
            "            \r",
            "        if not coefficients:\r",
            "            self._raise_sanitized_error(ValueError, \"Coefficients list cannot be empty\")\r",
            "            \r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "        \r",
            "        # Use the enhanced commitment creation method for better security\r",
            "        return self.create_enhanced_commitments(coefficients, context)\r",
            "\r",
            "    def create_enhanced_commitments(self, coefficients: List[FieldElement], context: Optional[str] = None) -> CommitmentList:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create enhanced hash-based commitments with improved entropy handling\r",
            "            for low-entropy secrets (Baghery's method, 2025).\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients.\r",
            "            context (str, optional): Optional context string for domain separation.\r",
            "\r",
            "        Inputs:\r",
            "            coefficients: List of coefficients\r",
            "            context: Context string\r",
            "\r",
            "        Outputs:\r",
            "            list: List of (hash, randomizer) tuples.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If coefficients is not a list or context is not a string.\r",
            "            ParameterError: If coefficients list is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(coefficients, list):\r",
            "            raise TypeError(\"coefficients must be a list\")\r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "\r",
            "        if not coefficients:\r",
            "            self._raise_sanitized_error(\r",
            "                ParameterError, \"Coefficients list cannot be empty\"\r",
            "            )\r",
            "\r",
            "        # Convert all coefficients to integers and reduce modulo field prime\r",
            "        coeffs_int: List[\"gmpy2.mpz\"] = [gmpy2.mpz(coeff) % self.field.prime for coeff in coefficients]\r",
            "\r",
            "        # Check entropy of secret coefficient (first coefficient)\r",
            "        secret: \"gmpy2.mpz\" = coeffs_int[0]\r",
            "        low_entropy_threshold: int = (\r",
            "            256  # In bits (enhanced from previous 128-bit threshold)\r",
            "        )\r",
            "        might_have_low_entropy: bool = secret.bit_length() < low_entropy_threshold\r",
            "\r",
            "        # Create enhanced hash-based commitments\r",
            "        commitments: CommitmentList = []\r",
            "        i: int\r",
            "        coeff: \"gmpy2.mpz\"\r",
            "        for i, coeff in enumerate(coeffs_int):\r",
            "            # Generate secure randomizer\r",
            "            r_i: FieldElement = self.group.secure_random_element()\r",
            "\r",
            "            # Add extra entropy for the secret if needed\r",
            "            extra_entropy: Optional[bytes] = None\r",
            "            if i == 0 and might_have_low_entropy:\r",
            "                extra_entropy = secrets.token_bytes(32)\r",
            "\r",
            "            # Use the dedicated hash commitment function\r",
            "            commitment: FieldElement = self._compute_hash_commitment(\r",
            "                coeff, r_i, i, context or \"polynomial\", extra_entropy\r",
            "            )\r",
            "\r",
            "            # Store commitment and randomizer\r",
            "            commitments.append((commitment, r_i, extra_entropy))\r",
            "\r",
            "        return commitments\r",
            "\r",
            "    def _verify_share_hash_based_single(self, x: FieldElement, y: FieldElement, commitments: CommitmentList) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Single-instance share verification (internal use).\r",
            "\r",
            "        Arguments:\r",
            "            x (int): x-coordinate of the share.\r",
            "            y (int): y-coordinate of the share.\r",
            "            commitments (list): List of (commitment, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            x: x\r",
            "            y: y\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the share is valid, False otherwise.\r",
            "        \"\"\"\r",
            "        # Extract randomizers from commitments\r",
            "        randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "\r",
            "        # Compute combined randomizer\r",
            "        r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "\r",
            "        # Compute expected commitment\r",
            "        expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "        # Extract extra_entropy if present (should be in the first coefficient only)\r",
            "        extra_entropy: Optional[bytes] = None\r",
            "        if len(commitments) > 0 and len(commitments[0]) > 2:\r",
            "            extra_entropy = commitments[0][2]  # Get extra_entropy from first coefficient\r",
            "\r",
            "        # Verify using helper method\r",
            "        return self._verify_hash_based_commitment(\r",
            "            y, r_combined, x, expected_commitment, extra_entropy=extra_entropy\r",
            "        )\r",
            "\r",
            "    def verify_share(self, share_x: FieldElement, share_y: FieldElement, commitments: CommitmentList) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Fault-resistant share verification with redundant execution.\r",
            "\r",
            "            Verifies that a share (x, y) lies on the polynomial committed to by the commitments\r",
            "            using post-quantum secure hash-based verification with fault injection protection.\r",
            "\r",
            "        Arguments:\r",
            "            share_x (int): x-coordinate of the share.\r",
            "            share_y (int): y-coordinate of the share (the actual share value).\r",
            "            commitments (list): List of (commitment, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            share_x: x coordinate\r",
            "            share_y: y coordinate\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the share is valid, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or commitments is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(share_x, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_x must be an integer\")\r",
            "        if not isinstance(share_y, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_y must be an integer\")\r",
            "        if not isinstance(commitments, list) or not commitments:\r",
            "            raise TypeError(\"commitments must be a non-empty list\")\r",
            "\r",
            "        # Validate commitment format\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"commitments must be a list of (commitment, randomizer) tuples\"\r",
            "            )\r",
            "\r",
            "        # Convert to integers and use redundant verification\r",
            "        x: \"gmpy2.mpz\"\r",
            "        y: \"gmpy2.mpz\"\r",
            "        x, y = gmpy2.mpz(share_x), gmpy2.mpz(share_y)\r",
            "        return secure_redundant_execution(\r",
            "            self._verify_share_hash_based_single,\r",
            "            x,\r",
            "            y,\r",
            "            commitments,\r",
            "            sanitize_error_func=self._sanitize_error,\r",
            "            function_name=\"verify_share\",\r",
            "        )\r",
            "\r",
            "    def batch_verify_shares(self, shares: List[SharePoint], commitments: CommitmentList) -> VerificationResult:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Efficiently verify multiple shares against the same commitments.\r",
            "\r",
            "            Uses optimized batch verification for hash-based commitments with caching of\r",
            "            intermediate values for improved performance with large batches.\r",
            "\r",
            "        Arguments:\r",
            "            shares (list): List of (x, y) share tuples.\r",
            "            commitments (list): List of (commitment, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            shares: shares\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (all_valid: bool, results: Dict mapping share indices to verification results).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or are empty.\r",
            "            ValueError: If shares list is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(shares, list):\r",
            "            raise TypeError(\"shares must be a list of (x, y) tuples\")\r",
            "        if not shares:\r",
            "            self._raise_sanitized_error(ValueError, \"shares list cannot be empty\")\r",
            "        if not all(isinstance(s, tuple) and len(s) == 2 for s in shares):\r",
            "            raise TypeError(\"Each share must be a tuple of (x, y)\")\r",
            "\r",
            "        if not isinstance(commitments, list) or not commitments:\r",
            "            raise TypeError(\"commitments must be a non-empty list\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"commitments must be a list of (commitment, randomizer) tuples\"\r",
            "            )\r",
            "\r",
            "        results: Dict[int, bool] = {}\r",
            "        all_valid: bool = True\r",
            "\r",
            "        # Standard verification for small batches\r",
            "        if len(shares) < 5:\r",
            "            i: int\r",
            "            x: FieldElement\r",
            "            y: FieldElement\r",
            "            is_valid: bool\r",
            "            for i, (x, y) in enumerate(shares):\r",
            "                is_valid = self.verify_share(x, y, commitments)\r",
            "                results[i] = is_valid\r",
            "                # Use constant-time boolean operation\r",
            "                all_valid &= is_valid  # Constant-time AND\r",
            "            return all_valid, results\r",
            "\r",
            "        # Extract randomizers for more efficient processing\r",
            "        randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "\r",
            "        # Extract extra_entropy if present (only for first coefficient)\r",
            "        extra_entropy: Optional[bytes] = None\r",
            "        if len(commitments) > 0 and len(commitments[0]) > 2:\r",
            "            extra_entropy = commitments[0][2]\r",
            "\r",
            "        # For larger batches, use optimized verification approach with caching\r",
            "        # Precompute powers of x for each share to avoid redundant calculations\r",
            "        x_powers_cache: Dict[FieldElement, List[\"gmpy2.mpz\"]] = {}\r",
            "\r",
            "        # Prepare commitment combinations for each share\r",
            "        share_commitments: List[Tuple[FieldElement, FieldElement, FieldElement, FieldElement]] = []\r",
            "\r",
            "        # First pass: compute and cache powers of x and prepare combined values\r",
            "        x: FieldElement\r",
            "        y: FieldElement\r",
            "        for x, y in shares:\r",
            "            if x not in x_powers_cache:\r",
            "                # Compute and cache powers of x\r",
            "                powers: List[\"gmpy2.mpz\"] = [gmpy2.mpz(1)]  # x^0 = 1\r",
            "                current_power: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "                j: int\r",
            "                for j in range(1, len(commitments)):\r",
            "                    current_power = (current_power * gmpy2.mpz(x)) % self.field.prime\r",
            "                    powers.append(current_power)\r",
            "                x_powers_cache[x] = powers\r",
            "\r",
            "            # Use helper methods to compute randomizers and expected commitments\r",
            "            r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "            expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "            share_commitments.append((x, y, r_combined, expected_commitment))\r",
            "\r",
            "        # Second pass: verify each share with precomputed values (with batch processing)\r",
            "        batch_size: int = min(32, len(share_commitments))  # Process in reasonable batches\r",
            "\r",
            "        batch_start: int\r",
            "        for batch_start in range(0, len(share_commitments), batch_size):\r",
            "            batch_end: int = min(batch_start + batch_size, len(share_commitments))\r",
            "            batch: List[Tuple[FieldElement, FieldElement, FieldElement, FieldElement]] = share_commitments[batch_start:batch_end]\r",
            "\r",
            "            # Process verification in batches\r",
            "            i: int\r",
            "            idx: int\r",
            "            is_valid: bool\r",
            "            for i, (x, y, r_combined, expected_commitment) in enumerate(batch):\r",
            "                idx = batch_start + i\r",
            "                is_valid = self._verify_hash_based_commitment(\r",
            "                    y, r_combined, x, expected_commitment, extra_entropy=extra_entropy\r",
            "                )\r",
            "\r",
            "                results[idx] = is_valid\r",
            "                # Update boolean result using logical AND operation\r",
            "                all_valid &= is_valid  # Note: Not guaranteed to be constant-time\r",
            "\r",
            "        return all_valid, results\r",
            "\r",
            "    def serialize_commitments(self, commitments: CommitmentList) -> str:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Serialize commitment data with checksum for fault resistance.\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of (hash, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            str: String with base64-encoded serialized data with embedded checksum.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If commitments is not a list or has incorrect format.\r",
            "            ValueError: If commitments list is empty.\r",
            "            SerializationError: If serialization fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            self._raise_sanitized_error(ValueError, \"commitments list cannot be empty\")\r",
            "\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Extract commitment values\r",
            "        commitment_values: List[Tuple[int, int, Optional[str]]] = [\r",
            "            (int(c), int(r), e.hex() if e else None) for c, r, e in commitments\r",
            "        ]\r",
            "\r",
            "        # Create the data structure\r",
            "        result: Dict[str, Any] = {\r",
            "            \"version\": VSS_VERSION,\r",
            "            \"timestamp\": int(time.time()),\r",
            "            \"generator\": int(self.generator),\r",
            "            \"prime\": int(self.group.prime),\r",
            "            \"commitments\": commitment_values,\r",
            "            \"hash_based\": True,\r",
            "        }\r",
            "\r",
            "        try:\r",
            "            # Pack with msgpack for efficient serialization\r",
            "            packed_data: bytes = msgpack.packb(result)\r",
            "\r",
            "            # Compute checksum and create wrapper\r",
            "            checksum_wrapper: Dict[str, bytes] = {\r",
            "                \"data\": packed_data,\r",
            "                \"checksum\": compute_checksum(packed_data),\r",
            "            }\r",
            "\r",
            "            # Pack the wrapper and encode\r",
            "            packed_wrapper: bytes = msgpack.packb(checksum_wrapper)\r",
            "            return urlsafe_b64encode(packed_wrapper).decode(\"utf-8\")\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to serialize commitments: {e}\"\r",
            "            message = \"Serialization failed\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def deserialize_commitments(self, data: str) -> Tuple[CommitmentList, FieldElement, FieldElement, int, bool]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Deserialize commitment data with checksum verification\r",
            "\r",
            "        Arguments:\r",
            "            data (str): Serialized commitment data string.\r",
            "\r",
            "        Inputs:\r",
            "            data: Serialized data\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (commitments, generator, prime, timestamp, is_hash_based).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If data is not a string or is empty.\r",
            "            ValueError: If data is empty.\r",
            "            SerializationError: If deserialization or validation fails.\r",
            "            SecurityError: If checksum or cryptographic parameter validation fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(data, str):\r",
            "            self._raise_sanitized_error(TypeError, \"Data must be a string\")\r",
            "        if not data:\r",
            "            self._raise_sanitized_error(ValueError, \"Data cannot be empty\")\r",
            "\r",
            "        try:\r",
            "            # Decode from URL-safe base64\r",
            "            decoded: bytes = urlsafe_b64decode(data.encode(\"utf-8\"))\r",
            "\r",
            "            # Use Unpacker with security settings\r",
            "            unpacker: msgpack.Unpacker = msgpack.Unpacker(\r",
            "                use_list=False,  # Use tuples instead of lists for immutability\r",
            "                raw=True,  # Keep binary data as bytes\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,  # 10MB limit\r",
            "            )\r",
            "            unpacker.feed(decoded)\r",
            "\r",
            "            try:\r",
            "                # Unpack the checksum wrapper\r",
            "                wrapper: Dict[bytes, Any] = unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack msgpack data: {e}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Verify checksum - this is a critical security check\r",
            "            if b\"checksum\" not in wrapper or b\"data\" not in wrapper:\r",
            "                detailed_msg = f\"Detailed deserialization error - data format: {type(data)}, traceback: {traceback.format_exc()}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            packed_data: bytes = wrapper[b\"data\"]\r",
            "            expected_checksum: int = wrapper[b\"checksum\"]\r",
            "            actual_checksum: int = compute_checksum(packed_data)\r",
            "\r",
            "            if not constant_time_compare(actual_checksum, expected_checksum):\r",
            "                detailed_msg = f\"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}\"\r",
            "                message = \"Data integrity check failed - possible tampering detected\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Feed the inner data to a new Unpacker instance\r",
            "            inner_unpacker: msgpack.Unpacker = msgpack.Unpacker(\r",
            "                use_list=False,\r",
            "                raw=True,\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,\r",
            "            )\r",
            "            inner_unpacker.feed(packed_data)\r",
            "\r",
            "            try:\r",
            "                # Proceed with unpacking the actual data\r",
            "                unpacked: Dict[bytes, Any] = inner_unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack inner msgpack data: {e}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # With raw=True, keys will be bytes instead of strings\r",
            "            version_key: bytes = b\"version\"\r",
            "            version_bytes: bytes = VSS_VERSION.encode(\"utf-8\")\r",
            "\r",
            "            # Validate the version\r",
            "            if unpacked.get(version_key) != version_bytes:\r",
            "                detailed_msg = f\"Unsupported VSS version: {unpacked.get(version_key)}\"\r",
            "                message = \"Unsupported version\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate structure of deserialized data - note use of byte keys\r",
            "            if not isinstance(\r",
            "                unpacked.get(b\"commitments\"), tuple\r",
            "            ):  # was list, now tuple with use_list=False\r",
            "                detailed_msg = f\"Invalid commitment data: expected sequence, got {type(unpacked.get(b'commitments'))}\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            if not isinstance(unpacked.get(b\"generator\"), int):\r",
            "                detailed_msg = f\"Invalid generator: expected integer, got {type(unpacked.get(b'generator'))}\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            if not isinstance(unpacked.get(b\"prime\"), int):\r",
            "                detailed_msg = f\"Invalid prime: expected integer, got {type(unpacked.get(b'prime'))}\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Additional check for commitment structure\r",
            "            i: int\r",
            "            commitment: Tuple[Any, ...]\r",
            "            for i, commitment in enumerate(unpacked.get(b\"commitments\", tuple())):\r",
            "                if not isinstance(commitment, tuple) or len(commitment) not in (2, 3):\r",
            "                    detailed_msg = f\"Invalid commitment format at index {i}: expected (commitment, randomizer) or (commitment, randomizer, extra_entropy) tuple\"\r",
            "                    message = \"Invalid data structure\"\r",
            "                    self._raise_sanitized_error(\r",
            "                        SerializationError, message, detailed_msg\r",
            "                    )\r",
            "\r",
            "            # Extract the commitments and parameters\r",
            "            commitments: Tuple[Tuple[Any, ...], ...] = unpacked.get(b\"commitments\")\r",
            "            generator: int = unpacked.get(b\"generator\")\r",
            "            prime: int = unpacked.get(b\"prime\")\r",
            "            timestamp: int = unpacked.get(b\"timestamp\", 0)\r",
            "            is_hash_based: bool = unpacked.get(b\"hash_based\", True)  # Default to hash-based\r",
            "\r",
            "            # Enhanced validity checks\r",
            "            if not (commitments and generator and prime):\r",
            "                detailed_msg = \"Missing required fields in commitment data\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate that prime is actually prime\r",
            "            if prime not in SAFE_PRIMES.values() and self.config.safe_prime:\r",
            "                if not CyclicGroup._is_probable_prime(prime):\r",
            "                    detailed_msg = \"Deserialized prime value failed primality test\"\r",
            "                    message = \"Cryptographic parameter validation failed\"\r",
            "                    self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "                if self.config.safe_prime and not CyclicGroup._is_safe_prime(prime):\r",
            "                    detailed_msg = \"Deserialized prime is not a safe prime\"\r",
            "                    message = \"Cryptographic parameter validation failed\"\r",
            "                    self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Validate generator is in the correct range\r",
            "            if generator <= 1 or generator >= prime - 1:\r",
            "                detailed_msg = \"Deserialized generator is outside valid range\"\r",
            "                message = \"Cryptographic parameter validation failed\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Ensure the generator is valid for this prime\r",
            "            g: \"gmpy2.mpz\" = gmpy2.mpz(generator)\r",
            "            p: \"gmpy2.mpz\" = gmpy2.mpz(prime)\r",
            "            q: \"gmpy2.mpz\" = (p - 1) // 2  # For safe primes, q = (p-1)/2 is also prime\r",
            "            # A proper generator for a safe prime p=2q+1 should satisfy g^q \u2260 1 mod p\r",
            "            if gmpy2.powmod(g, q, p) == 1:\r",
            "                detailed_msg = \"Deserialized generator is not a valid group generator\"\r",
            "                message = \"Cryptographic parameter validation failed\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Additional validation to verify all commitment values are in the proper range\r",
            "            i: int\r",
            "            commitment_data: Tuple[Any, ...]\r",
            "            for i, commitment_data in enumerate(commitments):\r",
            "                if len(commitment_data) >= 2:\r",
            "                    commitment_value: int = commitment_data[0]\r",
            "                    randomizer: int = commitment_data[1]\r",
            "\r",
            "                    # Validate commitment and randomizer are in valid range\r",
            "                    if not (0 <= commitment_value < prime) or not (\r",
            "                        0 <= randomizer < prime\r",
            "                    ):\r",
            "                        detailed_msg = f\"Commitment or randomizer at index {i} is outside valid range\"\r",
            "                        message = \"Cryptographic parameter validation failed\"\r",
            "                        self._raise_sanitized_error(\r",
            "                            SecurityError, message, detailed_msg\r",
            "                        )\r",
            "\r",
            "            # Enforce hash-based commitments for post-quantum security\r",
            "            if not is_hash_based:\r",
            "                detailed_msg = \"Only hash-based commitments are supported in this post-quantum secure version\"\r",
            "                message = \"Unsupported commitment type\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Reconstruct hash-based commitments\r",
            "            reconstructed_commitments: CommitmentList = []\r",
            "            commitment_data: Tuple[Any,...]\r",
            "            for commitment_data in commitments:\r",
            "                if len(commitment_data) >= 3 and commitment_data[2]:\r",
            "                    # Has extra entropy - convert hex string back to bytes\r",
            "                    reconstructed_commitments.append(\r",
            "                        (\r",
            "                            gmpy2.mpz(commitment_data[0]),\r",
            "                            gmpy2.mpz(commitment_data[1]),\r",
            "                            bytes.fromhex(commitment_data[2]) if commitment_data[2] else None,\r",
            "                        )\r",
            "                    )\r",
            "                else:\r",
            "                    # No extra entropy\r",
            "                    reconstructed_commitments.append(\r",
            "                        (\r",
            "                            gmpy2.mpz(commitment_data[0]),\r",
            "                            gmpy2.mpz(commitment_data[1]),\r",
            "                            None,\r",
            "                        )\r",
            "                    )\r",
            "\r",
            "            return (\r",
            "                reconstructed_commitments,\r",
            "                gmpy2.mpz(generator),\r",
            "                gmpy2.mpz(prime),\r",
            "                timestamp,\r",
            "                is_hash_based,\r",
            "            )\r",
            "\r",
            "        except Exception as e:\r",
            "            if isinstance(e, (SerializationError, SecurityError)):\r",
            "                raise\r",
            "\r",
            "            detailed_msg = f\"Exception during deserialization: {str(e)}\"\r",
            "            message = \"Failed to deserialize commitments\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def verify_share_from_serialized(self, share_x: FieldElement, share_y: FieldElement, serialized_commitments: str) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verify a share against serialized commitment data.\r",
            "\r",
            "        Arguments:\r",
            "            share_x (int): x-coordinate of the share.\r",
            "            share_y (int): y-coordinate of the share.\r",
            "            serialized_commitments (str): Serialized commitment data.\r",
            "\r",
            "        Inputs:\r",
            "            share_x: x coordinate\r",
            "            share_y: y coordinate\r",
            "            serialized_commitments: serialized commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the share is valid, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or serialized_commitments is empty.\r",
            "            VerificationError: If deserialization or verification fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(share_x, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_x must be an integer\")\r",
            "        if not isinstance(share_y, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_y must be an integer\")\r",
            "        if not isinstance(serialized_commitments, str) or not serialized_commitments:\r",
            "            raise TypeError(\"serialized_commitments must be a non-empty string\")\r",
            "\r",
            "        try:\r",
            "            # Deserialize the commitments\r",
            "            commitments: CommitmentList\r",
            "            generator: FieldElement\r",
            "            prime: FieldElement\r",
            "            timestamp: int\r",
            "            is_hash_based: bool\r",
            "            commitments, generator, prime, timestamp, is_hash_based = (\r",
            "                self.deserialize_commitments(serialized_commitments)\r",
            "            )\r",
            "\r",
            "            # Create a group with the same parameters\r",
            "            group: CyclicGroup = CyclicGroup(prime=prime, generator=generator)\r",
            "\r",
            "            # Create a new VSS instance with this group\r",
            "            temp_config: VSSConfig = VSSConfig()\r",
            "            temp_vss: FeldmanVSS = FeldmanVSS(self.field, temp_config, group)\r",
            "\r",
            "            # Verify the share\r",
            "            return temp_vss.verify_share(share_x, share_y, commitments)\r",
            "\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Detailed verification failure for share ({share_x}, {share_y}): {str(e)}, Traceback: {traceback.format_exc()}\"\r",
            "            message = f\"Failed to verify share: {e}\"\r",
            "            self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "    def clear_cache(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Clear verification cache to free memory.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        self.group.clear_cache()\r",
            "\r",
            "    def __del__(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Clean up when the object is deleted.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        self.clear_cache()\r",
            "\r",
            "        # Securely wipe any sensitive data\r",
            "        if hasattr(self, \"generator\"):\r",
            "            del self.generator\r",
            "        if hasattr(self, \"field\"):\r",
            "            self.field.clear_cache()\r",
            "\r",
            "    def refresh_shares(\r",
            "        self,\r",
            "        shares: ShareDict,\r",
            "        threshold: int,\r",
            "        total_shares: int,\r",
            "        original_commitments: Optional[CommitmentList] = None,\r",
            "        participant_ids: Optional[List[int]] = None,\r",
            "    ) -> RefreshingResult:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Refresh shares while preserving the same secret using an optimized implementation\r",
            "            of Chen & Lindell's Protocol 5, providing stronger security guarantees in asynchronous\r",
            "            environments.\r",
            "\r",
            "        Arguments:\r",
            "            shares (dict): Dictionary mapping participant IDs to their shares {id: (x, y)}.\r",
            "            threshold (int): The secret sharing threshold.\r",
            "            total_shares (int): Total number of shares to generate.\r",
            "            original_commitments (list, optional): Original commitment values (optional, for proof validation).\r",
            "            participant_ids (list, optional): Optional list of IDs for participants (defaults to numeric IDs).\r",
            "\r",
            "        Inputs:\r",
            "            shares: shares\r",
            "            threshold: threshold\r",
            "            total_shares: total_shares\r",
            "            original_commitments: original commitments\r",
            "            participant_ids: participant_ids\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (new_shares, new_commitments, verification_data).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If threshold or total_shares are invalid, or participant_ids length is incorrect.\r",
            "            ParameterError: If not enough shares are provided.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(shares, dict):\r",
            "            raise TypeError(\r",
            "                \"shares must be a dictionary mapping participant IDs to (x, y) tuples\"\r",
            "            )\r",
            "        if not all(isinstance(v, tuple) and len(v) == 2 for v in shares.values()):\r",
            "            raise TypeError(\"Each share must be a tuple of (x, y)\")\r",
            "\r",
            "        if not isinstance(threshold, int) or threshold < 2:\r",
            "            raise ValueError(\"threshold must be an integer >= 2\")\r",
            "\r",
            "        if not isinstance(total_shares, int) or total_shares < threshold:\r",
            "            raise ValueError(\"total_shares must be an integer >= threshold\")\r",
            "\r",
            "        if original_commitments is not None and not isinstance(\r",
            "            original_commitments, list\r",
            "        ):\r",
            "            raise TypeError(\"original_commitments must be a list if provided\")\r",
            "\r",
            "        if participant_ids is not None:\r",
            "            if not isinstance(participant_ids, list):\r",
            "                raise TypeError(\"participant_ids must be a list if provided\")\r",
            "            if len(participant_ids) != total_shares:\r",
            "                raise ValueError(\"Number of participant_ids must match total_shares\")\r",
            "\r",
            "        if len(shares) < threshold:\r",
            "            detailed_msg = (\r",
            "                f\"Need at least {threshold} shares to refresh, got {len(shares)}\"\r",
            "            )\r",
            "            message = f\"Need at least {threshold} shares to refresh\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Set default participant IDs if not provided\r",
            "        if participant_ids is None:\r",
            "            participant_ids = list(range(1, total_shares + 1))\r",
            "\r",
            "        if len(participant_ids) != total_shares:\r",
            "            detailed_msg = \"Number of participant IDs must match total_shares\"\r",
            "            message = \"Invalid parameters\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Use enhanced additive resharing method (Chen & Lindell's Protocol 5)\r",
            "        # with optimizations for asynchronous environments\r",
            "        return self._refresh_shares_additive(\r",
            "            shares, threshold, total_shares, participant_ids\r",
            "        )\r",
            "\r",
            "    def _refresh_shares_additive(self, shares: ShareDict, threshold: int, total_shares: int, participant_ids: List[int]\r",
            "    ) -> RefreshingResult:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced refresh shares using optimized Chen & Lindell's Protocol 5 (additive resharing).\r",
            "\r",
            "            This implementation includes optimizations for:\r",
            "            1. Better performance in asynchronous environments\r",
            "            2. Reduced communication complexity\r",
            "            3. Improved resilience against adversarial parties\r",
            "            4. More efficient verification\r",
            "            5. Advanced Byzantine fault tolerance\r",
            "\r",
            "        Arguments:\r",
            "            shares (dict): Dictionary mapping participant IDs to their shares {id: (x, y)}.\r",
            "            threshold (int): The secret sharing threshold.\r",
            "            total_shares (int): Total number of shares to generate.\r",
            "            participant_ids (list): List of IDs for participants.\r",
            "\r",
            "        Inputs:\r",
            "            shares: shares\r",
            "            threshold: threshold\r",
            "            total_shares: total shares\r",
            "            participant_ids: participant ids\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (new_shares, new_commitments, verification_data).\r",
            "        \"\"\"\r",
            "        # Step 1: Each party creates a sharing of zero with enhanced verification\r",
            "        zero_sharings: Dict[int, ShareDict] = {}\r",
            "        zero_commitments: Dict[int, CommitmentList] = {}\r",
            "\r",
            "        # Use a deterministic seed derivation for each party to enable verification\r",
            "        # while reducing communication requirements\r",
            "        verification_seeds: Dict[int, bytes] = {}\r",
            "        master_seed: bytes = secrets.token_bytes(32)  # Generate master randomness\r",
            "\r",
            "        # Initialize verification_proofs dictionary\r",
            "        verification_proofs: Dict[int, Dict[int, Any]] = {p_id: {} for p_id in participant_ids}\r",
            "\r",
            "        party_id: int\r",
            "        for party_id in shares.keys():\r",
            "            # Derive a deterministic seed for this party\r",
            "            party_seed: bytes = self.hash_algorithm(\r",
            "                master_seed + str(party_id).encode()\r",
            "            ).digest()\r",
            "            verification_seeds[party_id] = party_seed\r",
            "\r",
            "            # Use the seed to generate a deterministic RNG\r",
            "            # Note: Using random.Random() with cryptographically strong seed is intentional here.\r",
            "            # We need deterministic but unpredictable randomness for the verification protocol.\r",
            "            # The security comes from party_seed being generated with a strong cryptographic hash.\r",
            "            party_rng: random.Random = random.Random(int.from_bytes(party_seed, byteorder=\"big\"))\r",
            "\r",
            "            # Generate a random polynomial of degree t-1 with constant term 0\r",
            "            zero_coeffs: List[FieldElement] = [gmpy2.mpz(0)]  # First coefficient is 0\r",
            "            _: int\r",
            "            for _ in range(1, threshold):\r",
            "                # Use the seeded RNG for deterministic coefficient generation\r",
            "                rand_value: int = party_rng.randrange(self.field.prime)\r",
            "                zero_coeffs.append(gmpy2.mpz(rand_value))\r",
            "\r",
            "            # Create shares for each participant using this polynomial\r",
            "            party_shares: ShareDict = {}\r",
            "            p_id: int\r",
            "            for p_id in participant_ids:\r",
            "                # Evaluate polynomial at the point corresponding to participant's ID\r",
            "                y_value: FieldElement = self._evaluate_polynomial(zero_coeffs, p_id)\r",
            "                party_shares[p_id] = (p_id, y_value)\r",
            "\r",
            "            # Create commitments to the zero polynomial coefficients with optimized batch processing\r",
            "            party_commitments: CommitmentList = self.create_commitments(zero_coeffs)\r",
            "\r",
            "            # More efficient verification for the zero constant term\r",
            "            # For hash-based commitments\r",
            "            commitment_value: FieldElement = party_commitments[0][0]\r",
            "            r_i: FieldElement = party_commitments[0][1]\r",
            "\r",
            "            # Use helper method for consistency\r",
            "            expected_zero_commitment: FieldElement = self._compute_hash_commitment(0, r_i, 0)\r",
            "\r",
            "            if not constant_time_compare(commitment_value, expected_zero_commitment):\r",
            "                detailed_msg = f\"Zero commitment verification failed for party {party_id}, commitment: {commitment_value}, expected: {expected_zero_commitment}\"\r",
            "                message = \"Zero commitment verification failed\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "            # Store this party's zero sharing and commitments\r",
            "            zero_sharings[party_id] = party_shares\r",
            "            zero_commitments[party_id] = party_commitments\r",
            "\r",
            "        # Step 2: Enhanced verification with improved Byzantine fault tolerance\r",
            "        # Optimized for better performance and security\r",
            "        verified_zero_shares: Dict[int, Dict[int, FieldElement]] = {p_id: {} for p_id in participant_ids}\r",
            "        invalid_shares_detected: Dict[int, List[int]] = {}\r",
            "        new_shares: ShareDict = {}\r",
            "        byzantine_parties: Dict[int, Dict[str, Any]] = {}\r",
            "\r",
            "        # Enhanced security parameters with dynamic adjustment\r",
            "        security_factor: float = max(0.5, 1.0 - (threshold / (2 * len(shares))))\r",
            "        min_verified_shares: int = max(threshold // 2, int(threshold * security_factor))\r",
            "\r",
            "        # Echo broadcast mechanism for consistency verification\r",
            "        # This adds Byzantine fault tolerance following Chen & Lindell's recommendations\r",
            "        echo_consistency: Dict[Tuple[int, int], bool] = self._process_echo_consistency(\r",
            "            zero_commitments, zero_sharings, participant_ids\r",
            "        )\r",
            "\r",
            "        # Identify Byzantine parties with adaptive quorum-based detection\r",
            "        byzantine_parties = {}\r",
            "        # Calculate consistency statistics per party\r",
            "        consistency_counts : Dict[int, Dict[str, int]]= {}\r",
            "        for (party_id, _), is_consistent in echo_consistency.items():\r",
            "            if party_id not in consistency_counts:\r",
            "                consistency_counts[party_id] = {\r",
            "                    \"consistent\": 0,\r",
            "                    \"inconsistent\": 0,\r",
            "                    \"total\": 0,\r",
            "                }\r",
            "\r",
            "            consistency_counts[party_id][\"total\"] += 1\r",
            "            if is_consistent:\r",
            "                consistency_counts[party_id][\"consistent\"] += 1\r",
            "            else:\r",
            "                consistency_counts[party_id][\"inconsistent\"] += 1\r",
            "\r",
            "        # Adaptive quorum calculation based on threat model and participant count\r",
            "        # More participants = higher required consistency ratio\r",
            "        base_quorum_ratio: float = 0.5  # Start at 50%\r",
            "        consistency_ratio_requirement: float = min(\r",
            "            0.8, base_quorum_ratio + 0.1 * (len(shares) / threshold - 1)\r",
            "        )\r",
            "\r",
            "        # Identify parties that failed to reach consistency quorum\r",
            "        \r",
            "        party_id: int\r",
            "        counts: Dict[str, int]\r",
            "        for party_id, counts in consistency_counts.items():\r",
            "            if counts[\"total\"] > 0:\r",
            "                consistency_ratio: float = counts[\"consistent\"] / counts[\"total\"]\r",
            "                if consistency_ratio < consistency_ratio_requirement:\r",
            "                    evidence: Dict[str, Union[str, float, int]] = {\r",
            "                        \"type\": \"insufficient_consistency_quorum\",\r",
            "                        \"consistency_ratio\": consistency_ratio,\r",
            "                        \"required_ratio\": consistency_ratio_requirement,\r",
            "                        \"consistent_count\": counts[\"consistent\"],\r",
            "                        \"inconsistent_count\": counts[\"inconsistent\"],\r",
            "                        \"total_checked\": counts[\"total\"],\r",
            "                    }\r",
            "                    byzantine_parties[party_id] = evidence\r",
            "                    warnings.warn(\r",
            "                        f\"Party {party_id} failed to reach consistency quorum \"\r",
            "                        f\"({consistency_ratio:.2f} < {consistency_ratio_requirement:.2f})\",\r",
            "                        SecurityWarning,\r",
            "                    )\r",
            "\r",
            "        # Standard Byzantine detection for each party\r",
            "        \r",
            "        party_id: int\r",
            "        for party_id in shares.keys():\r",
            "            if party_id in byzantine_parties:\r",
            "                continue  # Already identified as Byzantine\r",
            "\r",
            "            is_byzantine: bool\r",
            "            evidence: Dict[str, Any]\r",
            "            is_byzantine, evidence = self._detect_byzantine_behavior(\r",
            "                party_id,\r",
            "                zero_commitments[party_id],\r",
            "                zero_sharings[party_id],\r",
            "                echo_consistency,\r",
            "            )\r",
            "\r",
            "            if is_byzantine:\r",
            "                warnings.warn(\r",
            "                    f\"Detected Byzantine behavior from party {party_id}: {evidence.get('type', 'unknown')}\",\r",
            "                    SecurityWarning,\r",
            "                )\r",
            "                byzantine_parties[party_id] = evidence\r",
            "\r",
            "        # More efficient batch verification with adaptive batch sizing\r",
            "        batch_size: int = self._calculate_optimal_batch_size(\r",
            "            len(participant_ids), len(shares)\r",
            "        )\r",
            "\r",
            "        # Group shares by commitment set for more efficient batch verification\r",
            "        verification_batches: List[List[Tuple[int, int, int, int, CommitmentList]]] = self._prepare_verification_batches(\r",
            "            zero_sharings, zero_commitments, participant_ids, batch_size\r",
            "        )\r",
            "\r",
            "        # Process verification with improved parallelism\r",
            "        verification_results: List[Tuple[Tuple[int, int], bool]] = self._process_verification_batches(verification_batches)\r",
            "\r",
            "        # Process verification results with Byzantine exclusion\r",
            "        result: Tuple[Tuple[int, int], bool]\r",
            "        for (party_id, p_id), is_valid in verification_results:\r",
            "            # Skip shares from Byzantine parties\r",
            "            if party_id in byzantine_parties:\r",
            "                continue\r",
            "\r",
            "            # Changed default from True to False - more conservative security posture\r",
            "            if is_valid and echo_consistency.get((party_id, p_id), False):\r",
            "                # Store verified share with additional consistency check\r",
            "                share_value: FieldElement = self._get_share_value_from_results(\r",
            "                    party_id, p_id, zero_sharings\r",
            "                )\r",
            "                verified_zero_shares[p_id][party_id] = share_value\r",
            "            else:\r",
            "                # Enhanced detection of invalid shares\r",
            "                if p_id not in invalid_shares_detected:\r",
            "                    invalid_shares_detected[p_id] = []\r",
            "                invalid_shares_detected[p_id].append(party_id)\r",
            "\r",
            "                # Generate cryptographic proof with improved evidence collection\r",
            "                self._generate_invalidity_evidence(\r",
            "                    party_id,\r",
            "                    p_id,\r",
            "                    zero_sharings,\r",
            "                    zero_commitments,\r",
            "                    verification_proofs,\r",
            "                    is_valid,\r",
            "                    echo_consistency.get((party_id, p_id), False), # Changed default to False here too\r",
            "                )\r",
            "\r",
            "        # Improved collusion detection with network analysis algorithms\r",
            "        potential_collusion: List[int] = self._enhanced_collusion_detection(\r",
            "            invalid_shares_detected, shares.keys(), echo_consistency\r",
            "        )\r",
            "        p_id: int\r",
            "        # Process shares with adaptive security parameters\r",
            "        for p_id in participant_ids:\r",
            "            # Get original share with robust fallback\r",
            "            original_y: FieldElement = self._get_original_share_value(p_id, shares)\r",
            "\r",
            "            # Dynamic security threshold based on the situation\r",
            "            verified_count: int = len(verified_zero_shares[p_id])\r",
            "            required_threshold: int = self._determine_security_threshold(\r",
            "                threshold,\r",
            "                verified_count,\r",
            "                len(shares),\r",
            "                invalid_shares_detected.get(p_id, []),\r",
            "            )\r",
            "\r",
            "            # Enhanced security check with detailed diagnostics\r",
            "            if verified_count < required_threshold:\r",
            "                security_ratio: float = verified_count / threshold\r",
            "                diagnostics: Dict[str, Union[int, float, List[int]]] = {\r",
            "                    \"verified_count\": verified_count,\r",
            "                    \"threshold\": threshold,\r",
            "                    \"required_threshold\": required_threshold,\r",
            "                    \"security_ratio\": security_ratio,\r",
            "                    \"invalid_shares\": invalid_shares_detected.get(p_id, []),\r",
            "                    \"total_participants\": len(shares),\r",
            "                }\r",
            "\r",
            "                if verified_count < min_verified_shares:\r",
            "                    detailed_msg = (\r",
            "                        f\"Insufficient verified zero shares for participant {p_id}. \"\r",
            "                        f\"Security diagnostics: {diagnostics}. \"\r",
            "                        f\"Share refresh aborted for security reasons.\"\r",
            "                    )\r",
            "                    message = \"Insufficient verified shares\"\r",
            "                    self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "                else:\r",
            "                    warnings.warn(\r",
            "                        f\"Suboptimal number of verified zero shares for participant {p_id}. \"\r",
            "                        f\"Security diagnostics: {diagnostics}. \"\r",
            "                        f\"Proceeding with reduced security margin.\",\r",
            "                        SecurityWarning,\r",
            "                    )\r",
            "\r",
            "            # Optimized summation with constant-time operations to prevent timing attacks\r",
            "            sum_zero_shares: FieldElement = self._secure_sum_shares(\r",
            "                verified_zero_shares[p_id], self.field.prime\r",
            "            )\r",
            "\r",
            "            # Create new share with zero-knowledge consistency proof\r",
            "            new_y: FieldElement = (original_y + sum_zero_shares) % self.field.prime\r",
            "            new_shares[p_id] = (p_id, new_y)\r",
            "\r",
            "            # Generate proofs of correct share refreshing (optional)\r",
            "            if verified_count >= threshold:\r",
            "                # Only generate proofs when we have enough shares for full security\r",
            "                verification_proofs[p_id][\"consistency\"] = (\r",
            "                    self._generate_refresh_consistency_proof(\r",
            "                        p_id,\r",
            "                        original_y,\r",
            "                        sum_zero_shares,\r",
            "                        new_y,\r",
            "                        verified_zero_shares[p_id],\r",
            "                    )\r",
            "                )\r",
            "\r",
            "        # Add enhanced verification summary to verification_data\r",
            "        verification_summary: Dict[str, Any] = {\r",
            "            \"total_zero_shares_created\": len(zero_sharings) * len(participant_ids),\r",
            "            \"total_zero_shares_verified\": sum(\r",
            "                len(v) for v in verified_zero_shares.values()\r",
            "            ),\r",
            "            \"invalid_shares_detected\": invalid_shares_detected,\r",
            "            \"participants_with_full_verification\": sum(\r",
            "                1\r",
            "                for p_id in participant_ids\r",
            "                if len(verified_zero_shares[p_id]) == len(shares)\r",
            "            ),\r",
            "            \"potential_collusion_detected\": bool(potential_collusion),\r",
            "            \"byzantine_parties_excluded\": len(byzantine_parties),\r",
            "            \"byzantine_party_ids\": (\r",
            "                list(byzantine_parties.keys()) if byzantine_parties else []\r",
            "            ),\r",
            "            \"security_parameters\": {\r",
            "                \"min_verified_shares\": min_verified_shares,\r",
            "                \"security_factor\": security_factor,\r",
            "            },\r",
            "        }\r",
            "\r",
            "        # Step 3: Calculate the new commitments\r",
            "        # Extract x and y values from a subset of new shares for efficient reconstruction\r",
            "        sample_shares: List[SharePoint] = list(new_shares.values())[:threshold]\r",
            "        x_values: List[FieldElement] = [share[0] for share in sample_shares]\r",
            "        y_values: List[FieldElement] = [share[1] for share in sample_shares]\r",
            "\r",
            "        # Reconstruct the new polynomial coefficients via optimized interpolation\r",
            "        new_coeffs: List[FieldElement] = self._reconstruct_polynomial_coefficients(\r",
            "            x_values, y_values, threshold\r",
            "        )\r",
            "\r",
            "        # Create new commitments for these coefficients\r",
            "        new_commitments: CommitmentList = self.create_commitments(new_coeffs)\r",
            "\r",
            "        # Add the verification proofs and enhanced summary to the verification data\r",
            "        verification_data: Dict[str, Any] = {\r",
            "            \"original_shares_count\": len(shares),\r",
            "            \"threshold\": threshold,\r",
            "            \"zero_commitment_count\": len(zero_commitments),\r",
            "            \"timestamp\": int(time.time()),\r",
            "            \"protocol\": \"Enhanced-Chen-Lindell-PQ\",\r",
            "            \"verification_method\": \"batch-optimized\",\r",
            "            \"hash_based\": True,\r",
            "            \"verification_summary\": verification_summary,\r",
            "            \"seed_fingerprint\": hashlib.sha3_256(master_seed).hexdigest()[\r",
            "                :16\r",
            "            ],  # Fingerprint for verification\r",
            "            \"verification_proofs\": verification_proofs,\r",
            "        }\r",
            "\r",
            "        return new_shares, new_commitments, verification_data\r",
            "\r",
            "    def _secure_sum_shares(self, shares_dict: Dict[int, FieldElement], modulus: FieldElement) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Perform a secure constant-time summation of shares to prevent timing attacks.\r",
            "\r",
            "        Arguments:\r",
            "            shares_dict (dict): Dictionary of shares to sum.\r",
            "            modulus (int): The field modulus.\r",
            "\r",
            "        Inputs:\r",
            "            shares_dict: Dictionary of shares.\r",
            "            modulus: Modulus\r",
            "\r",
            "        Outputs:\r",
            "            int: Sum of shares modulo the field modulus.\r",
            "        \"\"\"\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        for _, value in sorted(\r",
            "            shares_dict.items()\r",
            "        ):  # Sort to ensure deterministic processing\r",
            "            result = (result + gmpy2.mpz(value)) % modulus\r",
            "        return int(result)\r",
            "\r",
            "    def _get_original_share_value(self, participant_id: int, shares: ShareDict) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Safely retrieve the original share value with proper validation.\r",
            "\r",
            "        Arguments:\r",
            "            participant_id (int): ID of the participant.\r",
            "            shares (dict): Dictionary of shares.\r",
            "\r",
            "        Inputs:\r",
            "            participant_id: Participant ID\r",
            "            shares: shares\r",
            "\r",
            "        Outputs:\r",
            "            int: Original y-value of the share.\r",
            "        \r",
            "        Raises:\r",
            "            SecurityError: If no valid original share is found for the participant.\r",
            "        \"\"\"\r",
            "        if participant_id in shares:\r",
            "            original_share: SharePoint = shares[participant_id]\r",
            "            # Validate the share structure\r",
            "            if isinstance(original_share, tuple) and len(original_share) == 2:\r",
            "                return original_share[1]\r",
            "\r",
            "        # Instead of returning 0, raise a security error to prevent silent failure\r",
            "        detailed_msg = f\"No valid original share found for participant {participant_id}.\"\r",
            "        message = \"Original share not found\"\r",
            "        self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "    def _determine_security_threshold(\r",
            "        self, base_threshold: int, verified_count: int, total_parties: int, invalid_parties: List[int]\r",
            "    ) -> int:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Determine the security threshold based on the current situation.\r",
            "\r",
            "            Uses an adaptive approach based on the number of invalid shares detected.\r",
            "\r",
            "        Arguments:\r",
            "            base_threshold (int): The base threshold value (t).\r",
            "            verified_count (int): Number of verified shares.\r",
            "            total_parties (int): Total number of participating parties.\r",
            "            invalid_parties (list): List of parties that provided invalid shares.\r",
            "\r",
            "        Inputs:\r",
            "            base_threshold: base threshold\r",
            "            verified_count: verified count\r",
            "            total_parties: total_parties\r",
            "            invalid_parties: invalid parties\r",
            "\r",
            "        Outputs:\r",
            "            int: The required threshold for secure operation.\r",
            "        \"\"\"\r",
            "        # Add explicit check for zero division with proper error handling\r",
            "        if total_parties <= 0:\r",
            "            detailed_msg = \"No participating parties available for threshold calculation\"\r",
            "            message = \"Invalid security parameters\"\r",
            "            self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "        \r",
            "        # Calculate the ratio of invalid to total parties\r",
            "        invalid_ratio: float = len(invalid_parties) / total_parties\r",
            "\r",
            "        required: int\r",
            "        if invalid_ratio > 0.25:\r",
            "            # High threat environment - increase security requirements\r",
            "            required = max(base_threshold, int(base_threshold * (1 + invalid_ratio)))\r",
            "        elif invalid_ratio > 0:\r",
            "            # Some threats detected - slight increase in requirements\r",
            "            required = base_threshold\r",
            "        else:\r",
            "            # No threats detected - can use standard threshold\r",
            "            required = base_threshold\r",
            "\r",
            "        # Never require more shares than are available\r",
            "        return min(required, total_parties)\r",
            "\r",
            "    def _detect_collusion_patterns(self, invalid_shares_detected: Dict[int, List[int]], party_ids: Set[int]) -> List[int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Detect potential collusion patterns among parties that provided invalid shares.\r",
            "\r",
            "        Arguments:\r",
            "            invalid_shares_detected (dict): Dictionary mapping participants to parties that gave them invalid shares.\r",
            "            party_ids (set): Set of all participating party IDs.\r",
            "\r",
            "        Inputs:\r",
            "            invalid_shares_detected: invalid_shares_detected\r",
            "            party_ids: party_ids\r",
            "\r",
            "        Outputs:\r",
            "            list: List of party IDs that might be colluding, or empty list if none detected.\r",
            "        \"\"\"\r",
            "        if not invalid_shares_detected:\r",
            "            return []\r",
            "\r",
            "        # Count how many times each party provided invalid shares\r",
            "        invalid_count: Dict[int, int] = {}\r",
            "        \r",
            "        for parties in invalid_shares_detected.values():\r",
            "            for party_id in parties:\r",
            "                invalid_count[party_id] = invalid_count.get(party_id, 0) + 1\r",
            "\r",
            "        # Calculate a suspicious threshold - parties that have more than 30% invalid shares\r",
            "        suspicious_threshold: float = 0.3 * len(invalid_shares_detected)\r",
            "        suspicious_parties: List[int] = [\r",
            "            party\r",
            "            for party, count in invalid_count.items()\r",
            "            if count > suspicious_threshold\r",
            "        ]\r",
            "\r",
            "        # Check for patterns indicating potential collusion\r",
            "        potential_colluders: List[int] = []\r",
            "\r",
            "        # If multiple suspicious parties targeted the same participants, they might be colluding\r",
            "        if len(suspicious_parties) > 1:\r",
            "            # Check for overlap in targeted participants\r",
            "            targeted_participants: Dict[int, Set[int]] = {}\r",
            "            participant_id: int\r",
            "            parties: List[int]\r",
            "            for participant_id, parties in invalid_shares_detected.items():\r",
            "                party_id: int\r",
            "                for party_id in parties:\r",
            "                    if party_id in suspicious_parties:\r",
            "                        if party_id not in targeted_participants:\r",
            "                            targeted_participants[party_id] = set()\r",
            "                        targeted_participants[party_id].add(participant_id)\r",
            "\r",
            "            # Look for significant overlap\r",
            "            p1: int\r",
            "            p2: int\r",
            "            for p1 in suspicious_parties:\r",
            "                for p2 in suspicious_parties:\r",
            "                    if (\r",
            "                        p1 < p2\r",
            "                        and p1 in targeted_participants\r",
            "                        and p2 in targeted_participants\r",
            "                    ):\r",
            "                        p1_targets: Set[int] = targeted_participants[p1]\r",
            "                        p2_targets: Set[int] = targeted_participants[p2]\r",
            "                        overlap: int = len(p1_targets.intersection(p2_targets))\r",
            "                        union: int = len(p1_targets.union(p2_targets))\r",
            "\r",
            "                        # If overlap ratio is high, add both to potential colluders\r",
            "                        if union > 0 and overlap / union > 0.7:\r",
            "                            if p1 not in potential_colluders:\r",
            "                                potential_colluders.append(p1)\r",
            "                            if p2 not in potential_colluders:\r",
            "                                potential_colluders.append(p2)\r",
            "\r",
            "        return potential_colluders\r",
            "\r",
            "    def _create_invalidity_proof(self, party_id: int, participant_id: int, share: SharePoint, commitments: CommitmentList) -> Dict[str, Any]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create a cryptographic proof that a share is invalid.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party that provided the invalid share.\r",
            "            participant_id (int): ID of the participant who received the share.\r",
            "            share (tuple): The invalid share (x, y).\r",
            "            commitments (list): The commitments against which the share was verified.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            participant_id: participant id\r",
            "            share: share\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            dict: A proof structure that can be verified by others.\r",
            "        \"\"\"\r",
            "        x: FieldElement\r",
            "        y: FieldElement\r",
            "        x, y = share\r",
            "\r",
            "        # Extract randomizers from commitments for hash-based verification\r",
            "        randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "\r",
            "        # Compute the combined randomizer for this point\r",
            "        r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "\r",
            "        # Compute the expected commitment\r",
            "        expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "        # Compute the actual commitment based on the share\r",
            "        actual_commitment: FieldElement = self._compute_hash_commitment(y, r_combined, x, \"verify\")\r",
            "\r",
            "        # Create a signature/timestamp for this proof\r",
            "        timestamp: int = int(time.time())\r",
            "        signature_input: bytes = self.group._enhanced_encode_for_hash(\r",
            "            party_id,\r",
            "            participant_id,\r",
            "            x,\r",
            "            y,\r",
            "            expected_commitment,\r",
            "            actual_commitment,\r",
            "            timestamp,\r",
            "            \"invalidity_proof\",\r",
            "        )\r",
            "\r",
            "        signature: str\r",
            "        if HAS_BLAKE3:\r",
            "            signature = blake3.blake3(signature_input).hexdigest()\r",
            "        else:\r",
            "            signature = hashlib.sha3_256(signature_input).hexdigest()\r",
            "\r",
            "        # Return the proof structure\r",
            "        return {\r",
            "            \"party_id\": party_id,\r",
            "            \"participant_id\": participant_id,\r",
            "            \"share_x\": int(x),\r",
            "            \"share_y\": int(y),\r",
            "            \"expected_commitment\": int(expected_commitment),\r",
            "            \"actual_commitment\": int(actual_commitment),\r",
            "            \"combined_randomizer\": int(r_combined),\r",
            "            \"timestamp\": timestamp,\r",
            "            \"signature\": signature,\r",
            "        }\r",
            "\r",
            "    def _generate_refresh_consistency_proof(\r",
            "        self, participant_id: int, original_y: FieldElement, sum_zero_shares: FieldElement, new_y: FieldElement, verified_shares: Dict[int, FieldElement]\r",
            "    ) -> Dict[str, Any]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a proof that the share refreshing was done correctly.\r",
            "\r",
            "        Arguments:\r",
            "            participant_id (int): ID of the participant.\r",
            "            original_y (int): Original share value.\r",
            "            sum_zero_shares (int): Sum of the zero shares.\r",
            "            new_y (int): New share value.\r",
            "            verified_shares (dict): Dictionary of verified zero shares.\r",
            "        Inputs:\r",
            "            participant_id: participant id\r",
            "            original_y: original y\r",
            "            sum_zero_shares: sum of zero shares\r",
            "            new_y: new y\r",
            "            verified_shares: verified shares\r",
            "\r",
            "        Outputs:\r",
            "            dict: Proof structure for verification.\r",
            "        \"\"\"\r",
            "        # Create a fingerprint of all verified shares\r",
            "        share_fingerprint: str = hashlib.sha3_256(\r",
            "            str(sorted([(k, v) for k, v in verified_shares.items()])).encode()\r",
            "        ).hexdigest()\r",
            "\r",
            "        # Verify that new_y = original_y + sum_zero_shares mod prime\r",
            "        check_value: FieldElement = (original_y + sum_zero_shares) % self.field.prime\r",
            "\r",
            "        # Generate proof timestamp and signature\r",
            "        timestamp: int = int(time.time())\r",
            "        signature_input: bytes = self.group._enhanced_encode_for_hash(\r",
            "            participant_id,\r",
            "            original_y,\r",
            "            sum_zero_shares,\r",
            "            new_y,\r",
            "            share_fingerprint,\r",
            "            timestamp,\r",
            "            \"consistency_proof\",\r",
            "        )\r",
            "        signature: str\r",
            "        if HAS_BLAKE3:\r",
            "            signature = blake3.blake3(signature_input).hexdigest()\r",
            "        else:\r",
            "            signature = hashlib.sha3_256(signature_input).hexdigest()\r",
            "\r",
            "        # Return the proof structure\r",
            "        return {\r",
            "            \"participant_id\": participant_id,\r",
            "            \"calculated_sum\": int(sum_zero_shares),\r",
            "            \"verified_shares_count\": len(verified_shares),\r",
            "            \"shares_fingerprint\": share_fingerprint,\r",
            "            \"consistency_check\": check_value == new_y,\r",
            "            \"timestamp\": timestamp,\r",
            "            \"signature\": signature,\r",
            "        }\r",
            "\r",
            "    def _process_echo_consistency(\r",
            "        self, zero_commitments: Dict[int, CommitmentList], zero_sharings: Dict[int, ShareDict], participant_ids: List[int]\r",
            "    ) -> Dict[Tuple[int, int], bool]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced echo consistency protocol for Byzantine fault detection.\r",
            "\r",
            "            This implementation provides stronger detection of equivocation (sending different\r",
            "            values to different participants) through secure cryptographic fingerprinting\r",
            "            and comprehensive evidence collection.\r",
            "\r",
            "        Arguments:\r",
            "            zero_commitments (dict): Dictionary of commitments from each party.\r",
            "            zero_sharings (dict): Dictionary of sharings from each party.\r",
            "            participant_ids (list): List of participant IDs.\r",
            "\r",
            "        Inputs:\r",
            "            zero_commitments: Commitments\r",
            "            zero_sharings: Sharings\r",
            "            participant_ids: Participant IDs\r",
            "\r",
            "        Outputs:\r",
            "            dict: Dictionary mapping (party_id, participant_id) to consistency result.\r",
            "            \r",
            "        Side Effects:\r",
            "            Stores Byzantine evidence in self._byzantine_evidence for later access by _detect_byzantine_behavior.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or structures.\r",
            "        \"\"\"\r",
            "\r",
            "        # Validate input parameter types\r",
            "        if not isinstance(zero_commitments, dict):\r",
            "            raise TypeError(\"zero_commitments must be a dictionary\")\r",
            "        if not isinstance(zero_sharings, dict):\r",
            "            raise TypeError(\"zero_sharings must be a dictionary\")\r",
            "        if not isinstance(participant_ids, list):\r",
            "            raise TypeError(\"participant_ids must be a list\")\r",
            "\r",
            "        # Validate the structure of zero_sharings\r",
            "        \r",
            "        party_id: int\r",
            "        party_shares: ShareDict\r",
            "        for party_id, party_shares in zero_sharings.items():\r",
            "            if not isinstance(party_shares, dict):\r",
            "                detailed_msg = (\r",
            "                    f\"Invalid share format for party {party_id}: expected dictionary\"\r",
            "                )\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "            p_id: int\r",
            "            share: SharePoint\r",
            "            for p_id, share in party_shares.items():\r",
            "                if not isinstance(share, tuple) or len(share) != 2:\r",
            "                    detailed_msg = f\"Invalid share from party {party_id} to participant {p_id}: expected (x, y) tuple\"\r",
            "                    message = \"Invalid data structure\"\r",
            "                    self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "\r",
            "        # Validate the structure of zero_commitments\r",
            "        \r",
            "        party_id: int\r",
            "        commitments: CommitmentList\r",
            "        for party_id, commitments in zero_commitments.items():\r",
            "            if not isinstance(commitments, list) or not commitments:\r",
            "                detailed_msg = f\"Invalid commitment format for party {party_id}: expected non-empty list\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "            if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "                detailed_msg = f\"Invalid commitment format for party {party_id}: expected list of (commitment, randomizer) tuples\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "\r",
            "        consistency_results: Dict[Tuple[int, int], bool] = {}\r",
            "\r",
            "        # Create cryptographically secure fingerprints of each sharing\r",
            "        share_fingerprints: Dict[int, Dict[int, bytes]] = {}\r",
            "\r",
            "        \r",
            "        party_id: int\r",
            "        party_shares: ShareDict\r",
            "        for party_id, party_shares in zero_sharings.items():\r",
            "            share_fingerprints[party_id] = {}\r",
            "            \r",
            "            p_id: int\r",
            "            x: FieldElement\r",
            "            y: FieldElement\r",
            "            for p_id, (x, y) in party_shares.items():\r",
            "                if p_id in participant_ids:\r",
            "                    # Create a secure fingerprint using proper domain separation\r",
            "                    message: bytes = self.group._enhanced_encode_for_hash(\r",
            "                        party_id, p_id, x, y, \"echo-consistency-check\"\r",
            "                    )\r",
            "                    fingerprint: bytes = self.hash_algorithm(message).digest()\r",
            "                    share_fingerprints[party_id][p_id] = fingerprint\r",
            "\r",
            "        # Echo broadcast phase: participants share what they received\r",
            "        echo_broadcasts: Dict[int, Dict[int, Tuple[SharePoint, bytes]]] = {}\r",
            "        p_id: int\r",
            "        for p_id in participant_ids:\r",
            "            echo_broadcasts[p_id] = {}\r",
            "            # Collect all shares this participant received\r",
            "            \r",
            "            party_id: int\r",
            "            for party_id in zero_sharings:\r",
            "                if p_id in zero_sharings[party_id]:\r",
            "                    share: SharePoint = zero_sharings[party_id][p_id]\r",
            "                    fingerprint: Optional[bytes] = share_fingerprints[party_id].get(p_id)\r",
            "                    if fingerprint:\r",
            "                        echo_broadcasts[p_id][party_id] = (share, fingerprint)\r",
            "\r",
            "        # Consistency check phase: compare what different participants received\r",
            "        byzantine_evidence: Dict[int, Dict[str, Any]] = {}\r",
            "        \r",
            "        p1_id: int\r",
            "        for p1_id in participant_ids:\r",
            "            \r",
            "            p2_id: int\r",
            "            for p2_id in participant_ids:\r",
            "                if p1_id >= p2_id:  # Only check each pair once\r",
            "                    continue\r",
            "\r",
            "                # Compare what p1 and p2 received from each party\r",
            "                \r",
            "                party_id: int\r",
            "                for party_id in zero_sharings:\r",
            "                    if (\r",
            "                        party_id in echo_broadcasts[p1_id]\r",
            "                        and party_id in echo_broadcasts[p2_id]\r",
            "                    ):\r",
            "\r",
            "                        # Extract shares and fingerprints\r",
            "                        p1_share: SharePoint\r",
            "                        p1_fingerprint: bytes\r",
            "                        p2_share: SharePoint\r",
            "                        p2_fingerprint: bytes\r",
            "                        (p1_share, p1_fingerprint) = echo_broadcasts[p1_id][party_id]\r",
            "                        (p2_share, p2_fingerprint) = echo_broadcasts[p2_id][party_id]\r",
            "\r",
            "                        # Check if party sent consistent values to both participants\r",
            "                        is_consistent: bool = p1_fingerprint == p2_fingerprint\r",
            "\r",
            "                        # Record consistency results for both participants\r",
            "                        consistency_results[(party_id, p1_id)] = is_consistent\r",
            "                        consistency_results[(party_id, p2_id)] = is_consistent\r",
            "\r",
            "                        # If inconsistent, collect evidence of Byzantine behavior\r",
            "                        if not is_consistent:\r",
            "                            if party_id not in byzantine_evidence:\r",
            "                                byzantine_evidence[party_id] = {\r",
            "                                    \"type\": \"equivocation\",\r",
            "                                    \"evidence\": [],\r",
            "                                }\r",
            "\r",
            "                            byzantine_evidence[party_id][\"evidence\"].append(\r",
            "                                {\r",
            "                                    \"participant1\": p1_id,\r",
            "                                    \"share1\": p1_share,\r",
            "                                    \"participant2\": p2_id,\r",
            "                                    \"share2\": p2_share,\r",
            "                                    \"fingerprint1\": p1_fingerprint.hex(),\r",
            "                                    \"fingerprint2\": p2_fingerprint.hex(),\r",
            "                                }\r",
            "                            )\r",
            "\r",
            "        # Store Byzantine evidence in a separate field rather than modifying the\r",
            "        # return structure to maintain compatibility with existing code\r",
            "        self._byzantine_evidence = byzantine_evidence\r",
            "\r",
            "        return consistency_results\r",
            "\r",
            "    def _calculate_optimal_batch_size(self, num_participants: int, security_level: int = None, num_shares: int = None) -> int:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "        Calculate the optimal batch size for verification based on system parameters.\r",
            "\r",
            "        Arguments:\r",
            "            num_participants (int): Number of participants.\r",
            "            security_level (int, optional): Security intensity level (0-10). Higher values result\r",
            "                                           in smaller batches for more granular verification.\r",
            "                                           Default is None (use standard calculation).\r",
            "            num_shares (int, optional): Total number of shares in the system, allowing for\r",
            "                                        more nuanced batch sizing when share distribution is uneven.\r",
            "\r",
            "        Inputs:\r",
            "            num_participants: num_participants\r",
            "            security_level: Optional parameter to adjust batch size based on security requirements\r",
            "            num_shares: Optional parameter to adjust for uneven share distribution\r",
            "\r",
            "        Outputs:\r",
            "            int: Optimal batch size for verification.\r",
            "        \"\"\"\r",
            "        # Validate security_level input\r",
            "        if security_level is not None:\r",
            "            if not isinstance(security_level, (int, float)) or security_level < 0 or security_level > 10:\r",
            "                warnings.warn(\"Invalid security_level (must be 0-10). Using default calculation.\", RuntimeWarning)\r",
            "                security_level = None\r",
            "        \r",
            "        # Validate num_shares input if provided\r",
            "        if num_shares is not None:\r",
            "            if not isinstance(num_shares, int) or num_shares <= 0:\r",
            "                warnings.warn(\"Invalid num_shares (must be a positive integer). Ignoring this parameter.\", RuntimeWarning)\r",
            "                num_shares = None\r",
            "                \r",
            "        # For small numbers, use a smaller batch size\r",
            "        if (num_participants < 10):\r",
            "            base_batch_size = min(8, num_participants)\r",
            "            # Even with small participants, adjust for highly uneven share distribution\r",
            "            if num_shares is not None and num_shares > num_participants * 10:\r",
            "                return max(2, int(base_batch_size / 2))  # More conservative reduction for small systems\r",
            "            return base_batch_size\r",
            "                \r",
            "        # Apply security level adjustment if specified\r",
            "        adjustment_factor = 1.0\r",
            "        if security_level is not None:\r",
            "            # Convert security level (0-10) to a reduction factor (1.0 to 0.4)\r",
            "            # Higher security = smaller batches for more granular verification\r",
            "            adjustment_factor = max(0.4, 1.0 - (security_level / 15))\r",
            "        \r",
            "        # For larger systems, use a batch size that balances efficiency\r",
            "        # with the ability to quickly identify problematic shares\r",
            "        cpu_count: int = 1\r",
            "        try:\r",
            "            import multiprocessing\r",
            "            import math\r",
            "            cpu_count = max(1, multiprocessing.cpu_count())\r",
            "            \r",
            "            # Hybrid approach: Consider both logarithmic scaling and CPU count\r",
            "            # Ensure minimum reasonable batch size with the max(4, ...) operation\r",
            "            log_factor = max(4, int(math.log2(max(2, num_participants)) * 4 * adjustment_factor))\r",
            "            cpu_factor = max(8, int(num_participants // cpu_count * adjustment_factor))\r",
            "            \r",
            "            # Use the smaller of the two factors to keep batches manageable\r",
            "            batch_size = min(32, min(log_factor, cpu_factor))\r",
            "            \r",
            "            # If num_shares is provided, adjust for highly skewed distributions\r",
            "            if num_shares is not None and num_shares > num_participants:\r",
            "                shares_per_participant = num_shares / max(1, num_participants)\r",
            "                if shares_per_participant > 10:  # Only adjust for highly uneven distributions\r",
            "                    # Use logarithmic scaling to avoid extreme reductions\r",
            "                    reduction_factor = min(3, math.log2(shares_per_participant) / 4)  # Cap the reduction\r",
            "                    batch_size = max(4, int(batch_size / reduction_factor))\r",
            "                    \r",
            "            return batch_size\r",
            "        except (ImportError, NotImplementedError):\r",
            "            pass\r",
            "\r",
            "        # Fallback to the original calculation with security adjustment\r",
            "        batch_size = min(32, max(8, int(num_participants // max(1, cpu_count) * adjustment_factor)))\r",
            "        \r",
            "        # Apply the share distribution adjustment to the fallback case as well\r",
            "        if num_shares is not None and num_shares > num_participants:\r",
            "            shares_per_participant = num_shares / max(1, num_participants)\r",
            "            if shares_per_participant > 10:\r",
            "                import math  # Import here in case it wasn't imported earlier\r",
            "                reduction_factor = min(3, math.log2(shares_per_participant) / 4)\r",
            "                batch_size = max(4, int(batch_size / reduction_factor))\r",
            "                \r",
            "        return batch_size\r",
            "\r",
            "    def _prepare_verification_batches(\r",
            "        self, zero_sharings: Dict[int, ShareDict], zero_commitments: Dict[int, CommitmentList], participant_ids: List[int], batch_size: int\r",
            "    ) -> List[List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Prepare efficient verification batches grouped by commitment set.\r",
            "\r",
            "        Arguments:\r",
            "            zero_sharings (dict): Dictionary of sharings from each party.\r",
            "            zero_commitments (dict): Dictionary of commitments from each party.\r",
            "            participant_ids (list): List of participant IDs.\r",
            "            batch_size (int): Size of each batch.\r",
            "\r",
            "        Inputs:\r",
            "            zero_sharings: zero_sharings\r",
            "            zero_commitments: zero_commitments\r",
            "            participant_ids: participant_ids\r",
            "            batch_size: batch_size\r",
            "\r",
            "        Outputs:\r",
            "            list: List of verification batches.\r",
            "        \"\"\"\r",
            "        verification_batches: List[List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]] = []\r",
            "\r",
            "        # Group shares by commitment set for efficient batch verification\r",
            "        commitment_groups: Dict[str, Tuple[CommitmentList, List[Tuple[int, int, FieldElement, FieldElement]]]] = {}\r",
            "        \r",
            "        party_id: int\r",
            "        party_commitments: CommitmentList\r",
            "        for party_id, party_commitments in zero_commitments.items():\r",
            "            # Use a cryptographic hash instead of Python's non-cryptographic hash()\r",
            "            # This prevents potential hash collisions that could lead to incorrect grouping\r",
            "            if HAS_BLAKE3:\r",
            "                hasher = blake3.blake3()\r",
            "            else:\r",
            "                # Fall back to SHA3-256 if BLAKE3 is not available\r",
            "                import hashlib\r",
            "                hasher = hashlib.sha3_256()\r",
            "                \r",
            "            # Generate a stable, cryptographically secure commitment key by hashing all commitment values\r",
            "            for c in party_commitments:\r",
            "                if isinstance(c, tuple) and len(c) > 0:\r",
            "                    # Convert the commitment value to bytes for hashing\r",
            "                    hasher.update(str(c[0]).encode('utf-8'))\r",
            "                    \r",
            "            commitment_key: str = hasher.hexdigest()\r",
            "\r",
            "            if commitment_key not in commitment_groups:\r",
            "                commitment_groups[commitment_key] = (party_commitments, [])\r",
            "\r",
            "            # Fixed: Use zero_sharings[party_id] instead of undefined party_shares\r",
            "            if party_id in zero_sharings:\r",
            "                p_id: int\r",
            "                x: FieldElement\r",
            "                y: FieldElement\r",
            "                for p_id, (x, y) in zero_sharings[party_id].items():\r",
            "                    if p_id in participant_ids:\r",
            "                        commitment_groups[commitment_key][1].append((party_id, p_id, x, y))\r",
            "\r",
            "        # Create batches with optimized size\r",
            "        \r",
            "        commitment_key: str\r",
            "        commitments: CommitmentList\r",
            "        items: List[Tuple[int, int, FieldElement, FieldElement]]\r",
            "        for commitment_key, (commitments, items) in commitment_groups.items():\r",
            "            i: int\r",
            "            for i in range(0, len(items), batch_size):\r",
            "                batch: List[Tuple[int, int, FieldElement, FieldElement]] = items[i : i + batch_size]\r",
            "                batch_items: List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]] = [\r",
            "                    (party_id, p_id, x, y, commitments)\r",
            "                    for party_id, p_id, x, y in batch\r",
            "                ]\r",
            "                verification_batches.append(batch_items)\r",
            "\r",
            "        return verification_batches\r",
            "\r",
            "    def _process_verification_batches(self, verification_batches: List[List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]]) -> List[Tuple[Tuple[int, int], bool]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Process verification batches with optimized parallelism.\r",
            "\r",
            "        Arguments:\r",
            "            verification_batches (list): List of verification batches.\r",
            "\r",
            "        Inputs:\r",
            "            verification_batches: verification_batches\r",
            "\r",
            "        Outputs:\r",
            "            list: List of verification results.\r",
            "        \"\"\"\r",
            "\r",
            "        def verify_batch(batch_items: List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]) ->  List[Tuple[Tuple[int, int], bool]]:\r",
            "            results: Dict[int, Tuple[int, int]] = {}\r",
            "            batch_shares: List[SharePoint] = []\r",
            "            \r",
            "            idx: int\r",
            "            party_id: int\r",
            "            p_id: int\r",
            "            x: FieldElement\r",
            "            y: FieldElement\r",
            "            commitments: CommitmentList\r",
            "            for idx, (party_id, p_id, x, y, commitments) in enumerate(batch_items):\r",
            "                batch_shares.append((x, y))\r",
            "                results[idx] = (party_id, p_id)\r",
            "\r",
            "            # Use batch verification when possible\r",
            "            verification_results: Dict[int, bool]\r",
            "            _: bool\r",
            "            if len(batch_shares) > 1:\r",
            "                _, verification_results = self.batch_verify_shares(\r",
            "                    batch_shares, commitments\r",
            "                )\r",
            "                return [\r",
            "                    (results[idx], is_valid)\r",
            "                    for idx, is_valid in verification_results.items()\r",
            "                ]\r",
            "            else:\r",
            "                # Fallback to individual verification\r",
            "                return [\r",
            "                    (results[idx], self.verify_share(x, y, commitments))\r",
            "                    for idx, (party_id, p_id, x, y, commitments) in enumerate(\r",
            "                        batch_items\r",
            "                    )\r",
            "                ]\r",
            "\r",
            "        # Try parallel verification with improved error handling\r",
            "        verification_results: List[Tuple[Tuple[int, int], bool]] = []\r",
            "        try:\r",
            "            import concurrent.futures\r",
            "\r",
            "            with concurrent.futures.ThreadPoolExecutor() as executor:\r",
            "                # Use a more robust approach for gathering results\r",
            "                future_to_batch: Dict[\"concurrent.futures.Future[List[Tuple[Tuple[int, int], bool]]]\", int] = {\r",
            "                    executor.submit(verify_batch, batch): i\r",
            "                    for i, batch in enumerate(verification_batches)\r",
            "                }\r",
            "\r",
            "                \r",
            "                future: \"concurrent.futures.Future[List[Tuple[Tuple[int, int], bool]]]\"\r",
            "                for future in concurrent.futures.as_completed(future_to_batch):\r",
            "                    try:\r",
            "                        batch_results:  List[Tuple[Tuple[int, int], bool]] = future.result()\r",
            "                        verification_results.extend(batch_results)\r",
            "                    except Exception as e:\r",
            "                        warnings.warn(\r",
            "                            f\"Error in verification batch: {e}\", RuntimeWarning\r",
            "                        )\r",
            "                        # Handle failed batch verification by marking all shares in the batch as invalid\r",
            "                        batch_index = future_to_batch.get(future, -1)\r",
            "                        if 0 <= batch_index < len(verification_batches):\r",
            "                            # Extract party_id and p_id from the failed batch and mark all as invalid\r",
            "                            failed_batch = verification_batches[batch_index]\r",
            "                            invalid_results = [\r",
            "                                ((party_id, p_id), False)\r",
            "                                for party_id, p_id, _, _, _ in failed_batch\r",
            "                            ]\r",
            "                            verification_results.extend(invalid_results)\r",
            "                            \r",
            "                            # Enhanced security warning with more detail for monitoring systems\r",
            "                            fail_msg = (f\"CRITICAL SECURITY ALERT: Batch verification failure detected. \"\r",
            "                                      f\"{len(invalid_results)} shares marked as invalid in batch {batch_index}. \"\r",
            "                                      f\"This could indicate a potential attack or data corruption.\")\r",
            "                            warnings.warn(fail_msg, category=SecurityWarning)\r",
            "                            \r",
            "                            # Log the detailed error for forensic analysis\r",
            "                            if logging:\r",
            "                                try:\r",
            "                                    logging.error(f\"{fail_msg} Original error: {e}\")\r",
            "                                except (ImportError, NameError):\r",
            "                                    pass  # If logging is not available, continue silently\r",
            "        except (ImportError, RuntimeError):\r",
            "            # Fallback to sequential verification with progress tracking\r",
            "            \r",
            "            batch:  List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]\r",
            "            for batch in verification_batches:\r",
            "                verification_results.extend(verify_batch(batch))\r",
            "\r",
            "        return verification_results\r",
            "\r",
            "    def _get_share_value_from_results(self, party_id: int, p_id: int, zero_sharings:  Dict[int, ShareDict]) -> Optional[FieldElement]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Get share value from zero sharings with proper validation.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party.\r",
            "            p_id (int): ID of the participant.\r",
            "            zero_sharings (dict): Dictionary of sharings.\r",
            "        Inputs:\r",
            "            party_id: party_id\r",
            "            p_id: p_id\r",
            "            zero_sharings: zero_sharings\r",
            "        Outputs:\r",
            "            Optional[int]: Share y-value or None if not found.\r",
            "        \"\"\"\r",
            "        if party_id in zero_sharings and p_id in zero_sharings[party_id]:\r",
            "            return zero_sharings[party_id][p_id][1]  # Return y-value\r",
            "\r",
            "        # This should not happen if verification passed\r",
            "        warnings.warn(\r",
            "            f\"Missing share for party {party_id}, participant {p_id}\", RuntimeWarning\r",
            "        )\r",
            "        return None  # Return None instead of 0 to avoid silent failures\r",
            "\r",
            "    def _generate_invalidity_evidence(\r",
            "        self,\r",
            "        party_id: int,\r",
            "        p_id: int,\r",
            "        zero_sharings: Dict[int, ShareDict],\r",
            "        zero_commitments: Dict[int, CommitmentList],\r",
            "        verification_proofs: Dict[int, Dict[int, Any]],\r",
            "        share_verification: bool,\r",
            "        echo_consistency: bool,\r",
            "    ) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate enhanced cryptographic evidence for invalid shares.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party providing the share.\r",
            "            p_id (int): ID of the participant receiving the share.\r",
            "            zero_sharings (dict): Dictionary of sharings.\r",
            "            zero_commitments (dict): Dictionary of commitments.\r",
            "            verification_proofs (dict): Dictionary to store proofs.\r",
            "            share_verification (bool): Whether share verification passed.\r",
            "            echo_consistency (bool): Whether echo consistency check passed.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            p_id: p_id\r",
            "            zero_sharings: zero_sharings\r",
            "            zero_commitments: zero_commitments\r",
            "            verification_proofs: verification_proofs\r",
            "            share_verification: share verification\r",
            "            echo consistency: echo consistency\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        try:\r",
            "            if p_id not in verification_proofs:\r",
            "                verification_proofs[p_id] = {}\r",
            "\r",
            "            # Get the share for detailed evidence\r",
            "            if party_id in zero_sharings and p_id in zero_sharings[party_id]:\r",
            "                share: SharePoint = zero_sharings[party_id][p_id]\r",
            "                commitments: Optional[CommitmentList] = zero_commitments.get(party_id)\r",
            "\r",
            "                if commitments:\r",
            "                    # Create comprehensive proof with additional evidence\r",
            "                    proof: Dict[str, Any] = self._create_invalidity_proof(\r",
            "                        party_id, p_id, share, commitments\r",
            "                    )\r",
            "\r",
            "                    # Add additional evidence about consistency checks\r",
            "                    proof[\"echo_consistency\"] = echo_consistency\r",
            "                    proof[\"share_verification\"] = share_verification\r",
            "\r",
            "                    # Add to verification proofs\r",
            "                    verification_proofs[p_id][party_id] = proof\r",
            "\r",
            "            # Log the issue for security monitoring\r",
            "            warnings.warn(\r",
            "                f\"Invalid share from party {party_id} for participant {p_id}. \"\r",
            "                f\"Verification: {share_verification}, Echo consistency: {echo_consistency}\",\r",
            "                SecurityWarning,\r",
            "            )\r",
            "        except Exception as e:\r",
            "            warnings.warn(f\"Failed to create invalidity proof: {e}\", RuntimeWarning)\r",
            "\r",
            "    def _enhanced_collusion_detection(\r",
            "        self, invalid_shares_detected: Dict[int, List[int]], party_ids: Set[int], echo_consistency: Dict[Tuple[int, int], bool]\r",
            "    ) -> List[int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced collusion detection with improved graph analysis.\r",
            "\r",
            "        Arguments:\r",
            "            invalid_shares_detected (dict): Dictionary of invalid shares.\r",
            "            party_ids (set): Set of party IDs.\r",
            "            echo_consistency (dict): Results of echo consistency checks.\r",
            "\r",
            "        Inputs:\r",
            "            invalid_shares_detected: invalid shares detected\r",
            "            party_ids: party ids\r",
            "            echo_consistency: echo consistency\r",
            "\r",
            "        Outputs:\r",
            "            list: List of potentially colluding parties.\r",
            "        \"\"\"\r",
            "        if not invalid_shares_detected:\r",
            "            return []\r",
            "\r",
            "        # Count how many times each party provided invalid shares\r",
            "        invalid_count: Dict[int, int] = {}\r",
            "        \r",
            "        parties: List[int]\r",
            "        for parties in invalid_shares_detected.values():\r",
            "            party_id: int\r",
            "            for party_id in parties:\r",
            "                invalid_count[party_id] = invalid_count.get(party_id, 0) + 1\r",
            "\r",
            "        # Calculate a suspicious threshold with dynamic adjustment\r",
            "        total_participants: int = len(invalid_shares_detected)\r",
            "        suspicious_threshold: float = max(1, 0.25 * total_participants)\r",
            "\r",
            "        # Identify suspicious parties with high invalid share counts\r",
            "        suspicious_parties: List[int] = [\r",
            "            party\r",
            "            for party, count in invalid_count.items()\r",
            "            if count > suspicious_threshold\r",
            "        ]\r",
            "\r",
            "        # Enhanced detection: look for patterns in echo consistency failures\r",
            "        if echo_consistency:\r",
            "            inconsistent_parties: Set[int] = set()\r",
            "            \r",
            "            party_id: int\r",
            "            is_consistent: bool\r",
            "            for (party_id, _), is_consistent in echo_consistency.items():\r",
            "                if not is_consistent and party_id not in inconsistent_parties:\r",
            "                    inconsistent_parties.add(party_id)\r",
            "\r",
            "            # Add parties with echo inconsistencies to suspicious list\r",
            "            party: int\r",
            "            for party in inconsistent_parties:\r",
            "                if party not in suspicious_parties:\r",
            "                    suspicious_parties.append(party)\r",
            "\r",
            "        # Identify potential collusion patterns\r",
            "        potential_colluders: List[int] = []\r",
            "\r",
            "        # Check for targeting patterns (multiple suspicious parties targeting the same participants)\r",
            "        if len(suspicious_parties) > 1:\r",
            "            targeted_participants: Dict[int, Set[int]] = {}\r",
            "            \r",
            "            party_id: int\r",
            "            for party_id in suspicious_parties:\r",
            "                targeted_participants[party_id] = set()\r",
            "                \r",
            "                p_id: int\r",
            "                parties: List[int]\r",
            "                for p_id, parties in invalid_shares_detected.items():\r",
            "                    if party_id in parties:\r",
            "                        targeted_participants[party_id].add(p_id)\r",
            "\r",
            "            # Find parties with similar targeting patterns\r",
            "            i: int\r",
            "            p1: int\r",
            "            for i, p1 in enumerate(suspicious_parties):\r",
            "                \r",
            "                p2: int\r",
            "                for p2 in suspicious_parties[i + 1 :]:\r",
            "                    if p1 in targeted_participants and p2 in targeted_participants:\r",
            "                        p1_targets: Set[int] = targeted_participants[p1]\r",
            "                        p2_targets: Set[int] = targeted_participants[p2]\r",
            "\r",
            "                        # Calculate Jaccard similarity of target sets\r",
            "                        if p1_targets and p2_targets:\r",
            "                            overlap: int = len(p1_targets.intersection(p2_targets))\r",
            "                            union: int = len(p1_targets.union(p2_targets))\r",
            "\r",
            "                            # Higher threshold (0.8) for stronger evidence\r",
            "                            if union > 0 and overlap / union > 0.8:\r",
            "                                if p1 not in potential_colluders:\r",
            "                                    potential_colluders.append(p1)\r",
            "                                if p2 not in potential_colluders:\r",
            "                                    potential_colluders.append(p2)\r",
            "\r",
            "        return potential_colluders\r",
            "\r",
            "    def create_polynomial_proof(self, coefficients: List[FieldElement], commitments: CommitmentList) -> ProofDict:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Creates a zero-knowledge proof of knowledge of the polynomial coefficients\r",
            "            using hash-based commitments for post-quantum security.\r",
            "\r",
            "            This implementation follows Baghery's secure framework with enhanced domain\r",
            "            separation and proper randomization to ensure security against quantum attacks.\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "            commitments (list): Commitments to these coefficients (list of tuples).\r",
            "\r",
            "        Inputs:\r",
            "            coefficients: coefficients\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            dict: Proof data structure containing the necessary components for verification.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or structures.\r",
            "            ValueError: If coefficients or commitments lists are empty.\r",
            "        \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(coefficients, list):\r",
            "            raise TypeError(\"coefficients must be a list\")\r",
            "        if not coefficients:\r",
            "            raise ValueError(\"coefficients list cannot be empty\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            raise ValueError(\"commitments list cannot be empty\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Convert coefficients to integers for consistent arithmetic\r",
            "        coeffs_int: List[FieldElement] = [gmpy2.mpz(coeff) % self.field.prime for coeff in coefficients]\r",
            "\r",
            "        # Generate secure random blinding factors\r",
            "        blindings: List[FieldElement] = [self.group.secure_random_element() for _ in range(len(coeffs_int))]\r",
            "\r",
            "        # Create hash-based commitments to blinding factors with domain separation\r",
            "        blinding_commitments: List[Tuple[FieldElement, FieldElement]] = []\r",
            "        i: int\r",
            "        b: FieldElement\r",
            "        for i, b in enumerate(blindings):\r",
            "            # Generate secure randomizer for each blinding factor\r",
            "            r_b: FieldElement = self.group.secure_random_element()\r",
            "\r",
            "            # Compute hash-based commitment with context for domain separation\r",
            "            commitment: FieldElement = self._compute_hash_commitment(\r",
            "                b, r_b, i, \"polynomial_proof_blinding\"\r",
            "            )\r",
            "            blinding_commitments.append((commitment, r_b))\r",
            "\r",
            "        # Generate timestamp for the proof\r",
            "        timestamp: int = int(time.time())\r",
            "        \r",
            "        # Generate non-interactive challenge using Fiat-Shamir transform with enhanced encoding\r",
            "        # Include all public values in the challenge computation to prevent manipulation\r",
            "        challenge_input: bytes = self.group._enhanced_encode_for_hash(\r",
            "            \"polynomial_proof\",  # Domain separator\r",
            "            self.generator,\r",
            "            self.group.prime,\r",
            "            [c[0] if isinstance(c, tuple) and len(c) > 0 else 0 for c in commitments],  # Commitment values, safely accessed\r",
            "            [bc[0] if isinstance(bc, tuple) and len(bc) > 0 else 0 for bc in blinding_commitments],  # Blinding commitment values, safely accessed\r",
            "            timestamp,  # Use the same timestamp that will be stored in the proof\r",
            "        )\r",
            "\r",
            "        # Hash the challenge input using the configured hash algorithm\r",
            "        challenge_hash: bytes = self.hash_algorithm(challenge_input).digest()\r",
            "        challenge: FieldElement = int.from_bytes(challenge_hash, \"big\") % self.field.prime\r",
            "\r",
            "        # Compute responses using sensitive coefficients - this should be constant-time\r",
            "        responses: List[FieldElement] = [\r",
            "            (b + challenge * a) % self.field.prime\r",
            "            for b, a in zip(blindings, coeffs_int)\r",
            "        ]\r",
            "\r",
            "        # Safely extract commitment randomizers regardless of tuple length\r",
            "        commitment_randomizers: List[FieldElement] = []\r",
            "        for c in commitments:\r",
            "            if len(c) >= 2:\r",
            "                commitment_randomizers.append(c[1])\r",
            "            else:\r",
            "                raise ValueError(\"Each commitment must contain at least two elements (commitment, randomizer)\")\r",
            "\r",
            "        # Return complete proof structure including all values needed for verification\r",
            "        proof: ProofDict = {\r",
            "            \"blinding_commitments\": blinding_commitments,\r",
            "            \"challenge\": int(challenge),\r",
            "            \"responses\": [int(r) for r in responses],\r",
            "            \"commitment_randomizers\": [int(r) for r in commitment_randomizers],\r",
            "            \"blinding_randomizers\": [int(r) for _, r in blinding_commitments],\r",
            "            \"timestamp\": timestamp,  # Store timestamp for verification\r",
            "        }\r",
            "        return proof\r",
            "    \r",
            "    def verify_polynomial_proof(self, proof: ProofDict, commitments: CommitmentList) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verifies a zero-knowledge proof of knowledge of polynomial coefficients\r",
            "            using hash-based commitment verification for post-quantum security.\r",
            "\r",
            "            This method validates that the prover knows the coefficients without revealing them,\r",
            "            using only the hash-based commitments and the provided proof.\r",
            "\r",
            "        Arguments:\r",
            "            proof (dict): Proof data structure from create_polynomial_proof.\r",
            "            commitments (list): Commitments to the polynomial coefficients (list of tuples).\r",
            "\r",
            "        Inputs:\r",
            "            proof: proof\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if verification succeeds, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If commitments list is empty or proof structure is invalid.\r",
            "    \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(proof, dict):\r",
            "            raise TypeError(\"proof must be a dictionary\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            raise ValueError(\"commitments list cannot be empty\")\r",
            "\r",
            "        # Extract proof components with parameter validation\r",
            "        blinding_commitments: List[Tuple[FieldElement, FieldElement]]\r",
            "        challenge: FieldElement\r",
            "        responses: List[FieldElement]\r",
            "        commitment_randomizers: List[FieldElement]\r",
            "        blinding_randomizers: List[FieldElement]\r",
            "        timestamp: int\r",
            "\r",
            "        try:\r",
            "            blinding_commitments = proof[\"blinding_commitments\"]\r",
            "            challenge = proof[\"challenge\"]\r",
            "            responses = proof[\"responses\"]\r",
            "            commitment_randomizers = proof[\"commitment_randomizers\"]\r",
            "            blinding_randomizers = proof[\"blinding_randomizers\"]\r",
            "            timestamp = proof.get(\"timestamp\")  # Get timestamp for challenge reconstruction\r",
            "        except (KeyError, TypeError) as e:\r",
            "            raise ValueError(f\"Incomplete or malformed proof structure: {str(e)}\")\r",
            "\r",
            "        # Enhanced validation for proof structure - changed from warnings to exceptions for security-critical failures\r",
            "        if not isinstance(blinding_commitments, list):\r",
            "            raise ValueError(\"blinding_commitments must be a list\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in blinding_commitments):\r",
            "            raise ValueError(\"Each blinding commitment must be a tuple with at least (commitment, randomizer)\")\r",
            "        if not isinstance(challenge, (int, gmpy2.mpz)):\r",
            "            raise ValueError(\"challenge must be an integer\")\r",
            "        if not isinstance(responses, list) or not all(\r",
            "            isinstance(r, (int, gmpy2.mpz)) for r in responses\r",
            "        ):\r",
            "            raise ValueError(\"responses must be a list of integers\")\r",
            "\r",
            "        # Validate that all component lists have the correct size\r",
            "        if (\r",
            "            len(responses) != len(commitments)\r",
            "            or len(blinding_commitments) != len(commitments)\r",
            "            or len(commitment_randomizers) != len(commitments)\r",
            "            or len(blinding_randomizers) != len(commitments)\r",
            "        ):\r",
            "            detailed_msg = f\"Inconsistent lengths in proof components. responses: {len(responses)}, commitments: {len(commitments)}, blinding_commitments: {len(blinding_commitments)}, commitment_randomizers: {len(commitment_randomizers)}, blinding_randomizers: {len(blinding_randomizers)}\"\r",
            "            raise ValueError(f\"Invalid proof structure: {detailed_msg}\")\r",
            "\r",
            "        # Convert challenge to gmpy2.mpz once before the loop to avoid repeated conversion\r",
            "        challenge_mpz = gmpy2.mpz(challenge)\r",
            "        \r",
            "        # Verify each coefficient's proof - MODIFIED to prevent timing side-channels\r",
            "        all_valid: bool = True  # Track verification results without early return\r",
            "\r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Verify response equation for hash-based commitments:\r",
            "            # H(z_i, r_z_i, i) = C_b_i + challenge * C_i\r",
            "\r",
            "            # 1. Compute combined randomizer for the response: r_z_i = r_b_i + challenge * r_i\r",
            "            response_randomizer: FieldElement = (\r",
            "                blinding_randomizers[i] + challenge_mpz * commitment_randomizers[i]\r",
            "            ) % self.field.prime\r",
            "\r",
            "            # 2. Compute the hash commitment for the response\r",
            "            computed_commitment: FieldElement = self._compute_hash_commitment(\r",
            "                responses[i], response_randomizer, i, \"polynomial_proof_response\"\r",
            "            )\r",
            "\r",
            "            # 3. Compute the expected commitment: C_b_i + challenge * C_i\r",
            "            # Fixed: Safer access to tuple elements with validation\r",
            "            if not isinstance(blinding_commitments[i], tuple) or len(blinding_commitments[i]) < 1:\r",
            "                raise ValueError(f\"Invalid blinding commitment format at index {i}\")\r",
            "                \r",
            "            blinding_commitment_value: FieldElement = blinding_commitments[i][0]\r",
            "            \r",
            "            # Fixed: Safer way to access commitment values without unsafe cast\r",
            "            if not isinstance(commitments[i], tuple) or len(commitments[i]) < 1:\r",
            "                raise ValueError(f\"Invalid commitment format at index {i}\")\r",
            "                \r",
            "            commitment_value: FieldElement = commitments[i][0]\r",
            "            \r",
            "            # Convert to consistent numeric types for arithmetic\r",
            "            blinding_commitment_value = gmpy2.mpz(blinding_commitment_value)\r",
            "            commitment_value = gmpy2.mpz(commitment_value)\r",
            "            \r",
            "            expected_commitment: FieldElement = (\r",
            "                blinding_commitment_value + challenge_mpz * commitment_value\r",
            "            ) % self.group.prime\r",
            "\r",
            "            # 4. Update validity flag without early return\r",
            "            all_valid &= constant_time_compare(computed_commitment, expected_commitment)\r",
            "\r",
            "        return all_valid\r",
            "\r",
            "    def _detect_byzantine_behavior(\r",
            "        self, party_id: int, commitments: CommitmentList, shares: ShareDict, consistency_results: Optional[Dict[Tuple[int, int], bool]] = None\r",
            "    ) -> Tuple[bool, Dict[str, Any]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced Byzantine fault detection for comprehensive security analysis.\r",
            "\r",
            "            Detects multiple types of malicious behavior including inconsistent shares,\r",
            "            invalid commitments, and equivocation.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party to check.\r",
            "            commitments (list): Commitments from this party.\r",
            "            shares (dict): Shares distributed by this party.\r",
            "            consistency_results (dict, optional): Results from echo consistency checks.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            commitments: commitments\r",
            "            shares: shares\r",
            "            consistency_results: consistency results\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (is_byzantine, evidence).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "        \"\"\"\r",
            "        evidence: Dict[str, Any] = {}\r",
            "        is_byzantine: bool = False\r",
            "\r",
            "        # Input validation\r",
            "        if not isinstance(party_id, (int, str)):\r",
            "            raise TypeError(\"party_id must be an integer or string\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not isinstance(shares, dict):\r",
            "            raise TypeError(\"shares must be a dictionary\")\r",
            "        if consistency_results is not None and not isinstance(\r",
            "            consistency_results, dict\r",
            "        ):\r",
            "            raise TypeError(\"consistency_results must be a dictionary if provided\")\r",
            "\r",
            "        # Check 1: Are all commitments valid?\r",
            "        if not commitments:\r",
            "            evidence[\"invalid_commitments\"] = \"Missing commitments\"\r",
            "            return True, evidence\r",
            "        \r",
            "        # Validate first commitment structure more thoroughly  \r",
            "        if not isinstance(commitments[0], tuple):\r",
            "            evidence[\"invalid_commitments\"] = \"Malformed commitment (not a tuple)\"\r",
            "            return True, evidence\r",
            "            \r",
            "        # For hash-based commitments, verify the first coefficient is a commitment to 0\r",
            "        # Safely extract randomizer regardless of tuple length\r",
            "        if len(commitments[0]) >= 2:\r",
            "            randomizer: FieldElement = commitments[0][1]\r",
            "            expected: FieldElement = self._compute_hash_commitment(0, randomizer, 0, \"polynomial\")\r",
            "            \r",
            "            # Safely access first element of commitment tuple\r",
            "            commitment_value = commitments[0][0] if commitments[0] else None\r",
            "            if commitment_value is None or not constant_time_compare(commitment_value, expected):\r",
            "                evidence[\"invalid_zero_commitment\"] = {\r",
            "                    \"commitment\": int(commitment_value) if commitment_value is not None else None,\r",
            "                    \"expected\": int(expected),\r",
            "                }\r",
            "                is_byzantine = True\r",
            "        else:\r",
            "            evidence[\"invalid_commitment_structure\"] = \"First commitment has incorrect format (insufficient elements)\"\r",
            "            is_byzantine = True\r",
            "\r",
            "        # Check 2: Are all shares consistent with the commitments?\r",
            "        share_consistency: Dict[int, bool] = {}\r",
            "        \r",
            "        recipient_id: int\r",
            "        x: FieldElement\r",
            "        y: FieldElement\r",
            "        for recipient_id, (x, y) in shares.items():\r",
            "            # Verify this share against the commitments\r",
            "            is_valid: bool = self.verify_share(x, y, commitments)\r",
            "            share_consistency[recipient_id] = is_valid\r",
            "\r",
            "            if not is_valid:\r",
            "                if \"inconsistent_shares\" not in evidence:\r",
            "                    evidence[\"inconsistent_shares\"] = {}\r",
            "\r",
            "                # Compute values needed for verification for better diagnostics\r",
            "                randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "                r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "                expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "                # Extract extra_entropy if present (should be in the first coefficient only)\r",
            "                extra_entropy: Optional[bytes] = None\r",
            "                if len(commitments) > 0 and isinstance(commitments[0], tuple) and len(commitments[0]) > 2:\r",
            "                    extra_entropy = commitments[0][2]  # Get extra_entropy from first coefficient\r",
            "\r",
            "                actual_commitment: FieldElement = self._compute_hash_commitment(\r",
            "                    y, r_combined, x, \"verify\", extra_entropy\r",
            "                )\r",
            "\r",
            "                evidence[\"inconsistent_shares\"][recipient_id] = {\r",
            "                    \"x\": int(x),\r",
            "                    \"y\": int(y),\r",
            "                    \"expected_commitment\": int(expected_commitment),\r",
            "                    \"actual_commitment\": int(actual_commitment),\r",
            "                    \"combined_randomizer\": int(r_combined),\r",
            "                }\r",
            "                is_byzantine = True\r",
            "\r",
            "        # Check 3: Look for evidence of equivocation from consistency checks\r",
            "        if (\r",
            "            hasattr(self, \"_byzantine_evidence\")\r",
            "            and party_id in self._byzantine_evidence\r",
            "        ):\r",
            "            evidence[\"equivocation\"] = self._byzantine_evidence[party_id]\r",
            "            is_byzantine = True\r",
            "\r",
            "        return is_byzantine, evidence\r",
            "\r",
            "    def detect_byzantine_party(\r",
            "        self, party_id: int, commitments: CommitmentList, shares: ShareDict, consistency_results: Optional[Dict[Tuple[int, int], bool]] = None\r",
            "    ) -> Tuple[bool, Dict[str, Any]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Public method to detect Byzantine behavior from a specific party.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party to analyze.\r",
            "            commitments (list): Commitments from this party.\r",
            "            shares (dict): Shares distributed by this party.\r",
            "            consistency_results (dict, optional): Optional consistency check results.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            commitments: commitments\r",
            "            shares: shares\r",
            "            consistency_results: consistency results\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (is_byzantine, evidence_details).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If commitments list is empty.\r",
            "        \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(party_id, (int, str)):\r",
            "            raise TypeError(\"party_id must be an integer or string\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            self._raise_sanitized_error(ValueError, \"commitments list cannot be empty\")\r",
            "        if not isinstance(shares, dict):\r",
            "            raise TypeError(\"shares must be a dictionary\")\r",
            "        if consistency_results is not None and not isinstance(\r",
            "            consistency_results, dict\r",
            "        ):\r",
            "            raise TypeError(\"consistency_results must be a dictionary if provided\")\r",
            "\r",
            "        return self._detect_byzantine_behavior(\r",
            "            party_id, commitments, shares, consistency_results\r",
            "        )\r",
            "\r",
            "    def _evaluate_polynomial(self, coefficients: List[FieldElement], x: int) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Evaluate polynomial at point x using constant-time Horner's method.\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "            x (int): Point at which to evaluate the polynomial.\r",
            "        Inputs:\r",
            "            coefficients: Coefficients\r",
            "            x: x\r",
            "\r",
            "        Outputs:\r",
            "            int: Value of polynomial at point x.\r",
            "        \"\"\"\r",
            "        x_int: \"gmpy2.mpz\" = gmpy2.mpz(x)\r",
            "\r",
            "        # Estimate memory requirements for this operation\r",
            "        total_bits: int = sum(\r",
            "            c.bit_length() if hasattr(c, \"bit_length\") else gmpy2.mpz(c).bit_length()\r",
            "            for c in coefficients\r",
            "        )\r",
            "        if not check_memory_safety(\"mul\", x_int, total_bits):\r",
            "            raise MemoryError(\r",
            "                \"Polynomial evaluation would exceed memory limits. \"\r",
            "                \"The polynomial coefficients or evaluation point may be too large.\"\r",
            "            )\r",
            "\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        coeff: FieldElement\r",
            "        for coeff in reversed(coefficients):\r",
            "            result = (result * x_int + gmpy2.mpz(coeff)) % self.field.prime\r",
            "        return result\r",
            "\r",
            "    def _reconstruct_polynomial_coefficients(self, x_values: List[FieldElement], y_values: List[FieldElement], threshold: int) -> List[FieldElement]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Reconstruct polynomial coefficients using quantum-resistant interpolation.\r",
            "\r",
            "        Arguments:\r",
            "            x_values (list): List of x-coordinates.\r",
            "            y_values (list): List of corresponding y-coordinates.\r",
            "            threshold (int): Degree of the polynomial to reconstruct (k).\r",
            "\r",
            "        Inputs:\r",
            "            x_values: x_values\r",
            "            y_values: y_values\r",
            "            threshold: threshold\r",
            "\r",
            "        Outputs:\r",
            "            list: List of reconstructed polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "\r",
            "        Raises:\r",
            "            ParameterError: If not enough points are provided or x-values are not unique.\r",
            "            VerificationError: If the matrix is singular during reconstruction.\r",
            "        \"\"\"\r",
            "        if len(x_values) < threshold:\r",
            "            detailed_msg = f\"Need at least {threshold} points to reconstruct a degree {threshold-1} polynomial, got {len(x_values)}\"\r",
            "            message = f\"Need at least {threshold} points to reconstruct\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Verify that the first 'threshold' x values we'll use are unique\r",
            "        # Convert to int to ensure hashability\r",
            "        if len({int(x) for x in x_values[:threshold]}) < threshold:\r",
            "            detailed_msg = f\"Need at least {threshold} unique x values to reconstruct polynomial, got: {x_values[:threshold]}\"\r",
            "            message = f\"Need at least {threshold} unique x values\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Validate memory usage for matrix operations\r",
            "        max_bit_length: int = max(\r",
            "            max(x.bit_length() if hasattr(x, \"bit_length\") else 0 for x in x_values),\r",
            "            max(y.bit_length() if hasattr(y, \"bit_length\") else 0 for y in y_values),\r",
            "        )\r",
            "\r",
            "        if not check_memory_safety(\"matrix\", threshold, max_bit_length):\r",
            "            raise MemoryError(\r",
            "                f\"Polynomial reconstruction with threshold {threshold} and values of \"\r",
            "                f\"approximately {max_bit_length} bits would exceed memory limits.\"\r",
            "            )\r",
            "\r",
            "        # Use only the required number of points\r",
            "        x_values = x_values[:threshold]\r",
            "        y_values = y_values[:threshold]\r",
            "        prime: FieldElement = self.field.prime\r",
            "\r",
            "        # Special case for threshold=1 (constant polynomial)\r",
            "        if threshold == 1:\r",
            "            return [y_values[0]]\r",
            "\r",
            "        # For threshold > 1, use matrix-based approach\r",
            "        # Create Vandermonde matrix for the system of equations\r",
            "        matrix: List[List[FieldElement]] = []\r",
            "        x: FieldElement\r",
            "        for x in x_values:\r",
            "            row: List[FieldElement] = []\r",
            "            j: int\r",
            "            for j in range(threshold):\r",
            "                row.append(gmpy2.powmod(x, j, prime))\r",
            "            matrix.append(row)\r",
            "\r",
            "        # Solve the system using secure Gaussian elimination\r",
            "        return self._secure_matrix_solve(matrix, y_values, prime)\r",
            "\r",
            "    def _secure_matrix_solve(self, matrix: List[List[FieldElement]], vector: List[FieldElement], prime: Optional[FieldElement] = None) -> List[FieldElement]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "        Solve a linear system using side-channel resistant Gaussian elimination.\r",
            "\r",
            "        Arguments:\r",
            "            matrix (list): Coefficient matrix.\r",
            "            vector (list): Right-hand side vector.\r",
            "            prime (int, optional): Field prime for modular arithmetic.\r",
            "\r",
            "        Inputs:\r",
            "            matrix: matrix\r",
            "            vector: vector\r",
            "            prime: prime\r",
            "\r",
            "        Outputs:\r",
            "            list: Solution vector containing polynomial coefficients.\r",
            "\r",
            "        Raises:\r",
            "            VerificationError: If a non-invertible value is encountered during matrix operations.\r",
            "        \"\"\"\r",
            "        if prime is None:\r",
            "            prime = self.field.prime\r",
            "        else:\r",
            "            # Validate that prime is actually prime when provided externally\r",
            "            if not gmpy2.is_prime(gmpy2.mpz(prime)):\r",
            "                detailed_msg = f\"The provided value {prime} is not a prime number\"\r",
            "                message = \"Invalid prime parameter\"\r",
            "                self._raise_sanitized_error(ValueError, message, detailed_msg)\r",
            "\r",
            "        n: int = len(vector)\r",
            "        \r",
            "        # Validate matrix dimensions match vector length\r",
            "        if len(matrix) != n:\r",
            "            detailed_msg = f\"Matrix rows ({len(matrix)}) must match vector length ({n})\"\r",
            "            message = \"Incompatible dimensions\"\r",
            "            self._raise_sanitized_error(ValueError, message, detailed_msg)\r",
            "            \r",
            "        # Also validate each row has correct length\r",
            "        for i, row in enumerate(matrix):\r",
            "            if len(row) != n:\r",
            "                detailed_msg = f\"Matrix row {i} has {len(row)} elements, but needs {n}\"\r",
            "                message = \"Matrix is not square\"\r",
            "                self._raise_sanitized_error(ValueError, message, detailed_msg)\r",
            "\r",
            "        # Check matrix size limits to prevent memory issues\r",
            "        if n > 1000:  # Reasonable limit for matrix size\r",
            "            raise MemoryError(\r",
            "                f\"Matrix size {n}x{n} exceeds safe processing limits. \"\r",
            "                f\"Consider reducing polynomial degree or threshold.\"\r",
            "            )\r",
            "\r",
            "        # Estimate memory requirements for the matrix operations\r",
            "        max_element: int = 0\r",
            "        \r",
            "        row: List[FieldElement]\r",
            "        for row in matrix:\r",
            "            element: FieldElement\r",
            "            for element in row:\r",
            "                element_size: int = (\r",
            "                    element.bit_length() if hasattr(element, \"bit_length\") else 0\r",
            "                )\r",
            "                max_element = max(max_element, element_size)\r",
            "\r",
            "        # Estimate total memory for the matrix operations - improved calculation\r",
            "        estimated_memory: int = n * n * max(max_element, prime.bit_length()) // 8  # in bytes\r",
            "        if estimated_memory > 1024 * 1024 * 1024:  # 1GB limit\r",
            "            raise MemoryError(\r",
            "                f\"Matrix operation would require approximately {estimated_memory/(1024*1024):.2f}MB, \"\r",
            "                f\"which exceeds the safe limit.\"\r",
            "            )\r",
            "\r",
            "        # Convert to gmpy2 types\r",
            "        matrix_mpz: List[List[\"gmpy2.mpz\"]] = [[gmpy2.mpz(x) for x in row] for row in matrix]\r",
            "        vector_mpz: List[\"gmpy2.mpz\"] = [gmpy2.mpz(x) for x in vector]\r",
            "\r",
            "\r",
            "        # Forward elimination with side-channel resistant operations\r",
            "        i: int\r",
            "        for i in range(n):\r",
            "            # Find pivot using secure method\r",
            "            pivot_row: Optional[int] = self._find_secure_pivot(matrix_mpz, i, n)\r",
            "\r",
            "            if pivot_row is None:\r",
            "                detailed_msg = \"Matrix is singular, cannot solve the system\"\r",
            "                message = \"Matrix is singular\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "            # Implement more side-channel resistant row swap\r",
            "            # Instead of conditional swap, we swap all needed elements unconditionally\r",
            "            # This approach reduces timing variations due to branching\r",
            "            # Only swap columns from the current pivot column onward\r",
            "            for col in range(i, n):\r",
            "                # Constant-time swap using arithmetic operations with explicit int conversion\r",
            "                should_swap = int(pivot_row != i)  # Explicitly convert bool to int\r",
            "                temp = matrix_mpz[i][col]\r",
            "                matrix_mpz[i][col] = should_swap * matrix_mpz[pivot_row][col] + (1 - should_swap) * matrix_mpz[i][col]\r",
            "                matrix_mpz[pivot_row][col] = should_swap * temp + (1 - should_swap) * matrix_mpz[pivot_row][col]\r",
            "            \r",
            "            # Also swap vector elements in a similar manner\r",
            "            temp_v = vector_mpz[i]\r",
            "            should_swap = int(pivot_row != i)  # Explicitly convert bool to int\r",
            "            vector_mpz[i] = should_swap * vector_mpz[pivot_row] + (1 - should_swap) * vector_mpz[i]\r",
            "            vector_mpz[pivot_row] = should_swap * temp_v + (1 - should_swap) * vector_mpz[pivot_row]\r",
            "\r",
            "            # Calculate inverse of pivot using gmpy2.invert instead of powmod\r",
            "            # This is more appropriate for modular inversion in constant time\r",
            "            pivot: \"gmpy2.mpz\" = matrix_mpz[i][i]\r",
            "            pivot_inverse: \"gmpy2.mpz\"\r",
            "            try:\r",
            "                pivot_inverse = gmpy2.invert(pivot, prime)\r",
            "            except ZeroDivisionError:\r",
            "                detailed_msg = f\"Value {pivot} is not invertible modulo {prime}\"\r",
            "                message = \"Value is not invertible\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "            # Scale current row\r",
            "            for j in range(i, n):\r",
            "                matrix_mpz[i][j] = (matrix_mpz[i][j] * pivot_inverse) % prime\r",
            "            vector_mpz[i] = (vector_mpz[i] * pivot_inverse) % prime\r",
            "\r",
            "            # Eliminate other rows with constant-time operations\r",
            "            for j in range(n):\r",
            "                if j != i:\r",
            "                    factor: \"gmpy2.mpz\" = matrix_mpz[j][i]\r",
            "                    for k in range(i, n):\r",
            "                        matrix_mpz[j][k] = (matrix_mpz[j][k] - factor * matrix_mpz[i][k]) % prime\r",
            "                    vector_mpz[j] = (vector_mpz[j] - factor * vector_mpz[i]) % prime\r",
            "\r",
            "        return vector_mpz\r",
            "\r",
            "    def _find_secure_pivot(self, matrix: List[List[\"gmpy2.mpz\"]], col: int, n: int) -> Optional[int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Find a non-zero pivot using side-channel resistant selection.\r",
            "\r",
            "            This method implements a randomized pivot selection strategy that prevents\r",
            "            timing-based side-channel attacks during Gaussian elimination. Instead of\r",
            "            selecting the first suitable pivot (which would create timing variations),\r",
            "            it assigns random values to all potential pivots and selects one with minimal\r",
            "            random value, ensuring constant-time behavior regardless of matrix content.\r",
            "\r",
            "        Arguments:\r",
            "            matrix (list): The matrix being processed.\r",
            "            col (int): Current column index.\r",
            "            n (int): Matrix dimension.\r",
            "\r",
            "        Inputs:\r",
            "            matrix: Matrix of coefficients.\r",
            "            col: Current column being processed.\r",
            "            n: Matrix dimension.\r",
            "\r",
            "        Outputs:\r",
            "            int: Index of selected pivot row or None if no valid pivot exists.\r",
            "\r",
            "        Security properties:\r",
            "            - Constant-time with respect to the values in the matrix\r",
            "            - Uses cryptographically secure randomness via secrets.token_bytes()\r",
            "            - Resistant to timing side-channel attacks\r",
            "            - Prevents information leakage about matrix structure\r",
            "        \"\"\"\r",
            "        # Generate a single random block for all rows at once (more efficient)\r",
            "        range_size: int = n - col\r",
            "        all_random_bytes: bytes = secrets.token_bytes(32 * range_size)\r",
            "\r",
            "        # Find the valid pivot with the smallest random value\r",
            "        min_value: float = float(\"inf\")\r",
            "        pivot_row: Optional[int] = None\r",
            "\r",
            "        # Track if we found any non-zero pivot (for improved security)\r",
            "        found_any_nonzero: bool = False\r",
            "\r",
            "        k: int\r",
            "        for k in range(range_size):\r",
            "            row: int = col + k\r",
            "            # Extract random value for this row\r",
            "            offset: int = k * 32\r",
            "            row_random: int = int.from_bytes(\r",
            "                all_random_bytes[offset : offset + 32], byteorder=\"big\"\r",
            "            )\r",
            "\r",
            "            # Update minimum if valid pivot and has smaller random value\r",
            "            is_nonzero: bool = matrix[row][col] != 0\r",
            "            found_any_nonzero = found_any_nonzero or is_nonzero\r",
            "            \r",
            "            # Use a constant-time approach to update min_value and pivot_row\r",
            "            # Improved constant-time selection using integer masks\r",
            "            swap_mask = int(is_nonzero and row_random < min_value)\r",
            "            min_value = swap_mask * row_random + (1 - swap_mask) * min_value\r",
            "            # Use arithmetic instead of conditional assignment for constant time\r",
            "            new_pivot = row * swap_mask + (pivot_row or 0) * (1 - swap_mask)\r",
            "            pivot_row = new_pivot if (pivot_row is not None or swap_mask) else None\r",
            "\r",
            "        # Enhanced error check - ensure we're not returning a row with a zero pivot\r",
            "        # in case the constant-time logic has a subtle bug\r",
            "        if pivot_row is not None and matrix[pivot_row][col] == 0:\r",
            "            # This should never happen but acts as a safety check\r",
            "            if found_any_nonzero:\r",
            "                # Something went wrong with our constant-time selection\r",
            "                detailed_msg = \"Security error: Selected a zero pivot despite non-zero pivots being available\"\r",
            "                message = \"Security error in pivot selection\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "            return None\r",
            "\r",
            "        return pivot_row\r",
            "        \r",
            "\r",
            "    def create_commitments_with_proof(self, coefficients: List[FieldElement], context: Optional[str] = None) -> Tuple[CommitmentList, ProofDict]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create commitments to polynomial coefficients and generate a zero-knowledge\r",
            "            proof of knowledge of the coefficients in one combined operation.\r",
            "\r",
            "            This provides a more efficient way to generate both commitments and proofs\r",
            "            and is recommended for share distribution where proof of knowledge is needed.\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "            context (str, optional): Optional context string for domain separation.\r",
            "\r",
            "        Inputs:\r",
            "            coefficients: coefficients\r",
            "            context: context\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (commitments, proof) where both are suitable for verification.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(coefficients, list) or not coefficients:\r",
            "            raise TypeError(\"coefficients must be a non-empty list\")\r",
            "\r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "\r",
            "        # Create commitments first\r",
            "        commitments: CommitmentList = self.create_commitments(coefficients, context)\r",
            "\r",
            "        # Generate zero-knowledge proof of knowledge\r",
            "        proof: ProofDict = self.create_polynomial_proof(coefficients, commitments)\r",
            "\r",
            "        return commitments, proof\r",
            "\r",
            "    def verify_commitments_with_proof(self, commitments: CommitmentList, proof: ProofDict, strict_verification: bool = False) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verify that a zero-knowledge proof demonstrates knowledge of the\r",
            "            polynomial coefficients committed to by the given commitments.\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of commitments to polynomial coefficients.\r",
            "            proof (dict): Zero-knowledge proof structure from create_polynomial_proof.\r",
            "            strict_verification (bool): If True, raises an error on challenge verification failure.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "            proof: proof\r",
            "            strict_verification: strict_verification\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the proof is valid, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If commitments list is empty.\r",
            "            SecurityWarning: If proof is missing required keys.\r",
            "            VerificationError: If strict_verification is True and verification fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            raise ValueError(\"commitments list cannot be empty\")\r",
            "        if not isinstance(proof, dict):\r",
            "            raise TypeError(\"proof must be a dictionary\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Validate proof has all required keys before proceeding\r",
            "        required_keys: List[str] = [\r",
            "            \"blinding_commitments\",\r",
            "            \"challenge\",\r",
            "            \"responses\",\r",
            "            \"commitment_randomizers\",\r",
            "            \"blinding_randomizers\",\r",
            "        ]\r",
            "        if not all(key in proof for key in required_keys):\r",
            "            warnings.warn(\"Proof missing required keys\", SecurityWarning)\r",
            "            return False\r",
            "\r",
            "        # Verify the proof with added challenge consistency check\r",
            "        is_valid = self.verify_polynomial_proof(proof, commitments)\r",
            "        \r",
            "        # Optionally check challenge consistency more strictly\r",
            "        if is_valid and not self._verify_challenge_consistency(proof, commitments):\r",
            "            if strict_verification:\r",
            "                detailed_msg = \"Proof verification passed but challenge value appears inconsistent\"\r",
            "                message = \"Challenge verification failed\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "            warnings.warn(\"Challenge verification failed\", SecurityWarning)\r",
            "            return False\r",
            "            \r",
            "        return is_valid\r",
            "\r",
            "    def serialize_commitments_with_proof(self, commitments: CommitmentList, proof: ProofDict) -> str:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Serialize commitments and associated zero-knowledge proof for storage or transmission\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of (hash, randomizer) tuples.\r",
            "            proof (dict): Zero-knowledge proof structure from create_polynomial_proof.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "            proof: proof\r",
            "\r",
            "        Outputs:\r",
            "            str: String with base64-encoded serialized data.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If proof is missing required keys.\r",
            "            SerializationError: If serialization fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(commitments, list) or not commitments:\r",
            "            raise TypeError(\"commitments must be a non-empty list\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple of at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Add validation for proof parameter\r",
            "        if not isinstance(proof, dict):\r",
            "            raise TypeError(\"proof must be a dictionary\")\r",
            "\r",
            "        required_proof_keys: List[str] = [\r",
            "            \"blinding_commitments\",\r",
            "            \"challenge\",\r",
            "            \"responses\",\r",
            "            \"commitment_randomizers\",\r",
            "            \"blinding_randomizers\",\r",
            "            \"timestamp\",\r",
            "        ]\r",
            "        for key in required_proof_keys:\r",
            "            if key not in proof:\r",
            "                raise ValueError(f\"proof is missing required key: {key}\")\r",
            "\r",
            "        if (\r",
            "            not isinstance(proof[\"blinding_commitments\"], list)\r",
            "            or not proof[\"blinding_commitments\"]\r",
            "        ):\r",
            "            raise TypeError(\"proof['blinding_commitments'] must be a non-empty list\")\r",
            "        if not all(\r",
            "            isinstance(c, tuple) and len(c) >= 2 for c in proof[\"blinding_commitments\"]\r",
            "        ):\r",
            "            raise TypeError(\r",
            "                \"Each blinding commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # First serialize the commitments as before\r",
            "        commitment_values: List[Tuple[int, int, Optional[str]]] = [\r",
            "            (int(c), int(r), e.hex() if e else None) for c, r, e in commitments\r",
            "        ]\r",
            "\r",
            "        # Process proof data for serialization\r",
            "        serializable_proof: Dict[str, Any] = {\r",
            "            \"blinding_commitments\": [\r",
            "                (int(c), int(r)) for c, r in proof[\"blinding_commitments\"]\r",
            "            ],\r",
            "            \"challenge\": int(proof[\"challenge\"]),\r",
            "            \"responses\": [int(r) for r in proof[\"responses\"]],\r",
            "            \"commitment_randomizers\": [int(r) for r in proof[\"commitment_randomizers\"]],\r",
            "            \"blinding_randomizers\": [int(r) for r in proof[\"blinding_randomizers\"]],\r",
            "            \"timestamp\": int(proof[\"timestamp\"]),\r",
            "        }\r",
            "\r",
            "        result: Dict[str, Any] = {\r",
            "            \"version\": VSS_VERSION,\r",
            "            \"timestamp\": int(time.time()),\r",
            "            \"generator\": int(self.generator),\r",
            "            \"prime\": int(self.group.prime),\r",
            "            \"commitments\": commitment_values,\r",
            "            \"hash_based\": True,\r",
            "            \"proof\": serializable_proof,\r",
            "            \"has_proof\": True,\r",
            "        }\r",
            "\r",
            "        # Pack with msgpack for efficient serialization\r",
            "        try:\r",
            "            packed_data: bytes = msgpack.packb(result)\r",
            "\r",
            "            # Compute checksum and create wrapper\r",
            "            checksum_wrapper: Dict[str, Any] = {\r",
            "                \"data\": packed_data,\r",
            "                \"checksum\": compute_checksum(packed_data),\r",
            "            }\r",
            "\r",
            "            # Pack the wrapper and encode\r",
            "            packed_wrapper: bytes = msgpack.packb(checksum_wrapper)\r",
            "            return urlsafe_b64encode(packed_wrapper).decode(\"utf-8\")\r",
            "\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to serialize commitments with proof: {e}\"\r",
            "            message = \"Serialization failed\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def deserialize_commitments_with_proof(self, data: str) -> Tuple[CommitmentList, ProofDict, FieldElement, FieldElement, int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Deserialize commitment data including zero-knowledge proof with enhanced security checks\r",
            "\r",
            "        Arguments:\r",
            "            data (str): Serialized commitment data string.\r",
            "\r",
            "        Inputs:\r",
            "            data: Serialized data\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (commitments, proof, generator, prime, timestamp).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If data is not a string or is empty.\r",
            "            SerializationError: If deserialization or validation fails.\r",
            "            SecurityError: If data integrity checks fail.\r",
            "        \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(data, str):\r",
            "            raise TypeError(\"data must be a string\")\r",
            "        if not data:\r",
            "            raise ValueError(\"data cannot be empty\")\r",
            "\r",
            "        try:\r",
            "            # Decode and unpack the data\r",
            "            decoded: bytes = urlsafe_b64decode(data.encode(\"utf-8\"))\r",
            "\r",
            "            # Use Unpacker with security settings - matching the approach in deserialize_commitments\r",
            "            unpacker: \"msgpack.Unpacker\" = msgpack.Unpacker(\r",
            "                use_list=False,  # Use tuples instead of lists for immutability\r",
            "                raw=True,  # Keep binary data as bytes\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,  # 10MB limit\r",
            "            )\r",
            "            unpacker.feed(decoded)\r",
            "\r",
            "            # Define constants for dictionary keys to ensure consistency\r",
            "            CHECKSUM_KEY = b\"checksum\"\r",
            "            DATA_KEY = b\"data\"\r",
            "            HAS_PROOF_KEY = b\"has_proof\"\r",
            "            PROOF_KEY = b\"proof\"\r",
            "            \r",
            "            # Keys for the proof dictionary\r",
            "            BLINDING_COMMITMENTS_KEY = b\"blinding_commitments\"\r",
            "            CHALLENGE_KEY = b\"challenge\"\r",
            "            RESPONSES_KEY = b\"responses\"\r",
            "            COMMITMENT_RANDOMIZERS_KEY = b\"commitment_randomizers\" \r",
            "            BLINDING_RANDOMIZERS_KEY = b\"blinding_randomizers\"\r",
            "            TIMESTAMP_KEY = b\"timestamp\"\r",
            "\r",
            "            wrapper_dict: Dict[bytes, Any]\r",
            "            try:\r",
            "                # Unpack the checksum wrapper\r",
            "                wrapper_dict = unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack msgpack data: {e}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Verify checksum - this is a critical security check\r",
            "            if CHECKSUM_KEY not in wrapper_dict or DATA_KEY not in wrapper_dict:\r",
            "                detailed_msg = \"Missing checksum or data fields in deserialized content\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            packed_data: bytes = wrapper_dict[DATA_KEY]\r",
            "            expected_checksum: int = wrapper_dict[CHECKSUM_KEY]\r",
            "            actual_checksum: int = compute_checksum(packed_data)\r",
            "\r",
            "            # Use constant-time comparison to prevent timing attacks\r",
            "            if not constant_time_compare(actual_checksum, expected_checksum):\r",
            "                detailed_msg = f\"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}\"\r",
            "                message = \"Data integrity check failed - possible tampering detected\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Feed the inner data to a new Unpacker instance\r",
            "            inner_unpacker: \"msgpack.Unpacker\" = msgpack.Unpacker(\r",
            "                use_list=False,\r",
            "                raw=True,\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,\r",
            "            )\r",
            "            inner_unpacker.feed(packed_data)\r",
            "\r",
            "            unpacked_dict: Dict[bytes, Any]\r",
            "            try:\r",
            "                # Proceed with unpacking the actual data\r",
            "                unpacked_dict = inner_unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack inner msgpack data: {e}\"\r",
            "                message = \"Failed to unpack data\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # First deserialize commitments using the existing method\r",
            "            commitments: CommitmentList\r",
            "            generator: FieldElement\r",
            "            prime: FieldElement\r",
            "            timestamp: int\r",
            "            is_hash_based: bool\r",
            "            commitments, generator, prime, timestamp, is_hash_based = (\r",
            "                self.deserialize_commitments(data)\r",
            "            )\r",
            "\r",
            "            # Check if proof data is present\r",
            "            has_proof: bool = unpacked_dict.get(HAS_PROOF_KEY, False)\r",
            "            if not has_proof:\r",
            "                detailed_msg = \"No proof data found in serialized commitments\"\r",
            "                message = \"Missing proof data\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Extract and reconstruct proof\r",
            "            serialized_proof: Optional[Dict[bytes, Any]] = unpacked_dict.get(PROOF_KEY)\r",
            "            if not serialized_proof:\r",
            "                detailed_msg = \"Missing proof data in serialized commitments\"\r",
            "                message = \"Missing proof data\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate proof structure more thoroughly\r",
            "            required_keys: List[bytes] = [\r",
            "                BLINDING_COMMITMENTS_KEY,\r",
            "                CHALLENGE_KEY,\r",
            "                RESPONSES_KEY,\r",
            "                COMMITMENT_RANDOMIZERS_KEY,\r",
            "                BLINDING_RANDOMIZERS_KEY,\r",
            "                TIMESTAMP_KEY,\r",
            "            ]\r",
            "            \r",
            "            for key in required_keys:\r",
            "                if key not in serialized_proof:\r",
            "                    detailed_msg = f\"Proof missing required field: {key.decode('utf-8')}\"\r",
            "                    message = \"Invalid proof structure\"\r",
            "                    self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate types and structures with safer access patterns\r",
            "            if not isinstance(serialized_proof.get(BLINDING_COMMITMENTS_KEY), tuple):\r",
            "                detailed_msg = \"blinding_commitments must be a sequence\"\r",
            "                message = \"Invalid proof structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            if not isinstance(serialized_proof.get(CHALLENGE_KEY), int):\r",
            "                detailed_msg = \"challenge must be an integer\"\r",
            "                message = \"Invalid proof structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Additional validations...\r",
            "            # ...existing code...\r",
            "\r",
            "            # Improved exception handling for specific error types\r",
            "            try:\r",
            "                # Reconstruct the proof with proper structure\r",
            "                proof: ProofDict = {\r",
            "                    \"blinding_commitments\": [\r",
            "                        (gmpy2.mpz(c), gmpy2.mpz(r))\r",
            "                        for c, r in serialized_proof[BLINDING_COMMITMENTS_KEY]\r",
            "                    ],\r",
            "                    \"challenge\": gmpy2.mpz(serialized_proof[CHALLENGE_KEY]),\r",
            "                    \"responses\": [gmpy2.mpz(r) for r in serialized_proof[RESPONSES_KEY]],\r",
            "                    \"commitment_randomizers\": [\r",
            "                        gmpy2.mpz(r) for r in serialized_proof[COMMITMENT_RANDOMIZERS_KEY]\r",
            "                    ],\r",
            "                    \"blinding_randomizers\": [\r",
            "                        gmpy2.mpz(r) for r in serialized_proof[BLINDING_RANDOMIZERS_KEY]\r",
            "                    ],\r",
            "                    \"timestamp\": serialized_proof[TIMESTAMP_KEY],\r",
            "                }\r",
            "            except (TypeError, ValueError, IndexError) as e:\r",
            "                detailed_msg = f\"Failed to convert proof components: {e}\"\r",
            "                message = \"Invalid proof format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "            except Exception as e:\r",
            "                # Keep this general exception handler as a last resort\r",
            "                detailed_msg = f\"Unexpected error reconstructing proof: {e}\"\r",
            "                message = \"Proof reconstruction failed\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Add challenge verification\r",
            "            # Note: This is a simplified example - actual implementation would \r",
            "            # need to call the appropriate challenge computation function\r",
            "            try:\r",
            "                # Verify proof internally with challenge recomputation\r",
            "                if not self._verify_challenge_consistency(proof, commitments):\r",
            "                    warnings.warn(\"Challenge verification failed\", SecurityWarning)\r",
            "                    return commitments, proof, generator, prime, timestamp\r",
            "            except Exception as e:\r",
            "                # Only warn about challenge verification issues, don't fail\r",
            "                warnings.warn(f\"Challenge verification error: {e}\", SecurityWarning)\r",
            "\r",
            "            return commitments, proof, generator, prime, timestamp\r",
            "        except (SerializationError, SecurityError):\r",
            "            # Re-raise specific security exceptions\r",
            "            raise\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to deserialize commitments with proof: {e}\"\r",
            "            message = \"Deserialization failed\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def verify_share_with_proof(self, share_x: FieldElement, share_y: FieldElement, serialized_data: str) -> Tuple[bool, bool]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Comprehensive verification of a share against serialized commitment data with proof\r",
            "\r",
            "        Arguments:\r",
            "            share_x (int): x-coordinate of the share.\r",
            "            share_y (int): y-coordinate of the share.\r",
            "            serialized_data (str): Serialized commitment data with proof.\r",
            "\r",
            "        Inputs:\r",
            "            share_x: share x\r",
            "            share_y: share y\r",
            "            serialized_data: serialized data\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (share_valid, proof_valid) indicating validation results.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            VerificationError: If verification fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(share_x, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_x must be an integer\")\r",
            "        if not isinstance(share_y, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_y must be an integer\")\r",
            "        if not isinstance(serialized_data, str) or not serialized_data:\r",
            "            raise TypeError(\"serialized_data must be a non-empty string\")\r",
            "\r",
            "        try:\r",
            "            # Deserialize the commitments and proof\r",
            "            commitments: CommitmentList\r",
            "            proof: ProofDict\r",
            "            generator: FieldElement\r",
            "            prime: FieldElement\r",
            "            timestamp: int\r",
            "            \r",
            "            commitments, proof, generator, prime, timestamp = (\r",
            "                self.deserialize_commitments_with_proof(serialized_data)\r",
            "            )\r",
            "\r",
            "            # Create a group with the same parameters\r",
            "            group: CyclicGroup = CyclicGroup(prime=prime, generator=generator)\r",
            "\r",
            "            # Create a new VSS instance with this group\r",
            "            temp_config: VSSConfig = VSSConfig()\r",
            "            temp_vss: FeldmanVSS = FeldmanVSS(self.field, temp_config, group)\r",
            "\r",
            "            # Verify both the share and the proof\r",
            "            share_valid: bool = temp_vss.verify_share(share_x, share_y, commitments)\r",
            "            proof_valid: bool = temp_vss.verify_commitments_with_proof(commitments, proof)\r",
            "\r",
            "            return share_valid, proof_valid\r",
            "\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to verify share with proof: {e}\"\r",
            "            message = \"Verification failed\"\r",
            "            self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "\r",
            "# Simplified factory function focused on post-quantum security\r",
            "def get_feldman_vss(field: Any, **kwargs: Any) -> FeldmanVSS:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Factory function to create a post-quantum secure FeldmanVSS instance.\r",
            "\r",
            "    Arguments:\r",
            "        field: MersennePrimeField instance.\r",
            "        **kwargs: Additional configuration parameters.\r",
            "\r",
            "    Inputs:\r",
            "        field: Field\r",
            "\r",
            "    Outputs:\r",
            "        FeldmanVSS: FeldmanVSS instance configured for post-quantum security.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If field is None or does not have a 'prime' attribute of the correct type.\r",
            "    \"\"\"\r",
            "    # Add validation for field parameter\r",
            "    if field is None:\r",
            "        raise TypeError(\"field cannot be None\")\r",
            "\r",
            "    if not hasattr(field, \"prime\"):\r",
            "        raise TypeError(\"field must have 'prime' attribute\")\r",
            "\r",
            "    if not isinstance(field.prime, (int, gmpy2.mpz)):\r",
            "        raise TypeError(\"field.prime must be an integer type\")\r",
            "\r",
            "    config: Optional[VSSConfig] = kwargs.get(\"config\", None)\r",
            "\r",
            "    if config is None:\r",
            "        config = VSSConfig(\r",
            "            prime_bits=4096,  # Always use at least 3072 bits for post-quantum security\r",
            "            safe_prime=True,\r",
            "            use_blake3=True,\r",
            "        )\r",
            "\r",
            "    return FeldmanVSS(field, config)\r",
            "\r",
            "\r",
            "# Integration helper for the main Shamir Secret Sharing implementation\r",
            "def create_vss_from_shamir(shamir_instance: Any) -> FeldmanVSS:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Create a post-quantum secure FeldmanVSS instance compatible with a ShamirSecretSharing instance\r",
            "\r",
            "    Arguments:\r",
            "        shamir_instance: A ShamirSecretSharing instance.\r",
            "\r",
            "    Inputs:\r",
            "        shamir_instance: Shamir instance\r",
            "\r",
            "    Outputs:\r",
            "        FeldmanVSS: FeldmanVSS instance configured to work with the Shamir instance.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If shamir_instance does not have the required attributes.\r",
            "    \"\"\"\r",
            "    # Validate the shamir_instance has required attributes\r",
            "    if not hasattr(shamir_instance, \"field\"):\r",
            "        raise TypeError(\"shamir_instance must have a 'field' attribute\")\r",
            "\r",
            "    if not hasattr(shamir_instance.field, \"prime\"):\r",
            "        raise TypeError(\"shamir_instance.field must have a 'prime' attribute\")\r",
            "\r",
            "    # Get the field from the Shamir instance\r",
            "    field: Any = shamir_instance.field\r",
            "\r",
            "    # Configure VSS based on Shamir's parameters\r",
            "    prime_bits: int = field.prime.bit_length()\r",
            "\r",
            "    if prime_bits < MIN_PRIME_BITS:\r",
            "        warnings.warn(\r",
            "            f\"Shamir instance uses {prime_bits}-bit prime which is less than the \"\r",
            "            f\"recommended {MIN_PRIME_BITS} bits for post-quantum security. \"\r",
            "            f\"Consider regenerating your Shamir instance with stronger parameters.\",\r",
            "            SecurityWarning,\r",
            "        )\r",
            "\r",
            "    # Create a post-quantum secure VSS instance\r",
            "    return get_feldman_vss(field)\r",
            "\r",
            "\r",
            "# Add a helper function to integrate with Pedersen VSS\r",
            "def integrate_with_pedersen(feldman_vss: FeldmanVSS, pedersen_vss: Any, shares: ShareDict, coefficients: List[FieldElement]) -> Dict[str, Any]:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Integrate Feldman VSS with Pedersen VSS for dual verification.\r",
            "\r",
            "        This provides both the binding property from Feldman VSS and the\r",
            "        hiding property from Pedersen VSS, offering the best of both approaches.\r",
            "\r",
            "    Arguments:\r",
            "        feldman_vss: FeldmanVSS instance.\r",
            "        pedersen_vss: PedersenVSS instance.\r",
            "        shares: Dictionary of shares from Shamir secret sharing.\r",
            "        coefficients: Polynomial coefficients used for share generation.\r",
            "\r",
            "    Inputs:\r",
            "        feldman_vss: feldman vss\r",
            "        pedersen_vss: pedersen vss\r",
            "        shares: shares\r",
            "        coefficients: coefficients\r",
            "\r",
            "    Outputs:\r",
            "        dict: Dictionary with both Feldman and Pedersen verification data.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If inputs have incorrect types.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not isinstance(feldman_vss, FeldmanVSS):\r",
            "        raise TypeError(\"feldman_vss must be a FeldmanVSS instance\")\r",
            "    if not hasattr(pedersen_vss, \"create_commitments\"):\r",
            "        raise TypeError(\"pedersen_vss must have a create_commitments method\")\r",
            "    if not isinstance(shares, dict):\r",
            "        raise TypeError(\"shares must be a dictionary\")\r",
            "    if not isinstance(coefficients, list) or not coefficients:\r",
            "        raise TypeError(\"coefficients must be a non-empty list\")\r",
            "\r",
            "    # Generate Feldman commitments\r",
            "    feldman_commitments: CommitmentList = feldman_vss.create_commitments(coefficients)\r",
            "\r",
            "    # Generate Pedersen commitments\r",
            "    pedersen_commitments: List[FieldElement] = pedersen_vss.create_commitments(coefficients)\r",
            "\r",
            "    # Create a zero-knowledge proof that both commitment sets commit to the same values\r",
            "    # This demonstrates that the Feldman and Pedersen schemes are using the same polynomial\r",
            "    proof:  Dict[str, Any] = create_dual_commitment_proof(\r",
            "        feldman_vss,\r",
            "        pedersen_vss,\r",
            "        coefficients,\r",
            "        feldman_commitments,\r",
            "        pedersen_commitments,\r",
            "    )\r",
            "\r",
            "    # Serialize the commitments\r",
            "    feldman_serialized: str = feldman_vss.serialize_commitments(feldman_commitments)\r",
            "    pedersen_serialized: str = pedersen_vss.serialize_commitments(pedersen_commitments)\r",
            "\r",
            "    return {\r",
            "        \"feldman_commitments\": feldman_serialized,\r",
            "        \"pedersen_commitments\": pedersen_serialized,\r",
            "        \"dual_proof\": proof,\r",
            "        \"version\": VSS_VERSION,\r",
            "    }\r",
            "\r",
            "\r",
            "def create_dual_commitment_proof(\r",
            "    feldman_vss: FeldmanVSS, pedersen_vss: Any, coefficients: List[FieldElement], feldman_commitments: CommitmentList, pedersen_commitments: List[FieldElement]\r",
            ") -> Dict[str, Any]:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Create a zero-knowledge proof that Feldman and Pedersen commitments\r",
            "        are to the same polynomial coefficients.\r",
            "\r",
            "    Arguments:\r",
            "        feldman_vss: FeldmanVSS instance.\r",
            "        pedersen_vss: PedersenVSS instance.\r",
            "        coefficients: The polynomial coefficients.\r",
            "        feldman_commitments: Commitments created by Feldman scheme.\r",
            "        pedersen_commitments: Commitments created by Pedersen scheme.\r",
            "\r",
            "    Inputs:\r",
            "        feldman_vss: feldman_vss\r",
            "        pedersen_vss: pedersen_vss\r",
            "        coefficients: coefficients\r",
            "        feldman_commitments: feldman_commitments\r",
            "        pedersen_commitments: pedersen_commitments\r",
            "\r",
            "    Outputs:\r",
            "        dict: Proof data structure.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If inputs have incorrect types.\r",
            "        ValueError: If input lists have inconsistent lengths.\r",
            "    \"\"\"\r",
            "    # Input validation for all parameters\r",
            "    if not isinstance(feldman_vss, FeldmanVSS):\r",
            "        raise TypeError(\"feldman_vss must be a FeldmanVSS instance\")\r",
            "\r",
            "    if not hasattr(pedersen_vss, \"commit_to_blinding_factors\"):\r",
            "        raise TypeError(\"pedersen_vss must have a 'commit_to_blinding_factors' method\")\r",
            "\r",
            "    if not hasattr(pedersen_vss, \"g\") or not hasattr(pedersen_vss, \"h\"):\r",
            "        raise TypeError(\"pedersen_vss must have 'g' and 'h' attributes\")\r",
            "\r",
            "    if not isinstance(coefficients, list) or not coefficients:\r",
            "        raise TypeError(\"coefficients must be a non-empty list\")\r",
            "\r",
            "    if not isinstance(feldman_commitments, list) or not feldman_commitments:\r",
            "        raise TypeError(\"feldman_commitments must be a non-empty list\")\r",
            "\r",
            "    if not isinstance(pedersen_commitments, list) or not pedersen_commitments:\r",
            "        raise TypeError(\"pedersen_commitments must be a non-empty list\")\r",
            "\r",
            "    if len(coefficients) != len(feldman_commitments) or len(coefficients) != len(\r",
            "        pedersen_commitments\r",
            "    ):\r",
            "        raise ValueError(\r",
            "            \"coefficients, feldman_commitments, and pedersen_commitments must have the same length\"\r",
            "        )\r",
            "\r",
            "    # Generate random blinding factors\r",
            "    blindings: List[FieldElement] = [\r",
            "        feldman_vss.group.secure_random_element() for _ in range(len(coefficients))\r",
            "    ]\r",
            "\r",
            "    # Check if we're using hash-based commitments\r",
            "    is_hash_based: bool = isinstance(feldman_commitments[0], tuple)\r",
            "\r",
            "    # Create Feldman commitments to the blinding factors\r",
            "    feldman_blinding_commitments:  List[Union[Tuple[FieldElement, FieldElement], FieldElement]] = []\r",
            "\r",
            "    if is_hash_based:\r",
            "        # Create hash-based blinding commitments (with randomizers)\r",
            "        i: int\r",
            "        b: FieldElement\r",
            "        for i, b in enumerate(blindings):\r",
            "            # Generate secure randomizer for each blinding factor\r",
            "            r_b: FieldElement = feldman_vss.group.secure_random_element()\r",
            "\r",
            "            # Use helper method to compute commitment\r",
            "            commitment: FieldElement = feldman_vss._compute_hash_commitment(b, r_b, i, \"blinding\")\r",
            "\r",
            "            # Store commitment and randomizer as tuple\r",
            "            feldman_blinding_commitments.append((commitment, r_b))\r",
            "    else:\r",
            "        # Create standard blinding commitments (just exponentiation)\r",
            "        feldman_blinding_commitments = [\r",
            "            feldman_vss.group.secure_exp(feldman_vss.generator, b) for b in blindings\r",
            "        ]\r",
            "\r",
            "    # Create Pedersen commitments to the blinding factors\r",
            "    pedersen_blinding_commitments: List[FieldElement] = pedersen_vss.commit_to_blinding_factors(blindings)\r",
            "\r",
            "    # Generate challenge using Fiat-Shamir transform\r",
            "    challenge_input: bytes = feldman_vss.group._enhanced_encode_for_hash(\r",
            "        feldman_vss.generator,\r",
            "        pedersen_vss.g,\r",
            "        pedersen_vss.h,\r",
            "        [fc[0] if isinstance(fc, tuple) else fc for fc in feldman_commitments],\r",
            "        pedersen_commitments,\r",
            "        [\r",
            "            fbc[0] if isinstance(fbc, tuple) else fbc\r",
            "            for fbc in feldman_blinding_commitments\r",
            "        ],\r",
            "        pedersen_blinding_commitments,\r",
            "    )\r",
            "\r",
            "    # Hash the challenge input\r",
            "    challenge_hash: bytes\r",
            "    if HAS_BLAKE3:\r",
            "        challenge_hash = blake3.blake3(challenge_input).digest()\r",
            "    else:\r",
            "        challenge_hash = hashlib.sha3_256(challenge_input).digest()\r",
            "\r",
            "    challenge: FieldElement = int.from_bytes(challenge_hash, \"big\") % feldman_vss.field.prime\r",
            "\r",
            "    # Compute responses\r",
            "    responses: List[FieldElement] = [\r",
            "        (b + challenge * c) % feldman_vss.field.prime\r",
            "        for b, c in zip(blindings, coefficients)\r",
            "    ]\r",
            "\r",
            "    # For hash-based commitments, include combined randomizers for verification\r",
            "    response_randomizers: Optional[List[FieldElement]] = None\r",
            "    if is_hash_based:\r",
            "        response_randomizers = []\r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Fix tuple unpacking - feldman_blinding_commitments are (commitment, randomizer) tuples\r",
            "            commitment_b, r_b = feldman_blinding_commitments[i]  # type: ignore\r",
            "            commitment_a, r_a = feldman_commitments[i]  # type: ignore\r",
            "            r_combined: FieldElement = (r_b + challenge * r_a) % feldman_vss.field.prime\r",
            "            response_randomizers.append(r_combined)\r",
            "\r",
            "    # Return the proof structure\r",
            "    proof: Dict[str, Any] = {\r",
            "        \"feldman_blinding_commitments\": feldman_blinding_commitments,\r",
            "        \"pedersen_blinding_commitments\": pedersen_blinding_commitments,\r",
            "        \"challenge\": int(challenge),\r",
            "        \"responses\": [int(r) for r in responses],\r",
            "    }\r",
            "\r",
            "    # Add response randomizers if using hash-based commitments\r",
            "    if response_randomizers is not None:\r",
            "        proof[\"response_randomizers\"] = [int(r) for r in response_randomizers]\r",
            "\r",
            "    return proof\r",
            "\r",
            "\r",
            "def verify_dual_commitments(\r",
            "    feldman_vss: FeldmanVSS, pedersen_vss: Any, feldman_commitments: CommitmentList, pedersen_commitments: List[FieldElement], proof: Dict[str, Any]\r",
            ") -> bool:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Verify that the Feldman and Pedersen commitments commit to the same values\r",
            "        using constant-time operations to prevent timing side-channels.\r",
            "\r",
            "    Arguments:\r",
            "        feldman_vss: FeldmanVSS instance.\r",
            "        pedersen_vss: PedersenVSS instance.\r",
            "        feldman_commitments: Feldman commitments.\r",
            "        pedersen_commitments: Pedersen commitments.\r",
            "        proof: Proof data structure from create_dual_commitment_proof.\r",
            "\r",
            "    Outputs:\r",
            "        bool: True if verification succeeds, False otherwise.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not isinstance(feldman_vss, FeldmanVSS):\r",
            "        raise TypeError(\"feldman_vss must be a FeldmanVSS instance\")\r",
            "    if not hasattr(pedersen_vss, \"verify_response_equation\"):\r",
            "        raise TypeError(\"pedersen_vss must have a verify_response_equation method\")\r",
            "    if not isinstance(feldman_commitments, list) or not feldman_commitments:\r",
            "        raise TypeError(\"feldman_commitments must be a non-empty list\")\r",
            "    if not isinstance(pedersen_commitments, list) or not pedersen_commitments:\r",
            "        raise TypeError(\"pedersen_commitments must be a non-empty list\")\r",
            "    if not isinstance(proof, dict):\r",
            "        raise TypeError(\"proof must be a dictionary\")\r",
            "\r",
            "    # Add length consistency validation\r",
            "    if len(feldman_commitments) != len(pedersen_commitments):\r",
            "        raise ValueError(\r",
            "            \"feldman_commitments and pedersen_commitments must have the same length\"\r",
            "        )\r",
            "\r",
            "    # Required proof components\r",
            "    required_keys: List[str] = [\r",
            "        \"feldman_blinding_commitments\",\r",
            "        \"pedersen_blinding_commitments\",\r",
            "        \"challenge\",\r",
            "        \"responses\",\r",
            "    ]\r",
            "    if not all(key in proof for key in required_keys):\r",
            "        raise ValueError(\"Proof is missing required components\")\r",
            "\r",
            "    # Validate component lengths\r",
            "    if len(proof[\"responses\"]) != len(feldman_commitments):\r",
            "        raise ValueError(\"Number of responses must match number of commitments\")\r",
            "    if len(proof[\"feldman_blinding_commitments\"]) != len(feldman_commitments):\r",
            "        raise ValueError(\r",
            "            \"Number of feldman_blinding_commitments must match number of commitments\"\r",
            "        )\r",
            "    if len(proof[\"pedersen_blinding_commitments\"]) != len(pedersen_commitments):\r",
            "        raise ValueError(\r",
            "            \"Number of pedersen_blinding_commitments must match number of commitments\"\r",
            "        )\r",
            "\r",
            "    # Extract proof components\r",
            "    feldman_blinding_commitments: List[Union[Tuple[FieldElement, FieldElement], FieldElement]] = proof[\"feldman_blinding_commitments\"]\r",
            "    pedersen_blinding_commitments: List[FieldElement] = proof[\"pedersen_blinding_commitments\"]\r",
            "    challenge: FieldElement = proof[\"challenge\"]\r",
            "    responses: List[FieldElement] = proof[\"responses\"]\r",
            "    response_randomizers: Optional[List[FieldElement]] = proof.get(\"response_randomizers\", None)\r",
            "\r",
            "    # Check if we're using hash-based commitments for Feldman VSS\r",
            "    is_hash_based: bool = isinstance(feldman_commitments[0], tuple)\r",
            "\r",
            "    # Initialize validity flag for constant-time verification\r",
            "    all_valid: bool = True\r",
            "\r",
            "    # Also validate in constant-time that response_randomizers has the right length if needed\r",
            "    if is_hash_based:\r",
            "        all_valid &= response_randomizers is not None\r",
            "        randomizers_valid_len: bool = (\r",
            "            len(response_randomizers) == len(responses)\r",
            "            if response_randomizers is not None\r",
            "            else False\r",
            "        )\r",
            "        all_valid &= randomizers_valid_len\r",
            "\r",
            "    # First verify Pedersen commitments - these use the same approach regardless\r",
            "    i: int\r",
            "    for i in range(len(responses)):\r",
            "        # Verify using Pedersen VSS verification method\r",
            "        pedersen_valid: bool = pedersen_vss.verify_response_equation(\r",
            "            responses[i],\r",
            "            challenge,\r",
            "            pedersen_blinding_commitments[i],\r",
            "            pedersen_commitments[i],\r",
            "        )\r",
            "        all_valid &= pedersen_valid\r",
            "\r",
            "    # Then verify Feldman commitments\r",
            "    if is_hash_based:\r",
            "        # For hash-based commitments, verification requires validating hash output\r",
            "        \r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Instead of skipping iterations, always compute but conditionally update result\r",
            "            response_value: FieldElement = responses[i]\r",
            "            \r",
            "            # Use safe default if randomizers are invalid\r",
            "            r_combined: FieldElement = 0\r",
            "            if response_randomizers is not None and i < len(response_randomizers):\r",
            "                r_combined = response_randomizers[i]\r",
            "            \r",
            "            # Always compute both sides for constant-time behavior\r",
            "            computed: FieldElement = feldman_vss._compute_hash_commitment(\r",
            "                response_value, r_combined, i, \"response\"\r",
            "            )\r",
            "\r",
            "            # Calculate expected commitment\r",
            "            commitment_value: FieldElement = feldman_commitments[i][0]  # type: ignore\r",
            "            blinding_commitment_value: FieldElement = feldman_blinding_commitments[i][0]  # type: ignore\r",
            "            \r",
            "            expected: FieldElement = (\r",
            "                blinding_commitment_value + challenge * commitment_value\r",
            "            ) % feldman_vss.group.prime\r",
            "            \r",
            "            # Determine if this verification should count using constant-time operations\r",
            "            should_check: bool = (response_randomizers is not None and i < len(response_randomizers))\r",
            "            equality_result: bool = constant_time_compare(computed, expected)\r",
            "            \r",
            "            # Improved constant-time conditional update\r",
            "            mask = -int(should_check)  # Creates all 1s (for True) or all 0s (for False)\r",
            "            masked_result = equality_result & mask\r",
            "            all_valid &= (masked_result | ~mask)  # Will be equality_result when should_check is True, otherwise 1\r",
            "    else:\r",
            "        # Standard Feldman commitment verification\r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Calculate left side: g^response[i]\r",
            "            left_side: FieldElement = feldman_vss.group.secure_exp(\r",
            "                feldman_vss.generator, responses[i]\r",
            "            )\r",
            "\r",
            "            # Calculate right side: blinding_commitment[i] * commitment[i]^challenge\r",
            "            commitment_term: FieldElement = feldman_vss.group.secure_exp(\r",
            "                feldman_commitments[i], challenge # type: ignore\r",
            "            )\r",
            "            right_side: FieldElement = feldman_vss.group.mul(\r",
            "                feldman_blinding_commitments[i], commitment_term # type: ignore\r",
            "            )\r",
            "\r",
            "            # Check equality using constant-time comparison\r",
            "            all_valid &= constant_time_compare(left_side, right_side)\r",
            "\r",
            "    return all_valid"
        ],
        "afterPatchFile": [
            "\"\"\"\r",
            "Post-Quantum Secure Feldman's Verifiable Secret Sharing (VSS) Implementation\r",
            "\r",
            "Version 0.8.0b2\r",
            "Developed in 2025 by David Osipov\r",
            "Licensed under the MIT License\r",
            "\r",
            "This module provides a secure, production-ready implementation of Feldman's VSS scheme\r",
            "with post-quantum security by design. It enhances Shamir's Secret Sharing with\r",
            "mathematical verification capabilities while remaining resistant to quantum attacks\r",
            "through hash-based commitments.\r",
            "\r",
            "Key Features:\r",
            "\r",
            "1.  **Post-Quantum Security:** Exclusively uses hash-based commitments (BLAKE3 or SHA3-256)\r",
            "    for proven resistance to quantum computer attacks. No reliance on discrete logarithm\r",
            "    problems.\r",
            "2.  **Secure Group Operations:** Employs the `CyclicGroup` class, which uses `gmpy2` for\r",
            "    efficient and secure modular arithmetic. Includes optimized exponentiation\r",
            "    (with precomputation and a thread-safe LRU cache) and multi-exponentiation.\r",
            "3.  **Efficient Batch Verification:** `batch_verify_shares` provides optimized verification\r",
            "    of multiple shares against the same commitments, significantly improving performance\r",
            "    for large numbers of shares.\r",
            "4.  **Serialization and Deserialization:** `serialize_commitments` and\r",
            "    `deserialize_commitments` methods provide secure serialization and deserialization of\r",
            "    commitment data, including checksums for integrity verification and handling of\r",
            "    extra entropy for low-entropy secrets.\r",
            "5.  **Comprehensive Validation and Error Handling:** Extensive input validation and error\r",
            "    handling throughout the code to prevent misuse and ensure robustness.  Includes\r",
            "    detailed error messages with forensic data collection for debugging and security\r",
            "    analysis. Sanitized errors are used by default to prevent information leakage.\r",
            "6.  **Fault Injection Countermeasures:** Uses redundant computation (`secure_redundant_execution`)\r",
            "    and constant-time comparisons (`constant_time_compare`) to mitigate fault injection attacks.\r",
            "7.  **Zero-Knowledge Proofs:** Supports the creation and verification of zero-knowledge\r",
            "    proofs of polynomial knowledge, allowing a prover to demonstrate knowledge of the\r",
            "    secret polynomial without revealing the coefficients.\r",
            "8.  **Share Refreshing:** Implements an enhanced version of Chen & Lindell's Protocol 5\r",
            "    for secure share refreshing, with improved Byzantine fault tolerance, adaptive\r",
            "    quorum-based Byzantine detection, and optimized verification.\r",
            "9.  **Integration with Pedersen VSS:** Includes helper functions (`integrate_with_pedersen`,\r",
            "    `create_dual_commitment_proof`, `verify_dual_commitments`) for combining Feldman VSS\r",
            "    with Pedersen VSS, providing both binding and hiding properties.\r",
            "10. **Configurable Parameters:** The `VSSConfig` class allows customization of security\r",
            "    parameters, including the prime bit length, safe prime usage, hash algorithm\r",
            "    (BLAKE3 or SHA3-256), and LRU cache size.\r",
            "11. **Deterministic Hashing:** Guarantees deterministic commitment generation across different\r",
            "    platforms and execution environments by using fixed-length byte representations for\r",
            "    integers in hash calculations.\r",
            "12. **Thread-Safe LRU Cache:** Employs a `SafeLRUCache` for efficient and thread-safe caching\r",
            "    of exponentiation results, with bounded memory usage.\r",
            "13. **Memory Safety:**  Includes a `MemoryMonitor` and `check_memory_safety` to prevent\r",
            "    excessive memory allocation, mitigating potential denial-of-service vulnerabilities.\r",
            "\r",
            "System Requirements:\r",
            "\r",
            "-   For threshold (t) = 50 with 4096-bit values: At least 2GB RAM\r",
            "-   For threshold (t) = 100 with 4096-bit values: At least 4GB RAM\r",
            "-   For threshold (t) > 100 with 4096-bit values: Consider increasing RAM or reducing parameters\r",
            "\r",
            "The memory requirements scale approximately as O(t\u00b2 * bit_length).\r",
            "\r",
            "**Python Version Compatibility:**\r",
            "\r",
            "-   **Minimum Supported Version: Python 3.8**\r",
            "-   **Recommended Version: Python 3.13.2 (or later)**\r",
            "\r",
            "While the library is designed to be compatible with Python 3.8 and above, using\r",
            "the latest stable release (currently 3.13.2) is highly recommended for optimal\r",
            "performance, security, and access to the latest language features.\r",
            "\r",
            "This library takes advantage of features introduced in various Python versions:\r",
            "\r",
            "-   **Python 3.6:** The `secrets` module, used for cryptographically secure random\r",
            "    number generation, was introduced in Python 3.6.\r",
            "-   **Python 3.7:** Data classes (`@dataclass` decorator), used in `VSSConfig`, were\r",
            "    introduced, simplifying class creation and reducing boilerplate code. Dictionaries\r",
            "    became ordered by insertion, providing more predictable behavior.\r",
            "-   **Python 3.8:**  `typing.TypedDict`, used for defining the structure of dictionaries\r",
            "    holding proof and verification data, became available. This version also introduced\r",
            "    positional-only parameters (though not currently used in this library, they represent\r",
            "    good practice for future development).  Audit hooks (PEP 578) were introduced,\r",
            "    allowing for better security monitoring (though not directly used by the library's core logic).\r",
            "-   **Python 3.9 and later:**  Continue to offer improvements in performance, type hinting,\r",
            "    and general security.  While not *strictly* required, using the newest Python\r",
            "    versions is generally beneficial. Python 3.13 specifically removed crypt module and\r",
            "    improved SSL, but does not have new cryptographic features.\r",
            "\r",
            "Security Considerations:\r",
            "\r",
            "-   Always uses at least 4096-bit prime fields for post-quantum security (configurable).\r",
            "-   Strongly recommends using safe primes (where (p-1)/2 is also prime) for enhanced security.\r",
            "-   Defaults to BLAKE3 for cryptographic hashing (faster and more secure than SHA3-256),\r",
            "    but falls back to SHA3-256 if BLAKE3 is not available.\r",
            "-   Designed for seamless integration with Shamir's Secret Sharing implementation.\r",
            "-   Implements countermeasures against timing attacks, fault injection attacks, and\r",
            "    Byzantine behavior.\r",
            "-   Uses cryptographically secure random number generation (secrets module) where needed.\r",
            "-   Provides detailed error messages for debugging and security analysis\r",
            "    (`sanitize_errors: bool = True` needs to be turned to `False`).\r",
            "\r",
            "Known Security Vulnerabilities:\r",
            "\r",
            "This library contains several timing side-channel and fault injection vulnerabilities that cannot be adequately addressed in pure Python:\r",
            "\r",
            "1.  **Timing Side-Channels in Matrix Operations**: Functions like `_find_secure_pivot` and `_secure_matrix_solve` cannot guarantee constant-time execution in Python, potentially leaking secret information.\r",
            "\r",
            "2.  **Non-Constant-Time Comparison**: The `constant_time_compare` function does not provide true constant-time guarantees due to Python's execution model.\r",
            "\r",
            "**Status**: These vulnerabilities require implementation in a lower-level language like Rust to fix properly. The library should be considered experimental until these issues are addressed.\r",
            "\r",
            "**Planned Resolution**: Future versions will integrate with Rust components for security-critical operations.\r",
            "\r",
            "**False-Positive Vulnerabilities:**\r",
            "\r",
            "1.  **Use of `random.Random()` in `_refresh_shares_additive`:** The code uses `random.Random()` seeded with cryptographically strong material (derived from a master secret and a party ID) within the `_refresh_shares_additive` function. While `random.Random()` is *not* generally suitable for cryptographic purposes, its use *here* is intentional and secure. The purpose is to generate *deterministic* but *unpredictable* values for the zero-sharing polynomials. The security comes from the cryptographically strong seed, *not* from the `random.Random()` algorithm itself. This is a deliberate design choice to enable verification and reduce communication overhead in the share refreshing protocol. It is *not* a source of cryptographic weakness.\r",
            "\r",
            "Note: This implementation is fully compatible with the ShamirSecretSharing class in\r",
            "the main module and is optimized to work in synergy with Pedersen VSS.\r",
            "\r",
            "Repository: https://github.com/DavidOsipov/PostQuantum-Feldman-VSS\r",
            "PyPI: https://pypi.org/project/PostQuantum-Feldman-VSS/\r",
            "\r",
            "Developer: David Osipov\r",
            "    Github Profile: https://github.com/DavidOsipov\r",
            "    Email: personal@david-osipov.vision\r",
            "    PGP key: https://openpgpkey.david-osipov.vision/.well-known/openpgpkey/david-osipov.vision/D3FC4983E500AC3F7F136EB80E55C4A47454E82E.asc\r",
            "    PGP fingerprint: D3FC 4983 E500 AC3F 7F13 6EB8 0E55 C4A4 7454 E82E\r",
            "    Website: https://david-osipov.vision\r",
            "    LinkedIn: https://www.linkedin.com/in/david-osipov/\r",
            "    \"\"\"\r",
            "\r",
            "# /// script\r",
            "# requires-python = \">=3.8\"\r",
            "# dependencies = [\r",
            "#   \"gmpy2 == 2.2.1\",\r",
            "#   \"msgpack == 1.1.0\",\r",
            "#   \"blake3 == 1.0.4; platform_system != 'Emscripten'\",\r",
            "#   \"psutil == 7.0.0; os_name != 'Emscripten'\" # Optional dependency\r",
            "# ]\r",
            "# ///\r",
            "\r",
            "# mypy: disallow-untyped-defs=False\r",
            "# mypy: disallow-incomplete-defs=False\r",
            "# pyright: reportOptionalMemberAccess=false\r",
            "\r",
            "import hashlib\r",
            "import logging\r",
            "import random\r",
            "import secrets\r",
            "import threading\r",
            "import time\r",
            "import traceback\r",
            "import warnings\r",
            "from base64 import urlsafe_b64decode, urlsafe_b64encode\r",
            "from collections import OrderedDict\r",
            "from typing import (\r",
            "    Any, Dict, List, Tuple, Optional, Union,\r",
            "    Callable, TypeVar, Generic, NoReturn, Type, Set, TypedDict,\r",
            ")\r",
            "from dataclasses import dataclass\r",
            "import msgpack\r",
            "\r",
            "# Import BLAKE3 for cryptographic hashing (faster and more secure than SHA3-256)\r",
            "import importlib.util\r",
            "\r",
            "HAS_BLAKE3 = importlib.util.find_spec(\"blake3\") is not None\r",
            "if HAS_BLAKE3:\r",
            "    import blake3\r",
            "else:\r",
            "    warnings.warn(\r",
            "        \"BLAKE3 library not found. Falling back to SHA3-256. \"\r",
            "        \"Install BLAKE3 with: pip install blake3\",\r",
            "        ImportWarning,\r",
            "    )\r",
            "\r",
            "# Import gmpy2 - now a strict requirement\r",
            "try:\r",
            "    import gmpy2\r",
            "except ImportError as exc:\r",
            "    raise ImportError(\r",
            "        \"gmpy2 library is required for this module. \"\r",
            "        \"Install gmpy2 with: pip install gmpy2\"\r",
            "    ) from exc\r",
            "\r",
            "logging.basicConfig(\r",
            "    level=logging.WARNING,\r",
            "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r",
            "    handlers=[logging.FileHandler(\"feldman_vss.log\"), logging.StreamHandler()],\r",
            ")\r",
            "logger = logging.getLogger(\"feldman_vss\")\r",
            "\r",
            "# Security parameters\r",
            "VSS_VERSION = \"VSS-0.8.0b2\"\r",
            "# Minimum size for secure prime fields for post-quantum security\r",
            "MIN_PRIME_BITS = 4096\r",
            "\r",
            "# Safe primes cache - these are primes p where (p-1)/2 is also prime\r",
            "# Using larger primes for post-quantum security\r",
            "SAFE_PRIMES = {\r",
            "    # Mimimal safe prime for 5 years is 3072. The recommended is 4096. These primes are from RFC 3526. More Modular Exponential (MODP) Diffie-Hellman groups for Internet Key Exchange (IKE).\r",
            "    3072: int(\r",
            "        \"FFFFFFFFFFFFFFFFC90FDAA22168C234C4C6628B80DC1CD129024E088A67CC74020BBEA63B139B22514A08798E3404DDEF9519B3CD3A431B302B0A6DF25F14374FE1356D6D51C245E485B576625E7EC6F44C42E9A637ED6B0BFF5CB6F406B7EDEE386BFB5A899FA5AE9F24117C4B1FE649286651ECE45B3DC2007CB8A163BF0598DA48361C55D39A69163FA8FD24CF5F83655D23DCA3AD961C62F356208552BB9ED529077096966D670C354E4ABC9804F1746C08CA18217C32905E462E36CE3BE39E772C180E86039B2783A2EC07A28FB5C55DF06F4C52C9DE2BCBF6955817183995497CEA956AE515D2261898FA051015728E5A8AAAC42DAD33170D04507A33A85521ABDF1CBA64ECFB850458DBEF0A8AEA71575D060C7DB3970F85A6E1E4C7ABF5AE8CDB0933D71E8C94E04A25619DCEE3D2261AD2EE6BF12FFA06D98A0864D87602733EC86A64521F2B18177B200CBBE117577A615D6C770988C0BAD946E208E24FA074E5AB3143DB5BFCE0FD108E4B82D120A93AD2CAFFFFFFFFFFFFFFFF\",\r",
            "        16,\r",
            "    ),\r",
            "    4096: int(\r",
            "        \"FFFFFFFFFFFFFFFFC90FDAA22168C234C4C6628B80DC1CD129024E088A67CC74020BBEA63B139B22514A08798E3404DDEF9519B3CD3A431B302B0A6DF25F14374FE1356D6D51C245E485B576625E7EC6F44C42E9A637ED6B0BFF5CB6F406B7EDEE386BFB5A899FA5AE9F24117C4B1FE649286651ECE45B3DC2007CB8A163BF0598DA48361C55D39A69163FA8FD24CF5F83655D23DCA3AD961C62F356208552BB9ED529077096966D670C354E4ABC9804F1746C08CA18217C32905E462E36CE3BE39E772C180E86039B2783A2EC07A28FB5C55DF06F4C52C9DE2BCBF6955817183995497CEA956AE515D2261898FA051015728E5A8AAAC42DAD33170D04507A33A85521ABDF1CBA64ECFB850458DBEF0A8AEA71575D060C7DB3970F85A6E1E4C7ABF5AE8CDB0933D71E8C94E04A25619DCEE3D2261AD2EE6BF12FFA06D98A0864D87602733EC86A64521F2B18177B200CBBE117577A615D6C770988C0BAD946E208E24FA074E5AB3143DB5BFCE0FD108E4B82D120A92108011A723C12A787E6D788719A10BDBA5B2699C327186AF4E23C1A946834B6150BDA2583E9CA2AD44CE8DBBBC2DB04DE8EF92E8EFC141FBECAA6287C59474E6BC05D99B2964FA090C3A2233BA186515BE7ED1F612970CEE2D7AFB81BDD762170481CD0069127D5B05AA993B4EA988D8FDDC186FFB7DC90A6C08F4DF435C934063199FFFFFFFFFFFFFFFF\",\r",
            "        16,\r",
            "    ),\r",
            "    6144: int(\r",
            "        \"FFFFFFFFFFFFFFFFC90FDAA22168C234C4C6628B80DC1CD129024E088A67CC74020BBEA63B139B22514A08798E3404DDEF9519B3CD3A431B302B0A6DF25F14374FE1356D6D51C245E485B576625E7EC6F44C42E9A637ED6B0BFF5CB6F406B7EDEE386BFB5A899FA5AE9F24117C4B1FE649286651ECE45B3DC2007CB8A163BF0598DA48361C55D39A69163FA8FD24CF5F83655D23DCA3AD961C62F356208552BB9ED529077096966D670C354E4ABC9804F1746C08CA18217C32905E462E36CE3BE39E772C180E86039B2783A2EC07A28FB5C55DF06F4C52C9DE2BCBF6955817183995497CEA956AE515D2261898FA051015728E5A8AAAC42DAD33170D04507A33A85521ABDF1CBA64ECFB850458DBEF0A8AEA71575D060C7DB3970F85A6E1E4C7ABF5AE8CDB0933D71E8C94E04A25619DCEE3D2261AD2EE6BF12FFA06D98A0864D87602733EC86A64521F2B18177B200CBBE117577A615D6C770988C0BAD946E208E24FA074E5AB3143DB5BFCE0FD108E4B82D120A92108011A723C12A787E6D788719A10BDBA5B2699C327186AF4E23C1A946834B6150BDA2583E9CA2AD44CE8DBBBC2DB04DE8EF92E8EFC141FBECAA6287C59474E6BC05D99B2964FA090C3A2233BA186515BE7ED1F612970CEE2D7AFB81BDD762170481CD0069127D5B05AA993B4EA988D8FDDC186FFB7DC90A6C08F4DF435C93402849236C3FAB4D27C7026C1D4DCB2602646DEC9751E763DBA37BDF8FF9406AD9E530EE5DB382F413001AEB06A53ED9027D831179727B0865A8918DA3EDBEBCF9B14ED44CE6CBACED4BB1BDB7F1447E6CC254B332051512BD7AF426FB8F401378CD2BF5983CA01C64B92ECF032EA15D1721D03F482D7CE6E74FEF6D55E702F46980C82B5A84031900B1C9E59E7C97FBEC7E8F323A97A7E36CC88BE0F1D45B7FF585AC54BD407B22B4154AACCC8F6D7EBF48E1D814CC5ED20F8037E0A79715EEF29BE32806A1D58BB7C5DA76F550AA3D8A1FBFF0EB19CCB1A313D55CDA56C9EC2EF29632387FE8D76E3C0468043E8F663F4860EE12BF2D5B0B7474D6E694F91E6DCC4024FFFFFFFFFFFFFFFF\",\r",
            "        16,\r",
            "    ),\r",
            "    8192: int(\r",
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r",
            "        16,\r",
            "    ),\r",
            "}\r",
            "\r",
            "\r",
            "# Type definitions\r",
            "ByzantineEvidenceDict = TypedDict('ByzantineEvidenceDict', {\r",
            "    'type': str,\r",
            "    'evidence': List[Dict[str, Any]],\r",
            "    'timestamp': int,\r",
            "    'signature': str\r",
            "})\r",
            "FieldElement = Union[int, \"gmpy2.mpz\"]  # Integer field elements\r",
            "SharePoint = Tuple[FieldElement, FieldElement]  # (x, y) coordinate\r",
            "ShareDict = Dict[int, SharePoint]  # Maps participant ID to share\r",
            "Randomizer = FieldElement  # Randomizer values for commitments\r",
            "InvalidityProofDict = TypedDict('InvalidityProofDict', {\r",
            "    'party_id': int,\r",
            "    'participant_id': int,\r",
            "    'share_x': FieldElement,\r",
            "    'share_y': FieldElement,\r",
            "    'expected_commitment': FieldElement,\r",
            "    'actual_commitment': FieldElement,\r",
            "    'combined_randomizer': FieldElement,\r",
            "    'timestamp': int,\r",
            "    'signature': str\r",
            "})\r",
            "VerificationDataDict = TypedDict('VerificationDataDict', {\r",
            "    'original_shares_count': int,\r",
            "    'threshold': int,\r",
            "    'zero_commitment_count': int,\r",
            "    'timestamp': int,\r",
            "    'protocol': str,\r",
            "    'verification_method': str,\r",
            "    'hash_based': bool,\r",
            "    'verification_summary': Dict[str, Any],\r",
            "    'seed_fingerprint': str,\r",
            "    'verification_proofs': Dict[int, Dict[int, Any]]\r",
            "})\r",
            "HashCommitment = Tuple[FieldElement, Randomizer, Optional[bytes]]  # (hash, randomizer, entropy)\r",
            "CommitmentList = List[HashCommitment]  # List of commitments\r",
            "ProofDict = TypedDict('ProofDict', {\r",
            "    'blinding_commitments': List[Tuple[FieldElement, FieldElement]],\r",
            "    'challenge': FieldElement,\r",
            "    'responses': List[FieldElement],\r",
            "    'commitment_randomizers': List[FieldElement],\r",
            "    'blinding_randomizers': List[FieldElement],\r",
            "    'timestamp': int\r",
            "})\r",
            "VerificationResult = Tuple[bool, Dict[int, bool]]\r",
            "RefreshingResult = Tuple[ShareDict, CommitmentList, Dict[str, Any]]\r",
            "\r",
            "# Type Aliases for Complex Types\r",
            "HashFunc = Callable[[bytes], Any]\r",
            "RedundantExecutorFunc = Callable[..., Any]\r",
            "\r",
            "# Type Aliases for Complex Types\r",
            "HashFunc = Callable[[bytes], Any]\r",
            "RedundantExecutorFunc = Callable[..., Any]\r",
            "\r",
            "# Custom warning for security issues\r",
            "class SecurityWarning(Warning):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Warning for potentially insecure configurations or operations\r",
            "    \"\"\"\r",
            "\r",
            "# Other exception classes\r",
            "class SecurityError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised for security-related issues in VSS\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"critical\", timestamp: Optional[int] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        super().__init__(message)\r",
            "class SerializationError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised for serialization or deserialization errors with enhanced \r",
            "        forensic data collection.\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"critical\", timestamp: Optional[int] = None,\r",
            "                 data_format: Optional[str] = None, checksum_info: Optional[Dict[str, Any]] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        self.data_format = data_format  # Stores format information about the serialized data\r",
            "        self.checksum_info = checksum_info  # Stores checksum validation details if applicable\r",
            "        super().__init__(message)\r",
            "        \r",
            "    def get_forensic_data(self) -> Dict[str, Any]:\r",
            "        \"\"\"Return all forensic information as a dictionary for logging or analysis\"\"\"\r",
            "        return {\r",
            "            \"message\": self.message,\r",
            "            \"detailed_info\": self.detailed_info,\r",
            "            \"severity\": self.severity,\r",
            "            \"timestamp\": self.timestamp,\r",
            "            \"data_format\": self.data_format,\r",
            "            \"checksum_info\": self.checksum_info,\r",
            "            \"error_type\": \"SerializationError\"\r",
            "        }\r",
            "\r",
            "class VerificationError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised when share verification fails with \r",
            "        comprehensive evidence collection.\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"critical\", timestamp: Optional[int] = None,\r",
            "                 share_info: Optional[Dict[str, Any]] = None, \r",
            "                 commitment_info: Optional[Dict[str, Any]] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        self.share_info = share_info  # Information about the share that failed verification\r",
            "        self.commitment_info = commitment_info  # Information about the commitments used\r",
            "        super().__init__(message)\r",
            "        \r",
            "    def get_forensic_data(self) -> Dict[str, Any]:\r",
            "        \"\"\"Return all forensic information as a dictionary for logging or analysis\"\"\"\r",
            "        return {\r",
            "            \"message\": self.message,\r",
            "            \"detailed_info\": self.detailed_info,\r",
            "            \"severity\": self.severity,\r",
            "            \"timestamp\": self.timestamp,\r",
            "            \"share_info\": self.share_info,\r",
            "            \"commitment_info\": self.commitment_info,\r",
            "            \"error_type\": \"VerificationError\"\r",
            "        }\r",
            "\r",
            "\r",
            "class ParameterError(Exception):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Exception raised for invalid parameters in VSS with enhanced\r",
            "        parameter validation data.\r",
            "    \"\"\"\r",
            "    def __init__(self, message: str, detailed_info: Optional[str] = None, \r",
            "                 severity: str = \"error\", timestamp: Optional[int] = None,\r",
            "                 parameter_name: Optional[str] = None, \r",
            "                 parameter_value: Optional[Any] = None,\r",
            "                 expected_type: Optional[str] = None):\r",
            "        self.message = message\r",
            "        self.detailed_info = detailed_info\r",
            "        self.severity = severity\r",
            "        self.timestamp = timestamp or int(time.time())\r",
            "        self.parameter_name = parameter_name  # Name of the invalid parameter\r",
            "        self.parameter_value = parameter_value  # Value of the invalid parameter\r",
            "        self.expected_type = expected_type  # Expected type or value range\r",
            "        super().__init__(message)\r",
            "        \r",
            "    def get_forensic_data(self) -> Dict[str, Any]:\r",
            "        \"\"\"Return all forensic information as a dictionary for logging or analysis\"\"\"\r",
            "        return {\r",
            "            \"message\": self.message,\r",
            "            \"detailed_info\": self.detailed_info,\r",
            "            \"severity\": self.severity,\r",
            "            \"timestamp\": self.timestamp,\r",
            "            \"parameter_name\": self.parameter_name,\r",
            "            \"parameter_value\": str(self.parameter_value),  # Convert to string to ensure serialization\r",
            "            \"expected_type\": self.expected_type,\r",
            "            \"error_type\": \"ParameterError\"\r",
            "        }\r",
            "\r",
            "@dataclass\r",
            "class VSSConfig:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Configuration parameters for Post-Quantum Secure Feldman VSS\r",
            "\r",
            "    Arguments:\r",
            "        prime_bits (int): Number of bits for the prime modulus. Default is 4096 for post-quantum security.\r",
            "        safe_prime (bool): Whether to use a safe prime (where (p-1)/2 is also prime). Default is True.\r",
            "        secure_serialization (bool): Whether to use a secure serialization format. Default is True.\r",
            "        use_blake3 (bool): Whether to use BLAKE3 for hashing (falls back to SHA3-256 if unavailable). Default is True.\r",
            "        cache_size (int): The size of the LRU cache for exponentiation. Default is 128.\r",
            "        sanitize_errors (bool): Whether to sanitize error messages. Default is True.\r",
            "\r",
            "    Inputs:\r",
            "        None\r",
            "\r",
            "    Outputs:\r",
            "        None\r",
            "    \"\"\"\r",
            "\r",
            "    prime_bits: int = 4096  # Post-quantum security default\r",
            "    safe_prime: bool = True  # Always use safe primes for better security\r",
            "    secure_serialization: bool = True\r",
            "    use_blake3: bool = (\r",
            "        True  # Whether to use BLAKE3 (falls back to SHA3-256 if unavailable)\r",
            "    )\r",
            "    cache_size: int = 128  # Default cache size for exponentiation results\r",
            "    sanitize_errors: bool = True  # Set to False in debug env for detailed errors\r",
            "\r",
            "    def __post_init__(self) -> None:\r",
            "        # Security check - enforce minimum prime size for post-quantum security\r",
            "        if self.prime_bits < MIN_PRIME_BITS:\r",
            "            warnings.warn(\r",
            "                f\"Using prime size less than {MIN_PRIME_BITS} bits is insecure against quantum attacks. \"\r",
            "                f\"Increasing to {MIN_PRIME_BITS} bits for post-quantum security.\",\r",
            "                SecurityWarning,\r",
            "            )\r",
            "            self.prime_bits = MIN_PRIME_BITS\r",
            "\r",
            "        if self.use_blake3 and not HAS_BLAKE3:\r",
            "            warnings.warn(\r",
            "                \"BLAKE3 requested but not installed. Falling back to SHA3-256. \"\r",
            "                \"Install BLAKE3 with: pip install blake3\",\r",
            "                RuntimeWarning,\r",
            "            )\r",
            "\r",
            "\r",
            "# Define type variables for our SafeLRUCache\r",
            "K = TypeVar('K')  # Key type\r",
            "V = TypeVar('V')  # Value type\r",
            "\r",
            "class SafeLRUCache(Generic[K, V]):\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Thread-safe LRU cache implementation for efficient caching with memory constraints.\r",
            "\r",
            "    Arguments:\r",
            "        capacity (int): Maximum number of items to store in the cache.\r",
            "    \r",
            "    Type Parameters:\r",
            "        K: Type of the keys in the cache\r",
            "        V: Type of the values in the cache\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(self, capacity: int) -> None:\r",
            "        self.capacity: int = capacity\r",
            "        self.cache: OrderedDict[K, V] = OrderedDict()\r",
            "        self.lock: threading.RLock = threading.RLock()  # Use RLock for compatibility with existing code\r",
            "\r",
            "    def get(self, key: K) -> Optional[V]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Get an item from the cache, moving it to most recently used position.\r",
            "\r",
            "        Arguments:\r",
            "            key (K): The key to retrieve.\r",
            "\r",
            "        Returns:\r",
            "            Optional[V]: The value associated with the key, or None if not found.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            if key in self.cache:\r",
            "                # Move to the end (most recently used)\r",
            "                value: V = self.cache.pop(key)\r",
            "                self.cache[key] = value\r",
            "                return value\r",
            "            return None\r",
            "\r",
            "    def put(self, key: K, value: V) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Add an item to the cache, evicting least recently used item if necessary.\r",
            "\r",
            "        Arguments:\r",
            "            key (K): The key to store.\r",
            "            value (V): The value to associate with the key.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            if key in self.cache:\r",
            "                # Remove existing item first\r",
            "                self.cache.pop(key)\r",
            "            elif len(self.cache) >= self.capacity:\r",
            "                # Remove the first item (least recently used)\r",
            "                self.cache.popitem(last=False)\r",
            "            # Add new item\r",
            "            self.cache[key] = value\r",
            "\r",
            "    def clear(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Clear the cache.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            self.cache.clear()\r",
            "\r",
            "    def __len__(self) -> int:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Return number of items in the cache.\r",
            "\r",
            "        Outputs:\r",
            "            int: The number of items in the cache.\r",
            "        \"\"\"\r",
            "        with self.lock:\r",
            "            return len(self.cache)\r",
            "\r",
            "\r",
            "# --- HELPER FUNCTIONS ---\r",
            "\r",
            "HashFunc = Callable[[bytes], Any]\r",
            "RedundantExecutorFunc = Callable[..., Any]\r",
            "\r",
            "def constant_time_compare(a: Union[int, str, bytes], b: Union[int, str, bytes]) -> bool:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Compare two values in constant time to prevent timing attacks.\r",
            "\r",
            "        This implementation handles integers, strings, and bytes with consistent\r",
            "        processing time regardless of where differences occur.\r",
            "\r",
            "    Arguments:\r",
            "        a (int, str, or bytes): First value to compare.\r",
            "        b (int, str, or bytes): Second value to compare.\r",
            "\r",
            "    Inputs:\r",
            "        a: First value to compare (int, str, or bytes)\r",
            "        b: Second value to compare (int, str, or bytes)\r",
            "\r",
            "    Outputs:\r",
            "        bool: True if values are equal, False otherwise.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if isinstance(a, gmpy2.mpz):\r",
            "        a = int(a)\r",
            "    if isinstance(b, gmpy2.mpz):\r",
            "        b = int(b)\r",
            "        \r",
            "    # Convert to bytes for consistent handling\r",
            "    if isinstance(a, int) and isinstance(b, int):\r",
            "        # For integers, ensure same bit length with padding\r",
            "        # Handle the case where a or b might be 0 (which doesn't have bit_length directly applicable)\r",
            "        a_bits = a.bit_length() if a != 0 and hasattr(a, 'bit_length') else 0\r",
            "        b_bits = b.bit_length() if b != 0 and hasattr(b, 'bit_length') else 0\r",
            "        bit_length: int = max(a_bits, b_bits, 8)  # Minimum 8 bits\r",
            "        byte_length: int = (bit_length + 7) // 8\r",
            "        a_bytes: bytes = a.to_bytes(byte_length, byteorder=\"big\")\r",
            "        b_bytes: bytes = b.to_bytes(byte_length, byteorder=\"big\")\r",
            "    elif isinstance(a, str) and isinstance(b, str):\r",
            "        a_bytes = a.encode(\"utf-8\")\r",
            "        b_bytes = b.encode(\"utf-8\")\r",
            "    elif isinstance(a, bytes) and isinstance(b, bytes):\r",
            "        a_bytes = a\r",
            "        b_bytes = b\r",
            "    else:\r",
            "        # For mixed types, use a consistent conversion approach\r",
            "        a_bytes = str(a).encode(\"utf-8\")\r",
            "        b_bytes = str(b).encode(\"utf-8\")\r",
            "\r",
            "    # Handle different lengths with a padded comparison\r",
            "    # to maintain constant time behavior\r",
            "    max_len: int = max(len(a_bytes), len(b_bytes))\r",
            "    a_bytes = a_bytes.ljust(max_len, b\"\\0\")\r",
            "    b_bytes = b_bytes.ljust(max_len, b\"\\0\")\r",
            "\r",
            "    # Constant-time comparison with the full length\r",
            "    result: int = 0\r",
            "    for x, y in zip(a_bytes, b_bytes):\r",
            "        result |= x ^ y\r",
            "\r",
            "    # Final result is 0 only if all bytes matched\r",
            "    return result == 0\r",
            "\r",
            "\r",
            "def estimate_mpz_size(n: Union[int, \"gmpy2.mpz\"]) -> int:\r",
            "    \"\"\"\r",
            "    Estimate memory required for a gmpy2.mpz number of given bit length.\r",
            "\r",
            "    Arguments:\r",
            "        n (int or gmpy2.mpz): Number to estimate size for, or its bit length\r",
            "\r",
            "    Returns:\r",
            "        int: Estimated memory size in bytes\r",
            "    \"\"\"\r",
            "    if isinstance(n, (int, gmpy2.mpz)):\r",
            "        bit_length: int = (\r",
            "            n.bit_length() if hasattr(n, \"bit_length\") and n != 0 else \r",
            "            gmpy2.mpz(n).bit_length() if n != 0 else 0\r",
            "        )\r",
            "    else:\r",
            "        bit_length = n  # Assume n is already a bit length\r",
            "\r",
            "    # GMP internally uses limbs (detect size if possible, default to 8 bytes on 64-bit systems)\r",
            "    limb_size: int = 8  # bytes\r",
            "    try:\r",
            "        # Try to detect actual limb size from system architecture\r",
            "        import platform\r",
            "\r",
            "        if platform.architecture()[0] == \"32bit\":\r",
            "            limb_size = 4\r",
            "    except ImportError:\r",
            "        pass\r",
            "\r",
            "    num_limbs: int = (bit_length + 63) // 64\r",
            "\r",
            "    # GMP object overhead (improved estimate with scaling factor)\r",
            "    base_overhead: int = 32  # base overhead\r",
            "    scaling_factor: float = 1 + (num_limbs // 1000) * 0.1  # Add 10% for every 1000 limbs\r",
            "    overhead: int = int(base_overhead * scaling_factor)\r",
            "\r",
            "    return (num_limbs * limb_size) + overhead\r",
            "\r",
            "\r",
            "def estimate_mpz_operation_memory(op_type: str, a_bits: int, b_bits: Optional[int] = None) -> int:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Estimate memory requirements for gmpy2 mpz operations.\r",
            "\r",
            "    Arguments:\r",
            "        op_type (str): Operation type ('add', 'mul', 'pow', etc.)\r",
            "        a_bits (int): Bit length of first operand.\r",
            "        b_bits (int, optional): Bit length of second operand.\r",
            "\r",
            "    Inputs:\r",
            "        op_type (str): Operation to estimate.\r",
            "        a_bits (int): Size of first number.\r",
            "        b_bits (int, optional): Size of second number.\r",
            "\r",
            "    Outputs:\r",
            "        int: Estimated memory requirement in bytes.\r",
            "\r",
            "    Raises:\r",
            "        ValueError: If operation type is unknown or inputs are invalid.\r",
            "    \"\"\"\r",
            "    if not isinstance(a_bits, int) or a_bits <= 0:\r",
            "        raise ValueError(\"a_bits must be a positive integer\")\r",
            "\r",
            "    if op_type in (\"add\", \"sub\"):\r",
            "        # For addition/subtraction, result is at most 1 bit larger\r",
            "        result_bits: int = max(a_bits, b_bits or 0) + 1\r",
            "    elif op_type == \"mul\":\r",
            "        if not isinstance(b_bits, int) or b_bits <= 0:\r",
            "            raise ValueError(\"b_bits must be a positive integer for multiplication\")\r",
            "        # For multiplication, result is sum of bit lengths\r",
            "        result_bits = a_bits + b_bits\r",
            "    elif op_type == \"pow\":\r",
            "        if not isinstance(b_bits, int) or b_bits <= 0:\r",
            "            raise ValueError(\"b_bits must be a positive integer for exponentiation\")\r",
            "        # For exponentiation a^b, result is approximately a_bits * b\r",
            "        if b_bits > 64:  # If exponent is very large\r",
            "            raise ValueError(\"Exponent too large for safe memory estimation\")\r",
            "        # Convert b_bits to approximate value of b\r",
            "        b_approx: int = min(2**b_bits - 1, 2**32)  # Cap to avoid overflow\r",
            "        result_bits = a_bits * b_approx\r",
            "    elif op_type == \"mod\":\r",
            "        # For modulo, result is at most the size of the modulus\r",
            "        result_bits = b_bits if b_bits else a_bits\r",
            "    else:\r",
            "        raise ValueError(f\"Unknown operation type: {op_type}\")\r",
            "\r",
            "    # Convert bits to bytes with ceiling division and add overhead factor\r",
            "    overhead_factor: float = 1.5  # Allow 50% extra for gmpy2 internal overhead\r",
            "    result_bytes: float = ((result_bits + 7) // 8) * overhead_factor\r",
            "\r",
            "    return int(result_bytes)\r",
            "\r",
            "\r",
            "def estimate_exp_result_size(base_bits: int, exponent: Union[int, \"gmpy2.mpz\"]) -> int:\r",
            "    \"\"\"\r",
            "    Estimate the bit length of base^exponent.\r",
            "\r",
            "    Arguments:\r",
            "        base_bits (int): Bit length of base\r",
            "        exponent (int): Exponent value\r",
            "\r",
            "    Returns:\r",
            "        int: Estimated bit length of result\r",
            "    \"\"\"\r",
            "    # For modular exponentiation, result won't exceed modulus size\r",
            "    if isinstance(exponent, (int, gmpy2.mpz)) and exponent <= 2**30:\r",
            "        # For reasonable exponents, we can estimate more precisely\r",
            "        return base_bits * min(exponent, 2**30)\r",
            "    else:\r",
            "        # For very large exponents, return a reasonable maximum\r",
            "        return base_bits * 2**30  # This would likely exceed memory anyway\r",
            "\r",
            "\r",
            "def get_system_memory() -> int:\r",
            "    \"\"\"\r",
            "    Get available system memory in bytes.\r",
            "\r",
            "    Returns:\r",
            "        int: Available memory in bytes, or a conservative estimate if detection fails\r",
            "    \"\"\"\r",
            "    try:\r",
            "        import psutil\r",
            "\r",
            "        return int(psutil.virtual_memory().available)\r",
            "    except ImportError:\r",
            "        # If psutil not available, use a conservative default\r",
            "        return 1 * 1024 * 1024 * 1024  # 1GB conservative estimate\r",
            "\r",
            "\r",
            "def check_memory_safety(operation: str, *args: Any, max_size_mb: int = 1024, reject_unknown: bool = False) -> bool:\r",
            "    \"\"\"\r",
            "    Check if operation can be performed safely without exceeding memory limits.\r",
            "\r",
            "    Arguments:\r",
            "        operation (str): Operation type ('exp', 'mul', etc.)\r",
            "        *args: Arguments to the operation\r",
            "        max_size_mb (int): Maximum allowed memory in MB\r",
            "        reject_unknown (bool): If True, rejects all unknown operations\r",
            "\r",
            "    Returns:\r",
            "        bool: True if operation is likely safe, False otherwise\r",
            "    \"\"\"\r",
            "    max_bytes: int = max_size_mb * 1024 * 1024\r",
            "\r",
            "    try:\r",
            "        if operation == \"exp\":\r",
            "            base: Any\r",
            "            exponent: Any\r",
            "            base, exponent = args[:2]  # Get first two arguments\r",
            "            # Get bit length of base\r",
            "            base_bits: int = (\r",
            "                base.bit_length()\r",
            "                if hasattr(base, \"bit_length\")\r",
            "                else gmpy2.mpz(base).bit_length()\r",
            "            )\r",
            "\r",
            "            # Modular exponentiation won't exceed modulus size\r",
            "            if len(args) >= 3 and args[2] is not None:  # If modulus provided\r",
            "                modulus: Any = args[2]\r",
            "                mod_bits: int = (\r",
            "                    modulus.bit_length()\r",
            "                    if hasattr(modulus, \"bit_length\")\r",
            "                    else gmpy2.mpz(modulus).bit_length()\r",
            "                )\r",
            "                result_bits: int = mod_bits\r",
            "            else:\r",
            "                # Estimate memory for non-modular exponentiation\r",
            "                # Handle both int and gmpy2.mpz exponents safely without conversion\r",
            "                if isinstance(exponent, (int, gmpy2.mpz)) and not isinstance(\r",
            "                    exponent, bool\r",
            "                ):\r",
            "                    # For very large exponents, use the exponent's bit length to estimate\r",
            "                    exp_bit_length: int = (\r",
            "                        exponent.bit_length()\r",
            "                        if hasattr(exponent, \"bit_length\")\r",
            "                        else gmpy2.mpz(exponent).bit_length()\r",
            "                    )\r",
            "\r",
            "                    # If exponent is small enough, use direct multiplication\r",
            "                    if exp_bit_length < 20:  # Exponents up to ~1 million\r",
            "                        result_bits = base_bits * min(int(exponent), 1_000_000)\r",
            "                    else:\r",
            "                        # For larger exponents, use a logarithmic estimation\r",
            "                        # log2(base^exp) = exp * log2(base)\r",
            "                        result_bits = min(\r",
            "                            exp_bit_length * base_bits, base_bits * 1_000_000\r",
            "                        )\r",
            "                else:\r",
            "                    # Default for non-numeric exponents\r",
            "                    result_bits = base_bits * 1000  # Very conservative\r",
            "\r",
            "            estimated_bytes: int = estimate_mpz_size(result_bits)\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        # Other operations remain unchanged\r",
            "        elif operation == \"mul\":\r",
            "            a: Any\r",
            "            b: Any\r",
            "            a, b = args\r",
            "            a_bits: int = (\r",
            "                a.bit_length() if hasattr(a, \"bit_length\") and a != 0 \r",
            "                else gmpy2.mpz(a).bit_length() if a != 0 \r",
            "                else 0\r",
            "            )\r",
            "            b_bits: int = (\r",
            "                b.bit_length() if hasattr(b, \"bit_length\") and b != 0\r",
            "                else gmpy2.mpz(b).bit_length() if b != 0\r",
            "                else 0\r",
            "            )\r",
            "            result_bits = a_bits + b_bits  # Multiplication roughly adds bit lengths\r",
            "            estimated_bytes = estimate_mpz_size(result_bits)\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        # Add polynomial operation specifics\r",
            "        elif operation == \"polynomial\":\r",
            "            degree: int\r",
            "            max_coeff_bits: int\r",
            "            degree, max_coeff_bits = args[:2]\r",
            "            # Estimate size based on degree and coefficient size\r",
            "            estimated_bytes: int = degree * estimate_mpz_size(max_coeff_bits)\r",
            "            # Add overhead for intermediate calculations\r",
            "            estimated_bytes *= 3  # Conservative factor\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        elif operation == \"matrix\":\r",
            "            # For matrix operations\r",
            "            n: int\r",
            "            bit_length: int\r",
            "            n, bit_length = args[0], args[1]\r",
            "            estimated_bytes = (n * n * bit_length) // 8\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        elif operation == \"polynomial_eval\":\r",
            "            # For polynomial evaluation\r",
            "            degree: int\r",
            "            coeff_bits: int\r",
            "            degree, coeff_bits = args[0], args[1]\r",
            "            estimated_bytes = degree * estimate_mpz_size(coeff_bits)\r",
            "            return estimated_bytes <= max_bytes\r",
            "\r",
            "        else:\r",
            "            # Reject unknown operations if policy dictates\r",
            "            if reject_unknown:\r",
            "                logger.warning(f\"Rejecting unknown operation '{operation}' due to safety policy\")\r",
            "                return False\r",
            "                \r",
            "            # Generic fallback for unknown operations with enhanced safety margins\r",
            "            logger.warning(\r",
            "                f\"Unknown operation '{operation}' in memory safety check. \"\r",
            "                f\"Using conservative estimation, but consider adding specific handling.\"\r",
            "            )\r",
            "            \r",
            "            # Estimate based on argument sizes with increased conservatism\r",
            "            total_bits: int = 0\r",
            "            unknown_arg_count: int = 0\r",
            "            collection_size: int = 0\r",
            "            max_bit_length: int = 0\r",
            "            \r",
            "            for arg in args:\r",
            "                if hasattr(arg, \"bit_length\"):\r",
            "                    # For integers and objects with bit_length method\r",
            "                    bit_len: int = arg.bit_length()\r",
            "                    total_bits += bit_len\r",
            "                    max_bit_length = max(max_bit_length, bit_len)\r",
            "                elif isinstance(arg, (int, float)):\r",
            "                    # For numeric types without bit_length\r",
            "                    total_bits += 64  # Conservative estimate\r",
            "                elif isinstance(arg, (list, tuple)):\r",
            "                    # For collections, track total size and count\r",
            "                    arg_len: int = len(arg)\r",
            "                    collection_size += arg_len\r",
            "                    total_bits += arg_len * 64  # Conservative estimate for each element\r",
            "                else:\r",
            "                    # For unknown types, add a larger conservative buffer\r",
            "                    unknown_arg_count += 1\r",
            "                    total_bits += 2048  # 2KB buffer per unknown argument\r",
            "            \r",
            "            # Apply a more aggressive scaling factor based on complexity indicators\r",
            "            scaling_factor: float = 3.0\r",
            "            \r",
            "            # Increase scaling for operations with multiple unknown args\r",
            "            if unknown_arg_count > 1:\r",
            "                scaling_factor *= (1 + (unknown_arg_count * 0.5))\r",
            "                \r",
            "            # Increase scaling for operations with large collections\r",
            "            if collection_size > 100:\r",
            "                scaling_factor *= (1 + (min(collection_size, 10000) / 1000))\r",
            "                \r",
            "            # Increase scaling based on max bit length\r",
            "            if max_bit_length > 1024:\r",
            "                scaling_factor *= (1 + (max_bit_length / 4096))\r",
            "            \r",
            "            # Calculate final estimate with the adaptive scaling factor\r",
            "            estimated_bytes: int = int(estimate_mpz_size(total_bits) * scaling_factor)\r",
            "            \r",
            "            # Set a minimum reasonable estimate (1/4 of max) for unknown operations\r",
            "            min_safe_bytes: int = max_bytes // 4\r",
            "            if estimated_bytes < min_safe_bytes:\r",
            "                logger.warning(\r",
            "                    f\"Increasing estimated memory for unknown operation '{operation}' \"\r",
            "                    f\"from {estimated_bytes} to {min_safe_bytes} bytes for safety\"\r",
            "                )\r",
            "                estimated_bytes = min_safe_bytes\r",
            "            \r",
            "            # Log detailed information about the estimation\r",
            "            logger.debug(\r",
            "                f\"Memory safety estimation for unknown operation '{operation}': \"\r",
            "                f\"{estimated_bytes} bytes (scaling factor: {scaling_factor:.2f}, \"\r",
            "                f\"{estimated_bytes/(1024*1024):.2f}MB/{max_size_mb}MB)\"\r",
            "            )\r",
            "            \r",
            "            # For completely unknown operations with many args, reject the operation\r",
            "            if unknown_arg_count > 3 and len(args) > 5:\r",
            "                logger.error(\r",
            "                    f\"Rejecting complex unknown operation '{operation}' with too many \"\r",
            "                    f\"unrecognized arguments for reliable memory safety estimation\"\r",
            "                )\r",
            "                return False\r",
            "                \r",
            "            return estimated_bytes <= max_bytes\r",
            "    except Exception as e:\r",
            "        # If estimation fails, reject the operation for safety\r",
            "        logger.error(f\"Error during memory safety check for '{operation}': {str(e)}\")\r",
            "        return False\r",
            "\r",
            "\r",
            "def compute_checksum(data: bytes) -> int:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Compute checksum of data using xxhash3_128 with cryptographic fallback.\r",
            "\r",
            "        This provides tamper-evidence for serialized data with excellent performance\r",
            "        when xxhash is available, falling back to cryptographic hashes when it's not.\r",
            "\r",
            "    Arguments:\r",
            "        data (bytes): The data for which to compute the checksum.\r",
            "\r",
            "    Inputs:\r",
            "        data: The data for which to compute the checksum.\r",
            "\r",
            "    Outputs:\r",
            "        int: The computed checksum.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not isinstance(data, bytes):\r",
            "        raise TypeError(\"data must be bytes\")\r",
            "\r",
            "    if HAS_BLAKE3:\r",
            "        # trunk-ignore(pyright/reportPossiblyUnboundVariable)\r",
            "        return int.from_bytes(blake3.blake3(data).digest()[:16], \"big\")\r",
            "    return int.from_bytes(hashlib.sha3_256(data).digest()[:16], \"big\")\r",
            "\r",
            "\r",
            "def secure_redundant_execution(\r",
            "    func: RedundantExecutorFunc,\r",
            "    *args: Any,\r",
            "    sanitize_error_func: Optional[Callable[[str, Optional[str]], str]] = None,\r",
            "    function_name: Optional[str] = None,\r",
            "    context: Optional[str] = None,\r",
            "    **kwargs: Any,\r",
            ") -> Any:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Execute a function multiple times with additional safeguards to detect fault injection.\r",
            "\r",
            "        Uses improved constant-time comparison techniques and increased redundancy. Adds\r",
            "        random execution ordering and timing variation to further harden against\r",
            "        sophisticated fault injection attacks.\r",
            "\r",
            "    Arguments:\r",
            "        func (Callable): Function to execute redundantly.\r",
            "        *args: Arguments to pass to the function.\r",
            "        sanitize_error_func (Callable, optional): Function to sanitize error messages.\r",
            "        function_name (str, optional): Name of the function for error context.\r",
            "        context (str, optional): Additional context information for error messages.\r",
            "        **kwargs: Keyword arguments to pass to the function.\r",
            "\r",
            "    Outputs:\r",
            "        Any: Result of computation if all checks pass.\r",
            "\r",
            "    Raises:\r",
            "        SecurityError: If any computation results don't match.\r",
            "        TypeError: If func is not callable.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not callable(func):\r",
            "        raise TypeError(\"func must be callable\")\r",
            "\r",
            "    # Use function name for better error reporting\r",
            "    if function_name is None and hasattr(func, \"__name__\"):\r",
            "        function_name = func.__name__\r",
            "    else:\r",
            "        function_name = function_name or \"unknown function\"\r",
            "\r",
            "    # Increase executions from 3 to 5 for better statistical reliability\r",
            "    num_executions: int = 5\r",
            "\r",
            "    # Introduce randomly-ordered execution to prevent predictable timing patterns\r",
            "    execution_order: List[int] = list(range(num_executions))\r",
            "    try:\r",
            "        # Use existing random module\r",
            "        random.shuffle(execution_order)\r",
            "    except Exception as e:\r",
            "        # Fall back to deterministic if shuffle fails\r",
            "        logger.debug(f\"Random shuffle failed, using deterministic order: {str(e)}\")\r",
            "\r",
            "    # Execute function multiple times with randomized ordering\r",
            "    results: List[Any] = []\r",
            "    failures: List[Tuple[int, str]] = []\r",
            "\r",
            "    try:\r",
            "        for idx in execution_order:\r",
            "            # Small random delay to decorrelate execution timing\r",
            "            try:\r",
            "                time.sleep(secrets.randbelow(10) / 1000)  # 0-9ms random delay\r",
            "            except Exception as e:\r",
            "                logger.debug(f\"Random delay failed, continuing without delay: {str(e)}\")\r",
            "\r",
            "            try:\r",
            "                results.append(func(*args, **kwargs))\r",
            "            except Exception as e:\r",
            "                # Track failures for better diagnostics\r",
            "                failures.append((idx, str(e)))\r",
            "                # Continue with other executions to prevent timing attacks\r",
            "                results.append(None)\r",
            "\r",
            "        # If we have failures, raise an appropriate error\r",
            "        if failures:\r",
            "            failure_details: str = \", \".join(\r",
            "                [f\"attempt {idx}: {err}\" for idx, err in failures]\r",
            "            )\r",
            "            detailed_message: str = (\r",
            "                f\"Function {function_name} failed during redundant execution: \"\r",
            "                f\"{failure_details}\"\r",
            "            )\r",
            "            message: str = \"Computation failed during security validation\"\r",
            "\r",
            "            # Log the detailed message\r",
            "            logger.error(detailed_message)\r",
            "\r",
            "            # Use sanitization function if provided\r",
            "            if sanitize_error_func is not None and callable(sanitize_error_func):\r",
            "                sanitized_message: str = sanitize_error_func(message, detailed_message)\r",
            "                raise SecurityError(sanitized_message)\r",
            "            else:\r",
            "                raise SecurityError(message)\r",
            "\r",
            "        # Handle the case where all executions succeeded but results don't match\r",
            "        if not all(result == results[0] for result in results):\r",
            "            # Improved constant-time comparison for all permutations\r",
            "            valid: bool = True\r",
            "            mismatch_details: List[str] = []\r",
            "\r",
            "            for i in range(len(results)):\r",
            "                for j in range(i + 1, len(results)):  # Only check unique pairs\r",
            "                    if isinstance(results[i], int) and isinstance(results[j], int):\r",
            "                        # For integers, use constant-time comparison\r",
            "                        result_match: bool = constant_time_compare(results[i], results[j])\r",
            "                        valid &= result_match\r",
            "                        if not result_match:\r",
            "                            mismatch_details.append(f\"Results {i} and {j} differ\")\r",
            "                    elif isinstance(results[i], bytes) and isinstance(\r",
            "                        results[j], bytes\r",
            "                    ):\r",
            "                        # For bytes, use constant-time comparison directly\r",
            "                        result_match = constant_time_compare(results[i], results[j])\r",
            "                        valid &= result_match\r",
            "                        if not result_match:\r",
            "                            mismatch_details.append(f\"Results {i} and {j} differ\")\r",
            "                    else:\r",
            "                        # For complex objects, use serialization with fallbacks\r",
            "                        try:\r",
            "                            # Use the already-imported msgpack\r",
            "                            serialized_i: bytes = msgpack.packb(results[i], use_bin_type=True)\r",
            "                            serialized_j: bytes = msgpack.packb(results[j], use_bin_type=True)\r",
            "                            result_match = constant_time_compare(\r",
            "                                serialized_i, serialized_j\r",
            "                            )\r",
            "                            valid &= result_match\r",
            "                            if not result_match:\r",
            "                                mismatch_details.append(f\"Results {i} and {j} differ\")\r",
            "                        except (TypeError, ValueError):\r",
            "                            # Fall back to string representation as last resort\r",
            "                            result_match = constant_time_compare(\r",
            "                                str(results[i]), str(results[j])\r",
            "                            )\r",
            "                            valid &= result_match\r",
            "                            if not result_match:\r",
            "                                mismatch_details.append(\r",
            "                                    f\"Results {i} and {j} differ (string comparison)\"\r",
            "                                )\r",
            "\r",
            "            # Apply final check with more detailed error for debugging\r",
            "            if not valid:\r",
            "                # For detailed logging but not user-facing\r",
            "                context_info: str = f\" in {context}\" if context else \"\"\r",
            "                detailed_message = (\r",
            "                    f\"Redundant computation mismatch detected in function: \"\r",
            "                    f\"{function_name}{context_info}. Mismatches: {mismatch_details}\"\r",
            "                )\r",
            "\r",
            "                # Generic message for user-facing errors but with better categorization\r",
            "                message = \"Computation result mismatch - potential fault injection attack detected\"\r",
            "\r",
            "                # Log the detailed message\r",
            "                logger.error(detailed_message)\r",
            "\r",
            "                # Use sanitization function if provided, otherwise use the generic message\r",
            "                if sanitize_error_func is not None and callable(sanitize_error_func):\r",
            "                    sanitized_message = sanitize_error_func(message, detailed_message)\r",
            "                    raise SecurityError(sanitized_message)\r",
            "                else:\r",
            "                    # Default behavior if no sanitization function provided\r",
            "                    raise SecurityError(message)\r",
            "\r",
            "        # Return a deterministically selected result to prevent timing side-channels\r",
            "        result_index: int = hash(str(results[0])) % len(results)\r",
            "        return results[result_index]\r",
            "\r",
            "    except Exception as e:\r",
            "        # Handle unexpected exceptions during processing\r",
            "        if isinstance(e, SecurityError):\r",
            "            raise  # Re-raise already processed security errors\r",
            "\r",
            "        detailed_message: str = f\"Unexpected error in secure redundant execution of {function_name}: {str(e)}\"\r",
            "        message: str = \"Security validation process failed\"\r",
            "        logger.error(detailed_message)\r",
            "\r",
            "        if sanitize_error_func is not None and callable(sanitize_error_func):\r",
            "            sanitized_message: str = sanitize_error_func(message, detailed_message)\r",
            "            raise SecurityError(sanitized_message) from e\r",
            "        else:\r",
            "            raise SecurityError(message) from e\r",
            "\r",
            "\r",
            "class MemoryMonitor:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Track estimated memory usage across operations to prevent gmpy2 memory allocation failures.\r",
            "\r",
            "    Attributes:\r",
            "        max_memory_mb (int): Maximum allowed memory usage in megabytes.\r",
            "        current_usage (int): Current estimated memory usage in bytes.\r",
            "        peak_usage (int): Peak memory usage recorded in bytes.\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(self, max_memory_mb: int = 1024) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Initialize memory monitor with specified memory limits.\r",
            "\r",
            "        Arguments:\r",
            "            max_memory_mb (int, optional): Maximum allowed memory in megabytes. Defaults to 1024.\r",
            "\r",
            "        Inputs:\r",
            "            max_memory_mb (int): Memory limit in megabytes.\r",
            "\r",
            "        Raises:\r",
            "            ValueError: If max_memory_mb is not positive.\r",
            "        \"\"\"\r",
            "        if not isinstance(max_memory_mb, (int, float)) or max_memory_mb <= 0:\r",
            "            raise ValueError(\"max_memory_mb must be a positive number\")\r",
            "\r",
            "        self.max_memory_mb: int = max_memory_mb\r",
            "        self.current_usage: int = 0\r",
            "        self.peak_usage: int = 0\r",
            "\r",
            "    def check_allocation(self, size_bytes: int) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if an allocation would exceed memory limits without modifying usage tracker.\r",
            "\r",
            "        Arguments:\r",
            "            size_bytes (int): Size of proposed allocation in bytes.\r",
            "\r",
            "        Inputs:\r",
            "            size_bytes (int): Memory size to check.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if allocation is safe, False if it would exceed limits.\r",
            "\r",
            "        Raises:\r",
            "            ValueError: If size_bytes is negative.\r",
            "            TypeError: If size_bytes is not an integer.\r",
            "        \"\"\"\r",
            "        if not isinstance(size_bytes, int):\r",
            "            raise TypeError(\"size_bytes must be an integer\")\r",
            "        if size_bytes < 0:\r",
            "            raise ValueError(\"size_bytes cannot be negative\")\r",
            "\r",
            "        max_bytes: int = self.max_memory_mb * 1024 * 1024\r",
            "        return self.current_usage + size_bytes <= max_bytes\r",
            "\r",
            "    def allocate(self, size_bytes: int) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Track a memory allocation, raising exception if it would exceed limits.\r",
            "\r",
            "        Arguments:\r",
            "            size_bytes (int): Size of allocation in bytes.\r",
            "\r",
            "        Inputs:\r",
            "            size_bytes (int): Memory size to allocate.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if allocation succeeded.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If allocation would exceed memory limit.\r",
            "            ValueError: If size_bytes is negative.\r",
            "            TypeError: If size_bytes is not an integer.\r",
            "        \"\"\"\r",
            "        if not isinstance(size_bytes, int):\r",
            "            raise TypeError(\"size_bytes must be an integer\")\r",
            "        if size_bytes < 0:\r",
            "            raise ValueError(\"size_bytes cannot be negative\")\r",
            "\r",
            "        if not self.check_allocation(size_bytes):\r",
            "            raise MemoryError(\r",
            "                f\"Operation would exceed memory limit of {self.max_memory_mb}MB\"\r",
            "            )\r",
            "\r",
            "        self.current_usage += size_bytes\r",
            "        self.peak_usage = max(self.peak_usage, self.current_usage)\r",
            "        return True\r",
            "\r",
            "    def release(self, size_bytes: int) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Track memory release after operation is complete.\r",
            "\r",
            "        Arguments:\r",
            "            size_bytes (int): Size of memory to release in bytes.\r",
            "\r",
            "        Inputs:\r",
            "            size_bytes (int): Memory size to release.\r",
            "\r",
            "        Raises:\r",
            "            ValueError: If size_bytes is negative or exceeds current usage.\r",
            "            TypeError: If size_bytes is not an integer.\r",
            "        \"\"\"\r",
            "        if not isinstance(size_bytes, int):\r",
            "            raise TypeError(\"size_bytes must be an integer\")\r",
            "        if size_bytes < 0:\r",
            "            raise ValueError(\"size_bytes cannot be negative\")\r",
            "        if size_bytes > self.current_usage:\r",
            "            raise ValueError(\"Cannot release more memory than currently allocated\")\r",
            "\r",
            "        self.current_usage -= size_bytes\r",
            "\r",
            "    def get_usage_stats(self) -> Dict[str, Union[int, float]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Get current memory usage statistics.\r",
            "\r",
            "        Outputs:\r",
            "            dict: Dictionary containing current and peak memory usage information.\r",
            "        \"\"\"\r",
            "        return {\r",
            "            \"current_bytes\": self.current_usage,\r",
            "            \"current_mb\": self.current_usage / (1024 * 1024),\r",
            "            \"peak_bytes\": self.peak_usage,\r",
            "            \"peak_mb\": self.peak_usage / (1024 * 1024),\r",
            "            \"max_mb\": self.max_memory_mb,\r",
            "            \"usage_percent\": (self.current_usage / (self.max_memory_mb * 1024 * 1024))\r",
            "            * 100,\r",
            "            \"peak_percent\": (self.peak_usage / (self.max_memory_mb * 1024 * 1024))\r",
            "            * 100,\r",
            "        }\r",
            "\r",
            "\r",
            "class CyclicGroup:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Enhanced cyclic group implementation for cryptographic operations with optimizations,\r",
            "        strictly using gmpy2 for all arithmetic.\r",
            "\r",
            "    Arguments:\r",
            "        prime (int, optional): Prime modulus. If None, a safe prime will be selected or generated.\r",
            "        generator (int, optional): Generator of the group. If None, a generator will be found.\r",
            "        prime_bits (int): Bit size for the prime if generating one (default 3072 for PQ security).\r",
            "        use_safe_prime (bool): Whether to use a safe prime (p where (p-1)/2 is also prime).\r",
            "        cache_size (int): The size of the LRU cache for exponentiation.\r",
            "\r",
            "    Inputs:\r",
            "        None\r",
            "\r",
            "    Outputs:\r",
            "        None\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(\r",
            "        self,\r",
            "        prime: Optional[int] = None,\r",
            "        generator: Optional[int] = None,\r",
            "        prime_bits: int = 4096,\r",
            "        use_safe_prime: bool = True,\r",
            "        cache_size: int = 128,\r",
            "        _precompute_window_size: Optional[int] = None,\r",
            "    ) -> None:\r",
            "        # For post-quantum security, we recommend at least 3072-bit primes\r",
            "        if prime_bits < 3072:\r",
            "            warnings.warn(\r",
            "                \"For post-quantum security, consider using prime_bits >= 3072\",\r",
            "                SecurityWarning,\r",
            "            )\r",
            "\r",
            "        # Use provided prime or select one\r",
            "        if prime is not None:\r",
            "            self.prime: \"gmpy2.mpz\" = gmpy2.mpz(prime)\r",
            "            # Verify primality if not using a known safe prime\r",
            "            if self.prime not in SAFE_PRIMES.values() and use_safe_prime:\r",
            "                if not CyclicGroup._is_probable_prime(self.prime):\r",
            "                    raise ParameterError(\"Provided value is not a prime\")\r",
            "                if use_safe_prime and not CyclicGroup._is_safe_prime(self.prime):\r",
            "                    raise ParameterError(\"Provided prime is not a safe prime\")\r",
            "        else:\r",
            "            # Use cached safe prime if available and requested\r",
            "            if use_safe_prime and prime_bits in SAFE_PRIMES:\r",
            "                self.prime = gmpy2.mpz(SAFE_PRIMES[prime_bits])\r",
            "            else:\r",
            "                # Generate a prime of appropriate size\r",
            "                # Note: For production, generating safe primes is very slow\r",
            "                # and should be done offline or use precomputed values\r",
            "                if use_safe_prime:\r",
            "                    warnings.warn(\r",
            "                        \"Generating a safe prime is computationally expensive. \"\r",
            "                        \"Consider using precomputed safe primes for better performance.\",\r",
            "                        RuntimeWarning,\r",
            "                    )\r",
            "                    self.prime = self._generate_safe_prime(prime_bits)\r",
            "                else:\r",
            "                    self.prime = self._generate_prime(prime_bits)\r",
            "\r",
            "        # Set or find generator\r",
            "        if generator is not None:\r",
            "            self.generator: \"gmpy2.mpz\" = gmpy2.mpz(generator % self.prime)\r",
            "            if not self._is_generator(self.generator):\r",
            "                raise ParameterError(\"Provided value is not a generator of the group\")\r",
            "        else:\r",
            "            self.generator = self._find_generator()\r",
            "\r",
            "        # Cache initialization with SafeLRUCache\r",
            "        self.cached_powers: SafeLRUCache = SafeLRUCache(capacity=cache_size)\r",
            "\r",
            "        # Pre-compute fixed-base exponentiations for common operations\r",
            "        self._precompute_exponent_length: int = self.prime.bit_length()\r",
            "        self._precompute_window_size: Optional[int] = _precompute_window_size\r",
            "        self._precomputed_powers: Dict[Union[int, str], Any] = self._precompute_powers()\r",
            "\r",
            "    @staticmethod\r",
            "    def _is_probable_prime(n: Union[int, \"gmpy2.mpz\"], k: int = 40) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if n is probably prime using Miller-Rabin test.\r",
            "\r",
            "        Arguments:\r",
            "            n (int): Number to test.\r",
            "            k (int): Number of rounds (higher is more accurate).\r",
            "\r",
            "        Inputs:\r",
            "            n (int): Number to test.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if n is probably prime, False otherwise.\r",
            "        \"\"\"\r",
            "        if n <= 1:\r",
            "            return False\r",
            "        if n <= 3:\r",
            "            return True\r",
            "        if n % 2 == 0:\r",
            "            return False\r",
            "\r",
            "        # Write n as 2^r * d + 1\r",
            "        r: int\r",
            "        d: Union[int, \"gmpy2.mpz\"]\r",
            "        r, d = 0, n - 1\r",
            "        while d % 2 == 0:\r",
            "            r += 1\r",
            "            d //= 2\r",
            "\r",
            "        # Witness loop\r",
            "        a: int\r",
            "        x: \"gmpy2.mpz\"\r",
            "        for _ in range(k):\r",
            "            a = secrets.randbelow(n - 3) + 2\r",
            "            x = gmpy2.powmod(a, d, n)\r",
            "            if x == 1 or x == n - 1:\r",
            "                continue\r",
            "            for _ in range(r - 1):\r",
            "                x = gmpy2.powmod(x, 2, n)\r",
            "                if x == n - 1:\r",
            "                    break\r",
            "            else:\r",
            "                return False\r",
            "        return True\r",
            "\r",
            "    @staticmethod\r",
            "    def _is_safe_prime(p: Union[int, \"gmpy2.mpz\"]) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if p is a safe prime (p=2q+1 where q is prime).\r",
            "\r",
            "        Arguments:\r",
            "            p (int): Number to check.\r",
            "\r",
            "        Inputs:\r",
            "            p (int): Number to check.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if p is a safe prime, False otherwise.\r",
            "        \"\"\"\r",
            "        return CyclicGroup._is_probable_prime((p - 1) // 2)\r",
            "\r",
            "    def _generate_prime(self, bits: int) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a random prime of specified bits.\r",
            "\r",
            "        Arguments:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Inputs:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Outputs:\r",
            "            int: Generated prime number.\r",
            "        \"\"\"\r",
            "        p: int\r",
            "        while True:\r",
            "            # Generate random odd number of requested bit size\r",
            "            p = secrets.randbits(bits) | (1 << (bits - 1)) | 1\r",
            "            if self._is_probable_prime(p):\r",
            "                return gmpy2.mpz(p)\r",
            "\r",
            "    def _generate_safe_prime(self, bits: int) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a safe prime p where (p-1)/2 is also prime.\r",
            "\r",
            "        Arguments:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Inputs:\r",
            "            bits (int): Number of bits for the prime.\r",
            "\r",
            "        Outputs:\r",
            "            int: Generated safe prime number.\r",
            "        \"\"\"\r",
            "        # This is very slow for large bit sizes - should be done offline\r",
            "        q: \"gmpy2.mpz\"\r",
            "        p: \"gmpy2.mpz\"\r",
            "        while True:\r",
            "            # Generate candidate q\r",
            "            q = self._generate_prime(bits - 1)\r",
            "            # Compute p = 2q + 1\r",
            "            p = 2 * q + 1\r",
            "            if self._is_probable_prime(p):\r",
            "                return gmpy2.mpz(p)\r",
            "\r",
            "    def _is_generator(self, g: Union[int, \"gmpy2.mpz\"]) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Check if g is a generator of the group.\r",
            "            For a safe prime p = 2q + 1, we need to check:\r",
            "            1. g \u2260 0, 1, p-1\r",
            "            2. g^q \u2260 1 mod p\r",
            "\r",
            "        Arguments:\r",
            "            g (int): Element to check.\r",
            "\r",
            "        Inputs:\r",
            "            g (int): Element to check.\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if g is a generator, False otherwise.\r",
            "        \"\"\"\r",
            "        if g <= 1 or g >= self.prime - 1:\r",
            "            return False\r",
            "\r",
            "        # For a safe prime p=2q+1, we check if g^q != 1 mod p\r",
            "        # This confirms g generates a subgroup of order q\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        return gmpy2.powmod(g, q, self.prime) != 1\r",
            "\r",
            "    def _find_generator(self) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Find a generator for the q-order subgroup of the cyclic group.\r",
            "            For a safe prime p=2q+1, this finds an element of order q.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            int: Generator of the group.\r",
            "        \"\"\"\r",
            "        # For a safe prime p=2q+1, we want a generator of the q-order subgroup\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "\r",
            "        # Try quadratic residues: for g in Z_p*, g^2 generates the q-order subgroup\r",
            "        h: int\r",
            "        g: \"gmpy2.mpz\"\r",
            "        for _ in range(10000):  # Try multiple times with different values\r",
            "            h = secrets.randbelow(self.prime - 3) + 2  # Random value in [2, p-2]\r",
            "            g = gmpy2.powmod(h, 2, self.prime)  # Square to get quadratic residue\r",
            "\r",
            "            # Skip if g=1, which doesn't generate anything interesting\r",
            "            if g == 1:\r",
            "                continue\r",
            "\r",
            "            # Verify g^q = 1 mod p (ensuring it's in the q-order subgroup)\r",
            "            if gmpy2.powmod(g, q, self.prime) == 1 and g != 1:\r",
            "                return g\r",
            "\r",
            "        # Fallback to standard values that are often generators\r",
            "        standard_candidates: List[int] = [2, 3, 5, 7, 11, 13, 17]\r",
            "        for g in standard_candidates:\r",
            "            if g < self.prime and self._is_generator(g):\r",
            "                return gmpy2.mpz(g)\r",
            "\r",
            "        raise RuntimeError(\"Failed to find a generator for the group\")\r",
            "\r",
            "    def _precompute_powers(self) -> Dict[Union[int, str], Any]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Pre-compute powers of the generator for faster exponentiation with multi-level windows.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            dict: Precomputed powers of the generator.\r",
            "        \"\"\"\r",
            "        bits: int = self.prime.bit_length()\r",
            "\r",
            "        # Dynamic window sizing based on prime size\r",
            "        small_window: int\r",
            "        if self._precompute_window_size is not None:\r",
            "            small_window = self._precompute_window_size\r",
            "        else:\r",
            "            # Enhanced adaptive logic with better scaling\r",
            "            if bits > 8192:\r",
            "                small_window = 8  # Conservative for very large primes\r",
            "            elif bits > 6144:\r",
            "                small_window = 7\r",
            "            elif bits > 4096:\r",
            "                small_window = 6\r",
            "            elif bits > 3072:\r",
            "                small_window = 5\r",
            "            else:\r",
            "                small_window = 4  # Minimum size for good performance\r",
            "\r",
            "        # Large window remains at 8 for consistent big jumps\r",
            "        large_window: int = 8\r",
            "        large_step: int = 2**small_window\r",
            "\r",
            "        # Rest of the method remains unchanged\r",
            "        precomputed: Dict[Union[int, str], Any] = {}\r",
            "\r",
            "        # Small window exponents for fine-grained values\r",
            "        j: int\r",
            "        for j in range(2**small_window):\r",
            "            precomputed[j] = gmpy2.powmod(self.generator, j, self.prime)\r",
            "\r",
            "        # Large window exponents for bigger jumps\r",
            "        large_exponents: Dict[int, \"gmpy2.mpz\"] = {}\r",
            "        k: int\r",
            "        for k in range(1, 2 ** (large_window - small_window)):\r",
            "            large_exponents[k] = gmpy2.powmod(\r",
            "                self.generator, k * large_step, self.prime\r",
            "            )\r",
            "\r",
            "        # Add to precomputed dict\r",
            "        precomputed.update(\r",
            "            {\r",
            "                \"large_window\": large_exponents,\r",
            "                \"small_bits\": small_window,\r",
            "                \"large_step\": large_step,\r",
            "            }\r",
            "        )\r",
            "\r",
            "        return precomputed\r",
            "\r",
            "    def exp(self, base: Union[int, \"gmpy2.mpz\"], exponent: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Thread-safe exponentiation in the group: base^exponent mod prime with optimizations.\r",
            "            NOT suitable for secret exponents - use secure_exp() instead for sensitive values.\r",
            "\r",
            "        Arguments:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Inputs:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the exponentiation.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If the operation would likely exceed available memory\r",
            "        \"\"\"\r",
            "        # Use precomputation for generator base if available\r",
            "        if base == self.generator and self._precomputed_powers:\r",
            "            return self._exp_with_precomputation(exponent)\r",
            "\r",
            "        # Normalize inputs\r",
            "        base_mpz: \"gmpy2.mpz\" = gmpy2.mpz(base % self.prime)\r",
            "        \r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        exponent_mpz: \"gmpy2.mpz\" = gmpy2.mpz(exponent % q)  # More efficient than % (self.prime - 1)\r",
            "\r",
            "        # Check memory safety before proceeding\r",
            "        if not check_memory_safety(\"exp\", base_mpz, exponent_mpz, self.prime):\r",
            "            raise MemoryError(\r",
            "                \"Attempted exponentiation would exceed memory limits. \"\r",
            "                \"Consider using a smaller exponent or larger system memory.\"\r",
            "            )\r",
            "\r",
            "        # Check cache for common operations\r",
            "        cache_key: Tuple[\"gmpy2.mpz\", \"gmpy2.mpz\"] = (base_mpz, exponent_mpz)\r",
            "\r",
            "        # Thread-safe cache access using SafeLRUCache methods\r",
            "        result: Optional[\"gmpy2.mpz\"] = self.cached_powers.get(cache_key)\r",
            "        if result is not None:\r",
            "            return result\r",
            "\r",
            "        # Use efficient binary exponentiation for large numbers\r",
            "        result = gmpy2.powmod(base_mpz, exponent_mpz, self.prime)\r",
            "\r",
            "        # Cache the result using SafeLRUCache's put method (no need to check size)\r",
            "        self.cached_powers.put(cache_key, result)\r",
            "        return result\r",
            "\r",
            "    def _exp_with_precomputation(self, exponent: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Exponentiation using multi-level window technique with precomputed values.\r",
            "\r",
            "        Arguments:\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Inputs:\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the exponentiation.\r",
            "        \"\"\"\r",
            "\r",
            "        if exponent == 0:\r",
            "            return gmpy2.mpz(1)\r",
            "\r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2 \r",
            "        exponent_mpz: \"gmpy2.mpz\" = gmpy2.mpz(exponent) % q\r",
            "\r",
            "        # Extract window parameters\r",
            "        small_bits: int = self._precomputed_powers[\"small_bits\"]\r",
            "        large_step: int = self._precomputed_powers[\"large_step\"]\r",
            "        large_window: Dict[int, \"gmpy2.mpz\"] = self._precomputed_powers.get(\"large_window\", {})\r",
            "\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "        remaining: \"gmpy2.mpz\" = exponent_mpz\r",
            "\r",
            "        # Process large steps first\r",
            "        large_count: int\r",
            "        max_step: int\r",
            "        while remaining >= large_step:\r",
            "            # Extract how many large steps to take\r",
            "            large_count = remaining // large_step\r",
            "            if large_count in large_window:\r",
            "                # Use precomputed large step\r",
            "                result = (result * large_window[large_count]) % self.prime\r",
            "                remaining -= large_count * large_step\r",
            "            else:\r",
            "                # Take the largest available step\r",
            "                max_step = max(\r",
            "                    (k for k in large_window.keys() if k <= large_count), default=0\r",
            "                )\r",
            "                if max_step > 0:\r",
            "                    result = (result * large_window[max_step]) % self.prime\r",
            "                    remaining -= max_step * large_step\r",
            "                else:\r",
            "                    # Fall back to small steps\r",
            "                    break\r",
            "\r",
            "        # Process remaining small steps\r",
            "        small_val: int\r",
            "        while remaining > 0:\r",
            "            # Extract small window bits\r",
            "            small_val = min(remaining, 2**small_bits - 1)\r",
            "            if small_val in self._precomputed_powers:\r",
            "                result = (result * self._precomputed_powers[small_val]) % self.prime\r",
            "                remaining -= small_val\r",
            "            else:\r",
            "                # This case shouldn't happen with full precomputation, but just in case\r",
            "                result = (\r",
            "                    result * gmpy2.powmod(self.generator, small_val, self.prime)\r",
            "                ) % self.prime\r",
            "                remaining -= small_val\r",
            "\r",
            "        return result\r",
            "\r",
            "    def mul(self, a: Union[int, \"gmpy2.mpz\"], b: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Multiply two elements in the group: (a * b) mod prime.\r",
            "\r",
            "        Arguments:\r",
            "            a (int): First element.\r",
            "            b (int): Second element.\r",
            "\r",
            "        Inputs:\r",
            "            a (int): First element.\r",
            "            b (int): Second element.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the multiplication.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If the operation would likely exceed available memory\r",
            "        \"\"\"\r",
            "        a_mpz: \"gmpy2.mpz\" = gmpy2.mpz(a)\r",
            "        b_mpz: \"gmpy2.mpz\" = gmpy2.mpz(b)\r",
            "\r",
            "        # Check memory safety before proceeding\r",
            "        if not check_memory_safety(\"mul\", a_mpz, b_mpz):\r",
            "            raise MemoryError(\r",
            "                \"Multiplication operation would exceed memory limits. \"\r",
            "                \"The operands are too large for available system memory.\"\r",
            "            )\r",
            "\r",
            "        return (a_mpz * b_mpz) % self.prime\r",
            "\r",
            "    def secure_random_element(self) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a secure random element in the group Z_p*.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            int: A random element in the range [1, prime-1].\r",
            "        \"\"\"\r",
            "        return gmpy2.mpz(secrets.randbelow(self.prime - 1) + 1)\r",
            "\r",
            "    def clear_cache(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Thread-safe clearing of exponentiation cache to free memory.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        # Use SafeLRUCache's clear method\r",
            "        self.cached_powers.clear()\r",
            "\r",
            "    def hash_to_group(self, data: bytes) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Hash arbitrary data to an element in the group with uniform distribution.\r",
            "            Uses strict rejection sampling with no fallback to biased methods, ensuring\r",
            "            perfect uniformity across the group range [1, prime-1].\r",
            "\r",
            "        Arguments:\r",
            "            data (bytes): The data to hash.\r",
            "\r",
            "        Inputs:\r",
            "            data (bytes): The data to hash.\r",
            "\r",
            "        Outputs:\r",
            "            int: An element in the range [1, prime-1] with uniform distribution.\r",
            "\r",
            "        Raises:\r",
            "            SecurityError: If unable to generate a uniformly distributed value after\r",
            "                        exhausting all attempts (extremely unlikely).\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(data, bytes):\r",
            "            raise TypeError(\"data must be bytes\")\r",
            "\r",
            "        # Calculate required bytes based on prime size with extra bytes to minimize bias\r",
            "        prime_bits: int = self.prime.bit_length()\r",
            "        required_bytes: int = (prime_bits + 7) // 8\r",
            "        extra_security_bytes: int = 32  # Increased from 16 for better security margin\r",
            "        total_bytes: int = required_bytes + extra_security_bytes\r",
            "\r",
            "        # Increase max attempts to reduce failure probability\r",
            "        max_attempts: int = 50000  # Increased from 10000\r",
            "        original_data: bytes = data\r",
            "\r",
            "        # Make multiple attempts with domain separation\r",
            "        attempt_round: int\r",
            "        for attempt_round in range(5):  # Increased from 3 rounds\r",
            "            counter: int = 0\r",
            "            while counter < max_attempts:\r",
            "                # Generate hash blocks with proper domain separation\r",
            "                hash_blocks: bytearray = bytearray()\r",
            "                block_counter: int = 0\r",
            "\r",
            "                # Domain separation prefix with version and attempt round\r",
            "                domain_prefix: bytes = f\"HTCG_PQS_v{VSS_VERSION}_r{attempt_round}_\".encode()\r",
            "\r",
            "                h: bytes\r",
            "                while len(hash_blocks) < total_bytes:\r",
            "                    block_data: bytes = (\r",
            "                        domain_prefix\r",
            "                        + original_data\r",
            "                        + counter.to_bytes(8, \"big\")\r",
            "                        + block_counter.to_bytes(8, \"big\")\r",
            "                    )\r",
            "\r",
            "                    if HAS_BLAKE3:\r",
            "                        h = blake3.blake3(block_data).digest(\r",
            "                            min(32, total_bytes - len(hash_blocks))\r",
            "                        )\r",
            "                    else:\r",
            "                        h = hashlib.sha3_256(block_data).digest()\r",
            "                    hash_blocks.extend(h)\r",
            "                    block_counter += 1\r",
            "\r",
            "                # Convert to integer, using only the necessary bytes\r",
            "                value: int = int.from_bytes(hash_blocks[:required_bytes], \"big\")\r",
            "\r",
            "                # Pure rejection sampling - accept ONLY if in valid range\r",
            "                if 1 <= value < self.prime:\r",
            "                    return gmpy2.mpz(value)\r",
            "\r",
            "                # If not in range, try again with a different hash input\r",
            "                counter += 1\r",
            "\r",
            "        # If we've exhausted all attempts across multiple rounds,\r",
            "        # this is an exceptional condition that should be treated as a security error\r",
            "        # We do NOT fall back to biased modular reduction\r",
            "        raise SecurityError(\r",
            "            f\"Failed to generate a uniform group element after {5 * max_attempts} attempts. \"\r",
            "            f\"This could indicate an implementation issue or an extraordinarily unlikely \"\r",
            "            f\"statistical event (probability approximately 2^-{30 + extra_security_bytes*8}).\"\r",
            "        )\r",
            "\r",
            "    def _enhanced_encode_for_hash(self, *args: Any, context: str = \"FeldmanVSS\") -> bytes:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Securely encode multiple values for hashing with enhanced domain separation.\r",
            "            Uses both type tagging and length-prefixing to prevent collision attacks.\r",
            "\r",
            "        Arguments:\r",
            "            *args: Values to encode for hashing.\r",
            "            context (str): Optional context string for domain separation (default: \"FeldmanVSS\").\r",
            "\r",
            "        Outputs:\r",
            "            bytes: Bytes ready for hashing.\r",
            "        \"\"\"\r",
            "        # Initialize encoded data\r",
            "        encoded: bytes = b\"\"\r",
            "\r",
            "        # Add protocol version identifier\r",
            "        encoded += VSS_VERSION.encode(\"utf-8\")\r",
            "\r",
            "        # Add context string with type tag and length prefixing for domain separation\r",
            "        context_bytes: bytes = context.encode(\"utf-8\")\r",
            "        encoded += b\"\\x01\"  # Type tag for context string\r",
            "        encoded += len(context_bytes).to_bytes(4, \"big\")\r",
            "        encoded += context_bytes\r",
            "\r",
            "        # Calculate byte length for integer serialization once\r",
            "        prime_bit_length: int = self.prime.bit_length()  # Changed from self.group.prime\r",
            "        byte_length: int = (prime_bit_length + 7) // 8\r",
            "\r",
            "        # Add each value with type tagging and length prefixing\r",
            "        arg: Any\r",
            "        arg_bytes: bytes\r",
            "        for arg in args:\r",
            "            # Convert to bytes with type-specific handling and tagging\r",
            "            if isinstance(arg, bytes):\r",
            "                encoded += b\"\\x00\"  # Tag for bytes\r",
            "                arg_bytes = arg\r",
            "            elif isinstance(arg, str):\r",
            "                encoded += b\"\\x01\"  # Tag for string\r",
            "                arg_bytes = arg.encode(\"utf-8\")\r",
            "            elif isinstance(arg, int) or isinstance(arg, gmpy2.mpz):\r",
            "                encoded += b\"\\x02\"  # Tag for int/mpz\r",
            "                arg_bytes = int(arg).to_bytes(byte_length, \"big\")\r",
            "            else:\r",
            "                encoded += b\"\\x03\"  # Tag for other types\r",
            "                arg_bytes = str(arg).encode(\"utf-8\")\r",
            "\r",
            "            # Add 4-byte length followed by the data itself\r",
            "            encoded += len(arg_bytes).to_bytes(4, \"big\")\r",
            "            encoded += arg_bytes\r",
            "\r",
            "        return encoded\r",
            "\r",
            "    def efficient_multi_exp(self, bases: List[Union[int, \"gmpy2.mpz\"]], exponents: List[Union[int, \"gmpy2.mpz\"]]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Efficient multi-exponentiation using simultaneous method.\r",
            "            Computes \u03a0(bases[i]^exponents[i]) mod prime.\r",
            "\r",
            "        Arguments:\r",
            "            bases (list): List of base values.\r",
            "            exponents (list): List of corresponding exponent values.\r",
            "\r",
            "        Inputs:\r",
            "            bases (list): List of base values.\r",
            "            exponents (list): List of corresponding exponent values.\r",
            "\r",
            "        Outputs:\r",
            "            int: Result of the multi-exponentiation.\r",
            "        \"\"\"\r",
            "        if len(bases) != len(exponents):\r",
            "            raise ValueError(\"Number of bases must equal number of exponents\")\r",
            "\r",
            "        if len(bases) <= 1:\r",
            "            if not bases:\r",
            "                return gmpy2.mpz(1)\r",
            "            return self.exp(bases[0], exponents[0])\r",
            "\r",
            "        # Normalize inputs\r",
            "        prime: \"gmpy2.mpz\" = self.prime\r",
            "        bases_mpz: List[\"gmpy2.mpz\"] = [gmpy2.mpz(b) % prime for b in bases]\r",
            "        \r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        exponents_mpz: List[\"gmpy2.mpz\"] = [gmpy2.mpz(e) % q for e in exponents]  # More efficient\r",
            "\r",
            "        # Estimate memory requirements\r",
            "        max_base_bits: int = max(b.bit_length() for b in bases_mpz)\r",
            "        max_exp_bits: int = max(e.bit_length() for e in exponents_mpz)\r",
            "        total_ops: int = len(bases_mpz)\r",
            "\r",
            "        # Check if this operation would be safe\r",
            "        if not check_memory_safety(\r",
            "            \"exp\", max_base_bits, max_exp_bits, prime.bit_length()\r",
            "        ):\r",
            "            raise MemoryError(\r",
            "                f\"Multi-exponentiation with {total_ops} operations of size {max_base_bits} bits \"\r",
            "                f\"would exceed memory limits. Consider reducing parameters.\"\r",
            "            )\r",
            "\r",
            "        # Choose window size based on number of bases\r",
            "        n: int = len(bases_mpz)\r",
            "        window_size: int = 2 if n <= 4 else 3 if n <= 16 else 4\r",
            "        max_bits: int = max((e.bit_length() for e in exponents_mpz), default=0)\r",
            "\r",
            "        # For small exponents, reduce window size\r",
            "        if max_bits < 128:\r",
            "            window_size = max(1, window_size - 1)\r",
            "\r",
            "        # Optimize precomputation strategy based on number of bases\r",
            "        precomp: Dict[int, \"gmpy2.mpz\"]\r",
            "        if n <= 8:\r",
            "            # For small n, precompute all possible combinations\r",
            "            precomp = {}\r",
            "            i: int\r",
            "            for i in range(1, 2**n):\r",
            "                product: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "                j: int\r",
            "                for j in range(n):\r",
            "                    if (i >> j) & 1:\r",
            "                        product = (product * bases_mpz[j]) % prime\r",
            "                precomp[i] = product\r",
            "        else:\r",
            "            # For larger n, use selective precomputation\r",
            "            precomp = {1 << j: bases_mpz[j] for j in range(n)}\r",
            "\r",
            "        # Main exponentiation loop using the precomputation\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "        i: int\r",
            "        idx: int\r",
            "        for i in range(max_bits - 1, -1, -1):\r",
            "            result = (result * result) % prime\r",
            "\r",
            "            # Determine which bases to include in this step\r",
            "            idx = 0\r",
            "            j: int\r",
            "            for j in range(n):\r",
            "                if (exponents_mpz[j] >> i) & 1:\r",
            "                    idx |= 1 << j\r",
            "\r",
            "            if idx > 0:\r",
            "                if n <= 8:\r",
            "                    # Use fully precomputed value\r",
            "                    result = (result * precomp[idx]) % prime\r",
            "                else:\r",
            "                    # Selectively multiply by needed bases\r",
            "                    for j in range(n):\r",
            "                        if (idx >> j) & 1:\r",
            "                            result = (result * bases_mpz[j]) % prime\r",
            "\r",
            "        return result\r",
            "\r",
            "    def secure_exp(self, base: Union[int, \"gmpy2.mpz\"], exponent: Union[int, \"gmpy2.mpz\"]) -> \"gmpy2.mpz\":\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Constant-time exponentiation for sensitive cryptographic operations.\r",
            "            Avoids all caching and timing side-channels to prevent exponent leakage.\r",
            "\r",
            "        Arguments:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value (sensitive).\r",
            "\r",
            "        Inputs:\r",
            "            base (int): Base value.\r",
            "            exponent (int): Exponent value.\r",
            "\r",
            "        Outputs:\r",
            "            int: base^exponent mod prime.\r",
            "\r",
            "        Raises:\r",
            "            MemoryError: If the operation would likely exceed available memory\r",
            "        \"\"\"\r",
            "        # Normalize inputs in a predictable way to avoid timing variations\r",
            "        int_base: \"gmpy2.mpz\" = gmpy2.mpz(base) % self.prime\r",
            "        \r",
            "        # Optimization: For safe primes p=2q+1, reduce modulo q instead of p-1\r",
            "        q: \"gmpy2.mpz\" = (self.prime - 1) // 2\r",
            "        int_exponent: \"gmpy2.mpz\" = gmpy2.mpz(exponent) % q  # More efficient\r",
            "\r",
            "        # Check memory safety before proceeding\r",
            "        if not check_memory_safety(\"exp\", int_base, int_exponent, self.prime):\r",
            "            raise MemoryError(\r",
            "                \"Attempted exponentiation would exceed memory limits. \"\r",
            "                \"Consider using a smaller exponent or larger system memory.\"\r",
            "            )\r",
            "\r",
            "        # Use gmpy2's powmod which implements constant-time modular exponentiation\r",
            "        return gmpy2.powmod(int_base, int_exponent, self.prime)\r",
            "\r",
            "\r",
            "# --- END OF HELPER FUNCTIONS ---\r",
            "\r",
            "\r",
            "class FeldmanVSS:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Post-Quantum Secure Feldman Verifiable Secret Sharing implementation.\r",
            "\r",
            "    Arguments:\r",
            "        field: Object with a prime attribute representing the field for polynomial operations.\r",
            "        config (VSSConfig, optional): VSSConfig object with configuration parameters. Defaults to a post-quantum secure configuration.\r",
            "        group (CyclicGroup, optional): Pre-configured CyclicGroup instance. If None, a new instance will be created.\r",
            "\r",
            "    Inputs:\r",
            "        None\r",
            "\r",
            "    Outputs:\r",
            "        None\r",
            "    \"\"\"\r",
            "\r",
            "    def __init__(self, field: Any, config: Optional[VSSConfig] = None, group: Optional[CyclicGroup] = None) -> None:\r",
            "        if not hasattr(field, \"prime\") or not isinstance(field.prime, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\r",
            "                \"Field must have a 'prime' attribute that is an integer or gmpy2.mpz.\"\r",
            "            )\r",
            "\r",
            "        self.field: Any = field\r",
            "        self.config: VSSConfig = config or VSSConfig()  # Always post-quantum secure by default\r",
            "        self._byzantine_evidence: Dict[int, Dict[str, Any]] = {}\r",
            "\r",
            "        # Initialize the cyclic group for commitments\r",
            "        if group is None:\r",
            "            # Use the enhanced CyclicGroup with appropriate security parameters\r",
            "            self.group: CyclicGroup = CyclicGroup(\r",
            "                prime_bits=self.config.prime_bits,\r",
            "                use_safe_prime=self.config.safe_prime,\r",
            "                cache_size=self.config.cache_size,\r",
            "            )\r",
            "        else:\r",
            "            self.group = group\r",
            "\r",
            "        # Store generator for commitments\r",
            "        self.generator: FieldElement = self.group.generator\r",
            "\r",
            "        # Initialize hash algorithm for use in various methods\r",
            "        self.hash_algorithm: HashFunc = (\r",
            "            blake3.blake3 if HAS_BLAKE3 and self.config.use_blake3 else hashlib.sha3_256\r",
            "        )\r",
            "\r",
            "    def _sanitize_error(self, message: str, detailed_message: Optional[str] = None) -> str:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Sanitize error messages based on configuration.\r",
            "\r",
            "        Arguments:\r",
            "            message (str): The original error message.\r",
            "            detailed_message (str, optional): Detailed information to log but not expose.\r",
            "\r",
            "        Outputs:\r",
            "            str: The sanitized message for external use.\r",
            "        \"\"\"\r",
            "        if detailed_message:\r",
            "            logger.error(detailed_message)\r",
            "\r",
            "        if self.config.sanitize_errors:\r",
            "            # Generic messages for different error categories\r",
            "            message_lower: str = message.lower()\r",
            "\r",
            "            # Enhanced categories for better coverage\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"insufficient\", \"quorum\", \"threshold\", \"not enough\"]\r",
            "            ):\r",
            "                return \"Security verification failed - share refresh aborted\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\r",
            "                    \"deserialized\",\r",
            "                    \"unpacked\",\r",
            "                    \"decode\",\r",
            "                    \"format\",\r",
            "                    \"structure\",\r",
            "                ]\r",
            "            ):\r",
            "                return \"Verification of cryptographic parameters failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\r",
            "                    \"tampering\",\r",
            "                    \"checksum\",\r",
            "                    \"integrity\",\r",
            "                    \"modified\",\r",
            "                    \"corrupted\",\r",
            "                ]\r",
            "            ):\r",
            "                return \"Data integrity check failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"byzan\", \"fault\", \"malicious\", \"attack\", \"adversary\"]\r",
            "            ):\r",
            "                return \"Protocol security violation detected\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"verify\", \"verif\", \"commit\", \"invalid\", \"mismatch\"]\r",
            "            ):\r",
            "                return \"Cryptographic verification failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"prime\", \"generator\", \"arithmetic\", \"computation\"]\r",
            "            ):\r",
            "                return \"Cryptographic parameter validation failed\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower for keyword in [\"timeout\", \"expired\", \"future\"]\r",
            "            ):\r",
            "                return \"Security timestamp verification failed\"\r",
            "\r",
            "            # Additional categories for better coverage\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"singular\", \"solve\", \"matrix\", \"gauss\"]\r",
            "            ):\r",
            "                return \"Matrix operation failed during cryptographic computation\"\r",
            "\r",
            "            if any(\r",
            "                keyword in message_lower\r",
            "                for keyword in [\"party\", \"participant\", \"diagnostics\"]\r",
            "            ):\r",
            "                return \"Participant verification failed\"\r",
            "\r",
            "            if any(keyword in message_lower for keyword in [\"hash\", \"blake3\", \"sha3\"]):\r",
            "                return \"Hash operation failed\"\r",
            "\r",
            "            # Default generic message\r",
            "            return \"Cryptographic operation failed\"\r",
            "        else:\r",
            "            return message\r",
            "\r",
            "    def _raise_sanitized_error(self, error_class: Type[Exception], message: str, detailed_message: Optional[str] = None) -> NoReturn:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Raise an error with a sanitized message based on configuration.\r",
            "\r",
            "        Arguments:\r",
            "            error_class: Exception class to raise.\r",
            "            message (str): The original error message.\r",
            "            detailed_message (str, optional): Detailed information to log but not expose.\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        sanitized: str = self._sanitize_error(message, detailed_message)\r",
            "        raise error_class(sanitized)\r",
            "\r",
            "    def _compute_hash_commitment_single(\r",
            "        self, \r",
            "        value: FieldElement, \r",
            "        randomizer: FieldElement, \r",
            "        index: int,\r",
            "        context: Optional[str] = None, \r",
            "        extra_entropy: Optional[bytes] = None\r",
            "    ) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Single-instance hash commitment computation (internal use).\r",
            "\r",
            "            Uses deterministic byte encoding for integers to ensure consistent commitment\r",
            "            values regardless of platform or execution environment, which is critical\r",
            "            for cryptographic security.\r",
            "\r",
            "        Arguments:\r",
            "            value (int): The value to commit to.\r",
            "            randomizer (int): The randomizer value.\r",
            "            index (int): The position index (not used in hash calculation, kept for API compatibility).\r",
            "            context (str, optional): Context string for domain separation. Defaults to \"polynomial\".\r",
            "            extra_entropy (bytes, optional): Extra entropy for low-entropy secrets.\r",
            "\r",
            "        Inputs:\r",
            "            value: The value to commit to.\r",
            "            randomizer: Randomizer.\r",
            "            index: Index (not used in hash computation)\r",
            "            context: Context string\r",
            "            extra_entropy: extra_entropy bytes\r",
            "\r",
            "        Outputs:\r",
            "            int: The computed hash commitment.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If index is negative.\r",
            "        \"\"\"\r",
            "\r",
            "        # Add input validation\r",
            "        if not isinstance(value, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"value must be an integer\")\r",
            "        if not isinstance(randomizer, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"randomizer must be an integer\")\r",
            "        if not isinstance(index, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"index must be an integer\")\r",
            "        if index < 0:\r",
            "            raise ValueError(\"index must be non-negative\")\r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "        if extra_entropy is not None and not isinstance(extra_entropy, bytes):\r",
            "            raise TypeError(\"extra_entropy must be bytes if provided\")\r",
            "\r",
            "        # Convert inputs to mpz to ensure consistent handling\r",
            "        value_mpz: \"gmpy2.mpz\" = gmpy2.mpz(value)\r",
            "        randomizer_mpz: \"gmpy2.mpz\" = gmpy2.mpz(randomizer)\r",
            "\r",
            "        # Calculate byte length based on prime size\r",
            "        prime_bit_length: int = self.group.prime.bit_length()\r",
            "        byte_length: int = (prime_bit_length + 7) // 8\r",
            "\r",
            "        # Prepare elements with proper byte encoding\r",
            "        elements: List[Any] = [\r",
            "            VSS_VERSION,  # Protocol version\r",
            "            \"COMMIT\",  # Fixed domain separator\r",
            "            context or \"polynomial\",  # Context with default\r",
            "            value_mpz.to_bytes(byte_length, \"big\"),  # Value to commit to\r",
            "            randomizer_mpz.to_bytes(byte_length, \"big\"),  # Randomizer value\r",
            "        ]\r",
            "\r",
            "        # Add extra entropy if provided for low-entropy secrets\r",
            "        if extra_entropy:\r",
            "            if isinstance(extra_entropy, bytes):\r",
            "                elements.append(extra_entropy)\r",
            "            else:\r",
            "                elements.append(str(extra_entropy).encode(\"utf-8\"))\r",
            "\r",
            "        # Use the consistent encoding method from the group class\r",
            "        encoded: bytes = self.group._enhanced_encode_for_hash(*elements)\r",
            "\r",
            "        # Use preferred hash algorithm\r",
            "        hash_output: bytes\r",
            "        if HAS_BLAKE3 and self.config.use_blake3:\r",
            "            hash_output = blake3.blake3(encoded).digest(32)\r",
            "        else:\r",
            "            hash_output = hashlib.sha3_256(encoded).digest()\r",
            "\r",
            "        return int.from_bytes(hash_output, \"big\") % self.group.prime\r",
            "\r",
            "    def _compute_hash_commitment(\r",
            "        self, \r",
            "        value: FieldElement, \r",
            "        randomizer: FieldElement, \r",
            "        index: int, \r",
            "        context: Optional[str] = None, \r",
            "        extra_entropy: Optional[bytes] = None\r",
            "    ) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced hash commitment function with redundant execution for fault resistance.\r",
            "\r",
            "            This function protects against fault injection attacks by computing the hash\r",
            "            commitment multiple times and verifying the results match.\r",
            "\r",
            "        Arguments:\r",
            "            value (int): The value to commit to.\r",
            "            randomizer (int): The randomizer value.\r",
            "            index (int): The position index.\r",
            "            context (str, optional): Context string for domain separation. Defaults to \"polynomial\".\r",
            "            extra_entropy (bytes, optional): Extra entropy for low-entropy secrets.\r",
            "\r",
            "        Inputs:\r",
            "          value: value\r",
            "          randomizer: randomizer\r",
            "          index: index\r",
            "          context: context\r",
            "          extra_entropy: extra entropy\r",
            "\r",
            "        Outputs:\r",
            "            int: The computed hash commitment.\r",
            "        \"\"\"\r",
            "        return secure_redundant_execution(\r",
            "            self._compute_hash_commitment_single,\r",
            "            value,\r",
            "            randomizer,\r",
            "            index,\r",
            "            context,\r",
            "            extra_entropy,\r",
            "            sanitize_error_func=self._sanitize_error,\r",
            "            function_name=\"_compute_hash_commitment\",\r",
            "        )\r",
            "\r",
            "    def _compute_combined_randomizer(self, randomizers: List[FieldElement], x: FieldElement) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Compute the combined randomizer for evaluating a polynomial at point x.\r",
            "\r",
            "        Arguments:\r",
            "            randomizers (list): List of randomizers for each coefficient.\r",
            "            x (int): Point at which to evaluate.\r",
            "\r",
            "        Inputs:\r",
            "            randomizers: List of randomizers.\r",
            "            x: Point at which to evaluate\r",
            "\r",
            "        Outputs:\r",
            "            int: Combined randomizer value for point x.\r",
            "        \"\"\"\r",
            "        r_combined: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        x_power: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "\r",
            "        r_i: FieldElement\r",
            "        for r_i in randomizers:\r",
            "            r_combined = (r_combined + gmpy2.mpz(r_i) * x_power) % self.group.prime\r",
            "            x_power = (x_power * gmpy2.mpz(x)) % self.group.prime\r",
            "\r",
            "        return r_combined\r",
            "\r",
            "    def _compute_expected_commitment(self, commitments: List[Union[Tuple[FieldElement, ...], FieldElement]], x: FieldElement) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Compute the expected commitment value for a polynomial at point x.\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of commitments for each coefficient.\r",
            "            x (int): Point at which to evaluate.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "            x: x\r",
            "\r",
            "        Outputs:\r",
            "            int: Expected commitment value at point x.\r",
            "        \"\"\"\r",
            "        expected: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        x_power: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "\r",
            "        c_i: Union[Tuple[FieldElement, ...], FieldElement]\r",
            "        for c_i in commitments:\r",
            "            # Extract commitment value from tuple if hash-based\r",
            "            commitment_value: \"gmpy2.mpz\" = gmpy2.mpz(c_i[0] if isinstance(c_i, tuple) else c_i)\r",
            "            expected = (expected + commitment_value * x_power) % self.group.prime\r",
            "            x_power = (x_power * gmpy2.mpz(x)) % self.group.prime\r",
            "\r",
            "        return expected\r",
            "\r",
            "    def _verify_hash_based_commitment(\r",
            "        self,\r",
            "        value: Union[int, \"gmpy2.mpz\"],  # Improved type annotation\r",
            "        combined_randomizer: Union[int, \"gmpy2.mpz\"],\r",
            "        x: Union[int, \"gmpy2.mpz\"],\r",
            "        expected_commitment: Union[int, \"gmpy2.mpz\"],\r",
            "        context: Optional[str] = None,\r",
            "        extra_entropy: Optional[bytes] = None,\r",
            "    ) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verify a hash-based commitment for a value at point x.\r",
            "\r",
            "        Arguments:\r",
            "            value (int): The value to verify.\r",
            "            combined_randomizer (int): Combined randomizer for this point.\r",
            "            x (int): The x-coordinate or index.\r",
            "            expected_commitment (int): The expected commitment value.\r",
            "            context (str, optional): Optional context string.\r",
            "            extra_entropy (bytes, optional): Extra entropy for low-entropy secrets.\r",
            "\r",
            "        Inputs:\r",
            "          value: value\r",
            "          combined_randomizer: combined randomizer\r",
            "          x: x\r",
            "          expected_commitment: expected commitment\r",
            "          context: context\r",
            "          extra_entropy: extra_entropy\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if verification succeeds, False otherwise.\r",
            "        \"\"\"\r",
            "        # Compute the hash commitment\r",
            "        computed_commitment: Union[int, \"gmpy2.mpz\"] = self._compute_hash_commitment(\r",
            "            value, combined_randomizer, x, context, extra_entropy\r",
            "        )\r",
            "\r",
            "        # Compare with expected commitment using constant-time comparison\r",
            "        return constant_time_compare(computed_commitment, expected_commitment)\r",
            "\r",
            "    def create_commitments(self, coefficients: List[FieldElement], context: Optional[str] = None) -> CommitmentList:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create post-quantum secure hash-based commitments to polynomial coefficients.\r",
            "    \r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081] where a\u2080 is the secret.\r",
            "            context (str, optional): Optional context string for domain separation.\r",
            "    \r",
            "        Inputs:\r",
            "            coefficients: List of coefficients\r",
            "            context: Context string\r",
            "    \r",
            "        Outputs:\r",
            "            list: List of (hash, randomizer) tuples representing hash-based commitments.\r",
            "    \r",
            "        Raises:\r",
            "            TypeError: If coefficients is not a list.\r",
            "            ValueError: If coefficients list is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(coefficients, list):\r",
            "            raise TypeError(\"coefficients must be a list\")\r",
            "            \r",
            "        if not coefficients:\r",
            "            self._raise_sanitized_error(ValueError, \"Coefficients list cannot be empty\")\r",
            "            \r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "        \r",
            "        # Use the enhanced commitment creation method for better security\r",
            "        return self.create_enhanced_commitments(coefficients, context)\r",
            "\r",
            "    def create_enhanced_commitments(self, coefficients: List[FieldElement], context: Optional[str] = None) -> CommitmentList:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create enhanced hash-based commitments with improved entropy handling\r",
            "            for low-entropy secrets (Baghery's method, 2025).\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients.\r",
            "            context (str, optional): Optional context string for domain separation.\r",
            "\r",
            "        Inputs:\r",
            "            coefficients: List of coefficients\r",
            "            context: Context string\r",
            "\r",
            "        Outputs:\r",
            "            list: List of (hash, randomizer) tuples.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If coefficients is not a list or context is not a string.\r",
            "            ParameterError: If coefficients list is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(coefficients, list):\r",
            "            raise TypeError(\"coefficients must be a list\")\r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "\r",
            "        if not coefficients:\r",
            "            self._raise_sanitized_error(\r",
            "                ParameterError, \"Coefficients list cannot be empty\"\r",
            "            )\r",
            "\r",
            "        # Convert all coefficients to integers and reduce modulo field prime\r",
            "        coeffs_int: List[\"gmpy2.mpz\"] = [gmpy2.mpz(coeff) % self.field.prime for coeff in coefficients]\r",
            "\r",
            "        # Check entropy of secret coefficient (first coefficient)\r",
            "        secret: \"gmpy2.mpz\" = coeffs_int[0]\r",
            "        low_entropy_threshold: int = (\r",
            "            256  # In bits (enhanced from previous 128-bit threshold)\r",
            "        )\r",
            "        might_have_low_entropy: bool = secret.bit_length() < low_entropy_threshold\r",
            "\r",
            "        # Create enhanced hash-based commitments\r",
            "        commitments: CommitmentList = []\r",
            "        i: int\r",
            "        coeff: \"gmpy2.mpz\"\r",
            "        for i, coeff in enumerate(coeffs_int):\r",
            "            # Generate secure randomizer\r",
            "            r_i: FieldElement = self.group.secure_random_element()\r",
            "\r",
            "            # Add extra entropy for the secret if needed\r",
            "            extra_entropy: Optional[bytes] = None\r",
            "            if i == 0 and might_have_low_entropy:\r",
            "                extra_entropy = secrets.token_bytes(32)\r",
            "\r",
            "            # Use the dedicated hash commitment function\r",
            "            commitment: FieldElement = self._compute_hash_commitment(\r",
            "                coeff, r_i, i, context or \"polynomial\", extra_entropy\r",
            "            )\r",
            "\r",
            "            # Store commitment and randomizer\r",
            "            commitments.append((commitment, r_i, extra_entropy))\r",
            "\r",
            "        return commitments\r",
            "\r",
            "    def _verify_share_hash_based_single(self, x: FieldElement, y: FieldElement, commitments: CommitmentList) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Single-instance share verification (internal use).\r",
            "\r",
            "        Arguments:\r",
            "            x (int): x-coordinate of the share.\r",
            "            y (int): y-coordinate of the share.\r",
            "            commitments (list): List of (commitment, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            x: x\r",
            "            y: y\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the share is valid, False otherwise.\r",
            "        \"\"\"\r",
            "        # Extract randomizers from commitments\r",
            "        randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "\r",
            "        # Compute combined randomizer\r",
            "        r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "\r",
            "        # Compute expected commitment\r",
            "        expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "        # Extract extra_entropy if present (should be in the first coefficient only)\r",
            "        extra_entropy: Optional[bytes] = None\r",
            "        if len(commitments) > 0 and len(commitments[0]) > 2:\r",
            "            extra_entropy = commitments[0][2]  # Get extra_entropy from first coefficient\r",
            "\r",
            "        # Verify using helper method\r",
            "        return self._verify_hash_based_commitment(\r",
            "            y, r_combined, x, expected_commitment, extra_entropy=extra_entropy\r",
            "        )\r",
            "\r",
            "    def verify_share(self, share_x: FieldElement, share_y: FieldElement, commitments: CommitmentList) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Fault-resistant share verification with redundant execution.\r",
            "\r",
            "            Verifies that a share (x, y) lies on the polynomial committed to by the commitments\r",
            "            using post-quantum secure hash-based verification with fault injection protection.\r",
            "\r",
            "        Arguments:\r",
            "            share_x (int): x-coordinate of the share.\r",
            "            share_y (int): y-coordinate of the share (the actual share value).\r",
            "            commitments (list): List of (commitment, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            share_x: x coordinate\r",
            "            share_y: y coordinate\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the share is valid, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or commitments is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(share_x, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_x must be an integer\")\r",
            "        if not isinstance(share_y, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_y must be an integer\")\r",
            "        if not isinstance(commitments, list) or not commitments:\r",
            "            raise TypeError(\"commitments must be a non-empty list\")\r",
            "\r",
            "        # Validate commitment format\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"commitments must be a list of (commitment, randomizer) tuples\"\r",
            "            )\r",
            "\r",
            "        # Convert to integers and use redundant verification\r",
            "        x: \"gmpy2.mpz\"\r",
            "        y: \"gmpy2.mpz\"\r",
            "        x, y = gmpy2.mpz(share_x), gmpy2.mpz(share_y)\r",
            "        return secure_redundant_execution(\r",
            "            self._verify_share_hash_based_single,\r",
            "            x,\r",
            "            y,\r",
            "            commitments,\r",
            "            sanitize_error_func=self._sanitize_error,\r",
            "            function_name=\"verify_share\",\r",
            "        )\r",
            "\r",
            "    def batch_verify_shares(self, shares: List[SharePoint], commitments: CommitmentList) -> VerificationResult:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Efficiently verify multiple shares against the same commitments.\r",
            "\r",
            "            Uses optimized batch verification for hash-based commitments with caching of\r",
            "            intermediate values for improved performance with large batches.\r",
            "\r",
            "        Arguments:\r",
            "            shares (list): List of (x, y) share tuples.\r",
            "            commitments (list): List of (commitment, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            shares: shares\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (all_valid: bool, results: Dict mapping share indices to verification results).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or are empty.\r",
            "            ValueError: If shares list is empty.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(shares, list):\r",
            "            raise TypeError(\"shares must be a list of (x, y) tuples\")\r",
            "        if not shares:\r",
            "            self._raise_sanitized_error(ValueError, \"shares list cannot be empty\")\r",
            "        if not all(isinstance(s, tuple) and len(s) == 2 for s in shares):\r",
            "            raise TypeError(\"Each share must be a tuple of (x, y)\")\r",
            "\r",
            "        if not isinstance(commitments, list) or not commitments:\r",
            "            raise TypeError(\"commitments must be a non-empty list\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"commitments must be a list of (commitment, randomizer) tuples\"\r",
            "            )\r",
            "\r",
            "        results: Dict[int, bool] = {}\r",
            "        all_valid: bool = True\r",
            "\r",
            "        # Standard verification for small batches\r",
            "        if len(shares) < 5:\r",
            "            i: int\r",
            "            x: FieldElement\r",
            "            y: FieldElement\r",
            "            is_valid: bool\r",
            "            for i, (x, y) in enumerate(shares):\r",
            "                is_valid = self.verify_share(x, y, commitments)\r",
            "                results[i] = is_valid\r",
            "                # Use constant-time boolean operation\r",
            "                all_valid &= is_valid  # Constant-time AND\r",
            "            return all_valid, results\r",
            "\r",
            "        # Extract randomizers for more efficient processing\r",
            "        randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "\r",
            "        # Extract extra_entropy if present (only for first coefficient)\r",
            "        extra_entropy: Optional[bytes] = None\r",
            "        if len(commitments) > 0 and len(commitments[0]) > 2:\r",
            "            extra_entropy = commitments[0][2]\r",
            "\r",
            "        # For larger batches, use optimized verification approach with caching\r",
            "        # Precompute powers of x for each share to avoid redundant calculations\r",
            "        x_powers_cache: Dict[FieldElement, List[\"gmpy2.mpz\"]] = {}\r",
            "\r",
            "        # Prepare commitment combinations for each share\r",
            "        share_commitments: List[Tuple[FieldElement, FieldElement, FieldElement, FieldElement]] = []\r",
            "\r",
            "        # First pass: compute and cache powers of x and prepare combined values\r",
            "        x: FieldElement\r",
            "        y: FieldElement\r",
            "        for x, y in shares:\r",
            "            if x not in x_powers_cache:\r",
            "                # Compute and cache powers of x\r",
            "                powers: List[\"gmpy2.mpz\"] = [gmpy2.mpz(1)]  # x^0 = 1\r",
            "                current_power: \"gmpy2.mpz\" = gmpy2.mpz(1)\r",
            "                j: int\r",
            "                for j in range(1, len(commitments)):\r",
            "                    current_power = (current_power * gmpy2.mpz(x)) % self.field.prime\r",
            "                    powers.append(current_power)\r",
            "                x_powers_cache[x] = powers\r",
            "\r",
            "            # Use helper methods to compute randomizers and expected commitments\r",
            "            r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "            expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "            share_commitments.append((x, y, r_combined, expected_commitment))\r",
            "\r",
            "        # Second pass: verify each share with precomputed values (with batch processing)\r",
            "        batch_size: int = min(32, len(share_commitments))  # Process in reasonable batches\r",
            "\r",
            "        batch_start: int\r",
            "        for batch_start in range(0, len(share_commitments), batch_size):\r",
            "            batch_end: int = min(batch_start + batch_size, len(share_commitments))\r",
            "            batch: List[Tuple[FieldElement, FieldElement, FieldElement, FieldElement]] = share_commitments[batch_start:batch_end]\r",
            "\r",
            "            # Process verification in batches\r",
            "            i: int\r",
            "            idx: int\r",
            "            is_valid: bool\r",
            "            for i, (x, y, r_combined, expected_commitment) in enumerate(batch):\r",
            "                idx = batch_start + i\r",
            "                is_valid = self._verify_hash_based_commitment(\r",
            "                    y, r_combined, x, expected_commitment, extra_entropy=extra_entropy\r",
            "                )\r",
            "\r",
            "                results[idx] = is_valid\r",
            "                # Update boolean result using logical AND operation\r",
            "                all_valid &= is_valid  # Note: Not guaranteed to be constant-time\r",
            "\r",
            "        return all_valid, results\r",
            "\r",
            "    def serialize_commitments(self, commitments: CommitmentList) -> str:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Serialize commitment data with checksum for fault resistance.\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of (hash, randomizer) tuples.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            str: String with base64-encoded serialized data with embedded checksum.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If commitments is not a list or has incorrect format.\r",
            "            ValueError: If commitments list is empty.\r",
            "            SerializationError: If serialization fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            self._raise_sanitized_error(ValueError, \"commitments list cannot be empty\")\r",
            "\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Extract commitment values\r",
            "        commitment_values: List[Tuple[int, int, Optional[str]]] = [\r",
            "            (int(c), int(r), e.hex() if e else None) for c, r, e in commitments\r",
            "        ]\r",
            "\r",
            "        # Create the data structure\r",
            "        result: Dict[str, Any] = {\r",
            "            \"version\": VSS_VERSION,\r",
            "            \"timestamp\": int(time.time()),\r",
            "            \"generator\": int(self.generator),\r",
            "            \"prime\": int(self.group.prime),\r",
            "            \"commitments\": commitment_values,\r",
            "            \"hash_based\": True,\r",
            "        }\r",
            "\r",
            "        try:\r",
            "            # Pack with msgpack for efficient serialization\r",
            "            packed_data: bytes = msgpack.packb(result)\r",
            "\r",
            "            # Compute checksum and create wrapper\r",
            "            checksum_wrapper: Dict[str, bytes] = {\r",
            "                \"data\": packed_data,\r",
            "                \"checksum\": compute_checksum(packed_data),\r",
            "            }\r",
            "\r",
            "            # Pack the wrapper and encode\r",
            "            packed_wrapper: bytes = msgpack.packb(checksum_wrapper)\r",
            "            return urlsafe_b64encode(packed_wrapper).decode(\"utf-8\")\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to serialize commitments: {e}\"\r",
            "            message = \"Serialization failed\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def deserialize_commitments(self, data: str) -> Tuple[CommitmentList, FieldElement, FieldElement, int, bool]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Deserialize commitment data with checksum verification\r",
            "\r",
            "        Arguments:\r",
            "            data (str): Serialized commitment data string.\r",
            "\r",
            "        Inputs:\r",
            "            data: Serialized data\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (commitments, generator, prime, timestamp, is_hash_based).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If data is not a string or is empty.\r",
            "            ValueError: If data is empty.\r",
            "            SerializationError: If deserialization or validation fails.\r",
            "            SecurityError: If checksum or cryptographic parameter validation fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(data, str):\r",
            "            self._raise_sanitized_error(TypeError, \"Data must be a string\")\r",
            "        if not data:\r",
            "            self._raise_sanitized_error(ValueError, \"Data cannot be empty\")\r",
            "\r",
            "        try:\r",
            "            # Decode from URL-safe base64\r",
            "            decoded: bytes = urlsafe_b64decode(data.encode(\"utf-8\"))\r",
            "\r",
            "            # Use Unpacker with security settings\r",
            "            unpacker: msgpack.Unpacker = msgpack.Unpacker(\r",
            "                use_list=False,  # Use tuples instead of lists for immutability\r",
            "                raw=True,  # Keep binary data as bytes\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,  # 10MB limit\r",
            "            )\r",
            "            unpacker.feed(decoded)\r",
            "\r",
            "            try:\r",
            "                # Unpack the checksum wrapper\r",
            "                wrapper: Dict[bytes, Any] = unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack msgpack data: {e}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Verify checksum - this is a critical security check\r",
            "            if b\"checksum\" not in wrapper or b\"data\" not in wrapper:\r",
            "                detailed_msg = f\"Detailed deserialization error - data format: {type(data)}, traceback: {traceback.format_exc()}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            packed_data: bytes = wrapper[b\"data\"]\r",
            "            expected_checksum: int = wrapper[b\"checksum\"]\r",
            "            actual_checksum: int = compute_checksum(packed_data)\r",
            "\r",
            "            if not constant_time_compare(actual_checksum, expected_checksum):\r",
            "                detailed_msg = f\"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}\"\r",
            "                message = \"Data integrity check failed - possible tampering detected\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Feed the inner data to a new Unpacker instance\r",
            "            inner_unpacker: msgpack.Unpacker = msgpack.Unpacker(\r",
            "                use_list=False,\r",
            "                raw=True,\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,\r",
            "            )\r",
            "            inner_unpacker.feed(packed_data)\r",
            "\r",
            "            try:\r",
            "                # Proceed with unpacking the actual data\r",
            "                unpacked: Dict[bytes, Any] = inner_unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack inner msgpack data: {e}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # With raw=True, keys will be bytes instead of strings\r",
            "            version_key: bytes = b\"version\"\r",
            "            version_bytes: bytes = VSS_VERSION.encode(\"utf-8\")\r",
            "\r",
            "            # Validate the version\r",
            "            if unpacked.get(version_key) != version_bytes:\r",
            "                detailed_msg = f\"Unsupported VSS version: {unpacked.get(version_key)}\"\r",
            "                message = \"Unsupported version\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate structure of deserialized data - note use of byte keys\r",
            "            if not isinstance(\r",
            "                unpacked.get(b\"commitments\"), tuple\r",
            "            ):  # was list, now tuple with use_list=False\r",
            "                detailed_msg = f\"Invalid commitment data: expected sequence, got {type(unpacked.get(b'commitments'))}\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            if not isinstance(unpacked.get(b\"generator\"), int):\r",
            "                detailed_msg = f\"Invalid generator: expected integer, got {type(unpacked.get(b'generator'))}\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            if not isinstance(unpacked.get(b\"prime\"), int):\r",
            "                detailed_msg = f\"Invalid prime: expected integer, got {type(unpacked.get(b'prime'))}\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Additional check for commitment structure\r",
            "            i: int\r",
            "            commitment: Tuple[Any, ...]\r",
            "            for i, commitment in enumerate(unpacked.get(b\"commitments\", tuple())):\r",
            "                if not isinstance(commitment, tuple) or len(commitment) not in (2, 3):\r",
            "                    detailed_msg = f\"Invalid commitment format at index {i}: expected (commitment, randomizer) or (commitment, randomizer, extra_entropy) tuple\"\r",
            "                    message = \"Invalid data structure\"\r",
            "                    self._raise_sanitized_error(\r",
            "                        SerializationError, message, detailed_msg\r",
            "                    )\r",
            "\r",
            "            # Extract the commitments and parameters\r",
            "            commitments: Tuple[Tuple[Any, ...], ...] = unpacked.get(b\"commitments\")\r",
            "            generator: int = unpacked.get(b\"generator\")\r",
            "            prime: int = unpacked.get(b\"prime\")\r",
            "            timestamp: int = unpacked.get(b\"timestamp\", 0)\r",
            "            is_hash_based: bool = unpacked.get(b\"hash_based\", True)  # Default to hash-based\r",
            "\r",
            "            # Enhanced validity checks\r",
            "            if not (commitments and generator and prime):\r",
            "                detailed_msg = \"Missing required fields in commitment data\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate that prime is actually prime\r",
            "            if prime not in SAFE_PRIMES.values() and self.config.safe_prime:\r",
            "                if not CyclicGroup._is_probable_prime(prime):\r",
            "                    detailed_msg = \"Deserialized prime value failed primality test\"\r",
            "                    message = \"Cryptographic parameter validation failed\"\r",
            "                    self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "                if self.config.safe_prime and not CyclicGroup._is_safe_prime(prime):\r",
            "                    detailed_msg = \"Deserialized prime is not a safe prime\"\r",
            "                    message = \"Cryptographic parameter validation failed\"\r",
            "                    self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Validate generator is in the correct range\r",
            "            if generator <= 1 or generator >= prime - 1:\r",
            "                detailed_msg = \"Deserialized generator is outside valid range\"\r",
            "                message = \"Cryptographic parameter validation failed\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Ensure the generator is valid for this prime\r",
            "            g: \"gmpy2.mpz\" = gmpy2.mpz(generator)\r",
            "            p: \"gmpy2.mpz\" = gmpy2.mpz(prime)\r",
            "            q: \"gmpy2.mpz\" = (p - 1) // 2  # For safe primes, q = (p-1)/2 is also prime\r",
            "            # A proper generator for a safe prime p=2q+1 should satisfy g^q \u2260 1 mod p\r",
            "            if gmpy2.powmod(g, q, p) == 1:\r",
            "                detailed_msg = \"Deserialized generator is not a valid group generator\"\r",
            "                message = \"Cryptographic parameter validation failed\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Additional validation to verify all commitment values are in the proper range\r",
            "            i: int\r",
            "            commitment_data: Tuple[Any, ...]\r",
            "            for i, commitment_data in enumerate(commitments):\r",
            "                if len(commitment_data) >= 2:\r",
            "                    commitment_value: int = commitment_data[0]\r",
            "                    randomizer: int = commitment_data[1]\r",
            "\r",
            "                    # Validate commitment and randomizer are in valid range\r",
            "                    if not (0 <= commitment_value < prime) or not (\r",
            "                        0 <= randomizer < prime\r",
            "                    ):\r",
            "                        detailed_msg = f\"Commitment or randomizer at index {i} is outside valid range\"\r",
            "                        message = \"Cryptographic parameter validation failed\"\r",
            "                        self._raise_sanitized_error(\r",
            "                            SecurityError, message, detailed_msg\r",
            "                        )\r",
            "\r",
            "            # Enforce hash-based commitments for post-quantum security\r",
            "            if not is_hash_based:\r",
            "                detailed_msg = \"Only hash-based commitments are supported in this post-quantum secure version\"\r",
            "                message = \"Unsupported commitment type\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Reconstruct hash-based commitments\r",
            "            reconstructed_commitments: CommitmentList = []\r",
            "            commitment_data: Tuple[Any,...]\r",
            "            for commitment_data in commitments:\r",
            "                if len(commitment_data) >= 3 and commitment_data[2]:\r",
            "                    # Has extra entropy - convert hex string back to bytes\r",
            "                    reconstructed_commitments.append(\r",
            "                        (\r",
            "                            gmpy2.mpz(commitment_data[0]),\r",
            "                            gmpy2.mpz(commitment_data[1]),\r",
            "                            bytes.fromhex(commitment_data[2]) if commitment_data[2] else None,\r",
            "                        )\r",
            "                    )\r",
            "                else:\r",
            "                    # No extra entropy\r",
            "                    reconstructed_commitments.append(\r",
            "                        (\r",
            "                            gmpy2.mpz(commitment_data[0]),\r",
            "                            gmpy2.mpz(commitment_data[1]),\r",
            "                            None,\r",
            "                        )\r",
            "                    )\r",
            "\r",
            "            return (\r",
            "                reconstructed_commitments,\r",
            "                gmpy2.mpz(generator),\r",
            "                gmpy2.mpz(prime),\r",
            "                timestamp,\r",
            "                is_hash_based,\r",
            "            )\r",
            "\r",
            "        except Exception as e:\r",
            "            if isinstance(e, (SerializationError, SecurityError)):\r",
            "                raise\r",
            "\r",
            "            detailed_msg = f\"Exception during deserialization: {str(e)}\"\r",
            "            message = \"Failed to deserialize commitments\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def verify_share_from_serialized(self, share_x: FieldElement, share_y: FieldElement, serialized_commitments: str) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verify a share against serialized commitment data.\r",
            "\r",
            "        Arguments:\r",
            "            share_x (int): x-coordinate of the share.\r",
            "            share_y (int): y-coordinate of the share.\r",
            "            serialized_commitments (str): Serialized commitment data.\r",
            "\r",
            "        Inputs:\r",
            "            share_x: x coordinate\r",
            "            share_y: y coordinate\r",
            "            serialized_commitments: serialized commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the share is valid, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or serialized_commitments is empty.\r",
            "            VerificationError: If deserialization or verification fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(share_x, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_x must be an integer\")\r",
            "        if not isinstance(share_y, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_y must be an integer\")\r",
            "        if not isinstance(serialized_commitments, str) or not serialized_commitments:\r",
            "            raise TypeError(\"serialized_commitments must be a non-empty string\")\r",
            "\r",
            "        try:\r",
            "            # Deserialize the commitments\r",
            "            commitments: CommitmentList\r",
            "            generator: FieldElement\r",
            "            prime: FieldElement\r",
            "            timestamp: int\r",
            "            is_hash_based: bool\r",
            "            commitments, generator, prime, timestamp, is_hash_based = (\r",
            "                self.deserialize_commitments(serialized_commitments)\r",
            "            )\r",
            "\r",
            "            # Create a group with the same parameters\r",
            "            group: CyclicGroup = CyclicGroup(prime=prime, generator=generator)\r",
            "\r",
            "            # Create a new VSS instance with this group\r",
            "            temp_config: VSSConfig = VSSConfig()\r",
            "            temp_vss: FeldmanVSS = FeldmanVSS(self.field, temp_config, group)\r",
            "\r",
            "            # Verify the share\r",
            "            return temp_vss.verify_share(share_x, share_y, commitments)\r",
            "\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Detailed verification failure for share ({share_x}, {share_y}): {str(e)}, Traceback: {traceback.format_exc()}\"\r",
            "            message = f\"Failed to verify share: {e}\"\r",
            "            self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "    def clear_cache(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Clear verification cache to free memory.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        self.group.clear_cache()\r",
            "\r",
            "    def __del__(self) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Clean up when the object is deleted.\r",
            "\r",
            "        Arguments:\r",
            "            None\r",
            "\r",
            "        Inputs:\r",
            "            None\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        self.clear_cache()\r",
            "\r",
            "        # Securely wipe any sensitive data\r",
            "        if hasattr(self, \"generator\"):\r",
            "            del self.generator\r",
            "        if hasattr(self, \"field\"):\r",
            "            self.field.clear_cache()\r",
            "\r",
            "    def refresh_shares(\r",
            "        self,\r",
            "        shares: ShareDict,\r",
            "        threshold: int,\r",
            "        total_shares: int,\r",
            "        original_commitments: Optional[CommitmentList] = None,\r",
            "        participant_ids: Optional[List[int]] = None,\r",
            "    ) -> RefreshingResult:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Refresh shares while preserving the same secret using an optimized implementation\r",
            "            of Chen & Lindell's Protocol 5, providing stronger security guarantees in asynchronous\r",
            "            environments.\r",
            "\r",
            "        Arguments:\r",
            "            shares (dict): Dictionary mapping participant IDs to their shares {id: (x, y)}.\r",
            "            threshold (int): The secret sharing threshold.\r",
            "            total_shares (int): Total number of shares to generate.\r",
            "            original_commitments (list, optional): Original commitment values (optional, for proof validation).\r",
            "            participant_ids (list, optional): Optional list of IDs for participants (defaults to numeric IDs).\r",
            "\r",
            "        Inputs:\r",
            "            shares: shares\r",
            "            threshold: threshold\r",
            "            total_shares: total_shares\r",
            "            original_commitments: original commitments\r",
            "            participant_ids: participant_ids\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (new_shares, new_commitments, verification_data).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If threshold or total_shares are invalid, or participant_ids length is incorrect.\r",
            "            ParameterError: If not enough shares are provided.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(shares, dict):\r",
            "            raise TypeError(\r",
            "                \"shares must be a dictionary mapping participant IDs to (x, y) tuples\"\r",
            "            )\r",
            "        if not all(isinstance(v, tuple) and len(v) == 2 for v in shares.values()):\r",
            "            raise TypeError(\"Each share must be a tuple of (x, y)\")\r",
            "\r",
            "        if not isinstance(threshold, int) or threshold < 2:\r",
            "            raise ValueError(\"threshold must be an integer >= 2\")\r",
            "\r",
            "        if not isinstance(total_shares, int) or total_shares < threshold:\r",
            "            raise ValueError(\"total_shares must be an integer >= threshold\")\r",
            "\r",
            "        if original_commitments is not None and not isinstance(\r",
            "            original_commitments, list\r",
            "        ):\r",
            "            raise TypeError(\"original_commitments must be a list if provided\")\r",
            "\r",
            "        if participant_ids is not None:\r",
            "            if not isinstance(participant_ids, list):\r",
            "                raise TypeError(\"participant_ids must be a list if provided\")\r",
            "            if len(participant_ids) != total_shares:\r",
            "                raise ValueError(\"Number of participant_ids must match total_shares\")\r",
            "\r",
            "        if len(shares) < threshold:\r",
            "            detailed_msg = (\r",
            "                f\"Need at least {threshold} shares to refresh, got {len(shares)}\"\r",
            "            )\r",
            "            message = f\"Need at least {threshold} shares to refresh\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Set default participant IDs if not provided\r",
            "        if participant_ids is None:\r",
            "            participant_ids = list(range(1, total_shares + 1))\r",
            "\r",
            "        if len(participant_ids) != total_shares:\r",
            "            detailed_msg = \"Number of participant IDs must match total_shares\"\r",
            "            message = \"Invalid parameters\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Use enhanced additive resharing method (Chen & Lindell's Protocol 5)\r",
            "        # with optimizations for asynchronous environments\r",
            "        return self._refresh_shares_additive(\r",
            "            shares, threshold, total_shares, participant_ids\r",
            "        )\r",
            "\r",
            "    def _refresh_shares_additive(self, shares: ShareDict, threshold: int, total_shares: int, participant_ids: List[int]\r",
            "    ) -> RefreshingResult:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced refresh shares using optimized Chen & Lindell's Protocol 5 (additive resharing).\r",
            "\r",
            "            This implementation includes optimizations for:\r",
            "            1. Better performance in asynchronous environments\r",
            "            2. Reduced communication complexity\r",
            "            3. Improved resilience against adversarial parties\r",
            "            4. More efficient verification\r",
            "            5. Advanced Byzantine fault tolerance\r",
            "\r",
            "        Arguments:\r",
            "            shares (dict): Dictionary mapping participant IDs to their shares {id: (x, y)}.\r",
            "            threshold (int): The secret sharing threshold.\r",
            "            total_shares (int): Total number of shares to generate.\r",
            "            participant_ids (list): List of IDs for participants.\r",
            "\r",
            "        Inputs:\r",
            "            shares: shares\r",
            "            threshold: threshold\r",
            "            total_shares: total shares\r",
            "            participant_ids: participant ids\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (new_shares, new_commitments, verification_data).\r",
            "        \"\"\"\r",
            "        # Step 1: Each party creates a sharing of zero with enhanced verification\r",
            "        zero_sharings: Dict[int, ShareDict] = {}\r",
            "        zero_commitments: Dict[int, CommitmentList] = {}\r",
            "\r",
            "        # Use a deterministic seed derivation for each party to enable verification\r",
            "        # while reducing communication requirements\r",
            "        verification_seeds: Dict[int, bytes] = {}\r",
            "        master_seed: bytes = secrets.token_bytes(32)  # Generate master randomness\r",
            "\r",
            "        # Initialize verification_proofs dictionary\r",
            "        verification_proofs: Dict[int, Dict[int, Any]] = {p_id: {} for p_id in participant_ids}\r",
            "\r",
            "        party_id: int\r",
            "        for party_id in shares.keys():\r",
            "            # Derive a deterministic seed for this party\r",
            "            party_seed: bytes = self.hash_algorithm(\r",
            "                master_seed + str(party_id).encode()\r",
            "            ).digest()\r",
            "            verification_seeds[party_id] = party_seed\r",
            "\r",
            "            # Use the seed to generate a deterministic RNG\r",
            "            # Note: Using random.Random() with cryptographically strong seed is intentional here.\r",
            "            # We need deterministic but unpredictable randomness for the verification protocol.\r",
            "            # The security comes from party_seed being generated with a strong cryptographic hash.\r",
            "            party_rng: random.Random = random.Random(int.from_bytes(party_seed, byteorder=\"big\"))\r",
            "\r",
            "            # Generate a random polynomial of degree t-1 with constant term 0\r",
            "            zero_coeffs: List[FieldElement] = [gmpy2.mpz(0)]  # First coefficient is 0\r",
            "            _: int\r",
            "            for _ in range(1, threshold):\r",
            "                # Use the seeded RNG for deterministic coefficient generation\r",
            "                rand_value: int = party_rng.randrange(self.field.prime)\r",
            "                zero_coeffs.append(gmpy2.mpz(rand_value))\r",
            "\r",
            "            # Create shares for each participant using this polynomial\r",
            "            party_shares: ShareDict = {}\r",
            "            p_id: int\r",
            "            for p_id in participant_ids:\r",
            "                # Evaluate polynomial at the point corresponding to participant's ID\r",
            "                y_value: FieldElement = self._evaluate_polynomial(zero_coeffs, p_id)\r",
            "                party_shares[p_id] = (p_id, y_value)\r",
            "\r",
            "            # Create commitments to the zero polynomial coefficients with optimized batch processing\r",
            "            party_commitments: CommitmentList = self.create_commitments(zero_coeffs)\r",
            "\r",
            "            # More efficient verification for the zero constant term\r",
            "            # For hash-based commitments\r",
            "            commitment_value: FieldElement = party_commitments[0][0]\r",
            "            r_i: FieldElement = party_commitments[0][1]\r",
            "\r",
            "            # Use helper method for consistency\r",
            "            expected_zero_commitment: FieldElement = self._compute_hash_commitment(0, r_i, 0)\r",
            "\r",
            "            if not constant_time_compare(commitment_value, expected_zero_commitment):\r",
            "                detailed_msg = f\"Zero commitment verification failed for party {party_id}, commitment: {commitment_value}, expected: {expected_zero_commitment}\"\r",
            "                message = \"Zero commitment verification failed\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "            # Store this party's zero sharing and commitments\r",
            "            zero_sharings[party_id] = party_shares\r",
            "            zero_commitments[party_id] = party_commitments\r",
            "\r",
            "        # Step 2: Enhanced verification with improved Byzantine fault tolerance\r",
            "        # Optimized for better performance and security\r",
            "        verified_zero_shares: Dict[int, Dict[int, FieldElement]] = {p_id: {} for p_id in participant_ids}\r",
            "        invalid_shares_detected: Dict[int, List[int]] = {}\r",
            "        new_shares: ShareDict = {}\r",
            "        byzantine_parties: Dict[int, Dict[str, Any]] = {}\r",
            "\r",
            "        # Enhanced security parameters with dynamic adjustment\r",
            "        security_factor: float = max(0.5, 1.0 - (threshold / (2 * len(shares))))\r",
            "        min_verified_shares: int = max(threshold // 2, int(threshold * security_factor))\r",
            "\r",
            "        # Echo broadcast mechanism for consistency verification\r",
            "        # This adds Byzantine fault tolerance following Chen & Lindell's recommendations\r",
            "        echo_consistency: Dict[Tuple[int, int], bool] = self._process_echo_consistency(\r",
            "            zero_commitments, zero_sharings, participant_ids\r",
            "        )\r",
            "\r",
            "        # Identify Byzantine parties with adaptive quorum-based detection\r",
            "        byzantine_parties = {}\r",
            "        # Calculate consistency statistics per party\r",
            "        consistency_counts : Dict[int, Dict[str, int]]= {}\r",
            "        for (party_id, _), is_consistent in echo_consistency.items():\r",
            "            if party_id not in consistency_counts:\r",
            "                consistency_counts[party_id] = {\r",
            "                    \"consistent\": 0,\r",
            "                    \"inconsistent\": 0,\r",
            "                    \"total\": 0,\r",
            "                }\r",
            "\r",
            "            consistency_counts[party_id][\"total\"] += 1\r",
            "            if is_consistent:\r",
            "                consistency_counts[party_id][\"consistent\"] += 1\r",
            "            else:\r",
            "                consistency_counts[party_id][\"inconsistent\"] += 1\r",
            "\r",
            "        # Adaptive quorum calculation based on threat model and participant count\r",
            "        # More participants = higher required consistency ratio\r",
            "        base_quorum_ratio: float = 0.5  # Start at 50%\r",
            "        consistency_ratio_requirement: float = min(\r",
            "            0.8, base_quorum_ratio + 0.1 * (len(shares) / threshold - 1)\r",
            "        )\r",
            "\r",
            "        # Identify parties that failed to reach consistency quorum\r",
            "        \r",
            "        party_id: int\r",
            "        counts: Dict[str, int]\r",
            "        for party_id, counts in consistency_counts.items():\r",
            "            if counts[\"total\"] > 0:\r",
            "                consistency_ratio: float = counts[\"consistent\"] / counts[\"total\"]\r",
            "                if consistency_ratio < consistency_ratio_requirement:\r",
            "                    evidence: Dict[str, Union[str, float, int]] = {\r",
            "                        \"type\": \"insufficient_consistency_quorum\",\r",
            "                        \"consistency_ratio\": consistency_ratio,\r",
            "                        \"required_ratio\": consistency_ratio_requirement,\r",
            "                        \"consistent_count\": counts[\"consistent\"],\r",
            "                        \"inconsistent_count\": counts[\"inconsistent\"],\r",
            "                        \"total_checked\": counts[\"total\"],\r",
            "                    }\r",
            "                    byzantine_parties[party_id] = evidence\r",
            "                    warnings.warn(\r",
            "                        f\"Party {party_id} failed to reach consistency quorum \"\r",
            "                        f\"({consistency_ratio:.2f} < {consistency_ratio_requirement:.2f})\",\r",
            "                        SecurityWarning,\r",
            "                    )\r",
            "\r",
            "        # Standard Byzantine detection for each party\r",
            "        \r",
            "        party_id: int\r",
            "        for party_id in shares.keys():\r",
            "            if party_id in byzantine_parties:\r",
            "                continue  # Already identified as Byzantine\r",
            "\r",
            "            is_byzantine: bool\r",
            "            evidence: Dict[str, Any]\r",
            "            is_byzantine, evidence = self._detect_byzantine_behavior(\r",
            "                party_id,\r",
            "                zero_commitments[party_id],\r",
            "                zero_sharings[party_id],\r",
            "                echo_consistency,\r",
            "            )\r",
            "\r",
            "            if is_byzantine:\r",
            "                warnings.warn(\r",
            "                    f\"Detected Byzantine behavior from party {party_id}: {evidence.get('type', 'unknown')}\",\r",
            "                    SecurityWarning,\r",
            "                )\r",
            "                byzantine_parties[party_id] = evidence\r",
            "\r",
            "        # More efficient batch verification with adaptive batch sizing\r",
            "        batch_size: int = self._calculate_optimal_batch_size(\r",
            "            len(participant_ids), len(shares)\r",
            "        )\r",
            "\r",
            "        # Group shares by commitment set for more efficient batch verification\r",
            "        verification_batches: List[List[Tuple[int, int, int, int, CommitmentList]]] = self._prepare_verification_batches(\r",
            "            zero_sharings, zero_commitments, participant_ids, batch_size\r",
            "        )\r",
            "\r",
            "        # Process verification with improved parallelism\r",
            "        verification_results: List[Tuple[Tuple[int, int], bool]] = self._process_verification_batches(verification_batches)\r",
            "\r",
            "        # Process verification results with Byzantine exclusion\r",
            "        result: Tuple[Tuple[int, int], bool]\r",
            "        for (party_id, p_id), is_valid in verification_results:\r",
            "            # Skip shares from Byzantine parties\r",
            "            if party_id in byzantine_parties:\r",
            "                continue\r",
            "\r",
            "            # Changed default from True to False - more conservative security posture\r",
            "            if is_valid and echo_consistency.get((party_id, p_id), False):\r",
            "                # Store verified share with additional consistency check\r",
            "                share_value: FieldElement = self._get_share_value_from_results(\r",
            "                    party_id, p_id, zero_sharings\r",
            "                )\r",
            "                verified_zero_shares[p_id][party_id] = share_value\r",
            "            else:\r",
            "                # Enhanced detection of invalid shares\r",
            "                if p_id not in invalid_shares_detected:\r",
            "                    invalid_shares_detected[p_id] = []\r",
            "                invalid_shares_detected[p_id].append(party_id)\r",
            "\r",
            "                # Generate cryptographic proof with improved evidence collection\r",
            "                self._generate_invalidity_evidence(\r",
            "                    party_id,\r",
            "                    p_id,\r",
            "                    zero_sharings,\r",
            "                    zero_commitments,\r",
            "                    verification_proofs,\r",
            "                    is_valid,\r",
            "                    echo_consistency.get((party_id, p_id), False), # Changed default to False here too\r",
            "                )\r",
            "\r",
            "        # Improved collusion detection with network analysis algorithms\r",
            "        potential_collusion: List[int] = self._enhanced_collusion_detection(\r",
            "            invalid_shares_detected, shares.keys(), echo_consistency\r",
            "        )\r",
            "        p_id: int\r",
            "        # Process shares with adaptive security parameters\r",
            "        for p_id in participant_ids:\r",
            "            # Get original share with robust fallback\r",
            "            original_y: FieldElement = self._get_original_share_value(p_id, shares)\r",
            "\r",
            "            # Dynamic security threshold based on the situation\r",
            "            verified_count: int = len(verified_zero_shares[p_id])\r",
            "            required_threshold: int = self._determine_security_threshold(\r",
            "                threshold,\r",
            "                verified_count,\r",
            "                len(shares),\r",
            "                invalid_shares_detected.get(p_id, []),\r",
            "            )\r",
            "\r",
            "            # Enhanced security check with detailed diagnostics\r",
            "            if verified_count < required_threshold:\r",
            "                security_ratio: float = verified_count / threshold\r",
            "                diagnostics: Dict[str, Union[int, float, List[int]]] = {\r",
            "                    \"verified_count\": verified_count,\r",
            "                    \"threshold\": threshold,\r",
            "                    \"required_threshold\": required_threshold,\r",
            "                    \"security_ratio\": security_ratio,\r",
            "                    \"invalid_shares\": invalid_shares_detected.get(p_id, []),\r",
            "                    \"total_participants\": len(shares),\r",
            "                }\r",
            "\r",
            "                if verified_count < min_verified_shares:\r",
            "                    detailed_msg = (\r",
            "                        f\"Insufficient verified zero shares for participant {p_id}. \"\r",
            "                        f\"Security diagnostics: {diagnostics}. \"\r",
            "                        f\"Share refresh aborted for security reasons.\"\r",
            "                    )\r",
            "                    message = \"Insufficient verified shares\"\r",
            "                    self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "                else:\r",
            "                    warnings.warn(\r",
            "                        f\"Suboptimal number of verified zero shares for participant {p_id}. \"\r",
            "                        f\"Security diagnostics: {diagnostics}. \"\r",
            "                        f\"Proceeding with reduced security margin.\",\r",
            "                        SecurityWarning,\r",
            "                    )\r",
            "\r",
            "            # Optimized summation with constant-time operations to prevent timing attacks\r",
            "            sum_zero_shares: FieldElement = self._secure_sum_shares(\r",
            "                verified_zero_shares[p_id], self.field.prime\r",
            "            )\r",
            "\r",
            "            # Create new share with zero-knowledge consistency proof\r",
            "            new_y: FieldElement = (original_y + sum_zero_shares) % self.field.prime\r",
            "            new_shares[p_id] = (p_id, new_y)\r",
            "\r",
            "            # Generate proofs of correct share refreshing (optional)\r",
            "            if verified_count >= threshold:\r",
            "                # Only generate proofs when we have enough shares for full security\r",
            "                verification_proofs[p_id][\"consistency\"] = (\r",
            "                    self._generate_refresh_consistency_proof(\r",
            "                        p_id,\r",
            "                        original_y,\r",
            "                        sum_zero_shares,\r",
            "                        new_y,\r",
            "                        verified_zero_shares[p_id],\r",
            "                    )\r",
            "                )\r",
            "\r",
            "        # Add enhanced verification summary to verification_data\r",
            "        verification_summary: Dict[str, Any] = {\r",
            "            \"total_zero_shares_created\": len(zero_sharings) * len(participant_ids),\r",
            "            \"total_zero_shares_verified\": sum(\r",
            "                len(v) for v in verified_zero_shares.values()\r",
            "            ),\r",
            "            \"invalid_shares_detected\": invalid_shares_detected,\r",
            "            \"participants_with_full_verification\": sum(\r",
            "                1\r",
            "                for p_id in participant_ids\r",
            "                if len(verified_zero_shares[p_id]) == len(shares)\r",
            "            ),\r",
            "            \"potential_collusion_detected\": bool(potential_collusion),\r",
            "            \"byzantine_parties_excluded\": len(byzantine_parties),\r",
            "            \"byzantine_party_ids\": (\r",
            "                list(byzantine_parties.keys()) if byzantine_parties else []\r",
            "            ),\r",
            "            \"security_parameters\": {\r",
            "                \"min_verified_shares\": min_verified_shares,\r",
            "                \"security_factor\": security_factor,\r",
            "            },\r",
            "        }\r",
            "\r",
            "        # Step 3: Calculate the new commitments\r",
            "        # Extract x and y values from a subset of new shares for efficient reconstruction\r",
            "        sample_shares: List[SharePoint] = list(new_shares.values())[:threshold]\r",
            "        x_values: List[FieldElement] = [share[0] for share in sample_shares]\r",
            "        y_values: List[FieldElement] = [share[1] for share in sample_shares]\r",
            "\r",
            "        # Reconstruct the new polynomial coefficients via optimized interpolation\r",
            "        new_coeffs: List[FieldElement] = self._reconstruct_polynomial_coefficients(\r",
            "            x_values, y_values, threshold\r",
            "        )\r",
            "\r",
            "        # Create new commitments for these coefficients\r",
            "        new_commitments: CommitmentList = self.create_commitments(new_coeffs)\r",
            "\r",
            "        # Add the verification proofs and enhanced summary to the verification data\r",
            "        verification_data: Dict[str, Any] = {\r",
            "            \"original_shares_count\": len(shares),\r",
            "            \"threshold\": threshold,\r",
            "            \"zero_commitment_count\": len(zero_commitments),\r",
            "            \"timestamp\": int(time.time()),\r",
            "            \"protocol\": \"Enhanced-Chen-Lindell-PQ\",\r",
            "            \"verification_method\": \"batch-optimized\",\r",
            "            \"hash_based\": True,\r",
            "            \"verification_summary\": verification_summary,\r",
            "            \"seed_fingerprint\": hashlib.sha3_256(master_seed).hexdigest()[\r",
            "                :16\r",
            "            ],  # Fingerprint for verification\r",
            "            \"verification_proofs\": verification_proofs,\r",
            "        }\r",
            "\r",
            "        return new_shares, new_commitments, verification_data\r",
            "\r",
            "    def _secure_sum_shares(self, shares_dict: Dict[int, FieldElement], modulus: FieldElement) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Perform a secure constant-time summation of shares to prevent timing attacks.\r",
            "\r",
            "        Arguments:\r",
            "            shares_dict (dict): Dictionary of shares to sum.\r",
            "            modulus (int): The field modulus.\r",
            "\r",
            "        Inputs:\r",
            "            shares_dict: Dictionary of shares.\r",
            "            modulus: Modulus\r",
            "\r",
            "        Outputs:\r",
            "            int: Sum of shares modulo the field modulus.\r",
            "        \"\"\"\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        for _, value in sorted(\r",
            "            shares_dict.items()\r",
            "        ):  # Sort to ensure deterministic processing\r",
            "            result = (result + gmpy2.mpz(value)) % modulus\r",
            "        return int(result)\r",
            "\r",
            "    def _get_original_share_value(self, participant_id: int, shares: ShareDict) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Safely retrieve the original share value with proper validation.\r",
            "\r",
            "        Arguments:\r",
            "            participant_id (int): ID of the participant.\r",
            "            shares (dict): Dictionary of shares.\r",
            "\r",
            "        Inputs:\r",
            "            participant_id: Participant ID\r",
            "            shares: shares\r",
            "\r",
            "        Outputs:\r",
            "            int: Original y-value of the share.\r",
            "        \r",
            "        Raises:\r",
            "            SecurityError: If no valid original share is found for the participant.\r",
            "        \"\"\"\r",
            "        if participant_id in shares:\r",
            "            original_share: SharePoint = shares[participant_id]\r",
            "            # Validate the share structure\r",
            "            if isinstance(original_share, tuple) and len(original_share) == 2:\r",
            "                return original_share[1]\r",
            "\r",
            "        # Instead of returning 0, raise a security error to prevent silent failure\r",
            "        detailed_msg = f\"No valid original share found for participant {participant_id}.\"\r",
            "        message = \"Original share not found\"\r",
            "        self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "    def _determine_security_threshold(\r",
            "        self, base_threshold: int, verified_count: int, total_parties: int, invalid_parties: List[int]\r",
            "    ) -> int:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Determine the security threshold based on the current situation.\r",
            "\r",
            "            Uses an adaptive approach based on the number of invalid shares detected.\r",
            "\r",
            "        Arguments:\r",
            "            base_threshold (int): The base threshold value (t).\r",
            "            verified_count (int): Number of verified shares.\r",
            "            total_parties (int): Total number of participating parties.\r",
            "            invalid_parties (list): List of parties that provided invalid shares.\r",
            "\r",
            "        Inputs:\r",
            "            base_threshold: base threshold\r",
            "            verified_count: verified count\r",
            "            total_parties: total_parties\r",
            "            invalid_parties: invalid parties\r",
            "\r",
            "        Outputs:\r",
            "            int: The required threshold for secure operation.\r",
            "        \"\"\"\r",
            "        # Add explicit check for zero division with proper error handling\r",
            "        if total_parties <= 0:\r",
            "            detailed_msg = \"No participating parties available for threshold calculation\"\r",
            "            message = \"Invalid security parameters\"\r",
            "            self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "        \r",
            "        # Calculate the ratio of invalid to total parties\r",
            "        invalid_ratio: float = len(invalid_parties) / total_parties\r",
            "\r",
            "        required: int\r",
            "        if invalid_ratio > 0.25:\r",
            "            # High threat environment - increase security requirements\r",
            "            required = max(base_threshold, int(base_threshold * (1 + invalid_ratio)))\r",
            "        elif invalid_ratio > 0:\r",
            "            # Some threats detected - slight increase in requirements\r",
            "            required = base_threshold\r",
            "        else:\r",
            "            # No threats detected - can use standard threshold\r",
            "            required = base_threshold\r",
            "\r",
            "        # Never require more shares than are available\r",
            "        return min(required, total_parties)\r",
            "\r",
            "    def _detect_collusion_patterns(self, invalid_shares_detected: Dict[int, List[int]], party_ids: Set[int]) -> List[int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Detect potential collusion patterns among parties that provided invalid shares.\r",
            "\r",
            "        Arguments:\r",
            "            invalid_shares_detected (dict): Dictionary mapping participants to parties that gave them invalid shares.\r",
            "            party_ids (set): Set of all participating party IDs.\r",
            "\r",
            "        Inputs:\r",
            "            invalid_shares_detected: invalid_shares_detected\r",
            "            party_ids: party_ids\r",
            "\r",
            "        Outputs:\r",
            "            list: List of party IDs that might be colluding, or empty list if none detected.\r",
            "        \"\"\"\r",
            "        if not invalid_shares_detected:\r",
            "            return []\r",
            "\r",
            "        # Count how many times each party provided invalid shares\r",
            "        invalid_count: Dict[int, int] = {}\r",
            "        \r",
            "        for parties in invalid_shares_detected.values():\r",
            "            for party_id in parties:\r",
            "                invalid_count[party_id] = invalid_count.get(party_id, 0) + 1\r",
            "\r",
            "        # Calculate a suspicious threshold - parties that have more than 30% invalid shares\r",
            "        suspicious_threshold: float = 0.3 * len(invalid_shares_detected)\r",
            "        suspicious_parties: List[int] = [\r",
            "            party\r",
            "            for party, count in invalid_count.items()\r",
            "            if count > suspicious_threshold\r",
            "        ]\r",
            "\r",
            "        # Check for patterns indicating potential collusion\r",
            "        potential_colluders: List[int] = []\r",
            "\r",
            "        # If multiple suspicious parties targeted the same participants, they might be colluding\r",
            "        if len(suspicious_parties) > 1:\r",
            "            # Check for overlap in targeted participants\r",
            "            targeted_participants: Dict[int, Set[int]] = {}\r",
            "            participant_id: int\r",
            "            parties: List[int]\r",
            "            for participant_id, parties in invalid_shares_detected.items():\r",
            "                party_id: int\r",
            "                for party_id in parties:\r",
            "                    if party_id in suspicious_parties:\r",
            "                        if party_id not in targeted_participants:\r",
            "                            targeted_participants[party_id] = set()\r",
            "                        targeted_participants[party_id].add(participant_id)\r",
            "\r",
            "            # Look for significant overlap\r",
            "            p1: int\r",
            "            p2: int\r",
            "            for p1 in suspicious_parties:\r",
            "                for p2 in suspicious_parties:\r",
            "                    if (\r",
            "                        p1 < p2\r",
            "                        and p1 in targeted_participants\r",
            "                        and p2 in targeted_participants\r",
            "                    ):\r",
            "                        p1_targets: Set[int] = targeted_participants[p1]\r",
            "                        p2_targets: Set[int] = targeted_participants[p2]\r",
            "                        overlap: int = len(p1_targets.intersection(p2_targets))\r",
            "                        union: int = len(p1_targets.union(p2_targets))\r",
            "\r",
            "                        # If overlap ratio is high, add both to potential colluders\r",
            "                        if union > 0 and overlap / union > 0.7:\r",
            "                            if p1 not in potential_colluders:\r",
            "                                potential_colluders.append(p1)\r",
            "                            if p2 not in potential_colluders:\r",
            "                                potential_colluders.append(p2)\r",
            "\r",
            "        return potential_colluders\r",
            "\r",
            "    def _create_invalidity_proof(self, party_id: int, participant_id: int, share: SharePoint, commitments: CommitmentList) -> Dict[str, Any]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create a cryptographic proof that a share is invalid.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party that provided the invalid share.\r",
            "            participant_id (int): ID of the participant who received the share.\r",
            "            share (tuple): The invalid share (x, y).\r",
            "            commitments (list): The commitments against which the share was verified.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            participant_id: participant id\r",
            "            share: share\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            dict: A proof structure that can be verified by others.\r",
            "        \"\"\"\r",
            "        x: FieldElement\r",
            "        y: FieldElement\r",
            "        x, y = share\r",
            "\r",
            "        # Extract randomizers from commitments for hash-based verification\r",
            "        randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "\r",
            "        # Compute the combined randomizer for this point\r",
            "        r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "\r",
            "        # Compute the expected commitment\r",
            "        expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "        # Compute the actual commitment based on the share\r",
            "        actual_commitment: FieldElement = self._compute_hash_commitment(y, r_combined, x, \"verify\")\r",
            "\r",
            "        # Create a signature/timestamp for this proof\r",
            "        timestamp: int = int(time.time())\r",
            "        signature_input: bytes = self.group._enhanced_encode_for_hash(\r",
            "            party_id,\r",
            "            participant_id,\r",
            "            x,\r",
            "            y,\r",
            "            expected_commitment,\r",
            "            actual_commitment,\r",
            "            timestamp,\r",
            "            \"invalidity_proof\",\r",
            "        )\r",
            "\r",
            "        signature: str\r",
            "        if HAS_BLAKE3:\r",
            "            signature = blake3.blake3(signature_input).hexdigest()\r",
            "        else:\r",
            "            signature = hashlib.sha3_256(signature_input).hexdigest()\r",
            "\r",
            "        # Return the proof structure\r",
            "        return {\r",
            "            \"party_id\": party_id,\r",
            "            \"participant_id\": participant_id,\r",
            "            \"share_x\": int(x),\r",
            "            \"share_y\": int(y),\r",
            "            \"expected_commitment\": int(expected_commitment),\r",
            "            \"actual_commitment\": int(actual_commitment),\r",
            "            \"combined_randomizer\": int(r_combined),\r",
            "            \"timestamp\": timestamp,\r",
            "            \"signature\": signature,\r",
            "        }\r",
            "\r",
            "    def _generate_refresh_consistency_proof(\r",
            "        self, participant_id: int, original_y: FieldElement, sum_zero_shares: FieldElement, new_y: FieldElement, verified_shares: Dict[int, FieldElement]\r",
            "    ) -> Dict[str, Any]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate a proof that the share refreshing was done correctly.\r",
            "\r",
            "        Arguments:\r",
            "            participant_id (int): ID of the participant.\r",
            "            original_y (int): Original share value.\r",
            "            sum_zero_shares (int): Sum of the zero shares.\r",
            "            new_y (int): New share value.\r",
            "            verified_shares (dict): Dictionary of verified zero shares.\r",
            "        Inputs:\r",
            "            participant_id: participant id\r",
            "            original_y: original y\r",
            "            sum_zero_shares: sum of zero shares\r",
            "            new_y: new y\r",
            "            verified_shares: verified shares\r",
            "\r",
            "        Outputs:\r",
            "            dict: Proof structure for verification.\r",
            "        \"\"\"\r",
            "        # Create a fingerprint of all verified shares\r",
            "        share_fingerprint: str = hashlib.sha3_256(\r",
            "            str(sorted([(k, v) for k, v in verified_shares.items()])).encode()\r",
            "        ).hexdigest()\r",
            "\r",
            "        # Verify that new_y = original_y + sum_zero_shares mod prime\r",
            "        check_value: FieldElement = (original_y + sum_zero_shares) % self.field.prime\r",
            "\r",
            "        # Generate proof timestamp and signature\r",
            "        timestamp: int = int(time.time())\r",
            "        signature_input: bytes = self.group._enhanced_encode_for_hash(\r",
            "            participant_id,\r",
            "            original_y,\r",
            "            sum_zero_shares,\r",
            "            new_y,\r",
            "            share_fingerprint,\r",
            "            timestamp,\r",
            "            \"consistency_proof\",\r",
            "        )\r",
            "        signature: str\r",
            "        if HAS_BLAKE3:\r",
            "            signature = blake3.blake3(signature_input).hexdigest()\r",
            "        else:\r",
            "            signature = hashlib.sha3_256(signature_input).hexdigest()\r",
            "\r",
            "        # Return the proof structure\r",
            "        return {\r",
            "            \"participant_id\": participant_id,\r",
            "            \"calculated_sum\": int(sum_zero_shares),\r",
            "            \"verified_shares_count\": len(verified_shares),\r",
            "            \"shares_fingerprint\": share_fingerprint,\r",
            "            \"consistency_check\": check_value == new_y,\r",
            "            \"timestamp\": timestamp,\r",
            "            \"signature\": signature,\r",
            "        }\r",
            "\r",
            "    def _process_echo_consistency(\r",
            "        self, zero_commitments: Dict[int, CommitmentList], zero_sharings: Dict[int, ShareDict], participant_ids: List[int]\r",
            "    ) -> Dict[Tuple[int, int], bool]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced echo consistency protocol for Byzantine fault detection.\r",
            "\r",
            "            This implementation provides stronger detection of equivocation (sending different\r",
            "            values to different participants) through secure cryptographic fingerprinting\r",
            "            and comprehensive evidence collection.\r",
            "\r",
            "        Arguments:\r",
            "            zero_commitments (dict): Dictionary of commitments from each party.\r",
            "            zero_sharings (dict): Dictionary of sharings from each party.\r",
            "            participant_ids (list): List of participant IDs.\r",
            "\r",
            "        Inputs:\r",
            "            zero_commitments: Commitments\r",
            "            zero_sharings: Sharings\r",
            "            participant_ids: Participant IDs\r",
            "\r",
            "        Outputs:\r",
            "            dict: Dictionary mapping (party_id, participant_id) to consistency result.\r",
            "            \r",
            "        Side Effects:\r",
            "            Stores Byzantine evidence in self._byzantine_evidence for later access by _detect_byzantine_behavior.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or structures.\r",
            "        \"\"\"\r",
            "\r",
            "        # Validate input parameter types\r",
            "        if not isinstance(zero_commitments, dict):\r",
            "            raise TypeError(\"zero_commitments must be a dictionary\")\r",
            "        if not isinstance(zero_sharings, dict):\r",
            "            raise TypeError(\"zero_sharings must be a dictionary\")\r",
            "        if not isinstance(participant_ids, list):\r",
            "            raise TypeError(\"participant_ids must be a list\")\r",
            "\r",
            "        # Validate the structure of zero_sharings\r",
            "        \r",
            "        party_id: int\r",
            "        party_shares: ShareDict\r",
            "        for party_id, party_shares in zero_sharings.items():\r",
            "            if not isinstance(party_shares, dict):\r",
            "                detailed_msg = (\r",
            "                    f\"Invalid share format for party {party_id}: expected dictionary\"\r",
            "                )\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "            p_id: int\r",
            "            share: SharePoint\r",
            "            for p_id, share in party_shares.items():\r",
            "                if not isinstance(share, tuple) or len(share) != 2:\r",
            "                    detailed_msg = f\"Invalid share from party {party_id} to participant {p_id}: expected (x, y) tuple\"\r",
            "                    message = \"Invalid data structure\"\r",
            "                    self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "\r",
            "        # Validate the structure of zero_commitments\r",
            "        \r",
            "        party_id: int\r",
            "        commitments: CommitmentList\r",
            "        for party_id, commitments in zero_commitments.items():\r",
            "            if not isinstance(commitments, list) or not commitments:\r",
            "                detailed_msg = f\"Invalid commitment format for party {party_id}: expected non-empty list\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "            if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "                detailed_msg = f\"Invalid commitment format for party {party_id}: expected list of (commitment, randomizer) tuples\"\r",
            "                message = \"Invalid data structure\"\r",
            "                self._raise_sanitized_error(TypeError, message, detailed_msg)\r",
            "\r",
            "        consistency_results: Dict[Tuple[int, int], bool] = {}\r",
            "\r",
            "        # Create cryptographically secure fingerprints of each sharing\r",
            "        share_fingerprints: Dict[int, Dict[int, bytes]] = {}\r",
            "\r",
            "        \r",
            "        party_id: int\r",
            "        party_shares: ShareDict\r",
            "        for party_id, party_shares in zero_sharings.items():\r",
            "            share_fingerprints[party_id] = {}\r",
            "            \r",
            "            p_id: int\r",
            "            x: FieldElement\r",
            "            y: FieldElement\r",
            "            for p_id, (x, y) in party_shares.items():\r",
            "                if p_id in participant_ids:\r",
            "                    # Create a secure fingerprint using proper domain separation\r",
            "                    message: bytes = self.group._enhanced_encode_for_hash(\r",
            "                        party_id, p_id, x, y, \"echo-consistency-check\"\r",
            "                    )\r",
            "                    fingerprint: bytes = self.hash_algorithm(message).digest()\r",
            "                    share_fingerprints[party_id][p_id] = fingerprint\r",
            "\r",
            "        # Echo broadcast phase: participants share what they received\r",
            "        echo_broadcasts: Dict[int, Dict[int, Tuple[SharePoint, bytes]]] = {}\r",
            "        p_id: int\r",
            "        for p_id in participant_ids:\r",
            "            echo_broadcasts[p_id] = {}\r",
            "            # Collect all shares this participant received\r",
            "            \r",
            "            party_id: int\r",
            "            for party_id in zero_sharings:\r",
            "                if p_id in zero_sharings[party_id]:\r",
            "                    share: SharePoint = zero_sharings[party_id][p_id]\r",
            "                    fingerprint: Optional[bytes] = share_fingerprints[party_id].get(p_id)\r",
            "                    if fingerprint:\r",
            "                        echo_broadcasts[p_id][party_id] = (share, fingerprint)\r",
            "\r",
            "        # Consistency check phase: compare what different participants received\r",
            "        byzantine_evidence: Dict[int, Dict[str, Any]] = {}\r",
            "        \r",
            "        p1_id: int\r",
            "        for p1_id in participant_ids:\r",
            "            \r",
            "            p2_id: int\r",
            "            for p2_id in participant_ids:\r",
            "                if p1_id >= p2_id:  # Only check each pair once\r",
            "                    continue\r",
            "\r",
            "                # Compare what p1 and p2 received from each party\r",
            "                \r",
            "                party_id: int\r",
            "                for party_id in zero_sharings:\r",
            "                    if (\r",
            "                        party_id in echo_broadcasts[p1_id]\r",
            "                        and party_id in echo_broadcasts[p2_id]\r",
            "                    ):\r",
            "\r",
            "                        # Extract shares and fingerprints\r",
            "                        p1_share: SharePoint\r",
            "                        p1_fingerprint: bytes\r",
            "                        p2_share: SharePoint\r",
            "                        p2_fingerprint: bytes\r",
            "                        (p1_share, p1_fingerprint) = echo_broadcasts[p1_id][party_id]\r",
            "                        (p2_share, p2_fingerprint) = echo_broadcasts[p2_id][party_id]\r",
            "\r",
            "                        # Check if party sent consistent values to both participants\r",
            "                        is_consistent: bool = p1_fingerprint == p2_fingerprint\r",
            "\r",
            "                        # Record consistency results for both participants\r",
            "                        consistency_results[(party_id, p1_id)] = is_consistent\r",
            "                        consistency_results[(party_id, p2_id)] = is_consistent\r",
            "\r",
            "                        # If inconsistent, collect evidence of Byzantine behavior\r",
            "                        if not is_consistent:\r",
            "                            if party_id not in byzantine_evidence:\r",
            "                                byzantine_evidence[party_id] = {\r",
            "                                    \"type\": \"equivocation\",\r",
            "                                    \"evidence\": [],\r",
            "                                }\r",
            "\r",
            "                            byzantine_evidence[party_id][\"evidence\"].append(\r",
            "                                {\r",
            "                                    \"participant1\": p1_id,\r",
            "                                    \"share1\": p1_share,\r",
            "                                    \"participant2\": p2_id,\r",
            "                                    \"share2\": p2_share,\r",
            "                                    \"fingerprint1\": p1_fingerprint.hex(),\r",
            "                                    \"fingerprint2\": p2_fingerprint.hex(),\r",
            "                                }\r",
            "                            )\r",
            "\r",
            "        # Store Byzantine evidence in a separate field rather than modifying the\r",
            "        # return structure to maintain compatibility with existing code\r",
            "        self._byzantine_evidence = byzantine_evidence\r",
            "\r",
            "        return consistency_results\r",
            "\r",
            "    def _calculate_optimal_batch_size(self, num_participants: int, security_level: int = None, num_shares: int = None) -> int:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "        Calculate the optimal batch size for verification based on system parameters.\r",
            "\r",
            "        Arguments:\r",
            "            num_participants (int): Number of participants.\r",
            "            security_level (int, optional): Security intensity level (0-10). Higher values result\r",
            "                                           in smaller batches for more granular verification.\r",
            "                                           Default is None (use standard calculation).\r",
            "            num_shares (int, optional): Total number of shares in the system, allowing for\r",
            "                                        more nuanced batch sizing when share distribution is uneven.\r",
            "\r",
            "        Inputs:\r",
            "            num_participants: num_participants\r",
            "            security_level: Optional parameter to adjust batch size based on security requirements\r",
            "            num_shares: Optional parameter to adjust for uneven share distribution\r",
            "\r",
            "        Outputs:\r",
            "            int: Optimal batch size for verification.\r",
            "        \"\"\"\r",
            "        # Validate security_level input\r",
            "        if security_level is not None:\r",
            "            if not isinstance(security_level, (int, float)) or security_level < 0 or security_level > 10:\r",
            "                warnings.warn(\"Invalid security_level (must be 0-10). Using default calculation.\", RuntimeWarning)\r",
            "                security_level = None\r",
            "        \r",
            "        # Validate num_shares input if provided\r",
            "        if num_shares is not None:\r",
            "            if not isinstance(num_shares, int) or num_shares <= 0:\r",
            "                warnings.warn(\"Invalid num_shares (must be a positive integer). Ignoring this parameter.\", RuntimeWarning)\r",
            "                num_shares = None\r",
            "                \r",
            "        # For small numbers, use a smaller batch size\r",
            "        if (num_participants < 10):\r",
            "            base_batch_size = min(8, num_participants)\r",
            "            # Even with small participants, adjust for highly uneven share distribution\r",
            "            if num_shares is not None and num_shares > num_participants * 10:\r",
            "                return max(2, int(base_batch_size / 2))  # More conservative reduction for small systems\r",
            "            return base_batch_size\r",
            "                \r",
            "        # Apply security level adjustment if specified\r",
            "        adjustment_factor = 1.0\r",
            "        if security_level is not None:\r",
            "            # Convert security level (0-10) to a reduction factor (1.0 to 0.4)\r",
            "            # Higher security = smaller batches for more granular verification\r",
            "            adjustment_factor = max(0.4, 1.0 - (security_level / 15))\r",
            "        \r",
            "        # For larger systems, use a batch size that balances efficiency\r",
            "        # with the ability to quickly identify problematic shares\r",
            "        cpu_count: int = 1\r",
            "        try:\r",
            "            import multiprocessing\r",
            "            import math\r",
            "            cpu_count = max(1, multiprocessing.cpu_count())\r",
            "            \r",
            "            # Hybrid approach: Consider both logarithmic scaling and CPU count\r",
            "            # Ensure minimum reasonable batch size with the max(4, ...) operation\r",
            "            log_factor = max(4, int(math.log2(max(2, num_participants)) * 4 * adjustment_factor))\r",
            "            cpu_factor = max(8, int(num_participants // cpu_count * adjustment_factor))\r",
            "            \r",
            "            # Use the smaller of the two factors to keep batches manageable\r",
            "            batch_size = min(32, min(log_factor, cpu_factor))\r",
            "            \r",
            "            # If num_shares is provided, adjust for highly skewed distributions\r",
            "            if num_shares is not None and num_shares > num_participants:\r",
            "                shares_per_participant = num_shares / max(1, num_participants)\r",
            "                if shares_per_participant > 10:  # Only adjust for highly uneven distributions\r",
            "                    # Use logarithmic scaling to avoid extreme reductions\r",
            "                    reduction_factor = min(3, math.log2(shares_per_participant) / 4)  # Cap the reduction\r",
            "                    batch_size = max(4, int(batch_size / reduction_factor))\r",
            "                    \r",
            "            return batch_size\r",
            "        except (ImportError, NotImplementedError):\r",
            "            pass\r",
            "\r",
            "        # Fallback to the original calculation with security adjustment\r",
            "        batch_size = min(32, max(8, int(num_participants // max(1, cpu_count) * adjustment_factor)))\r",
            "        \r",
            "        # Apply the share distribution adjustment to the fallback case as well\r",
            "        if num_shares is not None and num_shares > num_participants:\r",
            "            shares_per_participant = num_shares / max(1, num_participants)\r",
            "            if shares_per_participant > 10:\r",
            "                import math  # Import here in case it wasn't imported earlier\r",
            "                reduction_factor = min(3, math.log2(shares_per_participant) / 4)\r",
            "                batch_size = max(4, int(batch_size / reduction_factor))\r",
            "                \r",
            "        return batch_size\r",
            "\r",
            "    def _prepare_verification_batches(\r",
            "        self, zero_sharings: Dict[int, ShareDict], zero_commitments: Dict[int, CommitmentList], participant_ids: List[int], batch_size: int\r",
            "    ) -> List[List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Prepare efficient verification batches grouped by commitment set.\r",
            "\r",
            "        Arguments:\r",
            "            zero_sharings (dict): Dictionary of sharings from each party.\r",
            "            zero_commitments (dict): Dictionary of commitments from each party.\r",
            "            participant_ids (list): List of participant IDs.\r",
            "            batch_size (int): Size of each batch.\r",
            "\r",
            "        Inputs:\r",
            "            zero_sharings: zero_sharings\r",
            "            zero_commitments: zero_commitments\r",
            "            participant_ids: participant_ids\r",
            "            batch_size: batch_size\r",
            "\r",
            "        Outputs:\r",
            "            list: List of verification batches.\r",
            "        \"\"\"\r",
            "        verification_batches: List[List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]] = []\r",
            "\r",
            "        # Group shares by commitment set for efficient batch verification\r",
            "        commitment_groups: Dict[str, Tuple[CommitmentList, List[Tuple[int, int, FieldElement, FieldElement]]]] = {}\r",
            "        \r",
            "        party_id: int\r",
            "        party_commitments: CommitmentList\r",
            "        for party_id, party_commitments in zero_commitments.items():\r",
            "            # Use a cryptographic hash instead of Python's non-cryptographic hash()\r",
            "            # This prevents potential hash collisions that could lead to incorrect grouping\r",
            "            if HAS_BLAKE3:\r",
            "                hasher = blake3.blake3()\r",
            "            else:\r",
            "                # Fall back to SHA3-256 if BLAKE3 is not available\r",
            "                import hashlib\r",
            "                hasher = hashlib.sha3_256()\r",
            "                \r",
            "            # Generate a stable, cryptographically secure commitment key by hashing all commitment values\r",
            "            for c in party_commitments:\r",
            "                if isinstance(c, tuple) and len(c) > 0:\r",
            "                    # Convert the commitment value to bytes for hashing\r",
            "                    hasher.update(str(c[0]).encode('utf-8'))\r",
            "                    \r",
            "            commitment_key: str = hasher.hexdigest()\r",
            "\r",
            "            if commitment_key not in commitment_groups:\r",
            "                commitment_groups[commitment_key] = (party_commitments, [])\r",
            "\r",
            "            # Fixed: Use zero_sharings[party_id] instead of undefined party_shares\r",
            "            if party_id in zero_sharings:\r",
            "                p_id: int\r",
            "                x: FieldElement\r",
            "                y: FieldElement\r",
            "                for p_id, (x, y) in zero_sharings[party_id].items():\r",
            "                    if p_id in participant_ids:\r",
            "                        commitment_groups[commitment_key][1].append((party_id, p_id, x, y))\r",
            "\r",
            "        # Create batches with optimized size\r",
            "        \r",
            "        commitment_key: str\r",
            "        commitments: CommitmentList\r",
            "        items: List[Tuple[int, int, FieldElement, FieldElement]]\r",
            "        for commitment_key, (commitments, items) in commitment_groups.items():\r",
            "            i: int\r",
            "            for i in range(0, len(items), batch_size):\r",
            "                batch: List[Tuple[int, int, FieldElement, FieldElement]] = items[i : i + batch_size]\r",
            "                batch_items: List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]] = [\r",
            "                    (party_id, p_id, x, y, commitments)\r",
            "                    for party_id, p_id, x, y in batch\r",
            "                ]\r",
            "                verification_batches.append(batch_items)\r",
            "\r",
            "        return verification_batches\r",
            "\r",
            "    def _process_verification_batches(self, verification_batches: List[List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]]) -> List[Tuple[Tuple[int, int], bool]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Process verification batches with optimized parallelism.\r",
            "\r",
            "        Arguments:\r",
            "            verification_batches (list): List of verification batches.\r",
            "\r",
            "        Inputs:\r",
            "            verification_batches: verification_batches\r",
            "\r",
            "        Outputs:\r",
            "            list: List of verification results.\r",
            "        \"\"\"\r",
            "\r",
            "        def verify_batch(batch_items: List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]) ->  List[Tuple[Tuple[int, int], bool]]:\r",
            "            results: Dict[int, Tuple[int, int]] = {}\r",
            "            batch_shares: List[SharePoint] = []\r",
            "            \r",
            "            idx: int\r",
            "            party_id: int\r",
            "            p_id: int\r",
            "            x: FieldElement\r",
            "            y: FieldElement\r",
            "            commitments: CommitmentList\r",
            "            for idx, (party_id, p_id, x, y, commitments) in enumerate(batch_items):\r",
            "                batch_shares.append((x, y))\r",
            "                results[idx] = (party_id, p_id)\r",
            "\r",
            "            # Use batch verification when possible\r",
            "            verification_results: Dict[int, bool]\r",
            "            _: bool\r",
            "            if len(batch_shares) > 1:\r",
            "                _, verification_results = self.batch_verify_shares(\r",
            "                    batch_shares, commitments\r",
            "                )\r",
            "                return [\r",
            "                    (results[idx], is_valid)\r",
            "                    for idx, is_valid in verification_results.items()\r",
            "                ]\r",
            "            else:\r",
            "                # Fallback to individual verification\r",
            "                return [\r",
            "                    (results[idx], self.verify_share(x, y, commitments))\r",
            "                    for idx, (party_id, p_id, x, y, commitments) in enumerate(\r",
            "                        batch_items\r",
            "                    )\r",
            "                ]\r",
            "\r",
            "        # Try parallel verification with improved error handling\r",
            "        verification_results: List[Tuple[Tuple[int, int], bool]] = []\r",
            "        try:\r",
            "            import concurrent.futures\r",
            "\r",
            "            with concurrent.futures.ThreadPoolExecutor() as executor:\r",
            "                # Use a more robust approach for gathering results\r",
            "                future_to_batch: Dict[\"concurrent.futures.Future[List[Tuple[Tuple[int, int], bool]]]\", int] = {\r",
            "                    executor.submit(verify_batch, batch): i\r",
            "                    for i, batch in enumerate(verification_batches)\r",
            "                }\r",
            "\r",
            "                \r",
            "                future: \"concurrent.futures.Future[List[Tuple[Tuple[int, int], bool]]]\"\r",
            "                for future in concurrent.futures.as_completed(future_to_batch):\r",
            "                    try:\r",
            "                        batch_results:  List[Tuple[Tuple[int, int], bool]] = future.result()\r",
            "                        verification_results.extend(batch_results)\r",
            "                    except Exception as e:\r",
            "                        warnings.warn(\r",
            "                            f\"Error in verification batch: {e}\", RuntimeWarning\r",
            "                        )\r",
            "                        # Handle failed batch verification by marking all shares in the batch as invalid\r",
            "                        batch_index = future_to_batch.get(future, -1)\r",
            "                        if 0 <= batch_index < len(verification_batches):\r",
            "                            # Extract party_id and p_id from the failed batch and mark all as invalid\r",
            "                            failed_batch = verification_batches[batch_index]\r",
            "                            invalid_results = [\r",
            "                                ((party_id, p_id), False)\r",
            "                                for party_id, p_id, _, _, _ in failed_batch\r",
            "                            ]\r",
            "                            verification_results.extend(invalid_results)\r",
            "                            \r",
            "                            # Enhanced security warning with more detail for monitoring systems\r",
            "                            fail_msg = (f\"CRITICAL SECURITY ALERT: Batch verification failure detected. \"\r",
            "                                      f\"{len(invalid_results)} shares marked as invalid in batch {batch_index}. \"\r",
            "                                      f\"This could indicate a potential attack or data corruption.\")\r",
            "                            warnings.warn(fail_msg, category=SecurityWarning)\r",
            "                            \r",
            "                            # Log the detailed error for forensic analysis\r",
            "                            if logging:\r",
            "                                try:\r",
            "                                    logging.error(f\"{fail_msg} Original error: {e}\")\r",
            "                                except (ImportError, NameError):\r",
            "                                    pass  # If logging is not available, continue silently\r",
            "        except (ImportError, RuntimeError):\r",
            "            # Fallback to sequential verification with progress tracking\r",
            "            \r",
            "            batch:  List[Tuple[int, int, FieldElement, FieldElement, CommitmentList]]\r",
            "            for batch in verification_batches:\r",
            "                verification_results.extend(verify_batch(batch))\r",
            "\r",
            "        return verification_results\r",
            "\r",
            "    def _get_share_value_from_results(self, party_id: int, p_id: int, zero_sharings:  Dict[int, ShareDict]) -> Optional[FieldElement]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Get share value from zero sharings with proper validation.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party.\r",
            "            p_id (int): ID of the participant.\r",
            "            zero_sharings (dict): Dictionary of sharings.\r",
            "        Inputs:\r",
            "            party_id: party_id\r",
            "            p_id: p_id\r",
            "            zero_sharings: zero_sharings\r",
            "        Outputs:\r",
            "            Optional[int]: Share y-value or None if not found.\r",
            "        \"\"\"\r",
            "        if party_id in zero_sharings and p_id in zero_sharings[party_id]:\r",
            "            return zero_sharings[party_id][p_id][1]  # Return y-value\r",
            "\r",
            "        # This should not happen if verification passed\r",
            "        warnings.warn(\r",
            "            f\"Missing share for party {party_id}, participant {p_id}\", RuntimeWarning\r",
            "        )\r",
            "        return None  # Return None instead of 0 to avoid silent failures\r",
            "\r",
            "    def _generate_invalidity_evidence(\r",
            "        self,\r",
            "        party_id: int,\r",
            "        p_id: int,\r",
            "        zero_sharings: Dict[int, ShareDict],\r",
            "        zero_commitments: Dict[int, CommitmentList],\r",
            "        verification_proofs: Dict[int, Dict[int, Any]],\r",
            "        share_verification: bool,\r",
            "        echo_consistency: bool,\r",
            "    ) -> None:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Generate enhanced cryptographic evidence for invalid shares.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party providing the share.\r",
            "            p_id (int): ID of the participant receiving the share.\r",
            "            zero_sharings (dict): Dictionary of sharings.\r",
            "            zero_commitments (dict): Dictionary of commitments.\r",
            "            verification_proofs (dict): Dictionary to store proofs.\r",
            "            share_verification (bool): Whether share verification passed.\r",
            "            echo_consistency (bool): Whether echo consistency check passed.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            p_id: p_id\r",
            "            zero_sharings: zero_sharings\r",
            "            zero_commitments: zero_commitments\r",
            "            verification_proofs: verification_proofs\r",
            "            share_verification: share verification\r",
            "            echo consistency: echo consistency\r",
            "\r",
            "        Outputs:\r",
            "            None\r",
            "        \"\"\"\r",
            "        try:\r",
            "            if p_id not in verification_proofs:\r",
            "                verification_proofs[p_id] = {}\r",
            "\r",
            "            # Get the share for detailed evidence\r",
            "            if party_id in zero_sharings and p_id in zero_sharings[party_id]:\r",
            "                share: SharePoint = zero_sharings[party_id][p_id]\r",
            "                commitments: Optional[CommitmentList] = zero_commitments.get(party_id)\r",
            "\r",
            "                if commitments:\r",
            "                    # Create comprehensive proof with additional evidence\r",
            "                    proof: Dict[str, Any] = self._create_invalidity_proof(\r",
            "                        party_id, p_id, share, commitments\r",
            "                    )\r",
            "\r",
            "                    # Add additional evidence about consistency checks\r",
            "                    proof[\"echo_consistency\"] = echo_consistency\r",
            "                    proof[\"share_verification\"] = share_verification\r",
            "\r",
            "                    # Add to verification proofs\r",
            "                    verification_proofs[p_id][party_id] = proof\r",
            "\r",
            "            # Log the issue for security monitoring\r",
            "            warnings.warn(\r",
            "                f\"Invalid share from party {party_id} for participant {p_id}. \"\r",
            "                f\"Verification: {share_verification}, Echo consistency: {echo_consistency}\",\r",
            "                SecurityWarning,\r",
            "            )\r",
            "        except Exception as e:\r",
            "            warnings.warn(f\"Failed to create invalidity proof: {e}\", RuntimeWarning)\r",
            "\r",
            "    def _enhanced_collusion_detection(\r",
            "        self, invalid_shares_detected: Dict[int, List[int]], party_ids: Set[int], echo_consistency: Dict[Tuple[int, int], bool]\r",
            "    ) -> List[int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced collusion detection with improved graph analysis.\r",
            "\r",
            "        Arguments:\r",
            "            invalid_shares_detected (dict): Dictionary of invalid shares.\r",
            "            party_ids (set): Set of party IDs.\r",
            "            echo_consistency (dict): Results of echo consistency checks.\r",
            "\r",
            "        Inputs:\r",
            "            invalid_shares_detected: invalid shares detected\r",
            "            party_ids: party ids\r",
            "            echo_consistency: echo consistency\r",
            "\r",
            "        Outputs:\r",
            "            list: List of potentially colluding parties.\r",
            "        \"\"\"\r",
            "        if not invalid_shares_detected:\r",
            "            return []\r",
            "\r",
            "        # Count how many times each party provided invalid shares\r",
            "        invalid_count: Dict[int, int] = {}\r",
            "        \r",
            "        parties: List[int]\r",
            "        for parties in invalid_shares_detected.values():\r",
            "            party_id: int\r",
            "            for party_id in parties:\r",
            "                invalid_count[party_id] = invalid_count.get(party_id, 0) + 1\r",
            "\r",
            "        # Calculate a suspicious threshold with dynamic adjustment\r",
            "        total_participants: int = len(invalid_shares_detected)\r",
            "        suspicious_threshold: float = max(1, 0.25 * total_participants)\r",
            "\r",
            "        # Identify suspicious parties with high invalid share counts\r",
            "        suspicious_parties: List[int] = [\r",
            "            party\r",
            "            for party, count in invalid_count.items()\r",
            "            if count > suspicious_threshold\r",
            "        ]\r",
            "\r",
            "        # Enhanced detection: look for patterns in echo consistency failures\r",
            "        if echo_consistency:\r",
            "            inconsistent_parties: Set[int] = set()\r",
            "            \r",
            "            party_id: int\r",
            "            is_consistent: bool\r",
            "            for (party_id, _), is_consistent in echo_consistency.items():\r",
            "                if not is_consistent and party_id not in inconsistent_parties:\r",
            "                    inconsistent_parties.add(party_id)\r",
            "\r",
            "            # Add parties with echo inconsistencies to suspicious list\r",
            "            party: int\r",
            "            for party in inconsistent_parties:\r",
            "                if party not in suspicious_parties:\r",
            "                    suspicious_parties.append(party)\r",
            "\r",
            "        # Identify potential collusion patterns\r",
            "        potential_colluders: List[int] = []\r",
            "\r",
            "        # Check for targeting patterns (multiple suspicious parties targeting the same participants)\r",
            "        if len(suspicious_parties) > 1:\r",
            "            targeted_participants: Dict[int, Set[int]] = {}\r",
            "            \r",
            "            party_id: int\r",
            "            for party_id in suspicious_parties:\r",
            "                targeted_participants[party_id] = set()\r",
            "                \r",
            "                p_id: int\r",
            "                parties: List[int]\r",
            "                for p_id, parties in invalid_shares_detected.items():\r",
            "                    if party_id in parties:\r",
            "                        targeted_participants[party_id].add(p_id)\r",
            "\r",
            "            # Find parties with similar targeting patterns\r",
            "            i: int\r",
            "            p1: int\r",
            "            for i, p1 in enumerate(suspicious_parties):\r",
            "                \r",
            "                p2: int\r",
            "                for p2 in suspicious_parties[i + 1 :]:\r",
            "                    if p1 in targeted_participants and p2 in targeted_participants:\r",
            "                        p1_targets: Set[int] = targeted_participants[p1]\r",
            "                        p2_targets: Set[int] = targeted_participants[p2]\r",
            "\r",
            "                        # Calculate Jaccard similarity of target sets\r",
            "                        if p1_targets and p2_targets:\r",
            "                            overlap: int = len(p1_targets.intersection(p2_targets))\r",
            "                            union: int = len(p1_targets.union(p2_targets))\r",
            "\r",
            "                            # Higher threshold (0.8) for stronger evidence\r",
            "                            if union > 0 and overlap / union > 0.8:\r",
            "                                if p1 not in potential_colluders:\r",
            "                                    potential_colluders.append(p1)\r",
            "                                if p2 not in potential_colluders:\r",
            "                                    potential_colluders.append(p2)\r",
            "\r",
            "        return potential_colluders\r",
            "\r",
            "    def create_polynomial_proof(self, coefficients: List[FieldElement], commitments: CommitmentList) -> ProofDict:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Creates a zero-knowledge proof of knowledge of the polynomial coefficients\r",
            "            using hash-based commitments for post-quantum security.\r",
            "\r",
            "            This implementation follows Baghery's secure framework with enhanced domain\r",
            "            separation and proper randomization to ensure security against quantum attacks.\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "            commitments (list): Commitments to these coefficients (list of tuples).\r",
            "\r",
            "        Inputs:\r",
            "            coefficients: coefficients\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            dict: Proof data structure containing the necessary components for verification.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types or structures.\r",
            "            ValueError: If coefficients or commitments lists are empty.\r",
            "        \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(coefficients, list):\r",
            "            raise TypeError(\"coefficients must be a list\")\r",
            "        if not coefficients:\r",
            "            raise ValueError(\"coefficients list cannot be empty\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            raise ValueError(\"commitments list cannot be empty\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Convert coefficients to integers for consistent arithmetic\r",
            "        coeffs_int: List[FieldElement] = [gmpy2.mpz(coeff) % self.field.prime for coeff in coefficients]\r",
            "\r",
            "        # Generate secure random blinding factors\r",
            "        blindings: List[FieldElement] = [self.group.secure_random_element() for _ in range(len(coeffs_int))]\r",
            "\r",
            "        # Create hash-based commitments to blinding factors with domain separation\r",
            "        blinding_commitments: List[Tuple[FieldElement, FieldElement]] = []\r",
            "        i: int\r",
            "        b: FieldElement\r",
            "        for i, b in enumerate(blindings):\r",
            "            # Generate secure randomizer for each blinding factor\r",
            "            r_b: FieldElement = self.group.secure_random_element()\r",
            "\r",
            "            # Compute hash-based commitment with context for domain separation\r",
            "            commitment: FieldElement = self._compute_hash_commitment(\r",
            "                b, r_b, i, \"polynomial_proof_blinding\"\r",
            "            )\r",
            "            blinding_commitments.append((commitment, r_b))\r",
            "\r",
            "        # Generate timestamp for the proof\r",
            "        timestamp: int = int(time.time())\r",
            "        \r",
            "        # Generate non-interactive challenge using Fiat-Shamir transform with enhanced encoding\r",
            "        # Include all public values in the challenge computation to prevent manipulation\r",
            "        challenge_input: bytes = self.group._enhanced_encode_for_hash(\r",
            "            \"polynomial_proof\",  # Domain separator\r",
            "            self.generator,\r",
            "            self.group.prime,\r",
            "            [c[0] if isinstance(c, tuple) and len(c) > 0 else 0 for c in commitments],  # Commitment values, safely accessed\r",
            "            [bc[0] if isinstance(bc, tuple) and len(bc) > 0 else 0 for bc in blinding_commitments],  # Blinding commitment values, safely accessed\r",
            "            timestamp,  # Use the same timestamp that will be stored in the proof\r",
            "        )\r",
            "\r",
            "        # Hash the challenge input using the configured hash algorithm\r",
            "        challenge_hash: bytes = self.hash_algorithm(challenge_input).digest()\r",
            "        challenge: FieldElement = int.from_bytes(challenge_hash, \"big\") % self.field.prime\r",
            "\r",
            "        # Compute responses using sensitive coefficients - this should be constant-time\r",
            "        responses: List[FieldElement] = [\r",
            "            (b + challenge * a) % self.field.prime\r",
            "            for b, a in zip(blindings, coeffs_int)\r",
            "        ]\r",
            "\r",
            "        # Safely extract commitment randomizers regardless of tuple length\r",
            "        commitment_randomizers: List[FieldElement] = []\r",
            "        for c in commitments:\r",
            "            if len(c) >= 2:\r",
            "                commitment_randomizers.append(c[1])\r",
            "            else:\r",
            "                raise ValueError(\"Each commitment must contain at least two elements (commitment, randomizer)\")\r",
            "\r",
            "        # Return complete proof structure including all values needed for verification\r",
            "        proof: ProofDict = {\r",
            "            \"blinding_commitments\": blinding_commitments,\r",
            "            \"challenge\": int(challenge),\r",
            "            \"responses\": [int(r) for r in responses],\r",
            "            \"commitment_randomizers\": [int(r) for r in commitment_randomizers],\r",
            "            \"blinding_randomizers\": [int(r) for _, r in blinding_commitments],\r",
            "            \"timestamp\": timestamp,  # Store timestamp for verification\r",
            "        }\r",
            "        return proof\r",
            "    \r",
            "    def verify_polynomial_proof(self, proof: ProofDict, commitments: CommitmentList) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verifies a zero-knowledge proof of knowledge of polynomial coefficients\r",
            "            using hash-based commitment verification for post-quantum security.\r",
            "\r",
            "            This method validates that the prover knows the coefficients without revealing them,\r",
            "            using only the hash-based commitments and the provided proof.\r",
            "\r",
            "        Arguments:\r",
            "            proof (dict): Proof data structure from create_polynomial_proof.\r",
            "            commitments (list): Commitments to the polynomial coefficients (list of tuples).\r",
            "\r",
            "        Inputs:\r",
            "            proof: proof\r",
            "            commitments: commitments\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if verification succeeds, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If commitments list is empty or proof structure is invalid.\r",
            "    \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(proof, dict):\r",
            "            raise TypeError(\"proof must be a dictionary\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            raise ValueError(\"commitments list cannot be empty\")\r",
            "\r",
            "        # Extract proof components with parameter validation\r",
            "        blinding_commitments: List[Tuple[FieldElement, FieldElement]]\r",
            "        challenge: FieldElement\r",
            "        responses: List[FieldElement]\r",
            "        commitment_randomizers: List[FieldElement]\r",
            "        blinding_randomizers: List[FieldElement]\r",
            "        timestamp: int\r",
            "\r",
            "        try:\r",
            "            blinding_commitments = proof[\"blinding_commitments\"]\r",
            "            challenge = proof[\"challenge\"]\r",
            "            responses = proof[\"responses\"]\r",
            "            commitment_randomizers = proof[\"commitment_randomizers\"]\r",
            "            blinding_randomizers = proof[\"blinding_randomizers\"]\r",
            "            timestamp = proof.get(\"timestamp\")  # Get timestamp for challenge reconstruction\r",
            "        except (KeyError, TypeError) as e:\r",
            "            raise ValueError(f\"Incomplete or malformed proof structure: {str(e)}\")\r",
            "\r",
            "        # Enhanced validation for proof structure - changed from warnings to exceptions for security-critical failures\r",
            "        if not isinstance(blinding_commitments, list):\r",
            "            raise ValueError(\"blinding_commitments must be a list\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in blinding_commitments):\r",
            "            raise ValueError(\"Each blinding commitment must be a tuple with at least (commitment, randomizer)\")\r",
            "        if not isinstance(challenge, (int, gmpy2.mpz)):\r",
            "            raise ValueError(\"challenge must be an integer\")\r",
            "        if not isinstance(responses, list) or not all(\r",
            "            isinstance(r, (int, gmpy2.mpz)) for r in responses\r",
            "        ):\r",
            "            raise ValueError(\"responses must be a list of integers\")\r",
            "\r",
            "        # Validate that all component lists have the correct size\r",
            "        if (\r",
            "            len(responses) != len(commitments)\r",
            "            or len(blinding_commitments) != len(commitments)\r",
            "            or len(commitment_randomizers) != len(commitments)\r",
            "            or len(blinding_randomizers) != len(commitments)\r",
            "        ):\r",
            "            detailed_msg = f\"Inconsistent lengths in proof components. responses: {len(responses)}, commitments: {len(commitments)}, blinding_commitments: {len(blinding_commitments)}, commitment_randomizers: {len(commitment_randomizers)}, blinding_randomizers: {len(blinding_randomizers)}\"\r",
            "            raise ValueError(f\"Invalid proof structure: {detailed_msg}\")\r",
            "\r",
            "        # Convert challenge to gmpy2.mpz once before the loop to avoid repeated conversion\r",
            "        challenge_mpz = gmpy2.mpz(challenge)\r",
            "        \r",
            "        # Verify each coefficient's proof - MODIFIED to prevent timing side-channels\r",
            "        all_valid: bool = True  # Track verification results without early return\r",
            "\r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Verify response equation for hash-based commitments:\r",
            "            # H(z_i, r_z_i, i) = C_b_i + challenge * C_i\r",
            "\r",
            "            # 1. Compute combined randomizer for the response: r_z_i = r_b_i + challenge * r_i\r",
            "            response_randomizer: FieldElement = (\r",
            "                blinding_randomizers[i] + challenge_mpz * commitment_randomizers[i]\r",
            "            ) % self.field.prime\r",
            "\r",
            "            # 2. Compute the hash commitment for the response\r",
            "            computed_commitment: FieldElement = self._compute_hash_commitment(\r",
            "                responses[i], response_randomizer, i, \"polynomial_proof_response\"\r",
            "            )\r",
            "\r",
            "            # 3. Compute the expected commitment: C_b_i + challenge * C_i\r",
            "            # Fixed: Safer access to tuple elements with validation\r",
            "            if not isinstance(blinding_commitments[i], tuple) or len(blinding_commitments[i]) < 1:\r",
            "                raise ValueError(f\"Invalid blinding commitment format at index {i}\")\r",
            "                \r",
            "            blinding_commitment_value: FieldElement = blinding_commitments[i][0]\r",
            "            \r",
            "            # Fixed: Safer way to access commitment values without unsafe cast\r",
            "            if not isinstance(commitments[i], tuple) or len(commitments[i]) < 1:\r",
            "                raise ValueError(f\"Invalid commitment format at index {i}\")\r",
            "                \r",
            "            commitment_value: FieldElement = commitments[i][0]\r",
            "            \r",
            "            # Convert to consistent numeric types for arithmetic\r",
            "            blinding_commitment_value = gmpy2.mpz(blinding_commitment_value)\r",
            "            commitment_value = gmpy2.mpz(commitment_value)\r",
            "            \r",
            "            expected_commitment: FieldElement = (\r",
            "                blinding_commitment_value + challenge_mpz * commitment_value\r",
            "            ) % self.group.prime\r",
            "\r",
            "            # 4. Update validity flag without early return\r",
            "            all_valid &= constant_time_compare(computed_commitment, expected_commitment)\r",
            "\r",
            "        return all_valid\r",
            "\r",
            "    def _detect_byzantine_behavior(\r",
            "        self, party_id: int, commitments: CommitmentList, shares: ShareDict, consistency_results: Optional[Dict[Tuple[int, int], bool]] = None\r",
            "    ) -> Tuple[bool, Dict[str, Any]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Enhanced Byzantine fault detection for comprehensive security analysis.\r",
            "\r",
            "            Detects multiple types of malicious behavior including inconsistent shares,\r",
            "            invalid commitments, and equivocation.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party to check.\r",
            "            commitments (list): Commitments from this party.\r",
            "            shares (dict): Shares distributed by this party.\r",
            "            consistency_results (dict, optional): Results from echo consistency checks.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            commitments: commitments\r",
            "            shares: shares\r",
            "            consistency_results: consistency results\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (is_byzantine, evidence).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "        \"\"\"\r",
            "        evidence: Dict[str, Any] = {}\r",
            "        is_byzantine: bool = False\r",
            "\r",
            "        # Input validation\r",
            "        if not isinstance(party_id, (int, str)):\r",
            "            raise TypeError(\"party_id must be an integer or string\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not isinstance(shares, dict):\r",
            "            raise TypeError(\"shares must be a dictionary\")\r",
            "        if consistency_results is not None and not isinstance(\r",
            "            consistency_results, dict\r",
            "        ):\r",
            "            raise TypeError(\"consistency_results must be a dictionary if provided\")\r",
            "\r",
            "        # Check 1: Are all commitments valid?\r",
            "        if not commitments:\r",
            "            evidence[\"invalid_commitments\"] = \"Missing commitments\"\r",
            "            return True, evidence\r",
            "        \r",
            "        # Validate first commitment structure more thoroughly  \r",
            "        if not isinstance(commitments[0], tuple):\r",
            "            evidence[\"invalid_commitments\"] = \"Malformed commitment (not a tuple)\"\r",
            "            return True, evidence\r",
            "            \r",
            "        # For hash-based commitments, verify the first coefficient is a commitment to 0\r",
            "        # Safely extract randomizer regardless of tuple length\r",
            "        if len(commitments[0]) >= 2:\r",
            "            randomizer: FieldElement = commitments[0][1]\r",
            "            expected: FieldElement = self._compute_hash_commitment(0, randomizer, 0, \"polynomial\")\r",
            "            \r",
            "            # Safely access first element of commitment tuple\r",
            "            commitment_value = commitments[0][0] if commitments[0] else None\r",
            "            if commitment_value is None or not constant_time_compare(commitment_value, expected):\r",
            "                evidence[\"invalid_zero_commitment\"] = {\r",
            "                    \"commitment\": int(commitment_value) if commitment_value is not None else None,\r",
            "                    \"expected\": int(expected),\r",
            "                }\r",
            "                is_byzantine = True\r",
            "        else:\r",
            "            evidence[\"invalid_commitment_structure\"] = \"First commitment has incorrect format (insufficient elements)\"\r",
            "            is_byzantine = True\r",
            "\r",
            "        # Check 2: Are all shares consistent with the commitments?\r",
            "        share_consistency: Dict[int, bool] = {}\r",
            "        \r",
            "        recipient_id: int\r",
            "        x: FieldElement\r",
            "        y: FieldElement\r",
            "        for recipient_id, (x, y) in shares.items():\r",
            "            # Verify this share against the commitments\r",
            "            is_valid: bool = self.verify_share(x, y, commitments)\r",
            "            share_consistency[recipient_id] = is_valid\r",
            "\r",
            "            if not is_valid:\r",
            "                if \"inconsistent_shares\" not in evidence:\r",
            "                    evidence[\"inconsistent_shares\"] = {}\r",
            "\r",
            "                # Compute values needed for verification for better diagnostics\r",
            "                randomizers: List[FieldElement] = [r_i for _, r_i, _ in commitments]\r",
            "                r_combined: FieldElement = self._compute_combined_randomizer(randomizers, x)\r",
            "                expected_commitment: FieldElement = self._compute_expected_commitment(commitments, x)\r",
            "\r",
            "                # Extract extra_entropy if present (should be in the first coefficient only)\r",
            "                extra_entropy: Optional[bytes] = None\r",
            "                if len(commitments) > 0 and isinstance(commitments[0], tuple) and len(commitments[0]) > 2:\r",
            "                    extra_entropy = commitments[0][2]  # Get extra_entropy from first coefficient\r",
            "\r",
            "                actual_commitment: FieldElement = self._compute_hash_commitment(\r",
            "                    y, r_combined, x, \"verify\", extra_entropy\r",
            "                )\r",
            "\r",
            "                evidence[\"inconsistent_shares\"][recipient_id] = {\r",
            "                    \"x\": int(x),\r",
            "                    \"y\": int(y),\r",
            "                    \"expected_commitment\": int(expected_commitment),\r",
            "                    \"actual_commitment\": int(actual_commitment),\r",
            "                    \"combined_randomizer\": int(r_combined),\r",
            "                }\r",
            "                is_byzantine = True\r",
            "\r",
            "        # Check 3: Look for evidence of equivocation from consistency checks\r",
            "        if (\r",
            "            hasattr(self, \"_byzantine_evidence\")\r",
            "            and party_id in self._byzantine_evidence\r",
            "        ):\r",
            "            evidence[\"equivocation\"] = self._byzantine_evidence[party_id]\r",
            "            is_byzantine = True\r",
            "\r",
            "        return is_byzantine, evidence\r",
            "\r",
            "    def detect_byzantine_party(\r",
            "        self, party_id: int, commitments: CommitmentList, shares: ShareDict, consistency_results: Optional[Dict[Tuple[int, int], bool]] = None\r",
            "    ) -> Tuple[bool, Dict[str, Any]]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Public method to detect Byzantine behavior from a specific party.\r",
            "\r",
            "        Arguments:\r",
            "            party_id (int): ID of the party to analyze.\r",
            "            commitments (list): Commitments from this party.\r",
            "            shares (dict): Shares distributed by this party.\r",
            "            consistency_results (dict, optional): Optional consistency check results.\r",
            "\r",
            "        Inputs:\r",
            "            party_id: party id\r",
            "            commitments: commitments\r",
            "            shares: shares\r",
            "            consistency_results: consistency results\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (is_byzantine, evidence_details).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If commitments list is empty.\r",
            "        \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(party_id, (int, str)):\r",
            "            raise TypeError(\"party_id must be an integer or string\")\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            self._raise_sanitized_error(ValueError, \"commitments list cannot be empty\")\r",
            "        if not isinstance(shares, dict):\r",
            "            raise TypeError(\"shares must be a dictionary\")\r",
            "        if consistency_results is not None and not isinstance(\r",
            "            consistency_results, dict\r",
            "        ):\r",
            "            raise TypeError(\"consistency_results must be a dictionary if provided\")\r",
            "\r",
            "        return self._detect_byzantine_behavior(\r",
            "            party_id, commitments, shares, consistency_results\r",
            "        )\r",
            "\r",
            "    def _evaluate_polynomial(self, coefficients: List[FieldElement], x: int) -> FieldElement:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Evaluate polynomial at point x using constant-time Horner's method.\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "            x (int): Point at which to evaluate the polynomial.\r",
            "        Inputs:\r",
            "            coefficients: Coefficients\r",
            "            x: x\r",
            "\r",
            "        Outputs:\r",
            "            int: Value of polynomial at point x.\r",
            "        \"\"\"\r",
            "        x_int: \"gmpy2.mpz\" = gmpy2.mpz(x)\r",
            "\r",
            "        # Estimate memory requirements for this operation\r",
            "        total_bits: int = sum(\r",
            "            c.bit_length() if hasattr(c, \"bit_length\") else gmpy2.mpz(c).bit_length()\r",
            "            for c in coefficients\r",
            "        )\r",
            "        if not check_memory_safety(\"mul\", x_int, total_bits):\r",
            "            raise MemoryError(\r",
            "                \"Polynomial evaluation would exceed memory limits. \"\r",
            "                \"The polynomial coefficients or evaluation point may be too large.\"\r",
            "            )\r",
            "\r",
            "        result: \"gmpy2.mpz\" = gmpy2.mpz(0)\r",
            "        coeff: FieldElement\r",
            "        for coeff in reversed(coefficients):\r",
            "            result = (result * x_int + gmpy2.mpz(coeff)) % self.field.prime\r",
            "        return result\r",
            "\r",
            "    def _reconstruct_polynomial_coefficients(self, x_values: List[FieldElement], y_values: List[FieldElement], threshold: int) -> List[FieldElement]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Reconstruct polynomial coefficients using quantum-resistant interpolation.\r",
            "\r",
            "        Arguments:\r",
            "            x_values (list): List of x-coordinates.\r",
            "            y_values (list): List of corresponding y-coordinates.\r",
            "            threshold (int): Degree of the polynomial to reconstruct (k).\r",
            "\r",
            "        Inputs:\r",
            "            x_values: x_values\r",
            "            y_values: y_values\r",
            "            threshold: threshold\r",
            "\r",
            "        Outputs:\r",
            "            list: List of reconstructed polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "\r",
            "        Raises:\r",
            "            ParameterError: If not enough points are provided or x-values are not unique.\r",
            "            VerificationError: If the matrix is singular during reconstruction.\r",
            "        \"\"\"\r",
            "        if len(x_values) < threshold:\r",
            "            detailed_msg = f\"Need at least {threshold} points to reconstruct a degree {threshold-1} polynomial, got {len(x_values)}\"\r",
            "            message = f\"Need at least {threshold} points to reconstruct\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Verify that the first 'threshold' x values we'll use are unique\r",
            "        # Convert to int to ensure hashability\r",
            "        if len({int(x) for x in x_values[:threshold]}) < threshold:\r",
            "            detailed_msg = f\"Need at least {threshold} unique x values to reconstruct polynomial, got: {x_values[:threshold]}\"\r",
            "            message = f\"Need at least {threshold} unique x values\"\r",
            "            self._raise_sanitized_error(ParameterError, message, detailed_msg)\r",
            "\r",
            "        # Validate memory usage for matrix operations\r",
            "        max_bit_length: int = max(\r",
            "            max(x.bit_length() if hasattr(x, \"bit_length\") else 0 for x in x_values),\r",
            "            max(y.bit_length() if hasattr(y, \"bit_length\") else 0 for y in y_values),\r",
            "        )\r",
            "\r",
            "        if not check_memory_safety(\"matrix\", threshold, max_bit_length):\r",
            "            raise MemoryError(\r",
            "                f\"Polynomial reconstruction with threshold {threshold} and values of \"\r",
            "                f\"approximately {max_bit_length} bits would exceed memory limits.\"\r",
            "            )\r",
            "\r",
            "        # Use only the required number of points\r",
            "        x_values = x_values[:threshold]\r",
            "        y_values = y_values[:threshold]\r",
            "        prime: FieldElement = self.field.prime\r",
            "\r",
            "        # Special case for threshold=1 (constant polynomial)\r",
            "        if threshold == 1:\r",
            "            return [y_values[0]]\r",
            "\r",
            "        # For threshold > 1, use matrix-based approach\r",
            "        # Create Vandermonde matrix for the system of equations\r",
            "        matrix: List[List[FieldElement]] = []\r",
            "        x: FieldElement\r",
            "        for x in x_values:\r",
            "            row: List[FieldElement] = []\r",
            "            j: int\r",
            "            for j in range(threshold):\r",
            "                row.append(gmpy2.powmod(x, j, prime))\r",
            "            matrix.append(row)\r",
            "\r",
            "        # Solve the system using secure Gaussian elimination\r",
            "        return self._secure_matrix_solve(matrix, y_values, prime)\r",
            "\r",
            "    def _secure_matrix_solve(self, matrix: List[List[FieldElement]], vector: List[FieldElement], prime: Optional[FieldElement] = None) -> List[FieldElement]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "        Solve a linear system using side-channel resistant Gaussian elimination.\r",
            "\r",
            "        Arguments:\r",
            "            matrix (list): Coefficient matrix.\r",
            "            vector (list): Right-hand side vector.\r",
            "            prime (int, optional): Field prime for modular arithmetic.\r",
            "\r",
            "        Inputs:\r",
            "            matrix: matrix\r",
            "            vector: vector\r",
            "            prime: prime\r",
            "\r",
            "        Outputs:\r",
            "            list: Solution vector containing polynomial coefficients.\r",
            "\r",
            "        Raises:\r",
            "            VerificationError: If a non-invertible value is encountered during matrix operations.\r",
            "        \"\"\"\r",
            "        if prime is None:\r",
            "            prime = self.field.prime\r",
            "        else:\r",
            "            # Validate that prime is actually prime when provided externally\r",
            "            if not gmpy2.is_prime(gmpy2.mpz(prime)):\r",
            "                detailed_msg = f\"The provided value {prime} is not a prime number\"\r",
            "                message = \"Invalid prime parameter\"\r",
            "                self._raise_sanitized_error(ValueError, message, detailed_msg)\r",
            "\r",
            "        n: int = len(vector)\r",
            "        \r",
            "        # Validate matrix dimensions match vector length\r",
            "        if len(matrix) != n:\r",
            "            detailed_msg = f\"Matrix rows ({len(matrix)}) must match vector length ({n})\"\r",
            "            message = \"Incompatible dimensions\"\r",
            "            self._raise_sanitized_error(ValueError, message, detailed_msg)\r",
            "            \r",
            "        # Also validate each row has correct length\r",
            "        for i, row in enumerate(matrix):\r",
            "            if len(row) != n:\r",
            "                detailed_msg = f\"Matrix row {i} has {len(row)} elements, but needs {n}\"\r",
            "                message = \"Matrix is not square\"\r",
            "                self._raise_sanitized_error(ValueError, message, detailed_msg)\r",
            "\r",
            "        # Check matrix size limits to prevent memory issues\r",
            "        if n > 1000:  # Reasonable limit for matrix size\r",
            "            raise MemoryError(\r",
            "                f\"Matrix size {n}x{n} exceeds safe processing limits. \"\r",
            "                f\"Consider reducing polynomial degree or threshold.\"\r",
            "            )\r",
            "\r",
            "        # Estimate memory requirements for the matrix operations\r",
            "        max_element: int = 0\r",
            "        \r",
            "        row: List[FieldElement]\r",
            "        for row in matrix:\r",
            "            element: FieldElement\r",
            "            for element in row:\r",
            "                element_size: int = (\r",
            "                    element.bit_length() if hasattr(element, \"bit_length\") else 0\r",
            "                )\r",
            "                max_element = max(max_element, element_size)\r",
            "\r",
            "        # Estimate total memory for the matrix operations - improved calculation\r",
            "        estimated_memory: int = n * n * max(max_element, prime.bit_length()) // 8  # in bytes\r",
            "        if estimated_memory > 1024 * 1024 * 1024:  # 1GB limit\r",
            "            raise MemoryError(\r",
            "                f\"Matrix operation would require approximately {estimated_memory/(1024*1024):.2f}MB, \"\r",
            "                f\"which exceeds the safe limit.\"\r",
            "            )\r",
            "\r",
            "        # Convert to gmpy2 types\r",
            "        matrix_mpz: List[List[\"gmpy2.mpz\"]] = [[gmpy2.mpz(x) for x in row] for row in matrix]\r",
            "        vector_mpz: List[\"gmpy2.mpz\"] = [gmpy2.mpz(x) for x in vector]\r",
            "\r",
            "\r",
            "        # Forward elimination with side-channel resistant operations\r",
            "        i: int\r",
            "        for i in range(n):\r",
            "            # Find pivot using secure method\r",
            "            pivot_row: Optional[int] = self._find_secure_pivot(matrix_mpz, i, n)\r",
            "\r",
            "            if pivot_row is None:\r",
            "                detailed_msg = \"Matrix is singular, cannot solve the system\"\r",
            "                message = \"Matrix is singular\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "            # Implement more side-channel resistant row swap\r",
            "            # Instead of conditional swap, we swap all needed elements unconditionally\r",
            "            # This approach reduces timing variations due to branching\r",
            "            # Only swap columns from the current pivot column onward\r",
            "            for col in range(i, n):\r",
            "                # Constant-time swap using arithmetic operations with explicit int conversion\r",
            "                should_swap = int(pivot_row != i)  # Explicitly convert bool to int\r",
            "                temp = matrix_mpz[i][col]\r",
            "                matrix_mpz[i][col] = should_swap * matrix_mpz[pivot_row][col] + (1 - should_swap) * matrix_mpz[i][col]\r",
            "                matrix_mpz[pivot_row][col] = should_swap * temp + (1 - should_swap) * matrix_mpz[pivot_row][col]\r",
            "            \r",
            "            # Also swap vector elements in a similar manner\r",
            "            temp_v = vector_mpz[i]\r",
            "            should_swap = int(pivot_row != i)  # Explicitly convert bool to int\r",
            "            vector_mpz[i] = should_swap * vector_mpz[pivot_row] + (1 - should_swap) * vector_mpz[i]\r",
            "            vector_mpz[pivot_row] = should_swap * temp_v + (1 - should_swap) * vector_mpz[pivot_row]\r",
            "\r",
            "            # Calculate inverse of pivot using gmpy2.invert instead of powmod\r",
            "            # This is more appropriate for modular inversion in constant time\r",
            "            pivot: \"gmpy2.mpz\" = matrix_mpz[i][i]\r",
            "            pivot_inverse: \"gmpy2.mpz\"\r",
            "            try:\r",
            "                pivot_inverse = gmpy2.invert(pivot, prime)\r",
            "            except ZeroDivisionError:\r",
            "                detailed_msg = f\"Value {pivot} is not invertible modulo {prime}\"\r",
            "                message = \"Value is not invertible\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "            # Scale current row\r",
            "            for j in range(i, n):\r",
            "                matrix_mpz[i][j] = (matrix_mpz[i][j] * pivot_inverse) % prime\r",
            "            vector_mpz[i] = (vector_mpz[i] * pivot_inverse) % prime\r",
            "\r",
            "            # Eliminate other rows with constant-time operations\r",
            "            for j in range(n):\r",
            "                if j != i:\r",
            "                    factor: \"gmpy2.mpz\" = matrix_mpz[j][i]\r",
            "                    for k in range(i, n):\r",
            "                        matrix_mpz[j][k] = (matrix_mpz[j][k] - factor * matrix_mpz[i][k]) % prime\r",
            "                    vector_mpz[j] = (vector_mpz[j] - factor * vector_mpz[i]) % prime\r",
            "\r",
            "        return vector_mpz\r",
            "\r",
            "    def _find_secure_pivot(self, matrix: List[List[\"gmpy2.mpz\"]], col: int, n: int) -> Optional[int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Find a non-zero pivot using side-channel resistant selection.\r",
            "\r",
            "            This method implements a randomized pivot selection strategy that prevents\r",
            "            timing-based side-channel attacks during Gaussian elimination. Instead of\r",
            "            selecting the first suitable pivot (which would create timing variations),\r",
            "            it assigns random values to all potential pivots and selects one with minimal\r",
            "            random value, ensuring constant-time behavior regardless of matrix content.\r",
            "\r",
            "        Arguments:\r",
            "            matrix (list): The matrix being processed.\r",
            "            col (int): Current column index.\r",
            "            n (int): Matrix dimension.\r",
            "\r",
            "        Inputs:\r",
            "            matrix: Matrix of coefficients.\r",
            "            col: Current column being processed.\r",
            "            n: Matrix dimension.\r",
            "\r",
            "        Outputs:\r",
            "            int: Index of selected pivot row or None if no valid pivot exists.\r",
            "\r",
            "        Security properties:\r",
            "            - Constant-time with respect to the values in the matrix\r",
            "            - Uses cryptographically secure randomness via secrets.token_bytes()\r",
            "            - Resistant to timing side-channel attacks\r",
            "            - Prevents information leakage about matrix structure\r",
            "        \"\"\"\r",
            "        # Generate a single random block for all rows at once (more efficient)\r",
            "        range_size: int = n - col\r",
            "        all_random_bytes: bytes = secrets.token_bytes(32 * range_size)\r",
            "\r",
            "        # Find the valid pivot with the smallest random value\r",
            "        min_value: float = float(\"inf\")\r",
            "        pivot_row: Optional[int] = None\r",
            "\r",
            "        # Track if we found any non-zero pivot (for improved security)\r",
            "        found_any_nonzero: bool = False\r",
            "\r",
            "        k: int\r",
            "        for k in range(range_size):\r",
            "            row: int = col + k\r",
            "            # Extract random value for this row\r",
            "            offset: int = k * 32\r",
            "            row_random: int = int.from_bytes(\r",
            "                all_random_bytes[offset : offset + 32], byteorder=\"big\"\r",
            "            )\r",
            "\r",
            "            # Update minimum if valid pivot and has smaller random value\r",
            "            is_nonzero: bool = matrix[row][col] != 0\r",
            "            found_any_nonzero = found_any_nonzero or is_nonzero\r",
            "            \r",
            "            # Use a constant-time approach to update min_value and pivot_row\r",
            "            # Improved constant-time selection using integer masks\r",
            "            swap_mask = int(is_nonzero and row_random < min_value)\r",
            "            min_value = swap_mask * row_random + (1 - swap_mask) * min_value\r",
            "            # Use arithmetic instead of conditional assignment for constant time\r",
            "            new_pivot = row * swap_mask + (pivot_row or 0) * (1 - swap_mask)\r",
            "            pivot_row = new_pivot if (pivot_row is not None or swap_mask) else None\r",
            "\r",
            "        # Enhanced error check - ensure we're not returning a row with a zero pivot\r",
            "        # in case the constant-time logic has a subtle bug\r",
            "        if pivot_row is not None and matrix[pivot_row][col] == 0:\r",
            "            # This should never happen but acts as a safety check\r",
            "            if found_any_nonzero:\r",
            "                # Something went wrong with our constant-time selection\r",
            "                detailed_msg = \"Security error: Selected a zero pivot despite non-zero pivots being available\"\r",
            "                message = \"Security error in pivot selection\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "            return None\r",
            "\r",
            "        return pivot_row\r",
            "        \r",
            "\r",
            "    def create_commitments_with_proof(self, coefficients: List[FieldElement], context: Optional[str] = None) -> Tuple[CommitmentList, ProofDict]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Create commitments to polynomial coefficients and generate a zero-knowledge\r",
            "            proof of knowledge of the coefficients in one combined operation.\r",
            "\r",
            "            This provides a more efficient way to generate both commitments and proofs\r",
            "            and is recommended for share distribution where proof of knowledge is needed.\r",
            "\r",
            "        Arguments:\r",
            "            coefficients (list): List of polynomial coefficients [a\u2080, a\u2081, ..., a\u2096\u208b\u2081].\r",
            "            context (str, optional): Optional context string for domain separation.\r",
            "\r",
            "        Inputs:\r",
            "            coefficients: coefficients\r",
            "            context: context\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (commitments, proof) where both are suitable for verification.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(coefficients, list) or not coefficients:\r",
            "            raise TypeError(\"coefficients must be a non-empty list\")\r",
            "\r",
            "        if context is not None and not isinstance(context, str):\r",
            "            raise TypeError(\"context must be a string if provided\")\r",
            "\r",
            "        # Create commitments first\r",
            "        commitments: CommitmentList = self.create_commitments(coefficients, context)\r",
            "\r",
            "        # Generate zero-knowledge proof of knowledge\r",
            "        proof: ProofDict = self.create_polynomial_proof(coefficients, commitments)\r",
            "\r",
            "        return commitments, proof\r",
            "\r",
            "    def verify_commitments_with_proof(self, commitments: CommitmentList, proof: ProofDict, strict_verification: bool = False) -> bool:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Verify that a zero-knowledge proof demonstrates knowledge of the\r",
            "            polynomial coefficients committed to by the given commitments.\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of commitments to polynomial coefficients.\r",
            "            proof (dict): Zero-knowledge proof structure from create_polynomial_proof.\r",
            "            strict_verification (bool): If True, raises an error on challenge verification failure.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "            proof: proof\r",
            "            strict_verification: strict_verification\r",
            "\r",
            "        Outputs:\r",
            "            bool: True if the proof is valid, False otherwise.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If commitments list is empty.\r",
            "            SecurityWarning: If proof is missing required keys.\r",
            "            VerificationError: If strict_verification is True and verification fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(commitments, list):\r",
            "            raise TypeError(\"commitments must be a list\")\r",
            "        if not commitments:\r",
            "            raise ValueError(\"commitments list cannot be empty\")\r",
            "        if not isinstance(proof, dict):\r",
            "            raise TypeError(\"proof must be a dictionary\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Validate proof has all required keys before proceeding\r",
            "        required_keys: List[str] = [\r",
            "            \"blinding_commitments\",\r",
            "            \"challenge\",\r",
            "            \"responses\",\r",
            "            \"commitment_randomizers\",\r",
            "            \"blinding_randomizers\",\r",
            "        ]\r",
            "        if not all(key in proof for key in required_keys):\r",
            "            warnings.warn(\"Proof missing required keys\", SecurityWarning)\r",
            "            return False\r",
            "\r",
            "        # Verify the proof with added challenge consistency check\r",
            "        is_valid = self.verify_polynomial_proof(proof, commitments)\r",
            "        \r",
            "        # Optionally check challenge consistency more strictly\r",
            "        if is_valid and not self._verify_challenge_consistency(proof, commitments):\r",
            "            if strict_verification:\r",
            "                detailed_msg = \"Proof verification passed but challenge value appears inconsistent\"\r",
            "                message = \"Challenge verification failed\"\r",
            "                self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "            warnings.warn(\"Challenge verification failed\", SecurityWarning)\r",
            "            return False\r",
            "            \r",
            "        return is_valid\r",
            "\r",
            "    def serialize_commitments_with_proof(self, commitments: CommitmentList, proof: ProofDict) -> str:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Serialize commitments and associated zero-knowledge proof for storage or transmission\r",
            "\r",
            "        Arguments:\r",
            "            commitments (list): List of (hash, randomizer) tuples.\r",
            "            proof (dict): Zero-knowledge proof structure from create_polynomial_proof.\r",
            "\r",
            "        Inputs:\r",
            "            commitments: commitments\r",
            "            proof: proof\r",
            "\r",
            "        Outputs:\r",
            "            str: String with base64-encoded serialized data.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            ValueError: If proof is missing required keys.\r",
            "            SerializationError: If serialization fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(commitments, list) or not commitments:\r",
            "            raise TypeError(\"commitments must be a non-empty list\")\r",
            "        if not all(isinstance(c, tuple) and len(c) >= 2 for c in commitments):\r",
            "            raise TypeError(\r",
            "                \"Each commitment must be a tuple of at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # Add validation for proof parameter\r",
            "        if not isinstance(proof, dict):\r",
            "            raise TypeError(\"proof must be a dictionary\")\r",
            "\r",
            "        required_proof_keys: List[str] = [\r",
            "            \"blinding_commitments\",\r",
            "            \"challenge\",\r",
            "            \"responses\",\r",
            "            \"commitment_randomizers\",\r",
            "            \"blinding_randomizers\",\r",
            "            \"timestamp\",\r",
            "        ]\r",
            "        for key in required_proof_keys:\r",
            "            if key not in proof:\r",
            "                raise ValueError(f\"proof is missing required key: {key}\")\r",
            "\r",
            "        if (\r",
            "            not isinstance(proof[\"blinding_commitments\"], list)\r",
            "            or not proof[\"blinding_commitments\"]\r",
            "        ):\r",
            "            raise TypeError(\"proof['blinding_commitments'] must be a non-empty list\")\r",
            "        if not all(\r",
            "            isinstance(c, tuple) and len(c) >= 2 for c in proof[\"blinding_commitments\"]\r",
            "        ):\r",
            "            raise TypeError(\r",
            "                \"Each blinding commitment must be a tuple with at least (commitment, randomizer)\"\r",
            "            )\r",
            "\r",
            "        # First serialize the commitments as before\r",
            "        commitment_values: List[Tuple[int, int, Optional[str]]] = [\r",
            "            (int(c), int(r), e.hex() if e else None) for c, r, e in commitments\r",
            "        ]\r",
            "\r",
            "        # Process proof data for serialization\r",
            "        serializable_proof: Dict[str, Any] = {\r",
            "            \"blinding_commitments\": [\r",
            "                (int(c), int(r)) for c, r in proof[\"blinding_commitments\"]\r",
            "            ],\r",
            "            \"challenge\": int(proof[\"challenge\"]),\r",
            "            \"responses\": [int(r) for r in proof[\"responses\"]],\r",
            "            \"commitment_randomizers\": [int(r) for r in proof[\"commitment_randomizers\"]],\r",
            "            \"blinding_randomizers\": [int(r) for r in proof[\"blinding_randomizers\"]],\r",
            "            \"timestamp\": int(proof[\"timestamp\"]),\r",
            "        }\r",
            "\r",
            "        result: Dict[str, Any] = {\r",
            "            \"version\": VSS_VERSION,\r",
            "            \"timestamp\": int(time.time()),\r",
            "            \"generator\": int(self.generator),\r",
            "            \"prime\": int(self.group.prime),\r",
            "            \"commitments\": commitment_values,\r",
            "            \"hash_based\": True,\r",
            "            \"proof\": serializable_proof,\r",
            "            \"has_proof\": True,\r",
            "        }\r",
            "\r",
            "        # Pack with msgpack for efficient serialization\r",
            "        try:\r",
            "            packed_data: bytes = msgpack.packb(result)\r",
            "\r",
            "            # Compute checksum and create wrapper\r",
            "            checksum_wrapper: Dict[str, Any] = {\r",
            "                \"data\": packed_data,\r",
            "                \"checksum\": compute_checksum(packed_data),\r",
            "            }\r",
            "\r",
            "            # Pack the wrapper and encode\r",
            "            packed_wrapper: bytes = msgpack.packb(checksum_wrapper)\r",
            "            return urlsafe_b64encode(packed_wrapper).decode(\"utf-8\")\r",
            "\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to serialize commitments with proof: {e}\"\r",
            "            message = \"Serialization failed\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def deserialize_commitments_with_proof(self, data: str) -> Tuple[CommitmentList, ProofDict, FieldElement, FieldElement, int]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Deserialize commitment data including zero-knowledge proof with enhanced security checks\r",
            "\r",
            "        Arguments:\r",
            "            data (str): Serialized commitment data string.\r",
            "\r",
            "        Inputs:\r",
            "            data: Serialized data\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (commitments, proof, generator, prime, timestamp).\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If data is not a string or is empty.\r",
            "            SerializationError: If deserialization or validation fails.\r",
            "            SecurityError: If data integrity checks fail.\r",
            "        \"\"\"\r",
            "        # Add validation\r",
            "        if not isinstance(data, str):\r",
            "            raise TypeError(\"data must be a string\")\r",
            "        if not data:\r",
            "            raise ValueError(\"data cannot be empty\")\r",
            "\r",
            "        try:\r",
            "            # Decode and unpack the data\r",
            "            decoded: bytes = urlsafe_b64decode(data.encode(\"utf-8\"))\r",
            "\r",
            "            # Use Unpacker with security settings - matching the approach in deserialize_commitments\r",
            "            unpacker: \"msgpack.Unpacker\" = msgpack.Unpacker(\r",
            "                use_list=False,  # Use tuples instead of lists for immutability\r",
            "                raw=True,  # Keep binary data as bytes\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,  # 10MB limit\r",
            "            )\r",
            "            unpacker.feed(decoded)\r",
            "\r",
            "            # Define constants for dictionary keys to ensure consistency\r",
            "            CHECKSUM_KEY = b\"checksum\"\r",
            "            DATA_KEY = b\"data\"\r",
            "            HAS_PROOF_KEY = b\"has_proof\"\r",
            "            PROOF_KEY = b\"proof\"\r",
            "            \r",
            "            # Keys for the proof dictionary\r",
            "            BLINDING_COMMITMENTS_KEY = b\"blinding_commitments\"\r",
            "            CHALLENGE_KEY = b\"challenge\"\r",
            "            RESPONSES_KEY = b\"responses\"\r",
            "            COMMITMENT_RANDOMIZERS_KEY = b\"commitment_randomizers\" \r",
            "            BLINDING_RANDOMIZERS_KEY = b\"blinding_randomizers\"\r",
            "            TIMESTAMP_KEY = b\"timestamp\"\r",
            "\r",
            "            wrapper_dict: Dict[bytes, Any]\r",
            "            try:\r",
            "                # Unpack the checksum wrapper\r",
            "                wrapper_dict = unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack msgpack data: {e}\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Verify checksum - this is a critical security check\r",
            "            if CHECKSUM_KEY not in wrapper_dict or DATA_KEY not in wrapper_dict:\r",
            "                detailed_msg = \"Missing checksum or data fields in deserialized content\"\r",
            "                message = \"Invalid data format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            packed_data: bytes = wrapper_dict[DATA_KEY]\r",
            "            expected_checksum: int = wrapper_dict[CHECKSUM_KEY]\r",
            "            actual_checksum: int = compute_checksum(packed_data)\r",
            "\r",
            "            # Use constant-time comparison to prevent timing attacks\r",
            "            if not constant_time_compare(actual_checksum, expected_checksum):\r",
            "                detailed_msg = f\"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}\"\r",
            "                message = \"Data integrity check failed - possible tampering detected\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Feed the inner data to a new Unpacker instance\r",
            "            inner_unpacker: \"msgpack.Unpacker\" = msgpack.Unpacker(\r",
            "                use_list=False,\r",
            "                raw=True,\r",
            "                strict_map_key=True,\r",
            "                max_buffer_size=10 * 1024 * 1024,\r",
            "            )\r",
            "            inner_unpacker.feed(packed_data)\r",
            "\r",
            "            unpacked_dict: Dict[bytes, Any]\r",
            "            try:\r",
            "                # Proceed with unpacking the actual data\r",
            "                unpacked_dict = inner_unpacker.unpack()\r",
            "            except (\r",
            "                msgpack.exceptions.ExtraData,\r",
            "                msgpack.exceptions.FormatError,\r",
            "                msgpack.exceptions.StackError,\r",
            "                msgpack.exceptions.BufferFull,\r",
            "                msgpack.exceptions.OutOfData,\r",
            "                ValueError,\r",
            "            ) as e:\r",
            "                detailed_msg = f\"Failed to unpack inner msgpack data: {e}\"\r",
            "                message = \"Failed to unpack data\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # First deserialize commitments using the existing method\r",
            "            commitments: CommitmentList\r",
            "            generator: FieldElement\r",
            "            prime: FieldElement\r",
            "            timestamp: int\r",
            "            is_hash_based: bool\r",
            "            commitments, generator, prime, timestamp, is_hash_based = (\r",
            "                self.deserialize_commitments(data)\r",
            "            )\r",
            "\r",
            "            # Check if proof data is present\r",
            "            has_proof: bool = unpacked_dict.get(HAS_PROOF_KEY, False)\r",
            "            if not has_proof:\r",
            "                detailed_msg = \"No proof data found in serialized commitments\"\r",
            "                message = \"Missing proof data\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Extract and reconstruct proof\r",
            "            serialized_proof: Optional[Dict[bytes, Any]] = unpacked_dict.get(PROOF_KEY)\r",
            "            if not serialized_proof:\r",
            "                detailed_msg = \"Missing proof data in serialized commitments\"\r",
            "                message = \"Missing proof data\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate proof structure more thoroughly\r",
            "            required_keys: List[bytes] = [\r",
            "                BLINDING_COMMITMENTS_KEY,\r",
            "                CHALLENGE_KEY,\r",
            "                RESPONSES_KEY,\r",
            "                COMMITMENT_RANDOMIZERS_KEY,\r",
            "                BLINDING_RANDOMIZERS_KEY,\r",
            "                TIMESTAMP_KEY,\r",
            "            ]\r",
            "            \r",
            "            for key in required_keys:\r",
            "                if key not in serialized_proof:\r",
            "                    detailed_msg = f\"Proof missing required field: {key.decode('utf-8')}\"\r",
            "                    message = \"Invalid proof structure\"\r",
            "                    self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Validate types and structures with safer access patterns\r",
            "            if not isinstance(serialized_proof.get(BLINDING_COMMITMENTS_KEY), tuple):\r",
            "                detailed_msg = \"blinding_commitments must be a sequence\"\r",
            "                message = \"Invalid proof structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            if not isinstance(serialized_proof.get(CHALLENGE_KEY), int):\r",
            "                detailed_msg = \"challenge must be an integer\"\r",
            "                message = \"Invalid proof structure\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "            # Additional validations...\r",
            "            # ...existing code...\r",
            "\r",
            "            # Improved exception handling for specific error types\r",
            "            try:\r",
            "                # Reconstruct the proof with proper structure\r",
            "                proof: ProofDict = {\r",
            "                    \"blinding_commitments\": [\r",
            "                        (gmpy2.mpz(c), gmpy2.mpz(r))\r",
            "                        for c, r in serialized_proof[BLINDING_COMMITMENTS_KEY]\r",
            "                    ],\r",
            "                    \"challenge\": gmpy2.mpz(serialized_proof[CHALLENGE_KEY]),\r",
            "                    \"responses\": [gmpy2.mpz(r) for r in serialized_proof[RESPONSES_KEY]],\r",
            "                    \"commitment_randomizers\": [\r",
            "                        gmpy2.mpz(r) for r in serialized_proof[COMMITMENT_RANDOMIZERS_KEY]\r",
            "                    ],\r",
            "                    \"blinding_randomizers\": [\r",
            "                        gmpy2.mpz(r) for r in serialized_proof[BLINDING_RANDOMIZERS_KEY]\r",
            "                    ],\r",
            "                    \"timestamp\": serialized_proof[TIMESTAMP_KEY],\r",
            "                }\r",
            "            except (TypeError, ValueError, IndexError) as e:\r",
            "                detailed_msg = f\"Failed to convert proof components: {e}\"\r",
            "                message = \"Invalid proof format\"\r",
            "                self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "            except Exception as e:\r",
            "                # Keep this general exception handler as a last resort\r",
            "                detailed_msg = f\"Unexpected error reconstructing proof: {e}\"\r",
            "                message = \"Proof reconstruction failed\"\r",
            "                self._raise_sanitized_error(SecurityError, message, detailed_msg)\r",
            "\r",
            "            # Add challenge verification\r",
            "            # Note: This is a simplified example - actual implementation would \r",
            "            # need to call the appropriate challenge computation function\r",
            "            try:\r",
            "                # Verify proof internally with challenge recomputation\r",
            "                if not self._verify_challenge_consistency(proof, commitments):\r",
            "                    warnings.warn(\"Challenge verification failed\", SecurityWarning)\r",
            "                    return commitments, proof, generator, prime, timestamp\r",
            "            except Exception as e:\r",
            "                # Only warn about challenge verification issues, don't fail\r",
            "                warnings.warn(f\"Challenge verification error: {e}\", SecurityWarning)\r",
            "\r",
            "            return commitments, proof, generator, prime, timestamp\r",
            "        except (SerializationError, SecurityError):\r",
            "            # Re-raise specific security exceptions\r",
            "            raise\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to deserialize commitments with proof: {e}\"\r",
            "            message = \"Deserialization failed\"\r",
            "            self._raise_sanitized_error(SerializationError, message, detailed_msg)\r",
            "\r",
            "    def verify_share_with_proof(self, share_x: FieldElement, share_y: FieldElement, serialized_data: str) -> Tuple[bool, bool]:\r",
            "        \"\"\"\r",
            "        Description:\r",
            "            Comprehensive verification of a share against serialized commitment data with proof\r",
            "\r",
            "        Arguments:\r",
            "            share_x (int): x-coordinate of the share.\r",
            "            share_y (int): y-coordinate of the share.\r",
            "            serialized_data (str): Serialized commitment data with proof.\r",
            "\r",
            "        Inputs:\r",
            "            share_x: share x\r",
            "            share_y: share y\r",
            "            serialized_data: serialized data\r",
            "\r",
            "        Outputs:\r",
            "            tuple: (share_valid, proof_valid) indicating validation results.\r",
            "\r",
            "        Raises:\r",
            "            TypeError: If inputs have incorrect types.\r",
            "            VerificationError: If verification fails.\r",
            "        \"\"\"\r",
            "        # Input validation\r",
            "        if not isinstance(share_x, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_x must be an integer\")\r",
            "        if not isinstance(share_y, (int, gmpy2.mpz)):\r",
            "            raise TypeError(\"share_y must be an integer\")\r",
            "        if not isinstance(serialized_data, str) or not serialized_data:\r",
            "            raise TypeError(\"serialized_data must be a non-empty string\")\r",
            "\r",
            "        try:\r",
            "            # Deserialize the commitments and proof\r",
            "            commitments: CommitmentList\r",
            "            proof: ProofDict\r",
            "            generator: FieldElement\r",
            "            prime: FieldElement\r",
            "            timestamp: int\r",
            "            \r",
            "            commitments, proof, generator, prime, timestamp = (\r",
            "                self.deserialize_commitments_with_proof(serialized_data)\r",
            "            )\r",
            "\r",
            "            # Create a group with the same parameters\r",
            "            group: CyclicGroup = CyclicGroup(prime=prime, generator=generator)\r",
            "\r",
            "            # Create a new VSS instance with this group\r",
            "            temp_config: VSSConfig = VSSConfig()\r",
            "            temp_vss: FeldmanVSS = FeldmanVSS(self.field, temp_config, group)\r",
            "\r",
            "            # Verify both the share and the proof\r",
            "            share_valid: bool = temp_vss.verify_share(share_x, share_y, commitments)\r",
            "            proof_valid: bool = temp_vss.verify_commitments_with_proof(commitments, proof)\r",
            "\r",
            "            return share_valid, proof_valid\r",
            "\r",
            "        except Exception as e:\r",
            "            detailed_msg = f\"Failed to verify share with proof: {e}\"\r",
            "            message = \"Verification failed\"\r",
            "            self._raise_sanitized_error(VerificationError, message, detailed_msg)\r",
            "\r",
            "\r",
            "# Simplified factory function focused on post-quantum security\r",
            "def get_feldman_vss(field: Any, **kwargs: Any) -> FeldmanVSS:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Factory function to create a post-quantum secure FeldmanVSS instance.\r",
            "\r",
            "    Arguments:\r",
            "        field: MersennePrimeField instance.\r",
            "        **kwargs: Additional configuration parameters.\r",
            "\r",
            "    Inputs:\r",
            "        field: Field\r",
            "\r",
            "    Outputs:\r",
            "        FeldmanVSS: FeldmanVSS instance configured for post-quantum security.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If field is None or does not have a 'prime' attribute of the correct type.\r",
            "    \"\"\"\r",
            "    # Add validation for field parameter\r",
            "    if field is None:\r",
            "        raise TypeError(\"field cannot be None\")\r",
            "\r",
            "    if not hasattr(field, \"prime\"):\r",
            "        raise TypeError(\"field must have 'prime' attribute\")\r",
            "\r",
            "    if not isinstance(field.prime, (int, gmpy2.mpz)):\r",
            "        raise TypeError(\"field.prime must be an integer type\")\r",
            "\r",
            "    config: Optional[VSSConfig] = kwargs.get(\"config\", None)\r",
            "\r",
            "    if config is None:\r",
            "        config = VSSConfig(\r",
            "            prime_bits=4096,  # Always use at least 3072 bits for post-quantum security\r",
            "            safe_prime=True,\r",
            "            use_blake3=True,\r",
            "        )\r",
            "\r",
            "    return FeldmanVSS(field, config)\r",
            "\r",
            "\r",
            "# Integration helper for the main Shamir Secret Sharing implementation\r",
            "def create_vss_from_shamir(shamir_instance: Any) -> FeldmanVSS:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Create a post-quantum secure FeldmanVSS instance compatible with a ShamirSecretSharing instance\r",
            "\r",
            "    Arguments:\r",
            "        shamir_instance: A ShamirSecretSharing instance.\r",
            "\r",
            "    Inputs:\r",
            "        shamir_instance: Shamir instance\r",
            "\r",
            "    Outputs:\r",
            "        FeldmanVSS: FeldmanVSS instance configured to work with the Shamir instance.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If shamir_instance does not have the required attributes.\r",
            "    \"\"\"\r",
            "    # Validate the shamir_instance has required attributes\r",
            "    if not hasattr(shamir_instance, \"field\"):\r",
            "        raise TypeError(\"shamir_instance must have a 'field' attribute\")\r",
            "\r",
            "    if not hasattr(shamir_instance.field, \"prime\"):\r",
            "        raise TypeError(\"shamir_instance.field must have a 'prime' attribute\")\r",
            "\r",
            "    # Get the field from the Shamir instance\r",
            "    field: Any = shamir_instance.field\r",
            "\r",
            "    # Configure VSS based on Shamir's parameters\r",
            "    prime_bits: int = field.prime.bit_length()\r",
            "\r",
            "    if prime_bits < MIN_PRIME_BITS:\r",
            "        warnings.warn(\r",
            "            f\"Shamir instance uses {prime_bits}-bit prime which is less than the \"\r",
            "            f\"recommended {MIN_PRIME_BITS} bits for post-quantum security. \"\r",
            "            f\"Consider regenerating your Shamir instance with stronger parameters.\",\r",
            "            SecurityWarning,\r",
            "        )\r",
            "\r",
            "    # Create a post-quantum secure VSS instance\r",
            "    return get_feldman_vss(field)\r",
            "\r",
            "\r",
            "# Add a helper function to integrate with Pedersen VSS\r",
            "def integrate_with_pedersen(feldman_vss: FeldmanVSS, pedersen_vss: Any, shares: ShareDict, coefficients: List[FieldElement]) -> Dict[str, Any]:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Integrate Feldman VSS with Pedersen VSS for dual verification.\r",
            "\r",
            "        This provides both the binding property from Feldman VSS and the\r",
            "        hiding property from Pedersen VSS, offering the best of both approaches.\r",
            "\r",
            "    Arguments:\r",
            "        feldman_vss: FeldmanVSS instance.\r",
            "        pedersen_vss: PedersenVSS instance.\r",
            "        shares: Dictionary of shares from Shamir secret sharing.\r",
            "        coefficients: Polynomial coefficients used for share generation.\r",
            "\r",
            "    Inputs:\r",
            "        feldman_vss: feldman vss\r",
            "        pedersen_vss: pedersen vss\r",
            "        shares: shares\r",
            "        coefficients: coefficients\r",
            "\r",
            "    Outputs:\r",
            "        dict: Dictionary with both Feldman and Pedersen verification data.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If inputs have incorrect types.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not isinstance(feldman_vss, FeldmanVSS):\r",
            "        raise TypeError(\"feldman_vss must be a FeldmanVSS instance\")\r",
            "    if not hasattr(pedersen_vss, \"create_commitments\"):\r",
            "        raise TypeError(\"pedersen_vss must have a create_commitments method\")\r",
            "    if not isinstance(shares, dict):\r",
            "        raise TypeError(\"shares must be a dictionary\")\r",
            "    if not isinstance(coefficients, list) or not coefficients:\r",
            "        raise TypeError(\"coefficients must be a non-empty list\")\r",
            "\r",
            "    # Generate Feldman commitments\r",
            "    feldman_commitments: CommitmentList = feldman_vss.create_commitments(coefficients)\r",
            "\r",
            "    # Generate Pedersen commitments\r",
            "    pedersen_commitments: List[FieldElement] = pedersen_vss.create_commitments(coefficients)\r",
            "\r",
            "    # Create a zero-knowledge proof that both commitment sets commit to the same values\r",
            "    # This demonstrates that the Feldman and Pedersen schemes are using the same polynomial\r",
            "    proof:  Dict[str, Any] = create_dual_commitment_proof(\r",
            "        feldman_vss,\r",
            "        pedersen_vss,\r",
            "        coefficients,\r",
            "        feldman_commitments,\r",
            "        pedersen_commitments,\r",
            "    )\r",
            "\r",
            "    # Serialize the commitments\r",
            "    feldman_serialized: str = feldman_vss.serialize_commitments(feldman_commitments)\r",
            "    pedersen_serialized: str = pedersen_vss.serialize_commitments(pedersen_commitments)\r",
            "\r",
            "    return {\r",
            "        \"feldman_commitments\": feldman_serialized,\r",
            "        \"pedersen_commitments\": pedersen_serialized,\r",
            "        \"dual_proof\": proof,\r",
            "        \"version\": VSS_VERSION,\r",
            "    }\r",
            "\r",
            "\r",
            "def create_dual_commitment_proof(\r",
            "    feldman_vss: FeldmanVSS, pedersen_vss: Any, coefficients: List[FieldElement], feldman_commitments: CommitmentList, pedersen_commitments: List[FieldElement]\r",
            ") -> Dict[str, Any]:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Create a zero-knowledge proof that Feldman and Pedersen commitments\r",
            "        are to the same polynomial coefficients.\r",
            "\r",
            "    Arguments:\r",
            "        feldman_vss: FeldmanVSS instance.\r",
            "        pedersen_vss: PedersenVSS instance.\r",
            "        coefficients: The polynomial coefficients.\r",
            "        feldman_commitments: Commitments created by Feldman scheme.\r",
            "        pedersen_commitments: Commitments created by Pedersen scheme.\r",
            "\r",
            "    Inputs:\r",
            "        feldman_vss: feldman_vss\r",
            "        pedersen_vss: pedersen_vss\r",
            "        coefficients: coefficients\r",
            "        feldman_commitments: feldman_commitments\r",
            "        pedersen_commitments: pedersen_commitments\r",
            "\r",
            "    Outputs:\r",
            "        dict: Proof data structure.\r",
            "\r",
            "    Raises:\r",
            "        TypeError: If inputs have incorrect types.\r",
            "        ValueError: If input lists have inconsistent lengths.\r",
            "    \"\"\"\r",
            "    # Input validation for all parameters\r",
            "    if not isinstance(feldman_vss, FeldmanVSS):\r",
            "        raise TypeError(\"feldman_vss must be a FeldmanVSS instance\")\r",
            "\r",
            "    if not hasattr(pedersen_vss, \"commit_to_blinding_factors\"):\r",
            "        raise TypeError(\"pedersen_vss must have a 'commit_to_blinding_factors' method\")\r",
            "\r",
            "    if not hasattr(pedersen_vss, \"g\") or not hasattr(pedersen_vss, \"h\"):\r",
            "        raise TypeError(\"pedersen_vss must have 'g' and 'h' attributes\")\r",
            "\r",
            "    if not isinstance(coefficients, list) or not coefficients:\r",
            "        raise TypeError(\"coefficients must be a non-empty list\")\r",
            "\r",
            "    if not isinstance(feldman_commitments, list) or not feldman_commitments:\r",
            "        raise TypeError(\"feldman_commitments must be a non-empty list\")\r",
            "\r",
            "    if not isinstance(pedersen_commitments, list) or not pedersen_commitments:\r",
            "        raise TypeError(\"pedersen_commitments must be a non-empty list\")\r",
            "\r",
            "    if len(coefficients) != len(feldman_commitments) or len(coefficients) != len(\r",
            "        pedersen_commitments\r",
            "    ):\r",
            "        raise ValueError(\r",
            "            \"coefficients, feldman_commitments, and pedersen_commitments must have the same length\"\r",
            "        )\r",
            "\r",
            "    # Generate random blinding factors\r",
            "    blindings: List[FieldElement] = [\r",
            "        feldman_vss.group.secure_random_element() for _ in range(len(coefficients))\r",
            "    ]\r",
            "\r",
            "    # Check if we're using hash-based commitments\r",
            "    is_hash_based: bool = isinstance(feldman_commitments[0], tuple)\r",
            "\r",
            "    # Create Feldman commitments to the blinding factors\r",
            "    feldman_blinding_commitments:  List[Union[Tuple[FieldElement, FieldElement], FieldElement]] = []\r",
            "\r",
            "    if is_hash_based:\r",
            "        # Create hash-based blinding commitments (with randomizers)\r",
            "        i: int\r",
            "        b: FieldElement\r",
            "        for i, b in enumerate(blindings):\r",
            "            # Generate secure randomizer for each blinding factor\r",
            "            r_b: FieldElement = feldman_vss.group.secure_random_element()\r",
            "\r",
            "            # Use helper method to compute commitment\r",
            "            commitment: FieldElement = feldman_vss._compute_hash_commitment(b, r_b, i, \"blinding\")\r",
            "\r",
            "            # Store commitment and randomizer as tuple\r",
            "            feldman_blinding_commitments.append((commitment, r_b))\r",
            "    else:\r",
            "        # Create standard blinding commitments (just exponentiation)\r",
            "        feldman_blinding_commitments = [\r",
            "            feldman_vss.group.secure_exp(feldman_vss.generator, b) for b in blindings\r",
            "        ]\r",
            "\r",
            "    # Create Pedersen commitments to the blinding factors\r",
            "    pedersen_blinding_commitments: List[FieldElement] = pedersen_vss.commit_to_blinding_factors(blindings)\r",
            "\r",
            "    # Generate challenge using Fiat-Shamir transform\r",
            "    challenge_input: bytes = feldman_vss.group._enhanced_encode_for_hash(\r",
            "        feldman_vss.generator,\r",
            "        pedersen_vss.g,\r",
            "        pedersen_vss.h,\r",
            "        [fc[0] if isinstance(fc, tuple) else fc for fc in feldman_commitments],\r",
            "        pedersen_commitments,\r",
            "        [\r",
            "            fbc[0] if isinstance(fbc, tuple) else fbc\r",
            "            for fbc in feldman_blinding_commitments\r",
            "        ],\r",
            "        pedersen_blinding_commitments,\r",
            "    )\r",
            "\r",
            "    # Hash the challenge input\r",
            "    challenge_hash: bytes\r",
            "    if HAS_BLAKE3:\r",
            "        challenge_hash = blake3.blake3(challenge_input).digest()\r",
            "    else:\r",
            "        challenge_hash = hashlib.sha3_256(challenge_input).digest()\r",
            "\r",
            "    challenge: FieldElement = int.from_bytes(challenge_hash, \"big\") % feldman_vss.field.prime\r",
            "\r",
            "    # Compute responses\r",
            "    responses: List[FieldElement] = [\r",
            "        (b + challenge * c) % feldman_vss.field.prime\r",
            "        for b, c in zip(blindings, coefficients)\r",
            "    ]\r",
            "\r",
            "    # For hash-based commitments, include combined randomizers for verification\r",
            "    response_randomizers: Optional[List[FieldElement]] = None\r",
            "    if is_hash_based:\r",
            "        response_randomizers = []\r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Fix tuple unpacking - feldman_blinding_commitments are (commitment, randomizer) tuples\r",
            "            commitment_b, r_b = feldman_blinding_commitments[i]  # type: ignore\r",
            "            commitment_a, r_a = feldman_commitments[i]  # type: ignore\r",
            "            r_combined: FieldElement = (r_b + challenge * r_a) % feldman_vss.field.prime\r",
            "            response_randomizers.append(r_combined)\r",
            "\r",
            "    # Return the proof structure\r",
            "    proof: Dict[str, Any] = {\r",
            "        \"feldman_blinding_commitments\": feldman_blinding_commitments,\r",
            "        \"pedersen_blinding_commitments\": pedersen_blinding_commitments,\r",
            "        \"challenge\": int(challenge),\r",
            "        \"responses\": [int(r) for r in responses],\r",
            "    }\r",
            "\r",
            "    # Add response randomizers if using hash-based commitments\r",
            "    if response_randomizers is not None:\r",
            "        proof[\"response_randomizers\"] = [int(r) for r in response_randomizers]\r",
            "\r",
            "    return proof\r",
            "\r",
            "\r",
            "def verify_dual_commitments(\r",
            "    feldman_vss: FeldmanVSS, pedersen_vss: Any, feldman_commitments: CommitmentList, pedersen_commitments: List[FieldElement], proof: Dict[str, Any]\r",
            ") -> bool:\r",
            "    \"\"\"\r",
            "    Description:\r",
            "        Verify that the Feldman and Pedersen commitments commit to the same values\r",
            "        using constant-time operations to prevent timing side-channels.\r",
            "\r",
            "    Arguments:\r",
            "        feldman_vss: FeldmanVSS instance.\r",
            "        pedersen_vss: PedersenVSS instance.\r",
            "        feldman_commitments: Feldman commitments.\r",
            "        pedersen_commitments: Pedersen commitments.\r",
            "        proof: Proof data structure from create_dual_commitment_proof.\r",
            "\r",
            "    Outputs:\r",
            "        bool: True if verification succeeds, False otherwise.\r",
            "    \"\"\"\r",
            "    # Input validation\r",
            "    if not isinstance(feldman_vss, FeldmanVSS):\r",
            "        raise TypeError(\"feldman_vss must be a FeldmanVSS instance\")\r",
            "    if not hasattr(pedersen_vss, \"verify_response_equation\"):\r",
            "        raise TypeError(\"pedersen_vss must have a verify_response_equation method\")\r",
            "    if not isinstance(feldman_commitments, list) or not feldman_commitments:\r",
            "        raise TypeError(\"feldman_commitments must be a non-empty list\")\r",
            "    if not isinstance(pedersen_commitments, list) or not pedersen_commitments:\r",
            "        raise TypeError(\"pedersen_commitments must be a non-empty list\")\r",
            "    if not isinstance(proof, dict):\r",
            "        raise TypeError(\"proof must be a dictionary\")\r",
            "\r",
            "    # Add length consistency validation\r",
            "    if len(feldman_commitments) != len(pedersen_commitments):\r",
            "        raise ValueError(\r",
            "            \"feldman_commitments and pedersen_commitments must have the same length\"\r",
            "        )\r",
            "\r",
            "    # Required proof components\r",
            "    required_keys: List[str] = [\r",
            "        \"feldman_blinding_commitments\",\r",
            "        \"pedersen_blinding_commitments\",\r",
            "        \"challenge\",\r",
            "        \"responses\",\r",
            "    ]\r",
            "    if not all(key in proof for key in required_keys):\r",
            "        raise ValueError(\"Proof is missing required components\")\r",
            "\r",
            "    # Validate component lengths\r",
            "    if len(proof[\"responses\"]) != len(feldman_commitments):\r",
            "        raise ValueError(\"Number of responses must match number of commitments\")\r",
            "    if len(proof[\"feldman_blinding_commitments\"]) != len(feldman_commitments):\r",
            "        raise ValueError(\r",
            "            \"Number of feldman_blinding_commitments must match number of commitments\"\r",
            "        )\r",
            "    if len(proof[\"pedersen_blinding_commitments\"]) != len(pedersen_commitments):\r",
            "        raise ValueError(\r",
            "            \"Number of pedersen_blinding_commitments must match number of commitments\"\r",
            "        )\r",
            "\r",
            "    # Extract proof components\r",
            "    feldman_blinding_commitments: List[Union[Tuple[FieldElement, FieldElement], FieldElement]] = proof[\"feldman_blinding_commitments\"]\r",
            "    pedersen_blinding_commitments: List[FieldElement] = proof[\"pedersen_blinding_commitments\"]\r",
            "    challenge: FieldElement = proof[\"challenge\"]\r",
            "    responses: List[FieldElement] = proof[\"responses\"]\r",
            "    response_randomizers: Optional[List[FieldElement]] = proof.get(\"response_randomizers\", None)\r",
            "\r",
            "    # Check if we're using hash-based commitments for Feldman VSS\r",
            "    is_hash_based: bool = isinstance(feldman_commitments[0], tuple)\r",
            "\r",
            "    # Initialize validity flag for constant-time verification\r",
            "    all_valid: bool = True\r",
            "\r",
            "    # Also validate in constant-time that response_randomizers has the right length if needed\r",
            "    if is_hash_based:\r",
            "        all_valid &= response_randomizers is not None\r",
            "        randomizers_valid_len: bool = (\r",
            "            len(response_randomizers) == len(responses)\r",
            "            if response_randomizers is not None\r",
            "            else False\r",
            "        )\r",
            "        all_valid &= randomizers_valid_len\r",
            "\r",
            "    # First verify Pedersen commitments - these use the same approach regardless\r",
            "    i: int\r",
            "    for i in range(len(responses)):\r",
            "        # Verify using Pedersen VSS verification method\r",
            "        pedersen_valid: bool = pedersen_vss.verify_response_equation(\r",
            "            responses[i],\r",
            "            challenge,\r",
            "            pedersen_blinding_commitments[i],\r",
            "            pedersen_commitments[i],\r",
            "        )\r",
            "        all_valid &= pedersen_valid\r",
            "\r",
            "    # Then verify Feldman commitments\r",
            "    if is_hash_based:\r",
            "        # For hash-based commitments, verification requires validating hash output\r",
            "        \r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Instead of skipping iterations, always compute but conditionally update result\r",
            "            response_value: FieldElement = responses[i]\r",
            "            \r",
            "            # Use safe default if randomizers are invalid\r",
            "            r_combined: FieldElement = 0\r",
            "            if response_randomizers is not None and i < len(response_randomizers):\r",
            "                r_combined = response_randomizers[i]\r",
            "            \r",
            "            # Always compute both sides for constant-time behavior\r",
            "            computed: FieldElement = feldman_vss._compute_hash_commitment(\r",
            "                response_value, r_combined, i, \"response\"\r",
            "            )\r",
            "\r",
            "            # Calculate expected commitment\r",
            "            commitment_value: FieldElement = feldman_commitments[i][0]  # type: ignore\r",
            "            blinding_commitment_value: FieldElement = feldman_blinding_commitments[i][0]  # type: ignore\r",
            "            \r",
            "            expected: FieldElement = (\r",
            "                blinding_commitment_value + challenge * commitment_value\r",
            "            ) % feldman_vss.group.prime\r",
            "            \r",
            "            # Determine if this verification should count using constant-time operations\r",
            "            should_check: bool = (response_randomizers is not None and i < len(response_randomizers))\r",
            "            equality_result: bool = constant_time_compare(computed, expected)\r",
            "            \r",
            "            # Improved constant-time conditional update\r",
            "            mask = -int(should_check)  # Creates all 1s (for True) or all 0s (for False)\r",
            "            masked_result = equality_result & mask\r",
            "            all_valid &= (masked_result | ~mask)  # Will be equality_result when should_check is True, otherwise 1\r",
            "    else:\r",
            "        # Standard Feldman commitment verification\r",
            "        i: int\r",
            "        for i in range(len(responses)):\r",
            "            # Calculate left side: g^response[i]\r",
            "            left_side: FieldElement = feldman_vss.group.secure_exp(\r",
            "                feldman_vss.generator, responses[i]\r",
            "            )\r",
            "\r",
            "            # Calculate right side: blinding_commitment[i] * commitment[i]^challenge\r",
            "            commitment_term: FieldElement = feldman_vss.group.secure_exp(\r",
            "                feldman_commitments[i], challenge # type: ignore\r",
            "            )\r",
            "            right_side: FieldElement = feldman_vss.group.mul(\r",
            "                feldman_blinding_commitments[i], commitment_term # type: ignore\r",
            "            )\r",
            "\r",
            "            # Check equality using constant-time comparison\r",
            "            all_valid &= constant_time_compare(left_side, right_side)\r",
            "\r",
            "    return all_valid"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "549": [
                "constant_time_compare"
            ],
            "591": [
                "estimate_mpz_size"
            ],
            "778": [
                "check_memory_safety"
            ],
            "779": [
                "check_memory_safety"
            ],
            "780": [
                "check_memory_safety"
            ],
            "783": [
                "check_memory_safety"
            ],
            "784": [
                "check_memory_safety"
            ],
            "785": [
                "check_memory_safety"
            ],
            "1019": [
                "secure_redundant_execution"
            ],
            "1086": [
                "secure_redundant_execution"
            ],
            "1106": [
                "secure_redundant_execution"
            ]
        },
        "addLocation": []
    }
}