{
    "vllm/core/block/prefix_caching_block.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "             from 0 to num_blocks - 1."
            },
            "1": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     \"\"\""
            },
            "2": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 67,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+    # Note that we use 'None' as a string here instead of None because"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+    # as of Python 3.12, hash(None) returns a constant predictable value."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+    # This could possibly make it easier to find and exploit hash"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+    # collisions. 'None' as a string will be hashed differently per process,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+    # but consistently within the same process. This is the same as the"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+    # behavior of None prior to Python 3.12."
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+    _none_hash: int = hash('None')"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    # Implements Block.Factory."
            },
            "12": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "     def __init__("
            },
            "13": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "         self,"
            },
            "14": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "         num_blocks: int,"
            },
            "15": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 131,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "         self.metric_data = CacheMetricData()"
            },
            "17": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 133,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # Implements Block.Factory."
            },
            "19": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "     def _create_block("
            },
            "20": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "         self,"
            },
            "21": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "         prev_block: Optional[Block],"
            },
            "22": {
                "beforePatchRowNumber": 737,
                "afterPatchRowNumber": 745,
                "PatchRowcode": "             such as adapters that influence the block, apart from the token_ids."
            },
            "23": {
                "beforePatchRowNumber": 738,
                "afterPatchRowNumber": 746,
                "PatchRowcode": "     \"\"\""
            },
            "24": {
                "beforePatchRowNumber": 739,
                "afterPatchRowNumber": 747,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 748,
                "PatchRowcode": "+    # Note that we use 'None' as a string here instead of None because"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 749,
                "PatchRowcode": "+    # as of Python 3.12, hash(None) returns a constant predictable value."
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 750,
                "PatchRowcode": "+    # This could possibly make it easier to find and exploit hash"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 751,
                "PatchRowcode": "+    # collisions. 'None' as a string will be hashed differently per process,"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 752,
                "PatchRowcode": "+    # but consistently within the same process. This is the same as the"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 753,
                "PatchRowcode": "+    # behavior of None prior to Python 3.12."
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 754,
                "PatchRowcode": "+    _none_hash: int = hash('None')"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 755,
                "PatchRowcode": "+"
            },
            "33": {
                "beforePatchRowNumber": 740,
                "afterPatchRowNumber": 756,
                "PatchRowcode": "     def __init__("
            },
            "34": {
                "beforePatchRowNumber": 741,
                "afterPatchRowNumber": 757,
                "PatchRowcode": "         self,"
            },
            "35": {
                "beforePatchRowNumber": 742,
                "afterPatchRowNumber": 758,
                "PatchRowcode": "         prev_block: Optional[Block],"
            },
            "36": {
                "beforePatchRowNumber": 891,
                "afterPatchRowNumber": 907,
                "PatchRowcode": " "
            },
            "37": {
                "beforePatchRowNumber": 892,
                "afterPatchRowNumber": 908,
                "PatchRowcode": "         is_first_block = self._prev_block is None"
            },
            "38": {
                "beforePatchRowNumber": 893,
                "afterPatchRowNumber": 909,
                "PatchRowcode": "         prev_block_hash = ("
            },
            "39": {
                "beforePatchRowNumber": 894,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            None if is_first_block else"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 910,
                "PatchRowcode": "+            self._none_hash if is_first_block else"
            },
            "41": {
                "beforePatchRowNumber": 895,
                "afterPatchRowNumber": 911,
                "PatchRowcode": "             self._prev_block.content_hash  # type: ignore"
            },
            "42": {
                "beforePatchRowNumber": 896,
                "afterPatchRowNumber": 912,
                "PatchRowcode": "         )"
            },
            "43": {
                "beforePatchRowNumber": 897,
                "afterPatchRowNumber": 913,
                "PatchRowcode": " "
            },
            "44": {
                "beforePatchRowNumber": 898,
                "afterPatchRowNumber": 914,
                "PatchRowcode": "         # Previous block exists but does not yet have a hash."
            },
            "45": {
                "beforePatchRowNumber": 899,
                "afterPatchRowNumber": 915,
                "PatchRowcode": "         # Return no hash in this case."
            },
            "46": {
                "beforePatchRowNumber": 900,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if prev_block_hash is None and not is_first_block:"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 916,
                "PatchRowcode": "+        if prev_block_hash == self._none_hash and not is_first_block:"
            },
            "48": {
                "beforePatchRowNumber": 901,
                "afterPatchRowNumber": 917,
                "PatchRowcode": "             return None"
            },
            "49": {
                "beforePatchRowNumber": 902,
                "afterPatchRowNumber": 918,
                "PatchRowcode": " "
            },
            "50": {
                "beforePatchRowNumber": 903,
                "afterPatchRowNumber": 919,
                "PatchRowcode": "         self._cached_content_hash = PrefixCachingBlock.hash_block_tokens("
            },
            "51": {
                "beforePatchRowNumber": 907,
                "afterPatchRowNumber": 923,
                "PatchRowcode": "             extra_hash=self._extra_hash)"
            },
            "52": {
                "beforePatchRowNumber": 908,
                "afterPatchRowNumber": 924,
                "PatchRowcode": "         return self._cached_content_hash"
            },
            "53": {
                "beforePatchRowNumber": 909,
                "afterPatchRowNumber": 925,
                "PatchRowcode": " "
            },
            "54": {
                "beforePatchRowNumber": 910,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    @staticmethod"
            },
            "55": {
                "beforePatchRowNumber": 911,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def hash_block_tokens(is_first_block: bool,"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 926,
                "PatchRowcode": "+    @classmethod"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 927,
                "PatchRowcode": "+    def hash_block_tokens(cls,"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 928,
                "PatchRowcode": "+                          is_first_block: bool,"
            },
            "59": {
                "beforePatchRowNumber": 912,
                "afterPatchRowNumber": 929,
                "PatchRowcode": "                           prev_block_hash: Optional[int],"
            },
            "60": {
                "beforePatchRowNumber": 913,
                "afterPatchRowNumber": 930,
                "PatchRowcode": "                           cur_block_token_ids: List[int],"
            },
            "61": {
                "beforePatchRowNumber": 914,
                "afterPatchRowNumber": 931,
                "PatchRowcode": "                           extra_hash: Optional[int] = None) -> int:"
            },
            "62": {
                "beforePatchRowNumber": 929,
                "afterPatchRowNumber": 946,
                "PatchRowcode": "         Returns:"
            },
            "63": {
                "beforePatchRowNumber": 930,
                "afterPatchRowNumber": 947,
                "PatchRowcode": "         - int: The computed hash value for the block."
            },
            "64": {
                "beforePatchRowNumber": 931,
                "afterPatchRowNumber": 948,
                "PatchRowcode": "         \"\"\""
            },
            "65": {
                "beforePatchRowNumber": 932,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        assert (prev_block_hash is None) == is_first_block"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 949,
                "PatchRowcode": "+        if is_first_block and prev_block_hash is None:"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 950,
                "PatchRowcode": "+            prev_block_hash = cls._none_hash"
            },
            "68": {
                "beforePatchRowNumber": 933,
                "afterPatchRowNumber": 951,
                "PatchRowcode": "         return hash((is_first_block, prev_block_hash, *cur_block_token_ids,"
            },
            "69": {
                "beforePatchRowNumber": 934,
                "afterPatchRowNumber": 952,
                "PatchRowcode": "                      extra_hash))"
            },
            "70": {
                "beforePatchRowNumber": 935,
                "afterPatchRowNumber": 953,
                "PatchRowcode": " "
            },
            "71": {
                "beforePatchRowNumber": 949,
                "afterPatchRowNumber": 967,
                "PatchRowcode": "     cached block hashes in the allocator."
            },
            "72": {
                "beforePatchRowNumber": 950,
                "afterPatchRowNumber": 968,
                "PatchRowcode": "     \"\"\""
            },
            "73": {
                "beforePatchRowNumber": 951,
                "afterPatchRowNumber": 969,
                "PatchRowcode": " "
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 970,
                "PatchRowcode": "+    # Note that we use 'None' as a string here instead of None because"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 971,
                "PatchRowcode": "+    # as of Python 3.12, hash(None) returns a constant predictable value."
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 972,
                "PatchRowcode": "+    # This could possibly make it easier to find and exploit hash"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 973,
                "PatchRowcode": "+    # collisions. 'None' as a string will be hashed differently per process,"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 974,
                "PatchRowcode": "+    # but consistently within the same process. This is the same as the"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 975,
                "PatchRowcode": "+    # behavior of None prior to Python 3.12."
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 976,
                "PatchRowcode": "+    _none_hash: int = hash('None')"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 977,
                "PatchRowcode": "+"
            },
            "82": {
                "beforePatchRowNumber": 952,
                "afterPatchRowNumber": 978,
                "PatchRowcode": "     def __init__("
            },
            "83": {
                "beforePatchRowNumber": 953,
                "afterPatchRowNumber": 979,
                "PatchRowcode": "         self,"
            },
            "84": {
                "beforePatchRowNumber": 954,
                "afterPatchRowNumber": 980,
                "PatchRowcode": "         allocator: DeviceAwareBlockAllocator,"
            },
            "85": {
                "beforePatchRowNumber": 994,
                "afterPatchRowNumber": 1020,
                "PatchRowcode": "         # We need to know the hash of the previous block to compute the hash of"
            },
            "86": {
                "beforePatchRowNumber": 995,
                "afterPatchRowNumber": 1021,
                "PatchRowcode": "         # the current block so that blocks could be uniquely identified across"
            },
            "87": {
                "beforePatchRowNumber": 996,
                "afterPatchRowNumber": 1022,
                "PatchRowcode": "         # sequences of prefixes."
            },
            "88": {
                "beforePatchRowNumber": 997,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        prev_block_hash = (None if cur_num_blocks_recorded == 0 else"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1023,
                "PatchRowcode": "+        prev_block_hash = (self._none_hash if cur_num_blocks_recorded == 0 else"
            },
            "90": {
                "beforePatchRowNumber": 998,
                "afterPatchRowNumber": 1024,
                "PatchRowcode": "                            block_hashes_recorded[-1])"
            },
            "91": {
                "beforePatchRowNumber": 999,
                "afterPatchRowNumber": 1025,
                "PatchRowcode": "         # Only update the computed block hashes for the new blocks"
            },
            "92": {
                "beforePatchRowNumber": 1000,
                "afterPatchRowNumber": 1026,
                "PatchRowcode": "         for i in range(cur_num_blocks_recorded, num_computed_blocks):"
            },
            "93": {
                "beforePatchRowNumber": 1009,
                "afterPatchRowNumber": 1035,
                "PatchRowcode": "             # This has to be kept in sync with the allocator's hash"
            },
            "94": {
                "beforePatchRowNumber": 1010,
                "afterPatchRowNumber": 1036,
                "PatchRowcode": "             # calculation."
            },
            "95": {
                "beforePatchRowNumber": 1011,
                "afterPatchRowNumber": 1037,
                "PatchRowcode": "             block_hash = PrefixCachingBlock.hash_block_tokens("
            },
            "96": {
                "beforePatchRowNumber": 1012,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                is_first_block=prev_block_hash is None,"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1038,
                "PatchRowcode": "+                is_first_block=prev_block_hash == self._none_hash,"
            },
            "98": {
                "beforePatchRowNumber": 1013,
                "afterPatchRowNumber": 1039,
                "PatchRowcode": "                 prev_block_hash=prev_block_hash,"
            },
            "99": {
                "beforePatchRowNumber": 1014,
                "afterPatchRowNumber": 1040,
                "PatchRowcode": "                 cur_block_token_ids=block_token_ids,"
            },
            "100": {
                "beforePatchRowNumber": 1015,
                "afterPatchRowNumber": 1041,
                "PatchRowcode": "                 extra_hash=extra_hash,"
            }
        },
        "frontPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "\"\"\"Token blocks.\"\"\"",
            "import sys",
            "from bisect import bisect_left",
            "from os.path import commonprefix",
            "from typing import (Callable, Dict, FrozenSet, Iterable, List, Optional, Set,",
            "                    Tuple)",
            "",
            "from vllm.core.block.common import (CacheMetricData, CopyOnWriteTracker,",
            "                                    get_all_blocks_recursively)",
            "from vllm.core.block.interfaces import (Block, BlockAllocator, BlockId, Device,",
            "                                        DeviceAwareBlockAllocator)",
            "from vllm.core.block.naive_block import (BlockPool, NaiveBlock,",
            "                                         NaiveBlockAllocator)",
            "from vllm.core.evictor import EvictionPolicy, Evictor, make_evictor",
            "from vllm.logger import init_logger",
            "from vllm.sequence import Sequence",
            "",
            "PrefixHash = int",
            "",
            "# By default, we init our block access time as _DEFAULT_LAST_ACCESSED_TIME",
            "# so that if we find one block is still hold _DEFAULT_LAST_ACCESSED_TIME,",
            "# then we know this block hasn't been accessed yet.",
            "_DEFAULT_LAST_ACCESSED_TIME = -1",
            "",
            "logger = init_logger(__name__)",
            "",
            "",
            "class BlockTracker:",
            "    \"\"\"Used to track the status of a block inside the prefix caching allocator",
            "    \"\"\"",
            "    __slots__ = (\"active\", \"last_accessed\", \"computed\")",
            "",
            "    def reset(self):",
            "        self.last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME",
            "        self.computed: bool = False",
            "",
            "    def __init__(self):",
            "        self.active: bool = False",
            "        self.reset()",
            "",
            "    def enable(self):",
            "        assert not self.active",
            "        self.active = True",
            "        self.reset()",
            "",
            "    def disable(self):",
            "        assert self.active",
            "        self.active = False",
            "        self.reset()",
            "",
            "",
            "class PrefixCachingBlockAllocator(BlockAllocator):",
            "    \"\"\"A block allocator that implements prefix caching.",
            "",
            "    The PrefixCachingBlockAllocator maintains a cache of blocks based on their",
            "    content hash. It reuses blocks with the same content hash to avoid redundant",
            "    memory allocation. The allocator also supports copy-on-write operations.",
            "",
            "    Args:",
            "        num_blocks (int): The total number of blocks to manage.",
            "        block_size (int): The size of each block in tokens.",
            "        block_ids(Optional[Iterable[int]], optional): An optional iterable of",
            "            block IDs. If not provided, block IDs will be assigned sequentially",
            "            from 0 to num_blocks - 1.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        num_blocks: int,",
            "        block_size: int,",
            "        block_ids: Optional[Iterable[int]] = None,",
            "        eviction_policy: EvictionPolicy = EvictionPolicy.LRU,",
            "    ):",
            "        if block_ids is None:",
            "            block_ids = range(num_blocks)",
            "",
            "        self._block_size = block_size",
            "",
            "        # A mapping of prefix hash to block index. All blocks which have a",
            "        # prefix hash will be in this dict, even if they have refcount 0.",
            "        self._cached_blocks: Dict[PrefixHash, BlockId] = {}",
            "",
            "        # A list of immutable block IDs that have been touched by scheduler",
            "        # and should be marked as computed after an entire batch of sequences",
            "        # are scheduled.",
            "        self._touched_blocks: Set[BlockId] = set()",
            "",
            "        # Used to track status of each physical block id",
            "        self._block_tracker: Dict[BlockId, BlockTracker] = {}",
            "        for block_id in block_ids:",
            "            self._block_tracker[block_id] = BlockTracker()",
            "",
            "        # Pre-allocate \"num_blocks * extra_factor\" block objects.",
            "        # The \"* extra_factor\" is a buffer to allow more block objects",
            "        # than physical blocks",
            "        extra_factor = 4",
            "        self._block_pool = BlockPool(self._block_size, self._create_block,",
            "                                     self, num_blocks * extra_factor)",
            "",
            "        # An allocator for blocks that do not have prefix hashes.",
            "        self._hashless_allocator = NaiveBlockAllocator(",
            "            create_block=self._create_block,  # type: ignore",
            "            num_blocks=num_blocks,",
            "            block_size=block_size,",
            "            block_ids=block_ids,",
            "            block_pool=self._block_pool,  # Share block pool here",
            "        )",
            "",
            "        # Evitor used to maintain how we want to handle those computed blocks",
            "        # if we find memory pressure is high.",
            "        self.eviction_policy = eviction_policy",
            "        self.evictor: Evictor = make_evictor(self.eviction_policy)",
            "",
            "        # We share the refcounter between allocators. This allows us to promote",
            "        # blocks originally allocated in the hashless allocator to immutable",
            "        # blocks.",
            "        self._refcounter = self._hashless_allocator.refcounter",
            "",
            "        self._cow_tracker = CopyOnWriteTracker(",
            "            refcounter=self._refcounter.as_readonly())",
            "",
            "        self.metric_data = CacheMetricData()",
            "",
            "    # Implements Block.Factory.",
            "    def _create_block(",
            "        self,",
            "        prev_block: Optional[Block],",
            "        token_ids: List[int],",
            "        block_size: int,",
            "        allocator: BlockAllocator,",
            "        block_id: Optional[int] = None,",
            "        computed: bool = False,",
            "        extra_hash: Optional[int] = None,",
            "    ) -> Block:",
            "        # Bind block to self.",
            "        allocator = self",
            "",
            "        return PrefixCachingBlock(",
            "            prev_block=prev_block,",
            "            token_ids=token_ids,",
            "            block_size=block_size,",
            "            block_id=block_id,",
            "            allocator=allocator,",
            "            computed=computed,",
            "            extra_hash=extra_hash,",
            "        )",
            "",
            "    def allocate_immutable_block(self,",
            "                                 prev_block: Optional[Block],",
            "                                 token_ids: List[int],",
            "                                 extra_hash: Optional[int] = None,",
            "                                 device: Optional[Device] = None) -> Block:",
            "        \"\"\"Allocates an immutable block with the given token IDs, reusing cached",
            "        blocks if possible.",
            "",
            "        Args:",
            "            prev_block (Optional[Block]): The previous block in the sequence.",
            "            token_ids (List[int]): The token IDs to be stored in the block.",
            "",
            "        Returns:",
            "            Block: The allocated immutable block.",
            "        \"\"\"",
            "        assert device is None",
            "        assert_prefix_caching_block_or_none(prev_block)",
            "",
            "        # First, try to create a block that points to cached data",
            "        block = self._block_pool.init_block(prev_block=prev_block,",
            "                                            token_ids=token_ids,",
            "                                            block_size=self._block_size,",
            "                                            physical_block_id=None,",
            "                                            extra_hash=extra_hash)",
            "        assert block.content_hash is not None",
            "",
            "        cached_block_id = self._cached_blocks.get(block.content_hash, None)",
            "        if cached_block_id is not None:",
            "            self.metric_data.query(hit=True)",
            "            block.block_id = cached_block_id",
            "            self._incr_refcount_cached_block(block)",
            "            return block",
            "        self.metric_data.query(hit=False)",
            "        self._block_pool.free_block(block)",
            "",
            "        # No cached block => Allocate a new block",
            "        block = self.allocate_mutable_block(prev_block, extra_hash=extra_hash)",
            "        block.append_token_ids(token_ids)",
            "        return block",
            "",
            "    def allocate_immutable_blocks(",
            "            self,",
            "            prev_block: Optional[Block],",
            "            block_token_ids: List[List[int]],",
            "            extra_hash: Optional[int] = None,",
            "            device: Optional[Device] = None) -> List[Block]:",
            "        blocks = []",
            "        for token_ids in block_token_ids:",
            "            prev_block = self.allocate_immutable_block(prev_block=prev_block,",
            "                                                       token_ids=token_ids,",
            "                                                       device=device,",
            "                                                       extra_hash=extra_hash)",
            "            blocks.append(prev_block)",
            "        return blocks",
            "",
            "    def allocate_mutable_block(self,",
            "                               prev_block: Optional[Block],",
            "                               extra_hash: Optional[int] = None,",
            "                               device: Optional[Device] = None) -> Block:",
            "        \"\"\"Allocates a mutable block. If there are no free blocks, this will",
            "        evict unused cached blocks.",
            "",
            "        Args:",
            "            prev_block (Block): The previous block in the sequence.",
            "                None is not allowed unlike it is super class.",
            "",
            "        Returns:",
            "            Block: The allocated mutable block.",
            "        \"\"\"",
            "        assert device is None",
            "        assert_prefix_caching_block_or_none(prev_block)",
            "",
            "        block_id = self._allocate_block_id()",
            "        block = self._block_pool.init_block(prev_block=prev_block,",
            "                                            token_ids=[],",
            "                                            block_size=self._block_size,",
            "                                            physical_block_id=block_id,",
            "                                            extra_hash=extra_hash)",
            "        assert not block.computed",
            "        assert block.content_hash is None",
            "        return block",
            "",
            "    def _incr_refcount_cached_block(self, block: Block) -> None:",
            "        # Set this block to be \"computed\" since it is pointing to a",
            "        # cached block id (which was already computed)",
            "        block.computed = True",
            "",
            "        block_id = block.block_id",
            "        assert block_id is not None",
            "",
            "        refcount = self._refcounter.incr(block_id)",
            "        if refcount == 1:",
            "            # In case a cached block was evicted, restore its tracking",
            "            if block_id in self.evictor:",
            "                self.evictor.remove(block_id)",
            "",
            "            self._track_block_id(block_id, computed=True)",
            "",
            "    def _decr_refcount_cached_block(self, block: Block) -> None:",
            "        # Ensure this is immutable/cached block",
            "        assert block.content_hash is not None",
            "",
            "        block_id = block.block_id",
            "        assert block_id is not None",
            "",
            "        refcount = self._refcounter.decr(block_id)",
            "        if refcount > 0:",
            "            block.block_id = None",
            "            return",
            "        else:",
            "            assert refcount == 0",
            "",
            "        # No longer used",
            "        assert block.content_hash in self._cached_blocks",
            "",
            "        # Add the cached block to the evictor",
            "        # (This keeps the cached block around so it can be reused)",
            "        self.evictor.add(block_id, block.content_hash, block.num_tokens_total,",
            "                         self._block_tracker[block_id].last_accessed)",
            "",
            "        # Stop tracking the block",
            "        self._untrack_block_id(block_id)",
            "",
            "        block.block_id = None",
            "",
            "    def _decr_refcount_hashless_block(self, block: Block) -> None:",
            "        block_id = block.block_id",
            "        assert block_id is not None",
            "",
            "        # We may have a fork case where block is shared,",
            "        # in which case, we cannot remove it from tracking",
            "        refcount = self._refcounter.get(block_id)",
            "        if refcount == 1:",
            "            self._untrack_block_id(block_id)",
            "",
            "        # Decrement refcount of the block_id, but do not free the block object",
            "        # itself (will be handled by the caller)",
            "        self._hashless_allocator.free(block, keep_block_object=True)",
            "",
            "    def _allocate_block_id(self) -> BlockId:",
            "        \"\"\"First tries to allocate a block id from the hashless allocator,",
            "        and if there are no blocks, then tries to evict an unused cached block.",
            "        \"\"\"",
            "        hashless_block_id = self._maybe_allocate_hashless_block_id()",
            "        if hashless_block_id is not None:",
            "            return hashless_block_id",
            "",
            "        evicted_block_id = self._maybe_allocate_evicted_block_id()",
            "        if evicted_block_id is not None:",
            "            return evicted_block_id",
            "",
            "        # No block available in hashless allocator, nor in unused cache blocks.",
            "        raise BlockAllocator.NoFreeBlocksError()",
            "",
            "    def _maybe_allocate_hashless_block_id(self) -> Optional[BlockId]:",
            "        try:",
            "            # Allocate mutable block and extract its block_id",
            "            block = self._hashless_allocator.allocate_mutable_block(",
            "                prev_block=None)",
            "            block_id = block.block_id",
            "            self._block_pool.free_block(block)",
            "",
            "            self._track_block_id(block_id, computed=False)",
            "            return block_id",
            "        except BlockAllocator.NoFreeBlocksError:",
            "            return None",
            "",
            "    def _maybe_allocate_evicted_block_id(self) -> Optional[BlockId]:",
            "        if self.evictor.num_blocks == 0:",
            "            return None",
            "",
            "        # Here we get an evicted block, which is only added",
            "        # into evictor if its ref counter is 0",
            "        # and since its content would be changed, we need",
            "        # to remove it from _cached_blocks's tracking list",
            "        block_id, content_hash_to_evict = self.evictor.evict()",
            "",
            "        # Sanity checks",
            "        assert content_hash_to_evict in self._cached_blocks",
            "        _block_id = self._cached_blocks[content_hash_to_evict]",
            "        assert self._refcounter.get(_block_id) == 0",
            "        assert _block_id == block_id",
            "",
            "        self._cached_blocks.pop(content_hash_to_evict)",
            "",
            "        self._refcounter.incr(block_id)",
            "        self._track_block_id(block_id, computed=False)",
            "",
            "        return block_id",
            "",
            "    def _free_block_id(self, block: Block) -> None:",
            "        \"\"\"Decrements the refcount of the block. The block may be in two ",
            "        possible states: (1) immutable/cached or (2) mutable/hashless. ",
            "        In the first case, the refcount is decremented directly and the block",
            "        may be possibly added to the evictor. In other case, hashless ",
            "        allocator free(..) with keep_block_object=True is called to only free",
            "        the block id (since the block object may be reused by the caller)",
            "        \"\"\"",
            "        block_id = block.block_id",
            "        assert block_id is not None, \"Freeing unallocated block is undefined\"",
            "",
            "        if block.content_hash is not None:",
            "            # Immutable: This type of block is always cached, and we want to",
            "            # keep it in the evictor for future reuse",
            "            self._decr_refcount_cached_block(block)",
            "        else:",
            "            # Mutable: This type of block is not cached, so we release it",
            "            # directly to the hashless allocator",
            "            self._decr_refcount_hashless_block(block)",
            "",
            "        assert block.block_id is None",
            "",
            "    def free(self, block: Block, keep_block_object: bool = False) -> None:",
            "        \"\"\"Release the block (look at free_block_id(..) docs)",
            "        \"\"\"",
            "        # Release the physical block index",
            "        self._free_block_id(block)",
            "",
            "        # Release the block object to the pool",
            "        if not keep_block_object:",
            "            self._block_pool.free_block(block)",
            "",
            "    def fork(self, last_block: Block) -> List[Block]:",
            "        \"\"\"Creates a new sequence of blocks that shares the same underlying",
            "        memory as the original sequence.",
            "",
            "        Args:",
            "            last_block (Block): The last block in the original sequence.",
            "",
            "        Returns:",
            "            List[Block]: The new sequence of blocks that shares the same memory",
            "                as the original sequence.",
            "        \"\"\"",
            "        source_blocks = get_all_blocks_recursively(last_block)",
            "",
            "        forked_blocks: List[Block] = []",
            "        prev_block = None",
            "        for block in source_blocks:",
            "            block_id = block.block_id",
            "            assert block_id is not None",
            "",
            "            refcount = self._refcounter.incr(block_id)",
            "            assert refcount != 1, \"can't fork free'd block_id = {}\".format(",
            "                block_id)",
            "",
            "            forked_block = self._block_pool.init_block(",
            "                prev_block=prev_block,",
            "                token_ids=block.token_ids,",
            "                block_size=self._block_size,",
            "                physical_block_id=block_id,",
            "                extra_hash=block.extra_hash)",
            "",
            "            forked_blocks.append(forked_block)",
            "            prev_block = forked_blocks[-1]",
            "",
            "        return forked_blocks",
            "",
            "    def get_num_free_blocks(self, device: Optional[Device] = None) -> int:",
            "        assert device is None",
            "        # The number of free blocks is the number of hashless free blocks",
            "        # plus the number of blocks evictor could free from its list.",
            "        return self._hashless_allocator.get_num_free_blocks(",
            "        ) + self.evictor.num_blocks",
            "",
            "    def get_num_total_blocks(self) -> int:",
            "        return self._hashless_allocator.get_num_total_blocks()",
            "",
            "    def get_physical_block_id(self, absolute_id: int) -> int:",
            "        \"\"\"Returns the zero-offset block id on certain block allocator",
            "        given the absolute block id.",
            "",
            "        Args:",
            "            absolute_id (int): The absolute block id for the block ",
            "                in whole allocator.",
            "",
            "        Returns:",
            "            int: The rzero-offset block id on certain device.",
            "        \"\"\"",
            "        return sorted(self.all_block_ids).index(absolute_id)",
            "",
            "    @property",
            "    def all_block_ids(self) -> FrozenSet[int]:",
            "        return self._hashless_allocator.all_block_ids",
            "",
            "    def get_prefix_cache_hit_rate(self) -> float:",
            "        return self.metric_data.get_hit_rate()",
            "",
            "    def reset_prefix_cache(self) -> bool:",
            "        \"\"\"Reset prefix cache. This function may be used in RLHF",
            "        flows to invalid prefix caching after the weights are updated,",
            "        or used for resetting prefix caching status for benchmarking.",
            "",
            "        Returns:",
            "            bool: True if the prefix cache is successfully reset,",
            "            False otherwise.",
            "        \"\"\"",
            "        num_used_blocks = (self.get_num_total_blocks() -",
            "                           self.get_num_free_blocks())",
            "        if num_used_blocks > 0:",
            "            logger.warning(",
            "                \"Failed to reset prefix cache because some \"",
            "                \"blocks (%d) are not freed yet\", num_used_blocks)",
            "            return False",
            "",
            "        # Free all blocks in the evictor.",
            "        while (block_id :=",
            "               self._maybe_allocate_evicted_block_id()) is not None:",
            "            self._hashless_allocator.free_block_id(block_id)",
            "",
            "        # Should not have any cached blocks because all blocks are evicted.",
            "        assert not self._cached_blocks",
            "",
            "        # Reset the evictor.",
            "        self.evictor = make_evictor(self.eviction_policy)",
            "",
            "        # Reset the block tracker.",
            "        for block_id in self._block_tracker:",
            "            self._block_tracker[block_id] = BlockTracker()",
            "",
            "        # Reset the metrics.",
            "        self.metric_data = CacheMetricData()",
            "",
            "        logger.info(\"Successfully reset prefix cache\")",
            "        return True",
            "",
            "    def is_block_cached(self, block: Block) -> bool:",
            "        assert block.content_hash is not None",
            "        return block.content_hash in self._cached_blocks",
            "",
            "    def promote_to_immutable_block(self, block: Block) -> BlockId:",
            "        \"\"\"Once a mutable block is full, it can be promoted to an immutable",
            "        block. This means that its content can be referenced by future blocks",
            "        having the same prefix.",
            "",
            "        Note that if we already have a cached block with the same content, we",
            "        will replace the newly-promoted block's mapping with the existing cached",
            "        block id.",
            "",
            "        Args:",
            "            block: The mutable block to be promoted.",
            "",
            "        Returns:",
            "            BlockId: Either the original block index, or the block index of",
            "                the previously cached block matching the same content.",
            "        \"\"\"",
            "        # Ensure block can be promoted",
            "        assert block.content_hash is not None",
            "        assert block.block_id is not None",
            "        assert self._refcounter.get(block.block_id) > 0",
            "",
            "        if block.content_hash not in self._cached_blocks:",
            "            # No cached content hash => Set this block as cached.",
            "            # Note that this block cannot be marked as computed yet",
            "            # because other sequences in the same batch cannot reuse",
            "            # this block.",
            "            self._cached_blocks[block.content_hash] = block.block_id",
            "            # Mark this block as touched so that it can be marked as",
            "            # computed after the entire batch of sequences are scheduled.",
            "            self._touched_blocks.add(block.block_id)",
            "            return block.block_id",
            "",
            "        # Reuse the cached content hash",
            "        self._decr_refcount_hashless_block(block)",
            "        block.block_id = self._cached_blocks[block.content_hash]",
            "",
            "        # Increment refcount of the cached block and (possibly) restore",
            "        # it from the evictor.",
            "        # Note that in this case, the block is marked as computed",
            "        self._incr_refcount_cached_block(block)",
            "",
            "        return block.block_id",
            "",
            "    def cow_block_if_not_appendable(self, block: Block) -> BlockId:",
            "        \"\"\"Performs a copy-on-write operation on the given block if it is not",
            "        appendable.",
            "",
            "        Args:",
            "            block (Block): The block to check for copy-on-write.",
            "",
            "        Returns:",
            "            BlockId: The block index of the new block if a copy-on-write ",
            "                operation was performed, or the original block index if",
            "                no copy-on-write was necessary.",
            "        \"\"\"",
            "        src_block_id = block.block_id",
            "        assert src_block_id is not None",
            "",
            "        if self._cow_tracker.is_appendable(block):",
            "            return src_block_id",
            "",
            "        self._free_block_id(block)",
            "        trg_block_id = self._allocate_block_id()",
            "",
            "        self._cow_tracker.record_cow(src_block_id, trg_block_id)",
            "",
            "        return trg_block_id",
            "",
            "    def clear_copy_on_writes(self) -> List[Tuple[BlockId, BlockId]]:",
            "        \"\"\"Returns the copy-on-write source->destination mapping and clears it.",
            "",
            "        Returns:",
            "            List[Tuple[BlockId, BlockId]]: A list mapping source",
            "                block indices to destination block indices.",
            "        \"\"\"",
            "        return self._cow_tracker.clear_cows()",
            "",
            "    def mark_blocks_as_accessed(self, block_ids: List[int],",
            "                                now: float) -> None:",
            "        \"\"\"Mark blocks as accessed, used in prefix caching.",
            "",
            "        If the block is added into evictor, we need to update corresponding",
            "        info in evictor's metadata.",
            "        \"\"\"",
            "",
            "        for block_id in block_ids:",
            "            if self._block_tracker[block_id].active:",
            "                self._block_tracker[block_id].last_accessed = now",
            "            elif block_id in self.evictor:",
            "                self.evictor.update(block_id, now)",
            "            else:",
            "                raise ValueError(",
            "                    \"Mark block as accessed which is not belonged to GPU\")",
            "",
            "    def mark_blocks_as_computed(self, block_ids: List[int]) -> None:",
            "        # Mark all touched blocks as computed.",
            "        for block_id in self._touched_blocks:",
            "            self._block_tracker[block_id].computed = True",
            "        self._touched_blocks.clear()",
            "",
            "    def _track_block_id(self, block_id: Optional[BlockId],",
            "                        computed: bool) -> None:",
            "        assert block_id is not None",
            "        self._block_tracker[block_id].enable()",
            "        self._block_tracker[block_id].computed = computed",
            "",
            "    def _untrack_block_id(self, block_id: Optional[BlockId]) -> None:",
            "        assert block_id is not None",
            "        self._block_tracker[block_id].disable()",
            "",
            "    def block_is_computed(self, block_id: int) -> bool:",
            "        if self._block_tracker[block_id].active:",
            "            return self._block_tracker[block_id].computed",
            "        else:",
            "            return block_id in self.evictor",
            "",
            "    def get_common_computed_block_ids(",
            "            self, computed_seq_block_ids: List[List[int]]) -> List[int]:",
            "        \"\"\"Return the block ids that are common for a given sequence group.",
            "",
            "        Only those blocks that are immutable and already be marked",
            "        compyted would be taken consideration.",
            "        \"\"\"",
            "",
            "        # NOTE We exclude the last block to avoid the case where the entire",
            "        # prompt is cached. This would cause erroneous behavior in model",
            "        # runner.",
            "",
            "        # It returns a list of int although type annotation says list of string.",
            "        if len(computed_seq_block_ids) == 1:",
            "            return computed_seq_block_ids[0]",
            "",
            "        return commonprefix([",
            "            ids for ids in computed_seq_block_ids  # type: ignore",
            "            if ids",
            "        ])",
            "",
            "    def get_num_full_blocks_touched(self, blocks: List[Block]) -> int:",
            "        \"\"\"Returns the number of full blocks that will be touched by",
            "        swapping in/out.",
            "",
            "        Args:",
            "            blocks: List of blocks to be swapped.",
            "        Returns:",
            "            int: the number of full blocks that will be touched by",
            "                swapping in/out the given blocks. Non full blocks are ignored",
            "                when deciding the number of blocks to touch.",
            "        \"\"\"",
            "        num_touched_blocks: int = 0",
            "        for block in blocks:",
            "            # If the block has a match in the cache and the cached",
            "            # block is not referenced, then we still count it as a",
            "            # touched block",
            "            if block.is_full and (not self.is_block_cached(block) or \\",
            "                (block.content_hash is not None and \\",
            "                self._cached_blocks[block.content_hash] in \\",
            "                        self.evictor)):",
            "                num_touched_blocks += 1",
            "        return num_touched_blocks",
            "",
            "    def swap_out(self, blocks: List[Block]) -> None:",
            "        \"\"\"Execute the swap out actions. Basically just free the ",
            "        given blocks.",
            "",
            "        Args:",
            "            blocks: List of blocks to be swapped out.",
            "        \"\"\"",
            "        for block in blocks:",
            "            self._free_block_id(block)",
            "",
            "    def swap_in(self, blocks: List[Block]) -> None:",
            "        \"\"\"Execute the swap in actions. Change the block id from ",
            "        old allocator to current allocator for each block to finish ",
            "        the block table update. ",
            "",
            "        Args:",
            "            blocks: List of blocks to be swapped in.",
            "        \"\"\"",
            "        for block in blocks:",
            "            # Here we allocate either immutable or mutable block and then",
            "            # extract its block_id. Note that the block object is released",
            "            # and the block_id is assigned to \"block\" to allow reusing the",
            "            # existing \"block\" object",
            "            if block.is_full:",
            "                tmp_block = self.allocate_immutable_block(",
            "                    prev_block=block.prev_block,",
            "                    token_ids=block.token_ids,",
            "                    extra_hash=block.extra_hash)",
            "            else:",
            "                tmp_block = self.allocate_mutable_block(",
            "                    prev_block=block.prev_block, extra_hash=block.extra_hash)",
            "                tmp_block.append_token_ids(block.token_ids)",
            "",
            "            block_id = tmp_block.block_id",
            "            self._block_pool.free_block(tmp_block)",
            "",
            "            block.block_id = block_id  # Assign block_id",
            "",
            "    def find_cached_blocks_prefix(self, block_hashes: List[int]) -> List[int]:",
            "        \"\"\"",
            "        Given a list of block hashes, return the prefix of the block hashes that",
            "        are all cached.",
            "",
            "        Since a block's block hash includes the hashes of all previous blocks,",
            "        and we only allocate/deallocate blocks in the entire sequence, so if a",
            "        block is cached, then all previous blocks are also cached. With this",
            "        property, we can use binary search to find the prefix of cached blocks.",
            "",
            "        Args:",
            "            block_hashes (List[int]): The list of block hashes.",
            "",
            "        Returns:",
            "            List[int]: The prefix of the `block_hashes` that are cached.",
            "        \"\"\"",
            "",
            "        def _block_is_cached(block_hash: PrefixHash) -> bool:",
            "            if block_hash not in self._cached_blocks:",
            "                return False",
            "",
            "            cached_block_id = self._cached_blocks[block_hash]",
            "            # We only consider the blocks that are marked as computed.",
            "            return self.block_is_computed(cached_block_id)",
            "",
            "        def _bisect_left(a, x, key: Callable[[PrefixHash], bool]) -> int:",
            "",
            "            # python <= 3.10 don't have the key argument",
            "            if sys.version_info < (3, 10):",
            "                a = [key(e) for e in a]",
            "                return bisect_left(a, x)",
            "            else:",
            "                return bisect_left(a, x, key=key)",
            "",
            "        # Look for the first block that's not cached, and returns the prefix",
            "        # i.e. blocks that are cached.",
            "        idx = _bisect_left(block_hashes,",
            "                           True,",
            "                           key=lambda x: not _block_is_cached(x))",
            "        return block_hashes[:idx]",
            "",
            "",
            "class PrefixCachingBlock(Block):",
            "    \"\"\"A block implementation that supports prefix caching.",
            "",
            "    The PrefixCachingBlock class represents a block of token IDs with prefix",
            "    caching capabilities. It wraps a NaiveBlock internally and provides",
            "    additional functionality for content hashing and promoting immutable blocks",
            "    with the prefix caching allocator.",
            "",
            "    Args:",
            "        prev_block (Optional[PrefixCachingBlock]): The previous block in the",
            "            sequence.",
            "        token_ids (List[int]): The initial token IDs to be stored in the block.",
            "        block_size (int): The maximum number of token IDs that can be stored in",
            "            the block.",
            "        allocator (BlockAllocator): The prefix",
            "            caching block allocator associated with this block.",
            "        block_id (Optional[int], optional): The physical block index",
            "            of this block. Defaults to None.",
            "        extra_hash (Optional[int]): The hash value of additional factors",
            "            such as adapters that influence the block, apart from the token_ids.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        prev_block: Optional[Block],",
            "        token_ids: List[int],",
            "        block_size: int,",
            "        allocator: BlockAllocator,",
            "        block_id: Optional[int] = None,",
            "        computed: bool = False,",
            "        extra_hash: Optional[int] = None,",
            "    ):",
            "        assert isinstance(allocator, PrefixCachingBlockAllocator), (",
            "            \"Currently this class is only tested with \"",
            "            \"PrefixCachingBlockAllocator. Got instead allocator = {}\".format(",
            "                allocator))",
            "        assert_prefix_caching_block_or_none(prev_block)",
            "",
            "        self._prev_block = prev_block",
            "        self._cached_content_hash: Optional[int] = None",
            "        self._cached_num_tokens_total: int = 0",
            "        self._allocator = allocator",
            "        self._last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME",
            "        self._computed = computed",
            "        self._extra_hash = extra_hash",
            "",
            "        # On the first time, we create the block object, and next we only",
            "        # reinitialize it",
            "        if hasattr(self, \"_block\"):",
            "            self._block.__init__(  # type: ignore[has-type]",
            "                prev_block=prev_block,",
            "                token_ids=token_ids,",
            "                block_size=block_size,",
            "                block_id=block_id,",
            "                allocator=self._allocator)",
            "        else:",
            "            self._block = NaiveBlock(prev_block=prev_block,",
            "                                     token_ids=token_ids,",
            "                                     block_size=block_size,",
            "                                     block_id=block_id,",
            "                                     allocator=self._allocator)",
            "",
            "        self._update_num_tokens_total()",
            "",
            "    def _update_num_tokens_total(self):",
            "        \"\"\"Incrementally computes the number of tokens that there is",
            "        till the current block (included)",
            "        \"\"\"",
            "        res = 0",
            "",
            "        # Add all previous blocks",
            "        if self._prev_block is not None:",
            "            res += self._prev_block.num_tokens_total",
            "",
            "        # Add current block",
            "        res += len(self.token_ids)",
            "",
            "        self._cached_num_tokens_total = res",
            "",
            "    @property",
            "    def computed(self) -> bool:",
            "        return self._computed",
            "",
            "    @computed.setter",
            "    def computed(self, value) -> None:",
            "        self._computed = value",
            "",
            "    @property",
            "    def last_accessed(self) -> float:",
            "        return self._last_accessed",
            "",
            "    @last_accessed.setter",
            "    def last_accessed(self, last_accessed_ts: float):",
            "        self._last_accessed = last_accessed_ts",
            "",
            "    def append_token_ids(self, token_ids: List[int]) -> None:",
            "        \"\"\"Appends the given token IDs to the block and registers the block as",
            "        immutable if the block becomes full.",
            "",
            "        Args:",
            "            token_ids (List[int]): The token IDs to be appended to the block.",
            "        \"\"\"",
            "        # Ensure this is mutable block (not promoted)",
            "        assert self.content_hash is None",
            "        assert not self.computed",
            "",
            "        if len(token_ids) == 0:",
            "            return",
            "",
            "        # Ensure there are input tokens",
            "        assert token_ids, \"Got token_ids = {}\".format(token_ids)",
            "",
            "        # Naive block handles CoW.",
            "        self._block.append_token_ids(token_ids)",
            "        self._update_num_tokens_total()",
            "",
            "        # If the content hash is present, then the block can be made immutable.",
            "        # Register ourselves with the allocator, potentially replacing the",
            "        # physical block index.",
            "        if self.content_hash is not None:",
            "            self.block_id = self._allocator.promote_to_immutable_block(self)",
            "",
            "    @property",
            "    def block_id(self) -> Optional[int]:",
            "        return self._block.block_id",
            "",
            "    @block_id.setter",
            "    def block_id(self, value) -> None:",
            "        self._block.block_id = value",
            "",
            "    @property",
            "    def is_full(self) -> bool:",
            "        return self._block.is_full",
            "",
            "    @property",
            "    def num_empty_slots(self) -> int:",
            "        return self._block.num_empty_slots",
            "",
            "    @property",
            "    def num_tokens_total(self) -> int:",
            "        return self._cached_num_tokens_total",
            "",
            "    @property",
            "    def block_size(self) -> int:",
            "        return self._block.block_size",
            "",
            "    @property",
            "    def token_ids(self) -> List[int]:",
            "        return self._block.token_ids",
            "",
            "    @property",
            "    def prev_block(self) -> Optional[Block]:",
            "        return self._prev_block",
            "",
            "    @property",
            "    def extra_hash(self) -> Optional[int]:",
            "        return self._extra_hash",
            "",
            "    @property",
            "    def content_hash(self) -> Optional[int]:",
            "        \"\"\"Return the content-based hash of the current block, or None if it is",
            "        not yet defined.",
            "",
            "        For the content-based hash to be defined, the current block must be",
            "        full.",
            "        \"\"\"",
            "        # If the hash is already computed, return it.",
            "        if self._cached_content_hash is not None:",
            "            return self._cached_content_hash",
            "",
            "        # We cannot compute a hash for the current block because it is not full.",
            "        if not self.is_full:",
            "            return None",
            "",
            "        is_first_block = self._prev_block is None",
            "        prev_block_hash = (",
            "            None if is_first_block else",
            "            self._prev_block.content_hash  # type: ignore",
            "        )",
            "",
            "        # Previous block exists but does not yet have a hash.",
            "        # Return no hash in this case.",
            "        if prev_block_hash is None and not is_first_block:",
            "            return None",
            "",
            "        self._cached_content_hash = PrefixCachingBlock.hash_block_tokens(",
            "            is_first_block,",
            "            prev_block_hash,",
            "            cur_block_token_ids=self.token_ids,",
            "            extra_hash=self._extra_hash)",
            "        return self._cached_content_hash",
            "",
            "    @staticmethod",
            "    def hash_block_tokens(is_first_block: bool,",
            "                          prev_block_hash: Optional[int],",
            "                          cur_block_token_ids: List[int],",
            "                          extra_hash: Optional[int] = None) -> int:",
            "        \"\"\"Computes a hash value corresponding to the contents of a block and",
            "        the contents of the preceding block(s). The hash value is used for",
            "        prefix caching.",
            "",
            "        Parameters:",
            "        - is_first_block (bool): A flag indicating if the block is the first in",
            "            the sequence.",
            "        - prev_block_hash (Optional[int]): The hash of the previous block. None",
            "            if this is the first block.",
            "        - cur_block_token_ids (List[int]): A list of token ids in the current",
            "            block. The current block is assumed to be full.",
            "        - extra_hash (Optional[int]): The hash value of additional factors",
            "            such as adapters that influence the block, apart from the token_ids.",
            "",
            "        Returns:",
            "        - int: The computed hash value for the block.",
            "        \"\"\"",
            "        assert (prev_block_hash is None) == is_first_block",
            "        return hash((is_first_block, prev_block_hash, *cur_block_token_ids,",
            "                     extra_hash))",
            "",
            "",
            "class ComputedBlocksTracker:",
            "    \"\"\"",
            "    Tracks the computed blocks for each sequence.",
            "",
            "    Internally, it maintains a map from sequence id to the list of block hashes",
            "    for the sequence. We cache the hashes of the full blocks for each sequence,",
            "    and make sure the hash is calculated in the same way as the allocator.",
            "    When a sequence is being decoded, we also update the sequence's hash",
            "    accordingly and incrementally.",
            "",
            "    From the sequence hash, with prefix caching enabled, we could also calculate",
            "    the number of cached tokens for the sequence by looking up the number of",
            "    cached block hashes in the allocator.",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        allocator: DeviceAwareBlockAllocator,",
            "        block_size: int,",
            "        enable_caching: bool,",
            "    ):",
            "        self._allocator = allocator",
            "        self._block_size = block_size",
            "        self._enable_caching = enable_caching",
            "",
            "        # A map from seq_id to the list of block hashes for the",
            "        # sequence. This is so that we don't have to recompute the block hashes",
            "        # for the sequence when we need to check if the sequence is cached.",
            "        # Note a block that's not full will not have its hash calculated and",
            "        # recorded.",
            "        self._seq_id_to_blocks_hashes: Dict[int, List[int]] = {}",
            "",
            "        # A map from seq_id to the number of tokens that are cached for the",
            "        # sequence.",
            "        # We need this so that a sequence in continuous prefill doesn't",
            "        # accidentally see its cached token count change. See comments in",
            "        # `get_num_cached_tokens` for more details.",
            "        self._seq_id_to_num_tokens_computed: Dict[int, int] = {}",
            "",
            "    def _update_seq_hashes(self, seq: Sequence) -> None:",
            "        \"\"\"Incrementally update the sequence's block hashes and record them.\"\"\"",
            "        assert self._enable_caching",
            "",
            "        block_hashes_recorded = self._seq_id_to_blocks_hashes.get(",
            "            seq.seq_id, [])",
            "        cur_num_blocks_recorded = len(block_hashes_recorded)",
            "        token_ids = seq.get_token_ids()",
            "        assert len(token_ids) >= cur_num_blocks_recorded * self._block_size, (",
            "            f\"The sequence has {len(token_ids)} tokens, but\"",
            "            f\" already recorded {cur_num_blocks_recorded} blocks. \"",
            "            \"This should not happen since we assume blocks are \"",
            "            \"only appended other than recomputation. When the sequence is \"",
            "            \"recomputed, we should have removed the info of the old blocks.\")",
            "        # Update the computed block hashes for the sequence. Since only full",
            "        # blocks are considered as \"computed\", we take floor here.",
            "        num_computed_blocks = len(token_ids) // self._block_size",
            "",
            "        # We need to know the hash of the previous block to compute the hash of",
            "        # the current block so that blocks could be uniquely identified across",
            "        # sequences of prefixes.",
            "        prev_block_hash = (None if cur_num_blocks_recorded == 0 else",
            "                           block_hashes_recorded[-1])",
            "        # Only update the computed block hashes for the new blocks",
            "        for i in range(cur_num_blocks_recorded, num_computed_blocks):",
            "            assert len(token_ids) >= (i + 1) * self._block_size",
            "            block_token_ids = token_ids[i * self._block_size:(i + 1) *",
            "                                        self._block_size]",
            "",
            "            # NOTE: If there are any factors affecting the block besides",
            "            # token_ids, they should be added as input to extra_hash.",
            "            extra_hash = seq.extra_hash()",
            "",
            "            # This has to be kept in sync with the allocator's hash",
            "            # calculation.",
            "            block_hash = PrefixCachingBlock.hash_block_tokens(",
            "                is_first_block=prev_block_hash is None,",
            "                prev_block_hash=prev_block_hash,",
            "                cur_block_token_ids=block_token_ids,",
            "                extra_hash=extra_hash,",
            "            )",
            "            block_hashes_recorded.append(block_hash)",
            "            prev_block_hash = block_hash",
            "",
            "        self._seq_id_to_blocks_hashes[seq.seq_id] = block_hashes_recorded",
            "",
            "    def get_num_cached_tokens(self, seq: Sequence) -> int:",
            "        if not self._enable_caching:",
            "            return 0",
            "",
            "        # We always try to update the sequence hashes on the fly.",
            "        # This is to ensure that we don't miss any cached tokens for the",
            "        # sequence during decode.",
            "        # This routine should only update hash for any new blocks too.",
            "        self._update_seq_hashes(seq)",
            "",
            "        num_computed_tokens_prev = self._seq_id_to_num_tokens_computed.get(",
            "            seq.seq_id, None)",
            "",
            "        # TODO(rickyx): This hack could be removed once we mark blocks as",
            "        # computed correctly with chunked prefills.",
            "        if num_computed_tokens_prev is not None and seq.is_prefill():",
            "            # For a sequence that is still in prefill, we don't",
            "            # recompute the number of cached tokens.",
            "            # This also handles correctly chunked prefill since currently",
            "            # we mark blocks as computed even if the sequence is still partially",
            "            # prefilled. So a continuously prefilled sequence should not",
            "            # see its cached token count change while running.",
            "            return num_computed_tokens_prev",
            "",
            "        block_hashes = self._seq_id_to_blocks_hashes[seq.seq_id]",
            "",
            "        # This is O(logN), where N is the number of blocks.",
            "        num_cached_blocks = len(",
            "            self._allocator.find_cached_blocks_prefix(block_hashes))",
            "        num_cached_tokens = num_cached_blocks * self._block_size",
            "        self._seq_id_to_num_tokens_computed[seq.seq_id] = num_cached_tokens",
            "        return num_cached_tokens",
            "",
            "    def remove_seq(self, seq_id: int) -> None:",
            "        \"\"\"Stop tracking the sequence.\"\"\"",
            "        if not self._enable_caching:",
            "            return",
            "        assert seq_id in self._seq_id_to_blocks_hashes",
            "        del self._seq_id_to_blocks_hashes[seq_id]",
            "",
            "        assert seq_id in self._seq_id_to_num_tokens_computed",
            "        del self._seq_id_to_num_tokens_computed[seq_id]",
            "",
            "",
            "class LastAccessBlocksTracker:",
            "    \"\"\"Manages the last access time of the tracked sequences, in order to allow",
            "    an efficient update of allocator's block last access times",
            "    \"\"\"",
            "",
            "    def __init__(self, allocator):",
            "        self._allocator = allocator",
            "        self._seq_last_access: Dict[int, Optional[float]] = {}",
            "",
            "    def add_seq(self, seq_id: int) -> None:",
            "        \"\"\"Start tracking seq_id",
            "        \"\"\"",
            "        assert seq_id not in self._seq_last_access",
            "        self._seq_last_access[seq_id] = None",
            "",
            "    def remove_seq(self, seq_id: int) -> None:",
            "        \"\"\"Stop tracking seq_id",
            "        \"\"\"",
            "        assert seq_id in self._seq_last_access",
            "        del self._seq_last_access[seq_id]",
            "",
            "    def update_last_access(self, seq_id: int, time: float) -> None:",
            "        assert seq_id in self._seq_last_access",
            "        self._seq_last_access[seq_id] = time",
            "",
            "    def update_seq_blocks_last_access(self, seq_id: int,",
            "                                      block_ids: List[int]) -> None:",
            "        assert seq_id in self._seq_last_access",
            "",
            "        ts = self._seq_last_access[seq_id]",
            "",
            "        if ts is None:",
            "            # No last access was recorded, no need to update.",
            "            return",
            "",
            "        self._allocator.mark_blocks_as_accessed(block_ids, ts)",
            "",
            "",
            "def assert_prefix_caching_block_or_none(block: Optional[Block]):",
            "    if block is None:",
            "        return",
            "    assert isinstance(block,",
            "                      PrefixCachingBlock), \"Got block = {}\".format(block)"
        ],
        "afterPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "\"\"\"Token blocks.\"\"\"",
            "import sys",
            "from bisect import bisect_left",
            "from os.path import commonprefix",
            "from typing import (Callable, Dict, FrozenSet, Iterable, List, Optional, Set,",
            "                    Tuple)",
            "",
            "from vllm.core.block.common import (CacheMetricData, CopyOnWriteTracker,",
            "                                    get_all_blocks_recursively)",
            "from vllm.core.block.interfaces import (Block, BlockAllocator, BlockId, Device,",
            "                                        DeviceAwareBlockAllocator)",
            "from vllm.core.block.naive_block import (BlockPool, NaiveBlock,",
            "                                         NaiveBlockAllocator)",
            "from vllm.core.evictor import EvictionPolicy, Evictor, make_evictor",
            "from vllm.logger import init_logger",
            "from vllm.sequence import Sequence",
            "",
            "PrefixHash = int",
            "",
            "# By default, we init our block access time as _DEFAULT_LAST_ACCESSED_TIME",
            "# so that if we find one block is still hold _DEFAULT_LAST_ACCESSED_TIME,",
            "# then we know this block hasn't been accessed yet.",
            "_DEFAULT_LAST_ACCESSED_TIME = -1",
            "",
            "logger = init_logger(__name__)",
            "",
            "",
            "class BlockTracker:",
            "    \"\"\"Used to track the status of a block inside the prefix caching allocator",
            "    \"\"\"",
            "    __slots__ = (\"active\", \"last_accessed\", \"computed\")",
            "",
            "    def reset(self):",
            "        self.last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME",
            "        self.computed: bool = False",
            "",
            "    def __init__(self):",
            "        self.active: bool = False",
            "        self.reset()",
            "",
            "    def enable(self):",
            "        assert not self.active",
            "        self.active = True",
            "        self.reset()",
            "",
            "    def disable(self):",
            "        assert self.active",
            "        self.active = False",
            "        self.reset()",
            "",
            "",
            "class PrefixCachingBlockAllocator(BlockAllocator):",
            "    \"\"\"A block allocator that implements prefix caching.",
            "",
            "    The PrefixCachingBlockAllocator maintains a cache of blocks based on their",
            "    content hash. It reuses blocks with the same content hash to avoid redundant",
            "    memory allocation. The allocator also supports copy-on-write operations.",
            "",
            "    Args:",
            "        num_blocks (int): The total number of blocks to manage.",
            "        block_size (int): The size of each block in tokens.",
            "        block_ids(Optional[Iterable[int]], optional): An optional iterable of",
            "            block IDs. If not provided, block IDs will be assigned sequentially",
            "            from 0 to num_blocks - 1.",
            "    \"\"\"",
            "",
            "    # Note that we use 'None' as a string here instead of None because",
            "    # as of Python 3.12, hash(None) returns a constant predictable value.",
            "    # This could possibly make it easier to find and exploit hash",
            "    # collisions. 'None' as a string will be hashed differently per process,",
            "    # but consistently within the same process. This is the same as the",
            "    # behavior of None prior to Python 3.12.",
            "    _none_hash: int = hash('None')",
            "",
            "    # Implements Block.Factory.",
            "    def __init__(",
            "        self,",
            "        num_blocks: int,",
            "        block_size: int,",
            "        block_ids: Optional[Iterable[int]] = None,",
            "        eviction_policy: EvictionPolicy = EvictionPolicy.LRU,",
            "    ):",
            "        if block_ids is None:",
            "            block_ids = range(num_blocks)",
            "",
            "        self._block_size = block_size",
            "",
            "        # A mapping of prefix hash to block index. All blocks which have a",
            "        # prefix hash will be in this dict, even if they have refcount 0.",
            "        self._cached_blocks: Dict[PrefixHash, BlockId] = {}",
            "",
            "        # A list of immutable block IDs that have been touched by scheduler",
            "        # and should be marked as computed after an entire batch of sequences",
            "        # are scheduled.",
            "        self._touched_blocks: Set[BlockId] = set()",
            "",
            "        # Used to track status of each physical block id",
            "        self._block_tracker: Dict[BlockId, BlockTracker] = {}",
            "        for block_id in block_ids:",
            "            self._block_tracker[block_id] = BlockTracker()",
            "",
            "        # Pre-allocate \"num_blocks * extra_factor\" block objects.",
            "        # The \"* extra_factor\" is a buffer to allow more block objects",
            "        # than physical blocks",
            "        extra_factor = 4",
            "        self._block_pool = BlockPool(self._block_size, self._create_block,",
            "                                     self, num_blocks * extra_factor)",
            "",
            "        # An allocator for blocks that do not have prefix hashes.",
            "        self._hashless_allocator = NaiveBlockAllocator(",
            "            create_block=self._create_block,  # type: ignore",
            "            num_blocks=num_blocks,",
            "            block_size=block_size,",
            "            block_ids=block_ids,",
            "            block_pool=self._block_pool,  # Share block pool here",
            "        )",
            "",
            "        # Evitor used to maintain how we want to handle those computed blocks",
            "        # if we find memory pressure is high.",
            "        self.eviction_policy = eviction_policy",
            "        self.evictor: Evictor = make_evictor(self.eviction_policy)",
            "",
            "        # We share the refcounter between allocators. This allows us to promote",
            "        # blocks originally allocated in the hashless allocator to immutable",
            "        # blocks.",
            "        self._refcounter = self._hashless_allocator.refcounter",
            "",
            "        self._cow_tracker = CopyOnWriteTracker(",
            "            refcounter=self._refcounter.as_readonly())",
            "",
            "        self.metric_data = CacheMetricData()",
            "",
            "    def _create_block(",
            "        self,",
            "        prev_block: Optional[Block],",
            "        token_ids: List[int],",
            "        block_size: int,",
            "        allocator: BlockAllocator,",
            "        block_id: Optional[int] = None,",
            "        computed: bool = False,",
            "        extra_hash: Optional[int] = None,",
            "    ) -> Block:",
            "        # Bind block to self.",
            "        allocator = self",
            "",
            "        return PrefixCachingBlock(",
            "            prev_block=prev_block,",
            "            token_ids=token_ids,",
            "            block_size=block_size,",
            "            block_id=block_id,",
            "            allocator=allocator,",
            "            computed=computed,",
            "            extra_hash=extra_hash,",
            "        )",
            "",
            "    def allocate_immutable_block(self,",
            "                                 prev_block: Optional[Block],",
            "                                 token_ids: List[int],",
            "                                 extra_hash: Optional[int] = None,",
            "                                 device: Optional[Device] = None) -> Block:",
            "        \"\"\"Allocates an immutable block with the given token IDs, reusing cached",
            "        blocks if possible.",
            "",
            "        Args:",
            "            prev_block (Optional[Block]): The previous block in the sequence.",
            "            token_ids (List[int]): The token IDs to be stored in the block.",
            "",
            "        Returns:",
            "            Block: The allocated immutable block.",
            "        \"\"\"",
            "        assert device is None",
            "        assert_prefix_caching_block_or_none(prev_block)",
            "",
            "        # First, try to create a block that points to cached data",
            "        block = self._block_pool.init_block(prev_block=prev_block,",
            "                                            token_ids=token_ids,",
            "                                            block_size=self._block_size,",
            "                                            physical_block_id=None,",
            "                                            extra_hash=extra_hash)",
            "        assert block.content_hash is not None",
            "",
            "        cached_block_id = self._cached_blocks.get(block.content_hash, None)",
            "        if cached_block_id is not None:",
            "            self.metric_data.query(hit=True)",
            "            block.block_id = cached_block_id",
            "            self._incr_refcount_cached_block(block)",
            "            return block",
            "        self.metric_data.query(hit=False)",
            "        self._block_pool.free_block(block)",
            "",
            "        # No cached block => Allocate a new block",
            "        block = self.allocate_mutable_block(prev_block, extra_hash=extra_hash)",
            "        block.append_token_ids(token_ids)",
            "        return block",
            "",
            "    def allocate_immutable_blocks(",
            "            self,",
            "            prev_block: Optional[Block],",
            "            block_token_ids: List[List[int]],",
            "            extra_hash: Optional[int] = None,",
            "            device: Optional[Device] = None) -> List[Block]:",
            "        blocks = []",
            "        for token_ids in block_token_ids:",
            "            prev_block = self.allocate_immutable_block(prev_block=prev_block,",
            "                                                       token_ids=token_ids,",
            "                                                       device=device,",
            "                                                       extra_hash=extra_hash)",
            "            blocks.append(prev_block)",
            "        return blocks",
            "",
            "    def allocate_mutable_block(self,",
            "                               prev_block: Optional[Block],",
            "                               extra_hash: Optional[int] = None,",
            "                               device: Optional[Device] = None) -> Block:",
            "        \"\"\"Allocates a mutable block. If there are no free blocks, this will",
            "        evict unused cached blocks.",
            "",
            "        Args:",
            "            prev_block (Block): The previous block in the sequence.",
            "                None is not allowed unlike it is super class.",
            "",
            "        Returns:",
            "            Block: The allocated mutable block.",
            "        \"\"\"",
            "        assert device is None",
            "        assert_prefix_caching_block_or_none(prev_block)",
            "",
            "        block_id = self._allocate_block_id()",
            "        block = self._block_pool.init_block(prev_block=prev_block,",
            "                                            token_ids=[],",
            "                                            block_size=self._block_size,",
            "                                            physical_block_id=block_id,",
            "                                            extra_hash=extra_hash)",
            "        assert not block.computed",
            "        assert block.content_hash is None",
            "        return block",
            "",
            "    def _incr_refcount_cached_block(self, block: Block) -> None:",
            "        # Set this block to be \"computed\" since it is pointing to a",
            "        # cached block id (which was already computed)",
            "        block.computed = True",
            "",
            "        block_id = block.block_id",
            "        assert block_id is not None",
            "",
            "        refcount = self._refcounter.incr(block_id)",
            "        if refcount == 1:",
            "            # In case a cached block was evicted, restore its tracking",
            "            if block_id in self.evictor:",
            "                self.evictor.remove(block_id)",
            "",
            "            self._track_block_id(block_id, computed=True)",
            "",
            "    def _decr_refcount_cached_block(self, block: Block) -> None:",
            "        # Ensure this is immutable/cached block",
            "        assert block.content_hash is not None",
            "",
            "        block_id = block.block_id",
            "        assert block_id is not None",
            "",
            "        refcount = self._refcounter.decr(block_id)",
            "        if refcount > 0:",
            "            block.block_id = None",
            "            return",
            "        else:",
            "            assert refcount == 0",
            "",
            "        # No longer used",
            "        assert block.content_hash in self._cached_blocks",
            "",
            "        # Add the cached block to the evictor",
            "        # (This keeps the cached block around so it can be reused)",
            "        self.evictor.add(block_id, block.content_hash, block.num_tokens_total,",
            "                         self._block_tracker[block_id].last_accessed)",
            "",
            "        # Stop tracking the block",
            "        self._untrack_block_id(block_id)",
            "",
            "        block.block_id = None",
            "",
            "    def _decr_refcount_hashless_block(self, block: Block) -> None:",
            "        block_id = block.block_id",
            "        assert block_id is not None",
            "",
            "        # We may have a fork case where block is shared,",
            "        # in which case, we cannot remove it from tracking",
            "        refcount = self._refcounter.get(block_id)",
            "        if refcount == 1:",
            "            self._untrack_block_id(block_id)",
            "",
            "        # Decrement refcount of the block_id, but do not free the block object",
            "        # itself (will be handled by the caller)",
            "        self._hashless_allocator.free(block, keep_block_object=True)",
            "",
            "    def _allocate_block_id(self) -> BlockId:",
            "        \"\"\"First tries to allocate a block id from the hashless allocator,",
            "        and if there are no blocks, then tries to evict an unused cached block.",
            "        \"\"\"",
            "        hashless_block_id = self._maybe_allocate_hashless_block_id()",
            "        if hashless_block_id is not None:",
            "            return hashless_block_id",
            "",
            "        evicted_block_id = self._maybe_allocate_evicted_block_id()",
            "        if evicted_block_id is not None:",
            "            return evicted_block_id",
            "",
            "        # No block available in hashless allocator, nor in unused cache blocks.",
            "        raise BlockAllocator.NoFreeBlocksError()",
            "",
            "    def _maybe_allocate_hashless_block_id(self) -> Optional[BlockId]:",
            "        try:",
            "            # Allocate mutable block and extract its block_id",
            "            block = self._hashless_allocator.allocate_mutable_block(",
            "                prev_block=None)",
            "            block_id = block.block_id",
            "            self._block_pool.free_block(block)",
            "",
            "            self._track_block_id(block_id, computed=False)",
            "            return block_id",
            "        except BlockAllocator.NoFreeBlocksError:",
            "            return None",
            "",
            "    def _maybe_allocate_evicted_block_id(self) -> Optional[BlockId]:",
            "        if self.evictor.num_blocks == 0:",
            "            return None",
            "",
            "        # Here we get an evicted block, which is only added",
            "        # into evictor if its ref counter is 0",
            "        # and since its content would be changed, we need",
            "        # to remove it from _cached_blocks's tracking list",
            "        block_id, content_hash_to_evict = self.evictor.evict()",
            "",
            "        # Sanity checks",
            "        assert content_hash_to_evict in self._cached_blocks",
            "        _block_id = self._cached_blocks[content_hash_to_evict]",
            "        assert self._refcounter.get(_block_id) == 0",
            "        assert _block_id == block_id",
            "",
            "        self._cached_blocks.pop(content_hash_to_evict)",
            "",
            "        self._refcounter.incr(block_id)",
            "        self._track_block_id(block_id, computed=False)",
            "",
            "        return block_id",
            "",
            "    def _free_block_id(self, block: Block) -> None:",
            "        \"\"\"Decrements the refcount of the block. The block may be in two ",
            "        possible states: (1) immutable/cached or (2) mutable/hashless. ",
            "        In the first case, the refcount is decremented directly and the block",
            "        may be possibly added to the evictor. In other case, hashless ",
            "        allocator free(..) with keep_block_object=True is called to only free",
            "        the block id (since the block object may be reused by the caller)",
            "        \"\"\"",
            "        block_id = block.block_id",
            "        assert block_id is not None, \"Freeing unallocated block is undefined\"",
            "",
            "        if block.content_hash is not None:",
            "            # Immutable: This type of block is always cached, and we want to",
            "            # keep it in the evictor for future reuse",
            "            self._decr_refcount_cached_block(block)",
            "        else:",
            "            # Mutable: This type of block is not cached, so we release it",
            "            # directly to the hashless allocator",
            "            self._decr_refcount_hashless_block(block)",
            "",
            "        assert block.block_id is None",
            "",
            "    def free(self, block: Block, keep_block_object: bool = False) -> None:",
            "        \"\"\"Release the block (look at free_block_id(..) docs)",
            "        \"\"\"",
            "        # Release the physical block index",
            "        self._free_block_id(block)",
            "",
            "        # Release the block object to the pool",
            "        if not keep_block_object:",
            "            self._block_pool.free_block(block)",
            "",
            "    def fork(self, last_block: Block) -> List[Block]:",
            "        \"\"\"Creates a new sequence of blocks that shares the same underlying",
            "        memory as the original sequence.",
            "",
            "        Args:",
            "            last_block (Block): The last block in the original sequence.",
            "",
            "        Returns:",
            "            List[Block]: The new sequence of blocks that shares the same memory",
            "                as the original sequence.",
            "        \"\"\"",
            "        source_blocks = get_all_blocks_recursively(last_block)",
            "",
            "        forked_blocks: List[Block] = []",
            "        prev_block = None",
            "        for block in source_blocks:",
            "            block_id = block.block_id",
            "            assert block_id is not None",
            "",
            "            refcount = self._refcounter.incr(block_id)",
            "            assert refcount != 1, \"can't fork free'd block_id = {}\".format(",
            "                block_id)",
            "",
            "            forked_block = self._block_pool.init_block(",
            "                prev_block=prev_block,",
            "                token_ids=block.token_ids,",
            "                block_size=self._block_size,",
            "                physical_block_id=block_id,",
            "                extra_hash=block.extra_hash)",
            "",
            "            forked_blocks.append(forked_block)",
            "            prev_block = forked_blocks[-1]",
            "",
            "        return forked_blocks",
            "",
            "    def get_num_free_blocks(self, device: Optional[Device] = None) -> int:",
            "        assert device is None",
            "        # The number of free blocks is the number of hashless free blocks",
            "        # plus the number of blocks evictor could free from its list.",
            "        return self._hashless_allocator.get_num_free_blocks(",
            "        ) + self.evictor.num_blocks",
            "",
            "    def get_num_total_blocks(self) -> int:",
            "        return self._hashless_allocator.get_num_total_blocks()",
            "",
            "    def get_physical_block_id(self, absolute_id: int) -> int:",
            "        \"\"\"Returns the zero-offset block id on certain block allocator",
            "        given the absolute block id.",
            "",
            "        Args:",
            "            absolute_id (int): The absolute block id for the block ",
            "                in whole allocator.",
            "",
            "        Returns:",
            "            int: The rzero-offset block id on certain device.",
            "        \"\"\"",
            "        return sorted(self.all_block_ids).index(absolute_id)",
            "",
            "    @property",
            "    def all_block_ids(self) -> FrozenSet[int]:",
            "        return self._hashless_allocator.all_block_ids",
            "",
            "    def get_prefix_cache_hit_rate(self) -> float:",
            "        return self.metric_data.get_hit_rate()",
            "",
            "    def reset_prefix_cache(self) -> bool:",
            "        \"\"\"Reset prefix cache. This function may be used in RLHF",
            "        flows to invalid prefix caching after the weights are updated,",
            "        or used for resetting prefix caching status for benchmarking.",
            "",
            "        Returns:",
            "            bool: True if the prefix cache is successfully reset,",
            "            False otherwise.",
            "        \"\"\"",
            "        num_used_blocks = (self.get_num_total_blocks() -",
            "                           self.get_num_free_blocks())",
            "        if num_used_blocks > 0:",
            "            logger.warning(",
            "                \"Failed to reset prefix cache because some \"",
            "                \"blocks (%d) are not freed yet\", num_used_blocks)",
            "            return False",
            "",
            "        # Free all blocks in the evictor.",
            "        while (block_id :=",
            "               self._maybe_allocate_evicted_block_id()) is not None:",
            "            self._hashless_allocator.free_block_id(block_id)",
            "",
            "        # Should not have any cached blocks because all blocks are evicted.",
            "        assert not self._cached_blocks",
            "",
            "        # Reset the evictor.",
            "        self.evictor = make_evictor(self.eviction_policy)",
            "",
            "        # Reset the block tracker.",
            "        for block_id in self._block_tracker:",
            "            self._block_tracker[block_id] = BlockTracker()",
            "",
            "        # Reset the metrics.",
            "        self.metric_data = CacheMetricData()",
            "",
            "        logger.info(\"Successfully reset prefix cache\")",
            "        return True",
            "",
            "    def is_block_cached(self, block: Block) -> bool:",
            "        assert block.content_hash is not None",
            "        return block.content_hash in self._cached_blocks",
            "",
            "    def promote_to_immutable_block(self, block: Block) -> BlockId:",
            "        \"\"\"Once a mutable block is full, it can be promoted to an immutable",
            "        block. This means that its content can be referenced by future blocks",
            "        having the same prefix.",
            "",
            "        Note that if we already have a cached block with the same content, we",
            "        will replace the newly-promoted block's mapping with the existing cached",
            "        block id.",
            "",
            "        Args:",
            "            block: The mutable block to be promoted.",
            "",
            "        Returns:",
            "            BlockId: Either the original block index, or the block index of",
            "                the previously cached block matching the same content.",
            "        \"\"\"",
            "        # Ensure block can be promoted",
            "        assert block.content_hash is not None",
            "        assert block.block_id is not None",
            "        assert self._refcounter.get(block.block_id) > 0",
            "",
            "        if block.content_hash not in self._cached_blocks:",
            "            # No cached content hash => Set this block as cached.",
            "            # Note that this block cannot be marked as computed yet",
            "            # because other sequences in the same batch cannot reuse",
            "            # this block.",
            "            self._cached_blocks[block.content_hash] = block.block_id",
            "            # Mark this block as touched so that it can be marked as",
            "            # computed after the entire batch of sequences are scheduled.",
            "            self._touched_blocks.add(block.block_id)",
            "            return block.block_id",
            "",
            "        # Reuse the cached content hash",
            "        self._decr_refcount_hashless_block(block)",
            "        block.block_id = self._cached_blocks[block.content_hash]",
            "",
            "        # Increment refcount of the cached block and (possibly) restore",
            "        # it from the evictor.",
            "        # Note that in this case, the block is marked as computed",
            "        self._incr_refcount_cached_block(block)",
            "",
            "        return block.block_id",
            "",
            "    def cow_block_if_not_appendable(self, block: Block) -> BlockId:",
            "        \"\"\"Performs a copy-on-write operation on the given block if it is not",
            "        appendable.",
            "",
            "        Args:",
            "            block (Block): The block to check for copy-on-write.",
            "",
            "        Returns:",
            "            BlockId: The block index of the new block if a copy-on-write ",
            "                operation was performed, or the original block index if",
            "                no copy-on-write was necessary.",
            "        \"\"\"",
            "        src_block_id = block.block_id",
            "        assert src_block_id is not None",
            "",
            "        if self._cow_tracker.is_appendable(block):",
            "            return src_block_id",
            "",
            "        self._free_block_id(block)",
            "        trg_block_id = self._allocate_block_id()",
            "",
            "        self._cow_tracker.record_cow(src_block_id, trg_block_id)",
            "",
            "        return trg_block_id",
            "",
            "    def clear_copy_on_writes(self) -> List[Tuple[BlockId, BlockId]]:",
            "        \"\"\"Returns the copy-on-write source->destination mapping and clears it.",
            "",
            "        Returns:",
            "            List[Tuple[BlockId, BlockId]]: A list mapping source",
            "                block indices to destination block indices.",
            "        \"\"\"",
            "        return self._cow_tracker.clear_cows()",
            "",
            "    def mark_blocks_as_accessed(self, block_ids: List[int],",
            "                                now: float) -> None:",
            "        \"\"\"Mark blocks as accessed, used in prefix caching.",
            "",
            "        If the block is added into evictor, we need to update corresponding",
            "        info in evictor's metadata.",
            "        \"\"\"",
            "",
            "        for block_id in block_ids:",
            "            if self._block_tracker[block_id].active:",
            "                self._block_tracker[block_id].last_accessed = now",
            "            elif block_id in self.evictor:",
            "                self.evictor.update(block_id, now)",
            "            else:",
            "                raise ValueError(",
            "                    \"Mark block as accessed which is not belonged to GPU\")",
            "",
            "    def mark_blocks_as_computed(self, block_ids: List[int]) -> None:",
            "        # Mark all touched blocks as computed.",
            "        for block_id in self._touched_blocks:",
            "            self._block_tracker[block_id].computed = True",
            "        self._touched_blocks.clear()",
            "",
            "    def _track_block_id(self, block_id: Optional[BlockId],",
            "                        computed: bool) -> None:",
            "        assert block_id is not None",
            "        self._block_tracker[block_id].enable()",
            "        self._block_tracker[block_id].computed = computed",
            "",
            "    def _untrack_block_id(self, block_id: Optional[BlockId]) -> None:",
            "        assert block_id is not None",
            "        self._block_tracker[block_id].disable()",
            "",
            "    def block_is_computed(self, block_id: int) -> bool:",
            "        if self._block_tracker[block_id].active:",
            "            return self._block_tracker[block_id].computed",
            "        else:",
            "            return block_id in self.evictor",
            "",
            "    def get_common_computed_block_ids(",
            "            self, computed_seq_block_ids: List[List[int]]) -> List[int]:",
            "        \"\"\"Return the block ids that are common for a given sequence group.",
            "",
            "        Only those blocks that are immutable and already be marked",
            "        compyted would be taken consideration.",
            "        \"\"\"",
            "",
            "        # NOTE We exclude the last block to avoid the case where the entire",
            "        # prompt is cached. This would cause erroneous behavior in model",
            "        # runner.",
            "",
            "        # It returns a list of int although type annotation says list of string.",
            "        if len(computed_seq_block_ids) == 1:",
            "            return computed_seq_block_ids[0]",
            "",
            "        return commonprefix([",
            "            ids for ids in computed_seq_block_ids  # type: ignore",
            "            if ids",
            "        ])",
            "",
            "    def get_num_full_blocks_touched(self, blocks: List[Block]) -> int:",
            "        \"\"\"Returns the number of full blocks that will be touched by",
            "        swapping in/out.",
            "",
            "        Args:",
            "            blocks: List of blocks to be swapped.",
            "        Returns:",
            "            int: the number of full blocks that will be touched by",
            "                swapping in/out the given blocks. Non full blocks are ignored",
            "                when deciding the number of blocks to touch.",
            "        \"\"\"",
            "        num_touched_blocks: int = 0",
            "        for block in blocks:",
            "            # If the block has a match in the cache and the cached",
            "            # block is not referenced, then we still count it as a",
            "            # touched block",
            "            if block.is_full and (not self.is_block_cached(block) or \\",
            "                (block.content_hash is not None and \\",
            "                self._cached_blocks[block.content_hash] in \\",
            "                        self.evictor)):",
            "                num_touched_blocks += 1",
            "        return num_touched_blocks",
            "",
            "    def swap_out(self, blocks: List[Block]) -> None:",
            "        \"\"\"Execute the swap out actions. Basically just free the ",
            "        given blocks.",
            "",
            "        Args:",
            "            blocks: List of blocks to be swapped out.",
            "        \"\"\"",
            "        for block in blocks:",
            "            self._free_block_id(block)",
            "",
            "    def swap_in(self, blocks: List[Block]) -> None:",
            "        \"\"\"Execute the swap in actions. Change the block id from ",
            "        old allocator to current allocator for each block to finish ",
            "        the block table update. ",
            "",
            "        Args:",
            "            blocks: List of blocks to be swapped in.",
            "        \"\"\"",
            "        for block in blocks:",
            "            # Here we allocate either immutable or mutable block and then",
            "            # extract its block_id. Note that the block object is released",
            "            # and the block_id is assigned to \"block\" to allow reusing the",
            "            # existing \"block\" object",
            "            if block.is_full:",
            "                tmp_block = self.allocate_immutable_block(",
            "                    prev_block=block.prev_block,",
            "                    token_ids=block.token_ids,",
            "                    extra_hash=block.extra_hash)",
            "            else:",
            "                tmp_block = self.allocate_mutable_block(",
            "                    prev_block=block.prev_block, extra_hash=block.extra_hash)",
            "                tmp_block.append_token_ids(block.token_ids)",
            "",
            "            block_id = tmp_block.block_id",
            "            self._block_pool.free_block(tmp_block)",
            "",
            "            block.block_id = block_id  # Assign block_id",
            "",
            "    def find_cached_blocks_prefix(self, block_hashes: List[int]) -> List[int]:",
            "        \"\"\"",
            "        Given a list of block hashes, return the prefix of the block hashes that",
            "        are all cached.",
            "",
            "        Since a block's block hash includes the hashes of all previous blocks,",
            "        and we only allocate/deallocate blocks in the entire sequence, so if a",
            "        block is cached, then all previous blocks are also cached. With this",
            "        property, we can use binary search to find the prefix of cached blocks.",
            "",
            "        Args:",
            "            block_hashes (List[int]): The list of block hashes.",
            "",
            "        Returns:",
            "            List[int]: The prefix of the `block_hashes` that are cached.",
            "        \"\"\"",
            "",
            "        def _block_is_cached(block_hash: PrefixHash) -> bool:",
            "            if block_hash not in self._cached_blocks:",
            "                return False",
            "",
            "            cached_block_id = self._cached_blocks[block_hash]",
            "            # We only consider the blocks that are marked as computed.",
            "            return self.block_is_computed(cached_block_id)",
            "",
            "        def _bisect_left(a, x, key: Callable[[PrefixHash], bool]) -> int:",
            "",
            "            # python <= 3.10 don't have the key argument",
            "            if sys.version_info < (3, 10):",
            "                a = [key(e) for e in a]",
            "                return bisect_left(a, x)",
            "            else:",
            "                return bisect_left(a, x, key=key)",
            "",
            "        # Look for the first block that's not cached, and returns the prefix",
            "        # i.e. blocks that are cached.",
            "        idx = _bisect_left(block_hashes,",
            "                           True,",
            "                           key=lambda x: not _block_is_cached(x))",
            "        return block_hashes[:idx]",
            "",
            "",
            "class PrefixCachingBlock(Block):",
            "    \"\"\"A block implementation that supports prefix caching.",
            "",
            "    The PrefixCachingBlock class represents a block of token IDs with prefix",
            "    caching capabilities. It wraps a NaiveBlock internally and provides",
            "    additional functionality for content hashing and promoting immutable blocks",
            "    with the prefix caching allocator.",
            "",
            "    Args:",
            "        prev_block (Optional[PrefixCachingBlock]): The previous block in the",
            "            sequence.",
            "        token_ids (List[int]): The initial token IDs to be stored in the block.",
            "        block_size (int): The maximum number of token IDs that can be stored in",
            "            the block.",
            "        allocator (BlockAllocator): The prefix",
            "            caching block allocator associated with this block.",
            "        block_id (Optional[int], optional): The physical block index",
            "            of this block. Defaults to None.",
            "        extra_hash (Optional[int]): The hash value of additional factors",
            "            such as adapters that influence the block, apart from the token_ids.",
            "    \"\"\"",
            "",
            "    # Note that we use 'None' as a string here instead of None because",
            "    # as of Python 3.12, hash(None) returns a constant predictable value.",
            "    # This could possibly make it easier to find and exploit hash",
            "    # collisions. 'None' as a string will be hashed differently per process,",
            "    # but consistently within the same process. This is the same as the",
            "    # behavior of None prior to Python 3.12.",
            "    _none_hash: int = hash('None')",
            "",
            "    def __init__(",
            "        self,",
            "        prev_block: Optional[Block],",
            "        token_ids: List[int],",
            "        block_size: int,",
            "        allocator: BlockAllocator,",
            "        block_id: Optional[int] = None,",
            "        computed: bool = False,",
            "        extra_hash: Optional[int] = None,",
            "    ):",
            "        assert isinstance(allocator, PrefixCachingBlockAllocator), (",
            "            \"Currently this class is only tested with \"",
            "            \"PrefixCachingBlockAllocator. Got instead allocator = {}\".format(",
            "                allocator))",
            "        assert_prefix_caching_block_or_none(prev_block)",
            "",
            "        self._prev_block = prev_block",
            "        self._cached_content_hash: Optional[int] = None",
            "        self._cached_num_tokens_total: int = 0",
            "        self._allocator = allocator",
            "        self._last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME",
            "        self._computed = computed",
            "        self._extra_hash = extra_hash",
            "",
            "        # On the first time, we create the block object, and next we only",
            "        # reinitialize it",
            "        if hasattr(self, \"_block\"):",
            "            self._block.__init__(  # type: ignore[has-type]",
            "                prev_block=prev_block,",
            "                token_ids=token_ids,",
            "                block_size=block_size,",
            "                block_id=block_id,",
            "                allocator=self._allocator)",
            "        else:",
            "            self._block = NaiveBlock(prev_block=prev_block,",
            "                                     token_ids=token_ids,",
            "                                     block_size=block_size,",
            "                                     block_id=block_id,",
            "                                     allocator=self._allocator)",
            "",
            "        self._update_num_tokens_total()",
            "",
            "    def _update_num_tokens_total(self):",
            "        \"\"\"Incrementally computes the number of tokens that there is",
            "        till the current block (included)",
            "        \"\"\"",
            "        res = 0",
            "",
            "        # Add all previous blocks",
            "        if self._prev_block is not None:",
            "            res += self._prev_block.num_tokens_total",
            "",
            "        # Add current block",
            "        res += len(self.token_ids)",
            "",
            "        self._cached_num_tokens_total = res",
            "",
            "    @property",
            "    def computed(self) -> bool:",
            "        return self._computed",
            "",
            "    @computed.setter",
            "    def computed(self, value) -> None:",
            "        self._computed = value",
            "",
            "    @property",
            "    def last_accessed(self) -> float:",
            "        return self._last_accessed",
            "",
            "    @last_accessed.setter",
            "    def last_accessed(self, last_accessed_ts: float):",
            "        self._last_accessed = last_accessed_ts",
            "",
            "    def append_token_ids(self, token_ids: List[int]) -> None:",
            "        \"\"\"Appends the given token IDs to the block and registers the block as",
            "        immutable if the block becomes full.",
            "",
            "        Args:",
            "            token_ids (List[int]): The token IDs to be appended to the block.",
            "        \"\"\"",
            "        # Ensure this is mutable block (not promoted)",
            "        assert self.content_hash is None",
            "        assert not self.computed",
            "",
            "        if len(token_ids) == 0:",
            "            return",
            "",
            "        # Ensure there are input tokens",
            "        assert token_ids, \"Got token_ids = {}\".format(token_ids)",
            "",
            "        # Naive block handles CoW.",
            "        self._block.append_token_ids(token_ids)",
            "        self._update_num_tokens_total()",
            "",
            "        # If the content hash is present, then the block can be made immutable.",
            "        # Register ourselves with the allocator, potentially replacing the",
            "        # physical block index.",
            "        if self.content_hash is not None:",
            "            self.block_id = self._allocator.promote_to_immutable_block(self)",
            "",
            "    @property",
            "    def block_id(self) -> Optional[int]:",
            "        return self._block.block_id",
            "",
            "    @block_id.setter",
            "    def block_id(self, value) -> None:",
            "        self._block.block_id = value",
            "",
            "    @property",
            "    def is_full(self) -> bool:",
            "        return self._block.is_full",
            "",
            "    @property",
            "    def num_empty_slots(self) -> int:",
            "        return self._block.num_empty_slots",
            "",
            "    @property",
            "    def num_tokens_total(self) -> int:",
            "        return self._cached_num_tokens_total",
            "",
            "    @property",
            "    def block_size(self) -> int:",
            "        return self._block.block_size",
            "",
            "    @property",
            "    def token_ids(self) -> List[int]:",
            "        return self._block.token_ids",
            "",
            "    @property",
            "    def prev_block(self) -> Optional[Block]:",
            "        return self._prev_block",
            "",
            "    @property",
            "    def extra_hash(self) -> Optional[int]:",
            "        return self._extra_hash",
            "",
            "    @property",
            "    def content_hash(self) -> Optional[int]:",
            "        \"\"\"Return the content-based hash of the current block, or None if it is",
            "        not yet defined.",
            "",
            "        For the content-based hash to be defined, the current block must be",
            "        full.",
            "        \"\"\"",
            "        # If the hash is already computed, return it.",
            "        if self._cached_content_hash is not None:",
            "            return self._cached_content_hash",
            "",
            "        # We cannot compute a hash for the current block because it is not full.",
            "        if not self.is_full:",
            "            return None",
            "",
            "        is_first_block = self._prev_block is None",
            "        prev_block_hash = (",
            "            self._none_hash if is_first_block else",
            "            self._prev_block.content_hash  # type: ignore",
            "        )",
            "",
            "        # Previous block exists but does not yet have a hash.",
            "        # Return no hash in this case.",
            "        if prev_block_hash == self._none_hash and not is_first_block:",
            "            return None",
            "",
            "        self._cached_content_hash = PrefixCachingBlock.hash_block_tokens(",
            "            is_first_block,",
            "            prev_block_hash,",
            "            cur_block_token_ids=self.token_ids,",
            "            extra_hash=self._extra_hash)",
            "        return self._cached_content_hash",
            "",
            "    @classmethod",
            "    def hash_block_tokens(cls,",
            "                          is_first_block: bool,",
            "                          prev_block_hash: Optional[int],",
            "                          cur_block_token_ids: List[int],",
            "                          extra_hash: Optional[int] = None) -> int:",
            "        \"\"\"Computes a hash value corresponding to the contents of a block and",
            "        the contents of the preceding block(s). The hash value is used for",
            "        prefix caching.",
            "",
            "        Parameters:",
            "        - is_first_block (bool): A flag indicating if the block is the first in",
            "            the sequence.",
            "        - prev_block_hash (Optional[int]): The hash of the previous block. None",
            "            if this is the first block.",
            "        - cur_block_token_ids (List[int]): A list of token ids in the current",
            "            block. The current block is assumed to be full.",
            "        - extra_hash (Optional[int]): The hash value of additional factors",
            "            such as adapters that influence the block, apart from the token_ids.",
            "",
            "        Returns:",
            "        - int: The computed hash value for the block.",
            "        \"\"\"",
            "        if is_first_block and prev_block_hash is None:",
            "            prev_block_hash = cls._none_hash",
            "        return hash((is_first_block, prev_block_hash, *cur_block_token_ids,",
            "                     extra_hash))",
            "",
            "",
            "class ComputedBlocksTracker:",
            "    \"\"\"",
            "    Tracks the computed blocks for each sequence.",
            "",
            "    Internally, it maintains a map from sequence id to the list of block hashes",
            "    for the sequence. We cache the hashes of the full blocks for each sequence,",
            "    and make sure the hash is calculated in the same way as the allocator.",
            "    When a sequence is being decoded, we also update the sequence's hash",
            "    accordingly and incrementally.",
            "",
            "    From the sequence hash, with prefix caching enabled, we could also calculate",
            "    the number of cached tokens for the sequence by looking up the number of",
            "    cached block hashes in the allocator.",
            "    \"\"\"",
            "",
            "    # Note that we use 'None' as a string here instead of None because",
            "    # as of Python 3.12, hash(None) returns a constant predictable value.",
            "    # This could possibly make it easier to find and exploit hash",
            "    # collisions. 'None' as a string will be hashed differently per process,",
            "    # but consistently within the same process. This is the same as the",
            "    # behavior of None prior to Python 3.12.",
            "    _none_hash: int = hash('None')",
            "",
            "    def __init__(",
            "        self,",
            "        allocator: DeviceAwareBlockAllocator,",
            "        block_size: int,",
            "        enable_caching: bool,",
            "    ):",
            "        self._allocator = allocator",
            "        self._block_size = block_size",
            "        self._enable_caching = enable_caching",
            "",
            "        # A map from seq_id to the list of block hashes for the",
            "        # sequence. This is so that we don't have to recompute the block hashes",
            "        # for the sequence when we need to check if the sequence is cached.",
            "        # Note a block that's not full will not have its hash calculated and",
            "        # recorded.",
            "        self._seq_id_to_blocks_hashes: Dict[int, List[int]] = {}",
            "",
            "        # A map from seq_id to the number of tokens that are cached for the",
            "        # sequence.",
            "        # We need this so that a sequence in continuous prefill doesn't",
            "        # accidentally see its cached token count change. See comments in",
            "        # `get_num_cached_tokens` for more details.",
            "        self._seq_id_to_num_tokens_computed: Dict[int, int] = {}",
            "",
            "    def _update_seq_hashes(self, seq: Sequence) -> None:",
            "        \"\"\"Incrementally update the sequence's block hashes and record them.\"\"\"",
            "        assert self._enable_caching",
            "",
            "        block_hashes_recorded = self._seq_id_to_blocks_hashes.get(",
            "            seq.seq_id, [])",
            "        cur_num_blocks_recorded = len(block_hashes_recorded)",
            "        token_ids = seq.get_token_ids()",
            "        assert len(token_ids) >= cur_num_blocks_recorded * self._block_size, (",
            "            f\"The sequence has {len(token_ids)} tokens, but\"",
            "            f\" already recorded {cur_num_blocks_recorded} blocks. \"",
            "            \"This should not happen since we assume blocks are \"",
            "            \"only appended other than recomputation. When the sequence is \"",
            "            \"recomputed, we should have removed the info of the old blocks.\")",
            "        # Update the computed block hashes for the sequence. Since only full",
            "        # blocks are considered as \"computed\", we take floor here.",
            "        num_computed_blocks = len(token_ids) // self._block_size",
            "",
            "        # We need to know the hash of the previous block to compute the hash of",
            "        # the current block so that blocks could be uniquely identified across",
            "        # sequences of prefixes.",
            "        prev_block_hash = (self._none_hash if cur_num_blocks_recorded == 0 else",
            "                           block_hashes_recorded[-1])",
            "        # Only update the computed block hashes for the new blocks",
            "        for i in range(cur_num_blocks_recorded, num_computed_blocks):",
            "            assert len(token_ids) >= (i + 1) * self._block_size",
            "            block_token_ids = token_ids[i * self._block_size:(i + 1) *",
            "                                        self._block_size]",
            "",
            "            # NOTE: If there are any factors affecting the block besides",
            "            # token_ids, they should be added as input to extra_hash.",
            "            extra_hash = seq.extra_hash()",
            "",
            "            # This has to be kept in sync with the allocator's hash",
            "            # calculation.",
            "            block_hash = PrefixCachingBlock.hash_block_tokens(",
            "                is_first_block=prev_block_hash == self._none_hash,",
            "                prev_block_hash=prev_block_hash,",
            "                cur_block_token_ids=block_token_ids,",
            "                extra_hash=extra_hash,",
            "            )",
            "            block_hashes_recorded.append(block_hash)",
            "            prev_block_hash = block_hash",
            "",
            "        self._seq_id_to_blocks_hashes[seq.seq_id] = block_hashes_recorded",
            "",
            "    def get_num_cached_tokens(self, seq: Sequence) -> int:",
            "        if not self._enable_caching:",
            "            return 0",
            "",
            "        # We always try to update the sequence hashes on the fly.",
            "        # This is to ensure that we don't miss any cached tokens for the",
            "        # sequence during decode.",
            "        # This routine should only update hash for any new blocks too.",
            "        self._update_seq_hashes(seq)",
            "",
            "        num_computed_tokens_prev = self._seq_id_to_num_tokens_computed.get(",
            "            seq.seq_id, None)",
            "",
            "        # TODO(rickyx): This hack could be removed once we mark blocks as",
            "        # computed correctly with chunked prefills.",
            "        if num_computed_tokens_prev is not None and seq.is_prefill():",
            "            # For a sequence that is still in prefill, we don't",
            "            # recompute the number of cached tokens.",
            "            # This also handles correctly chunked prefill since currently",
            "            # we mark blocks as computed even if the sequence is still partially",
            "            # prefilled. So a continuously prefilled sequence should not",
            "            # see its cached token count change while running.",
            "            return num_computed_tokens_prev",
            "",
            "        block_hashes = self._seq_id_to_blocks_hashes[seq.seq_id]",
            "",
            "        # This is O(logN), where N is the number of blocks.",
            "        num_cached_blocks = len(",
            "            self._allocator.find_cached_blocks_prefix(block_hashes))",
            "        num_cached_tokens = num_cached_blocks * self._block_size",
            "        self._seq_id_to_num_tokens_computed[seq.seq_id] = num_cached_tokens",
            "        return num_cached_tokens",
            "",
            "    def remove_seq(self, seq_id: int) -> None:",
            "        \"\"\"Stop tracking the sequence.\"\"\"",
            "        if not self._enable_caching:",
            "            return",
            "        assert seq_id in self._seq_id_to_blocks_hashes",
            "        del self._seq_id_to_blocks_hashes[seq_id]",
            "",
            "        assert seq_id in self._seq_id_to_num_tokens_computed",
            "        del self._seq_id_to_num_tokens_computed[seq_id]",
            "",
            "",
            "class LastAccessBlocksTracker:",
            "    \"\"\"Manages the last access time of the tracked sequences, in order to allow",
            "    an efficient update of allocator's block last access times",
            "    \"\"\"",
            "",
            "    def __init__(self, allocator):",
            "        self._allocator = allocator",
            "        self._seq_last_access: Dict[int, Optional[float]] = {}",
            "",
            "    def add_seq(self, seq_id: int) -> None:",
            "        \"\"\"Start tracking seq_id",
            "        \"\"\"",
            "        assert seq_id not in self._seq_last_access",
            "        self._seq_last_access[seq_id] = None",
            "",
            "    def remove_seq(self, seq_id: int) -> None:",
            "        \"\"\"Stop tracking seq_id",
            "        \"\"\"",
            "        assert seq_id in self._seq_last_access",
            "        del self._seq_last_access[seq_id]",
            "",
            "    def update_last_access(self, seq_id: int, time: float) -> None:",
            "        assert seq_id in self._seq_last_access",
            "        self._seq_last_access[seq_id] = time",
            "",
            "    def update_seq_blocks_last_access(self, seq_id: int,",
            "                                      block_ids: List[int]) -> None:",
            "        assert seq_id in self._seq_last_access",
            "",
            "        ts = self._seq_last_access[seq_id]",
            "",
            "        if ts is None:",
            "            # No last access was recorded, no need to update.",
            "            return",
            "",
            "        self._allocator.mark_blocks_as_accessed(block_ids, ts)",
            "",
            "",
            "def assert_prefix_caching_block_or_none(block: Optional[Block]):",
            "    if block is None:",
            "        return",
            "    assert isinstance(block,",
            "                      PrefixCachingBlock), \"Got block = {}\".format(block)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "125": [
                "PrefixCachingBlockAllocator"
            ],
            "894": [
                "PrefixCachingBlock",
                "content_hash"
            ],
            "900": [
                "PrefixCachingBlock",
                "content_hash"
            ],
            "910": [
                "PrefixCachingBlock"
            ],
            "911": [
                "PrefixCachingBlock",
                "hash_block_tokens"
            ],
            "932": [
                "PrefixCachingBlock",
                "hash_block_tokens"
            ],
            "997": [
                "ComputedBlocksTracker",
                "_update_seq_hashes"
            ],
            "1012": [
                "ComputedBlocksTracker",
                "_update_seq_hashes"
            ]
        },
        "addLocation": [
            "vllm.core.block.prefix_caching_block.PrefixCachingBlockAllocator.self",
            "vllm.core.block.prefix_caching_block.PrefixCachingBlockAllocator._create_block.allocator",
            "vllm.core.block.prefix_caching_block.PrefixCachingBlock._allocator",
            "vllm.core.block.prefix_caching_block.PrefixCachingBlock.__init__.allocator"
        ]
    },
    "vllm/v1/core/kv_cache_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 263,
                "PatchRowcode": "         The hash value of the block and the token ids in the block."
            },
            "1": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 264,
                "PatchRowcode": "         The entire tuple is used as the hash key of the block."
            },
            "2": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 265,
                "PatchRowcode": "     \"\"\""
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 266,
                "PatchRowcode": "+    if not parent_block_hash:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 267,
                "PatchRowcode": "+        # Note that we use 'None' as a string here instead of None because"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 268,
                "PatchRowcode": "+        # as of Python 3.12, hash(None) returns a constant predictable value."
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 269,
                "PatchRowcode": "+        # This could possibly make it easier to find and exploit hash"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 270,
                "PatchRowcode": "+        # collisions. 'None' as a string will be hashed differently per process,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 271,
                "PatchRowcode": "+        # but consistently within the same process. This is the same as the"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 272,
                "PatchRowcode": "+        # behavior of None prior to Python 3.12."
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 273,
                "PatchRowcode": "+        parent_block_hash = hash('None')"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 274,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 275,
                "PatchRowcode": "     curr_block_token_ids_tuple = tuple(curr_block_token_ids)"
            },
            "13": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 276,
                "PatchRowcode": "     return BlockHashType("
            },
            "14": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 277,
                "PatchRowcode": "         hash((parent_block_hash, curr_block_token_ids_tuple, extra_keys)),"
            }
        },
        "frontPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "\"\"\"KV-Cache Utilities.\"\"\"",
            "from collections.abc import Sequence",
            "from dataclasses import dataclass",
            "from typing import Any, List, NamedTuple, Optional, Tuple",
            "",
            "from vllm.config import VllmConfig",
            "from vllm.logger import init_logger",
            "from vllm.v1.kv_cache_interface import (KVCacheConfig, KVCacheSpec,",
            "                                        KVCacheTensor)",
            "from vllm.v1.request import Request",
            "",
            "logger = init_logger(__name__)",
            "",
            "",
            "class BlockHashType(NamedTuple):",
            "    \"\"\"Hash value of a block (int), the token IDs in the block, and extra keys.",
            "    We keep a tuple of token IDs and extra keys to reduce the likelihood of",
            "    hash collisions when the hash value is the same. But please note that ",
            "    hash collisions can still theoretically occur, albeit with an extremely ",
            "    low probability.",
            "    \"\"\"",
            "    # Hash value of the block in an integer.",
            "    hash_value: int",
            "    # Token IDs in the block.",
            "    token_ids: Tuple[int, ...]",
            "    # Extra keys for the block.",
            "    extra_keys: Optional[Any] = None",
            "",
            "",
            "@dataclass",
            "class KVCacheBlock:",
            "    \"\"\"KV-cache block metadata.\"\"\"",
            "    # Block ID, ranging from 0 to num_gpu_blocks - 1.",
            "    block_id: int",
            "    # Reference count.",
            "    ref_cnt: int = 0",
            "    # The hash of the block composed of (block hash, tuple of token IDs).",
            "    # It is only available when the block is full.",
            "    _block_hash: Optional[BlockHashType] = None",
            "",
            "    # Used to construct a doubly linked list for free blocks.",
            "    # These two attributes should only be manipulated by FreeKVCacheBlockQueue.",
            "    prev_free_block: Optional[\"KVCacheBlock\"] = None",
            "    next_free_block: Optional[\"KVCacheBlock\"] = None",
            "",
            "    def incr_ref(self):",
            "        self.ref_cnt += 1",
            "",
            "    def decr_ref(self):",
            "        self.ref_cnt -= 1",
            "",
            "    @property",
            "    def block_hash(self) -> Optional[BlockHashType]:",
            "        return self._block_hash",
            "",
            "    @block_hash.setter",
            "    def block_hash(self, block_hash: BlockHashType):",
            "        assert self.block_hash is None, (",
            "            \"The block already has a hash. This should not happen.\")",
            "        self._block_hash = block_hash",
            "",
            "    def reset_hash(self):",
            "        \"\"\"Reset the block hash when the block is evicted.\"\"\"",
            "        self._block_hash = None",
            "",
            "",
            "class FreeKVCacheBlockQueue:",
            "    \"\"\"This class organizes a list of KVCacheBlock objects to a doubly linked",
            "    list of free blocks. We implement this class instead of using Python",
            "    builtin deque to support removing a block in the middle of the queue",
            "    in O(1) time. To close the performance gap to the builtin deque which is",
            "    implemented in C++, this class does not allocate any Python objects when",
            "    manipulating the linked list. Instead, this class manipulates the ",
            "    prev_free_block and next_free_block attributes of the given blocks.",
            "",
            "    The queue is ordered by block ID in the beginning. When a block is allocated",
            "    and then freed, it will be appended back with the eviction order:",
            "    1. The least recent used block is at the front (LRU).",
            "    2. If two blocks have the same last accessed time (allocated by the",
            "       same sequence), the one with more hash tokens (the tail of a block",
            "       chain) is at the front.",
            "    Note that we maintain this order by reversing the block order when free",
            "    blocks of a request. This operation is outside of this class.",
            "",
            "    Args:",
            "        blocks: A list of KVCacheBlock objects.",
            "    \"\"\"",
            "",
            "    def __init__(self, blocks: List[KVCacheBlock]) -> None:",
            "        self.num_free_blocks = len(blocks)",
            "",
            "        # Initialize the doubly linked list of free blocks.",
            "        self.free_list_head: Optional[KVCacheBlock] = blocks[0]",
            "        self.free_list_tail: Optional[KVCacheBlock] = blocks[-1]",
            "        for i in range(self.num_free_blocks):",
            "            if i > 0:",
            "                blocks[i].prev_free_block = blocks[i - 1]",
            "            if i < self.num_free_blocks - 1:",
            "                blocks[i].next_free_block = blocks[i + 1]",
            "",
            "    def popleft(self) -> KVCacheBlock:",
            "        \"\"\"Pop the first free block and reduce num_free_blocks by 1.",
            "        ",
            "        Returns:",
            "            The first free block.",
            "        \"\"\"",
            "        if not self.free_list_head:",
            "            raise ValueError(\"No free blocks available\")",
            "",
            "        block = self.free_list_head",
            "        self.remove(block)",
            "        return block",
            "",
            "    def remove(self, block: KVCacheBlock) -> None:",
            "        \"\"\"Remove a block in the free list and reduce num_free_blocks by 1.",
            "        ",
            "        Args:",
            "            block: The block to remove.",
            "        \"\"\"",
            "        if block.prev_free_block is not None:",
            "            # Link the previous block to the next block.",
            "            block.prev_free_block.next_free_block = block.next_free_block",
            "        if block.next_free_block is not None:",
            "            # Link the next block to the previous block.",
            "            block.next_free_block.prev_free_block = block.prev_free_block",
            "",
            "        if block == self.free_list_head:",
            "            # Update the head if the block is the head.",
            "            self.free_list_head = block.next_free_block",
            "        if block == self.free_list_tail:",
            "            # Update the tail if the block is the tail.",
            "            self.free_list_tail = block.prev_free_block",
            "",
            "        # Remove the block from the linked list.",
            "        block.prev_free_block = block.next_free_block = None",
            "        self.num_free_blocks -= 1",
            "",
            "    def append(self, block: KVCacheBlock) -> None:",
            "        \"\"\"Put a block back into the free list and increase",
            "        num_free_blocks by 1.",
            "",
            "        Args:",
            "            block: The block to append.",
            "        \"\"\"",
            "        if self.free_list_tail is not None:",
            "            # Link the last block to the new block.",
            "            self.free_list_tail.next_free_block = block",
            "            block.prev_free_block = self.free_list_tail",
            "            self.free_list_tail = block",
            "        else:",
            "            # The free list is empty.",
            "            assert self.free_list_head is None",
            "            self.free_list_head = self.free_list_tail = block",
            "",
            "        block.next_free_block = None",
            "        self.num_free_blocks += 1",
            "",
            "    def get_all_free_blocks(self) -> List[KVCacheBlock]:",
            "        \"\"\"Get all free blocks in the free list. Mainly used for testing.",
            "        ",
            "        Returns:",
            "            A list of free blocks.",
            "        \"\"\"",
            "        ret = []",
            "        curr_block = self.free_list_head",
            "        while curr_block is not None:",
            "            ret.append(curr_block)",
            "            curr_block = curr_block.next_free_block",
            "        return ret",
            "",
            "",
            "def generate_block_hash_extra_keys(",
            "        request: Request, start_token_idx: int, end_token_idx: int,",
            "        start_mm_idx: int) -> Tuple[Optional[Tuple[Any, ...]], int]:",
            "    \"\"\"Generate extra keys for the block hash. The extra keys can come from",
            "    the multi-modal inputs and request specific metadata (e.g., LoRA ID).",
            "    For multi-modal inputs, the extra keys are (mm_hash, start_offset) that",
            "    indicate a mm input contained in the block and its starting offset in",
            "    the block tokens.",
            "    ",
            "    Args:",
            "        request: The request object.",
            "        start_token_idx: The start token index of the block.",
            "        end_token_idx: The end token index of the block.",
            "        start_mm_idx: The start multi-modal index of the block.",
            "    ",
            "    Returns:",
            "        A tuple of extra keys and the next multi-modal index.",
            "    \"\"\"",
            "",
            "    mm_positions, mm_hashes = request.mm_positions, request.mm_hashes",
            "    if not mm_positions:",
            "        return None, start_mm_idx",
            "",
            "    if mm_positions and len(mm_positions) != len(mm_hashes):",
            "        raise ValueError(",
            "            \"The number of multi-modal positions and hashes must match. This \"",
            "            \"is likely because you do not enable MM preprocessor hashing. \"",
            "            \"Please set disable_mm_preprocessor_cache=False.\")",
            "",
            "    # Note that we assume mm_positions is sorted by offset.",
            "    # We do not need to check all mm inputs if the start token index is out of",
            "    # range. This usually happens in the late prefill phase and decoding phase.",
            "    if mm_positions[-1][\"offset\"] + mm_positions[-1][",
            "            \"length\"] < start_token_idx:",
            "        return None, start_mm_idx",
            "",
            "    # Support start_mm_idx == -1 to indicate the last mm input.",
            "    if start_mm_idx < 0:",
            "        assert -start_mm_idx <= len(mm_positions)",
            "        start_mm_idx = len(mm_positions) + start_mm_idx",
            "",
            "    extra_keys = []",
            "    curr_mm_idx = start_mm_idx",
            "    while mm_positions and curr_mm_idx < len(mm_positions):",
            "        assert mm_hashes[curr_mm_idx] is not None",
            "        offset = mm_positions[curr_mm_idx][\"offset\"]",
            "        length = mm_positions[curr_mm_idx][\"length\"]",
            "        if end_token_idx > offset:",
            "            if start_token_idx > offset + length:",
            "                # This block has passed the current mm input.",
            "                curr_mm_idx += 1",
            "                continue",
            "",
            "            # The block contains the current mm input.",
            "            extra_keys.append(mm_hashes[curr_mm_idx])",
            "",
            "            if end_token_idx >= offset + length:",
            "                # If this block contains the end of the current mm input,",
            "                # move to the next mm input as this block may also contain",
            "                # the next mm input.",
            "                curr_mm_idx += 1",
            "            else:",
            "                # Otherwise this block is done with mm inputs.",
            "                break",
            "        else:",
            "            # This block has not reached the current mm input.",
            "            break",
            "    return tuple(extra_keys), curr_mm_idx",
            "",
            "",
            "def hash_block_tokens(",
            "        parent_block_hash: Optional[int],",
            "        curr_block_token_ids: Sequence[int],",
            "        extra_keys: Optional[Tuple[Any, ...]] = None) -> BlockHashType:",
            "    \"\"\"Computes a hash value corresponding to the contents of a block and",
            "    the contents of the preceding block(s). The hash value is used for",
            "    prefix caching. We use LRU cache for this function to avoid recomputing",
            "    hash values for the same block contents.",
            "",
            "    TODO: Support arbitrary metadata so that we could support more",
            "    features such as LoRA adapter.",
            "",
            "    Args:",
            "        parent_block_hash: The hash of the parent block. None",
            "            if this is the first block.",
            "        curr_block_token_ids: A list of token ids in the current",
            "            block. The current block is assumed to be full.",
            "        extra_keys: Extra keys for the block.",
            "",
            "    Returns:",
            "        The hash value of the block and the token ids in the block.",
            "        The entire tuple is used as the hash key of the block.",
            "    \"\"\"",
            "    curr_block_token_ids_tuple = tuple(curr_block_token_ids)",
            "    return BlockHashType(",
            "        hash((parent_block_hash, curr_block_token_ids_tuple, extra_keys)),",
            "        curr_block_token_ids_tuple, extra_keys)",
            "",
            "",
            "def hash_request_tokens(block_size: int,",
            "                        request: Request) -> List[BlockHashType]:",
            "    \"\"\"Computes hash values of a chain of blocks given a sequence of",
            "    token IDs. The hash value is used for prefix caching.",
            "",
            "    Args:",
            "        block_size: The size of each block.",
            "        request: The request object.",
            "",
            "    Returns:",
            "        The list of computed hash values.",
            "    \"\"\"",
            "    token_ids = request.all_token_ids",
            "    mm_positions, mm_hashes = request.mm_positions, request.mm_hashes",
            "    if mm_positions and len(mm_positions) != len(mm_hashes):",
            "        raise ValueError(",
            "            \"The number of multi-modal positions and hashes must match.\")",
            "",
            "    # TODO: Extend this to support other features such as LoRA.",
            "    need_extra_keys = bool(mm_positions)",
            "    extra_keys = None",
            "    curr_mm_idx = 0",
            "",
            "    ret = []",
            "    parent_block_hash_value = None",
            "    for start in range(0, len(token_ids), block_size):",
            "        end = start + block_size",
            "        block_token_ids = token_ids[start:end]",
            "        # Do not hash the block if it is not full.",
            "        if len(block_token_ids) < block_size:",
            "            break",
            "",
            "        # Add extra keys if the block is a multi-modal block.",
            "        if need_extra_keys:",
            "            extra_keys, curr_mm_idx = generate_block_hash_extra_keys(",
            "                request, start, end, curr_mm_idx)",
            "",
            "        block_hash = hash_block_tokens(parent_block_hash_value,",
            "                                       block_token_ids, extra_keys)",
            "        ret.append(block_hash)",
            "        parent_block_hash_value = block_hash.hash_value",
            "    return ret",
            "",
            "",
            "def check_enough_kv_cache_memory(vllm_config: VllmConfig,",
            "                                 kv_cache_spec: KVCacheSpec,",
            "                                 available_memory: int):",
            "    \"\"\"",
            "    Checks whether `available_memory` is enough for the KV cache to hold at ",
            "    least one request with the model's max_model_len.",
            "",
            "    Args:",
            "        vllm_config: The global VllmConfig",
            "        kv_cache_spec: The kv cache spec of the model",
            "        available_memory: Memory available for KV cache in bytes.",
            "",
            "    Raises:",
            "        ValueError: If there is not enough memory available for the KV cache.",
            "    \"\"\"",
            "",
            "    if available_memory <= 0:",
            "        raise ValueError(\"No available memory for the cache blocks. \"",
            "                         \"Try increasing `gpu_memory_utilization` when \"",
            "                         \"initializing the engine.\")",
            "",
            "    max_model_len = vllm_config.model_config.max_model_len",
            "    needed_memory = 0",
            "    for layer_spec in kv_cache_spec.values():",
            "        needed_memory += layer_spec.bytes_for_tokens(max_model_len)",
            "",
            "    if needed_memory > available_memory:",
            "        raise ValueError(",
            "            f\"To serve at least one request with the models's max seq len \"",
            "            f\"({max_model_len}), ({needed_memory/1024/1024/1024:.2f} GB KV \"",
            "            f\"cache is needed, which is larger than the available KV cache \"",
            "            f\"memory ({available_memory/1024/1024/1024:.2f} GB). Try \"",
            "            f\"increasing `gpu_memory_utilization` or decreasing \"",
            "            f\"`max_model_len` when initializing the engine.\")",
            "",
            "",
            "def is_kv_cache_type_uniform(kv_cache_spec: KVCacheSpec) -> bool:",
            "    \"\"\"",
            "    Whether all layers in the given KVCacheSpec have the same type of KV cache.",
            "",
            "    Args:",
            "        kv_cache_spec: The KVCacheSpec of the model",
            "",
            "    Returns:",
            "        True if all layers have the same type, False otherwise.",
            "    \"\"\"",
            "",
            "    layer_keys = set(layer.type_id for layer in kv_cache_spec.values())",
            "    return len(layer_keys) == 1",
            "",
            "",
            "def _get_kv_cache_config_uniform_type(vllm_config: VllmConfig,",
            "                                      kv_cache_spec: KVCacheSpec,",
            "                                      available_memory: int) -> KVCacheConfig:",
            "    \"\"\"",
            "    Generates the KV cache configuration for a model with one type of KV cache.",
            "    Divide the available memory equally among all layers.",
            "",
            "    Args:",
            "        vllm_config: The global VllmConfig",
            "        kv_cache_spec: The kv cache spec of the model",
            "        available_memory: Memory available for KV cache in bytes.",
            "",
            "    Returns:",
            "        The generated KVCacheConfig",
            "    \"\"\"",
            "",
            "    page_sizes = {layer.page_size_bytes for layer in kv_cache_spec.values()}",
            "    assert len(page_sizes) == 1",
            "    page_size = page_sizes.pop()",
            "",
            "    num_blocks = int(available_memory // page_size // len(kv_cache_spec))",
            "    num_blocks = max(num_blocks, 0)",
            "",
            "    if vllm_config.cache_config.num_gpu_blocks_override is not None:",
            "        num_gpu_blocks_override = \\",
            "            vllm_config.cache_config.num_gpu_blocks_override",
            "        logger.info(",
            "            \"Overriding num_gpu_blocks=%d with \"",
            "            \"num_gpu_blocks_override=%d\", num_blocks, num_gpu_blocks_override)",
            "        num_blocks = num_gpu_blocks_override",
            "",
            "    logger.info(\"# GPU blocks: %d\", num_blocks)",
            "    max_concurrency = (num_blocks * vllm_config.cache_config.block_size /",
            "                       vllm_config.model_config.max_model_len)",
            "    logger.info(\"Maximum concurrency for %s tokens per request: %.2fx\",",
            "                vllm_config.model_config.max_model_len, max_concurrency)",
            "",
            "    per_layer_size = page_size * num_blocks",
            "",
            "    kv_cache_config = KVCacheConfig(",
            "        num_blocks=num_blocks,",
            "        tensors={",
            "            layer_name: KVCacheTensor(size=per_layer_size)",
            "            for layer_name in kv_cache_spec",
            "        },",
            "        groups=[[layer_name for layer_name in kv_cache_spec]],",
            "        kv_cache_spec=kv_cache_spec)",
            "    return kv_cache_config",
            "",
            "",
            "def get_kv_cache_config(vllm_config: VllmConfig, kv_cache_spec: KVCacheSpec,",
            "                        available_memory: int) -> KVCacheConfig:",
            "    \"\"\"",
            "    Generates the KV cache configuration for a model",
            "    TODO: support hybrid models with more than one type of KV cache.",
            "",
            "    Args:",
            "        vllm_config: The global VllmConfig",
            "        kv_cache_spec: The kv cache spec of the model",
            "        available_memory: Memory available for KV cache in bytes.",
            "",
            "    Returns:",
            "        The generated KVCacheConfig",
            "    \"\"\"",
            "    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)",
            "    if is_kv_cache_type_uniform(kv_cache_spec):",
            "        # KV cache of all layers are the same, which is true for most models.",
            "        # Allocate the same amount of memory for each layer.",
            "        return _get_kv_cache_config_uniform_type(vllm_config, kv_cache_spec,",
            "                                                 available_memory)",
            "    else:",
            "        raise NotImplementedError"
        ],
        "afterPatchFile": [
            "# SPDX-License-Identifier: Apache-2.0",
            "\"\"\"KV-Cache Utilities.\"\"\"",
            "from collections.abc import Sequence",
            "from dataclasses import dataclass",
            "from typing import Any, List, NamedTuple, Optional, Tuple",
            "",
            "from vllm.config import VllmConfig",
            "from vllm.logger import init_logger",
            "from vllm.v1.kv_cache_interface import (KVCacheConfig, KVCacheSpec,",
            "                                        KVCacheTensor)",
            "from vllm.v1.request import Request",
            "",
            "logger = init_logger(__name__)",
            "",
            "",
            "class BlockHashType(NamedTuple):",
            "    \"\"\"Hash value of a block (int), the token IDs in the block, and extra keys.",
            "    We keep a tuple of token IDs and extra keys to reduce the likelihood of",
            "    hash collisions when the hash value is the same. But please note that ",
            "    hash collisions can still theoretically occur, albeit with an extremely ",
            "    low probability.",
            "    \"\"\"",
            "    # Hash value of the block in an integer.",
            "    hash_value: int",
            "    # Token IDs in the block.",
            "    token_ids: Tuple[int, ...]",
            "    # Extra keys for the block.",
            "    extra_keys: Optional[Any] = None",
            "",
            "",
            "@dataclass",
            "class KVCacheBlock:",
            "    \"\"\"KV-cache block metadata.\"\"\"",
            "    # Block ID, ranging from 0 to num_gpu_blocks - 1.",
            "    block_id: int",
            "    # Reference count.",
            "    ref_cnt: int = 0",
            "    # The hash of the block composed of (block hash, tuple of token IDs).",
            "    # It is only available when the block is full.",
            "    _block_hash: Optional[BlockHashType] = None",
            "",
            "    # Used to construct a doubly linked list for free blocks.",
            "    # These two attributes should only be manipulated by FreeKVCacheBlockQueue.",
            "    prev_free_block: Optional[\"KVCacheBlock\"] = None",
            "    next_free_block: Optional[\"KVCacheBlock\"] = None",
            "",
            "    def incr_ref(self):",
            "        self.ref_cnt += 1",
            "",
            "    def decr_ref(self):",
            "        self.ref_cnt -= 1",
            "",
            "    @property",
            "    def block_hash(self) -> Optional[BlockHashType]:",
            "        return self._block_hash",
            "",
            "    @block_hash.setter",
            "    def block_hash(self, block_hash: BlockHashType):",
            "        assert self.block_hash is None, (",
            "            \"The block already has a hash. This should not happen.\")",
            "        self._block_hash = block_hash",
            "",
            "    def reset_hash(self):",
            "        \"\"\"Reset the block hash when the block is evicted.\"\"\"",
            "        self._block_hash = None",
            "",
            "",
            "class FreeKVCacheBlockQueue:",
            "    \"\"\"This class organizes a list of KVCacheBlock objects to a doubly linked",
            "    list of free blocks. We implement this class instead of using Python",
            "    builtin deque to support removing a block in the middle of the queue",
            "    in O(1) time. To close the performance gap to the builtin deque which is",
            "    implemented in C++, this class does not allocate any Python objects when",
            "    manipulating the linked list. Instead, this class manipulates the ",
            "    prev_free_block and next_free_block attributes of the given blocks.",
            "",
            "    The queue is ordered by block ID in the beginning. When a block is allocated",
            "    and then freed, it will be appended back with the eviction order:",
            "    1. The least recent used block is at the front (LRU).",
            "    2. If two blocks have the same last accessed time (allocated by the",
            "       same sequence), the one with more hash tokens (the tail of a block",
            "       chain) is at the front.",
            "    Note that we maintain this order by reversing the block order when free",
            "    blocks of a request. This operation is outside of this class.",
            "",
            "    Args:",
            "        blocks: A list of KVCacheBlock objects.",
            "    \"\"\"",
            "",
            "    def __init__(self, blocks: List[KVCacheBlock]) -> None:",
            "        self.num_free_blocks = len(blocks)",
            "",
            "        # Initialize the doubly linked list of free blocks.",
            "        self.free_list_head: Optional[KVCacheBlock] = blocks[0]",
            "        self.free_list_tail: Optional[KVCacheBlock] = blocks[-1]",
            "        for i in range(self.num_free_blocks):",
            "            if i > 0:",
            "                blocks[i].prev_free_block = blocks[i - 1]",
            "            if i < self.num_free_blocks - 1:",
            "                blocks[i].next_free_block = blocks[i + 1]",
            "",
            "    def popleft(self) -> KVCacheBlock:",
            "        \"\"\"Pop the first free block and reduce num_free_blocks by 1.",
            "        ",
            "        Returns:",
            "            The first free block.",
            "        \"\"\"",
            "        if not self.free_list_head:",
            "            raise ValueError(\"No free blocks available\")",
            "",
            "        block = self.free_list_head",
            "        self.remove(block)",
            "        return block",
            "",
            "    def remove(self, block: KVCacheBlock) -> None:",
            "        \"\"\"Remove a block in the free list and reduce num_free_blocks by 1.",
            "        ",
            "        Args:",
            "            block: The block to remove.",
            "        \"\"\"",
            "        if block.prev_free_block is not None:",
            "            # Link the previous block to the next block.",
            "            block.prev_free_block.next_free_block = block.next_free_block",
            "        if block.next_free_block is not None:",
            "            # Link the next block to the previous block.",
            "            block.next_free_block.prev_free_block = block.prev_free_block",
            "",
            "        if block == self.free_list_head:",
            "            # Update the head if the block is the head.",
            "            self.free_list_head = block.next_free_block",
            "        if block == self.free_list_tail:",
            "            # Update the tail if the block is the tail.",
            "            self.free_list_tail = block.prev_free_block",
            "",
            "        # Remove the block from the linked list.",
            "        block.prev_free_block = block.next_free_block = None",
            "        self.num_free_blocks -= 1",
            "",
            "    def append(self, block: KVCacheBlock) -> None:",
            "        \"\"\"Put a block back into the free list and increase",
            "        num_free_blocks by 1.",
            "",
            "        Args:",
            "            block: The block to append.",
            "        \"\"\"",
            "        if self.free_list_tail is not None:",
            "            # Link the last block to the new block.",
            "            self.free_list_tail.next_free_block = block",
            "            block.prev_free_block = self.free_list_tail",
            "            self.free_list_tail = block",
            "        else:",
            "            # The free list is empty.",
            "            assert self.free_list_head is None",
            "            self.free_list_head = self.free_list_tail = block",
            "",
            "        block.next_free_block = None",
            "        self.num_free_blocks += 1",
            "",
            "    def get_all_free_blocks(self) -> List[KVCacheBlock]:",
            "        \"\"\"Get all free blocks in the free list. Mainly used for testing.",
            "        ",
            "        Returns:",
            "            A list of free blocks.",
            "        \"\"\"",
            "        ret = []",
            "        curr_block = self.free_list_head",
            "        while curr_block is not None:",
            "            ret.append(curr_block)",
            "            curr_block = curr_block.next_free_block",
            "        return ret",
            "",
            "",
            "def generate_block_hash_extra_keys(",
            "        request: Request, start_token_idx: int, end_token_idx: int,",
            "        start_mm_idx: int) -> Tuple[Optional[Tuple[Any, ...]], int]:",
            "    \"\"\"Generate extra keys for the block hash. The extra keys can come from",
            "    the multi-modal inputs and request specific metadata (e.g., LoRA ID).",
            "    For multi-modal inputs, the extra keys are (mm_hash, start_offset) that",
            "    indicate a mm input contained in the block and its starting offset in",
            "    the block tokens.",
            "    ",
            "    Args:",
            "        request: The request object.",
            "        start_token_idx: The start token index of the block.",
            "        end_token_idx: The end token index of the block.",
            "        start_mm_idx: The start multi-modal index of the block.",
            "    ",
            "    Returns:",
            "        A tuple of extra keys and the next multi-modal index.",
            "    \"\"\"",
            "",
            "    mm_positions, mm_hashes = request.mm_positions, request.mm_hashes",
            "    if not mm_positions:",
            "        return None, start_mm_idx",
            "",
            "    if mm_positions and len(mm_positions) != len(mm_hashes):",
            "        raise ValueError(",
            "            \"The number of multi-modal positions and hashes must match. This \"",
            "            \"is likely because you do not enable MM preprocessor hashing. \"",
            "            \"Please set disable_mm_preprocessor_cache=False.\")",
            "",
            "    # Note that we assume mm_positions is sorted by offset.",
            "    # We do not need to check all mm inputs if the start token index is out of",
            "    # range. This usually happens in the late prefill phase and decoding phase.",
            "    if mm_positions[-1][\"offset\"] + mm_positions[-1][",
            "            \"length\"] < start_token_idx:",
            "        return None, start_mm_idx",
            "",
            "    # Support start_mm_idx == -1 to indicate the last mm input.",
            "    if start_mm_idx < 0:",
            "        assert -start_mm_idx <= len(mm_positions)",
            "        start_mm_idx = len(mm_positions) + start_mm_idx",
            "",
            "    extra_keys = []",
            "    curr_mm_idx = start_mm_idx",
            "    while mm_positions and curr_mm_idx < len(mm_positions):",
            "        assert mm_hashes[curr_mm_idx] is not None",
            "        offset = mm_positions[curr_mm_idx][\"offset\"]",
            "        length = mm_positions[curr_mm_idx][\"length\"]",
            "        if end_token_idx > offset:",
            "            if start_token_idx > offset + length:",
            "                # This block has passed the current mm input.",
            "                curr_mm_idx += 1",
            "                continue",
            "",
            "            # The block contains the current mm input.",
            "            extra_keys.append(mm_hashes[curr_mm_idx])",
            "",
            "            if end_token_idx >= offset + length:",
            "                # If this block contains the end of the current mm input,",
            "                # move to the next mm input as this block may also contain",
            "                # the next mm input.",
            "                curr_mm_idx += 1",
            "            else:",
            "                # Otherwise this block is done with mm inputs.",
            "                break",
            "        else:",
            "            # This block has not reached the current mm input.",
            "            break",
            "    return tuple(extra_keys), curr_mm_idx",
            "",
            "",
            "def hash_block_tokens(",
            "        parent_block_hash: Optional[int],",
            "        curr_block_token_ids: Sequence[int],",
            "        extra_keys: Optional[Tuple[Any, ...]] = None) -> BlockHashType:",
            "    \"\"\"Computes a hash value corresponding to the contents of a block and",
            "    the contents of the preceding block(s). The hash value is used for",
            "    prefix caching. We use LRU cache for this function to avoid recomputing",
            "    hash values for the same block contents.",
            "",
            "    TODO: Support arbitrary metadata so that we could support more",
            "    features such as LoRA adapter.",
            "",
            "    Args:",
            "        parent_block_hash: The hash of the parent block. None",
            "            if this is the first block.",
            "        curr_block_token_ids: A list of token ids in the current",
            "            block. The current block is assumed to be full.",
            "        extra_keys: Extra keys for the block.",
            "",
            "    Returns:",
            "        The hash value of the block and the token ids in the block.",
            "        The entire tuple is used as the hash key of the block.",
            "    \"\"\"",
            "    if not parent_block_hash:",
            "        # Note that we use 'None' as a string here instead of None because",
            "        # as of Python 3.12, hash(None) returns a constant predictable value.",
            "        # This could possibly make it easier to find and exploit hash",
            "        # collisions. 'None' as a string will be hashed differently per process,",
            "        # but consistently within the same process. This is the same as the",
            "        # behavior of None prior to Python 3.12.",
            "        parent_block_hash = hash('None')",
            "",
            "    curr_block_token_ids_tuple = tuple(curr_block_token_ids)",
            "    return BlockHashType(",
            "        hash((parent_block_hash, curr_block_token_ids_tuple, extra_keys)),",
            "        curr_block_token_ids_tuple, extra_keys)",
            "",
            "",
            "def hash_request_tokens(block_size: int,",
            "                        request: Request) -> List[BlockHashType]:",
            "    \"\"\"Computes hash values of a chain of blocks given a sequence of",
            "    token IDs. The hash value is used for prefix caching.",
            "",
            "    Args:",
            "        block_size: The size of each block.",
            "        request: The request object.",
            "",
            "    Returns:",
            "        The list of computed hash values.",
            "    \"\"\"",
            "    token_ids = request.all_token_ids",
            "    mm_positions, mm_hashes = request.mm_positions, request.mm_hashes",
            "    if mm_positions and len(mm_positions) != len(mm_hashes):",
            "        raise ValueError(",
            "            \"The number of multi-modal positions and hashes must match.\")",
            "",
            "    # TODO: Extend this to support other features such as LoRA.",
            "    need_extra_keys = bool(mm_positions)",
            "    extra_keys = None",
            "    curr_mm_idx = 0",
            "",
            "    ret = []",
            "    parent_block_hash_value = None",
            "    for start in range(0, len(token_ids), block_size):",
            "        end = start + block_size",
            "        block_token_ids = token_ids[start:end]",
            "        # Do not hash the block if it is not full.",
            "        if len(block_token_ids) < block_size:",
            "            break",
            "",
            "        # Add extra keys if the block is a multi-modal block.",
            "        if need_extra_keys:",
            "            extra_keys, curr_mm_idx = generate_block_hash_extra_keys(",
            "                request, start, end, curr_mm_idx)",
            "",
            "        block_hash = hash_block_tokens(parent_block_hash_value,",
            "                                       block_token_ids, extra_keys)",
            "        ret.append(block_hash)",
            "        parent_block_hash_value = block_hash.hash_value",
            "    return ret",
            "",
            "",
            "def check_enough_kv_cache_memory(vllm_config: VllmConfig,",
            "                                 kv_cache_spec: KVCacheSpec,",
            "                                 available_memory: int):",
            "    \"\"\"",
            "    Checks whether `available_memory` is enough for the KV cache to hold at ",
            "    least one request with the model's max_model_len.",
            "",
            "    Args:",
            "        vllm_config: The global VllmConfig",
            "        kv_cache_spec: The kv cache spec of the model",
            "        available_memory: Memory available for KV cache in bytes.",
            "",
            "    Raises:",
            "        ValueError: If there is not enough memory available for the KV cache.",
            "    \"\"\"",
            "",
            "    if available_memory <= 0:",
            "        raise ValueError(\"No available memory for the cache blocks. \"",
            "                         \"Try increasing `gpu_memory_utilization` when \"",
            "                         \"initializing the engine.\")",
            "",
            "    max_model_len = vllm_config.model_config.max_model_len",
            "    needed_memory = 0",
            "    for layer_spec in kv_cache_spec.values():",
            "        needed_memory += layer_spec.bytes_for_tokens(max_model_len)",
            "",
            "    if needed_memory > available_memory:",
            "        raise ValueError(",
            "            f\"To serve at least one request with the models's max seq len \"",
            "            f\"({max_model_len}), ({needed_memory/1024/1024/1024:.2f} GB KV \"",
            "            f\"cache is needed, which is larger than the available KV cache \"",
            "            f\"memory ({available_memory/1024/1024/1024:.2f} GB). Try \"",
            "            f\"increasing `gpu_memory_utilization` or decreasing \"",
            "            f\"`max_model_len` when initializing the engine.\")",
            "",
            "",
            "def is_kv_cache_type_uniform(kv_cache_spec: KVCacheSpec) -> bool:",
            "    \"\"\"",
            "    Whether all layers in the given KVCacheSpec have the same type of KV cache.",
            "",
            "    Args:",
            "        kv_cache_spec: The KVCacheSpec of the model",
            "",
            "    Returns:",
            "        True if all layers have the same type, False otherwise.",
            "    \"\"\"",
            "",
            "    layer_keys = set(layer.type_id for layer in kv_cache_spec.values())",
            "    return len(layer_keys) == 1",
            "",
            "",
            "def _get_kv_cache_config_uniform_type(vllm_config: VllmConfig,",
            "                                      kv_cache_spec: KVCacheSpec,",
            "                                      available_memory: int) -> KVCacheConfig:",
            "    \"\"\"",
            "    Generates the KV cache configuration for a model with one type of KV cache.",
            "    Divide the available memory equally among all layers.",
            "",
            "    Args:",
            "        vllm_config: The global VllmConfig",
            "        kv_cache_spec: The kv cache spec of the model",
            "        available_memory: Memory available for KV cache in bytes.",
            "",
            "    Returns:",
            "        The generated KVCacheConfig",
            "    \"\"\"",
            "",
            "    page_sizes = {layer.page_size_bytes for layer in kv_cache_spec.values()}",
            "    assert len(page_sizes) == 1",
            "    page_size = page_sizes.pop()",
            "",
            "    num_blocks = int(available_memory // page_size // len(kv_cache_spec))",
            "    num_blocks = max(num_blocks, 0)",
            "",
            "    if vllm_config.cache_config.num_gpu_blocks_override is not None:",
            "        num_gpu_blocks_override = \\",
            "            vllm_config.cache_config.num_gpu_blocks_override",
            "        logger.info(",
            "            \"Overriding num_gpu_blocks=%d with \"",
            "            \"num_gpu_blocks_override=%d\", num_blocks, num_gpu_blocks_override)",
            "        num_blocks = num_gpu_blocks_override",
            "",
            "    logger.info(\"# GPU blocks: %d\", num_blocks)",
            "    max_concurrency = (num_blocks * vllm_config.cache_config.block_size /",
            "                       vllm_config.model_config.max_model_len)",
            "    logger.info(\"Maximum concurrency for %s tokens per request: %.2fx\",",
            "                vllm_config.model_config.max_model_len, max_concurrency)",
            "",
            "    per_layer_size = page_size * num_blocks",
            "",
            "    kv_cache_config = KVCacheConfig(",
            "        num_blocks=num_blocks,",
            "        tensors={",
            "            layer_name: KVCacheTensor(size=per_layer_size)",
            "            for layer_name in kv_cache_spec",
            "        },",
            "        groups=[[layer_name for layer_name in kv_cache_spec]],",
            "        kv_cache_spec=kv_cache_spec)",
            "    return kv_cache_config",
            "",
            "",
            "def get_kv_cache_config(vllm_config: VllmConfig, kv_cache_spec: KVCacheSpec,",
            "                        available_memory: int) -> KVCacheConfig:",
            "    \"\"\"",
            "    Generates the KV cache configuration for a model",
            "    TODO: support hybrid models with more than one type of KV cache.",
            "",
            "    Args:",
            "        vllm_config: The global VllmConfig",
            "        kv_cache_spec: The kv cache spec of the model",
            "        available_memory: Memory available for KV cache in bytes.",
            "",
            "    Returns:",
            "        The generated KVCacheConfig",
            "    \"\"\"",
            "    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)",
            "    if is_kv_cache_type_uniform(kv_cache_spec):",
            "        # KV cache of all layers are the same, which is true for most models.",
            "        # Allocate the same amount of memory for each layer.",
            "        return _get_kv_cache_config_uniform_type(vllm_config, kv_cache_spec,",
            "                                                 available_memory)",
            "    else:",
            "        raise NotImplementedError"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "openstack_dashboard.dashboards.project.stacks.forms.TemplateForm",
            "vllm.v1.core.kv_cache_utils.hash_request_tokens"
        ]
    }
}