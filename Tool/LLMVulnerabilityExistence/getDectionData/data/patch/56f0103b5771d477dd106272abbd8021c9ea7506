{
    "superset/config.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1203,
                "afterPatchRowNumber": 1203,
                "PatchRowcode": " # as such `create_engine(url, **params)`"
            },
            "1": {
                "beforePatchRowNumber": 1204,
                "afterPatchRowNumber": 1204,
                "PatchRowcode": " DB_CONNECTION_MUTATOR = None"
            },
            "2": {
                "beforePatchRowNumber": 1205,
                "afterPatchRowNumber": 1205,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1206,
                "PatchRowcode": "+# A set of disallowed SQL functions per engine. This is used to restrict the use of"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1207,
                "PatchRowcode": "+# unsafe SQL functions in SQL Lab and Charts. The keys of the dictionary are the engine"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1208,
                "PatchRowcode": "+# names, and the values are sets of disallowed functions."
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1209,
                "PatchRowcode": "+DISALLOWED_SQL_FUNCTIONS: dict[str, set[str]] = {"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1210,
                "PatchRowcode": "+    \"postgresql\": {\"version\", \"query_to_xml\", \"inet_server_addr\", \"inet_client_addr\"},"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1211,
                "PatchRowcode": "+    \"clickhouse\": {\"url\"},"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1212,
                "PatchRowcode": "+    \"mysql\": {\"version\"},"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1213,
                "PatchRowcode": "+}"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1214,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": 1206,
                "afterPatchRowNumber": 1215,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 1207,
                "afterPatchRowNumber": 1216,
                "PatchRowcode": " # A function that intercepts the SQL to be executed and can alter it."
            },
            "14": {
                "beforePatchRowNumber": 1208,
                "afterPatchRowNumber": 1217,
                "PatchRowcode": " # The use case is can be around adding some sort of comment header"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"The main config file for Superset",
            "",
            "All configuration in this file can be overridden by providing a superset_config",
            "in your PYTHONPATH as there is a ``from superset_config import *``",
            "at the end of this file.",
            "\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import imp  # pylint: disable=deprecated-module",
            "import importlib.util",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import sys",
            "from collections import OrderedDict",
            "from datetime import timedelta",
            "from email.mime.multipart import MIMEMultipart",
            "from importlib.resources import files",
            "from typing import Any, Callable, Literal, TYPE_CHECKING, TypedDict",
            "",
            "import pkg_resources",
            "from celery.schedules import crontab",
            "from flask import Blueprint",
            "from flask_appbuilder.security.manager import AUTH_DB",
            "from flask_caching.backends.base import BaseCache",
            "from pandas import Series",
            "from pandas._libs.parsers import STR_NA_VALUES  # pylint: disable=no-name-in-module",
            "from sqlalchemy.orm.query import Query",
            "",
            "from superset.advanced_data_type.plugins.internet_address import internet_address",
            "from superset.advanced_data_type.plugins.internet_port import internet_port",
            "from superset.advanced_data_type.types import AdvancedDataType",
            "from superset.constants import CHANGE_ME_SECRET_KEY",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.key_value.types import JsonKeyValueCodec",
            "from superset.stats_logger import DummyStatsLogger",
            "from superset.superset_typing import CacheConfig",
            "from superset.tasks.types import ExecutorType",
            "from superset.utils import core as utils",
            "from superset.utils.core import is_test, NO_TIME_RANGE, parse_boolean_string",
            "from superset.utils.encrypt import SQLAlchemyUtilsAdapter",
            "from superset.utils.log import DBEventLogger",
            "from superset.utils.logging_configurator import DefaultLoggingConfigurator",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from flask_appbuilder.security.sqla import models",
            "",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.slice import Slice",
            "",
            "# Realtime stats logger, a StatsD implementation exists",
            "STATS_LOGGER = DummyStatsLogger()",
            "EVENT_LOGGER = DBEventLogger()",
            "",
            "SUPERSET_LOG_VIEW = True",
            "",
            "BASE_DIR = pkg_resources.resource_filename(\"superset\", \"\")",
            "if \"SUPERSET_HOME\" in os.environ:",
            "    DATA_DIR = os.environ[\"SUPERSET_HOME\"]",
            "else:",
            "    DATA_DIR = os.path.expanduser(\"~/.superset\")",
            "",
            "# ---------------------------------------------------------",
            "# Superset specific config",
            "# ---------------------------------------------------------",
            "VERSION_INFO_FILE = str(files(\"superset\") / \"static/version_info.json\")",
            "PACKAGE_JSON_FILE = str(files(\"superset\") / \"static/assets/package.json\")",
            "",
            "",
            "# Multiple favicons can be specified here. The \"href\" property",
            "# is mandatory, but \"sizes,\" \"type,\" and \"rel\" are optional.",
            "# For example:",
            "# {",
            "#     \"href\":path/to/image.png\",",
            "#     \"sizes\": \"16x16\",",
            "#     \"type\": \"image/png\"",
            "#     \"rel\": \"icon\"",
            "# },",
            "FAVICONS = [{\"href\": \"/static/assets/images/favicon.png\"}]",
            "",
            "",
            "def _try_json_readversion(filepath: str) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"version\")",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def _try_json_readsha(filepath: str, length: int) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"GIT_SHA\")[:length]",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "#",
            "# If True, we will skip the call to load the logger config found in alembic.init",
            "#",
            "ALEMBIC_SKIP_LOG_CONFIG = False",
            "",
            "# Depending on the context in which this config is loaded, the",
            "# version_info.json file may or may not be available, as it is",
            "# generated on install via setup.py. In the event that we're",
            "# actually running Superset, we will have already installed,",
            "# therefore it WILL exist. When unit tests are running, however,",
            "# it WILL NOT exist, so we fall back to reading package.json",
            "VERSION_STRING = _try_json_readversion(VERSION_INFO_FILE) or _try_json_readversion(",
            "    PACKAGE_JSON_FILE",
            ")",
            "",
            "VERSION_SHA_LENGTH = 8",
            "VERSION_SHA = _try_json_readsha(VERSION_INFO_FILE, VERSION_SHA_LENGTH)",
            "",
            "# Build number is shown in the About section if available. This",
            "# can be replaced at build time to expose build information.",
            "BUILD_NUMBER = None",
            "",
            "# default viz used in chart explorer & SQL Lab explore",
            "DEFAULT_VIZ_TYPE = \"table\"",
            "",
            "# default row limit when requesting chart data",
            "ROW_LIMIT = 50000",
            "# default row limit when requesting samples from datasource in explore view",
            "SAMPLES_ROW_LIMIT = 1000",
            "# default row limit for native filters",
            "NATIVE_FILTER_DEFAULT_ROW_LIMIT = 1000",
            "# max rows retrieved by filter select auto complete",
            "FILTER_SELECT_ROW_LIMIT = 10000",
            "# default time filter in explore",
            "# values may be \"Last day\", \"Last week\", \"<ISO date> : now\", etc.",
            "DEFAULT_TIME_FILTER = NO_TIME_RANGE",
            "",
            "# This is an important setting, and should be lower than your",
            "# [load balancer / proxy / envoy / kong / ...] timeout settings.",
            "# You should also make sure to configure your WSGI server",
            "# (gunicorn, nginx, apache, ...) timeout setting to be <= to this setting",
            "SUPERSET_WEBSERVER_TIMEOUT = int(timedelta(minutes=1).total_seconds())",
            "",
            "# this 2 settings are used by dashboard period force refresh feature",
            "# When user choose auto force refresh frequency",
            "# < SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT",
            "# they will see warning message in the Refresh Interval Modal.",
            "# please check PR #9886",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT = 0",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE = None",
            "",
            "SUPERSET_DASHBOARD_POSITION_DATA_LIMIT = 65535",
            "CUSTOM_SECURITY_MANAGER = None",
            "SQLALCHEMY_TRACK_MODIFICATIONS = False",
            "# ---------------------------------------------------------",
            "",
            "# Your App secret key. Make sure you override it on superset_config.py",
            "# or use `SUPERSET_SECRET_KEY` environment variable.",
            "# Use a strong complex alphanumeric string and use a tool to help you generate",
            "# a sufficiently random sequence, ex: openssl rand -base64 42\"",
            "SECRET_KEY = os.environ.get(\"SUPERSET_SECRET_KEY\") or CHANGE_ME_SECRET_KEY",
            "",
            "# The SQLAlchemy connection string.",
            "SQLALCHEMY_DATABASE_URI = (",
            "    f\"\"\"sqlite:///{os.path.join(DATA_DIR, \"superset.db\")}?check_same_thread=false\"\"\"",
            ")",
            "",
            "# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'",
            "# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp'",
            "",
            "# In order to hook up a custom password store for all SQLALCHEMY connections",
            "# implement a function that takes a single argument of type 'sqla.engine.url',",
            "# returns a password and set SQLALCHEMY_CUSTOM_PASSWORD_STORE.",
            "#",
            "# e.g.:",
            "# def lookup_password(url):",
            "#     return 'secret'",
            "# SQLALCHEMY_CUSTOM_PASSWORD_STORE = lookup_password",
            "SQLALCHEMY_CUSTOM_PASSWORD_STORE = None",
            "",
            "#",
            "# The EncryptedFieldTypeAdapter is used whenever we're building SqlAlchemy models",
            "# which include sensitive fields that should be app-encrypted BEFORE sending",
            "# to the DB.",
            "#",
            "# Note: the default impl leverages SqlAlchemyUtils' EncryptedType, which defaults",
            "#  to AesEngine that uses AES-128 under the covers using the app's SECRET_KEY",
            "#  as key material. Do note that AesEngine allows for queryability over the",
            "#  encrypted fields.",
            "#",
            "#  To change the default engine you need to define your own adapter:",
            "#",
            "# e.g.:",
            "#",
            "# class AesGcmEncryptedAdapter(",
            "#     AbstractEncryptedFieldAdapter",
            "# ):",
            "#     def create(",
            "#         self,",
            "#         app_config: Optional[Dict[str, Any]],",
            "#         *args: List[Any],",
            "#         **kwargs: Optional[Dict[str, Any]],",
            "#     ) -> TypeDecorator:",
            "#         if app_config:",
            "#             return EncryptedType(",
            "#                 *args, app_config[\"SECRET_KEY\"], engine=AesGcmEngine, **kwargs",
            "#             )",
            "#         raise Exception(\"Missing app_config kwarg\")",
            "#",
            "#",
            "#  SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = AesGcmEncryptedAdapter",
            "SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = (  # pylint: disable=invalid-name",
            "    SQLAlchemyUtilsAdapter",
            ")",
            "# The limit of queries fetched for query search",
            "QUERY_SEARCH_LIMIT = 1000",
            "",
            "# Flask-WTF flag for CSRF",
            "WTF_CSRF_ENABLED = True",
            "",
            "# Add endpoints that need to be exempt from CSRF protection",
            "WTF_CSRF_EXEMPT_LIST = [",
            "    \"superset.views.core.log\",",
            "    \"superset.views.core.explore_json\",",
            "    \"superset.charts.data.api.data\",",
            "]",
            "",
            "# Whether to run the web server in debug mode or not",
            "DEBUG = os.environ.get(\"FLASK_DEBUG\")",
            "FLASK_USE_RELOAD = True",
            "",
            "# Enable profiling of Python calls. Turn this on and append ``?_instrument=1``",
            "# to the page to see the call stack.",
            "PROFILING = False",
            "",
            "# Superset allows server-side python stacktraces to be surfaced to the",
            "# user when this feature is on. This may have security implications",
            "# and it's more secure to turn it off in production settings.",
            "SHOW_STACKTRACE = False",
            "",
            "# Use all X-Forwarded headers when ENABLE_PROXY_FIX is True.",
            "# When proxying to a different port, set \"x_port\" to 0 to avoid downstream issues.",
            "ENABLE_PROXY_FIX = False",
            "PROXY_FIX_CONFIG = {\"x_for\": 1, \"x_proto\": 1, \"x_host\": 1, \"x_port\": 1, \"x_prefix\": 1}",
            "",
            "# Configuration for scheduling queries from SQL Lab.",
            "SCHEDULED_QUERIES: dict[str, Any] = {}",
            "",
            "# FAB Rate limiting: this is a security feature for preventing DDOS attacks. The",
            "# feature is on by default to make Superset secure by default, but you should",
            "# fine tune the limits to your needs. You can read more about the different",
            "# parameters here: https://flask-limiter.readthedocs.io/en/stable/configuration.html",
            "RATELIMIT_ENABLED = True",
            "RATELIMIT_APPLICATION = \"50 per second\"",
            "AUTH_RATE_LIMITED = True",
            "AUTH_RATE_LIMIT = \"5 per second\"",
            "# A storage location conforming to the scheme in storage-scheme. See the limits",
            "# library for allowed values: https://limits.readthedocs.io/en/stable/storage.html",
            "# RATELIMIT_STORAGE_URI = \"redis://host:port\"",
            "# A callable that returns the unique identity of the current request.",
            "# RATELIMIT_REQUEST_IDENTIFIER = flask.Request.endpoint",
            "",
            "# ------------------------------",
            "# GLOBALS FOR APP Builder",
            "# ------------------------------",
            "# Uncomment to setup Your App name",
            "APP_NAME = \"Superset\"",
            "",
            "# Specify the App icon",
            "APP_ICON = \"/static/assets/images/superset-logo-horiz.png\"",
            "",
            "# Specify where clicking the logo would take the user",
            "# e.g. setting it to '/' would take the user to '/superset/welcome/'",
            "LOGO_TARGET_PATH = None",
            "",
            "# Specify tooltip that should appear when hovering over the App Icon/Logo",
            "LOGO_TOOLTIP = \"\"",
            "",
            "# Specify any text that should appear to the right of the logo",
            "LOGO_RIGHT_TEXT: Callable[[], str] | str = \"\"",
            "",
            "# Enables SWAGGER UI for superset openapi spec",
            "# ex: http://localhost:8080/swagger/v1",
            "FAB_API_SWAGGER_UI = True",
            "",
            "# ----------------------------------------------------",
            "# AUTHENTICATION CONFIG",
            "# ----------------------------------------------------",
            "# The authentication type",
            "# AUTH_OID : Is for OpenID",
            "# AUTH_DB : Is for database (username/password)",
            "# AUTH_LDAP : Is for LDAP",
            "# AUTH_REMOTE_USER : Is for using REMOTE_USER from web server",
            "AUTH_TYPE = AUTH_DB",
            "",
            "# Uncomment to setup Full admin role name",
            "# AUTH_ROLE_ADMIN = 'Admin'",
            "",
            "# Uncomment to setup Public role name, no authentication needed",
            "# AUTH_ROLE_PUBLIC = 'Public'",
            "",
            "# Will allow user self registration",
            "# AUTH_USER_REGISTRATION = True",
            "",
            "# The default user self registration role",
            "# AUTH_USER_REGISTRATION_ROLE = \"Public\"",
            "",
            "# When using LDAP Auth, setup the LDAP server",
            "# AUTH_LDAP_SERVER = \"ldap://ldapserver.new\"",
            "",
            "# Uncomment to setup OpenID providers example for OpenID authentication",
            "# OPENID_PROVIDERS = [",
            "#    { 'name': 'Yahoo', 'url': 'https://open.login.yahoo.com/' },",
            "#    { 'name': 'Flickr', 'url': 'https://www.flickr.com/<username>' },",
            "",
            "# ---------------------------------------------------",
            "# Roles config",
            "# ---------------------------------------------------",
            "# Grant public role the same set of permissions as for a selected builtin role.",
            "# This is useful if one wants to enable anonymous users to view",
            "# dashboards. Explicit grant on specific datasets is still required.",
            "PUBLIC_ROLE_LIKE: str | None = None",
            "",
            "# ---------------------------------------------------",
            "# Babel config for translations",
            "# ---------------------------------------------------",
            "# Setup default language",
            "BABEL_DEFAULT_LOCALE = \"en\"",
            "# Your application default translation path",
            "BABEL_DEFAULT_FOLDER = \"superset/translations\"",
            "# The allowed translation for your app",
            "LANGUAGES = {",
            "    \"en\": {\"flag\": \"us\", \"name\": \"English\"},",
            "    \"es\": {\"flag\": \"es\", \"name\": \"Spanish\"},",
            "    \"it\": {\"flag\": \"it\", \"name\": \"Italian\"},",
            "    \"fr\": {\"flag\": \"fr\", \"name\": \"French\"},",
            "    \"zh\": {\"flag\": \"cn\", \"name\": \"Chinese\"},",
            "    \"ja\": {\"flag\": \"jp\", \"name\": \"Japanese\"},",
            "    \"de\": {\"flag\": \"de\", \"name\": \"German\"},",
            "    \"pt\": {\"flag\": \"pt\", \"name\": \"Portuguese\"},",
            "    \"pt_BR\": {\"flag\": \"br\", \"name\": \"Brazilian Portuguese\"},",
            "    \"ru\": {\"flag\": \"ru\", \"name\": \"Russian\"},",
            "    \"ko\": {\"flag\": \"kr\", \"name\": \"Korean\"},",
            "    \"sk\": {\"flag\": \"sk\", \"name\": \"Slovak\"},",
            "    \"sl\": {\"flag\": \"si\", \"name\": \"Slovenian\"},",
            "    \"nl\": {\"flag\": \"nl\", \"name\": \"Dutch\"},",
            "    \"uk\": {\"flag\": \"uk\", \"name\": \"Ukranian\"},",
            "}",
            "# Turning off i18n by default as translation in most languages are",
            "# incomplete and not well maintained.",
            "LANGUAGES = {}",
            "",
            "",
            "# Override the default d3 locale format",
            "# Default values are equivalent to",
            "# D3_FORMAT = {",
            "#     \"decimal\": \".\",           # - decimal place string (e.g., \".\").",
            "#     \"thousands\": \",\",         # - group separator string (e.g., \",\").",
            "#     \"grouping\": [3],          # - array of group sizes (e.g., [3]), cycled as needed.",
            "#     \"currency\": [\"$\", \"\"]     # - currency prefix/suffix strings (e.g., [\"$\", \"\"])",
            "# }",
            "# https://github.com/d3/d3-format/blob/main/README.md#formatLocale",
            "class D3Format(TypedDict, total=False):",
            "    decimal: str",
            "    thousands: str",
            "    grouping: list[int]",
            "    currency: list[str]",
            "",
            "",
            "D3_FORMAT: D3Format = {}",
            "",
            "CURRENCIES = [\"USD\", \"EUR\", \"GBP\", \"INR\", \"MXN\", \"JPY\", \"CNY\"]",
            "",
            "# ---------------------------------------------------",
            "# Feature flags",
            "# ---------------------------------------------------",
            "# Feature flags that are set by default go here. Their values can be",
            "# overwritten by those specified under FEATURE_FLAGS in superset_config.py",
            "# For example, DEFAULT_FEATURE_FLAGS = { 'FOO': True, 'BAR': False } here",
            "# and FEATURE_FLAGS = { 'BAR': True, 'BAZ': True } in superset_config.py",
            "# will result in combined feature flags of { 'FOO': True, 'BAR': True, 'BAZ': True }",
            "DEFAULT_FEATURE_FLAGS: dict[str, bool] = {",
            "    # When using a recent version of Druid that supports JOINs turn this on",
            "    \"DRUID_JOINS\": False,",
            "    \"DYNAMIC_PLUGINS\": False,",
            "    # With Superset 2.0, we are updating the default so that the legacy datasource",
            "    # editor no longer shows. Currently this is set to false so that the editor",
            "    # option does show, but we will be depreciating it.",
            "    \"DISABLE_LEGACY_DATASOURCE_EDITOR\": True,",
            "    \"ENABLE_TEMPLATE_PROCESSING\": False,",
            "    # Allow for javascript controls components",
            "    # this enables programmers to customize certain charts (like the",
            "    # geospatial ones) by inputting javascript in controls. This exposes",
            "    # an XSS security vulnerability",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\": False,  # deprecated",
            "    \"KV_STORE\": False,  # deprecated",
            "    # When this feature is enabled, nested types in Presto will be",
            "    # expanded into extra columns and/or arrays. This is experimental,",
            "    # and doesn't work with all nested types.",
            "    \"PRESTO_EXPAND_DATA\": False,",
            "    # Exposes API endpoint to compute thumbnails",
            "    \"THUMBNAILS\": False,",
            "    \"SHARE_QUERIES_VIA_KV_STORE\": False,",
            "    \"TAGGING_SYSTEM\": False,",
            "    \"SQLLAB_BACKEND_PERSISTENCE\": True,",
            "    \"LISTVIEWS_DEFAULT_CARD_VIEW\": False,",
            "    # When True, this escapes HTML (rather than rendering it) in Markdown components",
            "    \"ESCAPE_MARKDOWN_HTML\": False,",
            "    \"DASHBOARD_CROSS_FILTERS\": True,  # deprecated",
            "    \"DASHBOARD_VIRTUALIZATION\": True,",
            "    \"GLOBAL_ASYNC_QUERIES\": False,",
            "    \"EMBEDDED_SUPERSET\": False,",
            "    # Enables Alerts and reports new implementation",
            "    \"ALERT_REPORTS\": False,",
            "    \"DASHBOARD_RBAC\": False,",
            "    \"ENABLE_ADVANCED_DATA_TYPES\": False,",
            "    # Enabling ALERTS_ATTACH_REPORTS, the system sends email and slack message",
            "    # with screenshot and link",
            "    # Disables ALERTS_ATTACH_REPORTS, the system DOES NOT generate screenshot",
            "    # for report with type 'alert' and sends email and slack message with only link;",
            "    # for report with type 'report' still send with email and slack message with",
            "    # screenshot and link",
            "    \"ALERTS_ATTACH_REPORTS\": True,",
            "    # Allow users to export full CSV of table viz type.",
            "    # This could cause the server to run out of memory or compute.",
            "    \"ALLOW_FULL_CSV_EXPORT\": False,",
            "    \"ALLOW_ADHOC_SUBQUERY\": False,",
            "    \"USE_ANALAGOUS_COLORS\": False,",
            "    # Apply RLS rules to SQL Lab queries. This requires parsing and manipulating the",
            "    # query, and might break queries and/or allow users to bypass RLS. Use with care!",
            "    \"RLS_IN_SQLLAB\": False,",
            "    # Enable caching per impersonation key (e.g username) in a datasource where user",
            "    # impersonation is enabled",
            "    \"CACHE_IMPERSONATION\": False,",
            "    # Enable caching per user key for Superset cache (not database cache impersonation)",
            "    \"CACHE_QUERY_BY_USER\": False,",
            "    # Enable sharing charts with embedding",
            "    \"EMBEDDABLE_CHARTS\": True,",
            "    \"DRILL_TO_DETAIL\": True,",
            "    \"DRILL_BY\": True,",
            "    \"DATAPANEL_CLOSED_BY_DEFAULT\": False,",
            "    \"HORIZONTAL_FILTER_BAR\": False,",
            "    # The feature is off by default, and currently only supported in Presto and Postgres,",
            "    # and Bigquery.",
            "    # It also needs to be enabled on a per-database basis, by adding the key/value pair",
            "    # `cost_estimate_enabled: true` to the database `extra` attribute.",
            "    \"ESTIMATE_QUERY_COST\": False,",
            "    # Allow users to enable ssh tunneling when creating a DB.",
            "    # Users must check whether the DB engine supports SSH Tunnels",
            "    # otherwise enabling this flag won't have any effect on the DB.",
            "    \"SSH_TUNNELING\": False,",
            "    \"AVOID_COLORS_COLLISION\": True,",
            "    # Do not show user info in the menu",
            "    \"MENU_HIDE_USER_INFO\": False,",
            "    # Allows users to add a ``superset://`` DB that can query across databases. This is",
            "    # an experimental feature with potential security and performance risks, so use with",
            "    # caution. If the feature is enabled you can also set a limit for how much data is",
            "    # returned from each database in the ``SUPERSET_META_DB_LIMIT`` configuration value",
            "    # in this file.",
            "    \"ENABLE_SUPERSET_META_DB\": False,",
            "    # Set to True to replace Selenium with Playwright to execute reports and thumbnails.",
            "    # Unlike Selenium, Playwright reports support deck.gl visualizations",
            "    # Enabling this feature flag requires installing \"playwright\" pip package",
            "    \"PLAYWRIGHT_REPORTS_AND_THUMBNAILS\": False,",
            "    # Set to True to enable experimental chart plugins",
            "    \"CHART_PLUGINS_EXPERIMENTAL\": False,",
            "}",
            "",
            "# ------------------------------",
            "# SSH Tunnel",
            "# ------------------------------",
            "# Allow users to set the host used when connecting to the SSH Tunnel",
            "# as localhost and any other alias (0.0.0.0)",
            "# ----------------------------------------------------------------------",
            "#                             |",
            "# -------------+              |    +----------+",
            "#     LOCAL    |              |    |  REMOTE  | :22 SSH",
            "#     CLIENT   | <== SSH ========> |  SERVER  | :8080 web service",
            "# -------------+              |    +----------+",
            "#                             |",
            "#                          FIREWALL (only port 22 is open)",
            "",
            "# ----------------------------------------------------------------------",
            "SSH_TUNNEL_MANAGER_CLASS = \"superset.extensions.ssh.SSHManager\"",
            "SSH_TUNNEL_LOCAL_BIND_ADDRESS = \"127.0.0.1\"",
            "#: Timeout (seconds) for tunnel connection (open_channel timeout)",
            "SSH_TUNNEL_TIMEOUT_SEC = 10.0",
            "#: Timeout (seconds) for transport socket (``socket.settimeout``)",
            "SSH_TUNNEL_PACKET_TIMEOUT_SEC = 1.0",
            "",
            "",
            "# Feature flags may also be set via 'SUPERSET_FEATURE_' prefixed environment vars.",
            "DEFAULT_FEATURE_FLAGS.update(",
            "    {",
            "        k[len(\"SUPERSET_FEATURE_\") :]: parse_boolean_string(v)",
            "        for k, v in os.environ.items()",
            "        if re.search(r\"^SUPERSET_FEATURE_\\w+\", k)",
            "    }",
            ")",
            "",
            "# This is merely a default.",
            "FEATURE_FLAGS: dict[str, bool] = {}",
            "",
            "# A function that receives a dict of all feature flags",
            "# (DEFAULT_FEATURE_FLAGS merged with FEATURE_FLAGS)",
            "# can alter it, and returns a similar dict. Note the dict of feature",
            "# flags passed to the function is a deepcopy of the dict in the config,",
            "# and can therefore be mutated without side-effect",
            "#",
            "# GET_FEATURE_FLAGS_FUNC can be used to implement progressive rollouts,",
            "# role-based features, or a full on A/B testing framework.",
            "#",
            "# from flask import g, request",
            "# def GET_FEATURE_FLAGS_FUNC(feature_flags_dict: Dict[str, bool]) -> Dict[str, bool]:",
            "#     if hasattr(g, \"user\") and g.user.is_active:",
            "#         feature_flags_dict['some_feature'] = g.user and g.user.get_id() == 5",
            "#     return feature_flags_dict",
            "GET_FEATURE_FLAGS_FUNC: Callable[[dict[str, bool]], dict[str, bool]] | None = None",
            "# A function that receives a feature flag name and an optional default value.",
            "# Has a similar utility to GET_FEATURE_FLAGS_FUNC but it's useful to not force the",
            "# evaluation of all feature flags when just evaluating a single one.",
            "#",
            "# Note that the default `get_feature_flags` will evaluate each feature with this",
            "# callable when the config key is set, so don't use both GET_FEATURE_FLAGS_FUNC",
            "# and IS_FEATURE_ENABLED_FUNC in conjunction.",
            "IS_FEATURE_ENABLED_FUNC: Callable[[str, bool | None], bool] | None = None",
            "# A function that expands/overrides the frontend `bootstrap_data.common` object.",
            "# Can be used to implement custom frontend functionality,",
            "# or dynamically change certain configs.",
            "#",
            "# Values in `bootstrap_data.common` should have these characteristics:",
            "# - They are not specific to a page the user is visiting",
            "# - They do not contain secrets",
            "#",
            "# Takes as a parameter the common bootstrap payload before transformations.",
            "# Returns a dict containing data that should be added or overridden to the payload.",
            "COMMON_BOOTSTRAP_OVERRIDES_FUNC: Callable[",
            "    [dict[str, Any]], dict[str, Any]",
            "] = lambda data: {}  # default: empty dict",
            "",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES is used for adding custom categorical color schemes",
            "# example code for \"My custom warm to hot\" color scheme",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES = [",
            "#     {",
            "#         \"id\": 'myVisualizationColors',",
            "#         \"description\": '',",
            "#         \"label\": 'My Visualization Colors',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77',",
            "#          '#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_CATEGORICAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# THEME_OVERRIDES is used for adding custom theme to superset",
            "# example code for \"My theme\" custom scheme",
            "# THEME_OVERRIDES = {",
            "#   \"borderRadius\": 4,",
            "#   \"colors\": {",
            "#     \"primary\": {",
            "#       \"base\": 'red',",
            "#     },",
            "#     \"secondary\": {",
            "#       \"base\": 'green',",
            "#     },",
            "#     \"grayscale\": {",
            "#       \"base\": 'orange',",
            "#     }",
            "#   }",
            "# }",
            "",
            "THEME_OVERRIDES: dict[str, Any] = {}",
            "",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES is used for adding custom sequential color schemes",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES =  [",
            "#     {",
            "#         \"id\": 'warmToHot',",
            "#         \"description\": '',",
            "#         \"isDiverging\": True,",
            "#         \"label\": 'My custom warm to hot',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD',",
            "#          '#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_SEQUENTIAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# ---------------------------------------------------",
            "# Thumbnail config (behind feature flag)",
            "# ---------------------------------------------------",
            "# By default, thumbnails are rendered per user, and will fall back to the Selenium",
            "# user for anonymous users. Similar to Alerts & Reports, thumbnails",
            "# can be configured to always be rendered as a fixed user. See",
            "# `superset.tasks.types.ExecutorType` for a full list of executor options.",
            "# To always use a fixed user account, use the following configuration:",
            "# THUMBNAIL_EXECUTE_AS = [ExecutorType.SELENIUM]",
            "THUMBNAIL_SELENIUM_USER: str | None = \"admin\"",
            "THUMBNAIL_EXECUTE_AS = [ExecutorType.CURRENT_USER, ExecutorType.SELENIUM]",
            "",
            "# By default, thumbnail digests are calculated based on various parameters in the",
            "# chart/dashboard metadata, and in the case of user-specific thumbnails, the",
            "# username. To specify a custom digest function, use the following config parameters",
            "# to define callbacks that receive",
            "# 1. the model (dashboard or chart)",
            "# 2. the executor type (e.g. ExecutorType.SELENIUM)",
            "# 3. the executor's username (note, this is the executor as defined by",
            "# `THUMBNAIL_EXECUTE_AS`; the executor is only equal to the currently logged in",
            "# user if the executor type is equal to `ExecutorType.CURRENT_USER`)",
            "# and return the final digest string:",
            "THUMBNAIL_DASHBOARD_DIGEST_FUNC: None | (",
            "    Callable[[Dashboard, ExecutorType, str], str]",
            ") = None",
            "THUMBNAIL_CHART_DIGEST_FUNC: Callable[[Slice, ExecutorType, str], str] | None = None",
            "",
            "THUMBNAIL_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"NullCache\",",
            "    \"CACHE_NO_NULL_WARNING\": True,",
            "}",
            "",
            "# Time before selenium times out after trying to locate an element on the page and wait",
            "# for that element to load for a screenshot.",
            "SCREENSHOT_LOCATE_WAIT = int(timedelta(seconds=10).total_seconds())",
            "# Time before selenium times out after waiting for all DOM class elements named",
            "# \"loading\" are gone.",
            "SCREENSHOT_LOAD_WAIT = int(timedelta(minutes=1).total_seconds())",
            "# Selenium destroy retries",
            "SCREENSHOT_SELENIUM_RETRIES = 5",
            "# Give selenium an headstart, in seconds",
            "SCREENSHOT_SELENIUM_HEADSTART = 3",
            "# Wait for the chart animation, in seconds",
            "SCREENSHOT_SELENIUM_ANIMATION_WAIT = 5",
            "# Replace unexpected errors in screenshots with real error messages",
            "SCREENSHOT_REPLACE_UNEXPECTED_ERRORS = False",
            "# Max time to wait for error message modal to show up, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_VISIBLE = 5",
            "# Max time to wait for error message modal to close, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_INVISIBLE = 5",
            "# Event that Playwright waits for when loading a new page",
            "# Possible values: \"load\", \"commit\", \"domcontentloaded\", \"networkidle\"",
            "# Docs: https://playwright.dev/python/docs/api/class-page#page-goto-option-wait-until",
            "SCREENSHOT_PLAYWRIGHT_WAIT_EVENT = \"load\"",
            "# Default timeout for Playwright browser context for all operations",
            "SCREENSHOT_PLAYWRIGHT_DEFAULT_TIMEOUT = int(",
            "    timedelta(seconds=30).total_seconds() * 1000",
            ")",
            "",
            "# ---------------------------------------------------",
            "# Image and file configuration",
            "# ---------------------------------------------------",
            "# The file upload folder, when using models with files",
            "UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "UPLOAD_CHUNK_SIZE = 4096",
            "",
            "# The image upload folder, when using models with images",
            "IMG_UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "",
            "# The image upload url, when using models with images",
            "IMG_UPLOAD_URL = \"/static/uploads/\"",
            "# Setup image size default is (300, 200, True)",
            "# IMG_SIZE = (300, 200, True)",
            "",
            "# Default cache timeout, applies to all cache backends unless specifically overridden in",
            "# each cache config.",
            "CACHE_DEFAULT_TIMEOUT = int(timedelta(days=1).total_seconds())",
            "",
            "# Default cache for Superset objects",
            "CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for datasource metadata and query results",
            "DATA_CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for dashboard filter state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "FILTER_STATE_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=90).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# Cache for explore form data state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "EXPLORE_FORM_DATA_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=7).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# store cache keys by datasource UID (via CacheKey) for custom processing/invalidation",
            "STORE_CACHE_KEYS_IN_METADATA_DB = False",
            "",
            "# CORS Options",
            "ENABLE_CORS = False",
            "CORS_OPTIONS: dict[Any, Any] = {}",
            "",
            "# Sanitizes the HTML content used in markdowns to allow its rendering in a safe manner.",
            "# Disabling this option is not recommended for security reasons. If you wish to allow",
            "# valid safe elements that are not included in the default sanitization schema, use the",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS configuration.",
            "HTML_SANITIZATION = True",
            "",
            "# Use this configuration to extend the HTML sanitization schema.",
            "# By default we use the GitHub schema defined in",
            "# https://github.com/syntax-tree/hast-util-sanitize/blob/main/lib/schema.js",
            "# For example, the following configuration would allow the rendering of the",
            "# style attribute for div elements and the ftp protocol in hrefs:",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS = {",
            "#   \"attributes\": {",
            "#     \"div\": [\"style\"],",
            "#   },",
            "#   \"protocols\": {",
            "#     \"href\": [\"ftp\"],",
            "#   }",
            "# }",
            "# Be careful when extending the default schema to avoid XSS attacks.",
            "HTML_SANITIZATION_SCHEMA_EXTENSIONS: dict[str, Any] = {}",
            "",
            "# Chrome allows up to 6 open connections per domain at a time. When there are more",
            "# than 6 slices in dashboard, a lot of time fetch requests are queued up and wait for",
            "# next available socket. PR #5039 is trying to allow domain sharding for Superset,",
            "# and this feature will be enabled by configuration only (by default Superset",
            "# doesn't allow cross-domain request).",
            "SUPERSET_WEBSERVER_DOMAINS = None",
            "",
            "# Allowed format types for upload on Database view",
            "EXCEL_EXTENSIONS = {\"xlsx\", \"xls\"}",
            "CSV_EXTENSIONS = {\"csv\", \"tsv\", \"txt\"}",
            "COLUMNAR_EXTENSIONS = {\"parquet\", \"zip\"}",
            "ALLOWED_EXTENSIONS = {*EXCEL_EXTENSIONS, *CSV_EXTENSIONS, *COLUMNAR_EXTENSIONS}",
            "",
            "# Optional maximum file size in bytes when uploading a CSV",
            "CSV_UPLOAD_MAX_SIZE = None",
            "",
            "# CSV Options: key/value pairs that will be passed as argument to DataFrame.to_csv",
            "# method.",
            "# note: index option should not be overridden",
            "CSV_EXPORT = {\"encoding\": \"utf-8\"}",
            "",
            "# Excel Options: key/value pairs that will be passed as argument to DataFrame.to_excel",
            "# method.",
            "# note: index option should not be overridden",
            "EXCEL_EXPORT: dict[str, Any] = {}",
            "",
            "# ---------------------------------------------------",
            "# Time grain configurations",
            "# ---------------------------------------------------",
            "# List of time grains to disable in the application (see list of builtin",
            "# time grains in superset/db_engine_specs/base.py).",
            "# For example: to disable 1 second time grain:",
            "# TIME_GRAIN_DENYLIST = ['PT1S']",
            "TIME_GRAIN_DENYLIST: list[str] = []",
            "",
            "# Additional time grains to be supported using similar definitions as in",
            "# superset/db_engine_specs/base.py.",
            "# For example: To add a new 2 second time grain:",
            "# TIME_GRAIN_ADDONS = {'PT2S': '2 second'}",
            "TIME_GRAIN_ADDONS: dict[str, str] = {}",
            "",
            "# Implementation of additional time grains per engine.",
            "# The column to be truncated is denoted `{col}` in the expression.",
            "# For example: To implement 2 second time grain on clickhouse engine:",
            "# TIME_GRAIN_ADDON_EXPRESSIONS = {",
            "#     'clickhouse': {",
            "#         'PT2S': 'toDateTime(intDiv(toUInt32(toDateTime({col})), 2)*2)'",
            "#     }",
            "# }",
            "TIME_GRAIN_ADDON_EXPRESSIONS: dict[str, dict[str, str]] = {}",
            "",
            "# Map of custom time grains and artificial join column producers used",
            "# when generating the join key between results and time shifts.",
            "# See superset/common/query_context_processor.get_aggregated_join_column",
            "#",
            "# Example of a join column producer that aggregates by fiscal year",
            "# def join_producer(row: Series, column_index: int) -> str:",
            "#    return row[index].strftime(\"%F\")",
            "#",
            "# TIME_GRAIN_JOIN_COLUMN_PRODUCERS = {\"P1F\": join_producer}",
            "TIME_GRAIN_JOIN_COLUMN_PRODUCERS: dict[str, Callable[[Series, int], str]] = {}",
            "",
            "# ---------------------------------------------------",
            "# List of viz_types not allowed in your environment",
            "# For example: Disable pivot table and treemap:",
            "#  VIZ_TYPE_DENYLIST = ['pivot_table', 'treemap']",
            "# ---------------------------------------------------",
            "",
            "VIZ_TYPE_DENYLIST: list[str] = []",
            "",
            "# --------------------------------------------------",
            "# Modules, datasources and middleware to be registered",
            "# --------------------------------------------------",
            "DEFAULT_MODULE_DS_MAP = OrderedDict(",
            "    [",
            "        (\"superset.connectors.sqla.models\", [\"SqlaTable\"]),",
            "    ]",
            ")",
            "ADDITIONAL_MODULE_DS_MAP: dict[str, list[str]] = {}",
            "ADDITIONAL_MIDDLEWARE: list[Callable[..., Any]] = []",
            "",
            "# 1) https://docs.python-guide.org/writing/logging/",
            "# 2) https://docs.python.org/2/library/logging.config.html",
            "",
            "# Default configurator will consume the LOG_* settings below",
            "LOGGING_CONFIGURATOR = DefaultLoggingConfigurator()",
            "",
            "# Console Log Settings",
            "",
            "LOG_FORMAT = \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\"",
            "LOG_LEVEL = \"DEBUG\"",
            "",
            "# ---------------------------------------------------",
            "# Enable Time Rotate Log Handler",
            "# ---------------------------------------------------",
            "# LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL",
            "",
            "ENABLE_TIME_ROTATE = False",
            "TIME_ROTATE_LOG_LEVEL = \"DEBUG\"",
            "FILENAME = os.path.join(DATA_DIR, \"superset.log\")",
            "ROLLOVER = \"midnight\"",
            "INTERVAL = 1",
            "BACKUP_COUNT = 30",
            "",
            "# Custom logger for auditing queries. This can be used to send ran queries to a",
            "# structured immutable store for auditing purposes. The function is called for",
            "# every query ran, in both SQL Lab and charts/dashboards.",
            "# def QUERY_LOGGER(",
            "#     database,",
            "#     query,",
            "#     schema=None,",
            "#     client=None,",
            "#     security_manager=None,",
            "#     log_params=None,",
            "# ):",
            "#     pass",
            "QUERY_LOGGER = None",
            "",
            "# Set this API key to enable Mapbox visualizations",
            "MAPBOX_API_KEY = os.environ.get(\"MAPBOX_API_KEY\", \"\")",
            "",
            "# Maximum number of rows returned for any analytical database query",
            "SQL_MAX_ROW = 100000",
            "",
            "# Maximum number of rows displayed in SQL Lab UI",
            "# Is set to avoid out of memory/localstorage issues in browsers. Does not affect",
            "# exported CSVs",
            "DISPLAY_MAX_ROW = 10000",
            "",
            "# Default row limit for SQL Lab queries. Is overridden by setting a new limit in",
            "# the SQL Lab UI",
            "DEFAULT_SQLLAB_LIMIT = 1000",
            "",
            "# The limit for the Superset Meta DB when the feature flag ENABLE_SUPERSET_META_DB is on",
            "SUPERSET_META_DB_LIMIT: int | None = 1000",
            "",
            "# Adds a warning message on sqllab save query and schedule query modals.",
            "SQLLAB_SAVE_WARNING_MESSAGE = None",
            "SQLLAB_SCHEDULE_WARNING_MESSAGE = None",
            "",
            "# Force refresh while auto-refresh in dashboard",
            "DASHBOARD_AUTO_REFRESH_MODE: Literal[\"fetch\", \"force\"] = \"force\"",
            "# Dashboard auto refresh intervals",
            "DASHBOARD_AUTO_REFRESH_INTERVALS = [",
            "    [0, \"Don't refresh\"],",
            "    [10, \"10 seconds\"],",
            "    [30, \"30 seconds\"],",
            "    [60, \"1 minute\"],",
            "    [300, \"5 minutes\"],",
            "    [1800, \"30 minutes\"],",
            "    [3600, \"1 hour\"],",
            "    [21600, \"6 hours\"],",
            "    [43200, \"12 hours\"],",
            "    [86400, \"24 hours\"],",
            "]",
            "",
            "# This is used as a workaround for the alerts & reports scheduler task to get the time",
            "# celery beat triggered it, see https://github.com/celery/celery/issues/6974 for details",
            "CELERY_BEAT_SCHEDULER_EXPIRES = timedelta(weeks=1)",
            "",
            "# Default celery config is to use SQLA as a broker, in a production setting",
            "# you'll want to use a proper broker as specified here:",
            "# https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/index.html",
            "",
            "",
            "class CeleryConfig:  # pylint: disable=too-few-public-methods",
            "    broker_url = \"sqla+sqlite:///celerydb.sqlite\"",
            "    imports = (\"superset.sql_lab\", \"superset.tasks.scheduler\")",
            "    result_backend = \"db+sqlite:///celery_results.sqlite\"",
            "    worker_prefetch_multiplier = 1",
            "    task_acks_late = False",
            "    task_annotations = {",
            "        \"sql_lab.get_sql_results\": {",
            "            \"rate_limit\": \"100/s\",",
            "        },",
            "    }",
            "    beat_schedule = {",
            "        \"reports.scheduler\": {",
            "            \"task\": \"reports.scheduler\",",
            "            \"schedule\": crontab(minute=\"*\", hour=\"*\"),",
            "            \"options\": {\"expires\": int(CELERY_BEAT_SCHEDULER_EXPIRES.total_seconds())},",
            "        },",
            "        \"reports.prune_log\": {",
            "            \"task\": \"reports.prune_log\",",
            "            \"schedule\": crontab(minute=0, hour=0),",
            "        },",
            "    }",
            "",
            "",
            "CELERY_CONFIG = CeleryConfig  # pylint: disable=invalid-name",
            "",
            "# Set celery config to None to disable all the above configuration",
            "# CELERY_CONFIG = None",
            "",
            "# Additional static HTTP headers to be served by your Superset server. Note",
            "# Flask-Talisman applies the relevant security HTTP headers.",
            "#",
            "# DEFAULT_HTTP_HEADERS: sets default values for HTTP headers. These may be overridden",
            "# within the app",
            "# OVERRIDE_HTTP_HEADERS: sets override values for HTTP headers. These values will",
            "# override anything set within the app",
            "DEFAULT_HTTP_HEADERS: dict[str, Any] = {}",
            "OVERRIDE_HTTP_HEADERS: dict[str, Any] = {}",
            "HTTP_HEADERS: dict[str, Any] = {}",
            "",
            "# The db id here results in selecting this one as a default in SQL Lab",
            "DEFAULT_DB_ID = None",
            "",
            "# Timeout duration for SQL Lab synchronous queries",
            "SQLLAB_TIMEOUT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Timeout duration for SQL Lab query validation",
            "SQLLAB_VALIDATION_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# SQLLAB_DEFAULT_DBID",
            "SQLLAB_DEFAULT_DBID = None",
            "",
            "# The MAX duration a query can run for before being killed by celery.",
            "SQLLAB_ASYNC_TIME_LIMIT_SEC = int(timedelta(hours=6).total_seconds())",
            "",
            "# Some databases support running EXPLAIN queries that allow users to estimate",
            "# query costs before they run. These EXPLAIN queries should have a small",
            "# timeout.",
            "SQLLAB_QUERY_COST_ESTIMATE_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# The cost returned by the databases is a relative value; in order to map the cost to",
            "# a tangible value you need to define a custom formatter that takes into consideration",
            "# your specific infrastructure. For example, you could analyze queries a posteriori by",
            "# running EXPLAIN on them, and compute a histogram of relative costs to present the",
            "# cost as a percentile, this step is optional as every db engine spec has its own",
            "# query cost formatter, but it you wanna customize it you can define it inside the config:",
            "",
            "# def postgres_query_cost_formatter(",
            "#     result: List[Dict[str, Any]]",
            "# ) -> List[Dict[str, str]]:",
            "#     # 25, 50, 75% percentiles",
            "#     percentile_costs = [100.0, 1000.0, 10000.0]",
            "#",
            "#     out = []",
            "#     for row in result:",
            "#         relative_cost = row[\"Total cost\"]",
            "#         percentile = bisect.bisect_left(percentile_costs, relative_cost) + 1",
            "#         out.append({",
            "#             \"Relative cost\": relative_cost,",
            "#             \"Percentile\": str(percentile * 25) + \"%\",",
            "#         })",
            "#",
            "#     return out",
            "#",
            "# QUERY_COST_FORMATTERS_BY_ENGINE: {\"postgresql\": postgres_query_cost_formatter}",
            "QUERY_COST_FORMATTERS_BY_ENGINE: dict[",
            "    str, Callable[[list[dict[str, Any]]], list[dict[str, Any]]]",
            "] = {}",
            "",
            "# Flag that controls if limit should be enforced on the CTA (create table as queries).",
            "SQLLAB_CTAS_NO_LIMIT = False",
            "",
            "# This allows you to define custom logic around the \"CREATE TABLE AS\" or CTAS feature",
            "# in SQL Lab that defines where the target schema should be for a given user.",
            "# Database `CTAS Schema` has a precedence over this setting.",
            "# Example below returns a username and CTA queries will write tables into the schema",
            "# name `username`",
            "# SQLLAB_CTAS_SCHEMA_NAME_FUNC = lambda database, user, schema, sql: user.username",
            "# This is move involved example where depending on the database you can leverage data",
            "# available to assign schema for the CTA query:",
            "# def compute_schema_name(database: Database, user: User, schema: str, sql: str) -> str:",
            "#     if database.name == 'mysql_payments_slave':",
            "#         return 'tmp_superset_schema'",
            "#     if database.name == 'presto_gold':",
            "#         return user.username",
            "#     if database.name == 'analytics':",
            "#         if 'analytics' in [r.name for r in user.roles]:",
            "#             return 'analytics_cta'",
            "#         else:",
            "#             return f'tmp_{schema}'",
            "# Function accepts database object, user object, schema name and sql that will be run.",
            "SQLLAB_CTAS_SCHEMA_NAME_FUNC: None | (",
            "    Callable[[Database, models.User, str, str], str]",
            ") = None",
            "",
            "# If enabled, it can be used to store the results of long-running queries",
            "# in SQL Lab by using the \"Run Async\" button/feature",
            "RESULTS_BACKEND: BaseCache | None = None",
            "",
            "# Use PyArrow and MessagePack for async query results serialization,",
            "# rather than JSON. This feature requires additional testing from the",
            "# community before it is fully adopted, so this config option is provided",
            "# in order to disable should breaking issues be discovered.",
            "RESULTS_BACKEND_USE_MSGPACK = True",
            "",
            "# The S3 bucket where you want to store your external hive tables created",
            "# from CSV files. For example, 'companyname-superset'",
            "CSV_TO_HIVE_UPLOAD_S3_BUCKET = None",
            "",
            "# The directory within the bucket specified above that will",
            "# contain all the external tables",
            "CSV_TO_HIVE_UPLOAD_DIRECTORY = \"EXTERNAL_HIVE_TABLES/\"",
            "",
            "",
            "# Function that creates upload directory dynamically based on the",
            "# database used, user and schema provided.",
            "def CSV_TO_HIVE_UPLOAD_DIRECTORY_FUNC(  # pylint: disable=invalid-name",
            "    database: Database,",
            "    user: models.User,  # pylint: disable=unused-argument",
            "    schema: str | None,",
            ") -> str:",
            "    # Note the final empty path enforces a trailing slash.",
            "    return os.path.join(",
            "        CSV_TO_HIVE_UPLOAD_DIRECTORY, str(database.id), schema or \"\", \"\"",
            "    )",
            "",
            "",
            "# The namespace within hive where the tables created from",
            "# uploading CSVs will be stored.",
            "UPLOADED_CSV_HIVE_NAMESPACE: str | None = None",
            "",
            "# Function that computes the allowed schemas for the CSV uploads.",
            "# Allowed schemas will be a union of schemas_allowed_for_file_upload",
            "# db configuration and a result of this function.",
            "",
            "# mypy doesn't catch that if case ensures list content being always str",
            "ALLOWED_USER_CSV_SCHEMA_FUNC: Callable[[Database, models.User], list[str]] = (",
            "    lambda database, user: [UPLOADED_CSV_HIVE_NAMESPACE]",
            "    if UPLOADED_CSV_HIVE_NAMESPACE",
            "    else []",
            ")",
            "",
            "# Values that should be treated as nulls for the csv uploads.",
            "CSV_DEFAULT_NA_NAMES = list(STR_NA_VALUES)",
            "",
            "# A dictionary of items that gets merged into the Jinja context for",
            "# SQL Lab. The existing context gets updated with this dictionary,",
            "# meaning values for existing keys get overwritten by the content of this",
            "# dictionary. Exposing functionality through JINJA_CONTEXT_ADDONS has security",
            "# implications as it opens a window for a user to execute untrusted code.",
            "# It's important to make sure that the objects exposed (as well as objects attached",
            "# to those objects) are harmless. We recommend only exposing simple/pure functions that",
            "# return native types.",
            "JINJA_CONTEXT_ADDONS: dict[str, Callable[..., Any]] = {}",
            "",
            "# A dictionary of macro template processors (by engine) that gets merged into global",
            "# template processors. The existing template processors get updated with this",
            "# dictionary, which means the existing keys get overwritten by the content of this",
            "# dictionary. The customized addons don't necessarily need to use Jinja templating",
            "# language. This allows you to define custom logic to process templates on a per-engine",
            "# basis. Example value = `{\"presto\": CustomPrestoTemplateProcessor}`",
            "CUSTOM_TEMPLATE_PROCESSORS: dict[str, type[BaseTemplateProcessor]] = {}",
            "",
            "# Roles that are controlled by the API / Superset and should not be changed",
            "# by humans.",
            "ROBOT_PERMISSION_ROLES = [\"Public\", \"Gamma\", \"Alpha\", \"Admin\", \"sql_lab\"]",
            "",
            "CONFIG_PATH_ENV_VAR = \"SUPERSET_CONFIG_PATH\"",
            "",
            "# If a callable is specified, it will be called at app startup while passing",
            "# a reference to the Flask app. This can be used to alter the Flask app",
            "# in whatever way.",
            "# example: FLASK_APP_MUTATOR = lambda x: x.before_request = f",
            "FLASK_APP_MUTATOR = None",
            "",
            "# smtp server configuration",
            "EMAIL_NOTIFICATIONS = False  # all the emails are sent using dryrun",
            "SMTP_HOST = \"localhost\"",
            "SMTP_STARTTLS = True",
            "SMTP_SSL = False",
            "SMTP_USER = \"superset\"",
            "SMTP_PORT = 25",
            "SMTP_PASSWORD = \"superset\"",
            "SMTP_MAIL_FROM = \"superset@superset.com\"",
            "# If True creates a default SSL context with ssl.Purpose.CLIENT_AUTH using the",
            "# default system root CA certificates.",
            "SMTP_SSL_SERVER_AUTH = False",
            "ENABLE_CHUNK_ENCODING = False",
            "",
            "# Whether to bump the logging level to ERROR on the flask_appbuilder package",
            "# Set to False if/when debugging FAB related issues like",
            "# permission management",
            "SILENCE_FAB = True",
            "",
            "FAB_ADD_SECURITY_VIEWS = True",
            "FAB_ADD_SECURITY_PERMISSION_VIEW = False",
            "FAB_ADD_SECURITY_VIEW_MENU_VIEW = False",
            "FAB_ADD_SECURITY_PERMISSION_VIEWS_VIEW = False",
            "",
            "# The link to a page containing common errors and their resolutions",
            "# It will be appended at the bottom of sql_lab errors.",
            "TROUBLESHOOTING_LINK = \"\"",
            "",
            "# CSRF token timeout, set to None for a token that never expires",
            "WTF_CSRF_TIME_LIMIT = int(timedelta(weeks=1).total_seconds())",
            "",
            "# This link should lead to a page with instructions on how to gain access to a",
            "# Datasource. It will be placed at the bottom of permissions errors.",
            "PERMISSION_INSTRUCTIONS_LINK = \"\"",
            "",
            "# Integrate external Blueprints to the app by passing them to your",
            "# configuration. These blueprints will get integrated in the app",
            "BLUEPRINTS: list[Blueprint] = []",
            "",
            "# Provide a callable that receives a tracking_url and returns another",
            "# URL. This is used to translate internal Hadoop job tracker URL",
            "# into a proxied one",
            "",
            "",
            "# Transform SQL query tracking url for Hive and Presto engines. You may also",
            "# access information about the query itself by adding a second parameter",
            "# to your transformer function, e.g.:",
            "#   TRACKING_URL_TRANSFORMER = (",
            "#       lambda url, query: url if is_fresh(query) else None",
            "#   )",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "TRACKING_URL_TRANSFORMER = lambda url: url",
            "",
            "",
            "# customize the polling time of each engine",
            "DB_POLL_INTERVAL_SECONDS: dict[str, int] = {}",
            "",
            "# Interval between consecutive polls when using Presto Engine",
            "# See here: https://github.com/dropbox/PyHive/blob/8eb0aeab8ca300f3024655419b93dad926c1a351/pyhive/presto.py#L93  # pylint: disable=line-too-long,useless-suppression",
            "PRESTO_POLL_INTERVAL = int(timedelta(seconds=1).total_seconds())",
            "",
            "# Allow list of custom authentications for each DB engine.",
            "# Example:",
            "# from your.module import AuthClass",
            "# from another.extra import auth_method",
            "#",
            "# ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {",
            "#     \"trino\": {",
            "#         \"custom_auth\": AuthClass,",
            "#         \"another_auth_method\": auth_method,",
            "#     },",
            "# }",
            "ALLOWED_EXTRA_AUTHENTICATIONS: dict[str, dict[str, Callable[..., Any]]] = {}",
            "",
            "# The id of a template dashboard that should be copied to every new user",
            "DASHBOARD_TEMPLATE_ID = None",
            "",
            "# A callable that allows altering the database connection URL and params",
            "# on the fly, at runtime. This allows for things like impersonation or",
            "# arbitrary logic. For instance you can wire different users to",
            "# use different connection parameters, or pass their email address as the",
            "# username. The function receives the connection uri object, connection",
            "# params, the username, and returns the mutated uri and params objects.",
            "# Example:",
            "#   def DB_CONNECTION_MUTATOR(uri, params, username, security_manager, source):",
            "#       user = security_manager.find_user(username=username)",
            "#       if user and user.email:",
            "#           uri.username = user.email",
            "#       return uri, params",
            "#",
            "# Note that the returned uri and params are passed directly to sqlalchemy's",
            "# as such `create_engine(url, **params)`",
            "DB_CONNECTION_MUTATOR = None",
            "",
            "",
            "# A function that intercepts the SQL to be executed and can alter it.",
            "# The use case is can be around adding some sort of comment header",
            "# with information such as the username and worker node information",
            "#",
            "#    def SQL_QUERY_MUTATOR(",
            "#        sql,",
            "#        security_manager=security_manager,",
            "#        database=database,",
            "#    ):",
            "#        dttm = datetime.now().isoformat()",
            "#        return f\"-- [SQL LAB] {user_name} {dttm}\\n{sql}\"",
            "# For backward compatibility, you can unpack any of the above arguments in your",
            "# function definition, but keep the **kwargs as the last argument to allow new args",
            "# to be added later without any errors.",
            "def SQL_QUERY_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    sql: str, **kwargs: Any",
            ") -> str:",
            "    return sql",
            "",
            "",
            "# A variable that chooses whether to apply the SQL_QUERY_MUTATOR before or after splitting the input query",
            "# It allows for using the SQL_QUERY_MUTATOR function for more than comments",
            "# Usage: If you want to apply a change to every statement to a given query, set MUTATE_AFTER_SPLIT = True",
            "# An example use case is if data has role based access controls, and you want to apply",
            "# a SET ROLE statement alongside every user query. Changing this variable maintains",
            "# functionality for both the SQL_Lab and Charts.",
            "MUTATE_AFTER_SPLIT = False",
            "",
            "",
            "# This allows for a user to add header data to any outgoing emails. For example,",
            "# if you need to include metadata in the header or you want to change the specifications",
            "# of the email title, header, or sender.",
            "def EMAIL_HEADER_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    msg: MIMEMultipart, **kwargs: Any",
            ") -> MIMEMultipart:",
            "    return msg",
            "",
            "",
            "# Define a list of usernames to be excluded from all dropdown lists of users",
            "# Owners, filters for created_by, etc.",
            "# The users can also be excluded by overriding the get_exclude_users_from_lists method",
            "# in security manager",
            "EXCLUDE_USERS_FROM_LISTS: list[str] | None = None",
            "",
            "# For database connections, this dictionary will remove engines from the available",
            "# list/dropdown if you do not want these dbs to show as available.",
            "# The available list is generated by driver installed, and some engines have multiple",
            "# drivers.",
            "# e.g., DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {\"databricks\": {\"pyhive\", \"pyodbc\"}}",
            "DBS_AVAILABLE_DENYLIST: dict[str, set[str]] = {}",
            "",
            "# This auth provider is used by background (offline) tasks that need to access",
            "# protected resources. Can be overridden by end users in order to support",
            "# custom auth mechanisms",
            "MACHINE_AUTH_PROVIDER_CLASS = \"superset.utils.machine_auth.MachineAuthProvider\"",
            "",
            "# ---------------------------------------------------",
            "# Alerts & Reports",
            "# ---------------------------------------------------",
            "# Used for Alerts/Reports (Feature flask ALERT_REPORTS) to set the size for the",
            "# sliding cron window size, should be synced with the celery beat config minus 1 second",
            "ALERT_REPORTS_CRON_WINDOW_SIZE = 59",
            "ALERT_REPORTS_WORKING_TIME_OUT_KILL = True",
            "# Which user to attempt to execute Alerts/Reports as. By default,",
            "# execute as the primary owner of the alert/report (giving priority to the last",
            "# modifier and then the creator if either is contained within the list of owners,",
            "# otherwise the first owner will be used).",
            "#",
            "# To first try to execute as the creator in the owners list (if present), then fall",
            "# back to the creator, then the last modifier in the owners list (if present), then the",
            "# last modifier, then an owner and finally `THUMBNAIL_SELENIUM_USER`, set as follows:",
            "# ALERT_REPORTS_EXECUTE_AS = [",
            "#     ExecutorType.CREATOR_OWNER,",
            "#     ExecutorType.CREATOR,",
            "#     ExecutorType.MODIFIER_OWNER,",
            "#     ExecutorType.MODIFIER,",
            "#     ExecutorType.OWNER,",
            "#     ExecutorType.SELENIUM,",
            "# ]",
            "ALERT_REPORTS_EXECUTE_AS: list[ExecutorType] = [ExecutorType.OWNER]",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_TIME_OUT_LAG = int(timedelta(seconds=10).total_seconds())",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG = int(timedelta(seconds=1).total_seconds())",
            "# Default values that user using when creating alert",
            "ALERT_REPORTS_DEFAULT_WORKING_TIMEOUT = 3600",
            "ALERT_REPORTS_DEFAULT_RETENTION = 90",
            "ALERT_REPORTS_DEFAULT_CRON_VALUE = \"0 0 * * *\"  # every day",
            "# If set to true no notification is sent, the worker will just log a message.",
            "# Useful for debugging",
            "ALERT_REPORTS_NOTIFICATION_DRY_RUN = False",
            "# Max tries to run queries to prevent false errors caused by transient errors",
            "# being returned to users. Set to a value >1 to enable retries.",
            "ALERT_REPORTS_QUERY_EXECUTION_MAX_TRIES = 1",
            "# Custom width for screenshots",
            "ALERT_REPORTS_MIN_CUSTOM_SCREENSHOT_WIDTH = 600",
            "ALERT_REPORTS_MAX_CUSTOM_SCREENSHOT_WIDTH = 2400",
            "",
            "# A custom prefix to use on all Alerts & Reports emails",
            "EMAIL_REPORTS_SUBJECT_PREFIX = \"[Report] \"",
            "",
            "# The text for call-to-action link in Alerts & Reports emails",
            "EMAIL_REPORTS_CTA = \"Explore in Superset\"",
            "",
            "# Slack API token for the superset reports, either string or callable",
            "SLACK_API_TOKEN: Callable[[], str] | str | None = None",
            "SLACK_PROXY = None",
            "",
            "# The webdriver to use for generating reports. Use one of the following",
            "# firefox",
            "#   Requires: geckodriver and firefox installations",
            "#   Limitations: can be buggy at times",
            "# chrome:",
            "#   Requires: headless chrome",
            "#   Limitations: unable to generate screenshots of elements",
            "WEBDRIVER_TYPE = \"firefox\"",
            "",
            "# Window size - this will impact the rendering of the data",
            "WEBDRIVER_WINDOW = {",
            "    \"dashboard\": (1600, 2000),",
            "    \"slice\": (3000, 1200),",
            "    \"pixel_density\": 1,",
            "}",
            "",
            "# An optional override to the default auth hook used to provide auth to the offline",
            "# webdriver (when using Selenium) or browser context (when using Playwright - see",
            "# PLAYWRIGHT_REPORTS_AND_THUMBNAILS feature flag)",
            "WEBDRIVER_AUTH_FUNC = None",
            "",
            "# Any config options to be passed as-is to the webdriver",
            "WEBDRIVER_CONFIGURATION: dict[Any, Any] = {\"service_log_path\": \"/dev/null\"}",
            "",
            "# Additional args to be passed as arguments to the config object",
            "# Note: If using Chrome, you'll want to add the \"--marionette\" arg.",
            "WEBDRIVER_OPTION_ARGS = [\"--headless\"]",
            "",
            "# The base URL to query for accessing the user interface",
            "WEBDRIVER_BASEURL = \"http://0.0.0.0:8080/\"",
            "# The base URL for the email report hyperlinks.",
            "WEBDRIVER_BASEURL_USER_FRIENDLY = WEBDRIVER_BASEURL",
            "# Time selenium will wait for the page to load and render for the email report.",
            "EMAIL_PAGE_RENDER_WAIT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Send user to a link where they can report bugs",
            "BUG_REPORT_URL = None",
            "BUG_REPORT_TEXT = \"Report a bug\"",
            "BUG_REPORT_ICON = None  # Recommended size: 16x16",
            "",
            "# Send user to a link where they can read more about Superset",
            "DOCUMENTATION_URL = None",
            "DOCUMENTATION_TEXT = \"Documentation\"",
            "DOCUMENTATION_ICON = None  # Recommended size: 16x16",
            "",
            "# What is the Last N days relative in the time selector to:",
            "# 'today' means it is midnight (00:00:00) in the local timezone",
            "# 'now' means it is relative to the query issue time",
            "# If both start and end time is set to now, this will make the time",
            "# filter a moving window. By only setting the end time to now,",
            "# start time will be set to midnight, while end will be relative to",
            "# the query issue time.",
            "DEFAULT_RELATIVE_START_TIME = \"today\"",
            "DEFAULT_RELATIVE_END_TIME = \"today\"",
            "",
            "# Configure which SQL validator to use for each engine",
            "SQL_VALIDATORS_BY_ENGINE = {",
            "    \"presto\": \"PrestoDBSQLValidator\",",
            "    \"postgresql\": \"PostgreSQLValidator\",",
            "}",
            "",
            "# A list of preferred databases, in order. These databases will be",
            "# displayed prominently in the \"Add Database\" dialog. You should",
            "# use the \"engine_name\" attribute of the corresponding DB engine spec",
            "# in `superset/db_engine_specs/`.",
            "PREFERRED_DATABASES: list[str] = [",
            "    \"PostgreSQL\",",
            "    \"Presto\",",
            "    \"MySQL\",",
            "    \"SQLite\",",
            "    # etc.",
            "]",
            "# When adding a new database we try to connect to it. Depending on which parameters are",
            "# incorrect this could take a couple minutes, until the SQLAlchemy driver pinging the",
            "# database times out. Instead of relying on the driver timeout we can specify a shorter",
            "# one here.",
            "TEST_DATABASE_CONNECTION_TIMEOUT = timedelta(seconds=30)",
            "",
            "# Enable/disable CSP warning",
            "CONTENT_SECURITY_POLICY_WARNING = True",
            "",
            "# Do you want Talisman enabled?",
            "TALISMAN_ENABLED = utils.cast_to_boolean(os.environ.get(\"TALISMAN_ENABLED\", True))",
            "",
            "# If you want Talisman, how do you want it configured??",
            "TALISMAN_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"base-uri\": [\"'self'\"],",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [",
            "            \"'self'\",",
            "            \"blob:\",",
            "            \"data:\",",
            "            \"https://apachesuperset.gateway.scarf.sh\",",
            "            \"https://static.scarf.sh/\",",
            "        ],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'strict-dynamic'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "    \"session_cookie_secure\": False,",
            "}",
            "# React requires `eval` to work correctly in dev mode",
            "TALISMAN_DEV_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"base-uri\": [\"'self'\"],",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [",
            "            \"'self'\",",
            "            \"blob:\",",
            "            \"data:\",",
            "            \"https://apachesuperset.gateway.scarf.sh\",",
            "            \"https://static.scarf.sh/\",",
            "        ],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'unsafe-inline'\", \"'unsafe-eval'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "    \"session_cookie_secure\": False,",
            "}",
            "",
            "#",
            "# Flask session cookie options",
            "#",
            "# See https://flask.palletsprojects.com/en/1.1.x/security/#set-cookie-options",
            "# for details",
            "#",
            "SESSION_COOKIE_HTTPONLY = True  # Prevent cookie from being read by frontend JS?",
            "SESSION_COOKIE_SECURE = False  # Prevent cookie from being transmitted over non-tls?",
            "SESSION_COOKIE_SAMESITE: Literal[\"None\", \"Lax\", \"Strict\"] | None = \"Lax\"",
            "# Whether to use server side sessions from flask-session or Flask secure cookies",
            "SESSION_SERVER_SIDE = False",
            "# Example config using Redis as the backend for server side sessions",
            "# from flask_session import RedisSessionInterface",
            "#",
            "# SESSION_SERVER_SIDE = True",
            "# SESSION_USE_SIGNER = True",
            "# SESSION_TYPE = \"redis\"",
            "# SESSION_REDIS = Redis(host=\"localhost\", port=6379, db=0)",
            "#",
            "# Other possible config options and backends:",
            "# # https://flask-session.readthedocs.io/en/latest/config.html",
            "",
            "# Cache static resources.",
            "SEND_FILE_MAX_AGE_DEFAULT = int(timedelta(days=365).total_seconds())",
            "",
            "# URI to database storing the example data, points to",
            "# SQLALCHEMY_DATABASE_URI by default if set to `None`",
            "SQLALCHEMY_EXAMPLES_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"examples.db\")",
            "",
            "# Optional prefix to be added to all static asset paths when rendering the UI.",
            "# This is useful for hosting assets in an external CDN, for example",
            "STATIC_ASSETS_PREFIX = \"\"",
            "",
            "# Some sqlalchemy connection strings can open Superset to security risks.",
            "# Typically these should not be allowed.",
            "PREVENT_UNSAFE_DB_CONNECTIONS = True",
            "",
            "# If true all default urls on datasets will be handled as relative URLs by the frontend",
            "PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET = True",
            "",
            "# Define a list of allowed URLs for dataset data imports (v1).",
            "# Simple example to only allow URLs that belong to certain domains:",
            "# ALLOWED_IMPORT_URL_DOMAINS = [",
            "#     r\"^https://.+\\.domain1\\.com\\/?.*\", r\"^https://.+\\.domain2\\.com\\/?.*\"",
            "# ]",
            "DATASET_IMPORT_ALLOWED_DATA_URLS = [r\".*\"]",
            "",
            "# Path used to store SSL certificates that are generated when using custom certs.",
            "# Defaults to temporary directory.",
            "# Example: SSL_CERT_PATH = \"/certs\"",
            "SSL_CERT_PATH: str | None = None",
            "",
            "# SQLA table mutator, every time we fetch the metadata for a certain table",
            "# (superset.connectors.sqla.models.SqlaTable), we call this hook",
            "# to allow mutating the object with this callback.",
            "# This can be used to set any properties of the object based on naming",
            "# conventions and such. You can find examples in the tests.",
            "",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "SQLA_TABLE_MUTATOR = lambda table: table",
            "",
            "",
            "# Global async query config options.",
            "# Requires GLOBAL_ASYNC_QUERIES feature flag to be enabled.",
            "GLOBAL_ASYNC_QUERY_MANAGER_CLASS = (",
            "    \"superset.async_events.async_query_manager.AsyncQueryManager\"",
            ")",
            "GLOBAL_ASYNC_QUERIES_REDIS_CONFIG = {",
            "    \"port\": 6379,",
            "    \"host\": \"127.0.0.1\",",
            "    \"password\": \"\",",
            "    \"db\": 0,",
            "    \"ssl\": False,",
            "}",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_PREFIX = \"async-events-\"",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT = 1000",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT_FIREHOSE = 1000000",
            "GLOBAL_ASYNC_QUERIES_REGISTER_REQUEST_HANDLERS = True",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_NAME = \"async-token\"",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SECURE = False",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SAMESITE: None | (",
            "    Literal[\"None\", \"Lax\", \"Strict\"]",
            ") = None",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_DOMAIN = None",
            "GLOBAL_ASYNC_QUERIES_JWT_SECRET = \"test-secret-change-me\"",
            "GLOBAL_ASYNC_QUERIES_TRANSPORT: Literal[\"polling\", \"ws\"] = \"polling\"",
            "GLOBAL_ASYNC_QUERIES_POLLING_DELAY = int(",
            "    timedelta(milliseconds=500).total_seconds() * 1000",
            ")",
            "GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL = \"ws://127.0.0.1:8080/\"",
            "",
            "# Embedded config options",
            "GUEST_ROLE_NAME = \"Public\"",
            "GUEST_TOKEN_JWT_SECRET = \"test-guest-secret-change-me\"",
            "GUEST_TOKEN_JWT_ALGO = \"HS256\"",
            "GUEST_TOKEN_HEADER_NAME = \"X-GuestToken\"",
            "GUEST_TOKEN_JWT_EXP_SECONDS = 300  # 5 minutes",
            "# Guest token audience for the embedded superset, either string or callable",
            "GUEST_TOKEN_JWT_AUDIENCE: Callable[[], str] | str | None = None",
            "",
            "# A SQL dataset health check. Note if enabled it is strongly advised that the callable",
            "# be memoized to aid with performance, i.e.,",
            "#",
            "#    @cache_manager.cache.memoize(timeout=0)",
            "#    def DATASET_HEALTH_CHECK(datasource: SqlaTable) -> Optional[str]:",
            "#        if (",
            "#            datasource.sql and",
            "#            len(sql_parse.ParsedQuery(datasource.sql, strip_comments=True).tables) == 1",
            "#        ):",
            "#            return (",
            "#                \"This virtual dataset queries only one table and therefore could be \"",
            "#                \"replaced by querying the table directly.\"",
            "#            )",
            "#",
            "#        return None",
            "#",
            "# Within the FLASK_APP_MUTATOR callable, i.e., once the application and thus cache have",
            "# been initialized it is also necessary to add the following logic to blow the cache for",
            "# all datasources if the callback function changed.",
            "#",
            "#    def FLASK_APP_MUTATOR(app: Flask) -> None:",
            "#        name = \"DATASET_HEALTH_CHECK\"",
            "#        func = app.config[name]",
            "#        code = func.uncached.__code__.co_code",
            "#",
            "#        if cache_manager.cache.get(name) != code:",
            "#            cache_manager.cache.delete_memoized(func)",
            "#            cache_manager.cache.set(name, code, timeout=0)",
            "#",
            "DATASET_HEALTH_CHECK: Callable[[SqlaTable], str] | None = None",
            "",
            "# the advanced data type key should correspond to that set in the column metadata",
            "ADVANCED_DATA_TYPES: dict[str, AdvancedDataType] = {",
            "    \"internet_address\": internet_address,",
            "    \"port\": internet_port,",
            "}",
            "",
            "# By default, the Welcome page features all charts and dashboards the user has access",
            "# to. This can be changed to show only examples, or a custom view",
            "# by providing the title and a FAB filter:",
            "# WELCOME_PAGE_LAST_TAB = (",
            "#     \"Xyz\",",
            "#     [{\"col\": 'created_by', \"opr\": 'rel_o_m', \"value\": 10}],",
            "# )",
            "WELCOME_PAGE_LAST_TAB: (",
            "    Literal[\"examples\", \"all\"] | tuple[str, list[dict[str, Any]]]",
            ") = \"all\"",
            "",
            "# Max allowed size for a zipped file",
            "ZIPPED_FILE_MAX_SIZE = 100 * 1024 * 1024  # 100MB",
            "# Max allowed compression ratio for a zipped file",
            "ZIP_FILE_MAX_COMPRESS_RATIO = 200.0",
            "",
            "# Configuration for environment tag shown on the navbar. Setting 'text' to '' will hide the tag.",
            "# 'color' can either be a hex color code, or a dot-indexed theme color (e.g. error.base)",
            "ENVIRONMENT_TAG_CONFIG = {",
            "    \"variable\": \"SUPERSET_ENV\",",
            "    \"values\": {",
            "        \"debug\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"flask-debug\",",
            "        },",
            "        \"development\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"Development\",",
            "        },",
            "        \"production\": {",
            "            \"color\": \"\",",
            "            \"text\": \"\",",
            "        },",
            "    },",
            "}",
            "",
            "",
            "# Extra related query filters make it possible to limit which objects are shown",
            "# in the UI. For examples, to only show \"admin\" or users starting with the letter \"b\" in",
            "# the \"Owners\" dropdowns, you could add the following in your config:",
            "# def user_filter(query: Query, *args, *kwargs):",
            "#     from superset import security_manager",
            "#",
            "#     user_model = security_manager.user_model",
            "#     filters = [",
            "#         user_model.username == \"admin\",",
            "#         user_model.username.ilike(\"b%\"),",
            "#     ]",
            "#     return query.filter(or_(*filters))",
            "#",
            "#  EXTRA_RELATED_QUERY_FILTERS = {\"user\": user_filter}",
            "#",
            "# Similarly, to restrict the roles in the \"Roles\" dropdown you can provide a custom",
            "# filter callback for the \"role\" key.",
            "class ExtraRelatedQueryFilters(TypedDict, total=False):",
            "    role: Callable[[Query], Query]",
            "    user: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_RELATED_QUERY_FILTERS: ExtraRelatedQueryFilters = {}",
            "",
            "",
            "# Extra dynamic query filters make it possible to limit which objects are shown",
            "# in the UI before any other filtering is applied. Useful for example when",
            "# considering to filter using Feature Flags along with regular role filters",
            "# that get applied by default in our base_filters.",
            "# For example, to only show a database starting with the letter \"b\"",
            "# in the \"Database Connections\" list, you could add the following in your config:",
            "# def initial_database_filter(query: Query, *args, *kwargs):",
            "#     from superset.models.core import Database",
            "#",
            "#     filter = Database.database_name.startswith('b')",
            "#     return query.filter(filter)",
            "#",
            "#  EXTRA_DYNAMIC_QUERY_FILTERS = {\"database\": initial_database_filter}",
            "class ExtraDynamicQueryFilters(TypedDict, total=False):",
            "    databases: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_DYNAMIC_QUERY_FILTERS: ExtraDynamicQueryFilters = {}",
            "",
            "",
            "# -------------------------------------------------------------------",
            "# *                WARNING:  STOP EDITING  HERE                    *",
            "# -------------------------------------------------------------------",
            "# Don't add config values below this line since local configs won't be",
            "# able to override them.",
            "if CONFIG_PATH_ENV_VAR in os.environ:",
            "    # Explicitly import config module that is not necessarily in pythonpath; useful",
            "    # for case where app is being executed via pex.",
            "    cfg_path = os.environ[CONFIG_PATH_ENV_VAR]",
            "    try:",
            "        module = sys.modules[__name__]",
            "        override_conf = imp.load_source(\"superset_config\", cfg_path)",
            "        for key in dir(override_conf):",
            "            if key.isupper():",
            "                setattr(module, key, getattr(override_conf, key))",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{cfg_path}]\")",
            "    except Exception:",
            "        logger.exception(",
            "            \"Failed to import config for %s=%s\", CONFIG_PATH_ENV_VAR, cfg_path",
            "        )",
            "        raise",
            "elif importlib.util.find_spec(\"superset_config\") and not is_test():",
            "    try:",
            "        # pylint: disable=import-error,wildcard-import,unused-wildcard-import",
            "        import superset_config",
            "        from superset_config import *  # type: ignore",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{superset_config.__file__}]\")",
            "    except Exception:",
            "        logger.exception(\"Found but failed to import local superset_config\")",
            "        raise"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"The main config file for Superset",
            "",
            "All configuration in this file can be overridden by providing a superset_config",
            "in your PYTHONPATH as there is a ``from superset_config import *``",
            "at the end of this file.",
            "\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import imp  # pylint: disable=deprecated-module",
            "import importlib.util",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import sys",
            "from collections import OrderedDict",
            "from datetime import timedelta",
            "from email.mime.multipart import MIMEMultipart",
            "from importlib.resources import files",
            "from typing import Any, Callable, Literal, TYPE_CHECKING, TypedDict",
            "",
            "import pkg_resources",
            "from celery.schedules import crontab",
            "from flask import Blueprint",
            "from flask_appbuilder.security.manager import AUTH_DB",
            "from flask_caching.backends.base import BaseCache",
            "from pandas import Series",
            "from pandas._libs.parsers import STR_NA_VALUES  # pylint: disable=no-name-in-module",
            "from sqlalchemy.orm.query import Query",
            "",
            "from superset.advanced_data_type.plugins.internet_address import internet_address",
            "from superset.advanced_data_type.plugins.internet_port import internet_port",
            "from superset.advanced_data_type.types import AdvancedDataType",
            "from superset.constants import CHANGE_ME_SECRET_KEY",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.key_value.types import JsonKeyValueCodec",
            "from superset.stats_logger import DummyStatsLogger",
            "from superset.superset_typing import CacheConfig",
            "from superset.tasks.types import ExecutorType",
            "from superset.utils import core as utils",
            "from superset.utils.core import is_test, NO_TIME_RANGE, parse_boolean_string",
            "from superset.utils.encrypt import SQLAlchemyUtilsAdapter",
            "from superset.utils.log import DBEventLogger",
            "from superset.utils.logging_configurator import DefaultLoggingConfigurator",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from flask_appbuilder.security.sqla import models",
            "",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.slice import Slice",
            "",
            "# Realtime stats logger, a StatsD implementation exists",
            "STATS_LOGGER = DummyStatsLogger()",
            "EVENT_LOGGER = DBEventLogger()",
            "",
            "SUPERSET_LOG_VIEW = True",
            "",
            "BASE_DIR = pkg_resources.resource_filename(\"superset\", \"\")",
            "if \"SUPERSET_HOME\" in os.environ:",
            "    DATA_DIR = os.environ[\"SUPERSET_HOME\"]",
            "else:",
            "    DATA_DIR = os.path.expanduser(\"~/.superset\")",
            "",
            "# ---------------------------------------------------------",
            "# Superset specific config",
            "# ---------------------------------------------------------",
            "VERSION_INFO_FILE = str(files(\"superset\") / \"static/version_info.json\")",
            "PACKAGE_JSON_FILE = str(files(\"superset\") / \"static/assets/package.json\")",
            "",
            "",
            "# Multiple favicons can be specified here. The \"href\" property",
            "# is mandatory, but \"sizes,\" \"type,\" and \"rel\" are optional.",
            "# For example:",
            "# {",
            "#     \"href\":path/to/image.png\",",
            "#     \"sizes\": \"16x16\",",
            "#     \"type\": \"image/png\"",
            "#     \"rel\": \"icon\"",
            "# },",
            "FAVICONS = [{\"href\": \"/static/assets/images/favicon.png\"}]",
            "",
            "",
            "def _try_json_readversion(filepath: str) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"version\")",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def _try_json_readsha(filepath: str, length: int) -> str | None:",
            "    try:",
            "        with open(filepath) as f:",
            "            return json.load(f).get(\"GIT_SHA\")[:length]",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "#",
            "# If True, we will skip the call to load the logger config found in alembic.init",
            "#",
            "ALEMBIC_SKIP_LOG_CONFIG = False",
            "",
            "# Depending on the context in which this config is loaded, the",
            "# version_info.json file may or may not be available, as it is",
            "# generated on install via setup.py. In the event that we're",
            "# actually running Superset, we will have already installed,",
            "# therefore it WILL exist. When unit tests are running, however,",
            "# it WILL NOT exist, so we fall back to reading package.json",
            "VERSION_STRING = _try_json_readversion(VERSION_INFO_FILE) or _try_json_readversion(",
            "    PACKAGE_JSON_FILE",
            ")",
            "",
            "VERSION_SHA_LENGTH = 8",
            "VERSION_SHA = _try_json_readsha(VERSION_INFO_FILE, VERSION_SHA_LENGTH)",
            "",
            "# Build number is shown in the About section if available. This",
            "# can be replaced at build time to expose build information.",
            "BUILD_NUMBER = None",
            "",
            "# default viz used in chart explorer & SQL Lab explore",
            "DEFAULT_VIZ_TYPE = \"table\"",
            "",
            "# default row limit when requesting chart data",
            "ROW_LIMIT = 50000",
            "# default row limit when requesting samples from datasource in explore view",
            "SAMPLES_ROW_LIMIT = 1000",
            "# default row limit for native filters",
            "NATIVE_FILTER_DEFAULT_ROW_LIMIT = 1000",
            "# max rows retrieved by filter select auto complete",
            "FILTER_SELECT_ROW_LIMIT = 10000",
            "# default time filter in explore",
            "# values may be \"Last day\", \"Last week\", \"<ISO date> : now\", etc.",
            "DEFAULT_TIME_FILTER = NO_TIME_RANGE",
            "",
            "# This is an important setting, and should be lower than your",
            "# [load balancer / proxy / envoy / kong / ...] timeout settings.",
            "# You should also make sure to configure your WSGI server",
            "# (gunicorn, nginx, apache, ...) timeout setting to be <= to this setting",
            "SUPERSET_WEBSERVER_TIMEOUT = int(timedelta(minutes=1).total_seconds())",
            "",
            "# this 2 settings are used by dashboard period force refresh feature",
            "# When user choose auto force refresh frequency",
            "# < SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT",
            "# they will see warning message in the Refresh Interval Modal.",
            "# please check PR #9886",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT = 0",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE = None",
            "",
            "SUPERSET_DASHBOARD_POSITION_DATA_LIMIT = 65535",
            "CUSTOM_SECURITY_MANAGER = None",
            "SQLALCHEMY_TRACK_MODIFICATIONS = False",
            "# ---------------------------------------------------------",
            "",
            "# Your App secret key. Make sure you override it on superset_config.py",
            "# or use `SUPERSET_SECRET_KEY` environment variable.",
            "# Use a strong complex alphanumeric string and use a tool to help you generate",
            "# a sufficiently random sequence, ex: openssl rand -base64 42\"",
            "SECRET_KEY = os.environ.get(\"SUPERSET_SECRET_KEY\") or CHANGE_ME_SECRET_KEY",
            "",
            "# The SQLAlchemy connection string.",
            "SQLALCHEMY_DATABASE_URI = (",
            "    f\"\"\"sqlite:///{os.path.join(DATA_DIR, \"superset.db\")}?check_same_thread=false\"\"\"",
            ")",
            "",
            "# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'",
            "# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp'",
            "",
            "# In order to hook up a custom password store for all SQLALCHEMY connections",
            "# implement a function that takes a single argument of type 'sqla.engine.url',",
            "# returns a password and set SQLALCHEMY_CUSTOM_PASSWORD_STORE.",
            "#",
            "# e.g.:",
            "# def lookup_password(url):",
            "#     return 'secret'",
            "# SQLALCHEMY_CUSTOM_PASSWORD_STORE = lookup_password",
            "SQLALCHEMY_CUSTOM_PASSWORD_STORE = None",
            "",
            "#",
            "# The EncryptedFieldTypeAdapter is used whenever we're building SqlAlchemy models",
            "# which include sensitive fields that should be app-encrypted BEFORE sending",
            "# to the DB.",
            "#",
            "# Note: the default impl leverages SqlAlchemyUtils' EncryptedType, which defaults",
            "#  to AesEngine that uses AES-128 under the covers using the app's SECRET_KEY",
            "#  as key material. Do note that AesEngine allows for queryability over the",
            "#  encrypted fields.",
            "#",
            "#  To change the default engine you need to define your own adapter:",
            "#",
            "# e.g.:",
            "#",
            "# class AesGcmEncryptedAdapter(",
            "#     AbstractEncryptedFieldAdapter",
            "# ):",
            "#     def create(",
            "#         self,",
            "#         app_config: Optional[Dict[str, Any]],",
            "#         *args: List[Any],",
            "#         **kwargs: Optional[Dict[str, Any]],",
            "#     ) -> TypeDecorator:",
            "#         if app_config:",
            "#             return EncryptedType(",
            "#                 *args, app_config[\"SECRET_KEY\"], engine=AesGcmEngine, **kwargs",
            "#             )",
            "#         raise Exception(\"Missing app_config kwarg\")",
            "#",
            "#",
            "#  SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = AesGcmEncryptedAdapter",
            "SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = (  # pylint: disable=invalid-name",
            "    SQLAlchemyUtilsAdapter",
            ")",
            "# The limit of queries fetched for query search",
            "QUERY_SEARCH_LIMIT = 1000",
            "",
            "# Flask-WTF flag for CSRF",
            "WTF_CSRF_ENABLED = True",
            "",
            "# Add endpoints that need to be exempt from CSRF protection",
            "WTF_CSRF_EXEMPT_LIST = [",
            "    \"superset.views.core.log\",",
            "    \"superset.views.core.explore_json\",",
            "    \"superset.charts.data.api.data\",",
            "]",
            "",
            "# Whether to run the web server in debug mode or not",
            "DEBUG = os.environ.get(\"FLASK_DEBUG\")",
            "FLASK_USE_RELOAD = True",
            "",
            "# Enable profiling of Python calls. Turn this on and append ``?_instrument=1``",
            "# to the page to see the call stack.",
            "PROFILING = False",
            "",
            "# Superset allows server-side python stacktraces to be surfaced to the",
            "# user when this feature is on. This may have security implications",
            "# and it's more secure to turn it off in production settings.",
            "SHOW_STACKTRACE = False",
            "",
            "# Use all X-Forwarded headers when ENABLE_PROXY_FIX is True.",
            "# When proxying to a different port, set \"x_port\" to 0 to avoid downstream issues.",
            "ENABLE_PROXY_FIX = False",
            "PROXY_FIX_CONFIG = {\"x_for\": 1, \"x_proto\": 1, \"x_host\": 1, \"x_port\": 1, \"x_prefix\": 1}",
            "",
            "# Configuration for scheduling queries from SQL Lab.",
            "SCHEDULED_QUERIES: dict[str, Any] = {}",
            "",
            "# FAB Rate limiting: this is a security feature for preventing DDOS attacks. The",
            "# feature is on by default to make Superset secure by default, but you should",
            "# fine tune the limits to your needs. You can read more about the different",
            "# parameters here: https://flask-limiter.readthedocs.io/en/stable/configuration.html",
            "RATELIMIT_ENABLED = True",
            "RATELIMIT_APPLICATION = \"50 per second\"",
            "AUTH_RATE_LIMITED = True",
            "AUTH_RATE_LIMIT = \"5 per second\"",
            "# A storage location conforming to the scheme in storage-scheme. See the limits",
            "# library for allowed values: https://limits.readthedocs.io/en/stable/storage.html",
            "# RATELIMIT_STORAGE_URI = \"redis://host:port\"",
            "# A callable that returns the unique identity of the current request.",
            "# RATELIMIT_REQUEST_IDENTIFIER = flask.Request.endpoint",
            "",
            "# ------------------------------",
            "# GLOBALS FOR APP Builder",
            "# ------------------------------",
            "# Uncomment to setup Your App name",
            "APP_NAME = \"Superset\"",
            "",
            "# Specify the App icon",
            "APP_ICON = \"/static/assets/images/superset-logo-horiz.png\"",
            "",
            "# Specify where clicking the logo would take the user",
            "# e.g. setting it to '/' would take the user to '/superset/welcome/'",
            "LOGO_TARGET_PATH = None",
            "",
            "# Specify tooltip that should appear when hovering over the App Icon/Logo",
            "LOGO_TOOLTIP = \"\"",
            "",
            "# Specify any text that should appear to the right of the logo",
            "LOGO_RIGHT_TEXT: Callable[[], str] | str = \"\"",
            "",
            "# Enables SWAGGER UI for superset openapi spec",
            "# ex: http://localhost:8080/swagger/v1",
            "FAB_API_SWAGGER_UI = True",
            "",
            "# ----------------------------------------------------",
            "# AUTHENTICATION CONFIG",
            "# ----------------------------------------------------",
            "# The authentication type",
            "# AUTH_OID : Is for OpenID",
            "# AUTH_DB : Is for database (username/password)",
            "# AUTH_LDAP : Is for LDAP",
            "# AUTH_REMOTE_USER : Is for using REMOTE_USER from web server",
            "AUTH_TYPE = AUTH_DB",
            "",
            "# Uncomment to setup Full admin role name",
            "# AUTH_ROLE_ADMIN = 'Admin'",
            "",
            "# Uncomment to setup Public role name, no authentication needed",
            "# AUTH_ROLE_PUBLIC = 'Public'",
            "",
            "# Will allow user self registration",
            "# AUTH_USER_REGISTRATION = True",
            "",
            "# The default user self registration role",
            "# AUTH_USER_REGISTRATION_ROLE = \"Public\"",
            "",
            "# When using LDAP Auth, setup the LDAP server",
            "# AUTH_LDAP_SERVER = \"ldap://ldapserver.new\"",
            "",
            "# Uncomment to setup OpenID providers example for OpenID authentication",
            "# OPENID_PROVIDERS = [",
            "#    { 'name': 'Yahoo', 'url': 'https://open.login.yahoo.com/' },",
            "#    { 'name': 'Flickr', 'url': 'https://www.flickr.com/<username>' },",
            "",
            "# ---------------------------------------------------",
            "# Roles config",
            "# ---------------------------------------------------",
            "# Grant public role the same set of permissions as for a selected builtin role.",
            "# This is useful if one wants to enable anonymous users to view",
            "# dashboards. Explicit grant on specific datasets is still required.",
            "PUBLIC_ROLE_LIKE: str | None = None",
            "",
            "# ---------------------------------------------------",
            "# Babel config for translations",
            "# ---------------------------------------------------",
            "# Setup default language",
            "BABEL_DEFAULT_LOCALE = \"en\"",
            "# Your application default translation path",
            "BABEL_DEFAULT_FOLDER = \"superset/translations\"",
            "# The allowed translation for your app",
            "LANGUAGES = {",
            "    \"en\": {\"flag\": \"us\", \"name\": \"English\"},",
            "    \"es\": {\"flag\": \"es\", \"name\": \"Spanish\"},",
            "    \"it\": {\"flag\": \"it\", \"name\": \"Italian\"},",
            "    \"fr\": {\"flag\": \"fr\", \"name\": \"French\"},",
            "    \"zh\": {\"flag\": \"cn\", \"name\": \"Chinese\"},",
            "    \"ja\": {\"flag\": \"jp\", \"name\": \"Japanese\"},",
            "    \"de\": {\"flag\": \"de\", \"name\": \"German\"},",
            "    \"pt\": {\"flag\": \"pt\", \"name\": \"Portuguese\"},",
            "    \"pt_BR\": {\"flag\": \"br\", \"name\": \"Brazilian Portuguese\"},",
            "    \"ru\": {\"flag\": \"ru\", \"name\": \"Russian\"},",
            "    \"ko\": {\"flag\": \"kr\", \"name\": \"Korean\"},",
            "    \"sk\": {\"flag\": \"sk\", \"name\": \"Slovak\"},",
            "    \"sl\": {\"flag\": \"si\", \"name\": \"Slovenian\"},",
            "    \"nl\": {\"flag\": \"nl\", \"name\": \"Dutch\"},",
            "    \"uk\": {\"flag\": \"uk\", \"name\": \"Ukranian\"},",
            "}",
            "# Turning off i18n by default as translation in most languages are",
            "# incomplete and not well maintained.",
            "LANGUAGES = {}",
            "",
            "",
            "# Override the default d3 locale format",
            "# Default values are equivalent to",
            "# D3_FORMAT = {",
            "#     \"decimal\": \".\",           # - decimal place string (e.g., \".\").",
            "#     \"thousands\": \",\",         # - group separator string (e.g., \",\").",
            "#     \"grouping\": [3],          # - array of group sizes (e.g., [3]), cycled as needed.",
            "#     \"currency\": [\"$\", \"\"]     # - currency prefix/suffix strings (e.g., [\"$\", \"\"])",
            "# }",
            "# https://github.com/d3/d3-format/blob/main/README.md#formatLocale",
            "class D3Format(TypedDict, total=False):",
            "    decimal: str",
            "    thousands: str",
            "    grouping: list[int]",
            "    currency: list[str]",
            "",
            "",
            "D3_FORMAT: D3Format = {}",
            "",
            "CURRENCIES = [\"USD\", \"EUR\", \"GBP\", \"INR\", \"MXN\", \"JPY\", \"CNY\"]",
            "",
            "# ---------------------------------------------------",
            "# Feature flags",
            "# ---------------------------------------------------",
            "# Feature flags that are set by default go here. Their values can be",
            "# overwritten by those specified under FEATURE_FLAGS in superset_config.py",
            "# For example, DEFAULT_FEATURE_FLAGS = { 'FOO': True, 'BAR': False } here",
            "# and FEATURE_FLAGS = { 'BAR': True, 'BAZ': True } in superset_config.py",
            "# will result in combined feature flags of { 'FOO': True, 'BAR': True, 'BAZ': True }",
            "DEFAULT_FEATURE_FLAGS: dict[str, bool] = {",
            "    # When using a recent version of Druid that supports JOINs turn this on",
            "    \"DRUID_JOINS\": False,",
            "    \"DYNAMIC_PLUGINS\": False,",
            "    # With Superset 2.0, we are updating the default so that the legacy datasource",
            "    # editor no longer shows. Currently this is set to false so that the editor",
            "    # option does show, but we will be depreciating it.",
            "    \"DISABLE_LEGACY_DATASOURCE_EDITOR\": True,",
            "    \"ENABLE_TEMPLATE_PROCESSING\": False,",
            "    # Allow for javascript controls components",
            "    # this enables programmers to customize certain charts (like the",
            "    # geospatial ones) by inputting javascript in controls. This exposes",
            "    # an XSS security vulnerability",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\": False,  # deprecated",
            "    \"KV_STORE\": False,  # deprecated",
            "    # When this feature is enabled, nested types in Presto will be",
            "    # expanded into extra columns and/or arrays. This is experimental,",
            "    # and doesn't work with all nested types.",
            "    \"PRESTO_EXPAND_DATA\": False,",
            "    # Exposes API endpoint to compute thumbnails",
            "    \"THUMBNAILS\": False,",
            "    \"SHARE_QUERIES_VIA_KV_STORE\": False,",
            "    \"TAGGING_SYSTEM\": False,",
            "    \"SQLLAB_BACKEND_PERSISTENCE\": True,",
            "    \"LISTVIEWS_DEFAULT_CARD_VIEW\": False,",
            "    # When True, this escapes HTML (rather than rendering it) in Markdown components",
            "    \"ESCAPE_MARKDOWN_HTML\": False,",
            "    \"DASHBOARD_CROSS_FILTERS\": True,  # deprecated",
            "    \"DASHBOARD_VIRTUALIZATION\": True,",
            "    \"GLOBAL_ASYNC_QUERIES\": False,",
            "    \"EMBEDDED_SUPERSET\": False,",
            "    # Enables Alerts and reports new implementation",
            "    \"ALERT_REPORTS\": False,",
            "    \"DASHBOARD_RBAC\": False,",
            "    \"ENABLE_ADVANCED_DATA_TYPES\": False,",
            "    # Enabling ALERTS_ATTACH_REPORTS, the system sends email and slack message",
            "    # with screenshot and link",
            "    # Disables ALERTS_ATTACH_REPORTS, the system DOES NOT generate screenshot",
            "    # for report with type 'alert' and sends email and slack message with only link;",
            "    # for report with type 'report' still send with email and slack message with",
            "    # screenshot and link",
            "    \"ALERTS_ATTACH_REPORTS\": True,",
            "    # Allow users to export full CSV of table viz type.",
            "    # This could cause the server to run out of memory or compute.",
            "    \"ALLOW_FULL_CSV_EXPORT\": False,",
            "    \"ALLOW_ADHOC_SUBQUERY\": False,",
            "    \"USE_ANALAGOUS_COLORS\": False,",
            "    # Apply RLS rules to SQL Lab queries. This requires parsing and manipulating the",
            "    # query, and might break queries and/or allow users to bypass RLS. Use with care!",
            "    \"RLS_IN_SQLLAB\": False,",
            "    # Enable caching per impersonation key (e.g username) in a datasource where user",
            "    # impersonation is enabled",
            "    \"CACHE_IMPERSONATION\": False,",
            "    # Enable caching per user key for Superset cache (not database cache impersonation)",
            "    \"CACHE_QUERY_BY_USER\": False,",
            "    # Enable sharing charts with embedding",
            "    \"EMBEDDABLE_CHARTS\": True,",
            "    \"DRILL_TO_DETAIL\": True,",
            "    \"DRILL_BY\": True,",
            "    \"DATAPANEL_CLOSED_BY_DEFAULT\": False,",
            "    \"HORIZONTAL_FILTER_BAR\": False,",
            "    # The feature is off by default, and currently only supported in Presto and Postgres,",
            "    # and Bigquery.",
            "    # It also needs to be enabled on a per-database basis, by adding the key/value pair",
            "    # `cost_estimate_enabled: true` to the database `extra` attribute.",
            "    \"ESTIMATE_QUERY_COST\": False,",
            "    # Allow users to enable ssh tunneling when creating a DB.",
            "    # Users must check whether the DB engine supports SSH Tunnels",
            "    # otherwise enabling this flag won't have any effect on the DB.",
            "    \"SSH_TUNNELING\": False,",
            "    \"AVOID_COLORS_COLLISION\": True,",
            "    # Do not show user info in the menu",
            "    \"MENU_HIDE_USER_INFO\": False,",
            "    # Allows users to add a ``superset://`` DB that can query across databases. This is",
            "    # an experimental feature with potential security and performance risks, so use with",
            "    # caution. If the feature is enabled you can also set a limit for how much data is",
            "    # returned from each database in the ``SUPERSET_META_DB_LIMIT`` configuration value",
            "    # in this file.",
            "    \"ENABLE_SUPERSET_META_DB\": False,",
            "    # Set to True to replace Selenium with Playwright to execute reports and thumbnails.",
            "    # Unlike Selenium, Playwright reports support deck.gl visualizations",
            "    # Enabling this feature flag requires installing \"playwright\" pip package",
            "    \"PLAYWRIGHT_REPORTS_AND_THUMBNAILS\": False,",
            "    # Set to True to enable experimental chart plugins",
            "    \"CHART_PLUGINS_EXPERIMENTAL\": False,",
            "}",
            "",
            "# ------------------------------",
            "# SSH Tunnel",
            "# ------------------------------",
            "# Allow users to set the host used when connecting to the SSH Tunnel",
            "# as localhost and any other alias (0.0.0.0)",
            "# ----------------------------------------------------------------------",
            "#                             |",
            "# -------------+              |    +----------+",
            "#     LOCAL    |              |    |  REMOTE  | :22 SSH",
            "#     CLIENT   | <== SSH ========> |  SERVER  | :8080 web service",
            "# -------------+              |    +----------+",
            "#                             |",
            "#                          FIREWALL (only port 22 is open)",
            "",
            "# ----------------------------------------------------------------------",
            "SSH_TUNNEL_MANAGER_CLASS = \"superset.extensions.ssh.SSHManager\"",
            "SSH_TUNNEL_LOCAL_BIND_ADDRESS = \"127.0.0.1\"",
            "#: Timeout (seconds) for tunnel connection (open_channel timeout)",
            "SSH_TUNNEL_TIMEOUT_SEC = 10.0",
            "#: Timeout (seconds) for transport socket (``socket.settimeout``)",
            "SSH_TUNNEL_PACKET_TIMEOUT_SEC = 1.0",
            "",
            "",
            "# Feature flags may also be set via 'SUPERSET_FEATURE_' prefixed environment vars.",
            "DEFAULT_FEATURE_FLAGS.update(",
            "    {",
            "        k[len(\"SUPERSET_FEATURE_\") :]: parse_boolean_string(v)",
            "        for k, v in os.environ.items()",
            "        if re.search(r\"^SUPERSET_FEATURE_\\w+\", k)",
            "    }",
            ")",
            "",
            "# This is merely a default.",
            "FEATURE_FLAGS: dict[str, bool] = {}",
            "",
            "# A function that receives a dict of all feature flags",
            "# (DEFAULT_FEATURE_FLAGS merged with FEATURE_FLAGS)",
            "# can alter it, and returns a similar dict. Note the dict of feature",
            "# flags passed to the function is a deepcopy of the dict in the config,",
            "# and can therefore be mutated without side-effect",
            "#",
            "# GET_FEATURE_FLAGS_FUNC can be used to implement progressive rollouts,",
            "# role-based features, or a full on A/B testing framework.",
            "#",
            "# from flask import g, request",
            "# def GET_FEATURE_FLAGS_FUNC(feature_flags_dict: Dict[str, bool]) -> Dict[str, bool]:",
            "#     if hasattr(g, \"user\") and g.user.is_active:",
            "#         feature_flags_dict['some_feature'] = g.user and g.user.get_id() == 5",
            "#     return feature_flags_dict",
            "GET_FEATURE_FLAGS_FUNC: Callable[[dict[str, bool]], dict[str, bool]] | None = None",
            "# A function that receives a feature flag name and an optional default value.",
            "# Has a similar utility to GET_FEATURE_FLAGS_FUNC but it's useful to not force the",
            "# evaluation of all feature flags when just evaluating a single one.",
            "#",
            "# Note that the default `get_feature_flags` will evaluate each feature with this",
            "# callable when the config key is set, so don't use both GET_FEATURE_FLAGS_FUNC",
            "# and IS_FEATURE_ENABLED_FUNC in conjunction.",
            "IS_FEATURE_ENABLED_FUNC: Callable[[str, bool | None], bool] | None = None",
            "# A function that expands/overrides the frontend `bootstrap_data.common` object.",
            "# Can be used to implement custom frontend functionality,",
            "# or dynamically change certain configs.",
            "#",
            "# Values in `bootstrap_data.common` should have these characteristics:",
            "# - They are not specific to a page the user is visiting",
            "# - They do not contain secrets",
            "#",
            "# Takes as a parameter the common bootstrap payload before transformations.",
            "# Returns a dict containing data that should be added or overridden to the payload.",
            "COMMON_BOOTSTRAP_OVERRIDES_FUNC: Callable[",
            "    [dict[str, Any]], dict[str, Any]",
            "] = lambda data: {}  # default: empty dict",
            "",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES is used for adding custom categorical color schemes",
            "# example code for \"My custom warm to hot\" color scheme",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES = [",
            "#     {",
            "#         \"id\": 'myVisualizationColors',",
            "#         \"description\": '',",
            "#         \"label\": 'My Visualization Colors',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77',",
            "#          '#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_CATEGORICAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# THEME_OVERRIDES is used for adding custom theme to superset",
            "# example code for \"My theme\" custom scheme",
            "# THEME_OVERRIDES = {",
            "#   \"borderRadius\": 4,",
            "#   \"colors\": {",
            "#     \"primary\": {",
            "#       \"base\": 'red',",
            "#     },",
            "#     \"secondary\": {",
            "#       \"base\": 'green',",
            "#     },",
            "#     \"grayscale\": {",
            "#       \"base\": 'orange',",
            "#     }",
            "#   }",
            "# }",
            "",
            "THEME_OVERRIDES: dict[str, Any] = {}",
            "",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES is used for adding custom sequential color schemes",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES =  [",
            "#     {",
            "#         \"id\": 'warmToHot',",
            "#         \"description\": '',",
            "#         \"isDiverging\": True,",
            "#         \"label\": 'My custom warm to hot',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD',",
            "#          '#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_SEQUENTIAL_COLOR_SCHEMES: list[dict[str, Any]] = []",
            "",
            "# ---------------------------------------------------",
            "# Thumbnail config (behind feature flag)",
            "# ---------------------------------------------------",
            "# By default, thumbnails are rendered per user, and will fall back to the Selenium",
            "# user for anonymous users. Similar to Alerts & Reports, thumbnails",
            "# can be configured to always be rendered as a fixed user. See",
            "# `superset.tasks.types.ExecutorType` for a full list of executor options.",
            "# To always use a fixed user account, use the following configuration:",
            "# THUMBNAIL_EXECUTE_AS = [ExecutorType.SELENIUM]",
            "THUMBNAIL_SELENIUM_USER: str | None = \"admin\"",
            "THUMBNAIL_EXECUTE_AS = [ExecutorType.CURRENT_USER, ExecutorType.SELENIUM]",
            "",
            "# By default, thumbnail digests are calculated based on various parameters in the",
            "# chart/dashboard metadata, and in the case of user-specific thumbnails, the",
            "# username. To specify a custom digest function, use the following config parameters",
            "# to define callbacks that receive",
            "# 1. the model (dashboard or chart)",
            "# 2. the executor type (e.g. ExecutorType.SELENIUM)",
            "# 3. the executor's username (note, this is the executor as defined by",
            "# `THUMBNAIL_EXECUTE_AS`; the executor is only equal to the currently logged in",
            "# user if the executor type is equal to `ExecutorType.CURRENT_USER`)",
            "# and return the final digest string:",
            "THUMBNAIL_DASHBOARD_DIGEST_FUNC: None | (",
            "    Callable[[Dashboard, ExecutorType, str], str]",
            ") = None",
            "THUMBNAIL_CHART_DIGEST_FUNC: Callable[[Slice, ExecutorType, str], str] | None = None",
            "",
            "THUMBNAIL_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"NullCache\",",
            "    \"CACHE_NO_NULL_WARNING\": True,",
            "}",
            "",
            "# Time before selenium times out after trying to locate an element on the page and wait",
            "# for that element to load for a screenshot.",
            "SCREENSHOT_LOCATE_WAIT = int(timedelta(seconds=10).total_seconds())",
            "# Time before selenium times out after waiting for all DOM class elements named",
            "# \"loading\" are gone.",
            "SCREENSHOT_LOAD_WAIT = int(timedelta(minutes=1).total_seconds())",
            "# Selenium destroy retries",
            "SCREENSHOT_SELENIUM_RETRIES = 5",
            "# Give selenium an headstart, in seconds",
            "SCREENSHOT_SELENIUM_HEADSTART = 3",
            "# Wait for the chart animation, in seconds",
            "SCREENSHOT_SELENIUM_ANIMATION_WAIT = 5",
            "# Replace unexpected errors in screenshots with real error messages",
            "SCREENSHOT_REPLACE_UNEXPECTED_ERRORS = False",
            "# Max time to wait for error message modal to show up, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_VISIBLE = 5",
            "# Max time to wait for error message modal to close, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_INVISIBLE = 5",
            "# Event that Playwright waits for when loading a new page",
            "# Possible values: \"load\", \"commit\", \"domcontentloaded\", \"networkidle\"",
            "# Docs: https://playwright.dev/python/docs/api/class-page#page-goto-option-wait-until",
            "SCREENSHOT_PLAYWRIGHT_WAIT_EVENT = \"load\"",
            "# Default timeout for Playwright browser context for all operations",
            "SCREENSHOT_PLAYWRIGHT_DEFAULT_TIMEOUT = int(",
            "    timedelta(seconds=30).total_seconds() * 1000",
            ")",
            "",
            "# ---------------------------------------------------",
            "# Image and file configuration",
            "# ---------------------------------------------------",
            "# The file upload folder, when using models with files",
            "UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "UPLOAD_CHUNK_SIZE = 4096",
            "",
            "# The image upload folder, when using models with images",
            "IMG_UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "",
            "# The image upload url, when using models with images",
            "IMG_UPLOAD_URL = \"/static/uploads/\"",
            "# Setup image size default is (300, 200, True)",
            "# IMG_SIZE = (300, 200, True)",
            "",
            "# Default cache timeout, applies to all cache backends unless specifically overridden in",
            "# each cache config.",
            "CACHE_DEFAULT_TIMEOUT = int(timedelta(days=1).total_seconds())",
            "",
            "# Default cache for Superset objects",
            "CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for datasource metadata and query results",
            "DATA_CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for dashboard filter state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "FILTER_STATE_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=90).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# Cache for explore form data state. `CACHE_TYPE` defaults to `SupersetMetastoreCache`",
            "# that stores the values in the key-value table in the Superset metastore, as it's",
            "# required for Superset to operate correctly, but can be replaced by any",
            "# `Flask-Caching` backend.",
            "EXPLORE_FORM_DATA_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"SupersetMetastoreCache\",",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=7).total_seconds()),",
            "    # Should the timeout be reset when retrieving a cached value?",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "    # The following parameter only applies to `MetastoreCache`:",
            "    # How should entries be serialized/deserialized?",
            "    \"CODEC\": JsonKeyValueCodec(),",
            "}",
            "",
            "# store cache keys by datasource UID (via CacheKey) for custom processing/invalidation",
            "STORE_CACHE_KEYS_IN_METADATA_DB = False",
            "",
            "# CORS Options",
            "ENABLE_CORS = False",
            "CORS_OPTIONS: dict[Any, Any] = {}",
            "",
            "# Sanitizes the HTML content used in markdowns to allow its rendering in a safe manner.",
            "# Disabling this option is not recommended for security reasons. If you wish to allow",
            "# valid safe elements that are not included in the default sanitization schema, use the",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS configuration.",
            "HTML_SANITIZATION = True",
            "",
            "# Use this configuration to extend the HTML sanitization schema.",
            "# By default we use the GitHub schema defined in",
            "# https://github.com/syntax-tree/hast-util-sanitize/blob/main/lib/schema.js",
            "# For example, the following configuration would allow the rendering of the",
            "# style attribute for div elements and the ftp protocol in hrefs:",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS = {",
            "#   \"attributes\": {",
            "#     \"div\": [\"style\"],",
            "#   },",
            "#   \"protocols\": {",
            "#     \"href\": [\"ftp\"],",
            "#   }",
            "# }",
            "# Be careful when extending the default schema to avoid XSS attacks.",
            "HTML_SANITIZATION_SCHEMA_EXTENSIONS: dict[str, Any] = {}",
            "",
            "# Chrome allows up to 6 open connections per domain at a time. When there are more",
            "# than 6 slices in dashboard, a lot of time fetch requests are queued up and wait for",
            "# next available socket. PR #5039 is trying to allow domain sharding for Superset,",
            "# and this feature will be enabled by configuration only (by default Superset",
            "# doesn't allow cross-domain request).",
            "SUPERSET_WEBSERVER_DOMAINS = None",
            "",
            "# Allowed format types for upload on Database view",
            "EXCEL_EXTENSIONS = {\"xlsx\", \"xls\"}",
            "CSV_EXTENSIONS = {\"csv\", \"tsv\", \"txt\"}",
            "COLUMNAR_EXTENSIONS = {\"parquet\", \"zip\"}",
            "ALLOWED_EXTENSIONS = {*EXCEL_EXTENSIONS, *CSV_EXTENSIONS, *COLUMNAR_EXTENSIONS}",
            "",
            "# Optional maximum file size in bytes when uploading a CSV",
            "CSV_UPLOAD_MAX_SIZE = None",
            "",
            "# CSV Options: key/value pairs that will be passed as argument to DataFrame.to_csv",
            "# method.",
            "# note: index option should not be overridden",
            "CSV_EXPORT = {\"encoding\": \"utf-8\"}",
            "",
            "# Excel Options: key/value pairs that will be passed as argument to DataFrame.to_excel",
            "# method.",
            "# note: index option should not be overridden",
            "EXCEL_EXPORT: dict[str, Any] = {}",
            "",
            "# ---------------------------------------------------",
            "# Time grain configurations",
            "# ---------------------------------------------------",
            "# List of time grains to disable in the application (see list of builtin",
            "# time grains in superset/db_engine_specs/base.py).",
            "# For example: to disable 1 second time grain:",
            "# TIME_GRAIN_DENYLIST = ['PT1S']",
            "TIME_GRAIN_DENYLIST: list[str] = []",
            "",
            "# Additional time grains to be supported using similar definitions as in",
            "# superset/db_engine_specs/base.py.",
            "# For example: To add a new 2 second time grain:",
            "# TIME_GRAIN_ADDONS = {'PT2S': '2 second'}",
            "TIME_GRAIN_ADDONS: dict[str, str] = {}",
            "",
            "# Implementation of additional time grains per engine.",
            "# The column to be truncated is denoted `{col}` in the expression.",
            "# For example: To implement 2 second time grain on clickhouse engine:",
            "# TIME_GRAIN_ADDON_EXPRESSIONS = {",
            "#     'clickhouse': {",
            "#         'PT2S': 'toDateTime(intDiv(toUInt32(toDateTime({col})), 2)*2)'",
            "#     }",
            "# }",
            "TIME_GRAIN_ADDON_EXPRESSIONS: dict[str, dict[str, str]] = {}",
            "",
            "# Map of custom time grains and artificial join column producers used",
            "# when generating the join key between results and time shifts.",
            "# See superset/common/query_context_processor.get_aggregated_join_column",
            "#",
            "# Example of a join column producer that aggregates by fiscal year",
            "# def join_producer(row: Series, column_index: int) -> str:",
            "#    return row[index].strftime(\"%F\")",
            "#",
            "# TIME_GRAIN_JOIN_COLUMN_PRODUCERS = {\"P1F\": join_producer}",
            "TIME_GRAIN_JOIN_COLUMN_PRODUCERS: dict[str, Callable[[Series, int], str]] = {}",
            "",
            "# ---------------------------------------------------",
            "# List of viz_types not allowed in your environment",
            "# For example: Disable pivot table and treemap:",
            "#  VIZ_TYPE_DENYLIST = ['pivot_table', 'treemap']",
            "# ---------------------------------------------------",
            "",
            "VIZ_TYPE_DENYLIST: list[str] = []",
            "",
            "# --------------------------------------------------",
            "# Modules, datasources and middleware to be registered",
            "# --------------------------------------------------",
            "DEFAULT_MODULE_DS_MAP = OrderedDict(",
            "    [",
            "        (\"superset.connectors.sqla.models\", [\"SqlaTable\"]),",
            "    ]",
            ")",
            "ADDITIONAL_MODULE_DS_MAP: dict[str, list[str]] = {}",
            "ADDITIONAL_MIDDLEWARE: list[Callable[..., Any]] = []",
            "",
            "# 1) https://docs.python-guide.org/writing/logging/",
            "# 2) https://docs.python.org/2/library/logging.config.html",
            "",
            "# Default configurator will consume the LOG_* settings below",
            "LOGGING_CONFIGURATOR = DefaultLoggingConfigurator()",
            "",
            "# Console Log Settings",
            "",
            "LOG_FORMAT = \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\"",
            "LOG_LEVEL = \"DEBUG\"",
            "",
            "# ---------------------------------------------------",
            "# Enable Time Rotate Log Handler",
            "# ---------------------------------------------------",
            "# LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL",
            "",
            "ENABLE_TIME_ROTATE = False",
            "TIME_ROTATE_LOG_LEVEL = \"DEBUG\"",
            "FILENAME = os.path.join(DATA_DIR, \"superset.log\")",
            "ROLLOVER = \"midnight\"",
            "INTERVAL = 1",
            "BACKUP_COUNT = 30",
            "",
            "# Custom logger for auditing queries. This can be used to send ran queries to a",
            "# structured immutable store for auditing purposes. The function is called for",
            "# every query ran, in both SQL Lab and charts/dashboards.",
            "# def QUERY_LOGGER(",
            "#     database,",
            "#     query,",
            "#     schema=None,",
            "#     client=None,",
            "#     security_manager=None,",
            "#     log_params=None,",
            "# ):",
            "#     pass",
            "QUERY_LOGGER = None",
            "",
            "# Set this API key to enable Mapbox visualizations",
            "MAPBOX_API_KEY = os.environ.get(\"MAPBOX_API_KEY\", \"\")",
            "",
            "# Maximum number of rows returned for any analytical database query",
            "SQL_MAX_ROW = 100000",
            "",
            "# Maximum number of rows displayed in SQL Lab UI",
            "# Is set to avoid out of memory/localstorage issues in browsers. Does not affect",
            "# exported CSVs",
            "DISPLAY_MAX_ROW = 10000",
            "",
            "# Default row limit for SQL Lab queries. Is overridden by setting a new limit in",
            "# the SQL Lab UI",
            "DEFAULT_SQLLAB_LIMIT = 1000",
            "",
            "# The limit for the Superset Meta DB when the feature flag ENABLE_SUPERSET_META_DB is on",
            "SUPERSET_META_DB_LIMIT: int | None = 1000",
            "",
            "# Adds a warning message on sqllab save query and schedule query modals.",
            "SQLLAB_SAVE_WARNING_MESSAGE = None",
            "SQLLAB_SCHEDULE_WARNING_MESSAGE = None",
            "",
            "# Force refresh while auto-refresh in dashboard",
            "DASHBOARD_AUTO_REFRESH_MODE: Literal[\"fetch\", \"force\"] = \"force\"",
            "# Dashboard auto refresh intervals",
            "DASHBOARD_AUTO_REFRESH_INTERVALS = [",
            "    [0, \"Don't refresh\"],",
            "    [10, \"10 seconds\"],",
            "    [30, \"30 seconds\"],",
            "    [60, \"1 minute\"],",
            "    [300, \"5 minutes\"],",
            "    [1800, \"30 minutes\"],",
            "    [3600, \"1 hour\"],",
            "    [21600, \"6 hours\"],",
            "    [43200, \"12 hours\"],",
            "    [86400, \"24 hours\"],",
            "]",
            "",
            "# This is used as a workaround for the alerts & reports scheduler task to get the time",
            "# celery beat triggered it, see https://github.com/celery/celery/issues/6974 for details",
            "CELERY_BEAT_SCHEDULER_EXPIRES = timedelta(weeks=1)",
            "",
            "# Default celery config is to use SQLA as a broker, in a production setting",
            "# you'll want to use a proper broker as specified here:",
            "# https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/index.html",
            "",
            "",
            "class CeleryConfig:  # pylint: disable=too-few-public-methods",
            "    broker_url = \"sqla+sqlite:///celerydb.sqlite\"",
            "    imports = (\"superset.sql_lab\", \"superset.tasks.scheduler\")",
            "    result_backend = \"db+sqlite:///celery_results.sqlite\"",
            "    worker_prefetch_multiplier = 1",
            "    task_acks_late = False",
            "    task_annotations = {",
            "        \"sql_lab.get_sql_results\": {",
            "            \"rate_limit\": \"100/s\",",
            "        },",
            "    }",
            "    beat_schedule = {",
            "        \"reports.scheduler\": {",
            "            \"task\": \"reports.scheduler\",",
            "            \"schedule\": crontab(minute=\"*\", hour=\"*\"),",
            "            \"options\": {\"expires\": int(CELERY_BEAT_SCHEDULER_EXPIRES.total_seconds())},",
            "        },",
            "        \"reports.prune_log\": {",
            "            \"task\": \"reports.prune_log\",",
            "            \"schedule\": crontab(minute=0, hour=0),",
            "        },",
            "    }",
            "",
            "",
            "CELERY_CONFIG = CeleryConfig  # pylint: disable=invalid-name",
            "",
            "# Set celery config to None to disable all the above configuration",
            "# CELERY_CONFIG = None",
            "",
            "# Additional static HTTP headers to be served by your Superset server. Note",
            "# Flask-Talisman applies the relevant security HTTP headers.",
            "#",
            "# DEFAULT_HTTP_HEADERS: sets default values for HTTP headers. These may be overridden",
            "# within the app",
            "# OVERRIDE_HTTP_HEADERS: sets override values for HTTP headers. These values will",
            "# override anything set within the app",
            "DEFAULT_HTTP_HEADERS: dict[str, Any] = {}",
            "OVERRIDE_HTTP_HEADERS: dict[str, Any] = {}",
            "HTTP_HEADERS: dict[str, Any] = {}",
            "",
            "# The db id here results in selecting this one as a default in SQL Lab",
            "DEFAULT_DB_ID = None",
            "",
            "# Timeout duration for SQL Lab synchronous queries",
            "SQLLAB_TIMEOUT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Timeout duration for SQL Lab query validation",
            "SQLLAB_VALIDATION_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# SQLLAB_DEFAULT_DBID",
            "SQLLAB_DEFAULT_DBID = None",
            "",
            "# The MAX duration a query can run for before being killed by celery.",
            "SQLLAB_ASYNC_TIME_LIMIT_SEC = int(timedelta(hours=6).total_seconds())",
            "",
            "# Some databases support running EXPLAIN queries that allow users to estimate",
            "# query costs before they run. These EXPLAIN queries should have a small",
            "# timeout.",
            "SQLLAB_QUERY_COST_ESTIMATE_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# The cost returned by the databases is a relative value; in order to map the cost to",
            "# a tangible value you need to define a custom formatter that takes into consideration",
            "# your specific infrastructure. For example, you could analyze queries a posteriori by",
            "# running EXPLAIN on them, and compute a histogram of relative costs to present the",
            "# cost as a percentile, this step is optional as every db engine spec has its own",
            "# query cost formatter, but it you wanna customize it you can define it inside the config:",
            "",
            "# def postgres_query_cost_formatter(",
            "#     result: List[Dict[str, Any]]",
            "# ) -> List[Dict[str, str]]:",
            "#     # 25, 50, 75% percentiles",
            "#     percentile_costs = [100.0, 1000.0, 10000.0]",
            "#",
            "#     out = []",
            "#     for row in result:",
            "#         relative_cost = row[\"Total cost\"]",
            "#         percentile = bisect.bisect_left(percentile_costs, relative_cost) + 1",
            "#         out.append({",
            "#             \"Relative cost\": relative_cost,",
            "#             \"Percentile\": str(percentile * 25) + \"%\",",
            "#         })",
            "#",
            "#     return out",
            "#",
            "# QUERY_COST_FORMATTERS_BY_ENGINE: {\"postgresql\": postgres_query_cost_formatter}",
            "QUERY_COST_FORMATTERS_BY_ENGINE: dict[",
            "    str, Callable[[list[dict[str, Any]]], list[dict[str, Any]]]",
            "] = {}",
            "",
            "# Flag that controls if limit should be enforced on the CTA (create table as queries).",
            "SQLLAB_CTAS_NO_LIMIT = False",
            "",
            "# This allows you to define custom logic around the \"CREATE TABLE AS\" or CTAS feature",
            "# in SQL Lab that defines where the target schema should be for a given user.",
            "# Database `CTAS Schema` has a precedence over this setting.",
            "# Example below returns a username and CTA queries will write tables into the schema",
            "# name `username`",
            "# SQLLAB_CTAS_SCHEMA_NAME_FUNC = lambda database, user, schema, sql: user.username",
            "# This is move involved example where depending on the database you can leverage data",
            "# available to assign schema for the CTA query:",
            "# def compute_schema_name(database: Database, user: User, schema: str, sql: str) -> str:",
            "#     if database.name == 'mysql_payments_slave':",
            "#         return 'tmp_superset_schema'",
            "#     if database.name == 'presto_gold':",
            "#         return user.username",
            "#     if database.name == 'analytics':",
            "#         if 'analytics' in [r.name for r in user.roles]:",
            "#             return 'analytics_cta'",
            "#         else:",
            "#             return f'tmp_{schema}'",
            "# Function accepts database object, user object, schema name and sql that will be run.",
            "SQLLAB_CTAS_SCHEMA_NAME_FUNC: None | (",
            "    Callable[[Database, models.User, str, str], str]",
            ") = None",
            "",
            "# If enabled, it can be used to store the results of long-running queries",
            "# in SQL Lab by using the \"Run Async\" button/feature",
            "RESULTS_BACKEND: BaseCache | None = None",
            "",
            "# Use PyArrow and MessagePack for async query results serialization,",
            "# rather than JSON. This feature requires additional testing from the",
            "# community before it is fully adopted, so this config option is provided",
            "# in order to disable should breaking issues be discovered.",
            "RESULTS_BACKEND_USE_MSGPACK = True",
            "",
            "# The S3 bucket where you want to store your external hive tables created",
            "# from CSV files. For example, 'companyname-superset'",
            "CSV_TO_HIVE_UPLOAD_S3_BUCKET = None",
            "",
            "# The directory within the bucket specified above that will",
            "# contain all the external tables",
            "CSV_TO_HIVE_UPLOAD_DIRECTORY = \"EXTERNAL_HIVE_TABLES/\"",
            "",
            "",
            "# Function that creates upload directory dynamically based on the",
            "# database used, user and schema provided.",
            "def CSV_TO_HIVE_UPLOAD_DIRECTORY_FUNC(  # pylint: disable=invalid-name",
            "    database: Database,",
            "    user: models.User,  # pylint: disable=unused-argument",
            "    schema: str | None,",
            ") -> str:",
            "    # Note the final empty path enforces a trailing slash.",
            "    return os.path.join(",
            "        CSV_TO_HIVE_UPLOAD_DIRECTORY, str(database.id), schema or \"\", \"\"",
            "    )",
            "",
            "",
            "# The namespace within hive where the tables created from",
            "# uploading CSVs will be stored.",
            "UPLOADED_CSV_HIVE_NAMESPACE: str | None = None",
            "",
            "# Function that computes the allowed schemas for the CSV uploads.",
            "# Allowed schemas will be a union of schemas_allowed_for_file_upload",
            "# db configuration and a result of this function.",
            "",
            "# mypy doesn't catch that if case ensures list content being always str",
            "ALLOWED_USER_CSV_SCHEMA_FUNC: Callable[[Database, models.User], list[str]] = (",
            "    lambda database, user: [UPLOADED_CSV_HIVE_NAMESPACE]",
            "    if UPLOADED_CSV_HIVE_NAMESPACE",
            "    else []",
            ")",
            "",
            "# Values that should be treated as nulls for the csv uploads.",
            "CSV_DEFAULT_NA_NAMES = list(STR_NA_VALUES)",
            "",
            "# A dictionary of items that gets merged into the Jinja context for",
            "# SQL Lab. The existing context gets updated with this dictionary,",
            "# meaning values for existing keys get overwritten by the content of this",
            "# dictionary. Exposing functionality through JINJA_CONTEXT_ADDONS has security",
            "# implications as it opens a window for a user to execute untrusted code.",
            "# It's important to make sure that the objects exposed (as well as objects attached",
            "# to those objects) are harmless. We recommend only exposing simple/pure functions that",
            "# return native types.",
            "JINJA_CONTEXT_ADDONS: dict[str, Callable[..., Any]] = {}",
            "",
            "# A dictionary of macro template processors (by engine) that gets merged into global",
            "# template processors. The existing template processors get updated with this",
            "# dictionary, which means the existing keys get overwritten by the content of this",
            "# dictionary. The customized addons don't necessarily need to use Jinja templating",
            "# language. This allows you to define custom logic to process templates on a per-engine",
            "# basis. Example value = `{\"presto\": CustomPrestoTemplateProcessor}`",
            "CUSTOM_TEMPLATE_PROCESSORS: dict[str, type[BaseTemplateProcessor]] = {}",
            "",
            "# Roles that are controlled by the API / Superset and should not be changed",
            "# by humans.",
            "ROBOT_PERMISSION_ROLES = [\"Public\", \"Gamma\", \"Alpha\", \"Admin\", \"sql_lab\"]",
            "",
            "CONFIG_PATH_ENV_VAR = \"SUPERSET_CONFIG_PATH\"",
            "",
            "# If a callable is specified, it will be called at app startup while passing",
            "# a reference to the Flask app. This can be used to alter the Flask app",
            "# in whatever way.",
            "# example: FLASK_APP_MUTATOR = lambda x: x.before_request = f",
            "FLASK_APP_MUTATOR = None",
            "",
            "# smtp server configuration",
            "EMAIL_NOTIFICATIONS = False  # all the emails are sent using dryrun",
            "SMTP_HOST = \"localhost\"",
            "SMTP_STARTTLS = True",
            "SMTP_SSL = False",
            "SMTP_USER = \"superset\"",
            "SMTP_PORT = 25",
            "SMTP_PASSWORD = \"superset\"",
            "SMTP_MAIL_FROM = \"superset@superset.com\"",
            "# If True creates a default SSL context with ssl.Purpose.CLIENT_AUTH using the",
            "# default system root CA certificates.",
            "SMTP_SSL_SERVER_AUTH = False",
            "ENABLE_CHUNK_ENCODING = False",
            "",
            "# Whether to bump the logging level to ERROR on the flask_appbuilder package",
            "# Set to False if/when debugging FAB related issues like",
            "# permission management",
            "SILENCE_FAB = True",
            "",
            "FAB_ADD_SECURITY_VIEWS = True",
            "FAB_ADD_SECURITY_PERMISSION_VIEW = False",
            "FAB_ADD_SECURITY_VIEW_MENU_VIEW = False",
            "FAB_ADD_SECURITY_PERMISSION_VIEWS_VIEW = False",
            "",
            "# The link to a page containing common errors and their resolutions",
            "# It will be appended at the bottom of sql_lab errors.",
            "TROUBLESHOOTING_LINK = \"\"",
            "",
            "# CSRF token timeout, set to None for a token that never expires",
            "WTF_CSRF_TIME_LIMIT = int(timedelta(weeks=1).total_seconds())",
            "",
            "# This link should lead to a page with instructions on how to gain access to a",
            "# Datasource. It will be placed at the bottom of permissions errors.",
            "PERMISSION_INSTRUCTIONS_LINK = \"\"",
            "",
            "# Integrate external Blueprints to the app by passing them to your",
            "# configuration. These blueprints will get integrated in the app",
            "BLUEPRINTS: list[Blueprint] = []",
            "",
            "# Provide a callable that receives a tracking_url and returns another",
            "# URL. This is used to translate internal Hadoop job tracker URL",
            "# into a proxied one",
            "",
            "",
            "# Transform SQL query tracking url for Hive and Presto engines. You may also",
            "# access information about the query itself by adding a second parameter",
            "# to your transformer function, e.g.:",
            "#   TRACKING_URL_TRANSFORMER = (",
            "#       lambda url, query: url if is_fresh(query) else None",
            "#   )",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "TRACKING_URL_TRANSFORMER = lambda url: url",
            "",
            "",
            "# customize the polling time of each engine",
            "DB_POLL_INTERVAL_SECONDS: dict[str, int] = {}",
            "",
            "# Interval between consecutive polls when using Presto Engine",
            "# See here: https://github.com/dropbox/PyHive/blob/8eb0aeab8ca300f3024655419b93dad926c1a351/pyhive/presto.py#L93  # pylint: disable=line-too-long,useless-suppression",
            "PRESTO_POLL_INTERVAL = int(timedelta(seconds=1).total_seconds())",
            "",
            "# Allow list of custom authentications for each DB engine.",
            "# Example:",
            "# from your.module import AuthClass",
            "# from another.extra import auth_method",
            "#",
            "# ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {",
            "#     \"trino\": {",
            "#         \"custom_auth\": AuthClass,",
            "#         \"another_auth_method\": auth_method,",
            "#     },",
            "# }",
            "ALLOWED_EXTRA_AUTHENTICATIONS: dict[str, dict[str, Callable[..., Any]]] = {}",
            "",
            "# The id of a template dashboard that should be copied to every new user",
            "DASHBOARD_TEMPLATE_ID = None",
            "",
            "# A callable that allows altering the database connection URL and params",
            "# on the fly, at runtime. This allows for things like impersonation or",
            "# arbitrary logic. For instance you can wire different users to",
            "# use different connection parameters, or pass their email address as the",
            "# username. The function receives the connection uri object, connection",
            "# params, the username, and returns the mutated uri and params objects.",
            "# Example:",
            "#   def DB_CONNECTION_MUTATOR(uri, params, username, security_manager, source):",
            "#       user = security_manager.find_user(username=username)",
            "#       if user and user.email:",
            "#           uri.username = user.email",
            "#       return uri, params",
            "#",
            "# Note that the returned uri and params are passed directly to sqlalchemy's",
            "# as such `create_engine(url, **params)`",
            "DB_CONNECTION_MUTATOR = None",
            "",
            "# A set of disallowed SQL functions per engine. This is used to restrict the use of",
            "# unsafe SQL functions in SQL Lab and Charts. The keys of the dictionary are the engine",
            "# names, and the values are sets of disallowed functions.",
            "DISALLOWED_SQL_FUNCTIONS: dict[str, set[str]] = {",
            "    \"postgresql\": {\"version\", \"query_to_xml\", \"inet_server_addr\", \"inet_client_addr\"},",
            "    \"clickhouse\": {\"url\"},",
            "    \"mysql\": {\"version\"},",
            "}",
            "",
            "",
            "# A function that intercepts the SQL to be executed and can alter it.",
            "# The use case is can be around adding some sort of comment header",
            "# with information such as the username and worker node information",
            "#",
            "#    def SQL_QUERY_MUTATOR(",
            "#        sql,",
            "#        security_manager=security_manager,",
            "#        database=database,",
            "#    ):",
            "#        dttm = datetime.now().isoformat()",
            "#        return f\"-- [SQL LAB] {user_name} {dttm}\\n{sql}\"",
            "# For backward compatibility, you can unpack any of the above arguments in your",
            "# function definition, but keep the **kwargs as the last argument to allow new args",
            "# to be added later without any errors.",
            "def SQL_QUERY_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    sql: str, **kwargs: Any",
            ") -> str:",
            "    return sql",
            "",
            "",
            "# A variable that chooses whether to apply the SQL_QUERY_MUTATOR before or after splitting the input query",
            "# It allows for using the SQL_QUERY_MUTATOR function for more than comments",
            "# Usage: If you want to apply a change to every statement to a given query, set MUTATE_AFTER_SPLIT = True",
            "# An example use case is if data has role based access controls, and you want to apply",
            "# a SET ROLE statement alongside every user query. Changing this variable maintains",
            "# functionality for both the SQL_Lab and Charts.",
            "MUTATE_AFTER_SPLIT = False",
            "",
            "",
            "# This allows for a user to add header data to any outgoing emails. For example,",
            "# if you need to include metadata in the header or you want to change the specifications",
            "# of the email title, header, or sender.",
            "def EMAIL_HEADER_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    msg: MIMEMultipart, **kwargs: Any",
            ") -> MIMEMultipart:",
            "    return msg",
            "",
            "",
            "# Define a list of usernames to be excluded from all dropdown lists of users",
            "# Owners, filters for created_by, etc.",
            "# The users can also be excluded by overriding the get_exclude_users_from_lists method",
            "# in security manager",
            "EXCLUDE_USERS_FROM_LISTS: list[str] | None = None",
            "",
            "# For database connections, this dictionary will remove engines from the available",
            "# list/dropdown if you do not want these dbs to show as available.",
            "# The available list is generated by driver installed, and some engines have multiple",
            "# drivers.",
            "# e.g., DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {\"databricks\": {\"pyhive\", \"pyodbc\"}}",
            "DBS_AVAILABLE_DENYLIST: dict[str, set[str]] = {}",
            "",
            "# This auth provider is used by background (offline) tasks that need to access",
            "# protected resources. Can be overridden by end users in order to support",
            "# custom auth mechanisms",
            "MACHINE_AUTH_PROVIDER_CLASS = \"superset.utils.machine_auth.MachineAuthProvider\"",
            "",
            "# ---------------------------------------------------",
            "# Alerts & Reports",
            "# ---------------------------------------------------",
            "# Used for Alerts/Reports (Feature flask ALERT_REPORTS) to set the size for the",
            "# sliding cron window size, should be synced with the celery beat config minus 1 second",
            "ALERT_REPORTS_CRON_WINDOW_SIZE = 59",
            "ALERT_REPORTS_WORKING_TIME_OUT_KILL = True",
            "# Which user to attempt to execute Alerts/Reports as. By default,",
            "# execute as the primary owner of the alert/report (giving priority to the last",
            "# modifier and then the creator if either is contained within the list of owners,",
            "# otherwise the first owner will be used).",
            "#",
            "# To first try to execute as the creator in the owners list (if present), then fall",
            "# back to the creator, then the last modifier in the owners list (if present), then the",
            "# last modifier, then an owner and finally `THUMBNAIL_SELENIUM_USER`, set as follows:",
            "# ALERT_REPORTS_EXECUTE_AS = [",
            "#     ExecutorType.CREATOR_OWNER,",
            "#     ExecutorType.CREATOR,",
            "#     ExecutorType.MODIFIER_OWNER,",
            "#     ExecutorType.MODIFIER,",
            "#     ExecutorType.OWNER,",
            "#     ExecutorType.SELENIUM,",
            "# ]",
            "ALERT_REPORTS_EXECUTE_AS: list[ExecutorType] = [ExecutorType.OWNER]",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_TIME_OUT_LAG = int(timedelta(seconds=10).total_seconds())",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG = int(timedelta(seconds=1).total_seconds())",
            "# Default values that user using when creating alert",
            "ALERT_REPORTS_DEFAULT_WORKING_TIMEOUT = 3600",
            "ALERT_REPORTS_DEFAULT_RETENTION = 90",
            "ALERT_REPORTS_DEFAULT_CRON_VALUE = \"0 0 * * *\"  # every day",
            "# If set to true no notification is sent, the worker will just log a message.",
            "# Useful for debugging",
            "ALERT_REPORTS_NOTIFICATION_DRY_RUN = False",
            "# Max tries to run queries to prevent false errors caused by transient errors",
            "# being returned to users. Set to a value >1 to enable retries.",
            "ALERT_REPORTS_QUERY_EXECUTION_MAX_TRIES = 1",
            "# Custom width for screenshots",
            "ALERT_REPORTS_MIN_CUSTOM_SCREENSHOT_WIDTH = 600",
            "ALERT_REPORTS_MAX_CUSTOM_SCREENSHOT_WIDTH = 2400",
            "",
            "# A custom prefix to use on all Alerts & Reports emails",
            "EMAIL_REPORTS_SUBJECT_PREFIX = \"[Report] \"",
            "",
            "# The text for call-to-action link in Alerts & Reports emails",
            "EMAIL_REPORTS_CTA = \"Explore in Superset\"",
            "",
            "# Slack API token for the superset reports, either string or callable",
            "SLACK_API_TOKEN: Callable[[], str] | str | None = None",
            "SLACK_PROXY = None",
            "",
            "# The webdriver to use for generating reports. Use one of the following",
            "# firefox",
            "#   Requires: geckodriver and firefox installations",
            "#   Limitations: can be buggy at times",
            "# chrome:",
            "#   Requires: headless chrome",
            "#   Limitations: unable to generate screenshots of elements",
            "WEBDRIVER_TYPE = \"firefox\"",
            "",
            "# Window size - this will impact the rendering of the data",
            "WEBDRIVER_WINDOW = {",
            "    \"dashboard\": (1600, 2000),",
            "    \"slice\": (3000, 1200),",
            "    \"pixel_density\": 1,",
            "}",
            "",
            "# An optional override to the default auth hook used to provide auth to the offline",
            "# webdriver (when using Selenium) or browser context (when using Playwright - see",
            "# PLAYWRIGHT_REPORTS_AND_THUMBNAILS feature flag)",
            "WEBDRIVER_AUTH_FUNC = None",
            "",
            "# Any config options to be passed as-is to the webdriver",
            "WEBDRIVER_CONFIGURATION: dict[Any, Any] = {\"service_log_path\": \"/dev/null\"}",
            "",
            "# Additional args to be passed as arguments to the config object",
            "# Note: If using Chrome, you'll want to add the \"--marionette\" arg.",
            "WEBDRIVER_OPTION_ARGS = [\"--headless\"]",
            "",
            "# The base URL to query for accessing the user interface",
            "WEBDRIVER_BASEURL = \"http://0.0.0.0:8080/\"",
            "# The base URL for the email report hyperlinks.",
            "WEBDRIVER_BASEURL_USER_FRIENDLY = WEBDRIVER_BASEURL",
            "# Time selenium will wait for the page to load and render for the email report.",
            "EMAIL_PAGE_RENDER_WAIT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Send user to a link where they can report bugs",
            "BUG_REPORT_URL = None",
            "BUG_REPORT_TEXT = \"Report a bug\"",
            "BUG_REPORT_ICON = None  # Recommended size: 16x16",
            "",
            "# Send user to a link where they can read more about Superset",
            "DOCUMENTATION_URL = None",
            "DOCUMENTATION_TEXT = \"Documentation\"",
            "DOCUMENTATION_ICON = None  # Recommended size: 16x16",
            "",
            "# What is the Last N days relative in the time selector to:",
            "# 'today' means it is midnight (00:00:00) in the local timezone",
            "# 'now' means it is relative to the query issue time",
            "# If both start and end time is set to now, this will make the time",
            "# filter a moving window. By only setting the end time to now,",
            "# start time will be set to midnight, while end will be relative to",
            "# the query issue time.",
            "DEFAULT_RELATIVE_START_TIME = \"today\"",
            "DEFAULT_RELATIVE_END_TIME = \"today\"",
            "",
            "# Configure which SQL validator to use for each engine",
            "SQL_VALIDATORS_BY_ENGINE = {",
            "    \"presto\": \"PrestoDBSQLValidator\",",
            "    \"postgresql\": \"PostgreSQLValidator\",",
            "}",
            "",
            "# A list of preferred databases, in order. These databases will be",
            "# displayed prominently in the \"Add Database\" dialog. You should",
            "# use the \"engine_name\" attribute of the corresponding DB engine spec",
            "# in `superset/db_engine_specs/`.",
            "PREFERRED_DATABASES: list[str] = [",
            "    \"PostgreSQL\",",
            "    \"Presto\",",
            "    \"MySQL\",",
            "    \"SQLite\",",
            "    # etc.",
            "]",
            "# When adding a new database we try to connect to it. Depending on which parameters are",
            "# incorrect this could take a couple minutes, until the SQLAlchemy driver pinging the",
            "# database times out. Instead of relying on the driver timeout we can specify a shorter",
            "# one here.",
            "TEST_DATABASE_CONNECTION_TIMEOUT = timedelta(seconds=30)",
            "",
            "# Enable/disable CSP warning",
            "CONTENT_SECURITY_POLICY_WARNING = True",
            "",
            "# Do you want Talisman enabled?",
            "TALISMAN_ENABLED = utils.cast_to_boolean(os.environ.get(\"TALISMAN_ENABLED\", True))",
            "",
            "# If you want Talisman, how do you want it configured??",
            "TALISMAN_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"base-uri\": [\"'self'\"],",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [",
            "            \"'self'\",",
            "            \"blob:\",",
            "            \"data:\",",
            "            \"https://apachesuperset.gateway.scarf.sh\",",
            "            \"https://static.scarf.sh/\",",
            "        ],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'strict-dynamic'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "    \"session_cookie_secure\": False,",
            "}",
            "# React requires `eval` to work correctly in dev mode",
            "TALISMAN_DEV_CONFIG = {",
            "    \"content_security_policy\": {",
            "        \"base-uri\": [\"'self'\"],",
            "        \"default-src\": [\"'self'\"],",
            "        \"img-src\": [",
            "            \"'self'\",",
            "            \"blob:\",",
            "            \"data:\",",
            "            \"https://apachesuperset.gateway.scarf.sh\",",
            "            \"https://static.scarf.sh/\",",
            "        ],",
            "        \"worker-src\": [\"'self'\", \"blob:\"],",
            "        \"connect-src\": [",
            "            \"'self'\",",
            "            \"https://api.mapbox.com\",",
            "            \"https://events.mapbox.com\",",
            "        ],",
            "        \"object-src\": \"'none'\",",
            "        \"style-src\": [",
            "            \"'self'\",",
            "            \"'unsafe-inline'\",",
            "        ],",
            "        \"script-src\": [\"'self'\", \"'unsafe-inline'\", \"'unsafe-eval'\"],",
            "    },",
            "    \"content_security_policy_nonce_in\": [\"script-src\"],",
            "    \"force_https\": False,",
            "    \"session_cookie_secure\": False,",
            "}",
            "",
            "#",
            "# Flask session cookie options",
            "#",
            "# See https://flask.palletsprojects.com/en/1.1.x/security/#set-cookie-options",
            "# for details",
            "#",
            "SESSION_COOKIE_HTTPONLY = True  # Prevent cookie from being read by frontend JS?",
            "SESSION_COOKIE_SECURE = False  # Prevent cookie from being transmitted over non-tls?",
            "SESSION_COOKIE_SAMESITE: Literal[\"None\", \"Lax\", \"Strict\"] | None = \"Lax\"",
            "# Whether to use server side sessions from flask-session or Flask secure cookies",
            "SESSION_SERVER_SIDE = False",
            "# Example config using Redis as the backend for server side sessions",
            "# from flask_session import RedisSessionInterface",
            "#",
            "# SESSION_SERVER_SIDE = True",
            "# SESSION_USE_SIGNER = True",
            "# SESSION_TYPE = \"redis\"",
            "# SESSION_REDIS = Redis(host=\"localhost\", port=6379, db=0)",
            "#",
            "# Other possible config options and backends:",
            "# # https://flask-session.readthedocs.io/en/latest/config.html",
            "",
            "# Cache static resources.",
            "SEND_FILE_MAX_AGE_DEFAULT = int(timedelta(days=365).total_seconds())",
            "",
            "# URI to database storing the example data, points to",
            "# SQLALCHEMY_DATABASE_URI by default if set to `None`",
            "SQLALCHEMY_EXAMPLES_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"examples.db\")",
            "",
            "# Optional prefix to be added to all static asset paths when rendering the UI.",
            "# This is useful for hosting assets in an external CDN, for example",
            "STATIC_ASSETS_PREFIX = \"\"",
            "",
            "# Some sqlalchemy connection strings can open Superset to security risks.",
            "# Typically these should not be allowed.",
            "PREVENT_UNSAFE_DB_CONNECTIONS = True",
            "",
            "# If true all default urls on datasets will be handled as relative URLs by the frontend",
            "PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET = True",
            "",
            "# Define a list of allowed URLs for dataset data imports (v1).",
            "# Simple example to only allow URLs that belong to certain domains:",
            "# ALLOWED_IMPORT_URL_DOMAINS = [",
            "#     r\"^https://.+\\.domain1\\.com\\/?.*\", r\"^https://.+\\.domain2\\.com\\/?.*\"",
            "# ]",
            "DATASET_IMPORT_ALLOWED_DATA_URLS = [r\".*\"]",
            "",
            "# Path used to store SSL certificates that are generated when using custom certs.",
            "# Defaults to temporary directory.",
            "# Example: SSL_CERT_PATH = \"/certs\"",
            "SSL_CERT_PATH: str | None = None",
            "",
            "# SQLA table mutator, every time we fetch the metadata for a certain table",
            "# (superset.connectors.sqla.models.SqlaTable), we call this hook",
            "# to allow mutating the object with this callback.",
            "# This can be used to set any properties of the object based on naming",
            "# conventions and such. You can find examples in the tests.",
            "",
            "# pylint: disable-next=unnecessary-lambda-assignment",
            "SQLA_TABLE_MUTATOR = lambda table: table",
            "",
            "",
            "# Global async query config options.",
            "# Requires GLOBAL_ASYNC_QUERIES feature flag to be enabled.",
            "GLOBAL_ASYNC_QUERY_MANAGER_CLASS = (",
            "    \"superset.async_events.async_query_manager.AsyncQueryManager\"",
            ")",
            "GLOBAL_ASYNC_QUERIES_REDIS_CONFIG = {",
            "    \"port\": 6379,",
            "    \"host\": \"127.0.0.1\",",
            "    \"password\": \"\",",
            "    \"db\": 0,",
            "    \"ssl\": False,",
            "}",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_PREFIX = \"async-events-\"",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT = 1000",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT_FIREHOSE = 1000000",
            "GLOBAL_ASYNC_QUERIES_REGISTER_REQUEST_HANDLERS = True",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_NAME = \"async-token\"",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SECURE = False",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SAMESITE: None | (",
            "    Literal[\"None\", \"Lax\", \"Strict\"]",
            ") = None",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_DOMAIN = None",
            "GLOBAL_ASYNC_QUERIES_JWT_SECRET = \"test-secret-change-me\"",
            "GLOBAL_ASYNC_QUERIES_TRANSPORT: Literal[\"polling\", \"ws\"] = \"polling\"",
            "GLOBAL_ASYNC_QUERIES_POLLING_DELAY = int(",
            "    timedelta(milliseconds=500).total_seconds() * 1000",
            ")",
            "GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL = \"ws://127.0.0.1:8080/\"",
            "",
            "# Embedded config options",
            "GUEST_ROLE_NAME = \"Public\"",
            "GUEST_TOKEN_JWT_SECRET = \"test-guest-secret-change-me\"",
            "GUEST_TOKEN_JWT_ALGO = \"HS256\"",
            "GUEST_TOKEN_HEADER_NAME = \"X-GuestToken\"",
            "GUEST_TOKEN_JWT_EXP_SECONDS = 300  # 5 minutes",
            "# Guest token audience for the embedded superset, either string or callable",
            "GUEST_TOKEN_JWT_AUDIENCE: Callable[[], str] | str | None = None",
            "",
            "# A SQL dataset health check. Note if enabled it is strongly advised that the callable",
            "# be memoized to aid with performance, i.e.,",
            "#",
            "#    @cache_manager.cache.memoize(timeout=0)",
            "#    def DATASET_HEALTH_CHECK(datasource: SqlaTable) -> Optional[str]:",
            "#        if (",
            "#            datasource.sql and",
            "#            len(sql_parse.ParsedQuery(datasource.sql, strip_comments=True).tables) == 1",
            "#        ):",
            "#            return (",
            "#                \"This virtual dataset queries only one table and therefore could be \"",
            "#                \"replaced by querying the table directly.\"",
            "#            )",
            "#",
            "#        return None",
            "#",
            "# Within the FLASK_APP_MUTATOR callable, i.e., once the application and thus cache have",
            "# been initialized it is also necessary to add the following logic to blow the cache for",
            "# all datasources if the callback function changed.",
            "#",
            "#    def FLASK_APP_MUTATOR(app: Flask) -> None:",
            "#        name = \"DATASET_HEALTH_CHECK\"",
            "#        func = app.config[name]",
            "#        code = func.uncached.__code__.co_code",
            "#",
            "#        if cache_manager.cache.get(name) != code:",
            "#            cache_manager.cache.delete_memoized(func)",
            "#            cache_manager.cache.set(name, code, timeout=0)",
            "#",
            "DATASET_HEALTH_CHECK: Callable[[SqlaTable], str] | None = None",
            "",
            "# the advanced data type key should correspond to that set in the column metadata",
            "ADVANCED_DATA_TYPES: dict[str, AdvancedDataType] = {",
            "    \"internet_address\": internet_address,",
            "    \"port\": internet_port,",
            "}",
            "",
            "# By default, the Welcome page features all charts and dashboards the user has access",
            "# to. This can be changed to show only examples, or a custom view",
            "# by providing the title and a FAB filter:",
            "# WELCOME_PAGE_LAST_TAB = (",
            "#     \"Xyz\",",
            "#     [{\"col\": 'created_by', \"opr\": 'rel_o_m', \"value\": 10}],",
            "# )",
            "WELCOME_PAGE_LAST_TAB: (",
            "    Literal[\"examples\", \"all\"] | tuple[str, list[dict[str, Any]]]",
            ") = \"all\"",
            "",
            "# Max allowed size for a zipped file",
            "ZIPPED_FILE_MAX_SIZE = 100 * 1024 * 1024  # 100MB",
            "# Max allowed compression ratio for a zipped file",
            "ZIP_FILE_MAX_COMPRESS_RATIO = 200.0",
            "",
            "# Configuration for environment tag shown on the navbar. Setting 'text' to '' will hide the tag.",
            "# 'color' can either be a hex color code, or a dot-indexed theme color (e.g. error.base)",
            "ENVIRONMENT_TAG_CONFIG = {",
            "    \"variable\": \"SUPERSET_ENV\",",
            "    \"values\": {",
            "        \"debug\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"flask-debug\",",
            "        },",
            "        \"development\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"Development\",",
            "        },",
            "        \"production\": {",
            "            \"color\": \"\",",
            "            \"text\": \"\",",
            "        },",
            "    },",
            "}",
            "",
            "",
            "# Extra related query filters make it possible to limit which objects are shown",
            "# in the UI. For examples, to only show \"admin\" or users starting with the letter \"b\" in",
            "# the \"Owners\" dropdowns, you could add the following in your config:",
            "# def user_filter(query: Query, *args, *kwargs):",
            "#     from superset import security_manager",
            "#",
            "#     user_model = security_manager.user_model",
            "#     filters = [",
            "#         user_model.username == \"admin\",",
            "#         user_model.username.ilike(\"b%\"),",
            "#     ]",
            "#     return query.filter(or_(*filters))",
            "#",
            "#  EXTRA_RELATED_QUERY_FILTERS = {\"user\": user_filter}",
            "#",
            "# Similarly, to restrict the roles in the \"Roles\" dropdown you can provide a custom",
            "# filter callback for the \"role\" key.",
            "class ExtraRelatedQueryFilters(TypedDict, total=False):",
            "    role: Callable[[Query], Query]",
            "    user: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_RELATED_QUERY_FILTERS: ExtraRelatedQueryFilters = {}",
            "",
            "",
            "# Extra dynamic query filters make it possible to limit which objects are shown",
            "# in the UI before any other filtering is applied. Useful for example when",
            "# considering to filter using Feature Flags along with regular role filters",
            "# that get applied by default in our base_filters.",
            "# For example, to only show a database starting with the letter \"b\"",
            "# in the \"Database Connections\" list, you could add the following in your config:",
            "# def initial_database_filter(query: Query, *args, *kwargs):",
            "#     from superset.models.core import Database",
            "#",
            "#     filter = Database.database_name.startswith('b')",
            "#     return query.filter(filter)",
            "#",
            "#  EXTRA_DYNAMIC_QUERY_FILTERS = {\"database\": initial_database_filter}",
            "class ExtraDynamicQueryFilters(TypedDict, total=False):",
            "    databases: Callable[[Query], Query]",
            "",
            "",
            "EXTRA_DYNAMIC_QUERY_FILTERS: ExtraDynamicQueryFilters = {}",
            "",
            "",
            "# -------------------------------------------------------------------",
            "# *                WARNING:  STOP EDITING  HERE                    *",
            "# -------------------------------------------------------------------",
            "# Don't add config values below this line since local configs won't be",
            "# able to override them.",
            "if CONFIG_PATH_ENV_VAR in os.environ:",
            "    # Explicitly import config module that is not necessarily in pythonpath; useful",
            "    # for case where app is being executed via pex.",
            "    cfg_path = os.environ[CONFIG_PATH_ENV_VAR]",
            "    try:",
            "        module = sys.modules[__name__]",
            "        override_conf = imp.load_source(\"superset_config\", cfg_path)",
            "        for key in dir(override_conf):",
            "            if key.isupper():",
            "                setattr(module, key, getattr(override_conf, key))",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{cfg_path}]\")",
            "    except Exception:",
            "        logger.exception(",
            "            \"Failed to import config for %s=%s\", CONFIG_PATH_ENV_VAR, cfg_path",
            "        )",
            "        raise",
            "elif importlib.util.find_spec(\"superset_config\") and not is_test():",
            "    try:",
            "        # pylint: disable=import-error,wildcard-import,unused-wildcard-import",
            "        import superset_config",
            "        from superset_config import *  # type: ignore",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{superset_config.__file__}]\")",
            "    except Exception:",
            "        logger.exception(\"Found but failed to import local superset_config\")",
            "        raise"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "superset/db_engine_specs/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 59,
                "PatchRowcode": " from superset.constants import TimeGrain as TimeGrainConstants"
            },
            "1": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 60,
                "PatchRowcode": " from superset.databases.utils import make_url_safe"
            },
            "2": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " from superset.errors import ErrorLevel, SupersetError, SupersetErrorType"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+from superset.exceptions import DisallowedSQLFunction"
            },
            "4": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 63,
                "PatchRowcode": " from superset.sql_parse import ParsedQuery, Table"
            },
            "5": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " from superset.superset_typing import ResultSetColumnType, SQLAColumnType"
            },
            "6": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 65,
                "PatchRowcode": " from superset.utils import core as utils"
            },
            "7": {
                "beforePatchRowNumber": 1584,
                "afterPatchRowNumber": 1585,
                "PatchRowcode": "         \"\"\""
            },
            "8": {
                "beforePatchRowNumber": 1585,
                "afterPatchRowNumber": 1586,
                "PatchRowcode": "         if not cls.allows_sql_comments:"
            },
            "9": {
                "beforePatchRowNumber": 1586,
                "afterPatchRowNumber": 1587,
                "PatchRowcode": "             query = sql_parse.strip_comments_from_sql(query, engine=cls.engine)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1588,
                "PatchRowcode": "+        disallowed_functions = current_app.config[\"DISALLOWED_SQL_FUNCTIONS\"].get("
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1589,
                "PatchRowcode": "+            cls.engine, set()"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1590,
                "PatchRowcode": "+        )"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1591,
                "PatchRowcode": "+        if sql_parse.check_sql_functions_exist(query, disallowed_functions, cls.engine):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1592,
                "PatchRowcode": "+            raise DisallowedSQLFunction(disallowed_functions)"
            },
            "15": {
                "beforePatchRowNumber": 1587,
                "afterPatchRowNumber": 1593,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 1588,
                "afterPatchRowNumber": 1594,
                "PatchRowcode": "         if cls.arraysize:"
            },
            "17": {
                "beforePatchRowNumber": 1589,
                "afterPatchRowNumber": 1595,
                "PatchRowcode": "             cursor.arraysize = cls.arraysize"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Match, Pattern",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    cast,",
            "    ContextManager,",
            "    NamedTuple,",
            "    TYPE_CHECKING,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "import pandas as pd",
            "import sqlparse",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask import current_app",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from marshmallow import fields, Schema",
            "from marshmallow.validate import Range",
            "from sqlalchemy import column, select, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.interfaces import Compiled, Dialect",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.ext.compiler import compiles",
            "from sqlalchemy.sql import literal_column, quoted_name, text",
            "from sqlalchemy.sql.expression import ColumnClause, Select, TextAsFrom, TextClause",
            "from sqlalchemy.types import TypeEngine",
            "from sqlparse.tokens import CTE",
            "",
            "from superset import security_manager, sql_parse",
            "from superset.constants import TimeGrain as TimeGrainConstants",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.sql_parse import ParsedQuery, Table",
            "from superset.superset_typing import ResultSetColumnType, SQLAColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.core import ColumnSpec, GenericDataType",
            "from superset.utils.hashing import md5_sha_from_str",
            "from superset.utils.network import is_hostname_valid, is_port_open",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.models.core import Database",
            "    from superset.models.sql_lab import Query",
            "",
            "ColumnTypeMapping = tuple[",
            "    Pattern[str],",
            "    Union[TypeEngine, Callable[[Match[str]], TypeEngine]],",
            "    GenericDataType,",
            "]",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "def convert_inspector_columns(cols: list[SQLAColumnType]) -> list[ResultSetColumnType]:",
            "    result_set_columns: list[ResultSetColumnType] = []",
            "    for col in cols:",
            "        result_set_columns.append({\"column_name\": col.get(\"name\"), **col})  # type: ignore",
            "    return result_set_columns",
            "",
            "",
            "class TimeGrain(NamedTuple):",
            "    name: str  # TODO: redundant field, remove",
            "    label: str",
            "    function: str",
            "    duration: str | None",
            "",
            "",
            "builtin_time_grains: dict[str | None, str] = {",
            "    TimeGrainConstants.SECOND: __(\"Second\"),",
            "    TimeGrainConstants.FIVE_SECONDS: __(\"5 second\"),",
            "    TimeGrainConstants.THIRTY_SECONDS: __(\"30 second\"),",
            "    TimeGrainConstants.MINUTE: __(\"Minute\"),",
            "    TimeGrainConstants.FIVE_MINUTES: __(\"5 minute\"),",
            "    TimeGrainConstants.TEN_MINUTES: __(\"10 minute\"),",
            "    TimeGrainConstants.FIFTEEN_MINUTES: __(\"15 minute\"),",
            "    TimeGrainConstants.THIRTY_MINUTES: __(\"30 minute\"),",
            "    TimeGrainConstants.HOUR: __(\"Hour\"),",
            "    TimeGrainConstants.SIX_HOURS: __(\"6 hour\"),",
            "    TimeGrainConstants.DAY: __(\"Day\"),",
            "    TimeGrainConstants.WEEK: __(\"Week\"),",
            "    TimeGrainConstants.MONTH: __(\"Month\"),",
            "    TimeGrainConstants.QUARTER: __(\"Quarter\"),",
            "    TimeGrainConstants.YEAR: __(\"Year\"),",
            "    TimeGrainConstants.WEEK_STARTING_SUNDAY: __(\"Week starting Sunday\"),",
            "    TimeGrainConstants.WEEK_STARTING_MONDAY: __(\"Week starting Monday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SATURDAY: __(\"Week ending Saturday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SUNDAY: __(\"Week ending Sunday\"),",
            "}",
            "",
            "",
            "class TimestampExpression(",
            "    ColumnClause",
            "):  # pylint: disable=abstract-method, too-many-ancestors",
            "    def __init__(self, expr: str, col: ColumnClause, **kwargs: Any) -> None:",
            "        \"\"\"Sqlalchemy class that can be used to render native column elements respecting",
            "        engine-specific quoting rules as part of a string-based expression.",
            "",
            "        :param expr: Sql expression with '{col}' denoting the locations where the col",
            "        object will be rendered.",
            "        :param col: the target column",
            "        \"\"\"",
            "        super().__init__(expr, **kwargs)",
            "        self.col = col",
            "",
            "    @property",
            "    def _constructor(self) -> ColumnClause:",
            "        # Needed to ensure that the column label is rendered correctly when",
            "        # proxied to the outer query.",
            "        # See https://github.com/sqlalchemy/sqlalchemy/issues/4730",
            "        return ColumnClause",
            "",
            "",
            "@compiles(TimestampExpression)",
            "def compile_timegrain_expression(",
            "    element: TimestampExpression, compiler: Compiled, **kwargs: Any",
            ") -> str:",
            "    return element.name.replace(\"{col}\", compiler.process(element.col, **kwargs))",
            "",
            "",
            "class LimitMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"Enum the ways that limits can be applied\"\"\"",
            "",
            "    FETCH_MANY = \"fetch_many\"",
            "    WRAP_SQL = \"wrap_sql\"",
            "    FORCE_LIMIT = \"force_limit\"",
            "",
            "",
            "class MetricType(TypedDict, total=False):",
            "    \"\"\"",
            "    Type for metrics return by `get_metrics`.",
            "    \"\"\"",
            "",
            "    metric_name: str",
            "    expression: str",
            "    verbose_name: str | None",
            "    metric_type: str | None",
            "    description: str | None",
            "    d3format: str | None",
            "    currency: str | None",
            "    warning_text: str | None",
            "    extra: str | None",
            "",
            "",
            "class BaseEngineSpec:  # pylint: disable=too-many-public-methods",
            "    \"\"\"Abstract class for database engine specific configurations",
            "",
            "    Attributes:",
            "        allows_alias_to_source_column: Whether the engine is able to pick the",
            "                                       source column for aggregation clauses",
            "                                       used in ORDER BY when a column in SELECT",
            "                                       has an alias that is the same as a source",
            "                                       column.",
            "        allows_hidden_orderby_agg:     Whether the engine allows ORDER BY to",
            "                                       directly use aggregation clauses, without",
            "                                       having to add the same aggregation in SELECT.",
            "    \"\"\"",
            "",
            "    engine_name: str | None = None  # for user messages, overridden in child classes",
            "",
            "    # These attributes map the DB engine spec to one or more SQLAlchemy dialects/drivers;",
            "    # see the ``supports_url`` and ``supports_backend`` methods below.",
            "    engine = \"base\"  # str as defined in sqlalchemy.engine.engine",
            "    engine_aliases: set[str] = set()",
            "    drivers: dict[str, str] = {}",
            "    default_driver: str | None = None",
            "",
            "    # placeholder with the SQLAlchemy URI template",
            "    sqlalchemy_uri_placeholder = (",
            "        \"engine+driver://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "",
            "    disable_ssh_tunneling = False",
            "",
            "    _date_trunc_functions: dict[str, str] = {}",
            "    _time_grain_expressions: dict[str | None, str] = {}",
            "    _default_column_type_mappings: tuple[ColumnTypeMapping, ...] = (",
            "        (",
            "            re.compile(r\"^string\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^n((var)?char|text)\", re.IGNORECASE),",
            "            types.UnicodeText(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(var)?char\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(tiny|medium|long)?text\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallint\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^int(eger)?\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigint\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^long\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^decimal\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^numeric\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^float\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^double\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^real\", re.IGNORECASE),",
            "            types.REAL,",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallserial\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^serial\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigserial\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^money\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^timestamp\", re.IGNORECASE),",
            "            types.TIMESTAMP(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^datetime\", re.IGNORECASE),",
            "            types.DateTime(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^date\", re.IGNORECASE),",
            "            types.Date(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^time\", re.IGNORECASE),",
            "            types.Time(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^interval\", re.IGNORECASE),",
            "            types.Interval(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^bool(ean)?\", re.IGNORECASE),",
            "            types.Boolean(),",
            "            GenericDataType.BOOLEAN,",
            "        ),",
            "    )",
            "    # engine-specific type mappings to check prior to the defaults",
            "    column_type_mappings: tuple[ColumnTypeMapping, ...] = ()",
            "",
            "    # type-specific functions to mutate values received from the database.",
            "    # Needed on certain databases that return values in an unexpected format",
            "    column_type_mutators: dict[TypeEngine, Callable[[Any], Any]] = {}",
            "",
            "    # Does database support join-free timeslot grouping",
            "    time_groupby_inline = False",
            "    limit_method = LimitMethod.FORCE_LIMIT",
            "    allows_joins = True",
            "    allows_subqueries = True",
            "    allows_alias_in_select = True",
            "    allows_alias_in_orderby = True",
            "    allows_sql_comments = True",
            "    allows_escaped_colons = True",
            "",
            "    # Whether ORDER BY clause can use aliases created in SELECT",
            "    # that are the same as a source column",
            "    allows_alias_to_source_column = True",
            "",
            "    # Whether ORDER BY clause must appear in SELECT",
            "    # if True, then it doesn't have to.",
            "    allows_hidden_orderby_agg = True",
            "",
            "    # Whether ORDER BY clause can use sql calculated expression",
            "    # if True, use alias of select column for `order by`",
            "    # the True is safely for most database",
            "    # But for backward compatibility, False by default",
            "    allows_hidden_cc_in_orderby = False",
            "",
            "    # Whether allow CTE as subquery or regular CTE",
            "    # If True, then it will allow  in subquery ,",
            "    # if False it will allow as regular CTE",
            "    allows_cte_in_subquery = True",
            "    # Define alias for CTE",
            "    cte_alias = \"__cte\"",
            "    # Whether allow LIMIT clause in the SQL",
            "    # If True, then the database engine is allowed for LIMIT clause",
            "    # If False, then the database engine is allowed for TOP clause",
            "    allow_limit_clause = True",
            "    # This set will give keywords for select statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    select_keywords: set[str] = {\"SELECT\"}",
            "    # This set will give the keywords for data limit statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    top_keywords: set[str] = {\"TOP\"}",
            "    # A set of disallowed connection query parameters by driver name",
            "    disallow_uri_query_params: dict[str, set[str]] = {}",
            "    # A Dict of query parameters that will always be used on every connection",
            "    # by driver name",
            "    enforce_uri_query_params: dict[str, dict[str, Any]] = {}",
            "",
            "    force_column_alias_quotes = False",
            "    arraysize = 0",
            "    max_column_name_length: int | None = None",
            "    try_remove_schema_from_table_name = True  # pylint: disable=invalid-name",
            "    run_multiple_statements_as_one = False",
            "    custom_errors: dict[",
            "        Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]",
            "    ] = {}",
            "",
            "    # Whether the engine supports file uploads",
            "    # if True, database will be listed as option in the upload file form",
            "    supports_file_upload = True",
            "",
            "    # Is the DB engine spec able to change the default schema? This requires implementing",
            "    # a custom `adjust_engine_params` method.",
            "    supports_dynamic_schema = False",
            "",
            "    # Does the DB support catalogs? A catalog here is a group of schemas, and has",
            "    # different names depending on the DB: BigQuery calles it a \"project\", Postgres calls",
            "    # it a \"database\", Trino calls it a \"catalog\", etc.",
            "    supports_catalog = False",
            "",
            "    # Can the catalog be changed on a per-query basis?",
            "    supports_dynamic_catalog = False",
            "",
            "    @classmethod",
            "    def get_allows_alias_in_select(",
            "        cls, database: Database  # pylint: disable=unused-argument",
            "    ) -> bool:",
            "        \"\"\"",
            "        Method for dynamic `allows_alias_in_select`.",
            "",
            "        In Dremio this atribute is version-dependent, so Superset needs to inspect the",
            "        database configuration in order to determine it. This method allows engine-specs",
            "        to define dynamic values for the attribute.",
            "        \"\"\"",
            "        return cls.allows_alias_in_select",
            "",
            "    @classmethod",
            "    def supports_url(cls, url: URL) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy URL.",
            "",
            "        As an example, if a given DB engine spec has:",
            "",
            "            class PostgresDBEngineSpec:",
            "                engine = \"postgresql\"",
            "                engine_aliases = \"postgres\"",
            "                drivers = {",
            "                    \"psycopg2\": \"The default Postgres driver\",",
            "                    \"asyncpg\": \"An asynchronous Postgres driver\",",
            "                }",
            "",
            "        It would be used for all the following SQLAlchemy URIs:",
            "",
            "            - postgres://user:password@host/db",
            "            - postgresql://user:password@host/db",
            "            - postgres+asyncpg://user:password@host/db",
            "            - postgres+psycopg2://user:password@host/db",
            "            - postgresql+asyncpg://user:password@host/db",
            "            - postgresql+psycopg2://user:password@host/db",
            "",
            "        Note that SQLAlchemy has a default driver even if one is not specified:",
            "",
            "            >>> from sqlalchemy.engine.url import make_url",
            "            >>> make_url('postgres://').get_driver_name()",
            "            'psycopg2'",
            "",
            "        \"\"\"",
            "        backend = url.get_backend_name()",
            "        driver = url.get_driver_name()",
            "        return cls.supports_backend(backend, driver)",
            "",
            "    @classmethod",
            "    def supports_backend(cls, backend: str, driver: str | None = None) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy backend/driver.",
            "        \"\"\"",
            "        # check the backend first",
            "        if backend != cls.engine and backend not in cls.engine_aliases:",
            "            return False",
            "",
            "        # originally DB engine specs didn't declare any drivers and the check was made",
            "        # only on the engine; if that's the case, ignore the driver for backwards",
            "        # compatibility",
            "        if not cls.drivers or driver is None:",
            "            return True",
            "",
            "        return driver in cls.drivers",
            "",
            "    @classmethod",
            "    def get_default_schema(cls, database: Database) -> str | None:",
            "        \"\"\"",
            "        Return the default schema in a given database.",
            "        \"\"\"",
            "        with database.get_inspector_with_context() as inspector:",
            "            return inspector.default_schema_name",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the schema configured in a SQLALchemy URI and connection arguments, if any.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This is used to determine the schema of tables that aren't fully qualified, eg:",
            "",
            "            SELECT * FROM foo;",
            "",
            "        In the example above, the schema where the `foo` table lives depends on a few",
            "        factors:",
            "",
            "            1. For DB engine specs that allow dynamically changing the schema based on the",
            "               query we should use the query schema.",
            "            2. For DB engine specs that don't support dynamically changing the schema and",
            "               have the schema hardcoded in the SQLAlchemy URI we should use the schema",
            "               from the URI.",
            "            3. For DB engine specs that don't connect to a specific schema and can't",
            "               change it dynamically we need to probe the database for the default schema.",
            "",
            "        Determining the correct schema is crucial for managing access to data, so please",
            "        make sure you understand this logic when working on a new DB engine spec.",
            "        \"\"\"",
            "        # dynamic schema varies on a per-query basis",
            "        if cls.supports_dynamic_schema:",
            "            return query.schema",
            "",
            "        # check if the schema is stored in the SQLAlchemy URI or connection arguments",
            "        try:",
            "            connect_args = database.get_extra()[\"engine_params\"][\"connect_args\"]",
            "        except KeyError:",
            "            connect_args = {}",
            "        sqlalchemy_uri = make_url_safe(database.sqlalchemy_uri)",
            "        if schema := cls.get_schema_from_engine_params(sqlalchemy_uri, connect_args):",
            "            return schema",
            "",
            "        # return the default schema of the database",
            "        return cls.get_default_schema(database)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific exceptions into",
            "        Superset DBAPI exceptions",
            "",
            "        Note: On python 3.9 this method can be changed to a classmethod property",
            "        without the need of implementing a metaclass type",
            "",
            "        :return: A map of driver specific exception to superset custom exceptions",
            "        \"\"\"",
            "        return {}",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific parser method",
            "",
            "        :return: An Exception with a parsed string off the original exception",
            "        \"\"\"",
            "        return exception",
            "",
            "    @classmethod",
            "    def get_dbapi_mapped_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Get a superset custom DBAPI exception from the driver specific exception.",
            "",
            "        Override if the engine needs to perform extra changes to the exception, for",
            "        example change the exception message or implement custom more complex logic",
            "",
            "        :param exception: The driver specific exception",
            "        :return: Superset custom DBAPI exception",
            "        \"\"\"",
            "        new_exception = cls.get_dbapi_exception_mapping().get(type(exception))",
            "        if not new_exception:",
            "            return cls.parse_error_exception(exception)",
            "        return new_exception(str(exception))",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(  # pylint: disable=unused-argument",
            "        cls,",
            "        extra: dict[str, Any],",
            "    ) -> bool:",
            "        return False",
            "",
            "    @classmethod",
            "    def get_text_clause(cls, clause: str) -> TextClause:",
            "        \"\"\"",
            "        SQLAlchemy wrapper to ensure text clauses are escaped properly",
            "",
            "        :param clause: string clause with potentially unescaped characters",
            "        :return: text clause with escaped characters",
            "        \"\"\"",
            "        if cls.allows_escaped_colons:",
            "            clause = clause.replace(\":\", \"\\\\:\")",
            "        return text(clause)",
            "",
            "    @classmethod",
            "    def get_engine(",
            "        cls,",
            "        database: Database,",
            "        schema: str | None = None,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> ContextManager[Engine]:",
            "        \"\"\"",
            "        Return an engine context manager.",
            "",
            "            >>> with DBEngineSpec.get_engine(database, schema, source) as engine:",
            "            ...     connection = engine.connect()",
            "            ...     connection.execute(sql)",
            "",
            "        \"\"\"",
            "        return database.get_sqla_engine_with_context(schema=schema, source=source)",
            "",
            "    @classmethod",
            "    def get_timestamp_expr(",
            "        cls,",
            "        col: ColumnClause,",
            "        pdf: str | None,",
            "        time_grain: str | None,",
            "    ) -> TimestampExpression:",
            "        \"\"\"",
            "        Construct a TimestampExpression to be used in a SQLAlchemy query.",
            "",
            "        :param col: Target column for the TimestampExpression",
            "        :param pdf: date format (seconds or milliseconds)",
            "        :param time_grain: time grain, e.g. P1Y for 1 year",
            "        :return: TimestampExpression object",
            "        \"\"\"",
            "        if time_grain:",
            "            type_ = str(getattr(col, \"type\", \"\"))",
            "            time_expr = cls.get_time_grain_expressions().get(time_grain)",
            "            if not time_expr:",
            "                raise NotImplementedError(",
            "                    f\"No grain spec for {time_grain} for database {cls.engine}\"",
            "                )",
            "            if type_ and \"{func}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{func}\", date_trunc_function)",
            "            if type_ and \"{type}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{type}\", type_)",
            "        else:",
            "            time_expr = \"{col}\"",
            "",
            "        # if epoch, translate to DATE using db specific conf",
            "        if pdf == \"epoch_s\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_to_dttm())",
            "        elif pdf == \"epoch_ms\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_ms_to_dttm())",
            "",
            "        return TimestampExpression(time_expr, col, type_=col.type)",
            "",
            "    @classmethod",
            "    def get_time_grains(cls) -> tuple[TimeGrain, ...]:",
            "        \"\"\"",
            "        Generate a tuple of supported time grains.",
            "",
            "        :return: All time grains supported by the engine",
            "        \"\"\"",
            "",
            "        ret_list = []",
            "        time_grains = builtin_time_grains.copy()",
            "        time_grains.update(current_app.config[\"TIME_GRAIN_ADDONS\"])",
            "        for duration, func in cls.get_time_grain_expressions().items():",
            "            if duration in time_grains:",
            "                name = time_grains[duration]",
            "                ret_list.append(TimeGrain(name, _(name), func, duration))",
            "        return tuple(ret_list)",
            "",
            "    @classmethod",
            "    def _sort_time_grains(",
            "        cls, val: tuple[str | None, str], index: int",
            "    ) -> float | int | str:",
            "        \"\"\"",
            "        Return an ordered time-based value of a portion of a time grain",
            "        for sorting",
            "        Values are expected to be either None or start with P or PT",
            "        Have a numerical value in the middle and end with",
            "        a value for the time interval",
            "        It can also start or end with epoch start time denoting a range",
            "        i.e, week beginning or ending with a day",
            "        \"\"\"",
            "        pos = {",
            "            \"FIRST\": 0,",
            "            \"SECOND\": 1,",
            "            \"THIRD\": 2,",
            "            \"LAST\": 3,",
            "        }",
            "",
            "        if val[0] is None:",
            "            return pos[\"FIRST\"]",
            "",
            "        prog = re.compile(r\"(.*\\/)?(P|PT)([0-9\\.]+)(S|M|H|D|W|M|Y)(\\/.*)?\")",
            "        result = prog.match(val[0])",
            "",
            "        # for any time grains that don't match the format, put them at the end",
            "        if result is None:",
            "            return pos[\"LAST\"]",
            "",
            "        second_minute_hour = [\"S\", \"M\", \"H\"]",
            "        day_week_month_year = [\"D\", \"W\", \"M\", \"Y\"]",
            "        is_less_than_day = result.group(2) == \"PT\"",
            "        interval = result.group(4)",
            "        epoch_time_start_string = result.group(1) or result.group(5)",
            "        has_starting_or_ending = bool(len(epoch_time_start_string or \"\"))",
            "",
            "        def sort_day_week() -> int:",
            "            if has_starting_or_ending:",
            "                return pos[\"LAST\"]",
            "            if is_less_than_day:",
            "                return pos[\"SECOND\"]",
            "            return pos[\"THIRD\"]",
            "",
            "        def sort_interval() -> float:",
            "            if is_less_than_day:",
            "                return second_minute_hour.index(interval)",
            "            return day_week_month_year.index(interval)",
            "",
            "        # 0: all \"PT\" values should come before \"P\" values (i.e, PT10M)",
            "        # 1: order values within the above arrays (\"D\" before \"W\")",
            "        # 2: sort by numeric value (PT10M before PT15M)",
            "        # 3: sort by any week starting/ending values",
            "        plist = {",
            "            0: sort_day_week(),",
            "            1: pos[\"SECOND\"] if is_less_than_day else pos[\"THIRD\"],",
            "            2: sort_interval(),",
            "            3: float(result.group(3)),",
            "        }",
            "",
            "        return plist.get(index, 0)",
            "",
            "    @classmethod",
            "    def get_time_grain_expressions(cls) -> dict[str | None, str]:",
            "        \"\"\"",
            "        Return a dict of all supported time grains including any potential added grains",
            "        but excluding any potentially disabled grains in the config file.",
            "",
            "        :return: All time grain expressions supported by the engine",
            "        \"\"\"",
            "        # TODO: use @memoize decorator or similar to avoid recomputation on every call",
            "        time_grain_expressions = cls._time_grain_expressions.copy()",
            "        grain_addon_expressions = current_app.config[\"TIME_GRAIN_ADDON_EXPRESSIONS\"]",
            "        time_grain_expressions.update(grain_addon_expressions.get(cls.engine, {}))",
            "        denylist: list[str] = current_app.config[\"TIME_GRAIN_DENYLIST\"]",
            "        for key in denylist:",
            "            time_grain_expressions.pop(key, None)",
            "",
            "        return dict(",
            "            sorted(",
            "                time_grain_expressions.items(),",
            "                key=lambda x: (",
            "                    cls._sort_time_grains(x, 0),",
            "                    cls._sort_time_grains(x, 1),",
            "                    cls._sort_time_grains(x, 2),",
            "                    cls._sort_time_grains(x, 3),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "",
            "        :param cursor: Cursor instance",
            "        :param limit: Maximum number of rows to be returned by the cursor",
            "        :return: Result of query",
            "        \"\"\"",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            if cls.limit_method == LimitMethod.FETCH_MANY and limit:",
            "                return cursor.fetchmany(limit)",
            "            data = cursor.fetchall()",
            "            description = cursor.description or []",
            "            # Create a mapping between column name and a mutator function to normalize",
            "            # values with. The first two items in the description row are",
            "            # the column name and type.",
            "            column_mutators = {",
            "                row[0]: func",
            "                for row in description",
            "                if (",
            "                    func := cls.column_type_mutators.get(",
            "                        type(cls.get_sqla_column_type(cls.get_datatype(row[1])))",
            "                    )",
            "                )",
            "            }",
            "            if column_mutators:",
            "                indexes = {row[0]: idx for idx, row in enumerate(description)}",
            "                for row_idx, row in enumerate(data):",
            "                    new_row = list(row)",
            "                    for col, func in column_mutators.items():",
            "                        col_idx = indexes[col]",
            "                        new_row[col_idx] = func(row[col_idx])",
            "                    data[row_idx] = tuple(new_row)",
            "",
            "            return data",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def expand_data(",
            "        cls, columns: list[ResultSetColumnType], data: list[dict[Any, Any]]",
            "    ) -> tuple[",
            "        list[ResultSetColumnType], list[dict[Any, Any]], list[ResultSetColumnType]",
            "    ]:",
            "        \"\"\"",
            "        Some engines support expanding nested fields. See implementation in Presto",
            "        spec for details.",
            "",
            "        :param columns: columns selected in the query",
            "        :param data: original data set",
            "        :return: list of all columns(selected columns and their nested fields),",
            "                 expanded data set, listed of nested fields",
            "        \"\"\"",
            "        return columns, data, []",
            "",
            "    @classmethod",
            "    def alter_new_orm_column(cls, orm_col: TableColumn) -> None:",
            "        \"\"\"Allow altering default column attributes when first detected/added",
            "",
            "        For instance special column like `__time` for Druid can be",
            "        set to is_dttm=True. Note that this only gets called when new",
            "        columns are detected/created\"\"\"",
            "        # TODO: Fix circular import caused by importing TableColumn",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (seconds) to datetime that can be used in a",
            "        query. The reference column should be denoted as `{col}` in the return",
            "        expression, e.g. \"FROM_UNIXTIME({col})\"",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (milliseconds) to datetime that can be used",
            "        in a query.",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        return cls.epoch_to_dttm().replace(\"{col}\", \"({col}/1000)\")",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        \"\"\"",
            "        Change column type code from cursor description to string representation.",
            "",
            "        :param type_code: Type code from cursor description",
            "        :return: String representation of type code",
            "        \"\"\"",
            "        if isinstance(type_code, str) and type_code != \"\":",
            "            return type_code.upper()",
            "        return None",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        noop by default",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        return indexes",
            "",
            "    @classmethod",
            "    def extra_table_metadata(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Returns engine-specific table metadata",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name",
            "        :param schema_name: Schema name",
            "        :return: Engine-specific table metadata",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        return {}",
            "",
            "    @classmethod",
            "    def apply_limit_to_sql(",
            "        cls, sql: str, limit: int, database: Database, force: bool = False",
            "    ) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a LIMIT clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param database: Database instance",
            "        :return: SQL query with limit clause",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        if cls.limit_method == LimitMethod.WRAP_SQL:",
            "            sql = sql.strip(\"\\t\\n ;\")",
            "            qry = (",
            "                select(\"*\")",
            "                .select_from(TextAsFrom(text(sql), [\"*\"]).alias(\"inner_qry\"))",
            "                .limit(limit)",
            "            )",
            "            return database.compile_sqla_query(qry)",
            "",
            "        if cls.limit_method == LimitMethod.FORCE_LIMIT:",
            "            parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "            sql = parsed_query.set_or_update_query_limit(limit, force=force)",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def apply_top_to_sql(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a TOP clause",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param sql: SQL query",
            "        :return: SQL query with top clause",
            "        \"\"\"",
            "",
            "        cte = None",
            "        sql_remainder = None",
            "        sql = sql.strip(\" \\t\\n;\")",
            "        sql_statement = sqlparse.format(sql, strip_comments=True)",
            "        query_limit: int | None = sql_parse.extract_top_from_query(",
            "            sql_statement, cls.top_keywords",
            "        )",
            "        if not limit:",
            "            final_limit = query_limit",
            "        elif int(query_limit or 0) < limit and query_limit is not None:",
            "            final_limit = query_limit",
            "        else:",
            "            final_limit = limit",
            "        if not cls.allows_cte_in_subquery:",
            "            cte, sql_remainder = sql_parse.get_cte_remainder_query(sql_statement)",
            "        if cte:",
            "            str_statement = str(sql_remainder)",
            "            cte = cte + \"\\n\"",
            "        else:",
            "            cte = \"\"",
            "            str_statement = str(sql)",
            "        str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "",
            "        tokens = str_statement.rstrip().split(\" \")",
            "        tokens = [token for token in tokens if token]",
            "        if cls.top_not_in_sql(str_statement):",
            "            selects = [",
            "                i",
            "                for i, word in enumerate(tokens)",
            "                if word.upper() in cls.select_keywords",
            "            ]",
            "            first_select = selects[0]",
            "            if tokens[first_select + 1].upper() == \"DISTINCT\":",
            "                first_select += 1",
            "",
            "            tokens.insert(first_select + 1, \"TOP\")",
            "            tokens.insert(first_select + 2, str(final_limit))",
            "",
            "        next_is_limit_token = False",
            "        new_tokens = []",
            "",
            "        for token in tokens:",
            "            if token in cls.top_keywords:",
            "                next_is_limit_token = True",
            "            elif next_is_limit_token:",
            "                if token.isdigit():",
            "                    token = str(final_limit)",
            "                    next_is_limit_token = False",
            "            new_tokens.append(token)",
            "        sql = \" \".join(new_tokens)",
            "        return cte + sql",
            "",
            "    @classmethod",
            "    def top_not_in_sql(cls, sql: str) -> bool:",
            "        for top_word in cls.top_keywords:",
            "            if top_word.upper() in sql.upper():",
            "                return False",
            "        return True",
            "",
            "    @classmethod",
            "    def get_limit_from_sql(cls, sql: str) -> int | None:",
            "        \"\"\"",
            "        Extract limit from SQL query",
            "",
            "        :param sql: SQL query",
            "        :return: Value of limit clause in query",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.limit",
            "",
            "    @classmethod",
            "    def set_or_update_query_limit(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Create a query based on original query but with new limit clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: New limit to insert/replace into query",
            "        :return: Query with new limit",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.set_or_update_query_limit(limit)",
            "",
            "    @classmethod",
            "    def get_cte_query(cls, sql: str) -> str | None:",
            "        \"\"\"",
            "        Convert the input CTE based SQL to the SQL for virtual table conversion",
            "",
            "        :param sql: SQL query",
            "        :return: CTE with the main select query aliased as `__cte`",
            "",
            "        \"\"\"",
            "        if not cls.allows_cte_in_subquery:",
            "            stmt = sqlparse.parse(sql)[0]",
            "",
            "            # The first meaningful token for CTE will be with WITH",
            "            idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "            if not (token and token.ttype == CTE):",
            "                return None",
            "            idx, token = stmt.token_next(idx)",
            "            idx = stmt.token_index(token) + 1",
            "",
            "            # extract rest of the SQLs after CTE",
            "            remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "            return f\"WITH {token.value},\\n{cls.cte_alias} AS (\\n{remainder}\\n)\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: Database,",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        For regular engines this calls the `pandas.DataFrame.to_sql` method. Can be",
            "        overridden for engines that don't work well with this method, e.g. Hive and",
            "        BigQuery.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "",
            "        to_sql_kwargs[\"name\"] = table.table",
            "",
            "        if table.schema:",
            "            # Only add schema when it is preset and non-empty.",
            "            to_sql_kwargs[\"schema\"] = table.schema",
            "",
            "        with cls.get_engine(database) as engine:",
            "            if engine.dialect.supports_multivalues_insert:",
            "                to_sql_kwargs[\"method\"] = \"multi\"",
            "",
            "            df.to_sql(con=engine, **to_sql_kwargs)",
            "",
            "    @classmethod",
            "    def convert_dttm(  # pylint: disable=unused-argument",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Convert a Python `datetime` object to a SQL expression.",
            "",
            "        :param target_type: The target type of expression",
            "        :param dttm: The datetime object",
            "        :param db_extra: The database extra object",
            "        :return: The SQL expression",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Any, query: Query) -> None:",
            "        \"\"\"Handle a live cursor between the execute and fetchall calls",
            "",
            "        The flow works without this method doing anything, but it allows",
            "        for handling the cursor and updating progress information in the",
            "        query object\"\"\"",
            "        # TODO: Fix circular import error caused by importing sql_lab.Query",
            "",
            "    @classmethod",
            "    def execute_with_cursor(cls, cursor: Any, sql: str, query: Query) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        For most implementations this just makes calls to `execute` and",
            "        `handle_cursor` consecutively, but in some engines (e.g. Trino) we may",
            "        need to handle client limitations such as lack of async support and",
            "        perform a more complicated operation to get information from the cursor",
            "        in a timely manner and facilitate operations such as query stop",
            "        \"\"\"",
            "        logger.debug(\"Query %d: Running query: %s\", query.id, sql)",
            "        cls.execute(cursor, sql, async_=True)",
            "        logger.debug(\"Query %d: Handling cursor\", query.id)",
            "        cls.handle_cursor(cursor, query)",
            "",
            "    @classmethod",
            "    def extract_error_message(cls, ex: Exception) -> str:",
            "        return f\"{cls.engine} error: {cls._extract_error_message(ex)}\"",
            "",
            "    @classmethod",
            "    def _extract_error_message(cls, ex: Exception) -> str:",
            "        \"\"\"Extract error message for queries\"\"\"",
            "        return utils.error_msg_from_exception(ex)",
            "",
            "    @classmethod",
            "    def extract_errors(",
            "        cls, ex: Exception, context: dict[str, Any] | None = None",
            "    ) -> list[SupersetError]:",
            "        raw_message = cls._extract_error_message(ex)",
            "",
            "        context = context or {}",
            "        for regex, (message, error_type, extra) in cls.custom_errors.items():",
            "            if match := regex.search(raw_message):",
            "                params = {**context, **match.groupdict()}",
            "                extra[\"engine_name\"] = cls.engine_name",
            "                return [",
            "                    SupersetError(",
            "                        error_type=error_type,",
            "                        message=message % params,",
            "                        level=ErrorLevel.ERROR,",
            "                        extra=extra,",
            "                    )",
            "                ]",
            "",
            "        return [",
            "            SupersetError(",
            "                error_type=SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "                message=cls._extract_error_message(ex),",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"engine_name\": cls.engine_name},",
            "            )",
            "        ]",
            "",
            "    @classmethod",
            "    def adjust_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        uri: URL,",
            "        connect_args: dict[str, Any],",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> tuple[URL, dict[str, Any]]:",
            "        \"\"\"",
            "        Return a new URL and ``connect_args`` for a specific catalog/schema.",
            "",
            "        This is used in SQL Lab, allowing users to select a schema from the list of",
            "        schemas available in a given database, and have the query run with that schema as",
            "        the default one.",
            "",
            "        For some databases (like MySQL, Presto, Snowflake) this requires modifying the",
            "        SQLAlchemy URI before creating the connection. For others (like Postgres), it",
            "        requires additional parameters in ``connect_args`` or running pre-session",
            "        queries with ``set`` parameters.",
            "",
            "        When a DB engine spec implements this method or ``get_prequeries`` (see below) it",
            "        should also have the attribute ``supports_dynamic_schema`` set to true, so that",
            "        Superset knows in which schema a given query is running in order to enforce",
            "        permissions (see #23385 and #23401).",
            "",
            "        Currently, changing the catalog is not supported. The method accepts a catalog so",
            "        that when catalog support is added to Superset the interface remains the same.",
            "        This is important because DB engine specs can be installed from 3rd party",
            "        packages, so we want to keep these methods as stable as possible.",
            "        \"\"\"",
            "        return uri, {",
            "            **connect_args,",
            "            **cls.enforce_uri_query_params.get(uri.get_driver_name(), {}),",
            "        }",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,  # pylint: disable=unused-argument",
            "        schema: str | None = None,  # pylint: disable=unused-argument",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return pre-session queries.",
            "",
            "        These are currently used as an alternative to ``adjust_engine_params`` for",
            "        databases where the selected schema cannot be specified in the SQLAlchemy URI or",
            "        connection arguments.",
            "",
            "        For example, in order to specify a default schema in RDS we need to run a query",
            "        at the beginning of the session:",
            "",
            "            sql> set search_path = my_schema;",
            "",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def patch(cls) -> None:",
            "        \"\"\"",
            "        TODO: Improve docstring and refactor implementation in Hive",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def get_catalog_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs from database.",
            "",
            "        This needs to be implemented per database, since SQLAlchemy doesn't offer an",
            "        abstraction.",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def get_schema_names(cls, inspector: Inspector) -> list[str]:",
            "        \"\"\"",
            "        Get all schemas from database",
            "",
            "        :param inspector: SqlAlchemy inspector",
            "        :return: All schemas in the database",
            "        \"\"\"",
            "        return sorted(inspector.get_schema_names())",
            "",
            "    @classmethod",
            "    def get_table_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the real table names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The physical table names",
            "        \"\"\"",
            "",
            "        try:",
            "            tables = set(inspector.get_table_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            tables = {re.sub(f\"^{schema}\\\\.\", \"\", table) for table in tables}",
            "        return tables",
            "",
            "    @classmethod",
            "    def get_view_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the view names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The view names",
            "        \"\"\"",
            "",
            "        try:",
            "            views = set(inspector.get_view_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            views = {re.sub(f\"^{schema}\\\\.\", \"\", view) for view in views}",
            "        return views",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,  # pylint: disable=unused-argument",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return inspector.get_indexes(table_name, schema)",
            "",
            "    @classmethod",
            "    def get_table_comment(",
            "        cls, inspector: Inspector, table_name: str, schema: str | None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Get comment of table from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :return: comment of table",
            "        \"\"\"",
            "        comment = None",
            "        try:",
            "            comment = inspector.get_table_comment(table_name, schema)",
            "            comment = comment.get(\"text\") if isinstance(comment, dict) else None",
            "        except NotImplementedError:",
            "            # It's expected that some dialects don't implement the comment method",
            "            pass",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.error(\"Unexpected error while fetching table comment\", exc_info=True)",
            "            logger.exception(ex)",
            "        return comment",
            "",
            "    @classmethod",
            "    def get_columns(  # pylint: disable=unused-argument",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Get all columns from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :param options: Extra options to customise the display of columns in",
            "                        some databases",
            "        :return: All columns in table",
            "        \"\"\"",
            "        return convert_inspector_columns(",
            "            cast(list[SQLAColumnType], inspector.get_columns(table_name, schema))",
            "        )",
            "",
            "    @classmethod",
            "    def get_metrics(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[MetricType]:",
            "        \"\"\"",
            "        Get all metrics from a given schema and table.",
            "        \"\"\"",
            "        return [",
            "            {",
            "                \"metric_name\": \"count\",",
            "                \"verbose_name\": \"COUNT(*)\",",
            "                \"metric_type\": \"count\",",
            "                \"expression\": \"COUNT(*)\",",
            "            }",
            "        ]",
            "",
            "    @classmethod",
            "    def where_latest_partition(  # pylint: disable=too-many-arguments,unused-argument",
            "        cls,",
            "        table_name: str,",
            "        schema: str | None,",
            "        database: Database,",
            "        query: Select,",
            "        columns: list[ResultSetColumnType] | None = None,",
            "    ) -> Select | None:",
            "        \"\"\"",
            "        Add a where clause to a query to reference only the most recent partition",
            "",
            "        :param table_name: Table name",
            "        :param schema: Schema name",
            "        :param database: Database instance",
            "        :param query: SqlAlchemy query",
            "        :param columns: List of TableColumns",
            "        :return: SqlAlchemy query with additional where clause referencing the latest",
            "        partition",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database, TableColumn",
            "        return None",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        return [",
            "            literal_column(query_as)",
            "            if (query_as := c.get(\"query_as\"))",
            "            else column(c[\"column_name\"])",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments,too-many-locals",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: str | None = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: list[ResultSetColumnType] | None = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Generate a \"SELECT * from [schema.]table_name\" query with appropriate limit.",
            "",
            "        WARNING: expects only unquoted table and schema names.",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name, unquoted",
            "        :param engine: SqlAlchemy Engine instance",
            "        :param schema: Schema, unquoted",
            "        :param limit: limit to impose on query",
            "        :param show_cols: Show columns in query; otherwise use \"*\"",
            "        :param indent: Add indentation to query",
            "        :param latest_partition: Only query the latest partition",
            "        :param cols: Columns to include in query",
            "        :return: SQL query",
            "        \"\"\"",
            "        # pylint: disable=redefined-outer-name",
            "        fields: str | list[Any] = \"*\"",
            "        cols = cols or []",
            "        if (show_cols or latest_partition) and not cols:",
            "            cols = database.get_columns(table_name, schema)",
            "",
            "        if show_cols:",
            "            fields = cls._get_fields(cols)",
            "        quote = engine.dialect.identifier_preparer.quote",
            "        quote_schema = engine.dialect.identifier_preparer.quote_schema",
            "        if schema:",
            "            full_table_name = quote_schema(schema) + \".\" + quote(table_name)",
            "        else:",
            "            full_table_name = quote(table_name)",
            "",
            "        qry = select(fields).select_from(text(full_table_name))",
            "",
            "        if limit and cls.allow_limit_clause:",
            "            qry = qry.limit(limit)",
            "        if latest_partition:",
            "            partition_query = cls.where_latest_partition(",
            "                table_name, schema, database, qry, columns=cols",
            "            )",
            "            if partition_query is not None:",
            "                qry = partition_query",
            "        sql = database.compile_sqla_query(qry)",
            "        if indent:",
            "            sql = sqlparse.format(sql, reindent=True)",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        \"\"\"",
            "        Generate a SQL query that estimates the cost of a given statement.",
            "",
            "        :param statement: A single SQL statement",
            "        :param cursor: Cursor instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        \"\"\"",
            "        Format cost estimate.",
            "",
            "        :param raw_cost: Raw estimate from `estimate_query_cost`",
            "        :return: Human readable cost estimate",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def process_statement(cls, statement: str, database: Database) -> str:",
            "        \"\"\"",
            "        Process a SQL statement by stripping and mutating it.",
            "",
            "        :param statement: A single SQL statement",
            "        :param database: Database instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        parsed_query = ParsedQuery(statement, engine=cls.engine)",
            "        sql = parsed_query.stripped()",
            "        sql_query_mutator = current_app.config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = current_app.config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=database,",
            "            )",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: Database,",
            "        schema: str,",
            "        sql: str,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise Exception(  # pylint: disable=broad-exception-raised",
            "                \"Database does not support cost estimation\"",
            "            )",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        statements = parsed_query.get_statements()",
            "",
            "        costs = []",
            "        with database.get_raw_connection(schema=schema, source=source) as conn:",
            "            cursor = conn.cursor()",
            "            for statement in statements:",
            "                processed_statement = cls.process_statement(statement, database)",
            "                costs.append(cls.estimate_statement_cost(processed_statement, cursor))",
            "",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        if impersonate_user and username is not None:",
            "            url = url.set(username=username)",
            "",
            "        return url",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "",
            "        :param connect_args: config to be updated",
            "        :param uri: URI",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def execute(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: str,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"",
            "        Execute a SQL query",
            "",
            "        :param cursor: Cursor instance",
            "        :param query: Query to execute",
            "        :param kwargs: kwargs to be passed to cursor.execute()",
            "        :return:",
            "        \"\"\"",
            "        if not cls.allows_sql_comments:",
            "            query = sql_parse.strip_comments_from_sql(query, engine=cls.engine)",
            "",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            cursor.execute(query)",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def make_label_compatible(cls, label: str) -> str | quoted_name:",
            "        \"\"\"",
            "        Conditionally mutate and/or quote a sqlalchemy expression label. If",
            "        force_column_alias_quotes is set to True, return the label as a",
            "        sqlalchemy.sql.elements.quoted_name object to ensure that the select query",
            "        and query results have same case. Otherwise, return the mutated label as a",
            "        regular string. If maximum supported column name length is exceeded,",
            "        generate a truncated label by calling truncate_label().",
            "",
            "        :param label: expected expression label/alias",
            "        :return: conditionally mutated label supported by the db engine",
            "        \"\"\"",
            "        label_mutated = cls._mutate_label(label)",
            "        if (",
            "            cls.max_column_name_length",
            "            and len(label_mutated) > cls.max_column_name_length",
            "        ):",
            "            label_mutated = cls._truncate_label(label)",
            "        if cls.force_column_alias_quotes:",
            "            label_mutated = quoted_name(label_mutated, True)",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def get_column_types(",
            "        cls,",
            "        column_type: str | None,",
            "    ) -> tuple[TypeEngine, GenericDataType] | None:",
            "        \"\"\"",
            "        Return a sqlalchemy native column type and generic data type that corresponds",
            "        to the column type defined in the data source (return None to use default type",
            "        inferred by SQLAlchemy). Override `column_type_mappings` for specific needs",
            "        (see MSSQL for example of NCHAR/NVARCHAR handling).",
            "",
            "        :param column_type: Column type returned by inspector",
            "        :return: SQLAlchemy and generic Superset column types",
            "        \"\"\"",
            "        if not column_type:",
            "            return None",
            "",
            "        for regex, sqla_type, generic_type in (",
            "            cls.column_type_mappings + cls._default_column_type_mappings",
            "        ):",
            "            match = regex.match(column_type)",
            "            if not match:",
            "                continue",
            "            if callable(sqla_type):",
            "                return sqla_type(match), generic_type",
            "            return sqla_type, generic_type",
            "        return None",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        Most engines support mixed case aliases that can include numbers",
            "        and special characters, like commas, parentheses etc. For engines that",
            "        have restrictions on what types of aliases are supported, this method",
            "        can be overridden to ensure that labels conform to the engine's",
            "        limitations. Mutated labels should be deterministic (input label A always",
            "        yields output label X) and unique (input labels A and B don't yield the same",
            "        output label X).",
            "",
            "        :param label: Preferred expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        return label",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"",
            "        In the case that a label exceeds the max length supported by the engine,",
            "        this method is used to construct a deterministic and unique label based on",
            "        the original label. By default, this returns a md5 hash of the original label,",
            "        conditionally truncated if the length of the hash exceeds the max column length",
            "        of the engine.",
            "",
            "        :param label: Expected expression label",
            "        :return: Truncated label",
            "        \"\"\"",
            "        label = md5_sha_from_str(label)",
            "        # truncate hash if it exceeds max length",
            "        if cls.max_column_name_length and len(label) > cls.max_column_name_length:",
            "            label = label[: cls.max_column_name_length]",
            "        return label",
            "",
            "    @classmethod",
            "    def column_datatype_to_string(",
            "        cls, sqla_column_type: TypeEngine, dialect: Dialect",
            "    ) -> str:",
            "        \"\"\"",
            "        Convert sqlalchemy column type to string representation.",
            "        By default, removes collation and character encoding info to avoid",
            "        unnecessarily long datatypes.",
            "",
            "        :param sqla_column_type: SqlAlchemy column type",
            "        :param dialect: Sqlalchemy dialect",
            "        :return: Compiled column type",
            "        \"\"\"",
            "        sqla_column_type = sqla_column_type.copy()",
            "        if hasattr(sqla_column_type, \"collation\"):",
            "            sqla_column_type.collation = None",
            "        if hasattr(sqla_column_type, \"charset\"):",
            "            sqla_column_type.charset = None",
            "        return sqla_column_type.compile(dialect=dialect).upper()",
            "",
            "    @classmethod",
            "    def get_function_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get a list of function names that are able to be called on the database.",
            "        Used for SQL Lab autocomplete.",
            "",
            "        :param database: The database to get functions for",
            "        :return: A list of function names useable in the database",
            "        \"\"\"",
            "        return []",
            "",
            "    @staticmethod",
            "    def pyodbc_rows_to_tuples(data: list[Any]) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "        Convert pyodbc.Row objects from `fetch_data` to tuples.",
            "",
            "        :param data: List of tuples or pyodbc.Row objects",
            "        :return: List of tuples",
            "        \"\"\"",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [tuple(row) for row in data]",
            "        return data",
            "",
            "    @staticmethod",
            "    def mutate_db_for_connection_test(  # pylint: disable=unused-argument",
            "        database: Database,",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require passing additional parameters for validating database",
            "        connections. This method makes it possible to mutate the database instance prior",
            "        to testing if a connection is ok.",
            "",
            "        :param database: instance to be mutated",
            "        \"\"\"",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = {}",
            "        if database.extra:",
            "            try:",
            "                extra = json.loads(database.extra)",
            "            except json.JSONDecodeError as ex:",
            "                logger.error(ex, exc_info=True)",
            "                raise ex",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(  # pylint: disable=invalid-name",
            "        database: Database, params: dict[str, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require some sensitive information which do not conform to",
            "        the username:password syntax normally used by SQLAlchemy.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :param params: params to be updated",
            "        \"\"\"",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            params.update(encrypted_extra)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def is_readonly_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"Pessimistic readonly, 100% sure statement won't mutate anything\"\"\"",
            "        return (",
            "            parsed_query.is_select()",
            "            or parsed_query.is_explain()",
            "            or parsed_query.is_show()",
            "        )",
            "",
            "    @classmethod",
            "    def is_select_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"",
            "        Determine if the statement should be considered as SELECT statement.",
            "        Some query dialects do not contain \"SELECT\" word in queries (eg. Kusto)",
            "        \"\"\"",
            "        return parsed_query.is_select()",
            "",
            "    @classmethod",
            "    def get_column_spec(  # pylint: disable=unused-argument",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> ColumnSpec | None:",
            "        \"\"\"",
            "        Get generic type related specs regarding a native column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        if col_types := cls.get_column_types(native_type):",
            "            column_type, generic_type = col_types",
            "            is_dttm = generic_type == GenericDataType.TEMPORAL",
            "            return ColumnSpec(",
            "                sqla_type=column_type, generic_type=generic_type, is_dttm=is_dttm",
            "            )",
            "        return None",
            "",
            "    @classmethod",
            "    def get_sqla_column_type(",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> TypeEngine | None:",
            "        \"\"\"",
            "        Converts native database type to sqlalchemy column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        column_spec = cls.get_column_spec(",
            "            native_type=native_type,",
            "            db_extra=db_extra,",
            "            source=source,",
            "        )",
            "        return column_spec.sqla_type if column_spec else None",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query) -> None:",
            "        \"\"\"",
            "        Some databases may acquire the query cancelation id after the query",
            "        cancelation request has been received. For those cases, the db engine spec",
            "        can record the cancelation intent so that the query can either be stopped",
            "        prior to execution, or canceled once the query id is acquired.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def has_implicit_cancel(cls) -> bool:",
            "        \"\"\"",
            "        Return True if the live cursor handles the implicit cancelation of the query,",
            "        False otherwise.",
            "",
            "        :return: Whether the live cursor implicitly cancels the query",
            "        :see: handle_cursor",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Select identifiers from the database engine that uniquely identifies the",
            "        queries to cancel. The identifier is typically a session id, process id",
            "        or similar.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Query identifier",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def cancel_query(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "        cancel_query_id: str,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Value returned by get_cancel_query_payload or set in",
            "        other life-cycle methods of the query",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def parse_sql(cls, sql: str) -> list[str]:",
            "        return [str(s).strip(\" ;\") for s in sqlparse.parse(sql)]",
            "",
            "    @classmethod",
            "    def get_impersonation_key(cls, user: User | None) -> Any:",
            "        \"\"\"",
            "        Construct an impersonation key, by default it's the given username.",
            "",
            "        :param user: logged-in user",
            "",
            "        :returns: username if given user is not null",
            "        \"\"\"",
            "        return user.username if user else None",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: str | None) -> str | None:",
            "        \"\"\"",
            "        Mask ``encrypted_extra``.",
            "",
            "        This is used to remove any sensitive data in ``encrypted_extra`` when presenting",
            "        it to the user. For example, a private key might be replaced with a masked value",
            "        \"XXXXXXXXXX\". If the masked value is changed the corresponding entry is updated,",
            "        otherwise the old value is used (see ``unmask_encrypted_extra`` below).",
            "        \"\"\"",
            "        return encrypted_extra",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def unmask_encrypted_extra(cls, old: str | None, new: str | None) -> str | None:",
            "        \"\"\"",
            "        Remove masks from ``encrypted_extra``.",
            "",
            "        This method allows reusing existing values from the current encrypted extra on",
            "        updates. It's useful for reusing masked passwords, allowing keys to be updated",
            "        without having to provide sensitive data to the client.",
            "        \"\"\"",
            "        return new",
            "",
            "    @classmethod",
            "    def get_public_information(cls) -> dict[str, Any]:",
            "        \"\"\"",
            "        Construct a Dict with properties we want to expose.",
            "",
            "        :returns: Dict with properties of our class like supports_file_upload",
            "        and disable_ssh_tunneling",
            "        \"\"\"",
            "        return {",
            "            \"supports_file_upload\": cls.supports_file_upload,",
            "            \"disable_ssh_tunneling\": cls.disable_ssh_tunneling,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_database_uri(cls, sqlalchemy_uri: URL) -> None:",
            "        \"\"\"",
            "        Validates a database SQLAlchemy URI per engine spec.",
            "        Use this to implement a final validation for unwanted connection configuration",
            "",
            "        :param sqlalchemy_uri:",
            "        \"\"\"",
            "        if existing_disallowed := cls.disallow_uri_query_params.get(",
            "            sqlalchemy_uri.get_driver_name(), set()",
            "        ).intersection(sqlalchemy_uri.query):",
            "            raise ValueError(f\"Forbidden query parameter(s): {existing_disallowed}\")",
            "",
            "    @classmethod",
            "    def denormalize_name(cls, dialect: Dialect, name: str) -> str:",
            "        if (",
            "            hasattr(dialect, \"requires_name_normalize\")",
            "            and dialect.requires_name_normalize",
            "        ):",
            "            return dialect.denormalize_name(name)",
            "",
            "        return name",
            "",
            "",
            "# schema for adding a database by providing parameters instead of the",
            "# full SQLAlchemy URI",
            "class BasicParametersSchema(Schema):",
            "    username = fields.String(",
            "        required=True, allow_none=True, metadata={\"description\": __(\"Username\")}",
            "    )",
            "    password = fields.String(allow_none=True, metadata={\"description\": __(\"Password\")})",
            "    host = fields.String(",
            "        required=True, metadata={\"description\": __(\"Hostname or IP address\")}",
            "    )",
            "    port = fields.Integer(",
            "        required=True,",
            "        metadata={\"description\": __(\"Database port\")},",
            "        validate=Range(min=0, max=2**16, max_inclusive=False),",
            "    )",
            "    database = fields.String(",
            "        required=True, metadata={\"description\": __(\"Database name\")}",
            "    )",
            "    query = fields.Dict(",
            "        keys=fields.Str(),",
            "        values=fields.Raw(),",
            "        metadata={\"description\": __(\"Additional parameters\")},",
            "    )",
            "    encryption = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an encrypted connection to the database\")},",
            "    )",
            "    ssh = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an ssh tunnel connection to the database\")},",
            "    )",
            "",
            "",
            "class BasicParametersType(TypedDict, total=False):",
            "    username: str | None",
            "    password: str | None",
            "    host: str",
            "    port: int",
            "    database: str",
            "    query: dict[str, Any]",
            "    encryption: bool",
            "",
            "",
            "class BasicPropertiesType(TypedDict):",
            "    parameters: BasicParametersType",
            "",
            "",
            "class BasicParametersMixin:",
            "    \"\"\"",
            "    Mixin for configuring DB engine specs via a dictionary.",
            "",
            "    With this mixin the SQLAlchemy engine can be configured through",
            "    individual parameters, instead of the full SQLAlchemy URI. This",
            "    mixin is for the most common pattern of URI:",
            "",
            "        engine+driver://user:password@host:port/dbname[?key=value&key=value...]",
            "",
            "    \"\"\"",
            "",
            "    # schema describing the parameters used to configure the DB",
            "    parameters_schema = BasicParametersSchema()",
            "",
            "    # recommended driver name for the DB engine spec",
            "    default_driver = \"\"",
            "",
            "    # query parameter to enable encryption in the database connection",
            "    # for Postgres this would be `{\"sslmode\": \"verify-ca\"}`, eg.",
            "    encryption_parameters: dict[str, str] = {}",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(  # pylint: disable=unused-argument",
            "        cls,",
            "        parameters: BasicParametersType,",
            "        encrypted_extra: dict[str, str] | None = None,",
            "    ) -> str:",
            "        # make a copy so that we don't update the original",
            "        query = parameters.get(\"query\", {}).copy()",
            "        if parameters.get(\"encryption\"):",
            "            if not cls.encryption_parameters:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Unable to build a URL with encryption enabled\"",
            "                )",
            "            query.update(cls.encryption_parameters)",
            "",
            "        return str(",
            "            URL.create(",
            "                f\"{cls.engine}+{cls.default_driver}\".rstrip(\"+\"),  # type: ignore",
            "                username=parameters.get(\"username\"),",
            "                password=parameters.get(\"password\"),",
            "                host=parameters[\"host\"],",
            "                port=parameters[\"port\"],",
            "                database=parameters[\"database\"],",
            "                query=query,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(  # pylint: disable=unused-argument",
            "        cls, uri: str, encrypted_extra: dict[str, Any] | None = None",
            "    ) -> BasicParametersType:",
            "        url = make_url_safe(uri)",
            "        query = {",
            "            key: value",
            "            for (key, value) in url.query.items()",
            "            if (key, value) not in cls.encryption_parameters.items()",
            "        }",
            "        encryption = all(",
            "            item in url.query.items() for item in cls.encryption_parameters.items()",
            "        )",
            "        return {",
            "            \"username\": url.username,",
            "            \"password\": url.password,",
            "            \"host\": url.host,",
            "            \"port\": url.port,",
            "            \"database\": url.database,",
            "            \"query\": query,",
            "            \"encryption\": encryption,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls, properties: BasicPropertiesType",
            "    ) -> list[SupersetError]:",
            "        \"\"\"",
            "        Validates any number of parameters, for progressive validation.",
            "",
            "        If only the hostname is present it will check if the name is resolvable. As more",
            "        parameters are present in the request, more validation is done.",
            "        \"\"\"",
            "        errors: list[SupersetError] = []",
            "",
            "        required = {\"host\", \"port\", \"username\", \"database\"}",
            "        parameters = properties.get(\"parameters\", {})",
            "        present = {key for key in parameters if parameters.get(key, ())}",
            "",
            "        if missing := sorted(required - present):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=f'One or more parameters are missing: {\", \".join(missing)}',",
            "                    error_type=SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR,",
            "                    level=ErrorLevel.WARNING,",
            "                    extra={\"missing\": missing},",
            "                ),",
            "            )",
            "",
            "        host = parameters.get(\"host\", None)",
            "        if not host:",
            "            return errors",
            "        if not is_hostname_valid(host):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The hostname provided can't be resolved.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"host\"]},",
            "                ),",
            "            )",
            "            return errors",
            "",
            "        port = parameters.get(\"port\", None)",
            "        if not port:",
            "            return errors",
            "        try:",
            "            port = int(port)",
            "        except (ValueError, TypeError):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"Port must be a valid integer.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        if not (isinstance(port, int) and 0 <= port < 2**16):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=(",
            "                        \"The port must be an integer between 0 and 65535 \"",
            "                        \"(inclusive).\"",
            "                    ),",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        elif not is_port_open(host, port):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The port is closed.\",",
            "                    error_type=SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "",
            "        return errors",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.2\",",
            "            plugins=[MarshmallowPlugin()],",
            "        )",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Match, Pattern",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    cast,",
            "    ContextManager,",
            "    NamedTuple,",
            "    TYPE_CHECKING,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "import pandas as pd",
            "import sqlparse",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask import current_app",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from marshmallow import fields, Schema",
            "from marshmallow.validate import Range",
            "from sqlalchemy import column, select, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.interfaces import Compiled, Dialect",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.ext.compiler import compiles",
            "from sqlalchemy.sql import literal_column, quoted_name, text",
            "from sqlalchemy.sql.expression import ColumnClause, Select, TextAsFrom, TextClause",
            "from sqlalchemy.types import TypeEngine",
            "from sqlparse.tokens import CTE",
            "",
            "from superset import security_manager, sql_parse",
            "from superset.constants import TimeGrain as TimeGrainConstants",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import DisallowedSQLFunction",
            "from superset.sql_parse import ParsedQuery, Table",
            "from superset.superset_typing import ResultSetColumnType, SQLAColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.core import ColumnSpec, GenericDataType",
            "from superset.utils.hashing import md5_sha_from_str",
            "from superset.utils.network import is_hostname_valid, is_port_open",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.models.core import Database",
            "    from superset.models.sql_lab import Query",
            "",
            "ColumnTypeMapping = tuple[",
            "    Pattern[str],",
            "    Union[TypeEngine, Callable[[Match[str]], TypeEngine]],",
            "    GenericDataType,",
            "]",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "def convert_inspector_columns(cols: list[SQLAColumnType]) -> list[ResultSetColumnType]:",
            "    result_set_columns: list[ResultSetColumnType] = []",
            "    for col in cols:",
            "        result_set_columns.append({\"column_name\": col.get(\"name\"), **col})  # type: ignore",
            "    return result_set_columns",
            "",
            "",
            "class TimeGrain(NamedTuple):",
            "    name: str  # TODO: redundant field, remove",
            "    label: str",
            "    function: str",
            "    duration: str | None",
            "",
            "",
            "builtin_time_grains: dict[str | None, str] = {",
            "    TimeGrainConstants.SECOND: __(\"Second\"),",
            "    TimeGrainConstants.FIVE_SECONDS: __(\"5 second\"),",
            "    TimeGrainConstants.THIRTY_SECONDS: __(\"30 second\"),",
            "    TimeGrainConstants.MINUTE: __(\"Minute\"),",
            "    TimeGrainConstants.FIVE_MINUTES: __(\"5 minute\"),",
            "    TimeGrainConstants.TEN_MINUTES: __(\"10 minute\"),",
            "    TimeGrainConstants.FIFTEEN_MINUTES: __(\"15 minute\"),",
            "    TimeGrainConstants.THIRTY_MINUTES: __(\"30 minute\"),",
            "    TimeGrainConstants.HOUR: __(\"Hour\"),",
            "    TimeGrainConstants.SIX_HOURS: __(\"6 hour\"),",
            "    TimeGrainConstants.DAY: __(\"Day\"),",
            "    TimeGrainConstants.WEEK: __(\"Week\"),",
            "    TimeGrainConstants.MONTH: __(\"Month\"),",
            "    TimeGrainConstants.QUARTER: __(\"Quarter\"),",
            "    TimeGrainConstants.YEAR: __(\"Year\"),",
            "    TimeGrainConstants.WEEK_STARTING_SUNDAY: __(\"Week starting Sunday\"),",
            "    TimeGrainConstants.WEEK_STARTING_MONDAY: __(\"Week starting Monday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SATURDAY: __(\"Week ending Saturday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SUNDAY: __(\"Week ending Sunday\"),",
            "}",
            "",
            "",
            "class TimestampExpression(",
            "    ColumnClause",
            "):  # pylint: disable=abstract-method, too-many-ancestors",
            "    def __init__(self, expr: str, col: ColumnClause, **kwargs: Any) -> None:",
            "        \"\"\"Sqlalchemy class that can be used to render native column elements respecting",
            "        engine-specific quoting rules as part of a string-based expression.",
            "",
            "        :param expr: Sql expression with '{col}' denoting the locations where the col",
            "        object will be rendered.",
            "        :param col: the target column",
            "        \"\"\"",
            "        super().__init__(expr, **kwargs)",
            "        self.col = col",
            "",
            "    @property",
            "    def _constructor(self) -> ColumnClause:",
            "        # Needed to ensure that the column label is rendered correctly when",
            "        # proxied to the outer query.",
            "        # See https://github.com/sqlalchemy/sqlalchemy/issues/4730",
            "        return ColumnClause",
            "",
            "",
            "@compiles(TimestampExpression)",
            "def compile_timegrain_expression(",
            "    element: TimestampExpression, compiler: Compiled, **kwargs: Any",
            ") -> str:",
            "    return element.name.replace(\"{col}\", compiler.process(element.col, **kwargs))",
            "",
            "",
            "class LimitMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"Enum the ways that limits can be applied\"\"\"",
            "",
            "    FETCH_MANY = \"fetch_many\"",
            "    WRAP_SQL = \"wrap_sql\"",
            "    FORCE_LIMIT = \"force_limit\"",
            "",
            "",
            "class MetricType(TypedDict, total=False):",
            "    \"\"\"",
            "    Type for metrics return by `get_metrics`.",
            "    \"\"\"",
            "",
            "    metric_name: str",
            "    expression: str",
            "    verbose_name: str | None",
            "    metric_type: str | None",
            "    description: str | None",
            "    d3format: str | None",
            "    currency: str | None",
            "    warning_text: str | None",
            "    extra: str | None",
            "",
            "",
            "class BaseEngineSpec:  # pylint: disable=too-many-public-methods",
            "    \"\"\"Abstract class for database engine specific configurations",
            "",
            "    Attributes:",
            "        allows_alias_to_source_column: Whether the engine is able to pick the",
            "                                       source column for aggregation clauses",
            "                                       used in ORDER BY when a column in SELECT",
            "                                       has an alias that is the same as a source",
            "                                       column.",
            "        allows_hidden_orderby_agg:     Whether the engine allows ORDER BY to",
            "                                       directly use aggregation clauses, without",
            "                                       having to add the same aggregation in SELECT.",
            "    \"\"\"",
            "",
            "    engine_name: str | None = None  # for user messages, overridden in child classes",
            "",
            "    # These attributes map the DB engine spec to one or more SQLAlchemy dialects/drivers;",
            "    # see the ``supports_url`` and ``supports_backend`` methods below.",
            "    engine = \"base\"  # str as defined in sqlalchemy.engine.engine",
            "    engine_aliases: set[str] = set()",
            "    drivers: dict[str, str] = {}",
            "    default_driver: str | None = None",
            "",
            "    # placeholder with the SQLAlchemy URI template",
            "    sqlalchemy_uri_placeholder = (",
            "        \"engine+driver://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "",
            "    disable_ssh_tunneling = False",
            "",
            "    _date_trunc_functions: dict[str, str] = {}",
            "    _time_grain_expressions: dict[str | None, str] = {}",
            "    _default_column_type_mappings: tuple[ColumnTypeMapping, ...] = (",
            "        (",
            "            re.compile(r\"^string\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^n((var)?char|text)\", re.IGNORECASE),",
            "            types.UnicodeText(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(var)?char\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(tiny|medium|long)?text\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallint\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^int(eger)?\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigint\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^long\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^decimal\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^numeric\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^float\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^double\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^real\", re.IGNORECASE),",
            "            types.REAL,",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallserial\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^serial\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigserial\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^money\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^timestamp\", re.IGNORECASE),",
            "            types.TIMESTAMP(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^datetime\", re.IGNORECASE),",
            "            types.DateTime(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^date\", re.IGNORECASE),",
            "            types.Date(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^time\", re.IGNORECASE),",
            "            types.Time(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^interval\", re.IGNORECASE),",
            "            types.Interval(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^bool(ean)?\", re.IGNORECASE),",
            "            types.Boolean(),",
            "            GenericDataType.BOOLEAN,",
            "        ),",
            "    )",
            "    # engine-specific type mappings to check prior to the defaults",
            "    column_type_mappings: tuple[ColumnTypeMapping, ...] = ()",
            "",
            "    # type-specific functions to mutate values received from the database.",
            "    # Needed on certain databases that return values in an unexpected format",
            "    column_type_mutators: dict[TypeEngine, Callable[[Any], Any]] = {}",
            "",
            "    # Does database support join-free timeslot grouping",
            "    time_groupby_inline = False",
            "    limit_method = LimitMethod.FORCE_LIMIT",
            "    allows_joins = True",
            "    allows_subqueries = True",
            "    allows_alias_in_select = True",
            "    allows_alias_in_orderby = True",
            "    allows_sql_comments = True",
            "    allows_escaped_colons = True",
            "",
            "    # Whether ORDER BY clause can use aliases created in SELECT",
            "    # that are the same as a source column",
            "    allows_alias_to_source_column = True",
            "",
            "    # Whether ORDER BY clause must appear in SELECT",
            "    # if True, then it doesn't have to.",
            "    allows_hidden_orderby_agg = True",
            "",
            "    # Whether ORDER BY clause can use sql calculated expression",
            "    # if True, use alias of select column for `order by`",
            "    # the True is safely for most database",
            "    # But for backward compatibility, False by default",
            "    allows_hidden_cc_in_orderby = False",
            "",
            "    # Whether allow CTE as subquery or regular CTE",
            "    # If True, then it will allow  in subquery ,",
            "    # if False it will allow as regular CTE",
            "    allows_cte_in_subquery = True",
            "    # Define alias for CTE",
            "    cte_alias = \"__cte\"",
            "    # Whether allow LIMIT clause in the SQL",
            "    # If True, then the database engine is allowed for LIMIT clause",
            "    # If False, then the database engine is allowed for TOP clause",
            "    allow_limit_clause = True",
            "    # This set will give keywords for select statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    select_keywords: set[str] = {\"SELECT\"}",
            "    # This set will give the keywords for data limit statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    top_keywords: set[str] = {\"TOP\"}",
            "    # A set of disallowed connection query parameters by driver name",
            "    disallow_uri_query_params: dict[str, set[str]] = {}",
            "    # A Dict of query parameters that will always be used on every connection",
            "    # by driver name",
            "    enforce_uri_query_params: dict[str, dict[str, Any]] = {}",
            "",
            "    force_column_alias_quotes = False",
            "    arraysize = 0",
            "    max_column_name_length: int | None = None",
            "    try_remove_schema_from_table_name = True  # pylint: disable=invalid-name",
            "    run_multiple_statements_as_one = False",
            "    custom_errors: dict[",
            "        Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]",
            "    ] = {}",
            "",
            "    # Whether the engine supports file uploads",
            "    # if True, database will be listed as option in the upload file form",
            "    supports_file_upload = True",
            "",
            "    # Is the DB engine spec able to change the default schema? This requires implementing",
            "    # a custom `adjust_engine_params` method.",
            "    supports_dynamic_schema = False",
            "",
            "    # Does the DB support catalogs? A catalog here is a group of schemas, and has",
            "    # different names depending on the DB: BigQuery calles it a \"project\", Postgres calls",
            "    # it a \"database\", Trino calls it a \"catalog\", etc.",
            "    supports_catalog = False",
            "",
            "    # Can the catalog be changed on a per-query basis?",
            "    supports_dynamic_catalog = False",
            "",
            "    @classmethod",
            "    def get_allows_alias_in_select(",
            "        cls, database: Database  # pylint: disable=unused-argument",
            "    ) -> bool:",
            "        \"\"\"",
            "        Method for dynamic `allows_alias_in_select`.",
            "",
            "        In Dremio this atribute is version-dependent, so Superset needs to inspect the",
            "        database configuration in order to determine it. This method allows engine-specs",
            "        to define dynamic values for the attribute.",
            "        \"\"\"",
            "        return cls.allows_alias_in_select",
            "",
            "    @classmethod",
            "    def supports_url(cls, url: URL) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy URL.",
            "",
            "        As an example, if a given DB engine spec has:",
            "",
            "            class PostgresDBEngineSpec:",
            "                engine = \"postgresql\"",
            "                engine_aliases = \"postgres\"",
            "                drivers = {",
            "                    \"psycopg2\": \"The default Postgres driver\",",
            "                    \"asyncpg\": \"An asynchronous Postgres driver\",",
            "                }",
            "",
            "        It would be used for all the following SQLAlchemy URIs:",
            "",
            "            - postgres://user:password@host/db",
            "            - postgresql://user:password@host/db",
            "            - postgres+asyncpg://user:password@host/db",
            "            - postgres+psycopg2://user:password@host/db",
            "            - postgresql+asyncpg://user:password@host/db",
            "            - postgresql+psycopg2://user:password@host/db",
            "",
            "        Note that SQLAlchemy has a default driver even if one is not specified:",
            "",
            "            >>> from sqlalchemy.engine.url import make_url",
            "            >>> make_url('postgres://').get_driver_name()",
            "            'psycopg2'",
            "",
            "        \"\"\"",
            "        backend = url.get_backend_name()",
            "        driver = url.get_driver_name()",
            "        return cls.supports_backend(backend, driver)",
            "",
            "    @classmethod",
            "    def supports_backend(cls, backend: str, driver: str | None = None) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy backend/driver.",
            "        \"\"\"",
            "        # check the backend first",
            "        if backend != cls.engine and backend not in cls.engine_aliases:",
            "            return False",
            "",
            "        # originally DB engine specs didn't declare any drivers and the check was made",
            "        # only on the engine; if that's the case, ignore the driver for backwards",
            "        # compatibility",
            "        if not cls.drivers or driver is None:",
            "            return True",
            "",
            "        return driver in cls.drivers",
            "",
            "    @classmethod",
            "    def get_default_schema(cls, database: Database) -> str | None:",
            "        \"\"\"",
            "        Return the default schema in a given database.",
            "        \"\"\"",
            "        with database.get_inspector_with_context() as inspector:",
            "            return inspector.default_schema_name",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the schema configured in a SQLALchemy URI and connection arguments, if any.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This is used to determine the schema of tables that aren't fully qualified, eg:",
            "",
            "            SELECT * FROM foo;",
            "",
            "        In the example above, the schema where the `foo` table lives depends on a few",
            "        factors:",
            "",
            "            1. For DB engine specs that allow dynamically changing the schema based on the",
            "               query we should use the query schema.",
            "            2. For DB engine specs that don't support dynamically changing the schema and",
            "               have the schema hardcoded in the SQLAlchemy URI we should use the schema",
            "               from the URI.",
            "            3. For DB engine specs that don't connect to a specific schema and can't",
            "               change it dynamically we need to probe the database for the default schema.",
            "",
            "        Determining the correct schema is crucial for managing access to data, so please",
            "        make sure you understand this logic when working on a new DB engine spec.",
            "        \"\"\"",
            "        # dynamic schema varies on a per-query basis",
            "        if cls.supports_dynamic_schema:",
            "            return query.schema",
            "",
            "        # check if the schema is stored in the SQLAlchemy URI or connection arguments",
            "        try:",
            "            connect_args = database.get_extra()[\"engine_params\"][\"connect_args\"]",
            "        except KeyError:",
            "            connect_args = {}",
            "        sqlalchemy_uri = make_url_safe(database.sqlalchemy_uri)",
            "        if schema := cls.get_schema_from_engine_params(sqlalchemy_uri, connect_args):",
            "            return schema",
            "",
            "        # return the default schema of the database",
            "        return cls.get_default_schema(database)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific exceptions into",
            "        Superset DBAPI exceptions",
            "",
            "        Note: On python 3.9 this method can be changed to a classmethod property",
            "        without the need of implementing a metaclass type",
            "",
            "        :return: A map of driver specific exception to superset custom exceptions",
            "        \"\"\"",
            "        return {}",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific parser method",
            "",
            "        :return: An Exception with a parsed string off the original exception",
            "        \"\"\"",
            "        return exception",
            "",
            "    @classmethod",
            "    def get_dbapi_mapped_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Get a superset custom DBAPI exception from the driver specific exception.",
            "",
            "        Override if the engine needs to perform extra changes to the exception, for",
            "        example change the exception message or implement custom more complex logic",
            "",
            "        :param exception: The driver specific exception",
            "        :return: Superset custom DBAPI exception",
            "        \"\"\"",
            "        new_exception = cls.get_dbapi_exception_mapping().get(type(exception))",
            "        if not new_exception:",
            "            return cls.parse_error_exception(exception)",
            "        return new_exception(str(exception))",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(  # pylint: disable=unused-argument",
            "        cls,",
            "        extra: dict[str, Any],",
            "    ) -> bool:",
            "        return False",
            "",
            "    @classmethod",
            "    def get_text_clause(cls, clause: str) -> TextClause:",
            "        \"\"\"",
            "        SQLAlchemy wrapper to ensure text clauses are escaped properly",
            "",
            "        :param clause: string clause with potentially unescaped characters",
            "        :return: text clause with escaped characters",
            "        \"\"\"",
            "        if cls.allows_escaped_colons:",
            "            clause = clause.replace(\":\", \"\\\\:\")",
            "        return text(clause)",
            "",
            "    @classmethod",
            "    def get_engine(",
            "        cls,",
            "        database: Database,",
            "        schema: str | None = None,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> ContextManager[Engine]:",
            "        \"\"\"",
            "        Return an engine context manager.",
            "",
            "            >>> with DBEngineSpec.get_engine(database, schema, source) as engine:",
            "            ...     connection = engine.connect()",
            "            ...     connection.execute(sql)",
            "",
            "        \"\"\"",
            "        return database.get_sqla_engine_with_context(schema=schema, source=source)",
            "",
            "    @classmethod",
            "    def get_timestamp_expr(",
            "        cls,",
            "        col: ColumnClause,",
            "        pdf: str | None,",
            "        time_grain: str | None,",
            "    ) -> TimestampExpression:",
            "        \"\"\"",
            "        Construct a TimestampExpression to be used in a SQLAlchemy query.",
            "",
            "        :param col: Target column for the TimestampExpression",
            "        :param pdf: date format (seconds or milliseconds)",
            "        :param time_grain: time grain, e.g. P1Y for 1 year",
            "        :return: TimestampExpression object",
            "        \"\"\"",
            "        if time_grain:",
            "            type_ = str(getattr(col, \"type\", \"\"))",
            "            time_expr = cls.get_time_grain_expressions().get(time_grain)",
            "            if not time_expr:",
            "                raise NotImplementedError(",
            "                    f\"No grain spec for {time_grain} for database {cls.engine}\"",
            "                )",
            "            if type_ and \"{func}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{func}\", date_trunc_function)",
            "            if type_ and \"{type}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{type}\", type_)",
            "        else:",
            "            time_expr = \"{col}\"",
            "",
            "        # if epoch, translate to DATE using db specific conf",
            "        if pdf == \"epoch_s\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_to_dttm())",
            "        elif pdf == \"epoch_ms\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_ms_to_dttm())",
            "",
            "        return TimestampExpression(time_expr, col, type_=col.type)",
            "",
            "    @classmethod",
            "    def get_time_grains(cls) -> tuple[TimeGrain, ...]:",
            "        \"\"\"",
            "        Generate a tuple of supported time grains.",
            "",
            "        :return: All time grains supported by the engine",
            "        \"\"\"",
            "",
            "        ret_list = []",
            "        time_grains = builtin_time_grains.copy()",
            "        time_grains.update(current_app.config[\"TIME_GRAIN_ADDONS\"])",
            "        for duration, func in cls.get_time_grain_expressions().items():",
            "            if duration in time_grains:",
            "                name = time_grains[duration]",
            "                ret_list.append(TimeGrain(name, _(name), func, duration))",
            "        return tuple(ret_list)",
            "",
            "    @classmethod",
            "    def _sort_time_grains(",
            "        cls, val: tuple[str | None, str], index: int",
            "    ) -> float | int | str:",
            "        \"\"\"",
            "        Return an ordered time-based value of a portion of a time grain",
            "        for sorting",
            "        Values are expected to be either None or start with P or PT",
            "        Have a numerical value in the middle and end with",
            "        a value for the time interval",
            "        It can also start or end with epoch start time denoting a range",
            "        i.e, week beginning or ending with a day",
            "        \"\"\"",
            "        pos = {",
            "            \"FIRST\": 0,",
            "            \"SECOND\": 1,",
            "            \"THIRD\": 2,",
            "            \"LAST\": 3,",
            "        }",
            "",
            "        if val[0] is None:",
            "            return pos[\"FIRST\"]",
            "",
            "        prog = re.compile(r\"(.*\\/)?(P|PT)([0-9\\.]+)(S|M|H|D|W|M|Y)(\\/.*)?\")",
            "        result = prog.match(val[0])",
            "",
            "        # for any time grains that don't match the format, put them at the end",
            "        if result is None:",
            "            return pos[\"LAST\"]",
            "",
            "        second_minute_hour = [\"S\", \"M\", \"H\"]",
            "        day_week_month_year = [\"D\", \"W\", \"M\", \"Y\"]",
            "        is_less_than_day = result.group(2) == \"PT\"",
            "        interval = result.group(4)",
            "        epoch_time_start_string = result.group(1) or result.group(5)",
            "        has_starting_or_ending = bool(len(epoch_time_start_string or \"\"))",
            "",
            "        def sort_day_week() -> int:",
            "            if has_starting_or_ending:",
            "                return pos[\"LAST\"]",
            "            if is_less_than_day:",
            "                return pos[\"SECOND\"]",
            "            return pos[\"THIRD\"]",
            "",
            "        def sort_interval() -> float:",
            "            if is_less_than_day:",
            "                return second_minute_hour.index(interval)",
            "            return day_week_month_year.index(interval)",
            "",
            "        # 0: all \"PT\" values should come before \"P\" values (i.e, PT10M)",
            "        # 1: order values within the above arrays (\"D\" before \"W\")",
            "        # 2: sort by numeric value (PT10M before PT15M)",
            "        # 3: sort by any week starting/ending values",
            "        plist = {",
            "            0: sort_day_week(),",
            "            1: pos[\"SECOND\"] if is_less_than_day else pos[\"THIRD\"],",
            "            2: sort_interval(),",
            "            3: float(result.group(3)),",
            "        }",
            "",
            "        return plist.get(index, 0)",
            "",
            "    @classmethod",
            "    def get_time_grain_expressions(cls) -> dict[str | None, str]:",
            "        \"\"\"",
            "        Return a dict of all supported time grains including any potential added grains",
            "        but excluding any potentially disabled grains in the config file.",
            "",
            "        :return: All time grain expressions supported by the engine",
            "        \"\"\"",
            "        # TODO: use @memoize decorator or similar to avoid recomputation on every call",
            "        time_grain_expressions = cls._time_grain_expressions.copy()",
            "        grain_addon_expressions = current_app.config[\"TIME_GRAIN_ADDON_EXPRESSIONS\"]",
            "        time_grain_expressions.update(grain_addon_expressions.get(cls.engine, {}))",
            "        denylist: list[str] = current_app.config[\"TIME_GRAIN_DENYLIST\"]",
            "        for key in denylist:",
            "            time_grain_expressions.pop(key, None)",
            "",
            "        return dict(",
            "            sorted(",
            "                time_grain_expressions.items(),",
            "                key=lambda x: (",
            "                    cls._sort_time_grains(x, 0),",
            "                    cls._sort_time_grains(x, 1),",
            "                    cls._sort_time_grains(x, 2),",
            "                    cls._sort_time_grains(x, 3),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "",
            "        :param cursor: Cursor instance",
            "        :param limit: Maximum number of rows to be returned by the cursor",
            "        :return: Result of query",
            "        \"\"\"",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            if cls.limit_method == LimitMethod.FETCH_MANY and limit:",
            "                return cursor.fetchmany(limit)",
            "            data = cursor.fetchall()",
            "            description = cursor.description or []",
            "            # Create a mapping between column name and a mutator function to normalize",
            "            # values with. The first two items in the description row are",
            "            # the column name and type.",
            "            column_mutators = {",
            "                row[0]: func",
            "                for row in description",
            "                if (",
            "                    func := cls.column_type_mutators.get(",
            "                        type(cls.get_sqla_column_type(cls.get_datatype(row[1])))",
            "                    )",
            "                )",
            "            }",
            "            if column_mutators:",
            "                indexes = {row[0]: idx for idx, row in enumerate(description)}",
            "                for row_idx, row in enumerate(data):",
            "                    new_row = list(row)",
            "                    for col, func in column_mutators.items():",
            "                        col_idx = indexes[col]",
            "                        new_row[col_idx] = func(row[col_idx])",
            "                    data[row_idx] = tuple(new_row)",
            "",
            "            return data",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def expand_data(",
            "        cls, columns: list[ResultSetColumnType], data: list[dict[Any, Any]]",
            "    ) -> tuple[",
            "        list[ResultSetColumnType], list[dict[Any, Any]], list[ResultSetColumnType]",
            "    ]:",
            "        \"\"\"",
            "        Some engines support expanding nested fields. See implementation in Presto",
            "        spec for details.",
            "",
            "        :param columns: columns selected in the query",
            "        :param data: original data set",
            "        :return: list of all columns(selected columns and their nested fields),",
            "                 expanded data set, listed of nested fields",
            "        \"\"\"",
            "        return columns, data, []",
            "",
            "    @classmethod",
            "    def alter_new_orm_column(cls, orm_col: TableColumn) -> None:",
            "        \"\"\"Allow altering default column attributes when first detected/added",
            "",
            "        For instance special column like `__time` for Druid can be",
            "        set to is_dttm=True. Note that this only gets called when new",
            "        columns are detected/created\"\"\"",
            "        # TODO: Fix circular import caused by importing TableColumn",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (seconds) to datetime that can be used in a",
            "        query. The reference column should be denoted as `{col}` in the return",
            "        expression, e.g. \"FROM_UNIXTIME({col})\"",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (milliseconds) to datetime that can be used",
            "        in a query.",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        return cls.epoch_to_dttm().replace(\"{col}\", \"({col}/1000)\")",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        \"\"\"",
            "        Change column type code from cursor description to string representation.",
            "",
            "        :param type_code: Type code from cursor description",
            "        :return: String representation of type code",
            "        \"\"\"",
            "        if isinstance(type_code, str) and type_code != \"\":",
            "            return type_code.upper()",
            "        return None",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        noop by default",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        return indexes",
            "",
            "    @classmethod",
            "    def extra_table_metadata(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Returns engine-specific table metadata",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name",
            "        :param schema_name: Schema name",
            "        :return: Engine-specific table metadata",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        return {}",
            "",
            "    @classmethod",
            "    def apply_limit_to_sql(",
            "        cls, sql: str, limit: int, database: Database, force: bool = False",
            "    ) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a LIMIT clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param database: Database instance",
            "        :return: SQL query with limit clause",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        if cls.limit_method == LimitMethod.WRAP_SQL:",
            "            sql = sql.strip(\"\\t\\n ;\")",
            "            qry = (",
            "                select(\"*\")",
            "                .select_from(TextAsFrom(text(sql), [\"*\"]).alias(\"inner_qry\"))",
            "                .limit(limit)",
            "            )",
            "            return database.compile_sqla_query(qry)",
            "",
            "        if cls.limit_method == LimitMethod.FORCE_LIMIT:",
            "            parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "            sql = parsed_query.set_or_update_query_limit(limit, force=force)",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def apply_top_to_sql(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a TOP clause",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param sql: SQL query",
            "        :return: SQL query with top clause",
            "        \"\"\"",
            "",
            "        cte = None",
            "        sql_remainder = None",
            "        sql = sql.strip(\" \\t\\n;\")",
            "        sql_statement = sqlparse.format(sql, strip_comments=True)",
            "        query_limit: int | None = sql_parse.extract_top_from_query(",
            "            sql_statement, cls.top_keywords",
            "        )",
            "        if not limit:",
            "            final_limit = query_limit",
            "        elif int(query_limit or 0) < limit and query_limit is not None:",
            "            final_limit = query_limit",
            "        else:",
            "            final_limit = limit",
            "        if not cls.allows_cte_in_subquery:",
            "            cte, sql_remainder = sql_parse.get_cte_remainder_query(sql_statement)",
            "        if cte:",
            "            str_statement = str(sql_remainder)",
            "            cte = cte + \"\\n\"",
            "        else:",
            "            cte = \"\"",
            "            str_statement = str(sql)",
            "        str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "",
            "        tokens = str_statement.rstrip().split(\" \")",
            "        tokens = [token for token in tokens if token]",
            "        if cls.top_not_in_sql(str_statement):",
            "            selects = [",
            "                i",
            "                for i, word in enumerate(tokens)",
            "                if word.upper() in cls.select_keywords",
            "            ]",
            "            first_select = selects[0]",
            "            if tokens[first_select + 1].upper() == \"DISTINCT\":",
            "                first_select += 1",
            "",
            "            tokens.insert(first_select + 1, \"TOP\")",
            "            tokens.insert(first_select + 2, str(final_limit))",
            "",
            "        next_is_limit_token = False",
            "        new_tokens = []",
            "",
            "        for token in tokens:",
            "            if token in cls.top_keywords:",
            "                next_is_limit_token = True",
            "            elif next_is_limit_token:",
            "                if token.isdigit():",
            "                    token = str(final_limit)",
            "                    next_is_limit_token = False",
            "            new_tokens.append(token)",
            "        sql = \" \".join(new_tokens)",
            "        return cte + sql",
            "",
            "    @classmethod",
            "    def top_not_in_sql(cls, sql: str) -> bool:",
            "        for top_word in cls.top_keywords:",
            "            if top_word.upper() in sql.upper():",
            "                return False",
            "        return True",
            "",
            "    @classmethod",
            "    def get_limit_from_sql(cls, sql: str) -> int | None:",
            "        \"\"\"",
            "        Extract limit from SQL query",
            "",
            "        :param sql: SQL query",
            "        :return: Value of limit clause in query",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.limit",
            "",
            "    @classmethod",
            "    def set_or_update_query_limit(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Create a query based on original query but with new limit clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: New limit to insert/replace into query",
            "        :return: Query with new limit",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.set_or_update_query_limit(limit)",
            "",
            "    @classmethod",
            "    def get_cte_query(cls, sql: str) -> str | None:",
            "        \"\"\"",
            "        Convert the input CTE based SQL to the SQL for virtual table conversion",
            "",
            "        :param sql: SQL query",
            "        :return: CTE with the main select query aliased as `__cte`",
            "",
            "        \"\"\"",
            "        if not cls.allows_cte_in_subquery:",
            "            stmt = sqlparse.parse(sql)[0]",
            "",
            "            # The first meaningful token for CTE will be with WITH",
            "            idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "            if not (token and token.ttype == CTE):",
            "                return None",
            "            idx, token = stmt.token_next(idx)",
            "            idx = stmt.token_index(token) + 1",
            "",
            "            # extract rest of the SQLs after CTE",
            "            remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "            return f\"WITH {token.value},\\n{cls.cte_alias} AS (\\n{remainder}\\n)\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: Database,",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        For regular engines this calls the `pandas.DataFrame.to_sql` method. Can be",
            "        overridden for engines that don't work well with this method, e.g. Hive and",
            "        BigQuery.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "",
            "        to_sql_kwargs[\"name\"] = table.table",
            "",
            "        if table.schema:",
            "            # Only add schema when it is preset and non-empty.",
            "            to_sql_kwargs[\"schema\"] = table.schema",
            "",
            "        with cls.get_engine(database) as engine:",
            "            if engine.dialect.supports_multivalues_insert:",
            "                to_sql_kwargs[\"method\"] = \"multi\"",
            "",
            "            df.to_sql(con=engine, **to_sql_kwargs)",
            "",
            "    @classmethod",
            "    def convert_dttm(  # pylint: disable=unused-argument",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Convert a Python `datetime` object to a SQL expression.",
            "",
            "        :param target_type: The target type of expression",
            "        :param dttm: The datetime object",
            "        :param db_extra: The database extra object",
            "        :return: The SQL expression",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Any, query: Query) -> None:",
            "        \"\"\"Handle a live cursor between the execute and fetchall calls",
            "",
            "        The flow works without this method doing anything, but it allows",
            "        for handling the cursor and updating progress information in the",
            "        query object\"\"\"",
            "        # TODO: Fix circular import error caused by importing sql_lab.Query",
            "",
            "    @classmethod",
            "    def execute_with_cursor(cls, cursor: Any, sql: str, query: Query) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        For most implementations this just makes calls to `execute` and",
            "        `handle_cursor` consecutively, but in some engines (e.g. Trino) we may",
            "        need to handle client limitations such as lack of async support and",
            "        perform a more complicated operation to get information from the cursor",
            "        in a timely manner and facilitate operations such as query stop",
            "        \"\"\"",
            "        logger.debug(\"Query %d: Running query: %s\", query.id, sql)",
            "        cls.execute(cursor, sql, async_=True)",
            "        logger.debug(\"Query %d: Handling cursor\", query.id)",
            "        cls.handle_cursor(cursor, query)",
            "",
            "    @classmethod",
            "    def extract_error_message(cls, ex: Exception) -> str:",
            "        return f\"{cls.engine} error: {cls._extract_error_message(ex)}\"",
            "",
            "    @classmethod",
            "    def _extract_error_message(cls, ex: Exception) -> str:",
            "        \"\"\"Extract error message for queries\"\"\"",
            "        return utils.error_msg_from_exception(ex)",
            "",
            "    @classmethod",
            "    def extract_errors(",
            "        cls, ex: Exception, context: dict[str, Any] | None = None",
            "    ) -> list[SupersetError]:",
            "        raw_message = cls._extract_error_message(ex)",
            "",
            "        context = context or {}",
            "        for regex, (message, error_type, extra) in cls.custom_errors.items():",
            "            if match := regex.search(raw_message):",
            "                params = {**context, **match.groupdict()}",
            "                extra[\"engine_name\"] = cls.engine_name",
            "                return [",
            "                    SupersetError(",
            "                        error_type=error_type,",
            "                        message=message % params,",
            "                        level=ErrorLevel.ERROR,",
            "                        extra=extra,",
            "                    )",
            "                ]",
            "",
            "        return [",
            "            SupersetError(",
            "                error_type=SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "                message=cls._extract_error_message(ex),",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"engine_name\": cls.engine_name},",
            "            )",
            "        ]",
            "",
            "    @classmethod",
            "    def adjust_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        uri: URL,",
            "        connect_args: dict[str, Any],",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> tuple[URL, dict[str, Any]]:",
            "        \"\"\"",
            "        Return a new URL and ``connect_args`` for a specific catalog/schema.",
            "",
            "        This is used in SQL Lab, allowing users to select a schema from the list of",
            "        schemas available in a given database, and have the query run with that schema as",
            "        the default one.",
            "",
            "        For some databases (like MySQL, Presto, Snowflake) this requires modifying the",
            "        SQLAlchemy URI before creating the connection. For others (like Postgres), it",
            "        requires additional parameters in ``connect_args`` or running pre-session",
            "        queries with ``set`` parameters.",
            "",
            "        When a DB engine spec implements this method or ``get_prequeries`` (see below) it",
            "        should also have the attribute ``supports_dynamic_schema`` set to true, so that",
            "        Superset knows in which schema a given query is running in order to enforce",
            "        permissions (see #23385 and #23401).",
            "",
            "        Currently, changing the catalog is not supported. The method accepts a catalog so",
            "        that when catalog support is added to Superset the interface remains the same.",
            "        This is important because DB engine specs can be installed from 3rd party",
            "        packages, so we want to keep these methods as stable as possible.",
            "        \"\"\"",
            "        return uri, {",
            "            **connect_args,",
            "            **cls.enforce_uri_query_params.get(uri.get_driver_name(), {}),",
            "        }",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,  # pylint: disable=unused-argument",
            "        schema: str | None = None,  # pylint: disable=unused-argument",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return pre-session queries.",
            "",
            "        These are currently used as an alternative to ``adjust_engine_params`` for",
            "        databases where the selected schema cannot be specified in the SQLAlchemy URI or",
            "        connection arguments.",
            "",
            "        For example, in order to specify a default schema in RDS we need to run a query",
            "        at the beginning of the session:",
            "",
            "            sql> set search_path = my_schema;",
            "",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def patch(cls) -> None:",
            "        \"\"\"",
            "        TODO: Improve docstring and refactor implementation in Hive",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def get_catalog_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs from database.",
            "",
            "        This needs to be implemented per database, since SQLAlchemy doesn't offer an",
            "        abstraction.",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def get_schema_names(cls, inspector: Inspector) -> list[str]:",
            "        \"\"\"",
            "        Get all schemas from database",
            "",
            "        :param inspector: SqlAlchemy inspector",
            "        :return: All schemas in the database",
            "        \"\"\"",
            "        return sorted(inspector.get_schema_names())",
            "",
            "    @classmethod",
            "    def get_table_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the real table names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The physical table names",
            "        \"\"\"",
            "",
            "        try:",
            "            tables = set(inspector.get_table_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            tables = {re.sub(f\"^{schema}\\\\.\", \"\", table) for table in tables}",
            "        return tables",
            "",
            "    @classmethod",
            "    def get_view_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the view names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The view names",
            "        \"\"\"",
            "",
            "        try:",
            "            views = set(inspector.get_view_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            views = {re.sub(f\"^{schema}\\\\.\", \"\", view) for view in views}",
            "        return views",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,  # pylint: disable=unused-argument",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return inspector.get_indexes(table_name, schema)",
            "",
            "    @classmethod",
            "    def get_table_comment(",
            "        cls, inspector: Inspector, table_name: str, schema: str | None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Get comment of table from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :return: comment of table",
            "        \"\"\"",
            "        comment = None",
            "        try:",
            "            comment = inspector.get_table_comment(table_name, schema)",
            "            comment = comment.get(\"text\") if isinstance(comment, dict) else None",
            "        except NotImplementedError:",
            "            # It's expected that some dialects don't implement the comment method",
            "            pass",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.error(\"Unexpected error while fetching table comment\", exc_info=True)",
            "            logger.exception(ex)",
            "        return comment",
            "",
            "    @classmethod",
            "    def get_columns(  # pylint: disable=unused-argument",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Get all columns from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :param options: Extra options to customise the display of columns in",
            "                        some databases",
            "        :return: All columns in table",
            "        \"\"\"",
            "        return convert_inspector_columns(",
            "            cast(list[SQLAColumnType], inspector.get_columns(table_name, schema))",
            "        )",
            "",
            "    @classmethod",
            "    def get_metrics(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[MetricType]:",
            "        \"\"\"",
            "        Get all metrics from a given schema and table.",
            "        \"\"\"",
            "        return [",
            "            {",
            "                \"metric_name\": \"count\",",
            "                \"verbose_name\": \"COUNT(*)\",",
            "                \"metric_type\": \"count\",",
            "                \"expression\": \"COUNT(*)\",",
            "            }",
            "        ]",
            "",
            "    @classmethod",
            "    def where_latest_partition(  # pylint: disable=too-many-arguments,unused-argument",
            "        cls,",
            "        table_name: str,",
            "        schema: str | None,",
            "        database: Database,",
            "        query: Select,",
            "        columns: list[ResultSetColumnType] | None = None,",
            "    ) -> Select | None:",
            "        \"\"\"",
            "        Add a where clause to a query to reference only the most recent partition",
            "",
            "        :param table_name: Table name",
            "        :param schema: Schema name",
            "        :param database: Database instance",
            "        :param query: SqlAlchemy query",
            "        :param columns: List of TableColumns",
            "        :return: SqlAlchemy query with additional where clause referencing the latest",
            "        partition",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database, TableColumn",
            "        return None",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        return [",
            "            literal_column(query_as)",
            "            if (query_as := c.get(\"query_as\"))",
            "            else column(c[\"column_name\"])",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments,too-many-locals",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: str | None = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: list[ResultSetColumnType] | None = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Generate a \"SELECT * from [schema.]table_name\" query with appropriate limit.",
            "",
            "        WARNING: expects only unquoted table and schema names.",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name, unquoted",
            "        :param engine: SqlAlchemy Engine instance",
            "        :param schema: Schema, unquoted",
            "        :param limit: limit to impose on query",
            "        :param show_cols: Show columns in query; otherwise use \"*\"",
            "        :param indent: Add indentation to query",
            "        :param latest_partition: Only query the latest partition",
            "        :param cols: Columns to include in query",
            "        :return: SQL query",
            "        \"\"\"",
            "        # pylint: disable=redefined-outer-name",
            "        fields: str | list[Any] = \"*\"",
            "        cols = cols or []",
            "        if (show_cols or latest_partition) and not cols:",
            "            cols = database.get_columns(table_name, schema)",
            "",
            "        if show_cols:",
            "            fields = cls._get_fields(cols)",
            "        quote = engine.dialect.identifier_preparer.quote",
            "        quote_schema = engine.dialect.identifier_preparer.quote_schema",
            "        if schema:",
            "            full_table_name = quote_schema(schema) + \".\" + quote(table_name)",
            "        else:",
            "            full_table_name = quote(table_name)",
            "",
            "        qry = select(fields).select_from(text(full_table_name))",
            "",
            "        if limit and cls.allow_limit_clause:",
            "            qry = qry.limit(limit)",
            "        if latest_partition:",
            "            partition_query = cls.where_latest_partition(",
            "                table_name, schema, database, qry, columns=cols",
            "            )",
            "            if partition_query is not None:",
            "                qry = partition_query",
            "        sql = database.compile_sqla_query(qry)",
            "        if indent:",
            "            sql = sqlparse.format(sql, reindent=True)",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        \"\"\"",
            "        Generate a SQL query that estimates the cost of a given statement.",
            "",
            "        :param statement: A single SQL statement",
            "        :param cursor: Cursor instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        \"\"\"",
            "        Format cost estimate.",
            "",
            "        :param raw_cost: Raw estimate from `estimate_query_cost`",
            "        :return: Human readable cost estimate",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def process_statement(cls, statement: str, database: Database) -> str:",
            "        \"\"\"",
            "        Process a SQL statement by stripping and mutating it.",
            "",
            "        :param statement: A single SQL statement",
            "        :param database: Database instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        parsed_query = ParsedQuery(statement, engine=cls.engine)",
            "        sql = parsed_query.stripped()",
            "        sql_query_mutator = current_app.config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = current_app.config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=database,",
            "            )",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: Database,",
            "        schema: str,",
            "        sql: str,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise Exception(  # pylint: disable=broad-exception-raised",
            "                \"Database does not support cost estimation\"",
            "            )",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        statements = parsed_query.get_statements()",
            "",
            "        costs = []",
            "        with database.get_raw_connection(schema=schema, source=source) as conn:",
            "            cursor = conn.cursor()",
            "            for statement in statements:",
            "                processed_statement = cls.process_statement(statement, database)",
            "                costs.append(cls.estimate_statement_cost(processed_statement, cursor))",
            "",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        if impersonate_user and username is not None:",
            "            url = url.set(username=username)",
            "",
            "        return url",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "",
            "        :param connect_args: config to be updated",
            "        :param uri: URI",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def execute(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: str,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"",
            "        Execute a SQL query",
            "",
            "        :param cursor: Cursor instance",
            "        :param query: Query to execute",
            "        :param kwargs: kwargs to be passed to cursor.execute()",
            "        :return:",
            "        \"\"\"",
            "        if not cls.allows_sql_comments:",
            "            query = sql_parse.strip_comments_from_sql(query, engine=cls.engine)",
            "        disallowed_functions = current_app.config[\"DISALLOWED_SQL_FUNCTIONS\"].get(",
            "            cls.engine, set()",
            "        )",
            "        if sql_parse.check_sql_functions_exist(query, disallowed_functions, cls.engine):",
            "            raise DisallowedSQLFunction(disallowed_functions)",
            "",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            cursor.execute(query)",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def make_label_compatible(cls, label: str) -> str | quoted_name:",
            "        \"\"\"",
            "        Conditionally mutate and/or quote a sqlalchemy expression label. If",
            "        force_column_alias_quotes is set to True, return the label as a",
            "        sqlalchemy.sql.elements.quoted_name object to ensure that the select query",
            "        and query results have same case. Otherwise, return the mutated label as a",
            "        regular string. If maximum supported column name length is exceeded,",
            "        generate a truncated label by calling truncate_label().",
            "",
            "        :param label: expected expression label/alias",
            "        :return: conditionally mutated label supported by the db engine",
            "        \"\"\"",
            "        label_mutated = cls._mutate_label(label)",
            "        if (",
            "            cls.max_column_name_length",
            "            and len(label_mutated) > cls.max_column_name_length",
            "        ):",
            "            label_mutated = cls._truncate_label(label)",
            "        if cls.force_column_alias_quotes:",
            "            label_mutated = quoted_name(label_mutated, True)",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def get_column_types(",
            "        cls,",
            "        column_type: str | None,",
            "    ) -> tuple[TypeEngine, GenericDataType] | None:",
            "        \"\"\"",
            "        Return a sqlalchemy native column type and generic data type that corresponds",
            "        to the column type defined in the data source (return None to use default type",
            "        inferred by SQLAlchemy). Override `column_type_mappings` for specific needs",
            "        (see MSSQL for example of NCHAR/NVARCHAR handling).",
            "",
            "        :param column_type: Column type returned by inspector",
            "        :return: SQLAlchemy and generic Superset column types",
            "        \"\"\"",
            "        if not column_type:",
            "            return None",
            "",
            "        for regex, sqla_type, generic_type in (",
            "            cls.column_type_mappings + cls._default_column_type_mappings",
            "        ):",
            "            match = regex.match(column_type)",
            "            if not match:",
            "                continue",
            "            if callable(sqla_type):",
            "                return sqla_type(match), generic_type",
            "            return sqla_type, generic_type",
            "        return None",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        Most engines support mixed case aliases that can include numbers",
            "        and special characters, like commas, parentheses etc. For engines that",
            "        have restrictions on what types of aliases are supported, this method",
            "        can be overridden to ensure that labels conform to the engine's",
            "        limitations. Mutated labels should be deterministic (input label A always",
            "        yields output label X) and unique (input labels A and B don't yield the same",
            "        output label X).",
            "",
            "        :param label: Preferred expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        return label",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"",
            "        In the case that a label exceeds the max length supported by the engine,",
            "        this method is used to construct a deterministic and unique label based on",
            "        the original label. By default, this returns a md5 hash of the original label,",
            "        conditionally truncated if the length of the hash exceeds the max column length",
            "        of the engine.",
            "",
            "        :param label: Expected expression label",
            "        :return: Truncated label",
            "        \"\"\"",
            "        label = md5_sha_from_str(label)",
            "        # truncate hash if it exceeds max length",
            "        if cls.max_column_name_length and len(label) > cls.max_column_name_length:",
            "            label = label[: cls.max_column_name_length]",
            "        return label",
            "",
            "    @classmethod",
            "    def column_datatype_to_string(",
            "        cls, sqla_column_type: TypeEngine, dialect: Dialect",
            "    ) -> str:",
            "        \"\"\"",
            "        Convert sqlalchemy column type to string representation.",
            "        By default, removes collation and character encoding info to avoid",
            "        unnecessarily long datatypes.",
            "",
            "        :param sqla_column_type: SqlAlchemy column type",
            "        :param dialect: Sqlalchemy dialect",
            "        :return: Compiled column type",
            "        \"\"\"",
            "        sqla_column_type = sqla_column_type.copy()",
            "        if hasattr(sqla_column_type, \"collation\"):",
            "            sqla_column_type.collation = None",
            "        if hasattr(sqla_column_type, \"charset\"):",
            "            sqla_column_type.charset = None",
            "        return sqla_column_type.compile(dialect=dialect).upper()",
            "",
            "    @classmethod",
            "    def get_function_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get a list of function names that are able to be called on the database.",
            "        Used for SQL Lab autocomplete.",
            "",
            "        :param database: The database to get functions for",
            "        :return: A list of function names useable in the database",
            "        \"\"\"",
            "        return []",
            "",
            "    @staticmethod",
            "    def pyodbc_rows_to_tuples(data: list[Any]) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "        Convert pyodbc.Row objects from `fetch_data` to tuples.",
            "",
            "        :param data: List of tuples or pyodbc.Row objects",
            "        :return: List of tuples",
            "        \"\"\"",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [tuple(row) for row in data]",
            "        return data",
            "",
            "    @staticmethod",
            "    def mutate_db_for_connection_test(  # pylint: disable=unused-argument",
            "        database: Database,",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require passing additional parameters for validating database",
            "        connections. This method makes it possible to mutate the database instance prior",
            "        to testing if a connection is ok.",
            "",
            "        :param database: instance to be mutated",
            "        \"\"\"",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = {}",
            "        if database.extra:",
            "            try:",
            "                extra = json.loads(database.extra)",
            "            except json.JSONDecodeError as ex:",
            "                logger.error(ex, exc_info=True)",
            "                raise ex",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(  # pylint: disable=invalid-name",
            "        database: Database, params: dict[str, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require some sensitive information which do not conform to",
            "        the username:password syntax normally used by SQLAlchemy.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :param params: params to be updated",
            "        \"\"\"",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            params.update(encrypted_extra)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def is_readonly_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"Pessimistic readonly, 100% sure statement won't mutate anything\"\"\"",
            "        return (",
            "            parsed_query.is_select()",
            "            or parsed_query.is_explain()",
            "            or parsed_query.is_show()",
            "        )",
            "",
            "    @classmethod",
            "    def is_select_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"",
            "        Determine if the statement should be considered as SELECT statement.",
            "        Some query dialects do not contain \"SELECT\" word in queries (eg. Kusto)",
            "        \"\"\"",
            "        return parsed_query.is_select()",
            "",
            "    @classmethod",
            "    def get_column_spec(  # pylint: disable=unused-argument",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> ColumnSpec | None:",
            "        \"\"\"",
            "        Get generic type related specs regarding a native column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        if col_types := cls.get_column_types(native_type):",
            "            column_type, generic_type = col_types",
            "            is_dttm = generic_type == GenericDataType.TEMPORAL",
            "            return ColumnSpec(",
            "                sqla_type=column_type, generic_type=generic_type, is_dttm=is_dttm",
            "            )",
            "        return None",
            "",
            "    @classmethod",
            "    def get_sqla_column_type(",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> TypeEngine | None:",
            "        \"\"\"",
            "        Converts native database type to sqlalchemy column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        column_spec = cls.get_column_spec(",
            "            native_type=native_type,",
            "            db_extra=db_extra,",
            "            source=source,",
            "        )",
            "        return column_spec.sqla_type if column_spec else None",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query) -> None:",
            "        \"\"\"",
            "        Some databases may acquire the query cancelation id after the query",
            "        cancelation request has been received. For those cases, the db engine spec",
            "        can record the cancelation intent so that the query can either be stopped",
            "        prior to execution, or canceled once the query id is acquired.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def has_implicit_cancel(cls) -> bool:",
            "        \"\"\"",
            "        Return True if the live cursor handles the implicit cancelation of the query,",
            "        False otherwise.",
            "",
            "        :return: Whether the live cursor implicitly cancels the query",
            "        :see: handle_cursor",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Select identifiers from the database engine that uniquely identifies the",
            "        queries to cancel. The identifier is typically a session id, process id",
            "        or similar.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Query identifier",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def cancel_query(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "        cancel_query_id: str,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Value returned by get_cancel_query_payload or set in",
            "        other life-cycle methods of the query",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def parse_sql(cls, sql: str) -> list[str]:",
            "        return [str(s).strip(\" ;\") for s in sqlparse.parse(sql)]",
            "",
            "    @classmethod",
            "    def get_impersonation_key(cls, user: User | None) -> Any:",
            "        \"\"\"",
            "        Construct an impersonation key, by default it's the given username.",
            "",
            "        :param user: logged-in user",
            "",
            "        :returns: username if given user is not null",
            "        \"\"\"",
            "        return user.username if user else None",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: str | None) -> str | None:",
            "        \"\"\"",
            "        Mask ``encrypted_extra``.",
            "",
            "        This is used to remove any sensitive data in ``encrypted_extra`` when presenting",
            "        it to the user. For example, a private key might be replaced with a masked value",
            "        \"XXXXXXXXXX\". If the masked value is changed the corresponding entry is updated,",
            "        otherwise the old value is used (see ``unmask_encrypted_extra`` below).",
            "        \"\"\"",
            "        return encrypted_extra",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def unmask_encrypted_extra(cls, old: str | None, new: str | None) -> str | None:",
            "        \"\"\"",
            "        Remove masks from ``encrypted_extra``.",
            "",
            "        This method allows reusing existing values from the current encrypted extra on",
            "        updates. It's useful for reusing masked passwords, allowing keys to be updated",
            "        without having to provide sensitive data to the client.",
            "        \"\"\"",
            "        return new",
            "",
            "    @classmethod",
            "    def get_public_information(cls) -> dict[str, Any]:",
            "        \"\"\"",
            "        Construct a Dict with properties we want to expose.",
            "",
            "        :returns: Dict with properties of our class like supports_file_upload",
            "        and disable_ssh_tunneling",
            "        \"\"\"",
            "        return {",
            "            \"supports_file_upload\": cls.supports_file_upload,",
            "            \"disable_ssh_tunneling\": cls.disable_ssh_tunneling,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_database_uri(cls, sqlalchemy_uri: URL) -> None:",
            "        \"\"\"",
            "        Validates a database SQLAlchemy URI per engine spec.",
            "        Use this to implement a final validation for unwanted connection configuration",
            "",
            "        :param sqlalchemy_uri:",
            "        \"\"\"",
            "        if existing_disallowed := cls.disallow_uri_query_params.get(",
            "            sqlalchemy_uri.get_driver_name(), set()",
            "        ).intersection(sqlalchemy_uri.query):",
            "            raise ValueError(f\"Forbidden query parameter(s): {existing_disallowed}\")",
            "",
            "    @classmethod",
            "    def denormalize_name(cls, dialect: Dialect, name: str) -> str:",
            "        if (",
            "            hasattr(dialect, \"requires_name_normalize\")",
            "            and dialect.requires_name_normalize",
            "        ):",
            "            return dialect.denormalize_name(name)",
            "",
            "        return name",
            "",
            "",
            "# schema for adding a database by providing parameters instead of the",
            "# full SQLAlchemy URI",
            "class BasicParametersSchema(Schema):",
            "    username = fields.String(",
            "        required=True, allow_none=True, metadata={\"description\": __(\"Username\")}",
            "    )",
            "    password = fields.String(allow_none=True, metadata={\"description\": __(\"Password\")})",
            "    host = fields.String(",
            "        required=True, metadata={\"description\": __(\"Hostname or IP address\")}",
            "    )",
            "    port = fields.Integer(",
            "        required=True,",
            "        metadata={\"description\": __(\"Database port\")},",
            "        validate=Range(min=0, max=2**16, max_inclusive=False),",
            "    )",
            "    database = fields.String(",
            "        required=True, metadata={\"description\": __(\"Database name\")}",
            "    )",
            "    query = fields.Dict(",
            "        keys=fields.Str(),",
            "        values=fields.Raw(),",
            "        metadata={\"description\": __(\"Additional parameters\")},",
            "    )",
            "    encryption = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an encrypted connection to the database\")},",
            "    )",
            "    ssh = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an ssh tunnel connection to the database\")},",
            "    )",
            "",
            "",
            "class BasicParametersType(TypedDict, total=False):",
            "    username: str | None",
            "    password: str | None",
            "    host: str",
            "    port: int",
            "    database: str",
            "    query: dict[str, Any]",
            "    encryption: bool",
            "",
            "",
            "class BasicPropertiesType(TypedDict):",
            "    parameters: BasicParametersType",
            "",
            "",
            "class BasicParametersMixin:",
            "    \"\"\"",
            "    Mixin for configuring DB engine specs via a dictionary.",
            "",
            "    With this mixin the SQLAlchemy engine can be configured through",
            "    individual parameters, instead of the full SQLAlchemy URI. This",
            "    mixin is for the most common pattern of URI:",
            "",
            "        engine+driver://user:password@host:port/dbname[?key=value&key=value...]",
            "",
            "    \"\"\"",
            "",
            "    # schema describing the parameters used to configure the DB",
            "    parameters_schema = BasicParametersSchema()",
            "",
            "    # recommended driver name for the DB engine spec",
            "    default_driver = \"\"",
            "",
            "    # query parameter to enable encryption in the database connection",
            "    # for Postgres this would be `{\"sslmode\": \"verify-ca\"}`, eg.",
            "    encryption_parameters: dict[str, str] = {}",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(  # pylint: disable=unused-argument",
            "        cls,",
            "        parameters: BasicParametersType,",
            "        encrypted_extra: dict[str, str] | None = None,",
            "    ) -> str:",
            "        # make a copy so that we don't update the original",
            "        query = parameters.get(\"query\", {}).copy()",
            "        if parameters.get(\"encryption\"):",
            "            if not cls.encryption_parameters:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Unable to build a URL with encryption enabled\"",
            "                )",
            "            query.update(cls.encryption_parameters)",
            "",
            "        return str(",
            "            URL.create(",
            "                f\"{cls.engine}+{cls.default_driver}\".rstrip(\"+\"),  # type: ignore",
            "                username=parameters.get(\"username\"),",
            "                password=parameters.get(\"password\"),",
            "                host=parameters[\"host\"],",
            "                port=parameters[\"port\"],",
            "                database=parameters[\"database\"],",
            "                query=query,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(  # pylint: disable=unused-argument",
            "        cls, uri: str, encrypted_extra: dict[str, Any] | None = None",
            "    ) -> BasicParametersType:",
            "        url = make_url_safe(uri)",
            "        query = {",
            "            key: value",
            "            for (key, value) in url.query.items()",
            "            if (key, value) not in cls.encryption_parameters.items()",
            "        }",
            "        encryption = all(",
            "            item in url.query.items() for item in cls.encryption_parameters.items()",
            "        )",
            "        return {",
            "            \"username\": url.username,",
            "            \"password\": url.password,",
            "            \"host\": url.host,",
            "            \"port\": url.port,",
            "            \"database\": url.database,",
            "            \"query\": query,",
            "            \"encryption\": encryption,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls, properties: BasicPropertiesType",
            "    ) -> list[SupersetError]:",
            "        \"\"\"",
            "        Validates any number of parameters, for progressive validation.",
            "",
            "        If only the hostname is present it will check if the name is resolvable. As more",
            "        parameters are present in the request, more validation is done.",
            "        \"\"\"",
            "        errors: list[SupersetError] = []",
            "",
            "        required = {\"host\", \"port\", \"username\", \"database\"}",
            "        parameters = properties.get(\"parameters\", {})",
            "        present = {key for key in parameters if parameters.get(key, ())}",
            "",
            "        if missing := sorted(required - present):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=f'One or more parameters are missing: {\", \".join(missing)}',",
            "                    error_type=SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR,",
            "                    level=ErrorLevel.WARNING,",
            "                    extra={\"missing\": missing},",
            "                ),",
            "            )",
            "",
            "        host = parameters.get(\"host\", None)",
            "        if not host:",
            "            return errors",
            "        if not is_hostname_valid(host):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The hostname provided can't be resolved.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"host\"]},",
            "                ),",
            "            )",
            "            return errors",
            "",
            "        port = parameters.get(\"port\", None)",
            "        if not port:",
            "            return errors",
            "        try:",
            "            port = int(port)",
            "        except (ValueError, TypeError):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"Port must be a valid integer.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        if not (isinstance(port, int) and 0 <= port < 2**16):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=(",
            "                        \"The port must be an integer between 0 and 65535 \"",
            "                        \"(inclusive).\"",
            "                    ),",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        elif not is_port_open(host, port):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The port is closed.\",",
            "                    error_type=SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "",
            "        return errors",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.2\",",
            "            plugins=[MarshmallowPlugin()],",
            "        )",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "superset.db_engine_specs.base.BaseEngineSpec.execute_with_cursor"
        ]
    },
    "superset/db_engine_specs/trino.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from typing import Any, TYPE_CHECKING"
            },
            "1": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " import simplejson as json"
            },
            "3": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from flask import current_app"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+from flask import current_app, Flask"
            },
            "5": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from sqlalchemy.engine.reflection import Inspector"
            },
            "6": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from sqlalchemy.engine.url import URL"
            },
            "7": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " from sqlalchemy.exc import NoSuchTableError"
            },
            "8": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "         execute_result: dict[str, Any] = {}"
            },
            "9": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "         execute_event = threading.Event()"
            },
            "10": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 208,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        def _execute(results: dict[str, Any], event: threading.Event) -> None:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+        def _execute("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+            results: dict[str, Any], event: threading.Event, app: Flask"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+        ) -> None:"
            },
            "15": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "             logger.debug(\"Query %d: Running query: %s\", query_id, sql)"
            },
            "16": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": 213,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "             try:"
            },
            "18": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                cls.execute(cursor, sql)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 215,
                "PatchRowcode": "+                with app.app_context():"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+                    cls.execute(cursor, sql)"
            },
            "21": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "             except Exception as ex:  # pylint: disable=broad-except"
            },
            "22": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "                 results[\"error\"] = ex"
            },
            "23": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "             finally:"
            },
            "24": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 220,
                "PatchRowcode": "                 event.set()"
            },
            "25": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 221,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 222,
                "PatchRowcode": "         execute_thread = threading.Thread("
            },
            "27": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 223,
                "PatchRowcode": "             target=_execute,"
            },
            "28": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            args=(execute_result, execute_event),"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 224,
                "PatchRowcode": "+            args=("
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+                execute_result,"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+                execute_event,"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 227,
                "PatchRowcode": "+                # pylint: disable=protected-access"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 228,
                "PatchRowcode": "+                current_app._get_current_object(),"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 229,
                "PatchRowcode": "+            ),"
            },
            "35": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "         )"
            },
            "36": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 231,
                "PatchRowcode": "         execute_thread.start()"
            },
            "37": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 232,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import contextlib",
            "import logging",
            "import threading",
            "import time",
            "from typing import Any, TYPE_CHECKING",
            "",
            "import simplejson as json",
            "from flask import current_app",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.exc import NoSuchTableError",
            "",
            "from superset import db",
            "from superset.constants import QUERY_CANCEL_KEY, QUERY_EARLY_CANCEL_KEY, USER_AGENT",
            "from superset.databases.utils import make_url_safe",
            "from superset.db_engine_specs.base import BaseEngineSpec",
            "from superset.db_engine_specs.exceptions import (",
            "    SupersetDBAPIConnectionError,",
            "    SupersetDBAPIDatabaseError,",
            "    SupersetDBAPIOperationalError,",
            "    SupersetDBAPIProgrammingError,",
            ")",
            "from superset.db_engine_specs.presto import PrestoBaseEngineSpec",
            "from superset.models.sql_lab import Query",
            "from superset.superset_typing import ResultSetColumnType",
            "from superset.utils import core as utils",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database",
            "",
            "    with contextlib.suppress(ImportError):  # trino may not be installed",
            "        from trino.dbapi import Cursor",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class TrinoEngineSpec(PrestoBaseEngineSpec):",
            "    engine = \"trino\"",
            "    engine_name = \"Trino\"",
            "    allows_alias_to_source_column = False",
            "",
            "    @classmethod",
            "    def extra_table_metadata(",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        metadata = {}",
            "",
            "        if indexes := database.get_indexes(table_name, schema_name):",
            "            col_names, latest_parts = cls.latest_partition(",
            "                table_name,",
            "                schema_name,",
            "                database,",
            "                show_first=True,",
            "                indexes=indexes,",
            "            )",
            "",
            "            if not latest_parts:",
            "                latest_parts = tuple([None] * len(col_names))",
            "",
            "            metadata[\"partitions\"] = {",
            "                \"cols\": sorted(",
            "                    list(",
            "                        {",
            "                            column_name",
            "                            for index in indexes",
            "                            if index.get(\"name\") == \"partition\"",
            "                            for column_name in index.get(\"column_names\", [])",
            "                        }",
            "                    )",
            "                ),",
            "                \"latest\": dict(zip(col_names, latest_parts)),",
            "                \"partitionQuery\": cls._partition_query(",
            "                    table_name=table_name,",
            "                    schema=schema_name,",
            "                    indexes=indexes,",
            "                    database=database,",
            "                ),",
            "            }",
            "",
            "        if database.has_view_by_name(table_name, schema_name):",
            "            with database.get_inspector_with_context() as inspector:",
            "                metadata[\"view\"] = inspector.get_view_definition(",
            "                    table_name, schema_name",
            "                )",
            "",
            "        return metadata",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "        :param connect_args: config to be updated",
            "        :param uri: URI string",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "        url = make_url_safe(uri)",
            "        backend_name = url.get_backend_name()",
            "",
            "        # Must be Trino connection, enable impersonation, and set optional param",
            "        # auth=LDAP|KERBEROS",
            "        # Set principal_username=$effective_username",
            "        if backend_name == \"trino\" and username is not None:",
            "            connect_args[\"user\"] = username",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        # Do nothing and let update_impersonation_config take care of impersonation",
            "        return url",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(cls, extra: dict[str, Any]) -> bool:",
            "        return True",
            "",
            "    @classmethod",
            "    def get_tracking_url(cls, cursor: Cursor) -> str | None:",
            "        try:",
            "            return cursor.info_uri",
            "        except AttributeError:",
            "            with contextlib.suppress(AttributeError):",
            "                conn = cursor.connection",
            "                # pylint: disable=protected-access, line-too-long",
            "                return f\"{conn.http_scheme}://{conn.host}:{conn.port}/ui/query.html?{cursor._query.query_id}\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Cursor, query: Query) -> None:",
            "        \"\"\"",
            "        Handle a trino client cursor.",
            "",
            "        WARNING: if you execute a query, it will block until complete and you",
            "        will not be able to handle the cursor until complete. Use",
            "        `execute_with_cursor` instead, to handle this asynchronously.",
            "        \"\"\"",
            "",
            "        # Adds the executed query id to the extra payload so the query can be cancelled",
            "        cancel_query_id = cursor.query_id",
            "        logger.debug(\"Query %d: queryId %s found in cursor\", query.id, cancel_query_id)",
            "        query.set_extra_json_key(key=QUERY_CANCEL_KEY, value=cancel_query_id)",
            "",
            "        if tracking_url := cls.get_tracking_url(cursor):",
            "            query.tracking_url = tracking_url",
            "",
            "        db.session.commit()",
            "",
            "        # if query cancelation was requested prior to the handle_cursor call, but",
            "        # the query was still executed, trigger the actual query cancelation now",
            "        if query.extra.get(QUERY_EARLY_CANCEL_KEY):",
            "            cls.cancel_query(",
            "                cursor=cursor,",
            "                query=query,",
            "                cancel_query_id=cancel_query_id,",
            "            )",
            "",
            "        super().handle_cursor(cursor=cursor, query=query)",
            "",
            "    @classmethod",
            "    def execute_with_cursor(cls, cursor: Cursor, sql: str, query: Query) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        Trino's client blocks until the query is complete, so we need to run it",
            "        in another thread and invoke `handle_cursor` to poll for the query ID",
            "        to appear on the cursor in parallel.",
            "        \"\"\"",
            "        # Fetch the query ID beforehand, since it might fail inside the thread due to",
            "        # how the SQLAlchemy session is handled.",
            "        query_id = query.id",
            "",
            "        execute_result: dict[str, Any] = {}",
            "        execute_event = threading.Event()",
            "",
            "        def _execute(results: dict[str, Any], event: threading.Event) -> None:",
            "            logger.debug(\"Query %d: Running query: %s\", query_id, sql)",
            "",
            "            try:",
            "                cls.execute(cursor, sql)",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                results[\"error\"] = ex",
            "            finally:",
            "                event.set()",
            "",
            "        execute_thread = threading.Thread(",
            "            target=_execute,",
            "            args=(execute_result, execute_event),",
            "        )",
            "        execute_thread.start()",
            "",
            "        # Wait for a query ID to be available before handling the cursor, as",
            "        # it's required by that method; it may never become available on error.",
            "        while not cursor.query_id and not execute_event.is_set():",
            "            time.sleep(0.1)",
            "",
            "        logger.debug(\"Query %d: Handling cursor\", query_id)",
            "        cls.handle_cursor(cursor, query)",
            "",
            "        # Block until the query completes; same behaviour as the client itself",
            "        logger.debug(\"Query %d: Waiting for query to complete\", query_id)",
            "        execute_event.wait()",
            "",
            "        # Unfortunately we'll mangle the stack trace due to the thread, but",
            "        # throwing the original exception allows mapping database errors as normal",
            "        if err := execute_result.get(\"error\"):",
            "            raise err",
            "",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query) -> None:",
            "        if QUERY_CANCEL_KEY not in query.extra:",
            "            query.set_extra_json_key(QUERY_EARLY_CANCEL_KEY, True)",
            "            db.session.commit()",
            "",
            "    @classmethod",
            "    def cancel_query(cls, cursor: Cursor, query: Query, cancel_query_id: str) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Trino `queryId`",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "        try:",
            "            cursor.execute(",
            "                f\"CALL system.runtime.kill_query(query_id => '{cancel_query_id}',\"",
            "                \"message => 'Query cancelled by Superset')\"",
            "            )",
            "            cursor.fetchall()  # needed to trigger the call",
            "        except Exception:  # pylint: disable=broad-except",
            "            return False",
            "",
            "        return True",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = BaseEngineSpec.get_extra_params(database)",
            "        engine_params: dict[str, Any] = extra.setdefault(\"engine_params\", {})",
            "        connect_args: dict[str, Any] = engine_params.setdefault(\"connect_args\", {})",
            "",
            "        connect_args.setdefault(\"source\", USER_AGENT)",
            "",
            "        if database.server_cert:",
            "            connect_args[\"http_scheme\"] = \"https\"",
            "            connect_args[\"verify\"] = utils.create_ssl_cert_file(database.server_cert)",
            "",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(",
            "        database: Database,",
            "        params: dict[str, Any],",
            "    ) -> None:",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            auth_method = encrypted_extra.pop(\"auth_method\", None)",
            "            auth_params = encrypted_extra.pop(\"auth_params\", {})",
            "            if not auth_method:",
            "                return",
            "",
            "            connect_args = params.setdefault(\"connect_args\", {})",
            "            connect_args[\"http_scheme\"] = \"https\"",
            "            # pylint: disable=import-outside-toplevel",
            "            if auth_method == \"basic\":",
            "                from trino.auth import BasicAuthentication as trino_auth  # noqa",
            "            elif auth_method == \"kerberos\":",
            "                from trino.auth import KerberosAuthentication as trino_auth  # noqa",
            "            elif auth_method == \"certificate\":",
            "                from trino.auth import CertificateAuthentication as trino_auth  # noqa",
            "            elif auth_method == \"jwt\":",
            "                from trino.auth import JWTAuthentication as trino_auth  # noqa",
            "            else:",
            "                allowed_extra_auths = current_app.config[",
            "                    \"ALLOWED_EXTRA_AUTHENTICATIONS\"",
            "                ].get(\"trino\", {})",
            "                if auth_method in allowed_extra_auths:",
            "                    trino_auth = allowed_extra_auths.get(auth_method)",
            "                else:",
            "                    raise ValueError(",
            "                        f\"For security reason, custom authentication '{auth_method}' \"",
            "                        f\"must be listed in 'ALLOWED_EXTRA_AUTHENTICATIONS' config\"",
            "                    )",
            "",
            "            connect_args[\"auth\"] = trino_auth(**auth_params)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from requests import exceptions as requests_exceptions",
            "        from trino import exceptions as trino_exceptions",
            "",
            "        static_mapping: dict[type[Exception], type[Exception]] = {",
            "            requests_exceptions.ConnectionError: SupersetDBAPIConnectionError,",
            "        }",
            "",
            "        class _CustomMapping(dict[type[Exception], type[Exception]]):",
            "            def get(  # type: ignore[override]",
            "                self, item: type[Exception], default: type[Exception] | None = None",
            "            ) -> type[Exception] | None:",
            "                if static := static_mapping.get(item):",
            "                    return static",
            "                if issubclass(item, trino_exceptions.InternalError):",
            "                    return SupersetDBAPIDatabaseError",
            "                if issubclass(item, trino_exceptions.OperationalError):",
            "                    return SupersetDBAPIOperationalError",
            "                if issubclass(item, trino_exceptions.ProgrammingError):",
            "                    return SupersetDBAPIProgrammingError",
            "                return default",
            "",
            "        return _CustomMapping()",
            "",
            "    @classmethod",
            "    def _expand_columns(cls, col: ResultSetColumnType) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Expand the given column out to one or more columns by analysing their types,",
            "        descending into ROWS and expanding out their inner fields recursively.",
            "",
            "        We can only navigate named fields in ROWs in this way, so we can't expand out",
            "        MAP or ARRAY types, nor fields in ROWs which have no name (in fact the trino",
            "        library doesn't correctly parse unnamed fields in ROWs). We won't be able to",
            "        expand ROWs which are nested underneath any of those types, either.",
            "",
            "        Expanded columns are named foo.bar.baz and we provide a query_as property to",
            "        instruct the base engine spec how to correctly query them: instead of quoting",
            "        the whole string they have to be quoted like \"foo\".\"bar\".\"baz\" and we then",
            "        alias them to the full dotted string for ease of reference.",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from trino.sqlalchemy import datatype",
            "",
            "        cols = [col]",
            "        col_type = col.get(\"type\")",
            "",
            "        if not isinstance(col_type, datatype.ROW):",
            "            return cols",
            "",
            "        for inner_name, inner_type in col_type.attr_types:",
            "            outer_name = col[\"name\"]",
            "            name = \".\".join([outer_name, inner_name])",
            "            query_name = \".\".join([f'\"{piece}\"' for piece in name.split(\".\")])",
            "            column_spec = cls.get_column_spec(str(inner_type))",
            "            is_dttm = column_spec.is_dttm if column_spec else False",
            "",
            "            inner_col = ResultSetColumnType(",
            "                name=name,",
            "                column_name=name,",
            "                type=inner_type,",
            "                is_dttm=is_dttm,",
            "                query_as=f'{query_name} AS \"{name}\"',",
            "            )",
            "            cols.extend(cls._expand_columns(inner_col))",
            "",
            "        return cols",
            "",
            "    @classmethod",
            "    def get_columns(",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        If the \"expand_rows\" feature is enabled on the database via",
            "        \"schema_options\", expand the schema definition out to show all",
            "        subfields of nested ROWs as their appropriate dotted paths.",
            "        \"\"\"",
            "        base_cols = super().get_columns(inspector, table_name, schema, options)",
            "        if not (options or {}).get(\"expand_rows\"):",
            "            return base_cols",
            "",
            "        return [col for base_col in base_cols for col in cls._expand_columns(base_col)]",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        Trino dialect raises NoSuchTableError in get_indexes if table is empty.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "        try:",
            "            return super().get_indexes(database, inspector, table_name, schema)",
            "        except NoSuchTableError:",
            "            return []"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import contextlib",
            "import logging",
            "import threading",
            "import time",
            "from typing import Any, TYPE_CHECKING",
            "",
            "import simplejson as json",
            "from flask import current_app, Flask",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.exc import NoSuchTableError",
            "",
            "from superset import db",
            "from superset.constants import QUERY_CANCEL_KEY, QUERY_EARLY_CANCEL_KEY, USER_AGENT",
            "from superset.databases.utils import make_url_safe",
            "from superset.db_engine_specs.base import BaseEngineSpec",
            "from superset.db_engine_specs.exceptions import (",
            "    SupersetDBAPIConnectionError,",
            "    SupersetDBAPIDatabaseError,",
            "    SupersetDBAPIOperationalError,",
            "    SupersetDBAPIProgrammingError,",
            ")",
            "from superset.db_engine_specs.presto import PrestoBaseEngineSpec",
            "from superset.models.sql_lab import Query",
            "from superset.superset_typing import ResultSetColumnType",
            "from superset.utils import core as utils",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database",
            "",
            "    with contextlib.suppress(ImportError):  # trino may not be installed",
            "        from trino.dbapi import Cursor",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class TrinoEngineSpec(PrestoBaseEngineSpec):",
            "    engine = \"trino\"",
            "    engine_name = \"Trino\"",
            "    allows_alias_to_source_column = False",
            "",
            "    @classmethod",
            "    def extra_table_metadata(",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        metadata = {}",
            "",
            "        if indexes := database.get_indexes(table_name, schema_name):",
            "            col_names, latest_parts = cls.latest_partition(",
            "                table_name,",
            "                schema_name,",
            "                database,",
            "                show_first=True,",
            "                indexes=indexes,",
            "            )",
            "",
            "            if not latest_parts:",
            "                latest_parts = tuple([None] * len(col_names))",
            "",
            "            metadata[\"partitions\"] = {",
            "                \"cols\": sorted(",
            "                    list(",
            "                        {",
            "                            column_name",
            "                            for index in indexes",
            "                            if index.get(\"name\") == \"partition\"",
            "                            for column_name in index.get(\"column_names\", [])",
            "                        }",
            "                    )",
            "                ),",
            "                \"latest\": dict(zip(col_names, latest_parts)),",
            "                \"partitionQuery\": cls._partition_query(",
            "                    table_name=table_name,",
            "                    schema=schema_name,",
            "                    indexes=indexes,",
            "                    database=database,",
            "                ),",
            "            }",
            "",
            "        if database.has_view_by_name(table_name, schema_name):",
            "            with database.get_inspector_with_context() as inspector:",
            "                metadata[\"view\"] = inspector.get_view_definition(",
            "                    table_name, schema_name",
            "                )",
            "",
            "        return metadata",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "        :param connect_args: config to be updated",
            "        :param uri: URI string",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "        url = make_url_safe(uri)",
            "        backend_name = url.get_backend_name()",
            "",
            "        # Must be Trino connection, enable impersonation, and set optional param",
            "        # auth=LDAP|KERBEROS",
            "        # Set principal_username=$effective_username",
            "        if backend_name == \"trino\" and username is not None:",
            "            connect_args[\"user\"] = username",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        # Do nothing and let update_impersonation_config take care of impersonation",
            "        return url",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(cls, extra: dict[str, Any]) -> bool:",
            "        return True",
            "",
            "    @classmethod",
            "    def get_tracking_url(cls, cursor: Cursor) -> str | None:",
            "        try:",
            "            return cursor.info_uri",
            "        except AttributeError:",
            "            with contextlib.suppress(AttributeError):",
            "                conn = cursor.connection",
            "                # pylint: disable=protected-access, line-too-long",
            "                return f\"{conn.http_scheme}://{conn.host}:{conn.port}/ui/query.html?{cursor._query.query_id}\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Cursor, query: Query) -> None:",
            "        \"\"\"",
            "        Handle a trino client cursor.",
            "",
            "        WARNING: if you execute a query, it will block until complete and you",
            "        will not be able to handle the cursor until complete. Use",
            "        `execute_with_cursor` instead, to handle this asynchronously.",
            "        \"\"\"",
            "",
            "        # Adds the executed query id to the extra payload so the query can be cancelled",
            "        cancel_query_id = cursor.query_id",
            "        logger.debug(\"Query %d: queryId %s found in cursor\", query.id, cancel_query_id)",
            "        query.set_extra_json_key(key=QUERY_CANCEL_KEY, value=cancel_query_id)",
            "",
            "        if tracking_url := cls.get_tracking_url(cursor):",
            "            query.tracking_url = tracking_url",
            "",
            "        db.session.commit()",
            "",
            "        # if query cancelation was requested prior to the handle_cursor call, but",
            "        # the query was still executed, trigger the actual query cancelation now",
            "        if query.extra.get(QUERY_EARLY_CANCEL_KEY):",
            "            cls.cancel_query(",
            "                cursor=cursor,",
            "                query=query,",
            "                cancel_query_id=cancel_query_id,",
            "            )",
            "",
            "        super().handle_cursor(cursor=cursor, query=query)",
            "",
            "    @classmethod",
            "    def execute_with_cursor(cls, cursor: Cursor, sql: str, query: Query) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        Trino's client blocks until the query is complete, so we need to run it",
            "        in another thread and invoke `handle_cursor` to poll for the query ID",
            "        to appear on the cursor in parallel.",
            "        \"\"\"",
            "        # Fetch the query ID beforehand, since it might fail inside the thread due to",
            "        # how the SQLAlchemy session is handled.",
            "        query_id = query.id",
            "",
            "        execute_result: dict[str, Any] = {}",
            "        execute_event = threading.Event()",
            "",
            "        def _execute(",
            "            results: dict[str, Any], event: threading.Event, app: Flask",
            "        ) -> None:",
            "            logger.debug(\"Query %d: Running query: %s\", query_id, sql)",
            "",
            "            try:",
            "                with app.app_context():",
            "                    cls.execute(cursor, sql)",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                results[\"error\"] = ex",
            "            finally:",
            "                event.set()",
            "",
            "        execute_thread = threading.Thread(",
            "            target=_execute,",
            "            args=(",
            "                execute_result,",
            "                execute_event,",
            "                # pylint: disable=protected-access",
            "                current_app._get_current_object(),",
            "            ),",
            "        )",
            "        execute_thread.start()",
            "",
            "        # Wait for a query ID to be available before handling the cursor, as",
            "        # it's required by that method; it may never become available on error.",
            "        while not cursor.query_id and not execute_event.is_set():",
            "            time.sleep(0.1)",
            "",
            "        logger.debug(\"Query %d: Handling cursor\", query_id)",
            "        cls.handle_cursor(cursor, query)",
            "",
            "        # Block until the query completes; same behaviour as the client itself",
            "        logger.debug(\"Query %d: Waiting for query to complete\", query_id)",
            "        execute_event.wait()",
            "",
            "        # Unfortunately we'll mangle the stack trace due to the thread, but",
            "        # throwing the original exception allows mapping database errors as normal",
            "        if err := execute_result.get(\"error\"):",
            "            raise err",
            "",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query) -> None:",
            "        if QUERY_CANCEL_KEY not in query.extra:",
            "            query.set_extra_json_key(QUERY_EARLY_CANCEL_KEY, True)",
            "            db.session.commit()",
            "",
            "    @classmethod",
            "    def cancel_query(cls, cursor: Cursor, query: Query, cancel_query_id: str) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Trino `queryId`",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "        try:",
            "            cursor.execute(",
            "                f\"CALL system.runtime.kill_query(query_id => '{cancel_query_id}',\"",
            "                \"message => 'Query cancelled by Superset')\"",
            "            )",
            "            cursor.fetchall()  # needed to trigger the call",
            "        except Exception:  # pylint: disable=broad-except",
            "            return False",
            "",
            "        return True",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = BaseEngineSpec.get_extra_params(database)",
            "        engine_params: dict[str, Any] = extra.setdefault(\"engine_params\", {})",
            "        connect_args: dict[str, Any] = engine_params.setdefault(\"connect_args\", {})",
            "",
            "        connect_args.setdefault(\"source\", USER_AGENT)",
            "",
            "        if database.server_cert:",
            "            connect_args[\"http_scheme\"] = \"https\"",
            "            connect_args[\"verify\"] = utils.create_ssl_cert_file(database.server_cert)",
            "",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(",
            "        database: Database,",
            "        params: dict[str, Any],",
            "    ) -> None:",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            auth_method = encrypted_extra.pop(\"auth_method\", None)",
            "            auth_params = encrypted_extra.pop(\"auth_params\", {})",
            "            if not auth_method:",
            "                return",
            "",
            "            connect_args = params.setdefault(\"connect_args\", {})",
            "            connect_args[\"http_scheme\"] = \"https\"",
            "            # pylint: disable=import-outside-toplevel",
            "            if auth_method == \"basic\":",
            "                from trino.auth import BasicAuthentication as trino_auth  # noqa",
            "            elif auth_method == \"kerberos\":",
            "                from trino.auth import KerberosAuthentication as trino_auth  # noqa",
            "            elif auth_method == \"certificate\":",
            "                from trino.auth import CertificateAuthentication as trino_auth  # noqa",
            "            elif auth_method == \"jwt\":",
            "                from trino.auth import JWTAuthentication as trino_auth  # noqa",
            "            else:",
            "                allowed_extra_auths = current_app.config[",
            "                    \"ALLOWED_EXTRA_AUTHENTICATIONS\"",
            "                ].get(\"trino\", {})",
            "                if auth_method in allowed_extra_auths:",
            "                    trino_auth = allowed_extra_auths.get(auth_method)",
            "                else:",
            "                    raise ValueError(",
            "                        f\"For security reason, custom authentication '{auth_method}' \"",
            "                        f\"must be listed in 'ALLOWED_EXTRA_AUTHENTICATIONS' config\"",
            "                    )",
            "",
            "            connect_args[\"auth\"] = trino_auth(**auth_params)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from requests import exceptions as requests_exceptions",
            "        from trino import exceptions as trino_exceptions",
            "",
            "        static_mapping: dict[type[Exception], type[Exception]] = {",
            "            requests_exceptions.ConnectionError: SupersetDBAPIConnectionError,",
            "        }",
            "",
            "        class _CustomMapping(dict[type[Exception], type[Exception]]):",
            "            def get(  # type: ignore[override]",
            "                self, item: type[Exception], default: type[Exception] | None = None",
            "            ) -> type[Exception] | None:",
            "                if static := static_mapping.get(item):",
            "                    return static",
            "                if issubclass(item, trino_exceptions.InternalError):",
            "                    return SupersetDBAPIDatabaseError",
            "                if issubclass(item, trino_exceptions.OperationalError):",
            "                    return SupersetDBAPIOperationalError",
            "                if issubclass(item, trino_exceptions.ProgrammingError):",
            "                    return SupersetDBAPIProgrammingError",
            "                return default",
            "",
            "        return _CustomMapping()",
            "",
            "    @classmethod",
            "    def _expand_columns(cls, col: ResultSetColumnType) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Expand the given column out to one or more columns by analysing their types,",
            "        descending into ROWS and expanding out their inner fields recursively.",
            "",
            "        We can only navigate named fields in ROWs in this way, so we can't expand out",
            "        MAP or ARRAY types, nor fields in ROWs which have no name (in fact the trino",
            "        library doesn't correctly parse unnamed fields in ROWs). We won't be able to",
            "        expand ROWs which are nested underneath any of those types, either.",
            "",
            "        Expanded columns are named foo.bar.baz and we provide a query_as property to",
            "        instruct the base engine spec how to correctly query them: instead of quoting",
            "        the whole string they have to be quoted like \"foo\".\"bar\".\"baz\" and we then",
            "        alias them to the full dotted string for ease of reference.",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from trino.sqlalchemy import datatype",
            "",
            "        cols = [col]",
            "        col_type = col.get(\"type\")",
            "",
            "        if not isinstance(col_type, datatype.ROW):",
            "            return cols",
            "",
            "        for inner_name, inner_type in col_type.attr_types:",
            "            outer_name = col[\"name\"]",
            "            name = \".\".join([outer_name, inner_name])",
            "            query_name = \".\".join([f'\"{piece}\"' for piece in name.split(\".\")])",
            "            column_spec = cls.get_column_spec(str(inner_type))",
            "            is_dttm = column_spec.is_dttm if column_spec else False",
            "",
            "            inner_col = ResultSetColumnType(",
            "                name=name,",
            "                column_name=name,",
            "                type=inner_type,",
            "                is_dttm=is_dttm,",
            "                query_as=f'{query_name} AS \"{name}\"',",
            "            )",
            "            cols.extend(cls._expand_columns(inner_col))",
            "",
            "        return cols",
            "",
            "    @classmethod",
            "    def get_columns(",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        If the \"expand_rows\" feature is enabled on the database via",
            "        \"schema_options\", expand the schema definition out to show all",
            "        subfields of nested ROWs as their appropriate dotted paths.",
            "        \"\"\"",
            "        base_cols = super().get_columns(inspector, table_name, schema, options)",
            "        if not (options or {}).get(\"expand_rows\"):",
            "            return base_cols",
            "",
            "        return [col for base_col in base_cols for col in cls._expand_columns(base_col)]",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        Trino dialect raises NoSuchTableError in get_indexes if table is empty.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "        try:",
            "            return super().get_indexes(database, inspector, table_name, schema)",
            "        except NoSuchTableError:",
            "            return []"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "26": [],
            "209": [
                "TrinoEngineSpec",
                "execute_with_cursor",
                "_execute"
            ],
            "213": [
                "TrinoEngineSpec",
                "execute_with_cursor",
                "_execute"
            ],
            "221": [
                "TrinoEngineSpec",
                "execute_with_cursor"
            ]
        },
        "addLocation": []
    },
    "superset/exceptions.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": 295,
                "PatchRowcode": "             extra={\"messages\": exc.messages, \"payload\": payload},"
            },
            "1": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 296,
                "PatchRowcode": "         )"
            },
            "2": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": 297,
                "PatchRowcode": "         super().__init__(error)"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 299,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 300,
                "PatchRowcode": "+class DisallowedSQLFunction(SupersetErrorException):"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 301,
                "PatchRowcode": "+    \"\"\""
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+    Disallowed function found on SQL statement"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+    \"\"\""
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 304,
                "PatchRowcode": "+"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 305,
                "PatchRowcode": "+    def __init__(self, functions: set[str]):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 306,
                "PatchRowcode": "+        super().__init__("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 307,
                "PatchRowcode": "+            SupersetError("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 308,
                "PatchRowcode": "+                message=f\"SQL statement contains disallowed function(s): {functions}\","
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 309,
                "PatchRowcode": "+                error_type=SupersetErrorType.SYNTAX_ERROR,"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+                level=ErrorLevel.ERROR,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 311,
                "PatchRowcode": "+            )"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+        )"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from collections import defaultdict",
            "from typing import Any, Optional",
            "",
            "from flask_babel import gettext as _",
            "from marshmallow import ValidationError",
            "",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "",
            "",
            "class SupersetException(Exception):",
            "    status = 500",
            "    message = \"\"",
            "",
            "    def __init__(",
            "        self,",
            "        message: str = \"\",",
            "        exception: Optional[Exception] = None,",
            "        error_type: Optional[SupersetErrorType] = None,",
            "    ) -> None:",
            "        if message:",
            "            self.message = message",
            "        self._exception = exception",
            "        self._error_type = error_type",
            "        super().__init__(self.message)",
            "",
            "    @property",
            "    def exception(self) -> Optional[Exception]:",
            "        return self._exception",
            "",
            "    @property",
            "    def error_type(self) -> Optional[SupersetErrorType]:",
            "        return self._error_type",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        rv = {}",
            "        if hasattr(self, \"message\"):",
            "            rv[\"message\"] = self.message",
            "        if self.error_type:",
            "            rv[\"error_type\"] = self.error_type",
            "        if self.exception is not None and hasattr(self.exception, \"to_dict\"):",
            "            rv = {**rv, **self.exception.to_dict()}",
            "        return rv",
            "",
            "",
            "class SupersetErrorException(SupersetException):",
            "    \"\"\"Exceptions with a single SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(self, error: SupersetError, status: Optional[int] = None) -> None:",
            "        super().__init__(error.message)",
            "        self.error = error",
            "        if status is not None:",
            "            self.status = status",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return self.error.to_dict()",
            "",
            "",
            "class SupersetGenericErrorException(SupersetErrorException):",
            "    \"\"\"Exceptions that are too generic to have their own type\"\"\"",
            "",
            "    def __init__(self, message: str, status: Optional[int] = None) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                message=message,",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetErrorFromParamsException(SupersetErrorException):",
            "    \"\"\"Exceptions that pass in parameters to construct a SupersetError\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        error_type: SupersetErrorType,",
            "        message: str,",
            "        level: ErrorLevel,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                error_type=error_type, message=message, level=level, extra=extra or {}",
            "            )",
            "        )",
            "",
            "",
            "class SupersetErrorsException(SupersetException):",
            "    \"\"\"Exceptions with multiple SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(",
            "        self, errors: list[SupersetError], status: Optional[int] = None",
            "    ) -> None:",
            "        super().__init__(str(errors))",
            "        self.errors = errors",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetSyntaxErrorException(SupersetErrorsException):",
            "    status = 422",
            "    error_type = SupersetErrorType.SYNTAX_ERROR",
            "",
            "    def __init__(self, errors: list[SupersetError]) -> None:",
            "        super().__init__(errors)",
            "",
            "",
            "class SupersetTimeoutException(SupersetErrorFromParamsException):",
            "    status = 408",
            "",
            "",
            "class SupersetGenericDBErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetTemplateParamsErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        error: SupersetErrorType,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            error,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetSecurityException(SupersetErrorException):",
            "    status = 403",
            "",
            "    def __init__(",
            "        self, error: SupersetError, payload: Optional[dict[str, Any]] = None",
            "    ) -> None:",
            "        super().__init__(error)",
            "        self.payload = payload",
            "",
            "",
            "class SupersetVizException(SupersetErrorsException):",
            "    status = 400",
            "",
            "",
            "class NoDataException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class NullValueException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class SupersetTemplateException(SupersetException):",
            "    pass",
            "",
            "",
            "class SpatialException(SupersetException):",
            "    pass",
            "",
            "",
            "class CertificateException(SupersetException):",
            "    message = _(\"Invalid certificate\")",
            "",
            "",
            "class DatabaseNotFound(SupersetException):",
            "    status = 400",
            "",
            "",
            "class MissingUserContextException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryObjectValidationError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class AdvancedDataTypeResponseError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class InvalidPostProcessingError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class CacheLoadError(SupersetException):",
            "    status = 404",
            "",
            "",
            "class QueryClauseValidationException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class DashboardImportException(SupersetException):",
            "    pass",
            "",
            "",
            "class DatasetInvalidPermissionEvaluationException(SupersetException):",
            "    \"\"\"",
            "    When a dataset can't compute its permission name",
            "    \"\"\"",
            "",
            "",
            "class SerializationError(SupersetException):",
            "    pass",
            "",
            "",
            "class InvalidPayloadFormatError(SupersetErrorException):",
            "    status = 400",
            "",
            "    def __init__(self, message: str = \"Request payload has incorrect format\"):",
            "        error = SupersetError(",
            "            message=message,",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_FORMAT_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class InvalidPayloadSchemaError(SupersetErrorException):",
            "    status = 422",
            "",
            "    def __init__(self, error: ValidationError):",
            "        # dataclasses.asdict does not work with defaultdict, convert to dict",
            "        # https://bugs.python.org/issue35540",
            "        for k, v in error.messages.items():",
            "            if isinstance(v, defaultdict):",
            "                error.messages[k] = dict(v)",
            "        error = SupersetError(",
            "            message=\"An error happened when validating the request\",",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": error.messages},",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class SupersetCancelQueryException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class ColumnNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class SupersetMarshmallowValidationError(SupersetErrorException):",
            "    \"\"\"",
            "    Exception to be raised for Marshmallow validation errors.",
            "    \"\"\"",
            "",
            "    status = 422",
            "",
            "    def __init__(self, exc: ValidationError, payload: dict[str, Any]):",
            "        error = SupersetError(",
            "            message=_(\"The schema of the submitted payload is invalid.\"),",
            "            error_type=SupersetErrorType.MARSHMALLOW_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": exc.messages, \"payload\": payload},",
            "        )",
            "        super().__init__(error)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from collections import defaultdict",
            "from typing import Any, Optional",
            "",
            "from flask_babel import gettext as _",
            "from marshmallow import ValidationError",
            "",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "",
            "",
            "class SupersetException(Exception):",
            "    status = 500",
            "    message = \"\"",
            "",
            "    def __init__(",
            "        self,",
            "        message: str = \"\",",
            "        exception: Optional[Exception] = None,",
            "        error_type: Optional[SupersetErrorType] = None,",
            "    ) -> None:",
            "        if message:",
            "            self.message = message",
            "        self._exception = exception",
            "        self._error_type = error_type",
            "        super().__init__(self.message)",
            "",
            "    @property",
            "    def exception(self) -> Optional[Exception]:",
            "        return self._exception",
            "",
            "    @property",
            "    def error_type(self) -> Optional[SupersetErrorType]:",
            "        return self._error_type",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        rv = {}",
            "        if hasattr(self, \"message\"):",
            "            rv[\"message\"] = self.message",
            "        if self.error_type:",
            "            rv[\"error_type\"] = self.error_type",
            "        if self.exception is not None and hasattr(self.exception, \"to_dict\"):",
            "            rv = {**rv, **self.exception.to_dict()}",
            "        return rv",
            "",
            "",
            "class SupersetErrorException(SupersetException):",
            "    \"\"\"Exceptions with a single SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(self, error: SupersetError, status: Optional[int] = None) -> None:",
            "        super().__init__(error.message)",
            "        self.error = error",
            "        if status is not None:",
            "            self.status = status",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return self.error.to_dict()",
            "",
            "",
            "class SupersetGenericErrorException(SupersetErrorException):",
            "    \"\"\"Exceptions that are too generic to have their own type\"\"\"",
            "",
            "    def __init__(self, message: str, status: Optional[int] = None) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                message=message,",
            "                error_type=SupersetErrorType.GENERIC_BACKEND_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetErrorFromParamsException(SupersetErrorException):",
            "    \"\"\"Exceptions that pass in parameters to construct a SupersetError\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        error_type: SupersetErrorType,",
            "        message: str,",
            "        level: ErrorLevel,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetError(",
            "                error_type=error_type, message=message, level=level, extra=extra or {}",
            "            )",
            "        )",
            "",
            "",
            "class SupersetErrorsException(SupersetException):",
            "    \"\"\"Exceptions with multiple SupersetErrorType associated with them\"\"\"",
            "",
            "    def __init__(",
            "        self, errors: list[SupersetError], status: Optional[int] = None",
            "    ) -> None:",
            "        super().__init__(str(errors))",
            "        self.errors = errors",
            "        if status is not None:",
            "            self.status = status",
            "",
            "",
            "class SupersetSyntaxErrorException(SupersetErrorsException):",
            "    status = 422",
            "    error_type = SupersetErrorType.SYNTAX_ERROR",
            "",
            "    def __init__(self, errors: list[SupersetError]) -> None:",
            "        super().__init__(errors)",
            "",
            "",
            "class SupersetTimeoutException(SupersetErrorFromParamsException):",
            "    status = 408",
            "",
            "",
            "class SupersetGenericDBErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetTemplateParamsErrorException(SupersetErrorFromParamsException):",
            "    status = 400",
            "",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        error: SupersetErrorType,",
            "        level: ErrorLevel = ErrorLevel.ERROR,",
            "        extra: Optional[dict[str, Any]] = None,",
            "    ) -> None:",
            "        super().__init__(",
            "            error,",
            "            message,",
            "            level,",
            "            extra,",
            "        )",
            "",
            "",
            "class SupersetSecurityException(SupersetErrorException):",
            "    status = 403",
            "",
            "    def __init__(",
            "        self, error: SupersetError, payload: Optional[dict[str, Any]] = None",
            "    ) -> None:",
            "        super().__init__(error)",
            "        self.payload = payload",
            "",
            "",
            "class SupersetVizException(SupersetErrorsException):",
            "    status = 400",
            "",
            "",
            "class NoDataException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class NullValueException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class SupersetTemplateException(SupersetException):",
            "    pass",
            "",
            "",
            "class SpatialException(SupersetException):",
            "    pass",
            "",
            "",
            "class CertificateException(SupersetException):",
            "    message = _(\"Invalid certificate\")",
            "",
            "",
            "class DatabaseNotFound(SupersetException):",
            "    status = 400",
            "",
            "",
            "class MissingUserContextException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryObjectValidationError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class AdvancedDataTypeResponseError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class InvalidPostProcessingError(SupersetException):",
            "    status = 400",
            "",
            "",
            "class CacheLoadError(SupersetException):",
            "    status = 404",
            "",
            "",
            "class QueryClauseValidationException(SupersetException):",
            "    status = 400",
            "",
            "",
            "class DashboardImportException(SupersetException):",
            "    pass",
            "",
            "",
            "class DatasetInvalidPermissionEvaluationException(SupersetException):",
            "    \"\"\"",
            "    When a dataset can't compute its permission name",
            "    \"\"\"",
            "",
            "",
            "class SerializationError(SupersetException):",
            "    pass",
            "",
            "",
            "class InvalidPayloadFormatError(SupersetErrorException):",
            "    status = 400",
            "",
            "    def __init__(self, message: str = \"Request payload has incorrect format\"):",
            "        error = SupersetError(",
            "            message=message,",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_FORMAT_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class InvalidPayloadSchemaError(SupersetErrorException):",
            "    status = 422",
            "",
            "    def __init__(self, error: ValidationError):",
            "        # dataclasses.asdict does not work with defaultdict, convert to dict",
            "        # https://bugs.python.org/issue35540",
            "        for k, v in error.messages.items():",
            "            if isinstance(v, defaultdict):",
            "                error.messages[k] = dict(v)",
            "        error = SupersetError(",
            "            message=\"An error happened when validating the request\",",
            "            error_type=SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": error.messages},",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class SupersetCancelQueryException(SupersetException):",
            "    status = 422",
            "",
            "",
            "class QueryNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class ColumnNotFoundException(SupersetException):",
            "    status = 404",
            "",
            "",
            "class SupersetMarshmallowValidationError(SupersetErrorException):",
            "    \"\"\"",
            "    Exception to be raised for Marshmallow validation errors.",
            "    \"\"\"",
            "",
            "    status = 422",
            "",
            "    def __init__(self, exc: ValidationError, payload: dict[str, Any]):",
            "        error = SupersetError(",
            "            message=_(\"The schema of the submitted payload is invalid.\"),",
            "            error_type=SupersetErrorType.MARSHMALLOW_ERROR,",
            "            level=ErrorLevel.ERROR,",
            "            extra={\"messages\": exc.messages, \"payload\": payload},",
            "        )",
            "        super().__init__(error)",
            "",
            "",
            "class DisallowedSQLFunction(SupersetErrorException):",
            "    \"\"\"",
            "    Disallowed function found on SQL statement",
            "    \"\"\"",
            "",
            "    def __init__(self, functions: set[str]):",
            "        super().__init__(",
            "            SupersetError(",
            "                message=f\"SQL statement contains disallowed function(s): {functions}\",",
            "                error_type=SupersetErrorType.SYNTAX_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "superset/sql_parse.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from sqlparse import keywords"
            },
            "1": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " from sqlparse.lexer import Lexer"
            },
            "2": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " from sqlparse.sql import ("
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+    Function,"
            },
            "4": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     Identifier,"
            },
            "5": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "     IdentifierList,"
            },
            "6": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "     Parenthesis,"
            },
            "7": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 220,
                "PatchRowcode": "     return cte, remainder"
            },
            "8": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 221,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": 222,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 223,
                "PatchRowcode": "+def check_sql_functions_exist("
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 224,
                "PatchRowcode": "+    sql: str, function_list: set[str], engine: str | None = None"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 225,
                "PatchRowcode": "+) -> bool:"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+    \"\"\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 227,
                "PatchRowcode": "+    Check if the SQL statement contains any of the specified functions."
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 228,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 229,
                "PatchRowcode": "+    :param sql: The SQL statement"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 230,
                "PatchRowcode": "+    :param function_list: The list of functions to search for"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 231,
                "PatchRowcode": "+    :param engine: The engine to use for parsing the SQL statement"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 232,
                "PatchRowcode": "+    \"\"\""
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 233,
                "PatchRowcode": "+    return ParsedQuery(sql, engine=engine).check_functions_exist(function_list)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 234,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 235,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 236,
                "PatchRowcode": " def strip_comments_from_sql(statement: str, engine: str | None = None) -> str:"
            },
            "24": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 237,
                "PatchRowcode": "     \"\"\""
            },
            "25": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 238,
                "PatchRowcode": "     Strips comments from a SQL statement, does a simple test first"
            },
            "26": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": 302,
                "PatchRowcode": "             self._tables = self._extract_tables_from_sql()"
            },
            "27": {
                "beforePatchRowNumber": 289,
                "afterPatchRowNumber": 303,
                "PatchRowcode": "         return self._tables"
            },
            "28": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": 304,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 305,
                "PatchRowcode": "+    def _check_functions_exist_in_token("
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 306,
                "PatchRowcode": "+        self, token: Token, functions: set[str]"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 307,
                "PatchRowcode": "+    ) -> bool:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 308,
                "PatchRowcode": "+        if ("
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 309,
                "PatchRowcode": "+            isinstance(token, Function)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 310,
                "PatchRowcode": "+            and token.get_name() is not None"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 311,
                "PatchRowcode": "+            and token.get_name().lower() in functions"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+        ):"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 313,
                "PatchRowcode": "+            return True"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 314,
                "PatchRowcode": "+        if hasattr(token, \"tokens\"):"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 315,
                "PatchRowcode": "+            for inner_token in token.tokens:"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 316,
                "PatchRowcode": "+                if self._check_functions_exist_in_token(inner_token, functions):"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 317,
                "PatchRowcode": "+                    return True"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 318,
                "PatchRowcode": "+        return False"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 319,
                "PatchRowcode": "+"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 320,
                "PatchRowcode": "+    def check_functions_exist(self, functions: set[str]) -> bool:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 321,
                "PatchRowcode": "+        \"\"\""
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 322,
                "PatchRowcode": "+        Check if the SQL statement contains any of the specified functions."
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 323,
                "PatchRowcode": "+"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 324,
                "PatchRowcode": "+        :param functions: A set of functions to search for"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 325,
                "PatchRowcode": "+        :return: True if the statement contains any of the specified functions"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 326,
                "PatchRowcode": "+        \"\"\""
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 327,
                "PatchRowcode": "+        for statement in self._parsed:"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 328,
                "PatchRowcode": "+            for token in statement.tokens:"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 329,
                "PatchRowcode": "+                if self._check_functions_exist_in_token(token, functions):"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 330,
                "PatchRowcode": "+                    return True"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 331,
                "PatchRowcode": "+        return False"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": 333,
                "PatchRowcode": "     def _extract_tables_from_sql(self) -> set[Table]:"
            },
            "58": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": 334,
                "PatchRowcode": "         \"\"\""
            },
            "59": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": 335,
                "PatchRowcode": "         Extract all table references in a query."
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import logging",
            "import re",
            "import urllib.parse",
            "from collections.abc import Iterable, Iterator",
            "from dataclasses import dataclass",
            "from typing import Any, cast, TYPE_CHECKING",
            "",
            "import sqlparse",
            "from flask_babel import gettext as __",
            "from jinja2 import nodes",
            "from sqlalchemy import and_",
            "from sqlglot import exp, parse, parse_one",
            "from sqlglot.dialects import Dialects",
            "from sqlglot.errors import ParseError, SqlglotError",
            "from sqlglot.optimizer.scope import Scope, ScopeType, traverse_scope",
            "from sqlparse import keywords",
            "from sqlparse.lexer import Lexer",
            "from sqlparse.sql import (",
            "    Identifier,",
            "    IdentifierList,",
            "    Parenthesis,",
            "    remove_quotes,",
            "    Token,",
            "    TokenList,",
            "    Where,",
            ")",
            "from sqlparse.tokens import (",
            "    Comment,",
            "    CTE,",
            "    DDL,",
            "    DML,",
            "    Keyword,",
            "    Name,",
            "    Punctuation,",
            "    String,",
            "    Whitespace,",
            "    Wildcard,",
            ")",
            "from sqlparse.utils import imt",
            "",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    QueryClauseValidationException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.utils.backports import StrEnum",
            "",
            "try:",
            "    from sqloxide import parse_sql as sqloxide_parse",
            "except (ImportError, ModuleNotFoundError):",
            "    sqloxide_parse = None",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database",
            "",
            "RESULT_OPERATIONS = {\"UNION\", \"INTERSECT\", \"EXCEPT\", \"SELECT\"}",
            "ON_KEYWORD = \"ON\"",
            "PRECEDES_TABLE_NAME = {\"FROM\", \"JOIN\", \"DESCRIBE\", \"WITH\", \"LEFT JOIN\", \"RIGHT JOIN\"}",
            "CTE_PREFIX = \"CTE__\"",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# TODO: Workaround for https://github.com/andialbrecht/sqlparse/issues/652.",
            "# configure the Lexer to extend sqlparse",
            "# reference: https://sqlparse.readthedocs.io/en/stable/extending/",
            "lex = Lexer.get_default_instance()",
            "sqlparser_sql_regex = keywords.SQL_REGEX",
            "sqlparser_sql_regex.insert(25, (r\"'(''|\\\\\\\\|\\\\|[^'])*'\", sqlparse.tokens.String.Single))",
            "lex.set_SQL_REGEX(sqlparser_sql_regex)",
            "",
            "",
            "# mapping between DB engine specs and sqlglot dialects",
            "SQLGLOT_DIALECTS = {",
            "    \"ascend\": Dialects.HIVE,",
            "    \"awsathena\": Dialects.PRESTO,",
            "    \"bigquery\": Dialects.BIGQUERY,",
            "    \"clickhouse\": Dialects.CLICKHOUSE,",
            "    \"clickhousedb\": Dialects.CLICKHOUSE,",
            "    \"cockroachdb\": Dialects.POSTGRES,",
            "    # \"crate\": ???",
            "    # \"databend\": ???",
            "    \"databricks\": Dialects.DATABRICKS,",
            "    # \"db2\": ???",
            "    # \"dremio\": ???",
            "    \"drill\": Dialects.DRILL,",
            "    # \"druid\": ???",
            "    \"duckdb\": Dialects.DUCKDB,",
            "    # \"dynamodb\": ???",
            "    # \"elasticsearch\": ???",
            "    # \"exa\": ???",
            "    # \"firebird\": ???",
            "    # \"firebolt\": ???",
            "    \"gsheets\": Dialects.SQLITE,",
            "    \"hana\": Dialects.POSTGRES,",
            "    \"hive\": Dialects.HIVE,",
            "    # \"ibmi\": ???",
            "    # \"impala\": ???",
            "    # \"kustokql\": ???",
            "    # \"kylin\": ???",
            "    \"mssql\": Dialects.TSQL,",
            "    \"mysql\": Dialects.MYSQL,",
            "    \"netezza\": Dialects.POSTGRES,",
            "    # \"ocient\": ???",
            "    # \"odelasticsearch\": ???",
            "    \"oracle\": Dialects.ORACLE,",
            "    # \"pinot\": ???",
            "    \"postgresql\": Dialects.POSTGRES,",
            "    \"presto\": Dialects.PRESTO,",
            "    \"pydoris\": Dialects.DORIS,",
            "    \"redshift\": Dialects.REDSHIFT,",
            "    # \"risingwave\": ???",
            "    # \"rockset\": ???",
            "    \"shillelagh\": Dialects.SQLITE,",
            "    \"snowflake\": Dialects.SNOWFLAKE,",
            "    # \"solr\": ???",
            "    \"spark\": Dialects.SPARK,",
            "    \"sqlite\": Dialects.SQLITE,",
            "    \"starrocks\": Dialects.STARROCKS,",
            "    \"superset\": Dialects.SQLITE,",
            "    \"teradatasql\": Dialects.TERADATA,",
            "    \"trino\": Dialects.TRINO,",
            "    \"vertica\": Dialects.POSTGRES,",
            "}",
            "",
            "",
            "class CtasMethod(StrEnum):",
            "    TABLE = \"TABLE\"",
            "    VIEW = \"VIEW\"",
            "",
            "",
            "def _extract_limit_from_query(statement: TokenList) -> int | None:",
            "    \"\"\"",
            "    Extract limit clause from SQL statement.",
            "",
            "    :param statement: SQL statement",
            "    :return: Limit extracted from query, None if no limit present in statement",
            "    \"\"\"",
            "    idx, _ = statement.token_next_by(m=(Keyword, \"LIMIT\"))",
            "    if idx is not None:",
            "        _, token = statement.token_next(idx=idx)",
            "        if token:",
            "            if isinstance(token, IdentifierList):",
            "                # In case of \"LIMIT <offset>, <limit>\", find comma and extract",
            "                # first succeeding non-whitespace token",
            "                idx, _ = token.token_next_by(m=(sqlparse.tokens.Punctuation, \",\"))",
            "                _, token = token.token_next(idx=idx)",
            "            if token and token.ttype == sqlparse.tokens.Literal.Number.Integer:",
            "                return int(token.value)",
            "    return None",
            "",
            "",
            "def extract_top_from_query(statement: TokenList, top_keywords: set[str]) -> int | None:",
            "    \"\"\"",
            "    Extract top clause value from SQL statement.",
            "",
            "    :param statement: SQL statement",
            "    :param top_keywords: keywords that are considered as synonyms to TOP",
            "    :return: top value extracted from query, None if no top value present in statement",
            "    \"\"\"",
            "",
            "    str_statement = str(statement)",
            "    str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "    token = str_statement.rstrip().split(\" \")",
            "    token = [part for part in token if part]",
            "    top = None",
            "    for i, part in enumerate(token):",
            "        if part.upper() in top_keywords and len(token) - 1 > i:",
            "            try:",
            "                top = int(token[i + 1])",
            "            except ValueError:",
            "                top = None",
            "            break",
            "    return top",
            "",
            "",
            "def get_cte_remainder_query(sql: str) -> tuple[str | None, str]:",
            "    \"\"\"",
            "    parse the SQL and return the CTE and rest of the block to the caller",
            "",
            "    :param sql: SQL query",
            "    :return: CTE and remainder block to the caller",
            "",
            "    \"\"\"",
            "    cte: str | None = None",
            "    remainder = sql",
            "    stmt = sqlparse.parse(sql)[0]",
            "",
            "    # The first meaningful token for CTE will be with WITH",
            "    idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "    if not (token and token.ttype == CTE):",
            "        return cte, remainder",
            "    idx, token = stmt.token_next(idx)",
            "    idx = stmt.token_index(token) + 1",
            "",
            "    # extract rest of the SQLs after CTE",
            "    remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "    cte = f\"WITH {token.value}\"",
            "",
            "    return cte, remainder",
            "",
            "",
            "def strip_comments_from_sql(statement: str, engine: str | None = None) -> str:",
            "    \"\"\"",
            "    Strips comments from a SQL statement, does a simple test first",
            "    to avoid always instantiating the expensive ParsedQuery constructor",
            "",
            "    This is useful for engines that don't support comments",
            "",
            "    :param statement: A string with the SQL statement",
            "    :return: SQL statement without comments",
            "    \"\"\"",
            "    return (",
            "        ParsedQuery(statement, engine=engine).strip_comments()",
            "        if \"--\" in statement",
            "        else statement",
            "    )",
            "",
            "",
            "@dataclass(eq=True, frozen=True)",
            "class Table:",
            "    \"\"\"",
            "    A fully qualified SQL table conforming to [[catalog.]schema.]table.",
            "    \"\"\"",
            "",
            "    table: str",
            "    schema: str | None = None",
            "    catalog: str | None = None",
            "",
            "    def __str__(self) -> str:",
            "        \"\"\"",
            "        Return the fully qualified SQL table name.",
            "        \"\"\"",
            "",
            "        return \".\".join(",
            "            urllib.parse.quote(part, safe=\"\").replace(\".\", \"%2E\")",
            "            for part in [self.catalog, self.schema, self.table]",
            "            if part",
            "        )",
            "",
            "    def __eq__(self, __o: object) -> bool:",
            "        return str(self) == str(__o)",
            "",
            "",
            "class ParsedQuery:",
            "    def __init__(",
            "        self,",
            "        sql_statement: str,",
            "        strip_comments: bool = False,",
            "        engine: str | None = None,",
            "    ):",
            "        if strip_comments:",
            "            sql_statement = sqlparse.format(sql_statement, strip_comments=True)",
            "",
            "        self.sql: str = sql_statement",
            "        self._dialect = SQLGLOT_DIALECTS.get(engine) if engine else None",
            "        self._tables: set[Table] = set()",
            "        self._alias_names: set[str] = set()",
            "        self._limit: int | None = None",
            "",
            "        logger.debug(\"Parsing with sqlparse statement: %s\", self.sql)",
            "        self._parsed = sqlparse.parse(self.stripped())",
            "        for statement in self._parsed:",
            "            self._limit = _extract_limit_from_query(statement)",
            "",
            "    @property",
            "    def tables(self) -> set[Table]:",
            "        if not self._tables:",
            "            self._tables = self._extract_tables_from_sql()",
            "        return self._tables",
            "",
            "    def _extract_tables_from_sql(self) -> set[Table]:",
            "        \"\"\"",
            "        Extract all table references in a query.",
            "",
            "        Note: this uses sqlglot, since it's better at catching more edge cases.",
            "        \"\"\"",
            "        try:",
            "            statements = parse(self.stripped(), dialect=self._dialect)",
            "        except SqlglotError as ex:",
            "            logger.warning(\"Unable to parse SQL (%s): %s\", self._dialect, self.sql)",
            "",
            "            message = (",
            "                \"Error parsing near '{highlight}' at line {line}:{col}\".format(  # pylint: disable=consider-using-f-string",
            "                    **ex.errors[0]",
            "                )",
            "                if isinstance(ex, ParseError)",
            "                else str(ex)",
            "            )",
            "",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.QUERY_SECURITY_ACCESS_ERROR,",
            "                    message=__(",
            "                        f\"You may have an error in your SQL statement. {message}\"",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            ) from ex",
            "",
            "        return {",
            "            table",
            "            for statement in statements",
            "            for table in self._extract_tables_from_statement(statement)",
            "            if statement",
            "        }",
            "",
            "    def _extract_tables_from_statement(self, statement: exp.Expression) -> set[Table]:",
            "        \"\"\"",
            "        Extract all table references in a single statement.",
            "",
            "        Please not that this is not trivial; consider the following queries:",
            "",
            "            DESCRIBE some_table;",
            "            SHOW PARTITIONS FROM some_table;",
            "            WITH masked_name AS (SELECT * FROM some_table) SELECT * FROM masked_name;",
            "",
            "        See the unit tests for other tricky cases.",
            "        \"\"\"",
            "        sources: Iterable[exp.Table]",
            "",
            "        if isinstance(statement, exp.Describe):",
            "            # A `DESCRIBE` query has no sources in sqlglot, so we need to explicitly",
            "            # query for all tables.",
            "            sources = statement.find_all(exp.Table)",
            "        elif isinstance(statement, exp.Command):",
            "            # Commands, like `SHOW COLUMNS FROM foo`, have to be converted into a",
            "            # `SELECT` statetement in order to extract tables.",
            "            if not (literal := statement.find(exp.Literal)):",
            "                return set()",
            "",
            "            try:",
            "                pseudo_query = parse_one(",
            "                    f\"SELECT {literal.this}\",",
            "                    dialect=self._dialect,",
            "                )",
            "                sources = pseudo_query.find_all(exp.Table)",
            "            except SqlglotError:",
            "                return set()",
            "        else:",
            "            sources = [",
            "                source",
            "                for scope in traverse_scope(statement)",
            "                for source in scope.sources.values()",
            "                if isinstance(source, exp.Table) and not self._is_cte(source, scope)",
            "            ]",
            "",
            "        return {",
            "            Table(",
            "                source.name,",
            "                source.db if source.db != \"\" else None,",
            "                source.catalog if source.catalog != \"\" else None,",
            "            )",
            "            for source in sources",
            "        }",
            "",
            "    def _is_cte(self, source: exp.Table, scope: Scope) -> bool:",
            "        \"\"\"",
            "        Is the source a CTE?",
            "",
            "        CTEs in the parent scope look like tables (and are represented by",
            "        exp.Table objects), but should not be considered as such;",
            "        otherwise a user with access to table `foo` could access any table",
            "        with a query like this:",
            "",
            "            WITH foo AS (SELECT * FROM target_table) SELECT * FROM foo",
            "",
            "        \"\"\"",
            "        parent_sources = scope.parent.sources if scope.parent else {}",
            "        ctes_in_scope = {",
            "            name",
            "            for name, parent_scope in parent_sources.items()",
            "            if isinstance(parent_scope, Scope)",
            "            and parent_scope.scope_type == ScopeType.CTE",
            "        }",
            "",
            "        return source.name in ctes_in_scope",
            "",
            "    @property",
            "    def limit(self) -> int | None:",
            "        return self._limit",
            "",
            "    def _get_cte_tables(self, parsed: dict[str, Any]) -> list[dict[str, Any]]:",
            "        if \"with\" not in parsed:",
            "            return []",
            "        return parsed[\"with\"].get(\"cte_tables\", [])",
            "",
            "    def _check_cte_is_select(self, oxide_parse: list[dict[str, Any]]) -> bool:",
            "        \"\"\"",
            "        Check if a oxide parsed CTE contains only SELECT statements",
            "",
            "        :param oxide_parse: parsed CTE",
            "        :return: True if CTE is a SELECT statement",
            "        \"\"\"",
            "",
            "        def is_body_select(body: dict[str, Any]) -> bool:",
            "            if op := body.get(\"SetOperation\"):",
            "                return is_body_select(op[\"left\"]) and is_body_select(op[\"right\"])",
            "            return all(key == \"Select\" for key in body.keys())",
            "",
            "        for query in oxide_parse:",
            "            parsed_query = query[\"Query\"]",
            "            cte_tables = self._get_cte_tables(parsed_query)",
            "            for cte_table in cte_tables:",
            "                is_select = is_body_select(cte_table[\"query\"][\"body\"])",
            "                if not is_select:",
            "                    return False",
            "        return True",
            "",
            "    def is_select(self) -> bool:",
            "        # make sure we strip comments; prevents a bug with comments in the CTE",
            "        parsed = sqlparse.parse(self.strip_comments())",
            "        seen_select = False",
            "",
            "        for statement in parsed:",
            "            # Check if this is a CTE",
            "            if statement.is_group and statement[0].ttype == Keyword.CTE:",
            "                if sqloxide_parse is not None:",
            "                    try:",
            "                        if not self._check_cte_is_select(",
            "                            sqloxide_parse(self.strip_comments(), dialect=\"ansi\")",
            "                        ):",
            "                            return False",
            "                    except ValueError:",
            "                        # sqloxide was not able to parse the query, so let's continue with",
            "                        # sqlparse",
            "                        pass",
            "                inner_cte = self.get_inner_cte_expression(statement.tokens) or []",
            "                # Check if the inner CTE is a not a SELECT",
            "                if any(token.ttype == DDL for token in inner_cte) or any(",
            "                    token.ttype == DML and token.normalized != \"SELECT\"",
            "                    for token in inner_cte",
            "                ):",
            "                    return False",
            "",
            "            if statement.get_type() == \"SELECT\":",
            "                seen_select = True",
            "                continue",
            "",
            "            if statement.get_type() != \"UNKNOWN\":",
            "                return False",
            "",
            "            # for `UNKNOWN`, check all DDL/DML explicitly: only `SELECT` DML is allowed,",
            "            # and no DDL is allowed",
            "            if any(token.ttype == DDL for token in statement) or any(",
            "                token.ttype == DML and token.normalized != \"SELECT\"",
            "                for token in statement",
            "            ):",
            "                return False",
            "",
            "            if imt(statement.tokens[0], m=(Keyword, \"USE\")):",
            "                continue",
            "",
            "            # return false on `EXPLAIN`, `SET`, `SHOW`, etc.",
            "            if imt(statement.tokens[0], t=Keyword):",
            "                return False",
            "",
            "            if not any(",
            "                token.ttype == DML and token.normalized == \"SELECT\"",
            "                for token in statement",
            "            ):",
            "                return False",
            "",
            "        return seen_select",
            "",
            "    def get_inner_cte_expression(self, tokens: TokenList) -> TokenList | None:",
            "        for token in tokens:",
            "            if self._is_identifier(token):",
            "                for identifier_token in token.tokens:",
            "                    if (",
            "                        isinstance(identifier_token, Parenthesis)",
            "                        and identifier_token.is_group",
            "                    ):",
            "                        return identifier_token.tokens",
            "        return None",
            "",
            "    def is_valid_ctas(self) -> bool:",
            "        parsed = sqlparse.parse(self.strip_comments())",
            "        return parsed[-1].get_type() == \"SELECT\"",
            "",
            "    def is_valid_cvas(self) -> bool:",
            "        parsed = sqlparse.parse(self.strip_comments())",
            "        return len(parsed) == 1 and parsed[0].get_type() == \"SELECT\"",
            "",
            "    def is_explain(self) -> bool:",
            "        # Remove comments",
            "        statements_without_comments = sqlparse.format(",
            "            self.stripped(), strip_comments=True",
            "        )",
            "",
            "        # Explain statements will only be the first statement",
            "        return statements_without_comments.upper().startswith(\"EXPLAIN\")",
            "",
            "    def is_show(self) -> bool:",
            "        # Remove comments",
            "        statements_without_comments = sqlparse.format(",
            "            self.stripped(), strip_comments=True",
            "        )",
            "        # Show statements will only be the first statement",
            "        return statements_without_comments.upper().startswith(\"SHOW\")",
            "",
            "    def is_set(self) -> bool:",
            "        # Remove comments",
            "        statements_without_comments = sqlparse.format(",
            "            self.stripped(), strip_comments=True",
            "        )",
            "        # Set statements will only be the first statement",
            "        return statements_without_comments.upper().startswith(\"SET\")",
            "",
            "    def is_unknown(self) -> bool:",
            "        return self._parsed[0].get_type() == \"UNKNOWN\"",
            "",
            "    def stripped(self) -> str:",
            "        return self.sql.strip(\" \\t\\r\\n;\")",
            "",
            "    def strip_comments(self) -> str:",
            "        return sqlparse.format(self.stripped(), strip_comments=True)",
            "",
            "    def get_statements(self) -> list[str]:",
            "        \"\"\"Returns a list of SQL statements as strings, stripped\"\"\"",
            "        statements = []",
            "        for statement in self._parsed:",
            "            if statement:",
            "                sql = str(statement).strip(\" \\n;\\t\")",
            "                if sql:",
            "                    statements.append(sql)",
            "        return statements",
            "",
            "    @staticmethod",
            "    def get_table(tlist: TokenList) -> Table | None:",
            "        \"\"\"",
            "        Return the table if valid, i.e., conforms to the [[catalog.]schema.]table",
            "        construct.",
            "",
            "        :param tlist: The SQL tokens",
            "        :returns: The table if the name conforms",
            "        \"\"\"",
            "",
            "        # Strip the alias if present.",
            "        idx = len(tlist.tokens)",
            "",
            "        if tlist.has_alias():",
            "            ws_idx, _ = tlist.token_next_by(t=Whitespace)",
            "",
            "            if ws_idx != -1:",
            "                idx = ws_idx",
            "",
            "        tokens = tlist.tokens[:idx]",
            "",
            "        if (",
            "            len(tokens) in (1, 3, 5)",
            "            and all(imt(token, t=[Name, String]) for token in tokens[::2])",
            "            and all(imt(token, m=(Punctuation, \".\")) for token in tokens[1::2])",
            "        ):",
            "            return Table(*[remove_quotes(token.value) for token in tokens[::-2]])",
            "",
            "        return None",
            "",
            "    @staticmethod",
            "    def _is_identifier(token: Token) -> bool:",
            "        return isinstance(token, (IdentifierList, Identifier))",
            "",
            "    def as_create_table(",
            "        self,",
            "        table_name: str,",
            "        schema_name: str | None = None,",
            "        overwrite: bool = False,",
            "        method: CtasMethod = CtasMethod.TABLE,",
            "    ) -> str:",
            "        \"\"\"Reformats the query into the create table as query.",
            "",
            "        Works only for the single select SQL statements, in all other cases",
            "        the sql query is not modified.",
            "        :param table_name: table that will contain the results of the query execution",
            "        :param schema_name: schema name for the target table",
            "        :param overwrite: table_name will be dropped if true",
            "        :param method: method for the CTA query, currently view or table creation",
            "        :return: Create table as query",
            "        \"\"\"",
            "        exec_sql = \"\"",
            "        sql = self.stripped()",
            "        # TODO(bkyryliuk): quote full_table_name",
            "        full_table_name = f\"{schema_name}.{table_name}\" if schema_name else table_name",
            "        if overwrite:",
            "            exec_sql = f\"DROP {method} IF EXISTS {full_table_name};\\n\"",
            "        exec_sql += f\"CREATE {method} {full_table_name} AS \\n{sql}\"",
            "        return exec_sql",
            "",
            "    def set_or_update_query_limit(self, new_limit: int, force: bool = False) -> str:",
            "        \"\"\"Returns the query with the specified limit.",
            "",
            "        Does not change the underlying query if user did not apply the limit,",
            "        otherwise replaces the limit with the lower value between existing limit",
            "        in the query and new_limit.",
            "",
            "        :param new_limit: Limit to be incorporated into returned query",
            "        :return: The original query with new limit",
            "        \"\"\"",
            "        if not self._limit:",
            "            return f\"{self.stripped()}\\nLIMIT {new_limit}\"",
            "        limit_pos = None",
            "        statement = self._parsed[0]",
            "        # Add all items to before_str until there is a limit",
            "        for pos, item in enumerate(statement.tokens):",
            "            if item.ttype in Keyword and item.value.lower() == \"limit\":",
            "                limit_pos = pos",
            "                break",
            "        _, limit = statement.token_next(idx=limit_pos)",
            "        # Override the limit only when it exceeds the configured value.",
            "        if limit.ttype == sqlparse.tokens.Literal.Number.Integer and (",
            "            force or new_limit < int(limit.value)",
            "        ):",
            "            limit.value = new_limit",
            "        elif limit.is_group:",
            "            limit.value = f\"{next(limit.get_identifiers())}, {new_limit}\"",
            "",
            "        str_res = \"\"",
            "        for i in statement.tokens:",
            "            str_res += str(i.value)",
            "        return str_res",
            "",
            "",
            "def sanitize_clause(clause: str) -> str:",
            "    # clause = sqlparse.format(clause, strip_comments=True)",
            "    statements = sqlparse.parse(clause)",
            "    if len(statements) != 1:",
            "        raise QueryClauseValidationException(\"Clause contains multiple statements\")",
            "    open_parens = 0",
            "",
            "    previous_token = None",
            "    for token in statements[0]:",
            "        if token.value == \"/\" and previous_token and previous_token.value == \"*\":",
            "            raise QueryClauseValidationException(\"Closing unopened multiline comment\")",
            "        if token.value == \"*\" and previous_token and previous_token.value == \"/\":",
            "            raise QueryClauseValidationException(\"Unclosed multiline comment\")",
            "        if token.value in (\")\", \"(\"):",
            "            open_parens += 1 if token.value == \"(\" else -1",
            "            if open_parens < 0:",
            "                raise QueryClauseValidationException(",
            "                    \"Closing unclosed parenthesis in filter clause\"",
            "                )",
            "        previous_token = token",
            "    if open_parens > 0:",
            "        raise QueryClauseValidationException(\"Unclosed parenthesis in filter clause\")",
            "",
            "    if previous_token and previous_token.ttype in Comment:",
            "        if previous_token.value[-1] != \"\\n\":",
            "            clause = f\"{clause}\\n\"",
            "",
            "    return clause",
            "",
            "",
            "class InsertRLSState(StrEnum):",
            "    \"\"\"",
            "    State machine that scans for WHERE and ON clauses referencing tables.",
            "    \"\"\"",
            "",
            "    SCANNING = \"SCANNING\"",
            "    SEEN_SOURCE = \"SEEN_SOURCE\"",
            "    FOUND_TABLE = \"FOUND_TABLE\"",
            "",
            "",
            "def has_table_query(token_list: TokenList) -> bool:",
            "    \"\"\"",
            "    Return if a statement has a query reading from a table.",
            "",
            "        >>> has_table_query(sqlparse.parse(\"COUNT(*)\")[0])",
            "        False",
            "        >>> has_table_query(sqlparse.parse(\"SELECT * FROM table\")[0])",
            "        True",
            "",
            "    Note that queries reading from constant values return false:",
            "",
            "        >>> has_table_query(sqlparse.parse(\"SELECT * FROM (SELECT 1)\")[0])",
            "        False",
            "",
            "    \"\"\"",
            "    state = InsertRLSState.SCANNING",
            "    for token in token_list.tokens:",
            "        # Ignore comments",
            "        if isinstance(token, sqlparse.sql.Comment):",
            "            continue",
            "",
            "        # Recurse into child token list",
            "        if isinstance(token, TokenList) and has_table_query(token):",
            "            return True",
            "",
            "        # Found a source keyword (FROM/JOIN)",
            "        if imt(token, m=[(Keyword, \"FROM\"), (Keyword, \"JOIN\")]):",
            "            state = InsertRLSState.SEEN_SOURCE",
            "",
            "        # Found identifier/keyword after FROM/JOIN",
            "        elif state == InsertRLSState.SEEN_SOURCE and (",
            "            isinstance(token, sqlparse.sql.Identifier) or token.ttype == Keyword",
            "        ):",
            "            return True",
            "",
            "        # Found nothing, leaving source",
            "        elif state == InsertRLSState.SEEN_SOURCE and token.ttype != Whitespace:",
            "            state = InsertRLSState.SCANNING",
            "",
            "    return False",
            "",
            "",
            "def add_table_name(rls: TokenList, table: str) -> None:",
            "    \"\"\"",
            "    Modify a RLS expression inplace ensuring columns are fully qualified.",
            "    \"\"\"",
            "    tokens = rls.tokens[:]",
            "    while tokens:",
            "        token = tokens.pop(0)",
            "",
            "        if isinstance(token, Identifier) and token.get_parent_name() is None:",
            "            token.tokens = [",
            "                Token(Name, table),",
            "                Token(Punctuation, \".\"),",
            "                Token(Name, token.get_name()),",
            "            ]",
            "        elif isinstance(token, TokenList):",
            "            tokens.extend(token.tokens)",
            "",
            "",
            "def get_rls_for_table(",
            "    candidate: Token,",
            "    database_id: int,",
            "    default_schema: str | None,",
            ") -> TokenList | None:",
            "    \"\"\"",
            "    Given a table name, return any associated RLS predicates.",
            "    \"\"\"",
            "    # pylint: disable=import-outside-toplevel",
            "    from superset import db",
            "    from superset.connectors.sqla.models import SqlaTable",
            "",
            "    if not isinstance(candidate, Identifier):",
            "        candidate = Identifier([Token(Name, candidate.value)])",
            "",
            "    table = ParsedQuery.get_table(candidate)",
            "    if not table:",
            "        return None",
            "",
            "    dataset = (",
            "        db.session.query(SqlaTable)",
            "        .filter(",
            "            and_(",
            "                SqlaTable.database_id == database_id,",
            "                SqlaTable.schema == (table.schema or default_schema),",
            "                SqlaTable.table_name == table.table,",
            "            )",
            "        )",
            "        .one_or_none()",
            "    )",
            "    if not dataset:",
            "        return None",
            "",
            "    template_processor = dataset.get_template_processor()",
            "    predicate = \" AND \".join(",
            "        str(filter_)",
            "        for filter_ in dataset.get_sqla_row_level_filters(template_processor)",
            "    )",
            "    if not predicate:",
            "        return None",
            "",
            "    rls = sqlparse.parse(predicate)[0]",
            "    add_table_name(rls, table.table)",
            "",
            "    return rls",
            "",
            "",
            "def insert_rls_as_subquery(",
            "    token_list: TokenList,",
            "    database_id: int,",
            "    default_schema: str | None,",
            ") -> TokenList:",
            "    \"\"\"",
            "    Update a statement inplace applying any associated RLS predicates.",
            "",
            "    The RLS predicate is applied as subquery replacing the original table:",
            "",
            "        before: SELECT * FROM some_table WHERE 1=1",
            "        after:  SELECT * FROM (",
            "                  SELECT * FROM some_table WHERE some_table.id=42",
            "                ) AS some_table",
            "                WHERE 1=1",
            "",
            "    This method is safer than ``insert_rls_in_predicate``, but doesn't work in all",
            "    databases.",
            "    \"\"\"",
            "    rls: TokenList | None = None",
            "    state = InsertRLSState.SCANNING",
            "    for token in token_list.tokens:",
            "        # Recurse into child token list",
            "        if isinstance(token, TokenList):",
            "            i = token_list.tokens.index(token)",
            "            token_list.tokens[i] = insert_rls_as_subquery(",
            "                token,",
            "                database_id,",
            "                default_schema,",
            "            )",
            "",
            "        # Found a source keyword (FROM/JOIN)",
            "        if imt(token, m=[(Keyword, \"FROM\"), (Keyword, \"JOIN\")]):",
            "            state = InsertRLSState.SEEN_SOURCE",
            "",
            "        # Found identifier/keyword after FROM/JOIN, test for table",
            "        elif state == InsertRLSState.SEEN_SOURCE and (",
            "            isinstance(token, Identifier) or token.ttype == Keyword",
            "        ):",
            "            rls = get_rls_for_table(token, database_id, default_schema)",
            "            if rls:",
            "                # replace table with subquery",
            "                subquery_alias = (",
            "                    token.tokens[-1].value",
            "                    if isinstance(token, Identifier)",
            "                    else token.value",
            "                )",
            "                i = token_list.tokens.index(token)",
            "",
            "                # strip alias from table name",
            "                if isinstance(token, Identifier) and token.has_alias():",
            "                    whitespace_index = token.token_next_by(t=Whitespace)[0]",
            "                    token.tokens = token.tokens[:whitespace_index]",
            "",
            "                token_list.tokens[i] = Identifier(",
            "                    [",
            "                        Parenthesis(",
            "                            [",
            "                                Token(Punctuation, \"(\"),",
            "                                Token(DML, \"SELECT\"),",
            "                                Token(Whitespace, \" \"),",
            "                                Token(Wildcard, \"*\"),",
            "                                Token(Whitespace, \" \"),",
            "                                Token(Keyword, \"FROM\"),",
            "                                Token(Whitespace, \" \"),",
            "                                token,",
            "                                Token(Whitespace, \" \"),",
            "                                Where(",
            "                                    [",
            "                                        Token(Keyword, \"WHERE\"),",
            "                                        Token(Whitespace, \" \"),",
            "                                        rls,",
            "                                    ]",
            "                                ),",
            "                                Token(Punctuation, \")\"),",
            "                            ]",
            "                        ),",
            "                        Token(Whitespace, \" \"),",
            "                        Token(Keyword, \"AS\"),",
            "                        Token(Whitespace, \" \"),",
            "                        Identifier([Token(Name, subquery_alias)]),",
            "                    ]",
            "                )",
            "                state = InsertRLSState.SCANNING",
            "",
            "        # Found nothing, leaving source",
            "        elif state == InsertRLSState.SEEN_SOURCE and token.ttype != Whitespace:",
            "            state = InsertRLSState.SCANNING",
            "",
            "    return token_list",
            "",
            "",
            "def insert_rls_in_predicate(",
            "    token_list: TokenList,",
            "    database_id: int,",
            "    default_schema: str | None,",
            ") -> TokenList:",
            "    \"\"\"",
            "    Update a statement inplace applying any associated RLS predicates.",
            "",
            "    The RLS predicate is ``AND``ed to any existing predicates:",
            "",
            "        before: SELECT * FROM some_table WHERE 1=1",
            "        after:  SELECT * FROM some_table WHERE ( 1=1) AND some_table.id=42",
            "",
            "    \"\"\"",
            "    rls: TokenList | None = None",
            "    state = InsertRLSState.SCANNING",
            "    for token in token_list.tokens:",
            "        # Recurse into child token list",
            "        if isinstance(token, TokenList):",
            "            i = token_list.tokens.index(token)",
            "            token_list.tokens[i] = insert_rls_in_predicate(",
            "                token,",
            "                database_id,",
            "                default_schema,",
            "            )",
            "",
            "        # Found a source keyword (FROM/JOIN)",
            "        if imt(token, m=[(Keyword, \"FROM\"), (Keyword, \"JOIN\")]):",
            "            state = InsertRLSState.SEEN_SOURCE",
            "",
            "        # Found identifier/keyword after FROM/JOIN, test for table",
            "        elif state == InsertRLSState.SEEN_SOURCE and (",
            "            isinstance(token, Identifier) or token.ttype == Keyword",
            "        ):",
            "            rls = get_rls_for_table(token, database_id, default_schema)",
            "            if rls:",
            "                state = InsertRLSState.FOUND_TABLE",
            "",
            "        # Found WHERE clause, insert RLS. Note that we insert it even it already exists,",
            "        # to be on the safe side: it could be present in a clause like `1=1 OR RLS`.",
            "        elif state == InsertRLSState.FOUND_TABLE and isinstance(token, Where):",
            "            rls = cast(TokenList, rls)",
            "            token.tokens[1:1] = [Token(Whitespace, \" \"), Token(Punctuation, \"(\")]",
            "            token.tokens.extend(",
            "                [",
            "                    Token(Punctuation, \")\"),",
            "                    Token(Whitespace, \" \"),",
            "                    Token(Keyword, \"AND\"),",
            "                    Token(Whitespace, \" \"),",
            "                ]",
            "                + rls.tokens",
            "            )",
            "            state = InsertRLSState.SCANNING",
            "",
            "        # Found ON clause, insert RLS. The logic for ON is more complicated than the logic",
            "        # for WHERE because in the former the comparisons are siblings, while on the",
            "        # latter they are children.",
            "        elif (",
            "            state == InsertRLSState.FOUND_TABLE",
            "            and token.ttype == Keyword",
            "            and token.value.upper() == \"ON\"",
            "        ):",
            "            tokens = [",
            "                Token(Whitespace, \" \"),",
            "                rls,",
            "                Token(Whitespace, \" \"),",
            "                Token(Keyword, \"AND\"),",
            "                Token(Whitespace, \" \"),",
            "                Token(Punctuation, \"(\"),",
            "            ]",
            "            i = token_list.tokens.index(token)",
            "            token.parent.tokens[i + 1 : i + 1] = tokens",
            "            i += len(tokens) + 2",
            "",
            "            # close parenthesis after last existing comparison",
            "            j = 0",
            "            for j, sibling in enumerate(token_list.tokens[i:]):",
            "                # scan until we hit a non-comparison keyword (like ORDER BY) or a WHERE",
            "                if (",
            "                    sibling.ttype == Keyword",
            "                    and not imt(",
            "                        sibling, m=[(Keyword, \"AND\"), (Keyword, \"OR\"), (Keyword, \"NOT\")]",
            "                    )",
            "                    or isinstance(sibling, Where)",
            "                ):",
            "                    j -= 1",
            "                    break",
            "            token.parent.tokens[i + j + 1 : i + j + 1] = [",
            "                Token(Whitespace, \" \"),",
            "                Token(Punctuation, \")\"),",
            "                Token(Whitespace, \" \"),",
            "            ]",
            "",
            "            state = InsertRLSState.SCANNING",
            "",
            "        # Found table but no WHERE clause found, insert one",
            "        elif state == InsertRLSState.FOUND_TABLE and token.ttype != Whitespace:",
            "            i = token_list.tokens.index(token)",
            "            token_list.tokens[i:i] = [",
            "                Token(Whitespace, \" \"),",
            "                Where([Token(Keyword, \"WHERE\"), Token(Whitespace, \" \"), rls]),",
            "                Token(Whitespace, \" \"),",
            "            ]",
            "",
            "            state = InsertRLSState.SCANNING",
            "",
            "        # Found nothing, leaving source",
            "        elif state == InsertRLSState.SEEN_SOURCE and token.ttype != Whitespace:",
            "            state = InsertRLSState.SCANNING",
            "",
            "    # found table at the end of the statement; append a WHERE clause",
            "    if state == InsertRLSState.FOUND_TABLE:",
            "        token_list.tokens.extend(",
            "            [",
            "                Token(Whitespace, \" \"),",
            "                Where([Token(Keyword, \"WHERE\"), Token(Whitespace, \" \"), rls]),",
            "            ]",
            "        )",
            "",
            "    return token_list",
            "",
            "",
            "# mapping between sqloxide and SQLAlchemy dialects",
            "SQLOXIDE_DIALECTS = {",
            "    \"ansi\": {\"trino\", \"trinonative\", \"presto\"},",
            "    \"hive\": {\"hive\", \"databricks\"},",
            "    \"ms\": {\"mssql\"},",
            "    \"mysql\": {\"mysql\"},",
            "    \"postgres\": {",
            "        \"cockroachdb\",",
            "        \"hana\",",
            "        \"netezza\",",
            "        \"postgres\",",
            "        \"postgresql\",",
            "        \"redshift\",",
            "        \"vertica\",",
            "    },",
            "    \"snowflake\": {\"snowflake\"},",
            "    \"sqlite\": {\"sqlite\", \"gsheets\", \"shillelagh\"},",
            "    \"clickhouse\": {\"clickhouse\"},",
            "}",
            "",
            "RE_JINJA_VAR = re.compile(r\"\\{\\{[^\\{\\}]+\\}\\}\")",
            "RE_JINJA_BLOCK = re.compile(r\"\\{[%#][^\\{\\}%#]+[%#]\\}\")",
            "",
            "",
            "def extract_table_references(",
            "    sql_text: str, sqla_dialect: str, show_warning: bool = True",
            ") -> set[Table]:",
            "    \"\"\"",
            "    Return all the dependencies from a SQL sql_text.",
            "    \"\"\"",
            "    dialect = \"generic\"",
            "    tree = None",
            "",
            "    if sqloxide_parse:",
            "        for dialect, sqla_dialects in SQLOXIDE_DIALECTS.items():",
            "            if sqla_dialect in sqla_dialects:",
            "                break",
            "        sql_text = RE_JINJA_BLOCK.sub(\" \", sql_text)",
            "        sql_text = RE_JINJA_VAR.sub(\"abc\", sql_text)",
            "        try:",
            "            tree = sqloxide_parse(sql_text, dialect=dialect)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            if show_warning:",
            "                logger.warning(",
            "                    \"\\nUnable to parse query with sqloxide:\\n%s\\n%s\", sql_text, ex",
            "                )",
            "",
            "    # fallback to sqlparse",
            "    if not tree:",
            "        parsed = ParsedQuery(sql_text)",
            "        return parsed.tables",
            "",
            "    def find_nodes_by_key(element: Any, target: str) -> Iterator[Any]:",
            "        \"\"\"",
            "        Find all nodes in a SQL tree matching a given key.",
            "        \"\"\"",
            "        if isinstance(element, list):",
            "            for child in element:",
            "                yield from find_nodes_by_key(child, target)",
            "        elif isinstance(element, dict):",
            "            for key, value in element.items():",
            "                if key == target:",
            "                    yield value",
            "                else:",
            "                    yield from find_nodes_by_key(value, target)",
            "",
            "    return {",
            "        Table(*[part[\"value\"] for part in table[\"name\"][::-1]])",
            "        for table in find_nodes_by_key(tree, \"Table\")",
            "    }",
            "",
            "",
            "def extract_tables_from_jinja_sql(sql: str, database: Database) -> set[Table]:",
            "    \"\"\"",
            "    Extract all table references in the Jinjafied SQL statement.",
            "",
            "    Due to Jinja templating, a multiphase approach is necessary as the Jinjafied SQL",
            "    statement may represent invalid SQL which is non-parsable by SQLGlot.",
            "",
            "    Firstly, we extract any tables referenced within the confines of specific Jinja",
            "    macros. Secondly, we replace these non-SQL Jinja calls with a pseudo-benign SQL",
            "    expression to help ensure that the resulting SQL statements are parsable by",
            "    SQLGlot.",
            "",
            "    :param sql: The Jinjafied SQL statement",
            "    :param database: The database associated with the SQL statement",
            "    :returns: The set of tables referenced in the SQL statement",
            "    :raises SupersetSecurityException: If SQLGlot is unable to parse the SQL statement",
            "    :raises jinja2.exceptions.TemplateError: If the Jinjafied SQL could not be rendered",
            "    \"\"\"",
            "",
            "    from superset.jinja_context import (  # pylint: disable=import-outside-toplevel",
            "        get_template_processor,",
            "    )",
            "",
            "    processor = get_template_processor(database)",
            "    template = processor.env.parse(sql)",
            "",
            "    tables = set()",
            "",
            "    for node in template.find_all(nodes.Call):",
            "        if isinstance(node.node, nodes.Getattr) and node.node.attr in (",
            "            \"latest_partition\",",
            "            \"latest_sub_partition\",",
            "        ):",
            "            # Try to extract the table referenced in the macro.",
            "            try:",
            "                tables.add(",
            "                    Table(",
            "                        *[",
            "                            remove_quotes(part.strip())",
            "                            for part in node.args[0].as_const().split(\".\")[::-1]",
            "                            if len(node.args) == 1",
            "                        ]",
            "                    )",
            "                )",
            "            except nodes.Impossible:",
            "                pass",
            "",
            "            # Replace the potentially problematic Jinja macro with some benign SQL.",
            "            node.__class__ = nodes.TemplateData",
            "            node.fields = nodes.TemplateData.fields",
            "            node.data = \"NULL\"",
            "",
            "    return (",
            "        tables",
            "        | ParsedQuery(",
            "            sql_statement=processor.process_template(template),",
            "            engine=database.db_engine_spec.engine,",
            "        ).tables",
            "    )"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import logging",
            "import re",
            "import urllib.parse",
            "from collections.abc import Iterable, Iterator",
            "from dataclasses import dataclass",
            "from typing import Any, cast, TYPE_CHECKING",
            "",
            "import sqlparse",
            "from flask_babel import gettext as __",
            "from jinja2 import nodes",
            "from sqlalchemy import and_",
            "from sqlglot import exp, parse, parse_one",
            "from sqlglot.dialects import Dialects",
            "from sqlglot.errors import ParseError, SqlglotError",
            "from sqlglot.optimizer.scope import Scope, ScopeType, traverse_scope",
            "from sqlparse import keywords",
            "from sqlparse.lexer import Lexer",
            "from sqlparse.sql import (",
            "    Function,",
            "    Identifier,",
            "    IdentifierList,",
            "    Parenthesis,",
            "    remove_quotes,",
            "    Token,",
            "    TokenList,",
            "    Where,",
            ")",
            "from sqlparse.tokens import (",
            "    Comment,",
            "    CTE,",
            "    DDL,",
            "    DML,",
            "    Keyword,",
            "    Name,",
            "    Punctuation,",
            "    String,",
            "    Whitespace,",
            "    Wildcard,",
            ")",
            "from sqlparse.utils import imt",
            "",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    QueryClauseValidationException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.utils.backports import StrEnum",
            "",
            "try:",
            "    from sqloxide import parse_sql as sqloxide_parse",
            "except (ImportError, ModuleNotFoundError):",
            "    sqloxide_parse = None",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database",
            "",
            "RESULT_OPERATIONS = {\"UNION\", \"INTERSECT\", \"EXCEPT\", \"SELECT\"}",
            "ON_KEYWORD = \"ON\"",
            "PRECEDES_TABLE_NAME = {\"FROM\", \"JOIN\", \"DESCRIBE\", \"WITH\", \"LEFT JOIN\", \"RIGHT JOIN\"}",
            "CTE_PREFIX = \"CTE__\"",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# TODO: Workaround for https://github.com/andialbrecht/sqlparse/issues/652.",
            "# configure the Lexer to extend sqlparse",
            "# reference: https://sqlparse.readthedocs.io/en/stable/extending/",
            "lex = Lexer.get_default_instance()",
            "sqlparser_sql_regex = keywords.SQL_REGEX",
            "sqlparser_sql_regex.insert(25, (r\"'(''|\\\\\\\\|\\\\|[^'])*'\", sqlparse.tokens.String.Single))",
            "lex.set_SQL_REGEX(sqlparser_sql_regex)",
            "",
            "",
            "# mapping between DB engine specs and sqlglot dialects",
            "SQLGLOT_DIALECTS = {",
            "    \"ascend\": Dialects.HIVE,",
            "    \"awsathena\": Dialects.PRESTO,",
            "    \"bigquery\": Dialects.BIGQUERY,",
            "    \"clickhouse\": Dialects.CLICKHOUSE,",
            "    \"clickhousedb\": Dialects.CLICKHOUSE,",
            "    \"cockroachdb\": Dialects.POSTGRES,",
            "    # \"crate\": ???",
            "    # \"databend\": ???",
            "    \"databricks\": Dialects.DATABRICKS,",
            "    # \"db2\": ???",
            "    # \"dremio\": ???",
            "    \"drill\": Dialects.DRILL,",
            "    # \"druid\": ???",
            "    \"duckdb\": Dialects.DUCKDB,",
            "    # \"dynamodb\": ???",
            "    # \"elasticsearch\": ???",
            "    # \"exa\": ???",
            "    # \"firebird\": ???",
            "    # \"firebolt\": ???",
            "    \"gsheets\": Dialects.SQLITE,",
            "    \"hana\": Dialects.POSTGRES,",
            "    \"hive\": Dialects.HIVE,",
            "    # \"ibmi\": ???",
            "    # \"impala\": ???",
            "    # \"kustokql\": ???",
            "    # \"kylin\": ???",
            "    \"mssql\": Dialects.TSQL,",
            "    \"mysql\": Dialects.MYSQL,",
            "    \"netezza\": Dialects.POSTGRES,",
            "    # \"ocient\": ???",
            "    # \"odelasticsearch\": ???",
            "    \"oracle\": Dialects.ORACLE,",
            "    # \"pinot\": ???",
            "    \"postgresql\": Dialects.POSTGRES,",
            "    \"presto\": Dialects.PRESTO,",
            "    \"pydoris\": Dialects.DORIS,",
            "    \"redshift\": Dialects.REDSHIFT,",
            "    # \"risingwave\": ???",
            "    # \"rockset\": ???",
            "    \"shillelagh\": Dialects.SQLITE,",
            "    \"snowflake\": Dialects.SNOWFLAKE,",
            "    # \"solr\": ???",
            "    \"spark\": Dialects.SPARK,",
            "    \"sqlite\": Dialects.SQLITE,",
            "    \"starrocks\": Dialects.STARROCKS,",
            "    \"superset\": Dialects.SQLITE,",
            "    \"teradatasql\": Dialects.TERADATA,",
            "    \"trino\": Dialects.TRINO,",
            "    \"vertica\": Dialects.POSTGRES,",
            "}",
            "",
            "",
            "class CtasMethod(StrEnum):",
            "    TABLE = \"TABLE\"",
            "    VIEW = \"VIEW\"",
            "",
            "",
            "def _extract_limit_from_query(statement: TokenList) -> int | None:",
            "    \"\"\"",
            "    Extract limit clause from SQL statement.",
            "",
            "    :param statement: SQL statement",
            "    :return: Limit extracted from query, None if no limit present in statement",
            "    \"\"\"",
            "    idx, _ = statement.token_next_by(m=(Keyword, \"LIMIT\"))",
            "    if idx is not None:",
            "        _, token = statement.token_next(idx=idx)",
            "        if token:",
            "            if isinstance(token, IdentifierList):",
            "                # In case of \"LIMIT <offset>, <limit>\", find comma and extract",
            "                # first succeeding non-whitespace token",
            "                idx, _ = token.token_next_by(m=(sqlparse.tokens.Punctuation, \",\"))",
            "                _, token = token.token_next(idx=idx)",
            "            if token and token.ttype == sqlparse.tokens.Literal.Number.Integer:",
            "                return int(token.value)",
            "    return None",
            "",
            "",
            "def extract_top_from_query(statement: TokenList, top_keywords: set[str]) -> int | None:",
            "    \"\"\"",
            "    Extract top clause value from SQL statement.",
            "",
            "    :param statement: SQL statement",
            "    :param top_keywords: keywords that are considered as synonyms to TOP",
            "    :return: top value extracted from query, None if no top value present in statement",
            "    \"\"\"",
            "",
            "    str_statement = str(statement)",
            "    str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "    token = str_statement.rstrip().split(\" \")",
            "    token = [part for part in token if part]",
            "    top = None",
            "    for i, part in enumerate(token):",
            "        if part.upper() in top_keywords and len(token) - 1 > i:",
            "            try:",
            "                top = int(token[i + 1])",
            "            except ValueError:",
            "                top = None",
            "            break",
            "    return top",
            "",
            "",
            "def get_cte_remainder_query(sql: str) -> tuple[str | None, str]:",
            "    \"\"\"",
            "    parse the SQL and return the CTE and rest of the block to the caller",
            "",
            "    :param sql: SQL query",
            "    :return: CTE and remainder block to the caller",
            "",
            "    \"\"\"",
            "    cte: str | None = None",
            "    remainder = sql",
            "    stmt = sqlparse.parse(sql)[0]",
            "",
            "    # The first meaningful token for CTE will be with WITH",
            "    idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "    if not (token and token.ttype == CTE):",
            "        return cte, remainder",
            "    idx, token = stmt.token_next(idx)",
            "    idx = stmt.token_index(token) + 1",
            "",
            "    # extract rest of the SQLs after CTE",
            "    remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "    cte = f\"WITH {token.value}\"",
            "",
            "    return cte, remainder",
            "",
            "",
            "def check_sql_functions_exist(",
            "    sql: str, function_list: set[str], engine: str | None = None",
            ") -> bool:",
            "    \"\"\"",
            "    Check if the SQL statement contains any of the specified functions.",
            "",
            "    :param sql: The SQL statement",
            "    :param function_list: The list of functions to search for",
            "    :param engine: The engine to use for parsing the SQL statement",
            "    \"\"\"",
            "    return ParsedQuery(sql, engine=engine).check_functions_exist(function_list)",
            "",
            "",
            "def strip_comments_from_sql(statement: str, engine: str | None = None) -> str:",
            "    \"\"\"",
            "    Strips comments from a SQL statement, does a simple test first",
            "    to avoid always instantiating the expensive ParsedQuery constructor",
            "",
            "    This is useful for engines that don't support comments",
            "",
            "    :param statement: A string with the SQL statement",
            "    :return: SQL statement without comments",
            "    \"\"\"",
            "    return (",
            "        ParsedQuery(statement, engine=engine).strip_comments()",
            "        if \"--\" in statement",
            "        else statement",
            "    )",
            "",
            "",
            "@dataclass(eq=True, frozen=True)",
            "class Table:",
            "    \"\"\"",
            "    A fully qualified SQL table conforming to [[catalog.]schema.]table.",
            "    \"\"\"",
            "",
            "    table: str",
            "    schema: str | None = None",
            "    catalog: str | None = None",
            "",
            "    def __str__(self) -> str:",
            "        \"\"\"",
            "        Return the fully qualified SQL table name.",
            "        \"\"\"",
            "",
            "        return \".\".join(",
            "            urllib.parse.quote(part, safe=\"\").replace(\".\", \"%2E\")",
            "            for part in [self.catalog, self.schema, self.table]",
            "            if part",
            "        )",
            "",
            "    def __eq__(self, __o: object) -> bool:",
            "        return str(self) == str(__o)",
            "",
            "",
            "class ParsedQuery:",
            "    def __init__(",
            "        self,",
            "        sql_statement: str,",
            "        strip_comments: bool = False,",
            "        engine: str | None = None,",
            "    ):",
            "        if strip_comments:",
            "            sql_statement = sqlparse.format(sql_statement, strip_comments=True)",
            "",
            "        self.sql: str = sql_statement",
            "        self._dialect = SQLGLOT_DIALECTS.get(engine) if engine else None",
            "        self._tables: set[Table] = set()",
            "        self._alias_names: set[str] = set()",
            "        self._limit: int | None = None",
            "",
            "        logger.debug(\"Parsing with sqlparse statement: %s\", self.sql)",
            "        self._parsed = sqlparse.parse(self.stripped())",
            "        for statement in self._parsed:",
            "            self._limit = _extract_limit_from_query(statement)",
            "",
            "    @property",
            "    def tables(self) -> set[Table]:",
            "        if not self._tables:",
            "            self._tables = self._extract_tables_from_sql()",
            "        return self._tables",
            "",
            "    def _check_functions_exist_in_token(",
            "        self, token: Token, functions: set[str]",
            "    ) -> bool:",
            "        if (",
            "            isinstance(token, Function)",
            "            and token.get_name() is not None",
            "            and token.get_name().lower() in functions",
            "        ):",
            "            return True",
            "        if hasattr(token, \"tokens\"):",
            "            for inner_token in token.tokens:",
            "                if self._check_functions_exist_in_token(inner_token, functions):",
            "                    return True",
            "        return False",
            "",
            "    def check_functions_exist(self, functions: set[str]) -> bool:",
            "        \"\"\"",
            "        Check if the SQL statement contains any of the specified functions.",
            "",
            "        :param functions: A set of functions to search for",
            "        :return: True if the statement contains any of the specified functions",
            "        \"\"\"",
            "        for statement in self._parsed:",
            "            for token in statement.tokens:",
            "                if self._check_functions_exist_in_token(token, functions):",
            "                    return True",
            "        return False",
            "",
            "    def _extract_tables_from_sql(self) -> set[Table]:",
            "        \"\"\"",
            "        Extract all table references in a query.",
            "",
            "        Note: this uses sqlglot, since it's better at catching more edge cases.",
            "        \"\"\"",
            "        try:",
            "            statements = parse(self.stripped(), dialect=self._dialect)",
            "        except SqlglotError as ex:",
            "            logger.warning(\"Unable to parse SQL (%s): %s\", self._dialect, self.sql)",
            "",
            "            message = (",
            "                \"Error parsing near '{highlight}' at line {line}:{col}\".format(  # pylint: disable=consider-using-f-string",
            "                    **ex.errors[0]",
            "                )",
            "                if isinstance(ex, ParseError)",
            "                else str(ex)",
            "            )",
            "",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.QUERY_SECURITY_ACCESS_ERROR,",
            "                    message=__(",
            "                        f\"You may have an error in your SQL statement. {message}\"",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            ) from ex",
            "",
            "        return {",
            "            table",
            "            for statement in statements",
            "            for table in self._extract_tables_from_statement(statement)",
            "            if statement",
            "        }",
            "",
            "    def _extract_tables_from_statement(self, statement: exp.Expression) -> set[Table]:",
            "        \"\"\"",
            "        Extract all table references in a single statement.",
            "",
            "        Please not that this is not trivial; consider the following queries:",
            "",
            "            DESCRIBE some_table;",
            "            SHOW PARTITIONS FROM some_table;",
            "            WITH masked_name AS (SELECT * FROM some_table) SELECT * FROM masked_name;",
            "",
            "        See the unit tests for other tricky cases.",
            "        \"\"\"",
            "        sources: Iterable[exp.Table]",
            "",
            "        if isinstance(statement, exp.Describe):",
            "            # A `DESCRIBE` query has no sources in sqlglot, so we need to explicitly",
            "            # query for all tables.",
            "            sources = statement.find_all(exp.Table)",
            "        elif isinstance(statement, exp.Command):",
            "            # Commands, like `SHOW COLUMNS FROM foo`, have to be converted into a",
            "            # `SELECT` statetement in order to extract tables.",
            "            if not (literal := statement.find(exp.Literal)):",
            "                return set()",
            "",
            "            try:",
            "                pseudo_query = parse_one(",
            "                    f\"SELECT {literal.this}\",",
            "                    dialect=self._dialect,",
            "                )",
            "                sources = pseudo_query.find_all(exp.Table)",
            "            except SqlglotError:",
            "                return set()",
            "        else:",
            "            sources = [",
            "                source",
            "                for scope in traverse_scope(statement)",
            "                for source in scope.sources.values()",
            "                if isinstance(source, exp.Table) and not self._is_cte(source, scope)",
            "            ]",
            "",
            "        return {",
            "            Table(",
            "                source.name,",
            "                source.db if source.db != \"\" else None,",
            "                source.catalog if source.catalog != \"\" else None,",
            "            )",
            "            for source in sources",
            "        }",
            "",
            "    def _is_cte(self, source: exp.Table, scope: Scope) -> bool:",
            "        \"\"\"",
            "        Is the source a CTE?",
            "",
            "        CTEs in the parent scope look like tables (and are represented by",
            "        exp.Table objects), but should not be considered as such;",
            "        otherwise a user with access to table `foo` could access any table",
            "        with a query like this:",
            "",
            "            WITH foo AS (SELECT * FROM target_table) SELECT * FROM foo",
            "",
            "        \"\"\"",
            "        parent_sources = scope.parent.sources if scope.parent else {}",
            "        ctes_in_scope = {",
            "            name",
            "            for name, parent_scope in parent_sources.items()",
            "            if isinstance(parent_scope, Scope)",
            "            and parent_scope.scope_type == ScopeType.CTE",
            "        }",
            "",
            "        return source.name in ctes_in_scope",
            "",
            "    @property",
            "    def limit(self) -> int | None:",
            "        return self._limit",
            "",
            "    def _get_cte_tables(self, parsed: dict[str, Any]) -> list[dict[str, Any]]:",
            "        if \"with\" not in parsed:",
            "            return []",
            "        return parsed[\"with\"].get(\"cte_tables\", [])",
            "",
            "    def _check_cte_is_select(self, oxide_parse: list[dict[str, Any]]) -> bool:",
            "        \"\"\"",
            "        Check if a oxide parsed CTE contains only SELECT statements",
            "",
            "        :param oxide_parse: parsed CTE",
            "        :return: True if CTE is a SELECT statement",
            "        \"\"\"",
            "",
            "        def is_body_select(body: dict[str, Any]) -> bool:",
            "            if op := body.get(\"SetOperation\"):",
            "                return is_body_select(op[\"left\"]) and is_body_select(op[\"right\"])",
            "            return all(key == \"Select\" for key in body.keys())",
            "",
            "        for query in oxide_parse:",
            "            parsed_query = query[\"Query\"]",
            "            cte_tables = self._get_cte_tables(parsed_query)",
            "            for cte_table in cte_tables:",
            "                is_select = is_body_select(cte_table[\"query\"][\"body\"])",
            "                if not is_select:",
            "                    return False",
            "        return True",
            "",
            "    def is_select(self) -> bool:",
            "        # make sure we strip comments; prevents a bug with comments in the CTE",
            "        parsed = sqlparse.parse(self.strip_comments())",
            "        seen_select = False",
            "",
            "        for statement in parsed:",
            "            # Check if this is a CTE",
            "            if statement.is_group and statement[0].ttype == Keyword.CTE:",
            "                if sqloxide_parse is not None:",
            "                    try:",
            "                        if not self._check_cte_is_select(",
            "                            sqloxide_parse(self.strip_comments(), dialect=\"ansi\")",
            "                        ):",
            "                            return False",
            "                    except ValueError:",
            "                        # sqloxide was not able to parse the query, so let's continue with",
            "                        # sqlparse",
            "                        pass",
            "                inner_cte = self.get_inner_cte_expression(statement.tokens) or []",
            "                # Check if the inner CTE is a not a SELECT",
            "                if any(token.ttype == DDL for token in inner_cte) or any(",
            "                    token.ttype == DML and token.normalized != \"SELECT\"",
            "                    for token in inner_cte",
            "                ):",
            "                    return False",
            "",
            "            if statement.get_type() == \"SELECT\":",
            "                seen_select = True",
            "                continue",
            "",
            "            if statement.get_type() != \"UNKNOWN\":",
            "                return False",
            "",
            "            # for `UNKNOWN`, check all DDL/DML explicitly: only `SELECT` DML is allowed,",
            "            # and no DDL is allowed",
            "            if any(token.ttype == DDL for token in statement) or any(",
            "                token.ttype == DML and token.normalized != \"SELECT\"",
            "                for token in statement",
            "            ):",
            "                return False",
            "",
            "            if imt(statement.tokens[0], m=(Keyword, \"USE\")):",
            "                continue",
            "",
            "            # return false on `EXPLAIN`, `SET`, `SHOW`, etc.",
            "            if imt(statement.tokens[0], t=Keyword):",
            "                return False",
            "",
            "            if not any(",
            "                token.ttype == DML and token.normalized == \"SELECT\"",
            "                for token in statement",
            "            ):",
            "                return False",
            "",
            "        return seen_select",
            "",
            "    def get_inner_cte_expression(self, tokens: TokenList) -> TokenList | None:",
            "        for token in tokens:",
            "            if self._is_identifier(token):",
            "                for identifier_token in token.tokens:",
            "                    if (",
            "                        isinstance(identifier_token, Parenthesis)",
            "                        and identifier_token.is_group",
            "                    ):",
            "                        return identifier_token.tokens",
            "        return None",
            "",
            "    def is_valid_ctas(self) -> bool:",
            "        parsed = sqlparse.parse(self.strip_comments())",
            "        return parsed[-1].get_type() == \"SELECT\"",
            "",
            "    def is_valid_cvas(self) -> bool:",
            "        parsed = sqlparse.parse(self.strip_comments())",
            "        return len(parsed) == 1 and parsed[0].get_type() == \"SELECT\"",
            "",
            "    def is_explain(self) -> bool:",
            "        # Remove comments",
            "        statements_without_comments = sqlparse.format(",
            "            self.stripped(), strip_comments=True",
            "        )",
            "",
            "        # Explain statements will only be the first statement",
            "        return statements_without_comments.upper().startswith(\"EXPLAIN\")",
            "",
            "    def is_show(self) -> bool:",
            "        # Remove comments",
            "        statements_without_comments = sqlparse.format(",
            "            self.stripped(), strip_comments=True",
            "        )",
            "        # Show statements will only be the first statement",
            "        return statements_without_comments.upper().startswith(\"SHOW\")",
            "",
            "    def is_set(self) -> bool:",
            "        # Remove comments",
            "        statements_without_comments = sqlparse.format(",
            "            self.stripped(), strip_comments=True",
            "        )",
            "        # Set statements will only be the first statement",
            "        return statements_without_comments.upper().startswith(\"SET\")",
            "",
            "    def is_unknown(self) -> bool:",
            "        return self._parsed[0].get_type() == \"UNKNOWN\"",
            "",
            "    def stripped(self) -> str:",
            "        return self.sql.strip(\" \\t\\r\\n;\")",
            "",
            "    def strip_comments(self) -> str:",
            "        return sqlparse.format(self.stripped(), strip_comments=True)",
            "",
            "    def get_statements(self) -> list[str]:",
            "        \"\"\"Returns a list of SQL statements as strings, stripped\"\"\"",
            "        statements = []",
            "        for statement in self._parsed:",
            "            if statement:",
            "                sql = str(statement).strip(\" \\n;\\t\")",
            "                if sql:",
            "                    statements.append(sql)",
            "        return statements",
            "",
            "    @staticmethod",
            "    def get_table(tlist: TokenList) -> Table | None:",
            "        \"\"\"",
            "        Return the table if valid, i.e., conforms to the [[catalog.]schema.]table",
            "        construct.",
            "",
            "        :param tlist: The SQL tokens",
            "        :returns: The table if the name conforms",
            "        \"\"\"",
            "",
            "        # Strip the alias if present.",
            "        idx = len(tlist.tokens)",
            "",
            "        if tlist.has_alias():",
            "            ws_idx, _ = tlist.token_next_by(t=Whitespace)",
            "",
            "            if ws_idx != -1:",
            "                idx = ws_idx",
            "",
            "        tokens = tlist.tokens[:idx]",
            "",
            "        if (",
            "            len(tokens) in (1, 3, 5)",
            "            and all(imt(token, t=[Name, String]) for token in tokens[::2])",
            "            and all(imt(token, m=(Punctuation, \".\")) for token in tokens[1::2])",
            "        ):",
            "            return Table(*[remove_quotes(token.value) for token in tokens[::-2]])",
            "",
            "        return None",
            "",
            "    @staticmethod",
            "    def _is_identifier(token: Token) -> bool:",
            "        return isinstance(token, (IdentifierList, Identifier))",
            "",
            "    def as_create_table(",
            "        self,",
            "        table_name: str,",
            "        schema_name: str | None = None,",
            "        overwrite: bool = False,",
            "        method: CtasMethod = CtasMethod.TABLE,",
            "    ) -> str:",
            "        \"\"\"Reformats the query into the create table as query.",
            "",
            "        Works only for the single select SQL statements, in all other cases",
            "        the sql query is not modified.",
            "        :param table_name: table that will contain the results of the query execution",
            "        :param schema_name: schema name for the target table",
            "        :param overwrite: table_name will be dropped if true",
            "        :param method: method for the CTA query, currently view or table creation",
            "        :return: Create table as query",
            "        \"\"\"",
            "        exec_sql = \"\"",
            "        sql = self.stripped()",
            "        # TODO(bkyryliuk): quote full_table_name",
            "        full_table_name = f\"{schema_name}.{table_name}\" if schema_name else table_name",
            "        if overwrite:",
            "            exec_sql = f\"DROP {method} IF EXISTS {full_table_name};\\n\"",
            "        exec_sql += f\"CREATE {method} {full_table_name} AS \\n{sql}\"",
            "        return exec_sql",
            "",
            "    def set_or_update_query_limit(self, new_limit: int, force: bool = False) -> str:",
            "        \"\"\"Returns the query with the specified limit.",
            "",
            "        Does not change the underlying query if user did not apply the limit,",
            "        otherwise replaces the limit with the lower value between existing limit",
            "        in the query and new_limit.",
            "",
            "        :param new_limit: Limit to be incorporated into returned query",
            "        :return: The original query with new limit",
            "        \"\"\"",
            "        if not self._limit:",
            "            return f\"{self.stripped()}\\nLIMIT {new_limit}\"",
            "        limit_pos = None",
            "        statement = self._parsed[0]",
            "        # Add all items to before_str until there is a limit",
            "        for pos, item in enumerate(statement.tokens):",
            "            if item.ttype in Keyword and item.value.lower() == \"limit\":",
            "                limit_pos = pos",
            "                break",
            "        _, limit = statement.token_next(idx=limit_pos)",
            "        # Override the limit only when it exceeds the configured value.",
            "        if limit.ttype == sqlparse.tokens.Literal.Number.Integer and (",
            "            force or new_limit < int(limit.value)",
            "        ):",
            "            limit.value = new_limit",
            "        elif limit.is_group:",
            "            limit.value = f\"{next(limit.get_identifiers())}, {new_limit}\"",
            "",
            "        str_res = \"\"",
            "        for i in statement.tokens:",
            "            str_res += str(i.value)",
            "        return str_res",
            "",
            "",
            "def sanitize_clause(clause: str) -> str:",
            "    # clause = sqlparse.format(clause, strip_comments=True)",
            "    statements = sqlparse.parse(clause)",
            "    if len(statements) != 1:",
            "        raise QueryClauseValidationException(\"Clause contains multiple statements\")",
            "    open_parens = 0",
            "",
            "    previous_token = None",
            "    for token in statements[0]:",
            "        if token.value == \"/\" and previous_token and previous_token.value == \"*\":",
            "            raise QueryClauseValidationException(\"Closing unopened multiline comment\")",
            "        if token.value == \"*\" and previous_token and previous_token.value == \"/\":",
            "            raise QueryClauseValidationException(\"Unclosed multiline comment\")",
            "        if token.value in (\")\", \"(\"):",
            "            open_parens += 1 if token.value == \"(\" else -1",
            "            if open_parens < 0:",
            "                raise QueryClauseValidationException(",
            "                    \"Closing unclosed parenthesis in filter clause\"",
            "                )",
            "        previous_token = token",
            "    if open_parens > 0:",
            "        raise QueryClauseValidationException(\"Unclosed parenthesis in filter clause\")",
            "",
            "    if previous_token and previous_token.ttype in Comment:",
            "        if previous_token.value[-1] != \"\\n\":",
            "            clause = f\"{clause}\\n\"",
            "",
            "    return clause",
            "",
            "",
            "class InsertRLSState(StrEnum):",
            "    \"\"\"",
            "    State machine that scans for WHERE and ON clauses referencing tables.",
            "    \"\"\"",
            "",
            "    SCANNING = \"SCANNING\"",
            "    SEEN_SOURCE = \"SEEN_SOURCE\"",
            "    FOUND_TABLE = \"FOUND_TABLE\"",
            "",
            "",
            "def has_table_query(token_list: TokenList) -> bool:",
            "    \"\"\"",
            "    Return if a statement has a query reading from a table.",
            "",
            "        >>> has_table_query(sqlparse.parse(\"COUNT(*)\")[0])",
            "        False",
            "        >>> has_table_query(sqlparse.parse(\"SELECT * FROM table\")[0])",
            "        True",
            "",
            "    Note that queries reading from constant values return false:",
            "",
            "        >>> has_table_query(sqlparse.parse(\"SELECT * FROM (SELECT 1)\")[0])",
            "        False",
            "",
            "    \"\"\"",
            "    state = InsertRLSState.SCANNING",
            "    for token in token_list.tokens:",
            "        # Ignore comments",
            "        if isinstance(token, sqlparse.sql.Comment):",
            "            continue",
            "",
            "        # Recurse into child token list",
            "        if isinstance(token, TokenList) and has_table_query(token):",
            "            return True",
            "",
            "        # Found a source keyword (FROM/JOIN)",
            "        if imt(token, m=[(Keyword, \"FROM\"), (Keyword, \"JOIN\")]):",
            "            state = InsertRLSState.SEEN_SOURCE",
            "",
            "        # Found identifier/keyword after FROM/JOIN",
            "        elif state == InsertRLSState.SEEN_SOURCE and (",
            "            isinstance(token, sqlparse.sql.Identifier) or token.ttype == Keyword",
            "        ):",
            "            return True",
            "",
            "        # Found nothing, leaving source",
            "        elif state == InsertRLSState.SEEN_SOURCE and token.ttype != Whitespace:",
            "            state = InsertRLSState.SCANNING",
            "",
            "    return False",
            "",
            "",
            "def add_table_name(rls: TokenList, table: str) -> None:",
            "    \"\"\"",
            "    Modify a RLS expression inplace ensuring columns are fully qualified.",
            "    \"\"\"",
            "    tokens = rls.tokens[:]",
            "    while tokens:",
            "        token = tokens.pop(0)",
            "",
            "        if isinstance(token, Identifier) and token.get_parent_name() is None:",
            "            token.tokens = [",
            "                Token(Name, table),",
            "                Token(Punctuation, \".\"),",
            "                Token(Name, token.get_name()),",
            "            ]",
            "        elif isinstance(token, TokenList):",
            "            tokens.extend(token.tokens)",
            "",
            "",
            "def get_rls_for_table(",
            "    candidate: Token,",
            "    database_id: int,",
            "    default_schema: str | None,",
            ") -> TokenList | None:",
            "    \"\"\"",
            "    Given a table name, return any associated RLS predicates.",
            "    \"\"\"",
            "    # pylint: disable=import-outside-toplevel",
            "    from superset import db",
            "    from superset.connectors.sqla.models import SqlaTable",
            "",
            "    if not isinstance(candidate, Identifier):",
            "        candidate = Identifier([Token(Name, candidate.value)])",
            "",
            "    table = ParsedQuery.get_table(candidate)",
            "    if not table:",
            "        return None",
            "",
            "    dataset = (",
            "        db.session.query(SqlaTable)",
            "        .filter(",
            "            and_(",
            "                SqlaTable.database_id == database_id,",
            "                SqlaTable.schema == (table.schema or default_schema),",
            "                SqlaTable.table_name == table.table,",
            "            )",
            "        )",
            "        .one_or_none()",
            "    )",
            "    if not dataset:",
            "        return None",
            "",
            "    template_processor = dataset.get_template_processor()",
            "    predicate = \" AND \".join(",
            "        str(filter_)",
            "        for filter_ in dataset.get_sqla_row_level_filters(template_processor)",
            "    )",
            "    if not predicate:",
            "        return None",
            "",
            "    rls = sqlparse.parse(predicate)[0]",
            "    add_table_name(rls, table.table)",
            "",
            "    return rls",
            "",
            "",
            "def insert_rls_as_subquery(",
            "    token_list: TokenList,",
            "    database_id: int,",
            "    default_schema: str | None,",
            ") -> TokenList:",
            "    \"\"\"",
            "    Update a statement inplace applying any associated RLS predicates.",
            "",
            "    The RLS predicate is applied as subquery replacing the original table:",
            "",
            "        before: SELECT * FROM some_table WHERE 1=1",
            "        after:  SELECT * FROM (",
            "                  SELECT * FROM some_table WHERE some_table.id=42",
            "                ) AS some_table",
            "                WHERE 1=1",
            "",
            "    This method is safer than ``insert_rls_in_predicate``, but doesn't work in all",
            "    databases.",
            "    \"\"\"",
            "    rls: TokenList | None = None",
            "    state = InsertRLSState.SCANNING",
            "    for token in token_list.tokens:",
            "        # Recurse into child token list",
            "        if isinstance(token, TokenList):",
            "            i = token_list.tokens.index(token)",
            "            token_list.tokens[i] = insert_rls_as_subquery(",
            "                token,",
            "                database_id,",
            "                default_schema,",
            "            )",
            "",
            "        # Found a source keyword (FROM/JOIN)",
            "        if imt(token, m=[(Keyword, \"FROM\"), (Keyword, \"JOIN\")]):",
            "            state = InsertRLSState.SEEN_SOURCE",
            "",
            "        # Found identifier/keyword after FROM/JOIN, test for table",
            "        elif state == InsertRLSState.SEEN_SOURCE and (",
            "            isinstance(token, Identifier) or token.ttype == Keyword",
            "        ):",
            "            rls = get_rls_for_table(token, database_id, default_schema)",
            "            if rls:",
            "                # replace table with subquery",
            "                subquery_alias = (",
            "                    token.tokens[-1].value",
            "                    if isinstance(token, Identifier)",
            "                    else token.value",
            "                )",
            "                i = token_list.tokens.index(token)",
            "",
            "                # strip alias from table name",
            "                if isinstance(token, Identifier) and token.has_alias():",
            "                    whitespace_index = token.token_next_by(t=Whitespace)[0]",
            "                    token.tokens = token.tokens[:whitespace_index]",
            "",
            "                token_list.tokens[i] = Identifier(",
            "                    [",
            "                        Parenthesis(",
            "                            [",
            "                                Token(Punctuation, \"(\"),",
            "                                Token(DML, \"SELECT\"),",
            "                                Token(Whitespace, \" \"),",
            "                                Token(Wildcard, \"*\"),",
            "                                Token(Whitespace, \" \"),",
            "                                Token(Keyword, \"FROM\"),",
            "                                Token(Whitespace, \" \"),",
            "                                token,",
            "                                Token(Whitespace, \" \"),",
            "                                Where(",
            "                                    [",
            "                                        Token(Keyword, \"WHERE\"),",
            "                                        Token(Whitespace, \" \"),",
            "                                        rls,",
            "                                    ]",
            "                                ),",
            "                                Token(Punctuation, \")\"),",
            "                            ]",
            "                        ),",
            "                        Token(Whitespace, \" \"),",
            "                        Token(Keyword, \"AS\"),",
            "                        Token(Whitespace, \" \"),",
            "                        Identifier([Token(Name, subquery_alias)]),",
            "                    ]",
            "                )",
            "                state = InsertRLSState.SCANNING",
            "",
            "        # Found nothing, leaving source",
            "        elif state == InsertRLSState.SEEN_SOURCE and token.ttype != Whitespace:",
            "            state = InsertRLSState.SCANNING",
            "",
            "    return token_list",
            "",
            "",
            "def insert_rls_in_predicate(",
            "    token_list: TokenList,",
            "    database_id: int,",
            "    default_schema: str | None,",
            ") -> TokenList:",
            "    \"\"\"",
            "    Update a statement inplace applying any associated RLS predicates.",
            "",
            "    The RLS predicate is ``AND``ed to any existing predicates:",
            "",
            "        before: SELECT * FROM some_table WHERE 1=1",
            "        after:  SELECT * FROM some_table WHERE ( 1=1) AND some_table.id=42",
            "",
            "    \"\"\"",
            "    rls: TokenList | None = None",
            "    state = InsertRLSState.SCANNING",
            "    for token in token_list.tokens:",
            "        # Recurse into child token list",
            "        if isinstance(token, TokenList):",
            "            i = token_list.tokens.index(token)",
            "            token_list.tokens[i] = insert_rls_in_predicate(",
            "                token,",
            "                database_id,",
            "                default_schema,",
            "            )",
            "",
            "        # Found a source keyword (FROM/JOIN)",
            "        if imt(token, m=[(Keyword, \"FROM\"), (Keyword, \"JOIN\")]):",
            "            state = InsertRLSState.SEEN_SOURCE",
            "",
            "        # Found identifier/keyword after FROM/JOIN, test for table",
            "        elif state == InsertRLSState.SEEN_SOURCE and (",
            "            isinstance(token, Identifier) or token.ttype == Keyword",
            "        ):",
            "            rls = get_rls_for_table(token, database_id, default_schema)",
            "            if rls:",
            "                state = InsertRLSState.FOUND_TABLE",
            "",
            "        # Found WHERE clause, insert RLS. Note that we insert it even it already exists,",
            "        # to be on the safe side: it could be present in a clause like `1=1 OR RLS`.",
            "        elif state == InsertRLSState.FOUND_TABLE and isinstance(token, Where):",
            "            rls = cast(TokenList, rls)",
            "            token.tokens[1:1] = [Token(Whitespace, \" \"), Token(Punctuation, \"(\")]",
            "            token.tokens.extend(",
            "                [",
            "                    Token(Punctuation, \")\"),",
            "                    Token(Whitespace, \" \"),",
            "                    Token(Keyword, \"AND\"),",
            "                    Token(Whitespace, \" \"),",
            "                ]",
            "                + rls.tokens",
            "            )",
            "            state = InsertRLSState.SCANNING",
            "",
            "        # Found ON clause, insert RLS. The logic for ON is more complicated than the logic",
            "        # for WHERE because in the former the comparisons are siblings, while on the",
            "        # latter they are children.",
            "        elif (",
            "            state == InsertRLSState.FOUND_TABLE",
            "            and token.ttype == Keyword",
            "            and token.value.upper() == \"ON\"",
            "        ):",
            "            tokens = [",
            "                Token(Whitespace, \" \"),",
            "                rls,",
            "                Token(Whitespace, \" \"),",
            "                Token(Keyword, \"AND\"),",
            "                Token(Whitespace, \" \"),",
            "                Token(Punctuation, \"(\"),",
            "            ]",
            "            i = token_list.tokens.index(token)",
            "            token.parent.tokens[i + 1 : i + 1] = tokens",
            "            i += len(tokens) + 2",
            "",
            "            # close parenthesis after last existing comparison",
            "            j = 0",
            "            for j, sibling in enumerate(token_list.tokens[i:]):",
            "                # scan until we hit a non-comparison keyword (like ORDER BY) or a WHERE",
            "                if (",
            "                    sibling.ttype == Keyword",
            "                    and not imt(",
            "                        sibling, m=[(Keyword, \"AND\"), (Keyword, \"OR\"), (Keyword, \"NOT\")]",
            "                    )",
            "                    or isinstance(sibling, Where)",
            "                ):",
            "                    j -= 1",
            "                    break",
            "            token.parent.tokens[i + j + 1 : i + j + 1] = [",
            "                Token(Whitespace, \" \"),",
            "                Token(Punctuation, \")\"),",
            "                Token(Whitespace, \" \"),",
            "            ]",
            "",
            "            state = InsertRLSState.SCANNING",
            "",
            "        # Found table but no WHERE clause found, insert one",
            "        elif state == InsertRLSState.FOUND_TABLE and token.ttype != Whitespace:",
            "            i = token_list.tokens.index(token)",
            "            token_list.tokens[i:i] = [",
            "                Token(Whitespace, \" \"),",
            "                Where([Token(Keyword, \"WHERE\"), Token(Whitespace, \" \"), rls]),",
            "                Token(Whitespace, \" \"),",
            "            ]",
            "",
            "            state = InsertRLSState.SCANNING",
            "",
            "        # Found nothing, leaving source",
            "        elif state == InsertRLSState.SEEN_SOURCE and token.ttype != Whitespace:",
            "            state = InsertRLSState.SCANNING",
            "",
            "    # found table at the end of the statement; append a WHERE clause",
            "    if state == InsertRLSState.FOUND_TABLE:",
            "        token_list.tokens.extend(",
            "            [",
            "                Token(Whitespace, \" \"),",
            "                Where([Token(Keyword, \"WHERE\"), Token(Whitespace, \" \"), rls]),",
            "            ]",
            "        )",
            "",
            "    return token_list",
            "",
            "",
            "# mapping between sqloxide and SQLAlchemy dialects",
            "SQLOXIDE_DIALECTS = {",
            "    \"ansi\": {\"trino\", \"trinonative\", \"presto\"},",
            "    \"hive\": {\"hive\", \"databricks\"},",
            "    \"ms\": {\"mssql\"},",
            "    \"mysql\": {\"mysql\"},",
            "    \"postgres\": {",
            "        \"cockroachdb\",",
            "        \"hana\",",
            "        \"netezza\",",
            "        \"postgres\",",
            "        \"postgresql\",",
            "        \"redshift\",",
            "        \"vertica\",",
            "    },",
            "    \"snowflake\": {\"snowflake\"},",
            "    \"sqlite\": {\"sqlite\", \"gsheets\", \"shillelagh\"},",
            "    \"clickhouse\": {\"clickhouse\"},",
            "}",
            "",
            "RE_JINJA_VAR = re.compile(r\"\\{\\{[^\\{\\}]+\\}\\}\")",
            "RE_JINJA_BLOCK = re.compile(r\"\\{[%#][^\\{\\}%#]+[%#]\\}\")",
            "",
            "",
            "def extract_table_references(",
            "    sql_text: str, sqla_dialect: str, show_warning: bool = True",
            ") -> set[Table]:",
            "    \"\"\"",
            "    Return all the dependencies from a SQL sql_text.",
            "    \"\"\"",
            "    dialect = \"generic\"",
            "    tree = None",
            "",
            "    if sqloxide_parse:",
            "        for dialect, sqla_dialects in SQLOXIDE_DIALECTS.items():",
            "            if sqla_dialect in sqla_dialects:",
            "                break",
            "        sql_text = RE_JINJA_BLOCK.sub(\" \", sql_text)",
            "        sql_text = RE_JINJA_VAR.sub(\"abc\", sql_text)",
            "        try:",
            "            tree = sqloxide_parse(sql_text, dialect=dialect)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            if show_warning:",
            "                logger.warning(",
            "                    \"\\nUnable to parse query with sqloxide:\\n%s\\n%s\", sql_text, ex",
            "                )",
            "",
            "    # fallback to sqlparse",
            "    if not tree:",
            "        parsed = ParsedQuery(sql_text)",
            "        return parsed.tables",
            "",
            "    def find_nodes_by_key(element: Any, target: str) -> Iterator[Any]:",
            "        \"\"\"",
            "        Find all nodes in a SQL tree matching a given key.",
            "        \"\"\"",
            "        if isinstance(element, list):",
            "            for child in element:",
            "                yield from find_nodes_by_key(child, target)",
            "        elif isinstance(element, dict):",
            "            for key, value in element.items():",
            "                if key == target:",
            "                    yield value",
            "                else:",
            "                    yield from find_nodes_by_key(value, target)",
            "",
            "    return {",
            "        Table(*[part[\"value\"] for part in table[\"name\"][::-1]])",
            "        for table in find_nodes_by_key(tree, \"Table\")",
            "    }",
            "",
            "",
            "def extract_tables_from_jinja_sql(sql: str, database: Database) -> set[Table]:",
            "    \"\"\"",
            "    Extract all table references in the Jinjafied SQL statement.",
            "",
            "    Due to Jinja templating, a multiphase approach is necessary as the Jinjafied SQL",
            "    statement may represent invalid SQL which is non-parsable by SQLGlot.",
            "",
            "    Firstly, we extract any tables referenced within the confines of specific Jinja",
            "    macros. Secondly, we replace these non-SQL Jinja calls with a pseudo-benign SQL",
            "    expression to help ensure that the resulting SQL statements are parsable by",
            "    SQLGlot.",
            "",
            "    :param sql: The Jinjafied SQL statement",
            "    :param database: The database associated with the SQL statement",
            "    :returns: The set of tables referenced in the SQL statement",
            "    :raises SupersetSecurityException: If SQLGlot is unable to parse the SQL statement",
            "    :raises jinja2.exceptions.TemplateError: If the Jinjafied SQL could not be rendered",
            "    \"\"\"",
            "",
            "    from superset.jinja_context import (  # pylint: disable=import-outside-toplevel",
            "        get_template_processor,",
            "    )",
            "",
            "    processor = get_template_processor(database)",
            "    template = processor.env.parse(sql)",
            "",
            "    tables = set()",
            "",
            "    for node in template.find_all(nodes.Call):",
            "        if isinstance(node.node, nodes.Getattr) and node.node.attr in (",
            "            \"latest_partition\",",
            "            \"latest_sub_partition\",",
            "        ):",
            "            # Try to extract the table referenced in the macro.",
            "            try:",
            "                tables.add(",
            "                    Table(",
            "                        *[",
            "                            remove_quotes(part.strip())",
            "                            for part in node.args[0].as_const().split(\".\")[::-1]",
            "                            if len(node.args) == 1",
            "                        ]",
            "                    )",
            "                )",
            "            except nodes.Impossible:",
            "                pass",
            "",
            "            # Replace the potentially problematic Jinja macro with some benign SQL.",
            "            node.__class__ = nodes.TemplateData",
            "            node.fields = nodes.TemplateData.fields",
            "            node.data = \"NULL\"",
            "",
            "    return (",
            "        tables",
            "        | ParsedQuery(",
            "            sql_statement=processor.process_template(template),",
            "            engine=database.db_engine_spec.engine,",
            "        ).tables",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "superset.sql_parse.extract_tables_from_jinja_sql",
            "superset.sql_parse.ParsedQuery.self",
            "superset.sql_parse.extract_table_references",
            "superset.sql_parse.extract_table_references.parsed"
        ]
    }
}