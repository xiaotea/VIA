{
    "pandasai/agent/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 259,
                "afterPatchRowNumber": 259,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": 260,
                "PatchRowcode": "             self.assign_prompt_id()"
            },
            "2": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 261,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if self.check_malicious_keywords_in_query(query):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 262,
                "PatchRowcode": "+            if self.config.security in ["
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 263,
                "PatchRowcode": "+                \"standard\","
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 264,
                "PatchRowcode": "+                \"advanced\","
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 265,
                "PatchRowcode": "+            ] and self.check_malicious_keywords_in_query(query):"
            },
            "8": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "                 raise MaliciousQueryError("
            },
            "9": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 267,
                "PatchRowcode": "                     \"The query contains references to io or os modules or b64decode method which can be used to execute or access system resources in unsafe ways.\""
            },
            "10": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "                 )"
            }
        },
        "frontPatchFile": [
            "import json",
            "import os",
            "import re",
            "import uuid",
            "from typing import List, Optional, Union",
            "",
            "import pandasai.pandas as pd",
            "from pandasai.agent.base_security import BaseSecurity",
            "from pandasai.llm.bamboo_llm import BambooLLM",
            "from pandasai.pipelines.chat.chat_pipeline_input import ChatPipelineInput",
            "from pandasai.pipelines.chat.code_execution_pipeline_input import (",
            "    CodeExecutionPipelineInput,",
            ")",
            "from pandasai.vectorstores.vectorstore import VectorStore",
            "",
            "from ..config import load_config_from_json",
            "from ..connectors import BaseConnector, PandasConnector",
            "from ..constants import DEFAULT_CACHE_DIRECTORY, DEFAULT_CHART_DIRECTORY",
            "from ..exceptions import (",
            "    InvalidLLMOutputType,",
            "    MaliciousQueryError,",
            "    MissingVectorStoreError,",
            ")",
            "from ..helpers.df_info import df_type",
            "from ..helpers.folder import Folder",
            "from ..helpers.logger import Logger",
            "from ..helpers.memory import Memory",
            "from ..llm.base import LLM",
            "from ..llm.langchain import LangchainLLM, is_langchain_llm",
            "from ..pipelines.pipeline_context import PipelineContext",
            "from ..prompts.base import BasePrompt",
            "from ..prompts.clarification_questions_prompt import ClarificationQuestionPrompt",
            "from ..prompts.explain_prompt import ExplainPrompt",
            "from ..prompts.rephase_query_prompt import RephraseQueryPrompt",
            "from ..schemas.df_config import Config",
            "from ..skills import Skill",
            "from .callbacks import Callbacks",
            "",
            "",
            "class BaseAgent:",
            "    \"\"\"",
            "    Base Agent class to improve the conversational experience in PandasAI",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        dfs: Union[",
            "            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]",
            "        ],",
            "        config: Optional[Union[Config, dict]] = None,",
            "        memory_size: Optional[int] = 10,",
            "        vectorstore: Optional[VectorStore] = None,",
            "        description: str = None,",
            "        security: BaseSecurity = None,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            df (Union[pd.DataFrame, List[pd.DataFrame]]): Pandas or Modin dataframe",
            "            Polars or Database connectors",
            "            memory_size (int, optional): Conversation history to use during chat.",
            "            Defaults to 1.",
            "        \"\"\"",
            "        self.last_prompt = None",
            "        self.last_prompt_id = None",
            "        self.last_result = None",
            "        self.last_code_generated = None",
            "        self.last_code_executed = None",
            "        self.agent_info = description",
            "",
            "        self.conversation_id = uuid.uuid4()",
            "",
            "        self.dfs = self.get_dfs(dfs)",
            "",
            "        # Instantiate the context",
            "        self.config = self.get_config(config)",
            "        self.context = PipelineContext(",
            "            dfs=self.dfs,",
            "            config=self.config,",
            "            memory=Memory(memory_size, agent_info=description),",
            "            vectorstore=vectorstore,",
            "        )",
            "",
            "        # Instantiate the logger",
            "        self.logger = Logger(",
            "            save_logs=self.config.save_logs, verbose=self.config.verbose",
            "        )",
            "",
            "        # Instantiate the vectorstore",
            "        self._vectorstore = vectorstore",
            "",
            "        if self._vectorstore is None and os.environ.get(\"PANDASAI_API_KEY\"):",
            "            try:",
            "                from pandasai.vectorstores.bamboo_vectorstore import BambooVectorStore",
            "            except ImportError as e:",
            "                raise ImportError(",
            "                    \"Could not import BambooVectorStore. Please install the required dependencies.\"",
            "                ) from e",
            "",
            "            self._vectorstore = BambooVectorStore(logger=self.logger)",
            "            self.context.vectorstore = self._vectorstore",
            "",
            "        self._callbacks = Callbacks(self)",
            "",
            "        self.configure()",
            "",
            "        self.pipeline = None",
            "        self.security = security",
            "",
            "    def configure(self):",
            "        # Add project root path if save_charts_path is default",
            "        if (",
            "            self.config.save_charts",
            "            and self.config.save_charts_path == DEFAULT_CHART_DIRECTORY",
            "        ):",
            "            Folder.create(self.config.save_charts_path)",
            "",
            "        # Add project root path if cache_path is default",
            "        if self.config.enable_cache:",
            "            Folder.create(DEFAULT_CACHE_DIRECTORY)",
            "",
            "    def get_config(self, config: Union[Config, dict]):",
            "        \"\"\"",
            "        Load a config to be used to run the queries.",
            "",
            "        Args:",
            "            config (Union[Config, dict]): Config to be used",
            "        \"\"\"",
            "",
            "        config = load_config_from_json(config)",
            "",
            "        if isinstance(config, dict) and config.get(\"llm\") is not None:",
            "            config[\"llm\"] = self.get_llm(config[\"llm\"])",
            "",
            "        config = Config(**config)",
            "",
            "        if config.llm is None:",
            "            config.llm = BambooLLM()",
            "",
            "        return config",
            "",
            "    def get_llm(self, llm: LLM) -> LLM:",
            "        \"\"\"",
            "        Load a LLM to be used to run the queries.",
            "        Check if it is a PandasAI LLM or a Langchain LLM.",
            "        If it is a Langchain LLM, wrap it in a PandasAI LLM.",
            "",
            "        Args:",
            "            llm (object): LLMs option to be used for API access",
            "",
            "        Raises:",
            "            BadImportError: If the LLM is a Langchain LLM but the langchain package",
            "            is not installed",
            "        \"\"\"",
            "        if is_langchain_llm(llm):",
            "            llm = LangchainLLM(llm)",
            "",
            "        return llm",
            "",
            "    def get_dfs(",
            "        self,",
            "        dfs: Union[",
            "            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]",
            "        ],",
            "    ):",
            "        \"\"\"",
            "        Load all the dataframes to be used in the agent.",
            "",
            "        Args:",
            "            dfs (List[Union[pd.DataFrame, Any]]): Pandas dataframe",
            "        \"\"\"",
            "        # Inline import to avoid circular import",
            "        from pandasai.smart_dataframe import SmartDataframe",
            "",
            "        # If only one dataframe is passed, convert it to a list",
            "        if not isinstance(dfs, list):",
            "            dfs = [dfs]",
            "",
            "        connectors = []",
            "        for df in dfs:",
            "            if isinstance(df, BaseConnector):",
            "                connectors.append(df)",
            "            elif isinstance(df, (pd.DataFrame, pd.Series, list, dict, str)):",
            "                connectors.append(PandasConnector({\"original_df\": df}))",
            "            elif df_type(df) == \"modin\":",
            "                connectors.append(PandasConnector({\"original_df\": df}))",
            "            elif isinstance(df, SmartDataframe) and isinstance(",
            "                df.dataframe, BaseConnector",
            "            ):",
            "                connectors.append(df.dataframe)",
            "            else:",
            "                try:",
            "                    import polars as pl",
            "",
            "                    if isinstance(df, pl.DataFrame):",
            "                        from ..connectors.polars import PolarsConnector",
            "",
            "                        connectors.append(PolarsConnector({\"original_df\": df}))",
            "",
            "                    else:",
            "                        raise ValueError(",
            "                            \"Invalid input data. We cannot convert it to a dataframe.\"",
            "                        )",
            "                except ImportError as e:",
            "                    raise ValueError(",
            "                        \"Invalid input data. We cannot convert it to a dataframe.\"",
            "                    ) from e",
            "        return connectors",
            "",
            "    def add_skills(self, *skills: Skill):",
            "        \"\"\"",
            "        Add Skills to PandasAI",
            "        \"\"\"",
            "        self.context.skills_manager.add_skills(*skills)",
            "",
            "    def call_llm_with_prompt(self, prompt: BasePrompt):",
            "        \"\"\"",
            "        Call LLM with prompt using error handling to retry based on config",
            "        Args:",
            "            prompt (BasePrompt): BasePrompt to pass to LLM's",
            "        \"\"\"",
            "        retry_count = 0",
            "        while retry_count < self.context.config.max_retries:",
            "            try:",
            "                result: str = self.context.config.llm.call(prompt)",
            "                if prompt.validate(result):",
            "                    return result",
            "                else:",
            "                    raise InvalidLLMOutputType(\"Response validation failed!\")",
            "            except Exception:",
            "                if (",
            "                    not self.context.config.use_error_correction_framework",
            "                    or retry_count >= self.context.config.max_retries - 1",
            "                ):",
            "                    raise",
            "                retry_count += 1",
            "",
            "    def check_malicious_keywords_in_query(self, query):",
            "        dangerous_pattern = re.compile(",
            "            r\"\\b(os|io|chr|b64decode)\\b|\"",
            "            r\"(\\.os|\\.io|'os'|'io'|\\\"os\\\"|\\\"io\\\"|chr\\(|chr\\)|chr |\\(chr)\"",
            "        )",
            "        return bool(dangerous_pattern.search(query))",
            "",
            "    def chat(self, query: str, output_type: Optional[str] = None):",
            "        \"\"\"",
            "        Simulate a chat interaction with the assistant on Dataframe.",
            "        \"\"\"",
            "        if not self.pipeline:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error: No pipeline exists\"",
            "            )",
            "",
            "        try:",
            "            self.logger.log(f\"Question: {query}\")",
            "            self.logger.log(",
            "                f\"Running PandasAI with {self.context.config.llm.type} LLM...\"",
            "            )",
            "",
            "            self.assign_prompt_id()",
            "",
            "            if self.check_malicious_keywords_in_query(query):",
            "                raise MaliciousQueryError(",
            "                    \"The query contains references to io or os modules or b64decode method which can be used to execute or access system resources in unsafe ways.\"",
            "                )",
            "",
            "            if self.security and self.security.evaluate(query):",
            "                raise MaliciousQueryError(\"Query can result in a malicious code\")",
            "",
            "            pipeline_input = ChatPipelineInput(",
            "                query, output_type, self.conversation_id, self.last_prompt_id",
            "            )",
            "",
            "            return self.pipeline.run(pipeline_input)",
            "",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def generate_code(self, query: str, output_type: Optional[str] = None):",
            "        \"\"\"",
            "        Simulate code generation with the assistant on Dataframe.",
            "        \"\"\"",
            "        if not self.pipeline:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error: No pipeline exists\"",
            "            )",
            "        try:",
            "            self.logger.log(f\"Question: {query}\")",
            "            self.logger.log(",
            "                f\"Running PandasAI with {self.context.config.llm.type} LLM...\"",
            "            )",
            "",
            "            self.assign_prompt_id()",
            "",
            "            pipeline_input = ChatPipelineInput(",
            "                query, output_type, self.conversation_id, self.last_prompt_id",
            "            )",
            "",
            "            return self.pipeline.run_generate_code(pipeline_input)",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def execute_code(",
            "        self, code: Optional[str] = None, output_type: Optional[str] = None",
            "    ):",
            "        \"\"\"",
            "        Execute code Generated with the assistant on Dataframe.",
            "        \"\"\"",
            "        if not self.pipeline:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error: No pipeline exists to execute try Agent class\"",
            "            )",
            "        try:",
            "            if code is None:",
            "                code = self.last_code_generated",
            "            self.logger.log(f\"Code: {code}\")",
            "            self.logger.log(",
            "                f\"Running PandasAI with {self.context.config.llm.type} LLM...\"",
            "            )",
            "",
            "            self.assign_prompt_id()",
            "",
            "            pipeline_input = CodeExecutionPipelineInput(",
            "                code, output_type, self.conversation_id, self.last_prompt_id",
            "            )",
            "",
            "            return self.pipeline.run_execute_code(pipeline_input)",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def train(",
            "        self,",
            "        queries: Optional[List[str]] = None,",
            "        codes: Optional[List[str]] = None,",
            "        docs: Optional[List[str]] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Trains the context to be passed to model",
            "        Args:",
            "            queries (Optional[str], optional): user user",
            "            codes (Optional[str], optional): generated code",
            "            docs (Optional[List[str]], optional): additional docs",
            "        Raises:",
            "            ImportError: if default vector db lib is not installed it raises an error",
            "        \"\"\"",
            "        if self._vectorstore is None:",
            "            raise MissingVectorStoreError(",
            "                \"No vector store provided. Please provide a vector store to train the agent.\"",
            "            )",
            "",
            "        if (queries and not codes) or (not queries and codes):",
            "            raise ValueError(",
            "                \"If either queries or codes are provided, both must be provided.\"",
            "            )",
            "",
            "        if docs is not None:",
            "            self._vectorstore.add_docs(docs)",
            "",
            "        if queries and codes:",
            "            self._vectorstore.add_question_answer(queries, codes)",
            "",
            "        self.logger.log(\"Agent successfully trained on the data\")",
            "",
            "    def clear_memory(self):",
            "        \"\"\"",
            "        Clears the memory",
            "        \"\"\"",
            "        self.context.memory.clear()",
            "        self.conversation_id = uuid.uuid4()",
            "",
            "    def add_message(self, message, is_user=False):",
            "        \"\"\"",
            "        Add message to the memory. This is useful when you want to add a message",
            "        to the memory without calling the chat function (for example, when you",
            "        need to add a message from the agent).",
            "        \"\"\"",
            "        self.context.memory.add(message, is_user=is_user)",
            "",
            "    def assign_prompt_id(self):",
            "        \"\"\"Assign a prompt ID\"\"\"",
            "",
            "        self.last_prompt_id = uuid.uuid4()",
            "",
            "        if self.logger:",
            "            self.logger.log(f\"Prompt ID: {self.last_prompt_id}\")",
            "",
            "    def clarification_questions(self, query: str) -> List[str]:",
            "        \"\"\"",
            "        Generate clarification questions based on the data",
            "        \"\"\"",
            "        prompt = ClarificationQuestionPrompt(",
            "            context=self.context,",
            "            query=query,",
            "        )",
            "",
            "        result = self.call_llm_with_prompt(prompt)",
            "        self.logger.log(",
            "            f\"\"\"Clarification Questions:  {result}",
            "            \"\"\"",
            "        )",
            "        result = result.replace(\"```json\", \"\").replace(\"```\", \"\")",
            "        questions: list[str] = json.loads(result)",
            "        return questions[:3]",
            "",
            "    def start_new_conversation(self):",
            "        \"\"\"",
            "        Clears the previous conversation",
            "        \"\"\"",
            "        self.clear_memory()",
            "",
            "    def explain(self) -> str:",
            "        \"\"\"",
            "        Returns the explanation of the code how it reached to the solution",
            "        \"\"\"",
            "        try:",
            "            prompt = ExplainPrompt(",
            "                context=self.context,",
            "                code=self.last_code_executed,",
            "            )",
            "            response = self.call_llm_with_prompt(prompt)",
            "            self.logger.log(",
            "                f\"\"\"Explanation:  {response}",
            "                \"\"\"",
            "            )",
            "            return response",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to explain, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def rephrase_query(self, query: str):",
            "        try:",
            "            prompt = RephraseQueryPrompt(",
            "                context=self.context,",
            "                query=query,",
            "            )",
            "            response = self.call_llm_with_prompt(prompt)",
            "            self.logger.log(",
            "                f\"\"\"Rephrased Response:  {response}",
            "                \"\"\"",
            "            )",
            "            return response",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to rephrase query, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    @property",
            "    def logs(self):",
            "        return self.logger.logs",
            "",
            "    @property",
            "    def last_error(self):",
            "        raise NotImplementedError",
            "",
            "    @property",
            "    def last_query_log_id(self):",
            "        raise NotImplementedError"
        ],
        "afterPatchFile": [
            "import json",
            "import os",
            "import re",
            "import uuid",
            "from typing import List, Optional, Union",
            "",
            "import pandasai.pandas as pd",
            "from pandasai.agent.base_security import BaseSecurity",
            "from pandasai.llm.bamboo_llm import BambooLLM",
            "from pandasai.pipelines.chat.chat_pipeline_input import ChatPipelineInput",
            "from pandasai.pipelines.chat.code_execution_pipeline_input import (",
            "    CodeExecutionPipelineInput,",
            ")",
            "from pandasai.vectorstores.vectorstore import VectorStore",
            "",
            "from ..config import load_config_from_json",
            "from ..connectors import BaseConnector, PandasConnector",
            "from ..constants import DEFAULT_CACHE_DIRECTORY, DEFAULT_CHART_DIRECTORY",
            "from ..exceptions import (",
            "    InvalidLLMOutputType,",
            "    MaliciousQueryError,",
            "    MissingVectorStoreError,",
            ")",
            "from ..helpers.df_info import df_type",
            "from ..helpers.folder import Folder",
            "from ..helpers.logger import Logger",
            "from ..helpers.memory import Memory",
            "from ..llm.base import LLM",
            "from ..llm.langchain import LangchainLLM, is_langchain_llm",
            "from ..pipelines.pipeline_context import PipelineContext",
            "from ..prompts.base import BasePrompt",
            "from ..prompts.clarification_questions_prompt import ClarificationQuestionPrompt",
            "from ..prompts.explain_prompt import ExplainPrompt",
            "from ..prompts.rephase_query_prompt import RephraseQueryPrompt",
            "from ..schemas.df_config import Config",
            "from ..skills import Skill",
            "from .callbacks import Callbacks",
            "",
            "",
            "class BaseAgent:",
            "    \"\"\"",
            "    Base Agent class to improve the conversational experience in PandasAI",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        dfs: Union[",
            "            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]",
            "        ],",
            "        config: Optional[Union[Config, dict]] = None,",
            "        memory_size: Optional[int] = 10,",
            "        vectorstore: Optional[VectorStore] = None,",
            "        description: str = None,",
            "        security: BaseSecurity = None,",
            "    ):",
            "        \"\"\"",
            "        Args:",
            "            df (Union[pd.DataFrame, List[pd.DataFrame]]): Pandas or Modin dataframe",
            "            Polars or Database connectors",
            "            memory_size (int, optional): Conversation history to use during chat.",
            "            Defaults to 1.",
            "        \"\"\"",
            "        self.last_prompt = None",
            "        self.last_prompt_id = None",
            "        self.last_result = None",
            "        self.last_code_generated = None",
            "        self.last_code_executed = None",
            "        self.agent_info = description",
            "",
            "        self.conversation_id = uuid.uuid4()",
            "",
            "        self.dfs = self.get_dfs(dfs)",
            "",
            "        # Instantiate the context",
            "        self.config = self.get_config(config)",
            "        self.context = PipelineContext(",
            "            dfs=self.dfs,",
            "            config=self.config,",
            "            memory=Memory(memory_size, agent_info=description),",
            "            vectorstore=vectorstore,",
            "        )",
            "",
            "        # Instantiate the logger",
            "        self.logger = Logger(",
            "            save_logs=self.config.save_logs, verbose=self.config.verbose",
            "        )",
            "",
            "        # Instantiate the vectorstore",
            "        self._vectorstore = vectorstore",
            "",
            "        if self._vectorstore is None and os.environ.get(\"PANDASAI_API_KEY\"):",
            "            try:",
            "                from pandasai.vectorstores.bamboo_vectorstore import BambooVectorStore",
            "            except ImportError as e:",
            "                raise ImportError(",
            "                    \"Could not import BambooVectorStore. Please install the required dependencies.\"",
            "                ) from e",
            "",
            "            self._vectorstore = BambooVectorStore(logger=self.logger)",
            "            self.context.vectorstore = self._vectorstore",
            "",
            "        self._callbacks = Callbacks(self)",
            "",
            "        self.configure()",
            "",
            "        self.pipeline = None",
            "        self.security = security",
            "",
            "    def configure(self):",
            "        # Add project root path if save_charts_path is default",
            "        if (",
            "            self.config.save_charts",
            "            and self.config.save_charts_path == DEFAULT_CHART_DIRECTORY",
            "        ):",
            "            Folder.create(self.config.save_charts_path)",
            "",
            "        # Add project root path if cache_path is default",
            "        if self.config.enable_cache:",
            "            Folder.create(DEFAULT_CACHE_DIRECTORY)",
            "",
            "    def get_config(self, config: Union[Config, dict]):",
            "        \"\"\"",
            "        Load a config to be used to run the queries.",
            "",
            "        Args:",
            "            config (Union[Config, dict]): Config to be used",
            "        \"\"\"",
            "",
            "        config = load_config_from_json(config)",
            "",
            "        if isinstance(config, dict) and config.get(\"llm\") is not None:",
            "            config[\"llm\"] = self.get_llm(config[\"llm\"])",
            "",
            "        config = Config(**config)",
            "",
            "        if config.llm is None:",
            "            config.llm = BambooLLM()",
            "",
            "        return config",
            "",
            "    def get_llm(self, llm: LLM) -> LLM:",
            "        \"\"\"",
            "        Load a LLM to be used to run the queries.",
            "        Check if it is a PandasAI LLM or a Langchain LLM.",
            "        If it is a Langchain LLM, wrap it in a PandasAI LLM.",
            "",
            "        Args:",
            "            llm (object): LLMs option to be used for API access",
            "",
            "        Raises:",
            "            BadImportError: If the LLM is a Langchain LLM but the langchain package",
            "            is not installed",
            "        \"\"\"",
            "        if is_langchain_llm(llm):",
            "            llm = LangchainLLM(llm)",
            "",
            "        return llm",
            "",
            "    def get_dfs(",
            "        self,",
            "        dfs: Union[",
            "            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]",
            "        ],",
            "    ):",
            "        \"\"\"",
            "        Load all the dataframes to be used in the agent.",
            "",
            "        Args:",
            "            dfs (List[Union[pd.DataFrame, Any]]): Pandas dataframe",
            "        \"\"\"",
            "        # Inline import to avoid circular import",
            "        from pandasai.smart_dataframe import SmartDataframe",
            "",
            "        # If only one dataframe is passed, convert it to a list",
            "        if not isinstance(dfs, list):",
            "            dfs = [dfs]",
            "",
            "        connectors = []",
            "        for df in dfs:",
            "            if isinstance(df, BaseConnector):",
            "                connectors.append(df)",
            "            elif isinstance(df, (pd.DataFrame, pd.Series, list, dict, str)):",
            "                connectors.append(PandasConnector({\"original_df\": df}))",
            "            elif df_type(df) == \"modin\":",
            "                connectors.append(PandasConnector({\"original_df\": df}))",
            "            elif isinstance(df, SmartDataframe) and isinstance(",
            "                df.dataframe, BaseConnector",
            "            ):",
            "                connectors.append(df.dataframe)",
            "            else:",
            "                try:",
            "                    import polars as pl",
            "",
            "                    if isinstance(df, pl.DataFrame):",
            "                        from ..connectors.polars import PolarsConnector",
            "",
            "                        connectors.append(PolarsConnector({\"original_df\": df}))",
            "",
            "                    else:",
            "                        raise ValueError(",
            "                            \"Invalid input data. We cannot convert it to a dataframe.\"",
            "                        )",
            "                except ImportError as e:",
            "                    raise ValueError(",
            "                        \"Invalid input data. We cannot convert it to a dataframe.\"",
            "                    ) from e",
            "        return connectors",
            "",
            "    def add_skills(self, *skills: Skill):",
            "        \"\"\"",
            "        Add Skills to PandasAI",
            "        \"\"\"",
            "        self.context.skills_manager.add_skills(*skills)",
            "",
            "    def call_llm_with_prompt(self, prompt: BasePrompt):",
            "        \"\"\"",
            "        Call LLM with prompt using error handling to retry based on config",
            "        Args:",
            "            prompt (BasePrompt): BasePrompt to pass to LLM's",
            "        \"\"\"",
            "        retry_count = 0",
            "        while retry_count < self.context.config.max_retries:",
            "            try:",
            "                result: str = self.context.config.llm.call(prompt)",
            "                if prompt.validate(result):",
            "                    return result",
            "                else:",
            "                    raise InvalidLLMOutputType(\"Response validation failed!\")",
            "            except Exception:",
            "                if (",
            "                    not self.context.config.use_error_correction_framework",
            "                    or retry_count >= self.context.config.max_retries - 1",
            "                ):",
            "                    raise",
            "                retry_count += 1",
            "",
            "    def check_malicious_keywords_in_query(self, query):",
            "        dangerous_pattern = re.compile(",
            "            r\"\\b(os|io|chr|b64decode)\\b|\"",
            "            r\"(\\.os|\\.io|'os'|'io'|\\\"os\\\"|\\\"io\\\"|chr\\(|chr\\)|chr |\\(chr)\"",
            "        )",
            "        return bool(dangerous_pattern.search(query))",
            "",
            "    def chat(self, query: str, output_type: Optional[str] = None):",
            "        \"\"\"",
            "        Simulate a chat interaction with the assistant on Dataframe.",
            "        \"\"\"",
            "        if not self.pipeline:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error: No pipeline exists\"",
            "            )",
            "",
            "        try:",
            "            self.logger.log(f\"Question: {query}\")",
            "            self.logger.log(",
            "                f\"Running PandasAI with {self.context.config.llm.type} LLM...\"",
            "            )",
            "",
            "            self.assign_prompt_id()",
            "",
            "            if self.config.security in [",
            "                \"standard\",",
            "                \"advanced\",",
            "            ] and self.check_malicious_keywords_in_query(query):",
            "                raise MaliciousQueryError(",
            "                    \"The query contains references to io or os modules or b64decode method which can be used to execute or access system resources in unsafe ways.\"",
            "                )",
            "",
            "            if self.security and self.security.evaluate(query):",
            "                raise MaliciousQueryError(\"Query can result in a malicious code\")",
            "",
            "            pipeline_input = ChatPipelineInput(",
            "                query, output_type, self.conversation_id, self.last_prompt_id",
            "            )",
            "",
            "            return self.pipeline.run(pipeline_input)",
            "",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def generate_code(self, query: str, output_type: Optional[str] = None):",
            "        \"\"\"",
            "        Simulate code generation with the assistant on Dataframe.",
            "        \"\"\"",
            "        if not self.pipeline:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error: No pipeline exists\"",
            "            )",
            "        try:",
            "            self.logger.log(f\"Question: {query}\")",
            "            self.logger.log(",
            "                f\"Running PandasAI with {self.context.config.llm.type} LLM...\"",
            "            )",
            "",
            "            self.assign_prompt_id()",
            "",
            "            pipeline_input = ChatPipelineInput(",
            "                query, output_type, self.conversation_id, self.last_prompt_id",
            "            )",
            "",
            "            return self.pipeline.run_generate_code(pipeline_input)",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def execute_code(",
            "        self, code: Optional[str] = None, output_type: Optional[str] = None",
            "    ):",
            "        \"\"\"",
            "        Execute code Generated with the assistant on Dataframe.",
            "        \"\"\"",
            "        if not self.pipeline:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error: No pipeline exists to execute try Agent class\"",
            "            )",
            "        try:",
            "            if code is None:",
            "                code = self.last_code_generated",
            "            self.logger.log(f\"Code: {code}\")",
            "            self.logger.log(",
            "                f\"Running PandasAI with {self.context.config.llm.type} LLM...\"",
            "            )",
            "",
            "            self.assign_prompt_id()",
            "",
            "            pipeline_input = CodeExecutionPipelineInput(",
            "                code, output_type, self.conversation_id, self.last_prompt_id",
            "            )",
            "",
            "            return self.pipeline.run_execute_code(pipeline_input)",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to get your answers, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def train(",
            "        self,",
            "        queries: Optional[List[str]] = None,",
            "        codes: Optional[List[str]] = None,",
            "        docs: Optional[List[str]] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Trains the context to be passed to model",
            "        Args:",
            "            queries (Optional[str], optional): user user",
            "            codes (Optional[str], optional): generated code",
            "            docs (Optional[List[str]], optional): additional docs",
            "        Raises:",
            "            ImportError: if default vector db lib is not installed it raises an error",
            "        \"\"\"",
            "        if self._vectorstore is None:",
            "            raise MissingVectorStoreError(",
            "                \"No vector store provided. Please provide a vector store to train the agent.\"",
            "            )",
            "",
            "        if (queries and not codes) or (not queries and codes):",
            "            raise ValueError(",
            "                \"If either queries or codes are provided, both must be provided.\"",
            "            )",
            "",
            "        if docs is not None:",
            "            self._vectorstore.add_docs(docs)",
            "",
            "        if queries and codes:",
            "            self._vectorstore.add_question_answer(queries, codes)",
            "",
            "        self.logger.log(\"Agent successfully trained on the data\")",
            "",
            "    def clear_memory(self):",
            "        \"\"\"",
            "        Clears the memory",
            "        \"\"\"",
            "        self.context.memory.clear()",
            "        self.conversation_id = uuid.uuid4()",
            "",
            "    def add_message(self, message, is_user=False):",
            "        \"\"\"",
            "        Add message to the memory. This is useful when you want to add a message",
            "        to the memory without calling the chat function (for example, when you",
            "        need to add a message from the agent).",
            "        \"\"\"",
            "        self.context.memory.add(message, is_user=is_user)",
            "",
            "    def assign_prompt_id(self):",
            "        \"\"\"Assign a prompt ID\"\"\"",
            "",
            "        self.last_prompt_id = uuid.uuid4()",
            "",
            "        if self.logger:",
            "            self.logger.log(f\"Prompt ID: {self.last_prompt_id}\")",
            "",
            "    def clarification_questions(self, query: str) -> List[str]:",
            "        \"\"\"",
            "        Generate clarification questions based on the data",
            "        \"\"\"",
            "        prompt = ClarificationQuestionPrompt(",
            "            context=self.context,",
            "            query=query,",
            "        )",
            "",
            "        result = self.call_llm_with_prompt(prompt)",
            "        self.logger.log(",
            "            f\"\"\"Clarification Questions:  {result}",
            "            \"\"\"",
            "        )",
            "        result = result.replace(\"```json\", \"\").replace(\"```\", \"\")",
            "        questions: list[str] = json.loads(result)",
            "        return questions[:3]",
            "",
            "    def start_new_conversation(self):",
            "        \"\"\"",
            "        Clears the previous conversation",
            "        \"\"\"",
            "        self.clear_memory()",
            "",
            "    def explain(self) -> str:",
            "        \"\"\"",
            "        Returns the explanation of the code how it reached to the solution",
            "        \"\"\"",
            "        try:",
            "            prompt = ExplainPrompt(",
            "                context=self.context,",
            "                code=self.last_code_executed,",
            "            )",
            "            response = self.call_llm_with_prompt(prompt)",
            "            self.logger.log(",
            "                f\"\"\"Explanation:  {response}",
            "                \"\"\"",
            "            )",
            "            return response",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to explain, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    def rephrase_query(self, query: str):",
            "        try:",
            "            prompt = RephraseQueryPrompt(",
            "                context=self.context,",
            "                query=query,",
            "            )",
            "            response = self.call_llm_with_prompt(prompt)",
            "            self.logger.log(",
            "                f\"\"\"Rephrased Response:  {response}",
            "                \"\"\"",
            "            )",
            "            return response",
            "        except Exception as exception:",
            "            return (",
            "                \"Unfortunately, I was not able to rephrase query, \"",
            "                \"because of the following error:\\n\"",
            "                f\"\\n{exception}\\n\"",
            "            )",
            "",
            "    @property",
            "    def logs(self):",
            "        return self.logger.logs",
            "",
            "    @property",
            "    def last_error(self):",
            "        raise NotImplementedError",
            "",
            "    @property",
            "    def last_query_log_id(self):",
            "        raise NotImplementedError"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "262": [
                "BaseAgent",
                "chat"
            ]
        },
        "addLocation": []
    },
    "pandasai/helpers/optional.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "     return version"
            },
            "1": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 52,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def get_environment(additional_deps: List[dict]) -> dict:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+def get_environment(additional_deps: List[dict], secure: bool = True) -> dict:"
            },
            "5": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "     \"\"\""
            },
            "6": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "     Returns the environment for the code to be executed."
            },
            "7": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 57,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "         },"
            },
            "9": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     }"
            },
            "10": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 75,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    env[\"pd\"] = RestrictedPandas()"
            },
            "12": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    env[\"plt\"] = RestrictedMatplotlib()"
            },
            "13": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    env[\"np\"] = RestrictedNumpy()"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    if secure:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+        env[\"pd\"] = RestrictedPandas()"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+        env[\"plt\"] = RestrictedMatplotlib()"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+        env[\"np\"] = RestrictedNumpy()"
            },
            "18": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for lib in additional_deps:"
            },
            "20": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if lib[\"name\"] == \"seaborn\":"
            },
            "21": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            from pandasai.safe_libs.restricted_seaborn import RestrictedSeaborn"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        for lib in additional_deps:"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+            if lib[\"name\"] == \"seaborn\":"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+                from pandasai.safe_libs.restricted_seaborn import RestrictedSeaborn"
            },
            "25": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            env[\"sns\"] = RestrictedSeaborn()"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+                env[\"sns\"] = RestrictedSeaborn()"
            },
            "28": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 86,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if lib[\"name\"] == \"datetime\":"
            },
            "30": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            env[\"datetime\"] = RestrictedDatetime()"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+            if lib[\"name\"] == \"datetime\":"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+                env[\"datetime\"] = RestrictedDatetime()"
            },
            "33": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 89,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if lib[\"name\"] == \"json\":"
            },
            "35": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            env[\"json\"] = RestrictedJson()"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+            if lib[\"name\"] == \"json\":"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+                env[\"json\"] = RestrictedJson()"
            },
            "38": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "39": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if lib[\"name\"] == \"base64\":"
            },
            "40": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            env[\"base64\"] = RestrictedBase64()"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+            if lib[\"name\"] == \"base64\":"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+                env[\"base64\"] = RestrictedBase64()"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+    else:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+        env[\"pd\"] = import_dependency(\"pandas\")"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+        env[\"plt\"] = import_dependency(\"matplotlib.pyplot\")"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+        env[\"np\"] = import_dependency(\"numpy\")"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+        for lib in additional_deps:"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+            if lib[\"name\"] == \"seaborn\":"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+                env[\"sns\"] = import_dependency(\"seaborn\")"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+            if lib[\"name\"] == \"datetime\":"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+                env[\"datetime\"] = import_dependency(\"datetime\")"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+            if lib[\"name\"] == \"json\":"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+                env[\"json\"] = import_dependency(\"json\")"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+            if lib[\"name\"] == \"base64\":"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+                env[\"base64\"] = import_dependency(\"base64\")"
            },
            "61": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 113,
                "PatchRowcode": " "
            },
            "62": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "     return env"
            },
            "63": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 115,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\"Module to import optional dependencies.",
            "",
            "Source: Taken from pandas/compat/_optional.py",
            "\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import importlib",
            "import sys",
            "import warnings",
            "from typing import TYPE_CHECKING, List",
            "",
            "from pandas.util.version import Version",
            "",
            "from pandasai.constants import WHITELISTED_BUILTINS",
            "from pandasai.safe_libs.restricted_base64 import RestrictedBase64",
            "from pandasai.safe_libs.restricted_datetime import RestrictedDatetime",
            "from pandasai.safe_libs.restricted_json import RestrictedJson",
            "from pandasai.safe_libs.restricted_matplotlib import RestrictedMatplotlib",
            "from pandasai.safe_libs.restricted_numpy import RestrictedNumpy",
            "from pandasai.safe_libs.restricted_pandas import RestrictedPandas",
            "",
            "if TYPE_CHECKING:",
            "    import types",
            "",
            "# Minimum version required for each optional dependency",
            "",
            "VERSIONS = {",
            "    \"sklearn\": \"1.2.2\",",
            "    \"statsmodels\": \"0.14.0\",",
            "    \"seaborn\": \"0.12.2\",",
            "    \"plotly\": \"5.14.1\",",
            "    \"ggplot\": \"0.11.5\",",
            "    \"scipy\": \"1.9.0\",",
            "    \"streamlit\": \"1.23.1\",",
            "}",
            "",
            "# A mapping from import name to package name (on PyPI) for packages where",
            "# these two names are different.",
            "",
            "INSTALL_MAPPING = {}",
            "",
            "",
            "def get_version(module: types.ModuleType) -> str:",
            "    \"\"\"Get the version of a module.\"\"\"",
            "    version = getattr(module, \"__version__\", None)",
            "",
            "    if version is None:",
            "        raise ImportError(f\"Can't determine version for {module.__name__}\")",
            "",
            "    return version",
            "",
            "",
            "def get_environment(additional_deps: List[dict]) -> dict:",
            "    \"\"\"",
            "    Returns the environment for the code to be executed.",
            "",
            "    Returns (dict): A dictionary of environment variables",
            "    \"\"\"",
            "    env = {",
            "        **{",
            "            lib[\"alias\"]: (",
            "                getattr(import_dependency(lib[\"module\"]), lib[\"name\"])",
            "                if hasattr(import_dependency(lib[\"module\"]), lib[\"name\"])",
            "                else import_dependency(lib[\"module\"])",
            "            )",
            "            for lib in additional_deps",
            "        },",
            "        \"__builtins__\": {",
            "            **{builtin: __builtins__[builtin] for builtin in WHITELISTED_BUILTINS},",
            "            \"__build_class__\": __build_class__,",
            "            \"__name__\": \"__main__\",",
            "        },",
            "    }",
            "",
            "    env[\"pd\"] = RestrictedPandas()",
            "    env[\"plt\"] = RestrictedMatplotlib()",
            "    env[\"np\"] = RestrictedNumpy()",
            "",
            "    for lib in additional_deps:",
            "        if lib[\"name\"] == \"seaborn\":",
            "            from pandasai.safe_libs.restricted_seaborn import RestrictedSeaborn",
            "",
            "            env[\"sns\"] = RestrictedSeaborn()",
            "",
            "        if lib[\"name\"] == \"datetime\":",
            "            env[\"datetime\"] = RestrictedDatetime()",
            "",
            "        if lib[\"name\"] == \"json\":",
            "            env[\"json\"] = RestrictedJson()",
            "",
            "        if lib[\"name\"] == \"base64\":",
            "            env[\"base64\"] = RestrictedBase64()",
            "",
            "    return env",
            "",
            "",
            "def import_dependency(",
            "    name: str,",
            "    extra: str = \"\",",
            "    errors: str = \"raise\",",
            "    min_version: str | None = None,",
            "):",
            "    \"\"\"",
            "    Import an optional dependency.",
            "",
            "    By default, if a dependency is missing an ImportError with a nice",
            "    message will be raised. If a dependency is present, but too old,",
            "    we raise.",
            "",
            "    Args:",
            "        name (str): The module name.",
            "        extra (str): An additional text to include in the ImportError message.",
            "        errors (str): Representing an action to do when a dependency",
            "            is not found or its version is too old.",
            "            Possible values: \"raise\", \"warn\", \"ignore\":",
            "                * raise : Raise an ImportError",
            "                * warn : Only applicable when a module's version is too old.",
            "                  Warns that the version is too old and returns None",
            "                * ignore: If the module is not installed, return None, otherwise,",
            "                  return the module, even if the version is too old.",
            "                  It's expected that users validate the version locally when",
            "                  using ``errors=\"ignore\"`` (see. ``io/html.py``)",
            "        min_version (str): Specify a minimum version that is different from",
            "            the global pandas minimum version required. Defaults to None.",
            "",
            "    Returns:",
            "         Optional[module]:",
            "            The imported module, when found and the version is correct.",
            "            None is returned when the package is not found and `errors`",
            "            is False, or when the package's version is too old and `errors`",
            "            is `'warn'`.",
            "    \"\"\"",
            "",
            "    assert errors in {\"warn\", \"raise\", \"ignore\"}",
            "",
            "    package_name = INSTALL_MAPPING.get(name)",
            "    install_name = package_name if package_name is not None else name",
            "",
            "    msg = (",
            "        f\"Missing optional dependency '{install_name}'. {extra} \"",
            "        f\"Use pip or conda to install {install_name}.\"",
            "    )",
            "    try:",
            "        module = importlib.import_module(name)",
            "    except ImportError as exc:",
            "        if errors == \"raise\":",
            "            raise ImportError(msg) from exc",
            "        return None",
            "",
            "    # Handle submodules: if we have submodule, grab parent module from sys.modules",
            "    parent = name.split(\".\")[0]",
            "    if parent != name:",
            "        install_name = parent",
            "        module_to_get = sys.modules[install_name]",
            "    else:",
            "        module_to_get = module",
            "    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)",
            "    if minimum_version:",
            "        version = get_version(module_to_get)",
            "        if version and Version(version) < Version(minimum_version):",
            "            msg = (",
            "                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"",
            "                f\"(version '{version}' currently installed).\"",
            "            )",
            "            if errors == \"warn\":",
            "                warnings.warn(",
            "                    msg,",
            "                    UserWarning,",
            "                )",
            "                return None",
            "            if errors == \"raise\":",
            "                raise ImportError(msg)",
            "",
            "    return module"
        ],
        "afterPatchFile": [
            "\"\"\"Module to import optional dependencies.",
            "",
            "Source: Taken from pandas/compat/_optional.py",
            "\"\"\"",
            "",
            "from __future__ import annotations",
            "",
            "import importlib",
            "import sys",
            "import warnings",
            "from typing import TYPE_CHECKING, List",
            "",
            "from pandas.util.version import Version",
            "",
            "from pandasai.constants import WHITELISTED_BUILTINS",
            "from pandasai.safe_libs.restricted_base64 import RestrictedBase64",
            "from pandasai.safe_libs.restricted_datetime import RestrictedDatetime",
            "from pandasai.safe_libs.restricted_json import RestrictedJson",
            "from pandasai.safe_libs.restricted_matplotlib import RestrictedMatplotlib",
            "from pandasai.safe_libs.restricted_numpy import RestrictedNumpy",
            "from pandasai.safe_libs.restricted_pandas import RestrictedPandas",
            "",
            "if TYPE_CHECKING:",
            "    import types",
            "",
            "# Minimum version required for each optional dependency",
            "",
            "VERSIONS = {",
            "    \"sklearn\": \"1.2.2\",",
            "    \"statsmodels\": \"0.14.0\",",
            "    \"seaborn\": \"0.12.2\",",
            "    \"plotly\": \"5.14.1\",",
            "    \"ggplot\": \"0.11.5\",",
            "    \"scipy\": \"1.9.0\",",
            "    \"streamlit\": \"1.23.1\",",
            "}",
            "",
            "# A mapping from import name to package name (on PyPI) for packages where",
            "# these two names are different.",
            "",
            "INSTALL_MAPPING = {}",
            "",
            "",
            "def get_version(module: types.ModuleType) -> str:",
            "    \"\"\"Get the version of a module.\"\"\"",
            "    version = getattr(module, \"__version__\", None)",
            "",
            "    if version is None:",
            "        raise ImportError(f\"Can't determine version for {module.__name__}\")",
            "",
            "    return version",
            "",
            "",
            "def get_environment(additional_deps: List[dict], secure: bool = True) -> dict:",
            "    \"\"\"",
            "    Returns the environment for the code to be executed.",
            "",
            "    Returns (dict): A dictionary of environment variables",
            "    \"\"\"",
            "    env = {",
            "        **{",
            "            lib[\"alias\"]: (",
            "                getattr(import_dependency(lib[\"module\"]), lib[\"name\"])",
            "                if hasattr(import_dependency(lib[\"module\"]), lib[\"name\"])",
            "                else import_dependency(lib[\"module\"])",
            "            )",
            "            for lib in additional_deps",
            "        },",
            "        \"__builtins__\": {",
            "            **{builtin: __builtins__[builtin] for builtin in WHITELISTED_BUILTINS},",
            "            \"__build_class__\": __build_class__,",
            "            \"__name__\": \"__main__\",",
            "        },",
            "    }",
            "",
            "    if secure:",
            "        env[\"pd\"] = RestrictedPandas()",
            "        env[\"plt\"] = RestrictedMatplotlib()",
            "        env[\"np\"] = RestrictedNumpy()",
            "",
            "        for lib in additional_deps:",
            "            if lib[\"name\"] == \"seaborn\":",
            "                from pandasai.safe_libs.restricted_seaborn import RestrictedSeaborn",
            "",
            "                env[\"sns\"] = RestrictedSeaborn()",
            "",
            "            if lib[\"name\"] == \"datetime\":",
            "                env[\"datetime\"] = RestrictedDatetime()",
            "",
            "            if lib[\"name\"] == \"json\":",
            "                env[\"json\"] = RestrictedJson()",
            "",
            "            if lib[\"name\"] == \"base64\":",
            "                env[\"base64\"] = RestrictedBase64()",
            "",
            "    else:",
            "        env[\"pd\"] = import_dependency(\"pandas\")",
            "        env[\"plt\"] = import_dependency(\"matplotlib.pyplot\")",
            "        env[\"np\"] = import_dependency(\"numpy\")",
            "",
            "        for lib in additional_deps:",
            "            if lib[\"name\"] == \"seaborn\":",
            "                env[\"sns\"] = import_dependency(\"seaborn\")",
            "",
            "            if lib[\"name\"] == \"datetime\":",
            "                env[\"datetime\"] = import_dependency(\"datetime\")",
            "",
            "            if lib[\"name\"] == \"json\":",
            "                env[\"json\"] = import_dependency(\"json\")",
            "",
            "            if lib[\"name\"] == \"base64\":",
            "                env[\"base64\"] = import_dependency(\"base64\")",
            "",
            "    return env",
            "",
            "",
            "def import_dependency(",
            "    name: str,",
            "    extra: str = \"\",",
            "    errors: str = \"raise\",",
            "    min_version: str | None = None,",
            "):",
            "    \"\"\"",
            "    Import an optional dependency.",
            "",
            "    By default, if a dependency is missing an ImportError with a nice",
            "    message will be raised. If a dependency is present, but too old,",
            "    we raise.",
            "",
            "    Args:",
            "        name (str): The module name.",
            "        extra (str): An additional text to include in the ImportError message.",
            "        errors (str): Representing an action to do when a dependency",
            "            is not found or its version is too old.",
            "            Possible values: \"raise\", \"warn\", \"ignore\":",
            "                * raise : Raise an ImportError",
            "                * warn : Only applicable when a module's version is too old.",
            "                  Warns that the version is too old and returns None",
            "                * ignore: If the module is not installed, return None, otherwise,",
            "                  return the module, even if the version is too old.",
            "                  It's expected that users validate the version locally when",
            "                  using ``errors=\"ignore\"`` (see. ``io/html.py``)",
            "        min_version (str): Specify a minimum version that is different from",
            "            the global pandas minimum version required. Defaults to None.",
            "",
            "    Returns:",
            "         Optional[module]:",
            "            The imported module, when found and the version is correct.",
            "            None is returned when the package is not found and `errors`",
            "            is False, or when the package's version is too old and `errors`",
            "            is `'warn'`.",
            "    \"\"\"",
            "",
            "    assert errors in {\"warn\", \"raise\", \"ignore\"}",
            "",
            "    package_name = INSTALL_MAPPING.get(name)",
            "    install_name = package_name if package_name is not None else name",
            "",
            "    msg = (",
            "        f\"Missing optional dependency '{install_name}'. {extra} \"",
            "        f\"Use pip or conda to install {install_name}.\"",
            "    )",
            "    try:",
            "        module = importlib.import_module(name)",
            "    except ImportError as exc:",
            "        if errors == \"raise\":",
            "            raise ImportError(msg) from exc",
            "        return None",
            "",
            "    # Handle submodules: if we have submodule, grab parent module from sys.modules",
            "    parent = name.split(\".\")[0]",
            "    if parent != name:",
            "        install_name = parent",
            "        module_to_get = sys.modules[install_name]",
            "    else:",
            "        module_to_get = module",
            "    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)",
            "    if minimum_version:",
            "        version = get_version(module_to_get)",
            "        if version and Version(version) < Version(minimum_version):",
            "            msg = (",
            "                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"",
            "                f\"(version '{version}' currently installed).\"",
            "            )",
            "            if errors == \"warn\":",
            "                warnings.warn(",
            "                    msg,",
            "                    UserWarning,",
            "                )",
            "                return None",
            "            if errors == \"raise\":",
            "                raise ImportError(msg)",
            "",
            "    return module"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "54": [
                "get_environment"
            ],
            "76": [
                "get_environment"
            ],
            "77": [
                "get_environment"
            ],
            "78": [
                "get_environment"
            ],
            "80": [
                "get_environment"
            ],
            "81": [
                "get_environment"
            ],
            "82": [
                "get_environment"
            ],
            "84": [
                "get_environment"
            ],
            "86": [
                "get_environment"
            ],
            "87": [
                "get_environment"
            ],
            "89": [
                "get_environment"
            ],
            "90": [
                "get_environment"
            ],
            "92": [
                "get_environment"
            ],
            "93": [
                "get_environment"
            ]
        },
        "addLocation": []
    },
    "pandasai/pipelines/chat/code_cleaning.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "         return re.sub(r\"\"\"(['\"])([^'\"]*\\.png)\\1\"\"\", r\"\\1temp_chart.png\\1\", code)"
            },
            "1": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 122,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 123,
                "PatchRowcode": "     def get_code_to_run(self, code: str, context: CodeExecutionContext) -> Any:"
            },
            "3": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if self._is_malicious_code(code):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        if self._config.security in ["
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+            \"standard\","
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+            \"advanced\","
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+        ] and self._is_malicious_code(code):"
            },
            "8": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 128,
                "PatchRowcode": "             raise MaliciousQueryError("
            },
            "9": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 129,
                "PatchRowcode": "                 \"Code shouldn't use 'os', 'io' or 'chr', 'b64decode' functions as this could lead to malicious code execution.\""
            },
            "10": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "             )"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "         code = self._replace_plot_png(code)"
            },
            "13": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "         self._current_code_executed = code"
            },
            "14": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 134,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 475,
                "afterPatchRowNumber": 479,
                "PatchRowcode": "             if target_names and self._check_is_df_declaration(node):"
            },
            "16": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 480,
                "PatchRowcode": "                 # Construct dataframe from node"
            },
            "17": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": 481,
                "PatchRowcode": "                 code = \"\\n\".join(code_lines)"
            },
            "18": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                env = get_environment(self._additional_dependencies)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 482,
                "PatchRowcode": "+                env = get_environment("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 483,
                "PatchRowcode": "+                    self._additional_dependencies,"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 484,
                "PatchRowcode": "+                    secure=self._config.security in [\"standard\", \"advanced\"],"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 485,
                "PatchRowcode": "+                )"
            },
            "23": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": 486,
                "PatchRowcode": "                 env[\"dfs\"] = copy.deepcopy(self._get_originals(self._dfs))"
            },
            "24": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": 487,
                "PatchRowcode": "                 if context.skills_manager.used_skills:"
            },
            "25": {
                "beforePatchRowNumber": 481,
                "afterPatchRowNumber": 488,
                "PatchRowcode": "                     for skill_func_name in context.skills_manager.used_skills:"
            }
        },
        "frontPatchFile": [
            "import ast",
            "import copy",
            "import re",
            "import traceback",
            "import uuid",
            "from typing import Any, List, Union",
            "",
            "import astor",
            "",
            "from pandasai.connectors.pandas import PandasConnector",
            "from pandasai.helpers.optional import get_environment",
            "from pandasai.helpers.path import find_project_root",
            "from pandasai.helpers.skills_manager import SkillsManager",
            "from pandasai.helpers.sql import extract_table_names",
            "",
            "from ...connectors import BaseConnector",
            "from ...connectors.sql import SQLConnector",
            "from ...constants import RESTRICTED_LIBS, WHITELISTED_LIBRARIES",
            "from ...exceptions import (",
            "    BadImportError,",
            "    ExecuteSQLQueryNotUsed,",
            "    InvalidConfigError,",
            "    MaliciousQueryError,",
            ")",
            "from ...helpers.logger import Logger",
            "from ...helpers.save_chart import add_save_chart",
            "from ...schemas.df_config import Config",
            "from ..base_logic_unit import BaseLogicUnit",
            "from ..logic_unit_output import LogicUnitOutput",
            "from ..pipeline_context import PipelineContext",
            "",
            "",
            "class CodeExecutionContext:",
            "    def __init__(",
            "        self,",
            "        prompt_id: uuid.UUID,",
            "        skills_manager: SkillsManager,",
            "    ):",
            "        \"\"\"",
            "        Code Execution Context",
            "        Args:",
            "            prompt_id (uuid.UUID): Prompt ID",
            "            skills_manager (SkillsManager): Skills Manager",
            "        \"\"\"",
            "        self.skills_manager = skills_manager",
            "        self.prompt_id = prompt_id",
            "",
            "",
            "class FunctionCallVisitor(ast.NodeVisitor):",
            "    \"\"\"",
            "    Iterate over the code to find function calls",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        self.function_calls = []",
            "",
            "    def visit_Call(self, node):",
            "        if isinstance(node.func, ast.Name):",
            "            self.function_calls.append(node.func.id)",
            "        elif isinstance(node.func, ast.Attribute) and isinstance(",
            "            node.func.value, ast.Name",
            "        ):",
            "            self.function_calls.append(f\"{node.func.value.id}.{node.func.attr}\")",
            "        self.generic_visit(node)",
            "",
            "",
            "class CodeCleaning(BaseLogicUnit):",
            "    \"\"\"",
            "    Code Cleaning Stage",
            "    \"\"\"",
            "",
            "    _dfs: List",
            "    _config: Union[Config, dict]",
            "    _logger: Logger = None",
            "    _additional_dependencies: List[dict] = []",
            "    _current_code_executed: str = None",
            "",
            "    def __init__(self, on_failure=None, on_retry=None, **kwargs):",
            "        super().__init__(**kwargs)",
            "        self._function_call_visitor = FunctionCallVisitor()",
            "        self.on_failure = on_failure",
            "        self.on_retry = on_retry",
            "",
            "    def execute(self, input: Any, **kwargs) -> LogicUnitOutput:",
            "        context: PipelineContext = kwargs.get(\"context\")",
            "        self._dfs = context.dfs",
            "        self._config = context.config",
            "        self._logger = kwargs.get(\"logger\")",
            "",
            "        code_context = CodeExecutionContext(",
            "            context.get(\"last_prompt_id\"), context.skills_manager",
            "        )",
            "        code_to_run = input",
            "        try:",
            "            code_to_run = self.get_code_to_run(input, code_context)",
            "        except Exception as e:",
            "            traceback_errors = traceback.format_exc()",
            "            if self.on_failure:",
            "                self.on_failure(code_to_run, traceback_errors)",
            "            if self.on_retry:",
            "                return self.on_retry(code_to_run, e)",
            "            raise",
            "",
            "        context.add(\"additional_dependencies\", self._additional_dependencies)",
            "        context.add(\"current_code_executed\", self._current_code_executed)",
            "",
            "        return LogicUnitOutput(",
            "            code_to_run,",
            "            True,",
            "            \"Code Cleaned Successfully\",",
            "        )",
            "",
            "    def _replace_plot_png(self, code):",
            "        \"\"\"",
            "        Replace plot.png with temp_chart.png",
            "        Args:",
            "            code (str): Python code to execute",
            "        Returns:",
            "            str: Python code with plot.png replaced with temp_chart.png",
            "        \"\"\"",
            "        return re.sub(r\"\"\"(['\"])([^'\"]*\\.png)\\1\"\"\", r\"\\1temp_chart.png\\1\", code)",
            "",
            "    def get_code_to_run(self, code: str, context: CodeExecutionContext) -> Any:",
            "        if self._is_malicious_code(code):",
            "            raise MaliciousQueryError(",
            "                \"Code shouldn't use 'os', 'io' or 'chr', 'b64decode' functions as this could lead to malicious code execution.\"",
            "            )",
            "        code = self._replace_plot_png(code)",
            "        self._current_code_executed = code",
            "",
            "        # Add save chart code",
            "        if self._config.save_charts:",
            "            code = add_save_chart(",
            "                code,",
            "                logger=self._logger,",
            "                file_name=str(context.prompt_id),",
            "                save_charts_path_str=self._config.save_charts_path,",
            "            )",
            "        else:",
            "            # Temporarily save generated chart to display",
            "            code = add_save_chart(",
            "                code,",
            "                logger=self._logger,",
            "                file_name=\"temp_chart\",",
            "                save_charts_path_str=f\"{find_project_root()}/exports/charts\",",
            "            )",
            "",
            "        # If plt.show is in the code, remove that line",
            "        code = re.sub(r\"plt.show\\(\\)\", \"\", code)",
            "",
            "        # Reset used skills",
            "        context.skills_manager.used_skills = []",
            "",
            "        # Get the code to run removing unsafe imports and df overwrites",
            "        code_to_run = self._clean_code(code, context)",
            "",
            "        self._logger.log(",
            "            f\"\"\"",
            "Code running:",
            "```",
            "{code_to_run}",
            "        ```\"\"\"",
            "        )",
            "",
            "        return code_to_run",
            "",
            "    def _is_malicious_code(self, code) -> bool:",
            "        tree = ast.parse(code)",
            "",
            "        # Check for private attributes and access of restricted libs",
            "        def check_restricted_access(node):",
            "            \"\"\"Check if the node accesses restricted modules or private attributes.\"\"\"",
            "            if isinstance(node, ast.Attribute):",
            "                attr_chain = []",
            "                while isinstance(node, ast.Attribute):",
            "                    if node.attr.startswith(\"_\"):",
            "                        raise MaliciousQueryError(",
            "                            f\"Access to private attribute '{node.attr}' is not allowed.\"",
            "                        )",
            "                    attr_chain.insert(0, node.attr)",
            "                    node = node.value",
            "                if isinstance(node, ast.Name):",
            "                    attr_chain.insert(0, node.id)",
            "                    if any(module in RESTRICTED_LIBS for module in attr_chain):",
            "                        raise MaliciousQueryError(",
            "                            f\"Restricted access detected in attribute chain: {'.'.join(attr_chain)}\"",
            "                        )",
            "",
            "            elif isinstance(node, ast.Subscript) and isinstance(",
            "                node.value, ast.Attribute",
            "            ):",
            "                check_restricted_access(node.value)",
            "",
            "        for node in ast.walk(tree):",
            "            # Check 'import ...' statements",
            "            if isinstance(node, ast.Import):",
            "                for alias in node.names:",
            "                    sub_module_names = alias.name.split(\".\")",
            "                    if any(module in RESTRICTED_LIBS for module in sub_module_names):",
            "                        raise MaliciousQueryError(",
            "                            f\"Restricted library import detected: {alias.name}\"",
            "                        )",
            "",
            "            # Check 'from ... import ...' statements",
            "            elif isinstance(node, ast.ImportFrom):",
            "                sub_module_names = node.module.split(\".\")",
            "                if any(module in RESTRICTED_LIBS for module in sub_module_names):",
            "                    raise MaliciousQueryError(",
            "                        f\"Restricted library import detected: {node.module}\"",
            "                    )",
            "                if any(alias.name in RESTRICTED_LIBS for alias in node.names):",
            "                    raise MaliciousQueryError(",
            "                        \"Restricted library import detected in 'from ... import ...'\"",
            "                    )",
            "",
            "            # Check attribute access for restricted libraries",
            "            elif isinstance(node, (ast.Attribute, ast.Subscript)):",
            "                check_restricted_access(node)",
            "",
            "        dangerous_modules = [",
            "            \" os\",",
            "            \" io\",",
            "            \".os\",",
            "            \".io\",",
            "            \"'os'\",",
            "            \"'io'\",",
            "            '\"os\"',",
            "            '\"io\"',",
            "            \"chr(\",",
            "            \"chr)\",",
            "            \"chr \",",
            "            \"(chr\",",
            "            \"b64decode\",",
            "        ]",
            "",
            "        return any(",
            "            re.search(r\"\\b\" + re.escape(module) + r\"\\b\", code)",
            "            for module in dangerous_modules",
            "        )",
            "",
            "    def _is_jailbreak(self, node: ast.stmt) -> bool:",
            "        \"\"\"",
            "        Remove jailbreaks from the code to prevent malicious code execution.",
            "        Args:",
            "            node (ast.stmt): A code node to be checked.",
            "        Returns (bool):",
            "        \"\"\"",
            "",
            "        DANGEROUS_BUILTINS = [\"__subclasses__\", \"__builtins__\", \"__import__\"]",
            "",
            "        node_str = ast.dump(node)",
            "",
            "        return any(builtin in node_str for builtin in DANGEROUS_BUILTINS)",
            "",
            "    def _is_unsafe(self, node: ast.stmt) -> bool:",
            "        \"\"\"",
            "        Remove unsafe code from the code to prevent malicious code execution.",
            "",
            "        Args:",
            "            node (ast.stmt): A code node to be checked.",
            "",
            "        Returns (bool):",
            "        \"\"\"",
            "",
            "        code = astor.to_source(node)",
            "        return any(",
            "            (",
            "                method in code",
            "                for method in [",
            "                    \".to_csv\",",
            "                    \".to_excel\",",
            "                    \".to_json\",",
            "                    \".to_sql\",",
            "                    \".to_feather\",",
            "                    \".to_hdf\",",
            "                    \".to_parquet\",",
            "                    \".to_pickle\",",
            "                    \".to_gbq\",",
            "                    \".to_stata\",",
            "                    \".to_records\",",
            "                    \".to_latex\",",
            "                    \".to_html\",",
            "                    \".to_markdown\",",
            "                    \".to_clipboard\",",
            "                ]",
            "            )",
            "        )",
            "",
            "    def find_function_calls(self, node: ast.AST, context: CodeExecutionContext):",
            "        if isinstance(node, ast.Call):",
            "            if isinstance(node.func, ast.Name):",
            "                if context.skills_manager.skill_exists(node.func.id):",
            "                    context.skills_manager.add_used_skill(node.func.id)",
            "            elif isinstance(node.func, ast.Attribute) and isinstance(",
            "                node.func.value, ast.Name",
            "            ):",
            "                context.skills_manager.add_used_skill(",
            "                    f\"{node.func.value.id}.{node.func.attr}\"",
            "                )",
            "",
            "        for child_node in ast.iter_child_nodes(node):",
            "            self.find_function_calls(child_node, context)",
            "",
            "    def check_direct_sql_func_def_exists(self, node: ast.AST):",
            "        return (",
            "            self._validate_direct_sql(self._dfs)",
            "            and isinstance(node, ast.FunctionDef)",
            "            and node.name == \"execute_sql_query\"",
            "        )",
            "",
            "    def check_skill_func_def_exists(self, node: ast.AST, context: CodeExecutionContext):",
            "        return isinstance(",
            "            node, ast.FunctionDef",
            "        ) and context.skills_manager.skill_exists(node.name)",
            "",
            "    def _validate_direct_sql(self, dfs: List[BaseConnector]) -> bool:",
            "        \"\"\"",
            "        Raises error if they don't belong sqlconnector or have different credentials",
            "        Args:",
            "            dfs (List[BaseConnector]): list of BaseConnectors",
            "",
            "        Raises:",
            "            InvalidConfigError: Raise Error in case of config is set but criteria is not met",
            "        \"\"\"",
            "",
            "        if self._config.direct_sql:",
            "            if all(",
            "                (isinstance(df, SQLConnector) and df.equals(dfs[0])) for df in dfs",
            "            ) or all(",
            "                (isinstance(df, PandasConnector) and df.sql_enabled) for df in dfs",
            "            ):",
            "                return True",
            "            else:",
            "                raise InvalidConfigError(",
            "                    \"Direct requires all SQLConnector and they belong to same datasource \"",
            "                    \"and have same credentials\"",
            "                )",
            "        return False",
            "",
            "    def _replace_table_names(",
            "        self, sql_query: str, table_names: list, allowed_table_names: list",
            "    ):",
            "        regex_patterns = {",
            "            table_name: re.compile(r\"\\b\" + re.escape(table_name) + r\"\\b\")",
            "            for table_name in table_names",
            "        }",
            "        for table_name in table_names:",
            "            if table_name in allowed_table_names.keys():",
            "                quoted_table_name = allowed_table_names[table_name]",
            "                sql_query = regex_patterns[table_name].sub(quoted_table_name, sql_query)",
            "            else:",
            "                raise MaliciousQueryError(",
            "                    f\"Query uses unauthorized table: {table_name}.\"",
            "                )",
            "",
            "        return sql_query",
            "",
            "    def _clean_sql_query(self, sql_query: str) -> str:",
            "        \"\"\"",
            "        Clean sql query trim colon and make case-sensitive",
            "        Args:",
            "            sql_query (str): sql query",
            "",
            "        Returns:",
            "            str: updated sql query",
            "        \"\"\"",
            "        sql_query = sql_query.rstrip(\";\")",
            "        table_names = extract_table_names(sql_query)",
            "        allowed_table_names = {df.name: df.cs_table_name for df in self._dfs} | {",
            "            f'\"{df.name}\"': df.cs_table_name for df in self._dfs",
            "        }",
            "        return self._replace_table_names(sql_query, table_names, allowed_table_names)",
            "",
            "    def _validate_and_make_table_name_case_sensitive(self, node: ast.Assign):",
            "        \"\"\"",
            "        Validates whether table exists in specified dataset and convert name to case-sensitive",
            "        Args:",
            "            node (ast.Assign): code tree node",
            "",
            "        Returns:",
            "            node: return updated or same node",
            "        \"\"\"",
            "        if isinstance(node, ast.Assign):",
            "            # Check if the assigned value is a string constant and the target is 'sql_query'",
            "            if (",
            "                isinstance(node.value, ast.Constant)",
            "                and isinstance(node.value.value, str)",
            "                and isinstance(node.targets[0], ast.Name)",
            "                and node.targets[0].id in [\"sql_query\", \"query\"]",
            "            ):",
            "                sql_query = node.value.value",
            "                sql_query = self._clean_sql_query(sql_query)",
            "                node.value.value = sql_query",
            "            elif (",
            "                isinstance(node.value, ast.Call)",
            "                and isinstance(node.value.func, ast.Name)",
            "                and node.value.func.id == \"execute_sql_query\"",
            "                and len(node.value.args) == 1",
            "                and isinstance(node.value.args[0], ast.Constant)",
            "                and isinstance(node.value.args[0].value, str)",
            "            ):",
            "                sql_query = node.value.args[0].value",
            "                sql_query = self._clean_sql_query(sql_query)",
            "                node.value.args[0].value = sql_query",
            "",
            "        elif isinstance(node, ast.Expr) and isinstance(node.value, ast.Call):",
            "            # Check if the function call is to 'execute_sql_query' and has a string constant argument",
            "            if (",
            "                isinstance(node.value.func, ast.Name)",
            "                and node.value.func.id == \"execute_sql_query\"",
            "                and len(node.value.args) == 1",
            "                and isinstance(node.value.args[0], ast.Constant)",
            "                and isinstance(node.value.args[0].value, str)",
            "            ):",
            "                sql_query = node.value.args[0].value",
            "                sql_query = self._clean_sql_query(sql_query)",
            "                node.value.args[0].value = sql_query",
            "",
            "        return node",
            "",
            "    def _get_target_names(self, targets):",
            "        target_names = []",
            "        is_slice = False",
            "",
            "        for target in targets:",
            "            if isinstance(target, ast.Name) or (",
            "                isinstance(target, ast.Subscript) and isinstance(target.value, ast.Name)",
            "            ):",
            "                target_names.append(",
            "                    target.id if isinstance(target, ast.Name) else target.value.id",
            "                )",
            "                is_slice = isinstance(target, ast.Subscript)",
            "",
            "        return target_names, is_slice, target",
            "",
            "    def _check_is_df_declaration(self, node: ast.AST):",
            "        value = node.value",
            "        return (",
            "            isinstance(value, ast.Call)",
            "            and isinstance(value.func, ast.Attribute)",
            "            and isinstance(value.func.value, ast.Name)",
            "            and hasattr(value.func.value, \"id\")",
            "            and value.func.value.id == \"pd\"",
            "            and value.func.attr == \"DataFrame\"",
            "        )",
            "",
            "    def _get_originals(self, dfs):",
            "        \"\"\"",
            "        Get original dfs",
            "",
            "        Args:",
            "            dfs (list): List of dfs",
            "",
            "        Returns:",
            "            list: List of dfs",
            "        \"\"\"",
            "        original_dfs = []",
            "        for df in dfs:",
            "            if df is None:",
            "                original_dfs.append(None)",
            "                continue",
            "",
            "            df.execute()",
            "",
            "            original_dfs.append(df.pandas_df)",
            "",
            "        return original_dfs",
            "",
            "    def _extract_fix_dataframe_redeclarations(",
            "        self, node: ast.AST, code_lines: list[str], context: CodeExecutionContext",
            "    ) -> ast.AST:",
            "        if isinstance(node, ast.Assign):",
            "            target_names, is_slice, target = self._get_target_names(node.targets)",
            "",
            "            if target_names and self._check_is_df_declaration(node):",
            "                # Construct dataframe from node",
            "                code = \"\\n\".join(code_lines)",
            "                env = get_environment(self._additional_dependencies)",
            "                env[\"dfs\"] = copy.deepcopy(self._get_originals(self._dfs))",
            "                if context.skills_manager.used_skills:",
            "                    for skill_func_name in context.skills_manager.used_skills:",
            "                        skill = context.skills_manager.get_skill_by_func_name(",
            "                            skill_func_name",
            "                        )",
            "                        env[skill_func_name] = skill",
            "                exec(code, env)",
            "",
            "                df_generated = (",
            "                    env[target_names[0]][target.slice.value]",
            "                    if is_slice",
            "                    else env[target_names[0]]",
            "                )",
            "",
            "                # check if exists in provided dfs",
            "                for index, df in enumerate(self._dfs):",
            "                    head = df.get_head()",
            "                    if head.shape == df_generated.shape and head.columns.equals(",
            "                        df_generated.columns",
            "                    ):",
            "                        target_var = (",
            "                            ast.Subscript(",
            "                                value=ast.Name(id=target_names[0], ctx=ast.Load()),",
            "                                slice=target.slice,",
            "                                ctx=ast.Store(),",
            "                            )",
            "                            if is_slice",
            "                            else ast.Name(id=target_names[0], ctx=ast.Store())",
            "                        )",
            "                        return ast.Assign(",
            "                            targets=[target_var],",
            "                            value=ast.Subscript(",
            "                                value=ast.Name(id=\"dfs\", ctx=ast.Load()),",
            "                                slice=ast.Index(value=ast.Num(n=index)),",
            "                                ctx=ast.Load(),",
            "                            ),",
            "                        )",
            "        return None",
            "",
            "    def _clean_code(self, code: str, context: CodeExecutionContext) -> str:",
            "        \"\"\"",
            "        A method to clean the code to prevent malicious code execution.",
            "",
            "        Args:",
            "            code(str): A python code.",
            "",
            "        Returns:",
            "            str: A clean code string.",
            "",
            "        \"\"\"",
            "",
            "        # Clear recent optional dependencies",
            "        self._additional_dependencies = []",
            "",
            "        clean_code_lines = []",
            "",
            "        tree = ast.parse(code)",
            "",
            "        # Check for imports and the node where analyze_data is defined",
            "        new_body = []",
            "        execute_sql_query_used = False",
            "",
            "        # find function calls",
            "        self._function_call_visitor.visit(tree)",
            "",
            "        for node in tree.body:",
            "            if isinstance(node, (ast.Import, ast.ImportFrom)):",
            "                self._check_imports(node)",
            "                continue",
            "",
            "            if (",
            "                self._is_df_overwrite(node)",
            "                or self._is_jailbreak(node)",
            "                or self._is_unsafe(node)",
            "            ):",
            "                continue",
            "",
            "            # if generated code contain execute_sql_query def remove it",
            "            # function already defined",
            "            if self.check_direct_sql_func_def_exists(node):",
            "                continue",
            "",
            "            if self.check_skill_func_def_exists(node, context):",
            "                continue",
            "",
            "            # if generated code contain execute_sql_query usage",
            "            if (",
            "                self._validate_direct_sql(self._dfs)",
            "                and \"execute_sql_query\" in self._function_call_visitor.function_calls",
            "            ):",
            "                execute_sql_query_used = True",
            "",
            "            # Sanity for sql query the code should only use allowed tables",
            "            if self._config.direct_sql:",
            "                node = self._validate_and_make_table_name_case_sensitive(node)",
            "",
            "            self.find_function_calls(node, context)",
            "",
            "            clean_code_lines.append(astor.to_source(node))",
            "",
            "            new_body.append(",
            "                self._extract_fix_dataframe_redeclarations(",
            "                    node, clean_code_lines, context",
            "                )",
            "                or node",
            "            )",
            "",
            "        # Enforcing use of execute_sql_query via Error Prompt Pipeline",
            "        if self._config.direct_sql and not execute_sql_query_used:",
            "            raise ExecuteSQLQueryNotUsed(",
            "                \"For Direct SQL set to true, execute_sql_query function must be used. Generating Error Prompt!!!\"",
            "            )",
            "",
            "        new_tree = ast.Module(body=new_body)",
            "        return astor.to_source(new_tree, pretty_source=lambda x: \"\".join(x)).strip()",
            "",
            "    def _is_df_overwrite(self, node: ast.stmt) -> bool:",
            "        \"\"\"",
            "        Remove df declarations from the code to prevent malicious code execution.",
            "",
            "        Args:",
            "            node (ast.stmt): A code node to be checked.",
            "",
            "        Returns (bool):",
            "",
            "        \"\"\"",
            "",
            "        return (",
            "            isinstance(node, ast.Assign)",
            "            and isinstance(node.targets[0], ast.Name)",
            "            and node.targets[0].id == \"dfs\"",
            "        )",
            "",
            "    def _check_imports(self, node: Union[ast.Import, ast.ImportFrom]):",
            "        \"\"\"",
            "        Add whitelisted imports to _additional_dependencies.",
            "",
            "        Args:",
            "            node (object): ast.Import or ast.ImportFrom",
            "",
            "        Raises:",
            "            BadImportError: If the import is not whitelisted",
            "",
            "        \"\"\"",
            "        module = node.names[0].name if isinstance(node, ast.Import) else node.module",
            "        library = module.split(\".\")[0]",
            "",
            "        if library == \"pandas\":",
            "            return",
            "",
            "        whitelisted_libs = (",
            "            WHITELISTED_LIBRARIES + self._config.custom_whitelisted_dependencies",
            "        )",
            "",
            "        if library not in whitelisted_libs:",
            "            raise BadImportError(",
            "                f\"The library '{library}' is not in the list of whitelisted libraries. \"",
            "                \"To learn how to whitelist custom dependencies, visit: \"",
            "                \"https://docs.pandas-ai.com/custom-whitelisted-dependencies#custom-whitelisted-dependencies\"",
            "            )",
            "",
            "        for alias in node.names:",
            "            self._additional_dependencies.append(",
            "                {",
            "                    \"module\": module,",
            "                    \"name\": alias.name,",
            "                    \"alias\": alias.asname or alias.name,",
            "                }",
            "            )"
        ],
        "afterPatchFile": [
            "import ast",
            "import copy",
            "import re",
            "import traceback",
            "import uuid",
            "from typing import Any, List, Union",
            "",
            "import astor",
            "",
            "from pandasai.connectors.pandas import PandasConnector",
            "from pandasai.helpers.optional import get_environment",
            "from pandasai.helpers.path import find_project_root",
            "from pandasai.helpers.skills_manager import SkillsManager",
            "from pandasai.helpers.sql import extract_table_names",
            "",
            "from ...connectors import BaseConnector",
            "from ...connectors.sql import SQLConnector",
            "from ...constants import RESTRICTED_LIBS, WHITELISTED_LIBRARIES",
            "from ...exceptions import (",
            "    BadImportError,",
            "    ExecuteSQLQueryNotUsed,",
            "    InvalidConfigError,",
            "    MaliciousQueryError,",
            ")",
            "from ...helpers.logger import Logger",
            "from ...helpers.save_chart import add_save_chart",
            "from ...schemas.df_config import Config",
            "from ..base_logic_unit import BaseLogicUnit",
            "from ..logic_unit_output import LogicUnitOutput",
            "from ..pipeline_context import PipelineContext",
            "",
            "",
            "class CodeExecutionContext:",
            "    def __init__(",
            "        self,",
            "        prompt_id: uuid.UUID,",
            "        skills_manager: SkillsManager,",
            "    ):",
            "        \"\"\"",
            "        Code Execution Context",
            "        Args:",
            "            prompt_id (uuid.UUID): Prompt ID",
            "            skills_manager (SkillsManager): Skills Manager",
            "        \"\"\"",
            "        self.skills_manager = skills_manager",
            "        self.prompt_id = prompt_id",
            "",
            "",
            "class FunctionCallVisitor(ast.NodeVisitor):",
            "    \"\"\"",
            "    Iterate over the code to find function calls",
            "    \"\"\"",
            "",
            "    def __init__(self):",
            "        self.function_calls = []",
            "",
            "    def visit_Call(self, node):",
            "        if isinstance(node.func, ast.Name):",
            "            self.function_calls.append(node.func.id)",
            "        elif isinstance(node.func, ast.Attribute) and isinstance(",
            "            node.func.value, ast.Name",
            "        ):",
            "            self.function_calls.append(f\"{node.func.value.id}.{node.func.attr}\")",
            "        self.generic_visit(node)",
            "",
            "",
            "class CodeCleaning(BaseLogicUnit):",
            "    \"\"\"",
            "    Code Cleaning Stage",
            "    \"\"\"",
            "",
            "    _dfs: List",
            "    _config: Union[Config, dict]",
            "    _logger: Logger = None",
            "    _additional_dependencies: List[dict] = []",
            "    _current_code_executed: str = None",
            "",
            "    def __init__(self, on_failure=None, on_retry=None, **kwargs):",
            "        super().__init__(**kwargs)",
            "        self._function_call_visitor = FunctionCallVisitor()",
            "        self.on_failure = on_failure",
            "        self.on_retry = on_retry",
            "",
            "    def execute(self, input: Any, **kwargs) -> LogicUnitOutput:",
            "        context: PipelineContext = kwargs.get(\"context\")",
            "        self._dfs = context.dfs",
            "        self._config = context.config",
            "        self._logger = kwargs.get(\"logger\")",
            "",
            "        code_context = CodeExecutionContext(",
            "            context.get(\"last_prompt_id\"), context.skills_manager",
            "        )",
            "        code_to_run = input",
            "        try:",
            "            code_to_run = self.get_code_to_run(input, code_context)",
            "        except Exception as e:",
            "            traceback_errors = traceback.format_exc()",
            "            if self.on_failure:",
            "                self.on_failure(code_to_run, traceback_errors)",
            "            if self.on_retry:",
            "                return self.on_retry(code_to_run, e)",
            "            raise",
            "",
            "        context.add(\"additional_dependencies\", self._additional_dependencies)",
            "        context.add(\"current_code_executed\", self._current_code_executed)",
            "",
            "        return LogicUnitOutput(",
            "            code_to_run,",
            "            True,",
            "            \"Code Cleaned Successfully\",",
            "        )",
            "",
            "    def _replace_plot_png(self, code):",
            "        \"\"\"",
            "        Replace plot.png with temp_chart.png",
            "        Args:",
            "            code (str): Python code to execute",
            "        Returns:",
            "            str: Python code with plot.png replaced with temp_chart.png",
            "        \"\"\"",
            "        return re.sub(r\"\"\"(['\"])([^'\"]*\\.png)\\1\"\"\", r\"\\1temp_chart.png\\1\", code)",
            "",
            "    def get_code_to_run(self, code: str, context: CodeExecutionContext) -> Any:",
            "        if self._config.security in [",
            "            \"standard\",",
            "            \"advanced\",",
            "        ] and self._is_malicious_code(code):",
            "            raise MaliciousQueryError(",
            "                \"Code shouldn't use 'os', 'io' or 'chr', 'b64decode' functions as this could lead to malicious code execution.\"",
            "            )",
            "",
            "        code = self._replace_plot_png(code)",
            "        self._current_code_executed = code",
            "",
            "        # Add save chart code",
            "        if self._config.save_charts:",
            "            code = add_save_chart(",
            "                code,",
            "                logger=self._logger,",
            "                file_name=str(context.prompt_id),",
            "                save_charts_path_str=self._config.save_charts_path,",
            "            )",
            "        else:",
            "            # Temporarily save generated chart to display",
            "            code = add_save_chart(",
            "                code,",
            "                logger=self._logger,",
            "                file_name=\"temp_chart\",",
            "                save_charts_path_str=f\"{find_project_root()}/exports/charts\",",
            "            )",
            "",
            "        # If plt.show is in the code, remove that line",
            "        code = re.sub(r\"plt.show\\(\\)\", \"\", code)",
            "",
            "        # Reset used skills",
            "        context.skills_manager.used_skills = []",
            "",
            "        # Get the code to run removing unsafe imports and df overwrites",
            "        code_to_run = self._clean_code(code, context)",
            "",
            "        self._logger.log(",
            "            f\"\"\"",
            "Code running:",
            "```",
            "{code_to_run}",
            "        ```\"\"\"",
            "        )",
            "",
            "        return code_to_run",
            "",
            "    def _is_malicious_code(self, code) -> bool:",
            "        tree = ast.parse(code)",
            "",
            "        # Check for private attributes and access of restricted libs",
            "        def check_restricted_access(node):",
            "            \"\"\"Check if the node accesses restricted modules or private attributes.\"\"\"",
            "            if isinstance(node, ast.Attribute):",
            "                attr_chain = []",
            "                while isinstance(node, ast.Attribute):",
            "                    if node.attr.startswith(\"_\"):",
            "                        raise MaliciousQueryError(",
            "                            f\"Access to private attribute '{node.attr}' is not allowed.\"",
            "                        )",
            "                    attr_chain.insert(0, node.attr)",
            "                    node = node.value",
            "                if isinstance(node, ast.Name):",
            "                    attr_chain.insert(0, node.id)",
            "                    if any(module in RESTRICTED_LIBS for module in attr_chain):",
            "                        raise MaliciousQueryError(",
            "                            f\"Restricted access detected in attribute chain: {'.'.join(attr_chain)}\"",
            "                        )",
            "",
            "            elif isinstance(node, ast.Subscript) and isinstance(",
            "                node.value, ast.Attribute",
            "            ):",
            "                check_restricted_access(node.value)",
            "",
            "        for node in ast.walk(tree):",
            "            # Check 'import ...' statements",
            "            if isinstance(node, ast.Import):",
            "                for alias in node.names:",
            "                    sub_module_names = alias.name.split(\".\")",
            "                    if any(module in RESTRICTED_LIBS for module in sub_module_names):",
            "                        raise MaliciousQueryError(",
            "                            f\"Restricted library import detected: {alias.name}\"",
            "                        )",
            "",
            "            # Check 'from ... import ...' statements",
            "            elif isinstance(node, ast.ImportFrom):",
            "                sub_module_names = node.module.split(\".\")",
            "                if any(module in RESTRICTED_LIBS for module in sub_module_names):",
            "                    raise MaliciousQueryError(",
            "                        f\"Restricted library import detected: {node.module}\"",
            "                    )",
            "                if any(alias.name in RESTRICTED_LIBS for alias in node.names):",
            "                    raise MaliciousQueryError(",
            "                        \"Restricted library import detected in 'from ... import ...'\"",
            "                    )",
            "",
            "            # Check attribute access for restricted libraries",
            "            elif isinstance(node, (ast.Attribute, ast.Subscript)):",
            "                check_restricted_access(node)",
            "",
            "        dangerous_modules = [",
            "            \" os\",",
            "            \" io\",",
            "            \".os\",",
            "            \".io\",",
            "            \"'os'\",",
            "            \"'io'\",",
            "            '\"os\"',",
            "            '\"io\"',",
            "            \"chr(\",",
            "            \"chr)\",",
            "            \"chr \",",
            "            \"(chr\",",
            "            \"b64decode\",",
            "        ]",
            "",
            "        return any(",
            "            re.search(r\"\\b\" + re.escape(module) + r\"\\b\", code)",
            "            for module in dangerous_modules",
            "        )",
            "",
            "    def _is_jailbreak(self, node: ast.stmt) -> bool:",
            "        \"\"\"",
            "        Remove jailbreaks from the code to prevent malicious code execution.",
            "        Args:",
            "            node (ast.stmt): A code node to be checked.",
            "        Returns (bool):",
            "        \"\"\"",
            "",
            "        DANGEROUS_BUILTINS = [\"__subclasses__\", \"__builtins__\", \"__import__\"]",
            "",
            "        node_str = ast.dump(node)",
            "",
            "        return any(builtin in node_str for builtin in DANGEROUS_BUILTINS)",
            "",
            "    def _is_unsafe(self, node: ast.stmt) -> bool:",
            "        \"\"\"",
            "        Remove unsafe code from the code to prevent malicious code execution.",
            "",
            "        Args:",
            "            node (ast.stmt): A code node to be checked.",
            "",
            "        Returns (bool):",
            "        \"\"\"",
            "",
            "        code = astor.to_source(node)",
            "        return any(",
            "            (",
            "                method in code",
            "                for method in [",
            "                    \".to_csv\",",
            "                    \".to_excel\",",
            "                    \".to_json\",",
            "                    \".to_sql\",",
            "                    \".to_feather\",",
            "                    \".to_hdf\",",
            "                    \".to_parquet\",",
            "                    \".to_pickle\",",
            "                    \".to_gbq\",",
            "                    \".to_stata\",",
            "                    \".to_records\",",
            "                    \".to_latex\",",
            "                    \".to_html\",",
            "                    \".to_markdown\",",
            "                    \".to_clipboard\",",
            "                ]",
            "            )",
            "        )",
            "",
            "    def find_function_calls(self, node: ast.AST, context: CodeExecutionContext):",
            "        if isinstance(node, ast.Call):",
            "            if isinstance(node.func, ast.Name):",
            "                if context.skills_manager.skill_exists(node.func.id):",
            "                    context.skills_manager.add_used_skill(node.func.id)",
            "            elif isinstance(node.func, ast.Attribute) and isinstance(",
            "                node.func.value, ast.Name",
            "            ):",
            "                context.skills_manager.add_used_skill(",
            "                    f\"{node.func.value.id}.{node.func.attr}\"",
            "                )",
            "",
            "        for child_node in ast.iter_child_nodes(node):",
            "            self.find_function_calls(child_node, context)",
            "",
            "    def check_direct_sql_func_def_exists(self, node: ast.AST):",
            "        return (",
            "            self._validate_direct_sql(self._dfs)",
            "            and isinstance(node, ast.FunctionDef)",
            "            and node.name == \"execute_sql_query\"",
            "        )",
            "",
            "    def check_skill_func_def_exists(self, node: ast.AST, context: CodeExecutionContext):",
            "        return isinstance(",
            "            node, ast.FunctionDef",
            "        ) and context.skills_manager.skill_exists(node.name)",
            "",
            "    def _validate_direct_sql(self, dfs: List[BaseConnector]) -> bool:",
            "        \"\"\"",
            "        Raises error if they don't belong sqlconnector or have different credentials",
            "        Args:",
            "            dfs (List[BaseConnector]): list of BaseConnectors",
            "",
            "        Raises:",
            "            InvalidConfigError: Raise Error in case of config is set but criteria is not met",
            "        \"\"\"",
            "",
            "        if self._config.direct_sql:",
            "            if all(",
            "                (isinstance(df, SQLConnector) and df.equals(dfs[0])) for df in dfs",
            "            ) or all(",
            "                (isinstance(df, PandasConnector) and df.sql_enabled) for df in dfs",
            "            ):",
            "                return True",
            "            else:",
            "                raise InvalidConfigError(",
            "                    \"Direct requires all SQLConnector and they belong to same datasource \"",
            "                    \"and have same credentials\"",
            "                )",
            "        return False",
            "",
            "    def _replace_table_names(",
            "        self, sql_query: str, table_names: list, allowed_table_names: list",
            "    ):",
            "        regex_patterns = {",
            "            table_name: re.compile(r\"\\b\" + re.escape(table_name) + r\"\\b\")",
            "            for table_name in table_names",
            "        }",
            "        for table_name in table_names:",
            "            if table_name in allowed_table_names.keys():",
            "                quoted_table_name = allowed_table_names[table_name]",
            "                sql_query = regex_patterns[table_name].sub(quoted_table_name, sql_query)",
            "            else:",
            "                raise MaliciousQueryError(",
            "                    f\"Query uses unauthorized table: {table_name}.\"",
            "                )",
            "",
            "        return sql_query",
            "",
            "    def _clean_sql_query(self, sql_query: str) -> str:",
            "        \"\"\"",
            "        Clean sql query trim colon and make case-sensitive",
            "        Args:",
            "            sql_query (str): sql query",
            "",
            "        Returns:",
            "            str: updated sql query",
            "        \"\"\"",
            "        sql_query = sql_query.rstrip(\";\")",
            "        table_names = extract_table_names(sql_query)",
            "        allowed_table_names = {df.name: df.cs_table_name for df in self._dfs} | {",
            "            f'\"{df.name}\"': df.cs_table_name for df in self._dfs",
            "        }",
            "        return self._replace_table_names(sql_query, table_names, allowed_table_names)",
            "",
            "    def _validate_and_make_table_name_case_sensitive(self, node: ast.Assign):",
            "        \"\"\"",
            "        Validates whether table exists in specified dataset and convert name to case-sensitive",
            "        Args:",
            "            node (ast.Assign): code tree node",
            "",
            "        Returns:",
            "            node: return updated or same node",
            "        \"\"\"",
            "        if isinstance(node, ast.Assign):",
            "            # Check if the assigned value is a string constant and the target is 'sql_query'",
            "            if (",
            "                isinstance(node.value, ast.Constant)",
            "                and isinstance(node.value.value, str)",
            "                and isinstance(node.targets[0], ast.Name)",
            "                and node.targets[0].id in [\"sql_query\", \"query\"]",
            "            ):",
            "                sql_query = node.value.value",
            "                sql_query = self._clean_sql_query(sql_query)",
            "                node.value.value = sql_query",
            "            elif (",
            "                isinstance(node.value, ast.Call)",
            "                and isinstance(node.value.func, ast.Name)",
            "                and node.value.func.id == \"execute_sql_query\"",
            "                and len(node.value.args) == 1",
            "                and isinstance(node.value.args[0], ast.Constant)",
            "                and isinstance(node.value.args[0].value, str)",
            "            ):",
            "                sql_query = node.value.args[0].value",
            "                sql_query = self._clean_sql_query(sql_query)",
            "                node.value.args[0].value = sql_query",
            "",
            "        elif isinstance(node, ast.Expr) and isinstance(node.value, ast.Call):",
            "            # Check if the function call is to 'execute_sql_query' and has a string constant argument",
            "            if (",
            "                isinstance(node.value.func, ast.Name)",
            "                and node.value.func.id == \"execute_sql_query\"",
            "                and len(node.value.args) == 1",
            "                and isinstance(node.value.args[0], ast.Constant)",
            "                and isinstance(node.value.args[0].value, str)",
            "            ):",
            "                sql_query = node.value.args[0].value",
            "                sql_query = self._clean_sql_query(sql_query)",
            "                node.value.args[0].value = sql_query",
            "",
            "        return node",
            "",
            "    def _get_target_names(self, targets):",
            "        target_names = []",
            "        is_slice = False",
            "",
            "        for target in targets:",
            "            if isinstance(target, ast.Name) or (",
            "                isinstance(target, ast.Subscript) and isinstance(target.value, ast.Name)",
            "            ):",
            "                target_names.append(",
            "                    target.id if isinstance(target, ast.Name) else target.value.id",
            "                )",
            "                is_slice = isinstance(target, ast.Subscript)",
            "",
            "        return target_names, is_slice, target",
            "",
            "    def _check_is_df_declaration(self, node: ast.AST):",
            "        value = node.value",
            "        return (",
            "            isinstance(value, ast.Call)",
            "            and isinstance(value.func, ast.Attribute)",
            "            and isinstance(value.func.value, ast.Name)",
            "            and hasattr(value.func.value, \"id\")",
            "            and value.func.value.id == \"pd\"",
            "            and value.func.attr == \"DataFrame\"",
            "        )",
            "",
            "    def _get_originals(self, dfs):",
            "        \"\"\"",
            "        Get original dfs",
            "",
            "        Args:",
            "            dfs (list): List of dfs",
            "",
            "        Returns:",
            "            list: List of dfs",
            "        \"\"\"",
            "        original_dfs = []",
            "        for df in dfs:",
            "            if df is None:",
            "                original_dfs.append(None)",
            "                continue",
            "",
            "            df.execute()",
            "",
            "            original_dfs.append(df.pandas_df)",
            "",
            "        return original_dfs",
            "",
            "    def _extract_fix_dataframe_redeclarations(",
            "        self, node: ast.AST, code_lines: list[str], context: CodeExecutionContext",
            "    ) -> ast.AST:",
            "        if isinstance(node, ast.Assign):",
            "            target_names, is_slice, target = self._get_target_names(node.targets)",
            "",
            "            if target_names and self._check_is_df_declaration(node):",
            "                # Construct dataframe from node",
            "                code = \"\\n\".join(code_lines)",
            "                env = get_environment(",
            "                    self._additional_dependencies,",
            "                    secure=self._config.security in [\"standard\", \"advanced\"],",
            "                )",
            "                env[\"dfs\"] = copy.deepcopy(self._get_originals(self._dfs))",
            "                if context.skills_manager.used_skills:",
            "                    for skill_func_name in context.skills_manager.used_skills:",
            "                        skill = context.skills_manager.get_skill_by_func_name(",
            "                            skill_func_name",
            "                        )",
            "                        env[skill_func_name] = skill",
            "                exec(code, env)",
            "",
            "                df_generated = (",
            "                    env[target_names[0]][target.slice.value]",
            "                    if is_slice",
            "                    else env[target_names[0]]",
            "                )",
            "",
            "                # check if exists in provided dfs",
            "                for index, df in enumerate(self._dfs):",
            "                    head = df.get_head()",
            "                    if head.shape == df_generated.shape and head.columns.equals(",
            "                        df_generated.columns",
            "                    ):",
            "                        target_var = (",
            "                            ast.Subscript(",
            "                                value=ast.Name(id=target_names[0], ctx=ast.Load()),",
            "                                slice=target.slice,",
            "                                ctx=ast.Store(),",
            "                            )",
            "                            if is_slice",
            "                            else ast.Name(id=target_names[0], ctx=ast.Store())",
            "                        )",
            "                        return ast.Assign(",
            "                            targets=[target_var],",
            "                            value=ast.Subscript(",
            "                                value=ast.Name(id=\"dfs\", ctx=ast.Load()),",
            "                                slice=ast.Index(value=ast.Num(n=index)),",
            "                                ctx=ast.Load(),",
            "                            ),",
            "                        )",
            "        return None",
            "",
            "    def _clean_code(self, code: str, context: CodeExecutionContext) -> str:",
            "        \"\"\"",
            "        A method to clean the code to prevent malicious code execution.",
            "",
            "        Args:",
            "            code(str): A python code.",
            "",
            "        Returns:",
            "            str: A clean code string.",
            "",
            "        \"\"\"",
            "",
            "        # Clear recent optional dependencies",
            "        self._additional_dependencies = []",
            "",
            "        clean_code_lines = []",
            "",
            "        tree = ast.parse(code)",
            "",
            "        # Check for imports and the node where analyze_data is defined",
            "        new_body = []",
            "        execute_sql_query_used = False",
            "",
            "        # find function calls",
            "        self._function_call_visitor.visit(tree)",
            "",
            "        for node in tree.body:",
            "            if isinstance(node, (ast.Import, ast.ImportFrom)):",
            "                self._check_imports(node)",
            "                continue",
            "",
            "            if (",
            "                self._is_df_overwrite(node)",
            "                or self._is_jailbreak(node)",
            "                or self._is_unsafe(node)",
            "            ):",
            "                continue",
            "",
            "            # if generated code contain execute_sql_query def remove it",
            "            # function already defined",
            "            if self.check_direct_sql_func_def_exists(node):",
            "                continue",
            "",
            "            if self.check_skill_func_def_exists(node, context):",
            "                continue",
            "",
            "            # if generated code contain execute_sql_query usage",
            "            if (",
            "                self._validate_direct_sql(self._dfs)",
            "                and \"execute_sql_query\" in self._function_call_visitor.function_calls",
            "            ):",
            "                execute_sql_query_used = True",
            "",
            "            # Sanity for sql query the code should only use allowed tables",
            "            if self._config.direct_sql:",
            "                node = self._validate_and_make_table_name_case_sensitive(node)",
            "",
            "            self.find_function_calls(node, context)",
            "",
            "            clean_code_lines.append(astor.to_source(node))",
            "",
            "            new_body.append(",
            "                self._extract_fix_dataframe_redeclarations(",
            "                    node, clean_code_lines, context",
            "                )",
            "                or node",
            "            )",
            "",
            "        # Enforcing use of execute_sql_query via Error Prompt Pipeline",
            "        if self._config.direct_sql and not execute_sql_query_used:",
            "            raise ExecuteSQLQueryNotUsed(",
            "                \"For Direct SQL set to true, execute_sql_query function must be used. Generating Error Prompt!!!\"",
            "            )",
            "",
            "        new_tree = ast.Module(body=new_body)",
            "        return astor.to_source(new_tree, pretty_source=lambda x: \"\".join(x)).strip()",
            "",
            "    def _is_df_overwrite(self, node: ast.stmt) -> bool:",
            "        \"\"\"",
            "        Remove df declarations from the code to prevent malicious code execution.",
            "",
            "        Args:",
            "            node (ast.stmt): A code node to be checked.",
            "",
            "        Returns (bool):",
            "",
            "        \"\"\"",
            "",
            "        return (",
            "            isinstance(node, ast.Assign)",
            "            and isinstance(node.targets[0], ast.Name)",
            "            and node.targets[0].id == \"dfs\"",
            "        )",
            "",
            "    def _check_imports(self, node: Union[ast.Import, ast.ImportFrom]):",
            "        \"\"\"",
            "        Add whitelisted imports to _additional_dependencies.",
            "",
            "        Args:",
            "            node (object): ast.Import or ast.ImportFrom",
            "",
            "        Raises:",
            "            BadImportError: If the import is not whitelisted",
            "",
            "        \"\"\"",
            "        module = node.names[0].name if isinstance(node, ast.Import) else node.module",
            "        library = module.split(\".\")[0]",
            "",
            "        if library == \"pandas\":",
            "            return",
            "",
            "        whitelisted_libs = (",
            "            WHITELISTED_LIBRARIES + self._config.custom_whitelisted_dependencies",
            "        )",
            "",
            "        if library not in whitelisted_libs:",
            "            raise BadImportError(",
            "                f\"The library '{library}' is not in the list of whitelisted libraries. \"",
            "                \"To learn how to whitelist custom dependencies, visit: \"",
            "                \"https://docs.pandas-ai.com/custom-whitelisted-dependencies#custom-whitelisted-dependencies\"",
            "            )",
            "",
            "        for alias in node.names:",
            "            self._additional_dependencies.append(",
            "                {",
            "                    \"module\": module,",
            "                    \"name\": alias.name,",
            "                    \"alias\": alias.asname or alias.name,",
            "                }",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "124": [
                "CodeCleaning",
                "get_code_to_run"
            ],
            "478": [
                "CodeCleaning",
                "_extract_fix_dataframe_redeclarations"
            ]
        },
        "addLocation": []
    },
    "pandasai/pipelines/chat/code_execution.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "         # List the required dfs, so we can avoid to run the connectors"
            },
            "1": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "         # if the code does not need them"
            },
            "2": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "         dfs = self._required_dfs(code)"
            },
            "3": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        environment: dict = get_environment(self._additional_dependencies)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 156,
                "PatchRowcode": "+        environment: dict = get_environment("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 157,
                "PatchRowcode": "+            self._additional_dependencies,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 158,
                "PatchRowcode": "+            secure=self._config.security in [\"standard\", \"advanced\"],"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 159,
                "PatchRowcode": "+        )"
            },
            "8": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 160,
                "PatchRowcode": "         environment[\"dfs\"] = self._get_originals(dfs)"
            },
            "9": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 161,
                "PatchRowcode": "         if len(environment[\"dfs\"]) == 1:"
            },
            "10": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 162,
                "PatchRowcode": "             environment[\"df\"] = environment[\"dfs\"][0]"
            }
        },
        "frontPatchFile": [
            "import ast",
            "import logging",
            "import traceback",
            "from collections import defaultdict",
            "from typing import Any, Callable, Generator, List, Union",
            "",
            "from pandasai.exceptions import InvalidLLMOutputType, InvalidOutputValueMismatch",
            "from pandasai.pipelines.logic_unit_output import LogicUnitOutput",
            "from pandasai.responses.response_serializer import ResponseSerializer",
            "",
            "from ...exceptions import NoResultFoundError",
            "from ...helpers.logger import Logger",
            "from ...helpers.node_visitors import AssignmentVisitor, CallVisitor",
            "from ...helpers.optional import get_environment",
            "from ...helpers.output_validator import OutputValidator",
            "from ...schemas.df_config import Config",
            "from ..base_logic_unit import BaseLogicUnit",
            "from ..pipeline_context import PipelineContext",
            "from .code_cleaning import CodeExecutionContext",
            "",
            "",
            "class CodeExecution(BaseLogicUnit):",
            "    \"\"\"",
            "    Code Execution Stage",
            "    \"\"\"",
            "",
            "    _dfs: List",
            "    _config: Union[Config, dict]",
            "    _additional_dependencies: List[dict] = []",
            "    _current_code_executed: str = None",
            "    _retry_if_fail: bool = False",
            "    _ast_comparator_map: dict = {",
            "        ast.Eq: \"=\",",
            "        ast.NotEq: \"!=\",",
            "        ast.Lt: \"<\",",
            "        ast.LtE: \"<=\",",
            "        ast.Gt: \">\",",
            "        ast.GtE: \">=\",",
            "        ast.Is: \"is\",",
            "        ast.IsNot: \"is not\",",
            "        ast.In: \"in\",",
            "        ast.NotIn: \"not in\",",
            "    }",
            "",
            "    def __init__(",
            "        self,",
            "        on_failure: Callable[[str, Exception], None] = None,",
            "        on_retry: Callable[[str, Exception], None] = None,",
            "        **kwargs,",
            "    ):",
            "        super().__init__(**kwargs)",
            "        self.on_failure = on_failure",
            "        self.on_retry = on_retry",
            "",
            "    def execute(self, input: Any, **kwargs) -> Any:",
            "        \"\"\"",
            "        This method will return output according to",
            "        Implementation.",
            "",
            "        :param input: Your input data.",
            "        :param kwargs: A dictionary of keyword arguments.",
            "            - 'logger' (any): The logger for logging.",
            "            - 'config' (Config): Global configurations for the test",
            "            - 'context' (any): The execution context.",
            "",
            "        :return: The result of the execution.",
            "        \"\"\"",
            "        self.context: PipelineContext = kwargs.get(\"context\")",
            "        self._dfs = self.context.dfs",
            "        self._config = self.context.config",
            "        self._additional_dependencies = self.context.get(\"additional_dependencies\", [])",
            "        self._current_code_executed = self.context.get(\"current_code_executed\")",
            "        self.logger: Logger = kwargs.get(\"logger\")",
            "",
            "        # Execute the code",
            "        code_context = CodeExecutionContext(",
            "            self.context.get(\"last_prompt_id\"), self.context.skills_manager",
            "        )",
            "",
            "        retry_count = 0",
            "        code_to_run = input",
            "        result = None",
            "        while retry_count <= self.context.config.max_retries:",
            "            try:",
            "                result = self.execute_code(code_to_run, code_context)",
            "                if self.context.get(\"output_type\") != \"\" and (",
            "                    output_helper := self.context.get(\"output_type\")",
            "                ):",
            "                    (validation_ok, validation_errors) = OutputValidator.validate(",
            "                        output_helper, result",
            "                    )",
            "",
            "                    if not validation_ok:",
            "                        raise InvalidLLMOutputType(validation_errors)",
            "",
            "                if not OutputValidator.validate_result(result):",
            "                    raise InvalidOutputValueMismatch(",
            "                        f'Value type {type(result[\"value\"])} must match with type {result[\"type\"]}'",
            "                    )",
            "",
            "                break",
            "",
            "            except Exception as e:",
            "                traceback_errors = traceback.format_exc()",
            "                self.logger.log(f\"Failed with error: {traceback_errors}\", logging.ERROR)",
            "                if self.on_failure:",
            "                    self.on_failure(code_to_run, traceback_errors)",
            "",
            "                if (",
            "                    not self.context.config.use_error_correction_framework",
            "                    or retry_count >= self.context.config.max_retries",
            "                ):",
            "                    raise e",
            "",
            "                retry_count += 1",
            "",
            "                self.logger.log(",
            "                    f\"Failed to execute code retrying with a correction framework \"",
            "                    f\"[retry number: {retry_count}]\",",
            "                    level=logging.WARNING,",
            "                )",
            "",
            "                # TODO - Move this implement to main execute function",
            "                # Temporarily done for test cases this is to be fixed move to the main function",
            "                code_to_run = self._retry_run_code(",
            "                    code_to_run, self.context, self.logger, e",
            "                )",
            "",
            "        return LogicUnitOutput(",
            "            result,",
            "            True,",
            "            \"Code Executed Successfully\",",
            "            {\"content_type\": \"response\", \"value\": ResponseSerializer.serialize(result)},",
            "            final_track_output=True,",
            "        )",
            "",
            "    def execute_code(self, code: str, context: CodeExecutionContext) -> Any:",
            "        \"\"\"",
            "        Execute the python code generated by LLMs to answer the question",
            "        about the input dataframe. Run the code in the current context and return the",
            "        result.",
            "",
            "        Args:",
            "            code (str): Python code to execute.",
            "            context (CodeExecutionContext): Code Execution Context",
            "                    with prompt id and skills.",
            "",
            "        Returns:",
            "            Any: The result of the code execution. The type of the result depends",
            "                on the generated code.",
            "",
            "        \"\"\"",
            "        # List the required dfs, so we can avoid to run the connectors",
            "        # if the code does not need them",
            "        dfs = self._required_dfs(code)",
            "        environment: dict = get_environment(self._additional_dependencies)",
            "        environment[\"dfs\"] = self._get_originals(dfs)",
            "        if len(environment[\"dfs\"]) == 1:",
            "            environment[\"df\"] = environment[\"dfs\"][0]",
            "",
            "        if self._config.direct_sql:",
            "            environment[\"execute_sql_query\"] = self._dfs[0].execute_direct_sql_query",
            "",
            "        # Add skills to the env",
            "        if context.skills_manager.used_skills:",
            "            for skill_func_name in context.skills_manager.used_skills:",
            "                skill = context.skills_manager.get_skill_by_func_name(skill_func_name)",
            "                environment[skill_func_name] = skill",
            "",
            "        # Execute the code",
            "        exec(code, environment)",
            "",
            "        # Get the result",
            "        if \"result\" not in environment:",
            "            raise NoResultFoundError(\"No result returned\")",
            "",
            "        return environment[\"result\"]",
            "",
            "    def _required_dfs(self, code: str) -> List[str]:",
            "        \"\"\"",
            "        List the index of the DataFrames that are needed to execute the code. The goal",
            "        is to avoid to run the connectors if the code does not need them.",
            "",
            "        Args:",
            "            code (str): Python code to execute",
            "",
            "        Returns:",
            "            List[int]: A list of the index of the DataFrames that are needed to execute",
            "            the code.",
            "        \"\"\"",
            "",
            "        # Sometimes GPT-3.5/4 use a for loop to iterate over the dfs (even if there is only one)",
            "        # or they concatenate the dfs. In this case we need all the dfs",
            "        if \"for df in dfs\" in code or \"pd.concat(dfs\" in code:",
            "            return self._dfs",
            "",
            "        required_dfs = []",
            "        for i, df in enumerate(self._dfs):",
            "            if f\"dfs[{i}]\" in code:",
            "                required_dfs.append(df)",
            "            else:",
            "                required_dfs.append(None)",
            "        return required_dfs or self._dfs",
            "",
            "    def _get_originals(self, dfs):",
            "        \"\"\"",
            "        Get original dfs",
            "",
            "        Args:",
            "            dfs (list): List of dfs",
            "",
            "        Returns:",
            "            list: List of dfs",
            "        \"\"\"",
            "        original_dfs = []",
            "        for index, df in enumerate(dfs):",
            "            if df is None:",
            "                original_dfs.append(None)",
            "                continue",
            "",
            "            extracted_filters = self._extract_filters(self._current_code_executed)",
            "            filters = extracted_filters.get(f\"dfs[{index}]\", [])",
            "            df.set_additional_filters(filters)",
            "",
            "            df.execute()",
            "            # df.load_connector(partial=len(filters) > 0)",
            "",
            "            original_dfs.append(df.pandas_df)",
            "",
            "        return original_dfs",
            "",
            "    def _extract_filters(self, code) -> dict[str, list]:",
            "        \"\"\"",
            "        Extract filters to be applied to the dataframe from passed code.",
            "",
            "        Args:",
            "            code (str): A snippet of code to be parsed.",
            "",
            "        Returns:",
            "            dict: The dictionary containing all filters parsed from",
            "                the passed code. The dictionary has the following structure:",
            "                {",
            "                    \"<df_number>\": [",
            "                        (\"<left_operand>\", \"<operator>\", \"<right_operand>\")",
            "                    ]",
            "                }",
            "",
            "        Raises:",
            "            SyntaxError: If the code is unable to be parsed by `ast.parse()`.",
            "            Exception: If any exception is raised during working with nodes",
            "                of the code tree.",
            "        \"\"\"",
            "        try:",
            "            parsed_tree = ast.parse(code)",
            "        except SyntaxError:",
            "            self.logger.log(",
            "                \"Invalid code passed for extracting filters\", level=logging.ERROR",
            "            )",
            "            self.logger.log(f\"{traceback.format_exc()}\", level=logging.DEBUG)",
            "            raise",
            "",
            "        try:",
            "            filters = self._extract_comparisons(parsed_tree)",
            "        except Exception:",
            "            self.logger.log(",
            "                \"Unable to extract filters for passed code\", level=logging.ERROR",
            "            )",
            "            self.logger.log(f\"Error: {traceback.format_exc()}\", level=logging.DEBUG)",
            "            return {}",
            "",
            "        return filters",
            "",
            "    def _extract_comparisons(self, tree: ast.Module) -> dict[str, list]:",
            "        \"\"\"",
            "        Process nodes from passed tree to extract filters.",
            "",
            "        Collects all assignments in the tree.",
            "        Collects all function calls in the tree.",
            "        Walk over the tree and handle each comparison node.",
            "        For each comparison node, defined what `df` is this node related to.",
            "        Parse constants values from the comparison node.",
            "        Add to the result dict.",
            "",
            "        Args:",
            "            tree (str): A snippet of code to be parsed.",
            "",
            "        Returns:",
            "            dict: The `defaultdict(list)` instance containing all filters",
            "                parsed from the passed instructions tree. The dictionary has",
            "                the following structure:",
            "                {",
            "                    \"<df_number>\": [",
            "                        (\"<left_operand>\", \"<operator>\", \"<right_operand>\")",
            "                    ]",
            "                }",
            "        \"\"\"",
            "        comparisons = defaultdict(list)",
            "        current_df = \"dfs[0]\"",
            "",
            "        visitor = AssignmentVisitor()",
            "        visitor.visit(tree)",
            "        assignments = visitor.assignment_nodes",
            "",
            "        call_visitor = CallVisitor()",
            "        call_visitor.visit(tree)",
            "",
            "        for node in ast.walk(tree):",
            "            if isinstance(node, ast.Compare) and isinstance(node.left, ast.Subscript):",
            "                name, *slices = self._tokenize_operand(node.left)",
            "                current_df = (",
            "                    self._get_df_id_by_nearest_assignment(",
            "                        node.lineno, assignments, name",
            "                    )",
            "                    or current_df",
            "                )",
            "                left_str = slices[-1] if slices else name",
            "",
            "                for op, right in zip(node.ops, node.comparators):",
            "                    op_str = self._ast_comparator_map.get(type(op), \"Unknown\")",
            "                    name, *slices = self._tokenize_operand(right)",
            "                    right_str = slices[-1] if slices else name",
            "",
            "                    comparisons[current_df].append((left_str, op_str, right_str))",
            "        return comparisons",
            "",
            "    def _retry_run_code(",
            "        self,",
            "        code: str,",
            "        context: PipelineContext,",
            "        logger: Logger,",
            "        e: Exception,",
            "    ) -> str:",
            "        \"\"\"",
            "        A method to retry the code execution with error correction framework.",
            "",
            "        Args:",
            "            code (str): A python code",
            "            context (PipelineContext) : Pipeline Context",
            "            logger (Logger) : Logger",
            "            e (Exception): An exception",
            "            dataframes",
            "",
            "        Returns (str): A python code",
            "        \"\"\"",
            "        if self.on_retry:",
            "            return self.on_retry(code, e)",
            "        else:",
            "            raise e",
            "",
            "    @staticmethod",
            "    def _tokenize_operand(operand_node: ast.expr) -> Generator[str, None, None]:",
            "        \"\"\"",
            "        Utility generator function to get subscript slice constants.",
            "",
            "        Args:",
            "            operand_node (ast.expr):",
            "                The node to be tokenized.",
            "        Yields:",
            "            str: Token string.",
            "",
            "        Examples:",
            "            >>> code = '''",
            "            ... foo = [1, [2, 3], [[4, 5], [6, 7]]]",
            "            ... print(foo[2][1][0])",
            "            ... '''",
            "            >>> tree = ast.parse(code)",
            "            >>> res = CodeManager._tokenize_operand(tree.body[1].value.args[0])",
            "            >>> print(list(res))",
            "            ['foo', 2, 1, 0]",
            "        \"\"\"",
            "        if isinstance(operand_node, ast.Call):",
            "            yield operand_node.func.attr",
            "",
            "        if isinstance(operand_node, ast.Subscript):",
            "            slice_ = operand_node.slice.value",
            "            yield from CodeExecution._tokenize_operand(operand_node.value)",
            "            yield slice_",
            "",
            "        if isinstance(operand_node, ast.Name):",
            "            yield operand_node.id",
            "",
            "        if isinstance(operand_node, ast.Constant):",
            "            yield operand_node.value",
            "",
            "    @staticmethod",
            "    def _get_nearest_func_call(current_lineno, calls, func_name):",
            "        \"\"\"",
            "        Utility function to get the nearest previous call node.",
            "",
            "        Sort call nodes list (copy of the list) by line number.",
            "        Iterate over the call nodes list. If the call node's function name",
            "        equals to `func_name`, set `nearest_call` to the node object.",
            "",
            "        Args:",
            "            current_lineno (int): Number of the current processed line.",
            "            calls (list[ast.Assign]): List of call nodes.",
            "            func_name (str): Name of the target function.",
            "",
            "        Returns:",
            "            ast.Call: The node of the nearest previous call `<func_name>()`.",
            "        \"\"\"",
            "        for call in reversed(calls):",
            "            if call.lineno < current_lineno:",
            "                try:",
            "                    if call.func.attr == func_name:",
            "                        return call",
            "                except AttributeError:",
            "                    continue",
            "",
            "        return None",
            "",
            "    @staticmethod",
            "    def _get_df_id_by_nearest_assignment(",
            "        current_lineno: int, assignments: list[ast.Assign], target_name: str",
            "    ):",
            "        \"\"\"",
            "        Utility function to get df label by finding the nearest assignment.",
            "",
            "        Sort assignment nodes list (copy of the list) by line number.",
            "        Iterate over the assignment nodes list. If the assignment node's value",
            "        looks like `dfs[<index>]` and target label equals to `target_name`,",
            "        set `nearest_assignment` to \"dfs[<index>]\".",
            "",
            "        Args:",
            "            current_lineno (int): Number of the current processed line.",
            "            assignments (list[ast.Assign]): List of assignment nodes.",
            "            target_name (str): Name of the target variable. The assignment",
            "                node is supposed to assign to this name.",
            "",
            "        Returns:",
            "            str: The string representing df label, looks like \"dfs[<index>]\".",
            "        \"\"\"",
            "        nearest_assignment = None",
            "        assignments = sorted(assignments, key=lambda node: node.lineno)",
            "        for assignment in assignments:",
            "            if assignment.lineno > current_lineno:",
            "                return nearest_assignment",
            "            try:",
            "                is_subscript = isinstance(assignment.value, ast.Subscript)",
            "                dfs_on_the_right = assignment.value.value.id == \"dfs\"",
            "                assign_to_target = assignment.targets[0].id == target_name",
            "                if is_subscript and dfs_on_the_right and assign_to_target:",
            "                    nearest_assignment = f\"dfs[{assignment.value.slice.value}]\"",
            "            except AttributeError:",
            "                continue"
        ],
        "afterPatchFile": [
            "import ast",
            "import logging",
            "import traceback",
            "from collections import defaultdict",
            "from typing import Any, Callable, Generator, List, Union",
            "",
            "from pandasai.exceptions import InvalidLLMOutputType, InvalidOutputValueMismatch",
            "from pandasai.pipelines.logic_unit_output import LogicUnitOutput",
            "from pandasai.responses.response_serializer import ResponseSerializer",
            "",
            "from ...exceptions import NoResultFoundError",
            "from ...helpers.logger import Logger",
            "from ...helpers.node_visitors import AssignmentVisitor, CallVisitor",
            "from ...helpers.optional import get_environment",
            "from ...helpers.output_validator import OutputValidator",
            "from ...schemas.df_config import Config",
            "from ..base_logic_unit import BaseLogicUnit",
            "from ..pipeline_context import PipelineContext",
            "from .code_cleaning import CodeExecutionContext",
            "",
            "",
            "class CodeExecution(BaseLogicUnit):",
            "    \"\"\"",
            "    Code Execution Stage",
            "    \"\"\"",
            "",
            "    _dfs: List",
            "    _config: Union[Config, dict]",
            "    _additional_dependencies: List[dict] = []",
            "    _current_code_executed: str = None",
            "    _retry_if_fail: bool = False",
            "    _ast_comparator_map: dict = {",
            "        ast.Eq: \"=\",",
            "        ast.NotEq: \"!=\",",
            "        ast.Lt: \"<\",",
            "        ast.LtE: \"<=\",",
            "        ast.Gt: \">\",",
            "        ast.GtE: \">=\",",
            "        ast.Is: \"is\",",
            "        ast.IsNot: \"is not\",",
            "        ast.In: \"in\",",
            "        ast.NotIn: \"not in\",",
            "    }",
            "",
            "    def __init__(",
            "        self,",
            "        on_failure: Callable[[str, Exception], None] = None,",
            "        on_retry: Callable[[str, Exception], None] = None,",
            "        **kwargs,",
            "    ):",
            "        super().__init__(**kwargs)",
            "        self.on_failure = on_failure",
            "        self.on_retry = on_retry",
            "",
            "    def execute(self, input: Any, **kwargs) -> Any:",
            "        \"\"\"",
            "        This method will return output according to",
            "        Implementation.",
            "",
            "        :param input: Your input data.",
            "        :param kwargs: A dictionary of keyword arguments.",
            "            - 'logger' (any): The logger for logging.",
            "            - 'config' (Config): Global configurations for the test",
            "            - 'context' (any): The execution context.",
            "",
            "        :return: The result of the execution.",
            "        \"\"\"",
            "        self.context: PipelineContext = kwargs.get(\"context\")",
            "        self._dfs = self.context.dfs",
            "        self._config = self.context.config",
            "        self._additional_dependencies = self.context.get(\"additional_dependencies\", [])",
            "        self._current_code_executed = self.context.get(\"current_code_executed\")",
            "        self.logger: Logger = kwargs.get(\"logger\")",
            "",
            "        # Execute the code",
            "        code_context = CodeExecutionContext(",
            "            self.context.get(\"last_prompt_id\"), self.context.skills_manager",
            "        )",
            "",
            "        retry_count = 0",
            "        code_to_run = input",
            "        result = None",
            "        while retry_count <= self.context.config.max_retries:",
            "            try:",
            "                result = self.execute_code(code_to_run, code_context)",
            "                if self.context.get(\"output_type\") != \"\" and (",
            "                    output_helper := self.context.get(\"output_type\")",
            "                ):",
            "                    (validation_ok, validation_errors) = OutputValidator.validate(",
            "                        output_helper, result",
            "                    )",
            "",
            "                    if not validation_ok:",
            "                        raise InvalidLLMOutputType(validation_errors)",
            "",
            "                if not OutputValidator.validate_result(result):",
            "                    raise InvalidOutputValueMismatch(",
            "                        f'Value type {type(result[\"value\"])} must match with type {result[\"type\"]}'",
            "                    )",
            "",
            "                break",
            "",
            "            except Exception as e:",
            "                traceback_errors = traceback.format_exc()",
            "                self.logger.log(f\"Failed with error: {traceback_errors}\", logging.ERROR)",
            "                if self.on_failure:",
            "                    self.on_failure(code_to_run, traceback_errors)",
            "",
            "                if (",
            "                    not self.context.config.use_error_correction_framework",
            "                    or retry_count >= self.context.config.max_retries",
            "                ):",
            "                    raise e",
            "",
            "                retry_count += 1",
            "",
            "                self.logger.log(",
            "                    f\"Failed to execute code retrying with a correction framework \"",
            "                    f\"[retry number: {retry_count}]\",",
            "                    level=logging.WARNING,",
            "                )",
            "",
            "                # TODO - Move this implement to main execute function",
            "                # Temporarily done for test cases this is to be fixed move to the main function",
            "                code_to_run = self._retry_run_code(",
            "                    code_to_run, self.context, self.logger, e",
            "                )",
            "",
            "        return LogicUnitOutput(",
            "            result,",
            "            True,",
            "            \"Code Executed Successfully\",",
            "            {\"content_type\": \"response\", \"value\": ResponseSerializer.serialize(result)},",
            "            final_track_output=True,",
            "        )",
            "",
            "    def execute_code(self, code: str, context: CodeExecutionContext) -> Any:",
            "        \"\"\"",
            "        Execute the python code generated by LLMs to answer the question",
            "        about the input dataframe. Run the code in the current context and return the",
            "        result.",
            "",
            "        Args:",
            "            code (str): Python code to execute.",
            "            context (CodeExecutionContext): Code Execution Context",
            "                    with prompt id and skills.",
            "",
            "        Returns:",
            "            Any: The result of the code execution. The type of the result depends",
            "                on the generated code.",
            "",
            "        \"\"\"",
            "        # List the required dfs, so we can avoid to run the connectors",
            "        # if the code does not need them",
            "        dfs = self._required_dfs(code)",
            "        environment: dict = get_environment(",
            "            self._additional_dependencies,",
            "            secure=self._config.security in [\"standard\", \"advanced\"],",
            "        )",
            "        environment[\"dfs\"] = self._get_originals(dfs)",
            "        if len(environment[\"dfs\"]) == 1:",
            "            environment[\"df\"] = environment[\"dfs\"][0]",
            "",
            "        if self._config.direct_sql:",
            "            environment[\"execute_sql_query\"] = self._dfs[0].execute_direct_sql_query",
            "",
            "        # Add skills to the env",
            "        if context.skills_manager.used_skills:",
            "            for skill_func_name in context.skills_manager.used_skills:",
            "                skill = context.skills_manager.get_skill_by_func_name(skill_func_name)",
            "                environment[skill_func_name] = skill",
            "",
            "        # Execute the code",
            "        exec(code, environment)",
            "",
            "        # Get the result",
            "        if \"result\" not in environment:",
            "            raise NoResultFoundError(\"No result returned\")",
            "",
            "        return environment[\"result\"]",
            "",
            "    def _required_dfs(self, code: str) -> List[str]:",
            "        \"\"\"",
            "        List the index of the DataFrames that are needed to execute the code. The goal",
            "        is to avoid to run the connectors if the code does not need them.",
            "",
            "        Args:",
            "            code (str): Python code to execute",
            "",
            "        Returns:",
            "            List[int]: A list of the index of the DataFrames that are needed to execute",
            "            the code.",
            "        \"\"\"",
            "",
            "        # Sometimes GPT-3.5/4 use a for loop to iterate over the dfs (even if there is only one)",
            "        # or they concatenate the dfs. In this case we need all the dfs",
            "        if \"for df in dfs\" in code or \"pd.concat(dfs\" in code:",
            "            return self._dfs",
            "",
            "        required_dfs = []",
            "        for i, df in enumerate(self._dfs):",
            "            if f\"dfs[{i}]\" in code:",
            "                required_dfs.append(df)",
            "            else:",
            "                required_dfs.append(None)",
            "        return required_dfs or self._dfs",
            "",
            "    def _get_originals(self, dfs):",
            "        \"\"\"",
            "        Get original dfs",
            "",
            "        Args:",
            "            dfs (list): List of dfs",
            "",
            "        Returns:",
            "            list: List of dfs",
            "        \"\"\"",
            "        original_dfs = []",
            "        for index, df in enumerate(dfs):",
            "            if df is None:",
            "                original_dfs.append(None)",
            "                continue",
            "",
            "            extracted_filters = self._extract_filters(self._current_code_executed)",
            "            filters = extracted_filters.get(f\"dfs[{index}]\", [])",
            "            df.set_additional_filters(filters)",
            "",
            "            df.execute()",
            "            # df.load_connector(partial=len(filters) > 0)",
            "",
            "            original_dfs.append(df.pandas_df)",
            "",
            "        return original_dfs",
            "",
            "    def _extract_filters(self, code) -> dict[str, list]:",
            "        \"\"\"",
            "        Extract filters to be applied to the dataframe from passed code.",
            "",
            "        Args:",
            "            code (str): A snippet of code to be parsed.",
            "",
            "        Returns:",
            "            dict: The dictionary containing all filters parsed from",
            "                the passed code. The dictionary has the following structure:",
            "                {",
            "                    \"<df_number>\": [",
            "                        (\"<left_operand>\", \"<operator>\", \"<right_operand>\")",
            "                    ]",
            "                }",
            "",
            "        Raises:",
            "            SyntaxError: If the code is unable to be parsed by `ast.parse()`.",
            "            Exception: If any exception is raised during working with nodes",
            "                of the code tree.",
            "        \"\"\"",
            "        try:",
            "            parsed_tree = ast.parse(code)",
            "        except SyntaxError:",
            "            self.logger.log(",
            "                \"Invalid code passed for extracting filters\", level=logging.ERROR",
            "            )",
            "            self.logger.log(f\"{traceback.format_exc()}\", level=logging.DEBUG)",
            "            raise",
            "",
            "        try:",
            "            filters = self._extract_comparisons(parsed_tree)",
            "        except Exception:",
            "            self.logger.log(",
            "                \"Unable to extract filters for passed code\", level=logging.ERROR",
            "            )",
            "            self.logger.log(f\"Error: {traceback.format_exc()}\", level=logging.DEBUG)",
            "            return {}",
            "",
            "        return filters",
            "",
            "    def _extract_comparisons(self, tree: ast.Module) -> dict[str, list]:",
            "        \"\"\"",
            "        Process nodes from passed tree to extract filters.",
            "",
            "        Collects all assignments in the tree.",
            "        Collects all function calls in the tree.",
            "        Walk over the tree and handle each comparison node.",
            "        For each comparison node, defined what `df` is this node related to.",
            "        Parse constants values from the comparison node.",
            "        Add to the result dict.",
            "",
            "        Args:",
            "            tree (str): A snippet of code to be parsed.",
            "",
            "        Returns:",
            "            dict: The `defaultdict(list)` instance containing all filters",
            "                parsed from the passed instructions tree. The dictionary has",
            "                the following structure:",
            "                {",
            "                    \"<df_number>\": [",
            "                        (\"<left_operand>\", \"<operator>\", \"<right_operand>\")",
            "                    ]",
            "                }",
            "        \"\"\"",
            "        comparisons = defaultdict(list)",
            "        current_df = \"dfs[0]\"",
            "",
            "        visitor = AssignmentVisitor()",
            "        visitor.visit(tree)",
            "        assignments = visitor.assignment_nodes",
            "",
            "        call_visitor = CallVisitor()",
            "        call_visitor.visit(tree)",
            "",
            "        for node in ast.walk(tree):",
            "            if isinstance(node, ast.Compare) and isinstance(node.left, ast.Subscript):",
            "                name, *slices = self._tokenize_operand(node.left)",
            "                current_df = (",
            "                    self._get_df_id_by_nearest_assignment(",
            "                        node.lineno, assignments, name",
            "                    )",
            "                    or current_df",
            "                )",
            "                left_str = slices[-1] if slices else name",
            "",
            "                for op, right in zip(node.ops, node.comparators):",
            "                    op_str = self._ast_comparator_map.get(type(op), \"Unknown\")",
            "                    name, *slices = self._tokenize_operand(right)",
            "                    right_str = slices[-1] if slices else name",
            "",
            "                    comparisons[current_df].append((left_str, op_str, right_str))",
            "        return comparisons",
            "",
            "    def _retry_run_code(",
            "        self,",
            "        code: str,",
            "        context: PipelineContext,",
            "        logger: Logger,",
            "        e: Exception,",
            "    ) -> str:",
            "        \"\"\"",
            "        A method to retry the code execution with error correction framework.",
            "",
            "        Args:",
            "            code (str): A python code",
            "            context (PipelineContext) : Pipeline Context",
            "            logger (Logger) : Logger",
            "            e (Exception): An exception",
            "            dataframes",
            "",
            "        Returns (str): A python code",
            "        \"\"\"",
            "        if self.on_retry:",
            "            return self.on_retry(code, e)",
            "        else:",
            "            raise e",
            "",
            "    @staticmethod",
            "    def _tokenize_operand(operand_node: ast.expr) -> Generator[str, None, None]:",
            "        \"\"\"",
            "        Utility generator function to get subscript slice constants.",
            "",
            "        Args:",
            "            operand_node (ast.expr):",
            "                The node to be tokenized.",
            "        Yields:",
            "            str: Token string.",
            "",
            "        Examples:",
            "            >>> code = '''",
            "            ... foo = [1, [2, 3], [[4, 5], [6, 7]]]",
            "            ... print(foo[2][1][0])",
            "            ... '''",
            "            >>> tree = ast.parse(code)",
            "            >>> res = CodeManager._tokenize_operand(tree.body[1].value.args[0])",
            "            >>> print(list(res))",
            "            ['foo', 2, 1, 0]",
            "        \"\"\"",
            "        if isinstance(operand_node, ast.Call):",
            "            yield operand_node.func.attr",
            "",
            "        if isinstance(operand_node, ast.Subscript):",
            "            slice_ = operand_node.slice.value",
            "            yield from CodeExecution._tokenize_operand(operand_node.value)",
            "            yield slice_",
            "",
            "        if isinstance(operand_node, ast.Name):",
            "            yield operand_node.id",
            "",
            "        if isinstance(operand_node, ast.Constant):",
            "            yield operand_node.value",
            "",
            "    @staticmethod",
            "    def _get_nearest_func_call(current_lineno, calls, func_name):",
            "        \"\"\"",
            "        Utility function to get the nearest previous call node.",
            "",
            "        Sort call nodes list (copy of the list) by line number.",
            "        Iterate over the call nodes list. If the call node's function name",
            "        equals to `func_name`, set `nearest_call` to the node object.",
            "",
            "        Args:",
            "            current_lineno (int): Number of the current processed line.",
            "            calls (list[ast.Assign]): List of call nodes.",
            "            func_name (str): Name of the target function.",
            "",
            "        Returns:",
            "            ast.Call: The node of the nearest previous call `<func_name>()`.",
            "        \"\"\"",
            "        for call in reversed(calls):",
            "            if call.lineno < current_lineno:",
            "                try:",
            "                    if call.func.attr == func_name:",
            "                        return call",
            "                except AttributeError:",
            "                    continue",
            "",
            "        return None",
            "",
            "    @staticmethod",
            "    def _get_df_id_by_nearest_assignment(",
            "        current_lineno: int, assignments: list[ast.Assign], target_name: str",
            "    ):",
            "        \"\"\"",
            "        Utility function to get df label by finding the nearest assignment.",
            "",
            "        Sort assignment nodes list (copy of the list) by line number.",
            "        Iterate over the assignment nodes list. If the assignment node's value",
            "        looks like `dfs[<index>]` and target label equals to `target_name`,",
            "        set `nearest_assignment` to \"dfs[<index>]\".",
            "",
            "        Args:",
            "            current_lineno (int): Number of the current processed line.",
            "            assignments (list[ast.Assign]): List of assignment nodes.",
            "            target_name (str): Name of the target variable. The assignment",
            "                node is supposed to assign to this name.",
            "",
            "        Returns:",
            "            str: The string representing df label, looks like \"dfs[<index>]\".",
            "        \"\"\"",
            "        nearest_assignment = None",
            "        assignments = sorted(assignments, key=lambda node: node.lineno)",
            "        for assignment in assignments:",
            "            if assignment.lineno > current_lineno:",
            "                return nearest_assignment",
            "            try:",
            "                is_subscript = isinstance(assignment.value, ast.Subscript)",
            "                dfs_on_the_right = assignment.value.value.id == \"dfs\"",
            "                assign_to_target = assignment.targets[0].id == target_name",
            "                if is_subscript and dfs_on_the_right and assign_to_target:",
            "                    nearest_assignment = f\"dfs[{assignment.value.slice.value}]\"",
            "            except AttributeError:",
            "                continue"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "156": [
                "CodeExecution",
                "execute_code"
            ]
        },
        "addLocation": []
    },
    "pandasai/safe_libs/base_restricted_module.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": "             # Check for any suspicious arguments that might be used for importing"
            },
            "1": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": "             for arg in args + tuple(kwargs.values()):"
            },
            "2": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": "                 if isinstance(arg, str) and any("
            },
            "3": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    module in arg.lower()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+                    module == arg.lower()"
            },
            "5": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": "                     for module in [\"io\", \"os\", \"subprocess\", \"sys\", \"importlib\"]"
            },
            "6": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": "                 ):"
            },
            "7": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": "                     raise SecurityError("
            }
        },
        "frontPatchFile": [
            "class BaseRestrictedModule:",
            "    def _wrap_function(self, func):",
            "        def wrapper(*args, **kwargs):",
            "            # Check for any suspicious arguments that might be used for importing",
            "            for arg in args + tuple(kwargs.values()):",
            "                if isinstance(arg, str) and any(",
            "                    module in arg.lower()",
            "                    for module in [\"io\", \"os\", \"subprocess\", \"sys\", \"importlib\"]",
            "                ):",
            "                    raise SecurityError(",
            "                        f\"Potential security risk: '{arg}' is not allowed\"",
            "                    )",
            "            return func(*args, **kwargs)",
            "",
            "        return wrapper",
            "",
            "    def _wrap_class(self, cls):",
            "        class WrappedClass(cls):",
            "            def __getattribute__(self, name):",
            "                attr = super().__getattribute__(name)",
            "                return self._wrap_function(self, attr) if callable(attr) else attr",
            "",
            "        return WrappedClass",
            "",
            "",
            "class SecurityError(Exception):",
            "    pass"
        ],
        "afterPatchFile": [
            "class BaseRestrictedModule:",
            "    def _wrap_function(self, func):",
            "        def wrapper(*args, **kwargs):",
            "            # Check for any suspicious arguments that might be used for importing",
            "            for arg in args + tuple(kwargs.values()):",
            "                if isinstance(arg, str) and any(",
            "                    module == arg.lower()",
            "                    for module in [\"io\", \"os\", \"subprocess\", \"sys\", \"importlib\"]",
            "                ):",
            "                    raise SecurityError(",
            "                        f\"Potential security risk: '{arg}' is not allowed\"",
            "                    )",
            "            return func(*args, **kwargs)",
            "",
            "        return wrapper",
            "",
            "    def _wrap_class(self, cls):",
            "        class WrappedClass(cls):",
            "            def __getattribute__(self, name):",
            "                attr = super().__getattribute__(name)",
            "                return self._wrap_function(self, attr) if callable(attr) else attr",
            "",
            "        return WrappedClass",
            "",
            "",
            "class SecurityError(Exception):",
            "    pass"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "7": [
                "BaseRestrictedModule",
                "_wrap_function",
                "wrapper"
            ]
        },
        "addLocation": []
    },
    "pandasai/schemas/df_config.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing import Any, List, Optional, TypedDict"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+from typing import Any, List, Literal, Optional, TypedDict"
            },
            "2": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from pandasai.constants import DEFAULT_CHART_DIRECTORY"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " from pandasai.helpers.dataframe_serializer import DataframeSerializerType"
            },
            "5": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": "     log_server: LogServerConfig = None"
            },
            "6": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": "     direct_sql: bool = False"
            },
            "7": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "     dataframe_serializer: DataframeSerializerType = DataframeSerializerType.CSV"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+    security: Literal[\"standard\", \"none\", \"advanced\"] = \"standard\""
            },
            "9": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "     class Config:"
            },
            "11": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "         arbitrary_types_allowed = True"
            }
        },
        "frontPatchFile": [
            "from typing import Any, List, Optional, TypedDict",
            "",
            "from pandasai.constants import DEFAULT_CHART_DIRECTORY",
            "from pandasai.helpers.dataframe_serializer import DataframeSerializerType",
            "from pandasai.pydantic import BaseModel, Field, validator",
            "",
            "from ..llm import LLM, BambooLLM, LangchainLLM",
            "",
            "",
            "class LogServerConfig(TypedDict):",
            "    server_url: str",
            "    api_key: str",
            "",
            "",
            "class Config(BaseModel):",
            "    save_logs: bool = True",
            "    verbose: bool = False",
            "    enforce_privacy: bool = False",
            "    enable_cache: bool = True",
            "    use_error_correction_framework: bool = True",
            "    open_charts: bool = True",
            "    save_charts: bool = False",
            "    save_charts_path: str = DEFAULT_CHART_DIRECTORY",
            "    custom_whitelisted_dependencies: List[str] = Field(default_factory=list)",
            "    max_retries: int = 3",
            "    lazy_load_connector: bool = True",
            "    response_parser: Any = None",
            "    llm: LLM = None",
            "    data_viz_library: Optional[str] = \"\"",
            "    log_server: LogServerConfig = None",
            "    direct_sql: bool = False",
            "    dataframe_serializer: DataframeSerializerType = DataframeSerializerType.CSV",
            "",
            "    class Config:",
            "        arbitrary_types_allowed = True",
            "",
            "    @validator(\"llm\", always=True)",
            "    def validate_llm(cls, llm) -> LLM:",
            "        if not isinstance(llm, (LLM, LangchainLLM)):  # also covers llm is None",
            "            return BambooLLM()",
            "        return llm"
        ],
        "afterPatchFile": [
            "from typing import Any, List, Literal, Optional, TypedDict",
            "",
            "from pandasai.constants import DEFAULT_CHART_DIRECTORY",
            "from pandasai.helpers.dataframe_serializer import DataframeSerializerType",
            "from pandasai.pydantic import BaseModel, Field, validator",
            "",
            "from ..llm import LLM, BambooLLM, LangchainLLM",
            "",
            "",
            "class LogServerConfig(TypedDict):",
            "    server_url: str",
            "    api_key: str",
            "",
            "",
            "class Config(BaseModel):",
            "    save_logs: bool = True",
            "    verbose: bool = False",
            "    enforce_privacy: bool = False",
            "    enable_cache: bool = True",
            "    use_error_correction_framework: bool = True",
            "    open_charts: bool = True",
            "    save_charts: bool = False",
            "    save_charts_path: str = DEFAULT_CHART_DIRECTORY",
            "    custom_whitelisted_dependencies: List[str] = Field(default_factory=list)",
            "    max_retries: int = 3",
            "    lazy_load_connector: bool = True",
            "    response_parser: Any = None",
            "    llm: LLM = None",
            "    data_viz_library: Optional[str] = \"\"",
            "    log_server: LogServerConfig = None",
            "    direct_sql: bool = False",
            "    dataframe_serializer: DataframeSerializerType = DataframeSerializerType.CSV",
            "    security: Literal[\"standard\", \"none\", \"advanced\"] = \"standard\"",
            "",
            "    class Config:",
            "        arbitrary_types_allowed = True",
            "",
            "    @validator(\"llm\", always=True)",
            "    def validate_llm(cls, llm) -> LLM:",
            "        if not isinstance(llm, (LLM, LangchainLLM)):  # also covers llm is None",
            "            return BambooLLM()",
            "        return llm"
        ],
        "action": [
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1": []
        },
        "addLocation": []
    }
}