{
    "lektor/editor.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from lektor.constants import PRIMARY_ALT"
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from lektor.metaformat import serialize"
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from lektor.utils import atomic_open"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+from lektor.utils import cleanup_path"
            },
            "4": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from lektor.utils import increment_filename"
            },
            "5": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from lektor.utils import is_valid_id"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+from lektor.utils import parse_path"
            },
            "7": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from lektor.utils import secure_filename"
            },
            "8": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "     pass"
            },
            "11": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+def _is_valid_path(path: str) -> bool:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+    split_path = path.strip(\"/\").split(\"/\")"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+    if split_path == [\"\"]:"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+        split_path = []"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+    return parse_path(path) == split_path"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " def make_editor_session(pad, path, is_attachment=None, alt=PRIMARY_ALT, datamodel=None):"
            },
            "21": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "     \"\"\"Creates an editor session for the given path object.\"\"\""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+    if not _is_valid_path(path):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+        raise BadEdit(\"Invalid path\")"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+    path = cleanup_path(path)"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 50,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "     if alt != PRIMARY_ALT and not pad.db.config.is_valid_alternative(alt):"
            },
            "27": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "         raise BadEdit(\"Attempted to edit an invalid alternative (%s)\" % alt)"
            },
            "28": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import os",
            "import posixpath",
            "import shutil",
            "import warnings",
            "from collections import ChainMap",
            "from collections import OrderedDict",
            "from collections.abc import ItemsView",
            "from collections.abc import KeysView",
            "from collections.abc import Mapping",
            "from collections.abc import MutableMapping",
            "from collections.abc import ValuesView",
            "from contextlib import suppress",
            "from functools import wraps",
            "from itertools import chain",
            "",
            "from lektor.constants import PRIMARY_ALT",
            "from lektor.metaformat import serialize",
            "from lektor.utils import atomic_open",
            "from lektor.utils import increment_filename",
            "from lektor.utils import is_valid_id",
            "from lektor.utils import secure_filename",
            "",
            "",
            "implied_keys = {\"_id\", \"_path\", \"_gid\", \"_alt\", \"_source_alt\", \"_attachment_for\"}",
            "possibly_implied_keys = {\"_model\", \"_template\", \"_attachment_type\"}",
            "",
            "",
            "class BadEdit(Exception):",
            "    pass",
            "",
            "",
            "class BadDelete(BadEdit):",
            "    pass",
            "",
            "",
            "def make_editor_session(pad, path, is_attachment=None, alt=PRIMARY_ALT, datamodel=None):",
            "    \"\"\"Creates an editor session for the given path object.\"\"\"",
            "    if alt != PRIMARY_ALT and not pad.db.config.is_valid_alternative(alt):",
            "        raise BadEdit(\"Attempted to edit an invalid alternative (%s)\" % alt)",
            "",
            "    raw_data = pad.db.load_raw_data(path, cls=OrderedDict, alt=alt, fallback=False)",
            "    raw_data_fallback = None",
            "    if alt != PRIMARY_ALT:",
            "        raw_data_fallback = pad.db.load_raw_data(path, cls=OrderedDict)",
            "        all_data = OrderedDict()",
            "        all_data.update(raw_data_fallback or ())",
            "        all_data.update(raw_data or ())",
            "    else:",
            "        all_data = raw_data",
            "",
            "    id = posixpath.basename(path)",
            "    if not is_valid_id(id):",
            "        raise BadEdit(\"Invalid ID\")",
            "",
            "    record = None",
            "    exists = raw_data is not None or raw_data_fallback is not None",
            "    if raw_data is None:",
            "        raw_data = OrderedDict()",
            "",
            "    if is_attachment is None:",
            "        if not exists:",
            "            is_attachment = False",
            "        else:",
            "            is_attachment = bool(all_data.get(\"_attachment_for\"))",
            "    elif bool(all_data.get(\"_attachment_for\")) != is_attachment:",
            "        raise BadEdit(",
            "            \"The attachment flag passed is conflicting with the \"",
            "            \"record's attachment flag.\"",
            "        )",
            "",
            "    if exists:",
            "        # XXX: what about changing the datamodel after the fact?",
            "        if datamodel is not None:",
            "            raise BadEdit(",
            "                \"When editing an existing record, a datamodel must not be provided.\"",
            "            )",
            "        datamodel = pad.db.get_datamodel_for_raw_data(all_data, pad)",
            "    else:",
            "        if datamodel is None:",
            "            datamodel = pad.db.get_implied_datamodel(path, is_attachment, pad)",
            "        elif isinstance(datamodel, str):",
            "            datamodel = pad.db.datamodels[datamodel]",
            "",
            "    if exists:",
            "        record = pad.instance_from_data(dict(all_data), datamodel)",
            "",
            "    for key in implied_keys:",
            "        raw_data.pop(key, None)",
            "        if raw_data_fallback:",
            "            raw_data_fallback.pop(key, None)",
            "",
            "    return EditorSession(",
            "        pad,",
            "        id,",
            "        str(path),",
            "        raw_data,",
            "        raw_data_fallback,",
            "        datamodel,",
            "        record,",
            "        exists,",
            "        is_attachment,",
            "        alt,",
            "    )",
            "",
            "",
            "def _deprecated_data_proxy(wrapped):",
            "    \"\"\"Issue warning when deprecated mapping methods are used directly on",
            "    EditorSession.",
            "    \"\"\"",
            "",
            "    name = wrapped.__name__",
            "    newname = name[4:] if name.startswith(\"iter\") else name",
            "",
            "    @wraps(wrapped)",
            "    def wrapper(self, *args, **kwargs):",
            "        warnings.warn(",
            "            f\"EditorSession.{name} has been deprecated as of Lektor 3.3.2. \"",
            "            f\"Please use EditorSession.data.{newname} instead.\",",
            "            DeprecationWarning,",
            "            stacklevel=2,",
            "        )",
            "        return wrapped(self, *args, **kwargs)",
            "",
            "    return wrapper",
            "",
            "",
            "class EditorSession:",
            "    def __init__(",
            "        self,",
            "        pad,",
            "        id,",
            "        path,",
            "        original_data,",
            "        fallback_data,",
            "        datamodel,",
            "        record,",
            "        exists=True,",
            "        is_attachment=False,",
            "        alt=PRIMARY_ALT,",
            "    ):",
            "        self.id = id",
            "        self.pad = pad",
            "        self.path = path",
            "        self.record = record",
            "        self.exists = exists",
            "        self.datamodel = datamodel",
            "        self.is_root = path.strip(\"/\") == \"\"",
            "        self.alt = alt",
            "",
            "        slug_format = None",
            "        parent_name = posixpath.dirname(path)",
            "        if parent_name != path:",
            "            parent = pad.get(parent_name)",
            "            if parent is not None:",
            "                slug_format = parent.datamodel.child_config.slug_format",
            "        if slug_format is None:",
            "            slug_format = \"{{ this._id }}\"",
            "        self.slug_format = slug_format",
            "        self.implied_attachment_type = None",
            "",
            "        if is_attachment:",
            "            self.implied_attachment_type = pad.db.get_attachment_type(path)",
            "",
            "        self.data = MutableEditorData(original_data, fallback_data)",
            "",
            "        self._delete_this = False",
            "        self._recursive_delete = False",
            "        self._master_delete = False",
            "        self.is_attachment = is_attachment",
            "        self.closed = False",
            "",
            "    def to_json(self):",
            "        label = None",
            "        label_i18n = None",
            "        url_path = None",
            "        if self.record is not None:",
            "            label = self.record.record_label",
            "            label_i18n = self.record.get_record_label_i18n()",
            "            url_path = self.record.url_path",
            "        else:",
            "            label = self.id",
            "        can_be_deleted = not self.datamodel.protected and not self.is_root",
            "        return {",
            "            \"data\": dict(self.data.items()),",
            "            \"record_info\": {",
            "                \"id\": self.id,",
            "                \"path\": self.path,",
            "                \"exists\": self.exists,",
            "                \"label\": label,",
            "                \"label_i18n\": label_i18n,",
            "                \"url_path\": url_path,",
            "                \"alt\": self.alt,",
            "                \"is_attachment\": self.is_attachment,",
            "                \"can_be_deleted\": can_be_deleted,",
            "                \"slug_format\": self.slug_format,",
            "                \"implied_attachment_type\": self.implied_attachment_type,",
            "                \"default_template\": self.datamodel.get_default_template_name(),",
            "            },",
            "            \"datamodel\": self.datamodel.to_json(self.pad, self.record, self.alt),",
            "        }",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_value, tb):",
            "        if exc_type is not None:",
            "            self.rollback()",
            "        else:",
            "            self.commit()",
            "",
            "    def get_fs_path(self, alt=PRIMARY_ALT):",
            "        \"\"\"The path to the record file on the file system.\"\"\"",
            "        base = self.pad.db.to_fs_path(self.path)",
            "        suffix = \".lr\"",
            "        if alt != PRIMARY_ALT:",
            "            suffix = f\"+{alt}{suffix}\"",
            "        if self.is_attachment:",
            "            return base + suffix",
            "        return os.path.join(base, \"contents\" + suffix)",
            "",
            "    @property",
            "    def fs_path(self):",
            "        \"\"\"The file system path of the content file on disk.\"\"\"",
            "        return self.get_fs_path(self.alt)",
            "",
            "    @property",
            "    def attachment_fs_path(self):",
            "        \"\"\"The file system path of the actual attachment.\"\"\"",
            "        if self.is_attachment:",
            "            return self.pad.db.to_fs_path(self.path)",
            "        return None",
            "",
            "    def rollback(self):",
            "        \"\"\"Ignores all changes and rejects them.\"\"\"",
            "        if self.closed:",
            "            return",
            "        self.closed = True",
            "",
            "    def commit(self):",
            "        \"\"\"Saves changes back to the file system.\"\"\"",
            "        if not self.closed:",
            "            if self._delete_this:",
            "                self._delete_impl()",
            "            else:",
            "                self._save_impl()",
            "        self.closed = True",
            "",
            "    def delete(self, recursive=None, delete_master=False):",
            "        \"\"\"Deletes the record.  How the delete works depends on what is being",
            "        deleted:",
            "",
            "        *   delete attachment: recursive mode is silently ignored.  If",
            "            `delete_master` is set then the attachment is deleted, otherwise",
            "            only the metadata is deleted.",
            "        *   delete page: in recursive mode everything is deleted in which",
            "            case `delete_master` must be set to `True` or an error is",
            "            generated.  In fact, the default is to perform a recursive",
            "            delete in that case.  If `delete_master` is False, then only the",
            "            contents file of the current alt is deleted.",
            "",
            "        If a delete cannot be performed, an error is generated.",
            "        \"\"\"",
            "        if self.closed:",
            "            return",
            "        if recursive is None:",
            "            recursive = not self.is_attachment and delete_master",
            "        self._delete_this = True",
            "        self._recursive_delete = recursive",
            "        self._master_delete = delete_master",
            "",
            "    def add_attachment(self, filename, fp):",
            "        \"\"\"Stores a new attachment.  Returns `None` if the file already\"\"\"",
            "        if not self.exists:",
            "            raise BadEdit(\"Record does not exist.\")",
            "        if self.is_attachment:",
            "            raise BadEdit(\"Cannot attach something to an attachment.\")",
            "        if not self.datamodel.has_own_attachments:",
            "            raise BadEdit(\"Attachments are disabled for this page.\")",
            "        directory = self.pad.db.to_fs_path(self.path)",
            "",
            "        safe_filename = secure_filename(filename)",
            "",
            "        while 1:",
            "            fn = os.path.join(directory, safe_filename)",
            "            if not os.path.isfile(fn):",
            "                break",
            "            safe_filename = increment_filename(fn)",
            "",
            "        with atomic_open(fn, \"wb\") as f:",
            "            shutil.copyfileobj(fp, f)",
            "        return safe_filename",
            "",
            "    def _attachment_delete_impl(self):",
            "        files = [self.fs_path]",
            "        if self._master_delete:",
            "            files.append(self.attachment_fs_path)",
            "            for alt in self.pad.db.config.list_alternatives():",
            "                files.append(self.get_fs_path(alt))",
            "",
            "        for fn in files:",
            "            try:",
            "                os.unlink(fn)",
            "            except OSError:",
            "                pass",
            "",
            "    def _page_delete_impl(self):",
            "        directory = os.path.dirname(self.fs_path)",
            "",
            "        if self._recursive_delete:",
            "            with suppress(OSError):",
            "                shutil.rmtree(directory)",
            "            return",
            "        if self._master_delete:",
            "            raise BadDelete(",
            "                \"Master deletes of pages require that recursive deleting is enabled.\"",
            "            )",
            "",
            "        for fn in self.fs_path, directory:",
            "            try:",
            "                os.unlink(fn)",
            "            except OSError:",
            "                pass",
            "",
            "    def _delete_impl(self):",
            "        if self.alt != PRIMARY_ALT:",
            "            if self._master_delete:",
            "                raise BadDelete(",
            "                    \"Master deletes need to be done from the primary \"",
            "                    'alt.  Tried to delete from \"%s\"' % self.alt",
            "                )",
            "            if self._recursive_delete:",
            "                raise BadDelete(",
            "                    \"Cannot perform recursive delete from a non \"",
            "                    'primary alt.  Tried to delete from \"%s\"' % self.alt",
            "                )",
            "",
            "        if self.is_attachment:",
            "            self._attachment_delete_impl()",
            "        else:",
            "            self._page_delete_impl()",
            "",
            "    def _save_impl(self):",
            "        # When creating a new alt from a primary self.exists is True but",
            "        # the file does not exist yet.  In this case we want to explicitly",
            "        # create it anyways instead of bailing.",
            "        if not self.data.ischanged() and self.exists and os.path.exists(self.fs_path):",
            "            return",
            "",
            "        try:",
            "            os.makedirs(os.path.dirname(self.fs_path))",
            "        except OSError:",
            "            pass",
            "",
            "        with atomic_open(self.fs_path, \"wb\") as f:",
            "            for chunk in serialize(self.data.items(fallback=False), encoding=\"utf-8\"):",
            "                f.write(chunk)",
            "",
            "    def __repr__(self):",
            "        return \"<{} {!r}{}{}>\".format(",
            "            self.__class__.__name__,",
            "            self.path,",
            "            self.alt != PRIMARY_ALT and \" alt=%r\" % self.alt or \"\",",
            "            not self.exists and \" new\" or \"\",",
            "        )",
            "",
            "    # The mapping methods used to access the page data have been moved",
            "    # to EditorSession.data.",
            "    #",
            "    # We have left behind these proxy methods so as not to break any existing",
            "    # external code.",
            "    @_deprecated_data_proxy",
            "    def revert_key(self, key):",
            "        self.data.revert_key(key)",
            "",
            "    @_deprecated_data_proxy",
            "    def __contains__(self, key):",
            "        return key in self.data",
            "",
            "    @_deprecated_data_proxy",
            "    def __getitem__(self, key):",
            "        return self.data[key]",
            "",
            "    @_deprecated_data_proxy",
            "    def __len__(self):",
            "        return len(self.data)",
            "",
            "    @_deprecated_data_proxy",
            "    def __iter__(self):",
            "        return iter(self.data)",
            "",
            "    @_deprecated_data_proxy",
            "    def items(self, fallback=True):",
            "        return self.data.items(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def keys(self, fallback=True):",
            "        return self.data.keys(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def values(self, fallback=True):",
            "        return self.data.values(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def iteritems(self, fallback=True):",
            "        return self.data.items(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def iterkeys(self, fallback=True):",
            "        return self.data.keys(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def itervalues(self, fallback=True):",
            "        return self.data.values(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def __setitem__(self, key, value):",
            "        self.data[key] = value",
            "",
            "    @_deprecated_data_proxy",
            "    def update(self, *args, **kwargs):",
            "        self.data.update(*args, **kwargs)",
            "",
            "",
            "del _deprecated_data_proxy",
            "",
            "",
            "class EditorData(Mapping):",
            "    \"\"\"A read-only view of edited data.",
            "",
            "    This is a chained dict with (possibly) mutated data overlaid on",
            "    the original data for the record.",
            "    \"\"\"",
            "",
            "    def __init__(self, original_data, fallback_data=None, _changed_data=None):",
            "        if _changed_data is None:",
            "            _changed_data = {}",
            "        self.fallback_data = fallback_data",
            "        if fallback_data:",
            "            self._orig_data = ChainMap(original_data, fallback_data)",
            "            self._data = ChainMap(_changed_data, original_data, fallback_data)",
            "        else:",
            "            self._orig_data = original_data",
            "            self._data = ChainMap(_changed_data, original_data)",
            "",
            "    @property",
            "    def _changed_data(self):",
            "        return self._data.maps[0]",
            "",
            "    @property",
            "    def original_data(self):",
            "        return self._data.maps[1]",
            "",
            "    def ischanged(self):",
            "        return len(self._changed_data) > 0",
            "",
            "    def __getitem__(self, key):",
            "        rv = self._data.get(key)",
            "        if rv is None:",
            "            raise KeyError(key)",
            "        return rv",
            "",
            "    def __iter__(self):",
            "        data = self._data",
            "        fallback_data = self.fallback_data or {}",
            "",
            "        for key in _uniq(chain(self.original_data, fallback_data, sorted(data))):",
            "            if key not in implied_keys:",
            "                if data[key] is not None:",
            "                    yield key",
            "",
            "    def __len__(self):",
            "        data = self._data",
            "        return sum(",
            "            1 for key in data if key not in implied_keys and data[key] is not None",
            "        )",
            "",
            "    def keys(self, fallback=True):  # pylint: disable=arguments-differ",
            "        return KeysView(self if fallback else self._without_fallback())",
            "",
            "    def items(self, fallback=True):  # pylint: disable=arguments-differ",
            "        return ItemsView(self if fallback else self._without_fallback())",
            "",
            "    def values(self, fallback=True):  # pylint: disable=arguments-differ",
            "        return ValuesView(self if fallback else self._without_fallback())",
            "",
            "    def _without_fallback(self):",
            "        \"\"\"Return a copy of self, with fallback_data set to None.\"\"\"",
            "        if not self.fallback_data:",
            "            return self",
            "        return EditorData(self.original_data, _changed_data=self._changed_data)",
            "",
            "",
            "class MutableEditorData(EditorData, MutableMapping):",
            "    \"\"\"A mutable view of edited data.",
            "",
            "    This is a chained dict with (possibly) mutated data overlaid on",
            "    the original data for the record.",
            "    \"\"\"",
            "",
            "    def __setitem__(self, key, value):",
            "        if key in implied_keys:",
            "            raise KeyError(f\"Can not set implied key {key!r}\")",
            "        orig_value = self._orig_data.get(key)",
            "        if value != orig_value:",
            "            self._data[key] = value",
            "        elif key in possibly_implied_keys:",
            "            # If the key is in the possibly implied key set and set to",
            "            # that value, we will set it to changed anyways.  This allows",
            "            # overriding of such special keys.",
            "            self._data[key] = value",
            "        else:",
            "            self._data.pop(key, None)",
            "",
            "    def __delitem__(self, key):",
            "        if key in implied_keys or self._data.get(key) is None:",
            "            raise KeyError(key)",
            "        self[key] = None",
            "",
            "    def revert_key(self, key):",
            "        \"\"\"Reverts a key to the implied value.\"\"\"",
            "        self._data.pop(key, None)",
            "",
            "",
            "def _uniq(seq):",
            "    \"\"\"Return items from iterable in order, skipping items that have already been seen.",
            "",
            "    The items in ``seq`` must be hashable.",
            "    \"\"\"",
            "    seen = set()",
            "    for item in seq:",
            "        if item not in seen:",
            "            seen.add(item)",
            "            yield item"
        ],
        "afterPatchFile": [
            "import os",
            "import posixpath",
            "import shutil",
            "import warnings",
            "from collections import ChainMap",
            "from collections import OrderedDict",
            "from collections.abc import ItemsView",
            "from collections.abc import KeysView",
            "from collections.abc import Mapping",
            "from collections.abc import MutableMapping",
            "from collections.abc import ValuesView",
            "from contextlib import suppress",
            "from functools import wraps",
            "from itertools import chain",
            "",
            "from lektor.constants import PRIMARY_ALT",
            "from lektor.metaformat import serialize",
            "from lektor.utils import atomic_open",
            "from lektor.utils import cleanup_path",
            "from lektor.utils import increment_filename",
            "from lektor.utils import is_valid_id",
            "from lektor.utils import parse_path",
            "from lektor.utils import secure_filename",
            "",
            "",
            "implied_keys = {\"_id\", \"_path\", \"_gid\", \"_alt\", \"_source_alt\", \"_attachment_for\"}",
            "possibly_implied_keys = {\"_model\", \"_template\", \"_attachment_type\"}",
            "",
            "",
            "class BadEdit(Exception):",
            "    pass",
            "",
            "",
            "class BadDelete(BadEdit):",
            "    pass",
            "",
            "",
            "def _is_valid_path(path: str) -> bool:",
            "    split_path = path.strip(\"/\").split(\"/\")",
            "    if split_path == [\"\"]:",
            "        split_path = []",
            "    return parse_path(path) == split_path",
            "",
            "",
            "def make_editor_session(pad, path, is_attachment=None, alt=PRIMARY_ALT, datamodel=None):",
            "    \"\"\"Creates an editor session for the given path object.\"\"\"",
            "    if not _is_valid_path(path):",
            "        raise BadEdit(\"Invalid path\")",
            "    path = cleanup_path(path)",
            "",
            "    if alt != PRIMARY_ALT and not pad.db.config.is_valid_alternative(alt):",
            "        raise BadEdit(\"Attempted to edit an invalid alternative (%s)\" % alt)",
            "",
            "    raw_data = pad.db.load_raw_data(path, cls=OrderedDict, alt=alt, fallback=False)",
            "    raw_data_fallback = None",
            "    if alt != PRIMARY_ALT:",
            "        raw_data_fallback = pad.db.load_raw_data(path, cls=OrderedDict)",
            "        all_data = OrderedDict()",
            "        all_data.update(raw_data_fallback or ())",
            "        all_data.update(raw_data or ())",
            "    else:",
            "        all_data = raw_data",
            "",
            "    id = posixpath.basename(path)",
            "    if not is_valid_id(id):",
            "        raise BadEdit(\"Invalid ID\")",
            "",
            "    record = None",
            "    exists = raw_data is not None or raw_data_fallback is not None",
            "    if raw_data is None:",
            "        raw_data = OrderedDict()",
            "",
            "    if is_attachment is None:",
            "        if not exists:",
            "            is_attachment = False",
            "        else:",
            "            is_attachment = bool(all_data.get(\"_attachment_for\"))",
            "    elif bool(all_data.get(\"_attachment_for\")) != is_attachment:",
            "        raise BadEdit(",
            "            \"The attachment flag passed is conflicting with the \"",
            "            \"record's attachment flag.\"",
            "        )",
            "",
            "    if exists:",
            "        # XXX: what about changing the datamodel after the fact?",
            "        if datamodel is not None:",
            "            raise BadEdit(",
            "                \"When editing an existing record, a datamodel must not be provided.\"",
            "            )",
            "        datamodel = pad.db.get_datamodel_for_raw_data(all_data, pad)",
            "    else:",
            "        if datamodel is None:",
            "            datamodel = pad.db.get_implied_datamodel(path, is_attachment, pad)",
            "        elif isinstance(datamodel, str):",
            "            datamodel = pad.db.datamodels[datamodel]",
            "",
            "    if exists:",
            "        record = pad.instance_from_data(dict(all_data), datamodel)",
            "",
            "    for key in implied_keys:",
            "        raw_data.pop(key, None)",
            "        if raw_data_fallback:",
            "            raw_data_fallback.pop(key, None)",
            "",
            "    return EditorSession(",
            "        pad,",
            "        id,",
            "        str(path),",
            "        raw_data,",
            "        raw_data_fallback,",
            "        datamodel,",
            "        record,",
            "        exists,",
            "        is_attachment,",
            "        alt,",
            "    )",
            "",
            "",
            "def _deprecated_data_proxy(wrapped):",
            "    \"\"\"Issue warning when deprecated mapping methods are used directly on",
            "    EditorSession.",
            "    \"\"\"",
            "",
            "    name = wrapped.__name__",
            "    newname = name[4:] if name.startswith(\"iter\") else name",
            "",
            "    @wraps(wrapped)",
            "    def wrapper(self, *args, **kwargs):",
            "        warnings.warn(",
            "            f\"EditorSession.{name} has been deprecated as of Lektor 3.3.2. \"",
            "            f\"Please use EditorSession.data.{newname} instead.\",",
            "            DeprecationWarning,",
            "            stacklevel=2,",
            "        )",
            "        return wrapped(self, *args, **kwargs)",
            "",
            "    return wrapper",
            "",
            "",
            "class EditorSession:",
            "    def __init__(",
            "        self,",
            "        pad,",
            "        id,",
            "        path,",
            "        original_data,",
            "        fallback_data,",
            "        datamodel,",
            "        record,",
            "        exists=True,",
            "        is_attachment=False,",
            "        alt=PRIMARY_ALT,",
            "    ):",
            "        self.id = id",
            "        self.pad = pad",
            "        self.path = path",
            "        self.record = record",
            "        self.exists = exists",
            "        self.datamodel = datamodel",
            "        self.is_root = path.strip(\"/\") == \"\"",
            "        self.alt = alt",
            "",
            "        slug_format = None",
            "        parent_name = posixpath.dirname(path)",
            "        if parent_name != path:",
            "            parent = pad.get(parent_name)",
            "            if parent is not None:",
            "                slug_format = parent.datamodel.child_config.slug_format",
            "        if slug_format is None:",
            "            slug_format = \"{{ this._id }}\"",
            "        self.slug_format = slug_format",
            "        self.implied_attachment_type = None",
            "",
            "        if is_attachment:",
            "            self.implied_attachment_type = pad.db.get_attachment_type(path)",
            "",
            "        self.data = MutableEditorData(original_data, fallback_data)",
            "",
            "        self._delete_this = False",
            "        self._recursive_delete = False",
            "        self._master_delete = False",
            "        self.is_attachment = is_attachment",
            "        self.closed = False",
            "",
            "    def to_json(self):",
            "        label = None",
            "        label_i18n = None",
            "        url_path = None",
            "        if self.record is not None:",
            "            label = self.record.record_label",
            "            label_i18n = self.record.get_record_label_i18n()",
            "            url_path = self.record.url_path",
            "        else:",
            "            label = self.id",
            "        can_be_deleted = not self.datamodel.protected and not self.is_root",
            "        return {",
            "            \"data\": dict(self.data.items()),",
            "            \"record_info\": {",
            "                \"id\": self.id,",
            "                \"path\": self.path,",
            "                \"exists\": self.exists,",
            "                \"label\": label,",
            "                \"label_i18n\": label_i18n,",
            "                \"url_path\": url_path,",
            "                \"alt\": self.alt,",
            "                \"is_attachment\": self.is_attachment,",
            "                \"can_be_deleted\": can_be_deleted,",
            "                \"slug_format\": self.slug_format,",
            "                \"implied_attachment_type\": self.implied_attachment_type,",
            "                \"default_template\": self.datamodel.get_default_template_name(),",
            "            },",
            "            \"datamodel\": self.datamodel.to_json(self.pad, self.record, self.alt),",
            "        }",
            "",
            "    def __enter__(self):",
            "        return self",
            "",
            "    def __exit__(self, exc_type, exc_value, tb):",
            "        if exc_type is not None:",
            "            self.rollback()",
            "        else:",
            "            self.commit()",
            "",
            "    def get_fs_path(self, alt=PRIMARY_ALT):",
            "        \"\"\"The path to the record file on the file system.\"\"\"",
            "        base = self.pad.db.to_fs_path(self.path)",
            "        suffix = \".lr\"",
            "        if alt != PRIMARY_ALT:",
            "            suffix = f\"+{alt}{suffix}\"",
            "        if self.is_attachment:",
            "            return base + suffix",
            "        return os.path.join(base, \"contents\" + suffix)",
            "",
            "    @property",
            "    def fs_path(self):",
            "        \"\"\"The file system path of the content file on disk.\"\"\"",
            "        return self.get_fs_path(self.alt)",
            "",
            "    @property",
            "    def attachment_fs_path(self):",
            "        \"\"\"The file system path of the actual attachment.\"\"\"",
            "        if self.is_attachment:",
            "            return self.pad.db.to_fs_path(self.path)",
            "        return None",
            "",
            "    def rollback(self):",
            "        \"\"\"Ignores all changes and rejects them.\"\"\"",
            "        if self.closed:",
            "            return",
            "        self.closed = True",
            "",
            "    def commit(self):",
            "        \"\"\"Saves changes back to the file system.\"\"\"",
            "        if not self.closed:",
            "            if self._delete_this:",
            "                self._delete_impl()",
            "            else:",
            "                self._save_impl()",
            "        self.closed = True",
            "",
            "    def delete(self, recursive=None, delete_master=False):",
            "        \"\"\"Deletes the record.  How the delete works depends on what is being",
            "        deleted:",
            "",
            "        *   delete attachment: recursive mode is silently ignored.  If",
            "            `delete_master` is set then the attachment is deleted, otherwise",
            "            only the metadata is deleted.",
            "        *   delete page: in recursive mode everything is deleted in which",
            "            case `delete_master` must be set to `True` or an error is",
            "            generated.  In fact, the default is to perform a recursive",
            "            delete in that case.  If `delete_master` is False, then only the",
            "            contents file of the current alt is deleted.",
            "",
            "        If a delete cannot be performed, an error is generated.",
            "        \"\"\"",
            "        if self.closed:",
            "            return",
            "        if recursive is None:",
            "            recursive = not self.is_attachment and delete_master",
            "        self._delete_this = True",
            "        self._recursive_delete = recursive",
            "        self._master_delete = delete_master",
            "",
            "    def add_attachment(self, filename, fp):",
            "        \"\"\"Stores a new attachment.  Returns `None` if the file already\"\"\"",
            "        if not self.exists:",
            "            raise BadEdit(\"Record does not exist.\")",
            "        if self.is_attachment:",
            "            raise BadEdit(\"Cannot attach something to an attachment.\")",
            "        if not self.datamodel.has_own_attachments:",
            "            raise BadEdit(\"Attachments are disabled for this page.\")",
            "        directory = self.pad.db.to_fs_path(self.path)",
            "",
            "        safe_filename = secure_filename(filename)",
            "",
            "        while 1:",
            "            fn = os.path.join(directory, safe_filename)",
            "            if not os.path.isfile(fn):",
            "                break",
            "            safe_filename = increment_filename(fn)",
            "",
            "        with atomic_open(fn, \"wb\") as f:",
            "            shutil.copyfileobj(fp, f)",
            "        return safe_filename",
            "",
            "    def _attachment_delete_impl(self):",
            "        files = [self.fs_path]",
            "        if self._master_delete:",
            "            files.append(self.attachment_fs_path)",
            "            for alt in self.pad.db.config.list_alternatives():",
            "                files.append(self.get_fs_path(alt))",
            "",
            "        for fn in files:",
            "            try:",
            "                os.unlink(fn)",
            "            except OSError:",
            "                pass",
            "",
            "    def _page_delete_impl(self):",
            "        directory = os.path.dirname(self.fs_path)",
            "",
            "        if self._recursive_delete:",
            "            with suppress(OSError):",
            "                shutil.rmtree(directory)",
            "            return",
            "        if self._master_delete:",
            "            raise BadDelete(",
            "                \"Master deletes of pages require that recursive deleting is enabled.\"",
            "            )",
            "",
            "        for fn in self.fs_path, directory:",
            "            try:",
            "                os.unlink(fn)",
            "            except OSError:",
            "                pass",
            "",
            "    def _delete_impl(self):",
            "        if self.alt != PRIMARY_ALT:",
            "            if self._master_delete:",
            "                raise BadDelete(",
            "                    \"Master deletes need to be done from the primary \"",
            "                    'alt.  Tried to delete from \"%s\"' % self.alt",
            "                )",
            "            if self._recursive_delete:",
            "                raise BadDelete(",
            "                    \"Cannot perform recursive delete from a non \"",
            "                    'primary alt.  Tried to delete from \"%s\"' % self.alt",
            "                )",
            "",
            "        if self.is_attachment:",
            "            self._attachment_delete_impl()",
            "        else:",
            "            self._page_delete_impl()",
            "",
            "    def _save_impl(self):",
            "        # When creating a new alt from a primary self.exists is True but",
            "        # the file does not exist yet.  In this case we want to explicitly",
            "        # create it anyways instead of bailing.",
            "        if not self.data.ischanged() and self.exists and os.path.exists(self.fs_path):",
            "            return",
            "",
            "        try:",
            "            os.makedirs(os.path.dirname(self.fs_path))",
            "        except OSError:",
            "            pass",
            "",
            "        with atomic_open(self.fs_path, \"wb\") as f:",
            "            for chunk in serialize(self.data.items(fallback=False), encoding=\"utf-8\"):",
            "                f.write(chunk)",
            "",
            "    def __repr__(self):",
            "        return \"<{} {!r}{}{}>\".format(",
            "            self.__class__.__name__,",
            "            self.path,",
            "            self.alt != PRIMARY_ALT and \" alt=%r\" % self.alt or \"\",",
            "            not self.exists and \" new\" or \"\",",
            "        )",
            "",
            "    # The mapping methods used to access the page data have been moved",
            "    # to EditorSession.data.",
            "    #",
            "    # We have left behind these proxy methods so as not to break any existing",
            "    # external code.",
            "    @_deprecated_data_proxy",
            "    def revert_key(self, key):",
            "        self.data.revert_key(key)",
            "",
            "    @_deprecated_data_proxy",
            "    def __contains__(self, key):",
            "        return key in self.data",
            "",
            "    @_deprecated_data_proxy",
            "    def __getitem__(self, key):",
            "        return self.data[key]",
            "",
            "    @_deprecated_data_proxy",
            "    def __len__(self):",
            "        return len(self.data)",
            "",
            "    @_deprecated_data_proxy",
            "    def __iter__(self):",
            "        return iter(self.data)",
            "",
            "    @_deprecated_data_proxy",
            "    def items(self, fallback=True):",
            "        return self.data.items(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def keys(self, fallback=True):",
            "        return self.data.keys(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def values(self, fallback=True):",
            "        return self.data.values(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def iteritems(self, fallback=True):",
            "        return self.data.items(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def iterkeys(self, fallback=True):",
            "        return self.data.keys(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def itervalues(self, fallback=True):",
            "        return self.data.values(fallback)",
            "",
            "    @_deprecated_data_proxy",
            "    def __setitem__(self, key, value):",
            "        self.data[key] = value",
            "",
            "    @_deprecated_data_proxy",
            "    def update(self, *args, **kwargs):",
            "        self.data.update(*args, **kwargs)",
            "",
            "",
            "del _deprecated_data_proxy",
            "",
            "",
            "class EditorData(Mapping):",
            "    \"\"\"A read-only view of edited data.",
            "",
            "    This is a chained dict with (possibly) mutated data overlaid on",
            "    the original data for the record.",
            "    \"\"\"",
            "",
            "    def __init__(self, original_data, fallback_data=None, _changed_data=None):",
            "        if _changed_data is None:",
            "            _changed_data = {}",
            "        self.fallback_data = fallback_data",
            "        if fallback_data:",
            "            self._orig_data = ChainMap(original_data, fallback_data)",
            "            self._data = ChainMap(_changed_data, original_data, fallback_data)",
            "        else:",
            "            self._orig_data = original_data",
            "            self._data = ChainMap(_changed_data, original_data)",
            "",
            "    @property",
            "    def _changed_data(self):",
            "        return self._data.maps[0]",
            "",
            "    @property",
            "    def original_data(self):",
            "        return self._data.maps[1]",
            "",
            "    def ischanged(self):",
            "        return len(self._changed_data) > 0",
            "",
            "    def __getitem__(self, key):",
            "        rv = self._data.get(key)",
            "        if rv is None:",
            "            raise KeyError(key)",
            "        return rv",
            "",
            "    def __iter__(self):",
            "        data = self._data",
            "        fallback_data = self.fallback_data or {}",
            "",
            "        for key in _uniq(chain(self.original_data, fallback_data, sorted(data))):",
            "            if key not in implied_keys:",
            "                if data[key] is not None:",
            "                    yield key",
            "",
            "    def __len__(self):",
            "        data = self._data",
            "        return sum(",
            "            1 for key in data if key not in implied_keys and data[key] is not None",
            "        )",
            "",
            "    def keys(self, fallback=True):  # pylint: disable=arguments-differ",
            "        return KeysView(self if fallback else self._without_fallback())",
            "",
            "    def items(self, fallback=True):  # pylint: disable=arguments-differ",
            "        return ItemsView(self if fallback else self._without_fallback())",
            "",
            "    def values(self, fallback=True):  # pylint: disable=arguments-differ",
            "        return ValuesView(self if fallback else self._without_fallback())",
            "",
            "    def _without_fallback(self):",
            "        \"\"\"Return a copy of self, with fallback_data set to None.\"\"\"",
            "        if not self.fallback_data:",
            "            return self",
            "        return EditorData(self.original_data, _changed_data=self._changed_data)",
            "",
            "",
            "class MutableEditorData(EditorData, MutableMapping):",
            "    \"\"\"A mutable view of edited data.",
            "",
            "    This is a chained dict with (possibly) mutated data overlaid on",
            "    the original data for the record.",
            "    \"\"\"",
            "",
            "    def __setitem__(self, key, value):",
            "        if key in implied_keys:",
            "            raise KeyError(f\"Can not set implied key {key!r}\")",
            "        orig_value = self._orig_data.get(key)",
            "        if value != orig_value:",
            "            self._data[key] = value",
            "        elif key in possibly_implied_keys:",
            "            # If the key is in the possibly implied key set and set to",
            "            # that value, we will set it to changed anyways.  This allows",
            "            # overriding of such special keys.",
            "            self._data[key] = value",
            "        else:",
            "            self._data.pop(key, None)",
            "",
            "    def __delitem__(self, key):",
            "        if key in implied_keys or self._data.get(key) is None:",
            "            raise KeyError(key)",
            "        self[key] = None",
            "",
            "    def revert_key(self, key):",
            "        \"\"\"Reverts a key to the implied value.\"\"\"",
            "        self._data.pop(key, None)",
            "",
            "",
            "def _uniq(seq):",
            "    \"\"\"Return items from iterable in order, skipping items that have already been seen.",
            "",
            "    The items in ``seq`` must be hashable.",
            "    \"\"\"",
            "    seen = set()",
            "    for item in seq:",
            "        if item not in seen:",
            "            seen.add(item)",
            "            yield item"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "lektor.editor.make_editor_session",
            "pypdf.generic._data_structures"
        ]
    },
    "lektor/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 149,
                "afterPatchRowNumber": 149,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 150,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": 151,
                "PatchRowcode": " def untrusted_to_os_path(path):"
            },
            "3": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    path = path.strip(\"/\").replace(\"/\", os.path.sep)"
            },
            "4": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "     if not isinstance(path, str):"
            },
            "5": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "         path = path.decode(fs_enc, \"replace\")"
            },
            "6": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return path"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+    clean_path = cleanup_path(path)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 155,
                "PatchRowcode": "+    assert clean_path.startswith(\"/\")"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 156,
                "PatchRowcode": "+    return clean_path[1:].replace(\"/\", os.path.sep)"
            },
            "10": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 157,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 158,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 159,
                "PatchRowcode": " def is_path(path):"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import codecs",
            "import json",
            "import os",
            "import posixpath",
            "import re",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import threading",
            "import unicodedata",
            "import urllib.parse",
            "import uuid",
            "import warnings",
            "from contextlib import contextmanager",
            "from contextlib import suppress",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "from functools import lru_cache",
            "from functools import wraps",
            "from pathlib import PurePosixPath",
            "from typing import Any",
            "from typing import Callable",
            "from typing import ClassVar",
            "from typing import Hashable",
            "from typing import Iterable",
            "from typing import overload",
            "from typing import TypeVar",
            "",
            "from jinja2 import is_undefined",
            "from markupsafe import Markup",
            "from slugify import slugify as _slugify",
            "from werkzeug.http import http_date",
            "from werkzeug.urls import iri_to_uri",
            "from werkzeug.urls import uri_to_iri",
            "",
            "",
            "is_windows = os.name == \"nt\"",
            "",
            "_slash_escape = \"\\\\/\" not in json.dumps(\"/\")",
            "",
            "_last_num_re = re.compile(r\"^(.*)(\\d+)(.*?)$\")",
            "_list_marker = object()",
            "_value_marker = object()",
            "",
            "# Figure out our fs encoding, if it's ascii we upgrade to utf-8",
            "fs_enc = sys.getfilesystemencoding()",
            "try:",
            "    if codecs.lookup(fs_enc).name == \"ascii\":",
            "        fs_enc = \"utf-8\"",
            "except LookupError:",
            "    pass",
            "",
            "",
            "def split_virtual_path(path):",
            "    if \"@\" in path:",
            "        return path.split(\"@\", 1)",
            "    return path, None",
            "",
            "",
            "def _norm_join(a, b):",
            "    return posixpath.normpath(posixpath.join(a, b))",
            "",
            "",
            "def join_path(a, b):",
            "    \"\"\"Join two DB-paths.",
            "",
            "    It is assumed that both paths are already normalized in that",
            "    neither contains an extra \".\" or \"..\" components, double-slashes,",
            "    etc.",
            "    \"\"\"",
            "    # NB: This function is really only during URL resolution.  The only",
            "    # place that references it is lektor.source.SourceObject._resolve_url.",
            "",
            "    if posixpath.isabs(b):",
            "        return b",
            "",
            "    a_p, a_v = split_virtual_path(a)",
            "    b_p, b_v = split_virtual_path(b)",
            "",
            "    # Special case: paginations are considered special virtual paths",
            "    # where the parent is the actual parent of the page.  This however",
            "    # is explicitly not done if the path we join with refers to the",
            "    # current path (empty string or dot).",
            "    if b_p not in (\"\", \".\") and a_v and a_v.isdigit():",
            "        a_v = None",
            "",
            "    # New path has a virtual path, add that to it.",
            "    if b_v:",
            "        rv = _norm_join(a_p, b_p) + \"@\" + b_v",
            "    elif a_v:",
            "        rv = a_p + \"@\" + _norm_join(a_v, b_p)",
            "    else:",
            "        rv = _norm_join(a_p, b_p)",
            "    if rv[-2:] == \"@.\":",
            "        rv = rv[:-2]",
            "    return rv",
            "",
            "",
            "def cleanup_path(path):",
            "    # NB: POSIX allows for two leading slashes in a pathname, so we have to",
            "    # deal with the possiblity of leading double-slash ourself.",
            "    return posixpath.normpath(\"/\" + path.lstrip(\"/\"))",
            "",
            "",
            "def cleanup_url_path(url_path):",
            "    \"\"\"Clean up a URL path.",
            "",
            "    This strips any query, and/or fragment that may be present in the",
            "    input path.",
            "",
            "    Raises ValueError if the path contains a _scheme_",
            "    which is neither ``http`` nor ``https``, or a _netloc_.",
            "    \"\"\"",
            "    scheme, netloc, path, _, _ = urllib.parse.urlsplit(url_path, scheme=\"http\")",
            "    if scheme not in (\"http\", \"https\"):",
            "        raise ValueError(f\"Invalid scheme: {url_path!r}\")",
            "    if netloc:",
            "        raise ValueError(f\"Invalid netloc: {url_path!r}\")",
            "",
            "    # NB: POSIX allows for two leading slashes in a pathname, so we have to",
            "    # deal with the possiblity of leading double-slash ourself.",
            "    return posixpath.normpath(\"/\" + path.lstrip(\"/\"))",
            "",
            "",
            "def parse_path(path):",
            "    x = cleanup_path(path).strip(\"/\").split(\"/\")",
            "    if x == [\"\"]:",
            "        return []",
            "    return x",
            "",
            "",
            "def is_path_child_of(a, b, strict=True):",
            "    a_p, a_v = split_virtual_path(a)",
            "    b_p, b_v = split_virtual_path(b)",
            "    a_p = parse_path(a_p)",
            "    b_p = parse_path(b_p)",
            "    a_v = parse_path(a_v or \"\")",
            "    b_v = parse_path(b_v or \"\")",
            "",
            "    if not strict and a_p == b_p and a_v == b_v:",
            "        return True",
            "    if not a_v and b_v:",
            "        return False",
            "    if a_p == b_p and a_v[: len(b_v)] == b_v and len(a_v) > len(b_v):",
            "        return True",
            "    return a_p[: len(b_p)] == b_p and len(a_p) > len(b_p)",
            "",
            "",
            "def untrusted_to_os_path(path):",
            "    path = path.strip(\"/\").replace(\"/\", os.path.sep)",
            "    if not isinstance(path, str):",
            "        path = path.decode(fs_enc, \"replace\")",
            "    return path",
            "",
            "",
            "def is_path(path):",
            "    return os.path.sep in path or (os.path.altsep and os.path.altsep in path)",
            "",
            "",
            "def magic_split_ext(filename, ext_check=True):",
            "    \"\"\"Splits a filename into base and extension.  If ext check is enabled",
            "    (which is the default) then it verifies the extension is at least",
            "    reasonable.",
            "    \"\"\"",
            "",
            "    def bad_ext(ext):",
            "        if not ext_check:",
            "            return False",
            "        if not ext or ext.split() != [ext] or ext.strip() != ext:",
            "            return True",
            "        return False",
            "",
            "    parts = filename.rsplit(\".\", 2)",
            "    if len(parts) == 1:",
            "        return parts[0], \"\"",
            "    if len(parts) == 2 and not parts[0]:",
            "        return \".\" + parts[1], \"\"",
            "    if len(parts) == 3 and len(parts[1]) < 5:",
            "        ext = \".\".join(parts[1:])",
            "        if not bad_ext(ext):",
            "            return parts[0], ext",
            "    ext = parts[-1]",
            "    if bad_ext(ext):",
            "        return filename, \"\"",
            "    basename = \".\".join(parts[:-1])",
            "    return basename, ext",
            "",
            "",
            "def iter_dotted_path_prefixes(dotted_path):",
            "    pieces = dotted_path.split(\".\")",
            "    if len(pieces) == 1:",
            "        yield dotted_path, None",
            "    else:",
            "        for x in range(1, len(pieces)):",
            "            yield \".\".join(pieces[:x]), \".\".join(pieces[x:])",
            "",
            "",
            "def resolve_dotted_value(obj, dotted_path):",
            "    node = obj",
            "    for key in dotted_path.split(\".\"):",
            "        if isinstance(node, dict):",
            "            new_node = node.get(key)",
            "            if new_node is None and key.isdigit():",
            "                new_node = node.get(int(key))",
            "        elif isinstance(node, list):",
            "            try:",
            "                new_node = node[int(key)]",
            "            except (ValueError, TypeError, IndexError):",
            "                new_node = None",
            "        else:",
            "            new_node = None",
            "        node = new_node",
            "        if node is None:",
            "            break",
            "    return node",
            "",
            "",
            "def decode_flat_data(itemiter, dict_cls=dict):",
            "    def _split_key(name):",
            "        result = name.split(\".\")",
            "        for idx, part in enumerate(result):",
            "            if part.isdigit():",
            "                result[idx] = int(part)",
            "        return result",
            "",
            "    def _enter_container(container, key):",
            "        if key not in container:",
            "            return container.setdefault(key, dict_cls())",
            "        return container[key]",
            "",
            "    def _convert(container):",
            "        if _value_marker in container:",
            "            force_list = False",
            "            values = container.pop(_value_marker)",
            "            if container.pop(_list_marker, False):",
            "                force_list = True",
            "                values.extend(_convert(x[1]) for x in sorted(container.items()))",
            "            if not force_list and len(values) == 1:",
            "                values = values[0]",
            "",
            "            if not container:",
            "                return values",
            "            return _convert(container)",
            "        if container.pop(_list_marker, False):",
            "            return [_convert(x[1]) for x in sorted(container.items())]",
            "        return dict_cls((k, _convert(v)) for k, v in container.items())",
            "",
            "    result = dict_cls()",
            "",
            "    for key, value in itemiter:",
            "        parts = _split_key(key)",
            "        if not parts:",
            "            continue",
            "        container = result",
            "        for part in parts:",
            "            last_container = container",
            "            container = _enter_container(container, part)",
            "            last_container[_list_marker] = isinstance(part, int)",
            "        container[_value_marker] = [value]",
            "",
            "    return _convert(result)",
            "",
            "",
            "def merge(a, b):",
            "    \"\"\"Merges two values together.\"\"\"",
            "    if b is None and a is not None:",
            "        return a",
            "    if a is None:",
            "        return b",
            "    if isinstance(a, list) and isinstance(b, list):",
            "        for idx, (item_1, item_2) in enumerate(zip(a, b)):",
            "            a[idx] = merge(item_1, item_2)",
            "    if isinstance(a, dict) and isinstance(b, dict):",
            "        for key, value in b.items():",
            "            a[key] = merge(a.get(key), value)",
            "        return a",
            "    return a",
            "",
            "",
            "def slugify(text):",
            "    \"\"\"",
            "    A wrapper around python-slugify which preserves file extensions",
            "    and forward slashes.",
            "    \"\"\"",
            "",
            "    parts = text.split(\"/\")",
            "    parts[-1], ext = magic_split_ext(parts[-1])",
            "",
            "    out = \"/\".join(_slugify(part) for part in parts)",
            "",
            "    if ext:",
            "        return out + \".\" + ext",
            "    return out",
            "",
            "",
            "def secure_filename(filename, fallback_name=\"file\"):",
            "    base = filename.replace(\"/\", \" \").replace(\"\\\\\", \" \")",
            "    basename, ext = magic_split_ext(base)",
            "    rv = slugify(basename).lstrip(\".\")",
            "    if not rv:",
            "        rv = fallback_name",
            "    if ext:",
            "        return rv + \".\" + ext",
            "    return rv",
            "",
            "",
            "def increment_filename(filename):",
            "    directory, filename = os.path.split(filename)",
            "    basename, ext = magic_split_ext(filename, ext_check=False)",
            "",
            "    match = _last_num_re.match(basename)",
            "    if match is not None:",
            "        rv = match.group(1) + str(int(match.group(2)) + 1) + match.group(3)",
            "    else:",
            "        rv = basename + \"2\"",
            "",
            "    if ext:",
            "        rv += \".\" + ext",
            "    if directory:",
            "        return os.path.join(directory, rv)",
            "    return rv",
            "",
            "",
            "@lru_cache(maxsize=None)",
            "def locate_executable(exe_file, cwd=None, include_bundle_path=True):",
            "    \"\"\"Locates an executable in the search path.\"\"\"",
            "    choices = [exe_file]",
            "    resolve = True",
            "",
            "    # If it's already a path, we don't resolve.",
            "    if os.path.sep in exe_file or (os.path.altsep and os.path.altsep in exe_file):",
            "        resolve = False",
            "",
            "    extensions = os.environ.get(\"PATHEXT\", \"\").split(\";\")",
            "    _, ext = os.path.splitext(exe_file)",
            "    if (",
            "        os.name != \"nt\"",
            "        and \"\" not in extensions",
            "        or any(ext.lower() == extension.lower() for extension in extensions)",
            "    ):",
            "        extensions.insert(0, \"\")",
            "",
            "    if resolve:",
            "        paths = os.environ.get(\"PATH\", \"\").split(os.pathsep)",
            "        choices = [os.path.join(path, exe_file) for path in paths]",
            "",
            "    if os.name == \"nt\":",
            "        choices.append(os.path.join((cwd or os.getcwd()), exe_file))",
            "",
            "    try:",
            "        for path in choices:",
            "            for ext in extensions:",
            "                if os.access(path + ext, os.X_OK):",
            "                    return path + ext",
            "        return None",
            "    except OSError:",
            "        return None",
            "",
            "",
            "class JSONEncoder(json.JSONEncoder):",
            "    def default(self, o):  # pylint: disable=method-hidden",
            "        if is_undefined(o):",
            "            return None",
            "        if isinstance(o, datetime):",
            "            return http_date(o)",
            "        if isinstance(o, uuid.UUID):",
            "            return str(o)",
            "        if hasattr(o, \"__html__\"):",
            "            return str(o.__html__())",
            "        return json.JSONEncoder.default(self, o)",
            "",
            "",
            "def htmlsafe_json_dump(obj, **kwargs):",
            "    kwargs.setdefault(\"cls\", JSONEncoder)",
            "    rv = (",
            "        json.dumps(obj, **kwargs)",
            "        .replace(\"<\", \"\\\\u003c\")",
            "        .replace(\">\", \"\\\\u003e\")",
            "        .replace(\"&\", \"\\\\u0026\")",
            "        .replace(\"'\", \"\\\\u0027\")",
            "    )",
            "    if not _slash_escape:",
            "        rv = rv.replace(\"\\\\/\", \"/\")",
            "    return rv",
            "",
            "",
            "def tojson_filter(obj, **kwargs):",
            "    return Markup(htmlsafe_json_dump(obj, **kwargs))",
            "",
            "",
            "class Url(urllib.parse.SplitResult):",
            "    \"\"\"Make various parts of a URL accessible.",
            "",
            "    This is the type of the values exposed by Lektor record fields of type \"url\".",
            "",
            "    Since Lektor 3.4.0, this is essentially a `urllib.parse.SplitResult` as obtained by",
            "    calling `urlsplit` on the URL normalized to an IRI.",
            "",
            "    Generally, attributes such as ``netloc``, ``host``, ``path``, ``query``, and",
            "    ``fragment`` return the IRI (internationalied) versions of those components.",
            "",
            "    The URI (ASCII-encoded) version of the URL is available from the `ascii_url`",
            "    attribute.",
            "",
            "    NB: Changed in 3.4.0: The ``query`` attribute used to return the URI",
            "    (ASCII-encoded) version of the query \u2014 I'm not sure why. Now it returns",
            "    the IRI (internationalized) version of the query.",
            "",
            "    \"\"\"",
            "",
            "    def __new__(cls, value: str):",
            "        # XXX: deprecate use of constructor so that eventually we can make its signature",
            "        # match that of the SplitResult base class.",
            "        warnings.warn(",
            "            DeprecatedWarning(",
            "                \"Url\",",
            "                reason=(",
            "                    \"Direct construction of a Url instance is deprecated. \"",
            "                    \"Use the Url.from_string classmethod instead.\"",
            "                ),",
            "                version=\"3.4.0\",",
            "            ),",
            "            stacklevel=2,",
            "        )",
            "        return cls.from_string(value)",
            "",
            "    @classmethod",
            "    def from_string(cls, value: str) -> Url:",
            "        \"\"\"Construct instance from URL string.",
            "",
            "        The input URL can be a URI (all ASCII) or an IRI (internationalized).",
            "        \"\"\"",
            "        # The iri_to_uri operation is nominally idempotent \u2014 it can be passed either an",
            "        # IRI or a URI (or something inbetween) and will return a URI.  So to fully",
            "        # normalize input which can be either an IRI or a URI, first convert to URI,",
            "        # then to IRI.",
            "        iri = uri_to_iri(iri_to_uri(value))",
            "        obj = cls._make(urllib.parse.urlsplit(iri))",
            "        obj.url = value",
            "        return obj",
            "",
            "    def __str__(self) -> str:",
            "        \"\"\"The original un-normalized URL string.\"\"\"",
            "        return self.url",
            "",
            "    @property",
            "    def ascii_url(self) -> str:",
            "        \"\"\"The URL encoded to an all-ASCII URI.\"\"\"",
            "        return iri_to_uri(self.geturl())",
            "",
            "    @property",
            "    def ascii_host(self) -> str | None:",
            "        \"\"\"The hostname part of the URL IDNA-encoded to ASCII.\"\"\"",
            "        return urllib.parse.urlsplit(self.ascii_url).hostname",
            "",
            "    @property",
            "    def host(self) -> str | None:",
            "        \"\"\"The IRI (internationalized) version of the hostname.",
            "",
            "        This attribute is provided for backwards-compatibility.  New code should use the",
            "        ``hostname`` attribute instead.",
            "        \"\"\"",
            "        return self.hostname",
            "",
            "    @property",
            "    def anchor(self) -> str:",
            "        \"\"\"The IRI (internationalized) version of the \"anchor\" part of the URL.",
            "",
            "        This attribute is provided for backwards-compatibility.  New code should use the",
            "        ``fragment`` attribute instead.",
            "        \"\"\"",
            "        return self.fragment",
            "",
            "",
            "def is_unsafe_to_delete(path, base):",
            "    a = os.path.abspath(path)",
            "    b = os.path.abspath(base)",
            "    diff = os.path.relpath(a, b)",
            "    first = diff.split(os.path.sep)[0]",
            "    return first in (os.path.curdir, os.path.pardir)",
            "",
            "",
            "def prune_file_and_folder(name, base):",
            "    if is_unsafe_to_delete(name, base):",
            "        return False",
            "    try:",
            "        os.remove(name)",
            "    except OSError:",
            "        try:",
            "            os.rmdir(name)",
            "        except OSError:",
            "            return False",
            "    head, tail = os.path.split(name)",
            "    if not tail:",
            "        head, tail = os.path.split(head)",
            "    while head and tail:",
            "        try:",
            "            if is_unsafe_to_delete(head, base):",
            "                return False",
            "            os.rmdir(head)",
            "        except OSError:",
            "            break",
            "        head, tail = os.path.split(head)",
            "    return True",
            "",
            "",
            "def sort_normalize_string(s):",
            "    return unicodedata.normalize(\"NFD\", str(s).lower().strip())",
            "",
            "",
            "def get_dependent_url(url_path, suffix, ext=None):",
            "    url_directory, url_filename = posixpath.split(url_path)",
            "    url_base, url_ext = posixpath.splitext(url_filename)",
            "    if ext is None:",
            "        ext = url_ext",
            "    return posixpath.join(url_directory, url_base + \"@\" + suffix + ext)",
            "",
            "",
            "@contextmanager",
            "def atomic_open(filename, mode=\"r\", encoding=None):",
            "    if \"r\" not in mode:",
            "        fd, tmp_filename = tempfile.mkstemp(",
            "            dir=os.path.dirname(filename), prefix=\".__atomic-write\"",
            "        )",
            "        os.chmod(tmp_filename, 0o644)",
            "        f = os.fdopen(fd, mode)",
            "    else:",
            "        f = open(filename, mode=mode, encoding=encoding)",
            "        tmp_filename = None",
            "    try:",
            "        yield f",
            "    except Exception as e:",
            "        f.close()",
            "        _exc_type, exc_value, tb = sys.exc_info()",
            "        if tmp_filename is not None:",
            "            with suppress(OSError):",
            "                os.remove(tmp_filename)",
            "",
            "        if exc_value.__traceback__ is not tb:",
            "            raise exc_value.with_traceback(tb) from e",
            "        raise exc_value from e",
            "",
            "    f.close()",
            "    if tmp_filename is not None:",
            "        os.replace(tmp_filename, filename)",
            "",
            "",
            "def portable_popen(cmd, *args, **kwargs):",
            "    \"\"\"A portable version of subprocess.Popen that automatically locates",
            "    executables before invoking them.  This also looks for executables",
            "    in the bundle bin.",
            "    \"\"\"",
            "    if cmd[0] is None:",
            "        raise RuntimeError(\"No executable specified\")",
            "    exe = locate_executable(cmd[0], kwargs.get(\"cwd\"))",
            "    if exe is None:",
            "        raise RuntimeError('Could not locate executable \"%s\"' % cmd[0])",
            "",
            "    if isinstance(exe, str) and sys.platform != \"win32\":",
            "        exe = exe.encode(sys.getfilesystemencoding())",
            "    cmd[0] = exe",
            "    return subprocess.Popen(cmd, *args, **kwargs)",
            "",
            "",
            "def is_valid_id(value):",
            "    if value == \"\":",
            "        return True",
            "    return (",
            "        \"/\" not in value",
            "        and value.strip() == value",
            "        and value.split() == [value]",
            "        and not value.startswith(\".\")",
            "    )",
            "",
            "",
            "def secure_url(url: str) -> str:",
            "    parts = urllib.parse.urlsplit(url)",
            "    if parts.password is not None:",
            "        _, _, host_port = parts.netloc.rpartition(\"@\")",
            "        parts = parts._replace(netloc=f\"{parts.username}@{host_port}\")",
            "    return parts.geturl()",
            "",
            "",
            "def bool_from_string(val, default=None):",
            "    if val in (True, False, 1, 0):",
            "        return bool(val)",
            "    if isinstance(val, str):",
            "        val = val.lower()",
            "        if val in (\"true\", \"yes\", \"1\"):",
            "            return True",
            "        if val in (\"false\", \"no\", \"0\"):",
            "            return False",
            "    return default",
            "",
            "",
            "def make_relative_url(source, target):",
            "    \"\"\"",
            "    Returns the relative path (url) needed to navigate",
            "    from `source` to `target`.",
            "    \"\"\"",
            "",
            "    # WARNING: this logic makes some unwarranted assumptions about",
            "    # what is a directory and what isn't. Ideally, this function",
            "    # would be aware of the actual filesystem.",
            "    s_is_dir = source.endswith(\"/\")",
            "    t_is_dir = target.endswith(\"/\")",
            "",
            "    source = PurePosixPath(posixpath.normpath(source))",
            "    target = PurePosixPath(posixpath.normpath(target))",
            "",
            "    if not s_is_dir:",
            "        source = source.parent",
            "",
            "    relpath = str(get_relative_path(source, target))",
            "    if t_is_dir:",
            "        relpath += \"/\"",
            "",
            "    return relpath",
            "",
            "",
            "def get_relative_path(source, target):",
            "    \"\"\"",
            "    Returns the relative path needed to navigate from `source` to `target`.",
            "",
            "    get_relative_path(source: PurePosixPath,",
            "                      target: PurePosixPath) -> PurePosixPath",
            "    \"\"\"",
            "",
            "    if not source.is_absolute() and target.is_absolute():",
            "        raise ValueError(\"Cannot navigate from a relative path to an absolute one\")",
            "",
            "    if source.is_absolute() and not target.is_absolute():",
            "        # nothing to do",
            "        return target",
            "",
            "    if source.is_absolute() and target.is_absolute():",
            "        # convert them to relative paths to simplify the logic",
            "        source = source.relative_to(\"/\")",
            "        target = target.relative_to(\"/\")",
            "",
            "    # is the source an ancestor of the target?",
            "    try:",
            "        return target.relative_to(source)",
            "    except ValueError:",
            "        pass",
            "",
            "    # even if it isn't, one of the source's ancestors might be",
            "    # (and if not, the root will be the common ancestor)",
            "    distance = PurePosixPath(\".\")",
            "    for ancestor in source.parents:",
            "        distance /= \"..\"",
            "",
            "        try:",
            "            relpath = target.relative_to(ancestor)",
            "        except ValueError:",
            "            continue",
            "        else:",
            "            # prepend the distance to the common ancestor",
            "            return distance / relpath",
            "    # We should never get here.  (The last ancestor in source.parents will",
            "    # be '.' \u2014 target.relative_to('.') will always succeed.)",
            "    raise AssertionError(\"This should not happen\")",
            "",
            "",
            "def deg_to_dms(deg):",
            "    d = int(deg)",
            "    md = abs(deg - d) * 60",
            "    m = int(md)",
            "    sd = (md - m) * 60",
            "    return (d, m, sd)",
            "",
            "",
            "def format_lat_long(lat=None, long=None, secs=True):",
            "    def _format(value, sign):",
            "        d, m, sd = deg_to_dms(value)",
            "        return \"%d\u00b0 %d\u2032 %s%s\" % (",
            "            abs(d),",
            "            abs(m),",
            "            secs and (\"%d\u2033 \" % abs(sd)) or \"\",",
            "            sign[d < 0],",
            "        )",
            "",
            "    rv = []",
            "    if lat is not None:",
            "        rv.append(_format(lat, \"NS\"))",
            "    if long is not None:",
            "        rv.append(_format(long, \"EW\"))",
            "    return \", \".join(rv)",
            "",
            "",
            "def get_cache_dir():",
            "    if is_windows:",
            "        folder = os.environ.get(\"LOCALAPPDATA\")",
            "        if folder is None:",
            "            folder = os.environ.get(\"APPDATA\")",
            "            if folder is None:",
            "                folder = os.path.expanduser(\"~\")",
            "        return os.path.join(folder, \"Lektor\", \"Cache\")",
            "    if sys.platform == \"darwin\":",
            "        return os.path.join(os.path.expanduser(\"~/Library/Caches/Lektor\"))",
            "    return os.path.join(",
            "        os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")), \"lektor\"",
            "    )",
            "",
            "",
            "class URLBuilder:",
            "    def __init__(self):",
            "        self.items = []",
            "",
            "    def append(self, item):",
            "        if item is None:",
            "            return",
            "        item = str(item).strip(\"/\")",
            "        if item:",
            "            self.items.append(item)",
            "",
            "    def get_url(self, trailing_slash=None):",
            "        url = \"/\" + \"/\".join(self.items)",
            "        if trailing_slash is not None and not trailing_slash:",
            "            return url",
            "        if url == \"/\":",
            "            return url",
            "        if trailing_slash is None:",
            "            _, last = url.split(\"/\", 1)",
            "            if \".\" in last:",
            "                return url",
            "        return url + \"/\"",
            "",
            "",
            "def build_url(iterable, trailing_slash=None):",
            "    # NB: While this function is not used by Lektor itself, it is used",
            "    # by a number of plugins including: lektor-atom,",
            "    # lektor-gemini-capsule, lektor-index-pages, and lektor-tags.",
            "    builder = URLBuilder()",
            "    for item in iterable:",
            "        builder.append(item)",
            "    return builder.get_url(trailing_slash=trailing_slash)",
            "",
            "",
            "def comma_delimited(s):",
            "    \"\"\"Split a comma-delimited string.\"\"\"",
            "    for part in s.split(\",\"):",
            "        stripped = part.strip()",
            "        if stripped:",
            "            yield stripped",
            "",
            "",
            "def process_extra_flags(flags):",
            "    if isinstance(flags, dict):",
            "        return flags",
            "    rv = {}",
            "    for flag in flags or ():",
            "        if \":\" in flag:",
            "            k, v = flag.split(\":\", 1)",
            "            rv[k] = v",
            "        else:",
            "            rv[flag] = flag",
            "    return rv",
            "",
            "",
            "_H = TypeVar(\"_H\", bound=Hashable)",
            "",
            "",
            "def unique_everseen(seq: Iterable[_H]) -> Iterable[_H]:",
            "    \"\"\"Filter out duplicates from iterable.\"\"\"",
            "    # This is a less general version of more_itertools.unique_everseen.",
            "    # Should we need more general functionality, consider using that instead.",
            "    seen = set()",
            "    for val in seq:",
            "        if val not in seen:",
            "            seen.add(val)",
            "            yield val",
            "",
            "",
            "class RecursionCheck(threading.local):",
            "    \"\"\"A context manager that retains a count of how many times it's been entered.",
            "",
            "    Example:",
            "",
            "        >>> recursion_check = RecursionCheck()",
            "",
            "        >>> with recursion_check:",
            "        ...     assert recursion_check.level == 1",
            "        ...     with recursion_check as recursion_level:",
            "        ...         assert recursion_check.level == 2",
            "        ...         print(\"depth\", recursion_level)",
            "        ...     assert recursion_check.level == 1",
            "        ... assert recursion_check.level == 0",
            "        depth 2",
            "    \"\"\"",
            "",
            "    level = 0",
            "",
            "    def __enter__(self) -> bool:",
            "        self.level += 1",
            "        return self.level",
            "",
            "    def __exit__(self, _t, _v, _tb) -> None:",
            "        self.level -= 1",
            "",
            "",
            "class DeprecatedWarning(DeprecationWarning):",
            "    \"\"\"Warning category issued by our ``deprecated`` decorator.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        name: str,",
            "        reason: str | None = None,",
            "        version: str | None = None,",
            "    ):",
            "        self.name = name",
            "        self.reason = reason",
            "        self.version = version",
            "",
            "    def __str__(self) -> str:",
            "        message = f\"{self.name!r} is deprecated\"",
            "        if self.reason:",
            "            message += f\" ({self.reason})\"",
            "        if self.version:",
            "            message += f\" since version {self.version}\"",
            "        return message",
            "",
            "",
            "_F = TypeVar(\"_F\", bound=Callable[..., Any])",
            "",
            "",
            "@dataclass",
            "class _Deprecate:",
            "    \"\"\"A decorator to mark callables as deprecated.\"\"\"",
            "",
            "    name: str | None = None",
            "    reason: str | None = None",
            "    version: str | None = None",
            "    stacklevel: int = 1",
            "",
            "    _recursion_check: ClassVar = RecursionCheck()",
            "",
            "    def __call__(self, wrapped: _F) -> _F:",
            "        if not callable(wrapped):",
            "            raise TypeError(\"do not know how to deprecate {wrapped!r}\")",
            "",
            "        name = self.name or wrapped.__name__",
            "        message = DeprecatedWarning(name, self.reason, self.version)",
            "",
            "        @wraps(wrapped)",
            "        def wrapper(*args: Any, **kwargs: Any) -> Any:",
            "            with self._recursion_check as recursion_level:",
            "                if recursion_level == 1:",
            "                    warnings.warn(message, stacklevel=self.stacklevel + 1)",
            "                return wrapped(*args, **kwargs)",
            "",
            "        return wrapper  # type: ignore[return-value]",
            "",
            "",
            "@overload",
            "def deprecated(",
            "    __wrapped: Callable[..., Any],",
            "    *,",
            "    name: str | None = ...,",
            "    reason: str | None = ...,",
            "    version: str | None = ...,",
            "    stacklevel: int = ...,",
            ") -> Callable[..., Any]:",
            "    ...",
            "",
            "",
            "@overload",
            "def deprecated(",
            "    __reason: str,",
            "    *,",
            "    name: str | None = ...,",
            "    version: str | None = ...,",
            "    stacklevel: int = ...,",
            ") -> _Deprecate:",
            "    ...",
            "",
            "",
            "@overload",
            "def deprecated(",
            "    *,",
            "    name: str | None = ...,",
            "    reason: str | None = ...,",
            "    version: str | None = ...,",
            "    stacklevel: int = ...,",
            ") -> _Deprecate:",
            "    ...",
            "",
            "",
            "def deprecated(*args: Any, **kwargs: Any) -> _F | _Deprecate:",
            "    \"\"\"A decorator to mark callables or descriptors as deprecated.",
            "",
            "    This can be used to decorate functions, methods, classes, and descriptors.",
            "    In particular, this decorator can be applied to instances of ``property``,",
            "    ``functools.cached_property`` and ``werkzeug.utils.cached_property``.",
            "",
            "    When the decorated object is called (or \u2014 in the case of a descriptor \u2014 accessed), a",
            "    ``DeprecationWarning`` is issued.",
            "",
            "    The warning message will include the name of the decorated object, and may include",
            "    further information if provided from the ``reason`` and ``version`` arguments.",
            "",
            "    The ``name`` argument may be used to specify an alternative name to use when",
            "    generating the warning message. By default, the ``__name__`` attribute of the",
            "    decorated object is used.",
            "",
            "    The ``stacklevel`` argument controls which call in the call stack the warning",
            "    is attributed to. The default value, ``stacklevel=1`` means the warning is",
            "    reported for the immediate caller of the decorated object.  Higher values",
            "    attribute the warning callers further back in the stack.",
            "",
            "    \"\"\"",
            "    if len(args) > 1:",
            "        raise TypeError(\"deprecated accepts a maximum of one positional parameter\")",
            "",
            "    wrapped: _F | None = None",
            "    if args:",
            "        if isinstance(args[0], str):",
            "            kwargs.setdefault(\"reason\", args[0])",
            "        else:",
            "            wrapped = args[0]",
            "",
            "    deprecate = _Deprecate(**kwargs)",
            "    if wrapped is not None:",
            "        return deprecate(wrapped)",
            "    return deprecate"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import codecs",
            "import json",
            "import os",
            "import posixpath",
            "import re",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import threading",
            "import unicodedata",
            "import urllib.parse",
            "import uuid",
            "import warnings",
            "from contextlib import contextmanager",
            "from contextlib import suppress",
            "from dataclasses import dataclass",
            "from datetime import datetime",
            "from functools import lru_cache",
            "from functools import wraps",
            "from pathlib import PurePosixPath",
            "from typing import Any",
            "from typing import Callable",
            "from typing import ClassVar",
            "from typing import Hashable",
            "from typing import Iterable",
            "from typing import overload",
            "from typing import TypeVar",
            "",
            "from jinja2 import is_undefined",
            "from markupsafe import Markup",
            "from slugify import slugify as _slugify",
            "from werkzeug.http import http_date",
            "from werkzeug.urls import iri_to_uri",
            "from werkzeug.urls import uri_to_iri",
            "",
            "",
            "is_windows = os.name == \"nt\"",
            "",
            "_slash_escape = \"\\\\/\" not in json.dumps(\"/\")",
            "",
            "_last_num_re = re.compile(r\"^(.*)(\\d+)(.*?)$\")",
            "_list_marker = object()",
            "_value_marker = object()",
            "",
            "# Figure out our fs encoding, if it's ascii we upgrade to utf-8",
            "fs_enc = sys.getfilesystemencoding()",
            "try:",
            "    if codecs.lookup(fs_enc).name == \"ascii\":",
            "        fs_enc = \"utf-8\"",
            "except LookupError:",
            "    pass",
            "",
            "",
            "def split_virtual_path(path):",
            "    if \"@\" in path:",
            "        return path.split(\"@\", 1)",
            "    return path, None",
            "",
            "",
            "def _norm_join(a, b):",
            "    return posixpath.normpath(posixpath.join(a, b))",
            "",
            "",
            "def join_path(a, b):",
            "    \"\"\"Join two DB-paths.",
            "",
            "    It is assumed that both paths are already normalized in that",
            "    neither contains an extra \".\" or \"..\" components, double-slashes,",
            "    etc.",
            "    \"\"\"",
            "    # NB: This function is really only during URL resolution.  The only",
            "    # place that references it is lektor.source.SourceObject._resolve_url.",
            "",
            "    if posixpath.isabs(b):",
            "        return b",
            "",
            "    a_p, a_v = split_virtual_path(a)",
            "    b_p, b_v = split_virtual_path(b)",
            "",
            "    # Special case: paginations are considered special virtual paths",
            "    # where the parent is the actual parent of the page.  This however",
            "    # is explicitly not done if the path we join with refers to the",
            "    # current path (empty string or dot).",
            "    if b_p not in (\"\", \".\") and a_v and a_v.isdigit():",
            "        a_v = None",
            "",
            "    # New path has a virtual path, add that to it.",
            "    if b_v:",
            "        rv = _norm_join(a_p, b_p) + \"@\" + b_v",
            "    elif a_v:",
            "        rv = a_p + \"@\" + _norm_join(a_v, b_p)",
            "    else:",
            "        rv = _norm_join(a_p, b_p)",
            "    if rv[-2:] == \"@.\":",
            "        rv = rv[:-2]",
            "    return rv",
            "",
            "",
            "def cleanup_path(path):",
            "    # NB: POSIX allows for two leading slashes in a pathname, so we have to",
            "    # deal with the possiblity of leading double-slash ourself.",
            "    return posixpath.normpath(\"/\" + path.lstrip(\"/\"))",
            "",
            "",
            "def cleanup_url_path(url_path):",
            "    \"\"\"Clean up a URL path.",
            "",
            "    This strips any query, and/or fragment that may be present in the",
            "    input path.",
            "",
            "    Raises ValueError if the path contains a _scheme_",
            "    which is neither ``http`` nor ``https``, or a _netloc_.",
            "    \"\"\"",
            "    scheme, netloc, path, _, _ = urllib.parse.urlsplit(url_path, scheme=\"http\")",
            "    if scheme not in (\"http\", \"https\"):",
            "        raise ValueError(f\"Invalid scheme: {url_path!r}\")",
            "    if netloc:",
            "        raise ValueError(f\"Invalid netloc: {url_path!r}\")",
            "",
            "    # NB: POSIX allows for two leading slashes in a pathname, so we have to",
            "    # deal with the possiblity of leading double-slash ourself.",
            "    return posixpath.normpath(\"/\" + path.lstrip(\"/\"))",
            "",
            "",
            "def parse_path(path):",
            "    x = cleanup_path(path).strip(\"/\").split(\"/\")",
            "    if x == [\"\"]:",
            "        return []",
            "    return x",
            "",
            "",
            "def is_path_child_of(a, b, strict=True):",
            "    a_p, a_v = split_virtual_path(a)",
            "    b_p, b_v = split_virtual_path(b)",
            "    a_p = parse_path(a_p)",
            "    b_p = parse_path(b_p)",
            "    a_v = parse_path(a_v or \"\")",
            "    b_v = parse_path(b_v or \"\")",
            "",
            "    if not strict and a_p == b_p and a_v == b_v:",
            "        return True",
            "    if not a_v and b_v:",
            "        return False",
            "    if a_p == b_p and a_v[: len(b_v)] == b_v and len(a_v) > len(b_v):",
            "        return True",
            "    return a_p[: len(b_p)] == b_p and len(a_p) > len(b_p)",
            "",
            "",
            "def untrusted_to_os_path(path):",
            "    if not isinstance(path, str):",
            "        path = path.decode(fs_enc, \"replace\")",
            "    clean_path = cleanup_path(path)",
            "    assert clean_path.startswith(\"/\")",
            "    return clean_path[1:].replace(\"/\", os.path.sep)",
            "",
            "",
            "def is_path(path):",
            "    return os.path.sep in path or (os.path.altsep and os.path.altsep in path)",
            "",
            "",
            "def magic_split_ext(filename, ext_check=True):",
            "    \"\"\"Splits a filename into base and extension.  If ext check is enabled",
            "    (which is the default) then it verifies the extension is at least",
            "    reasonable.",
            "    \"\"\"",
            "",
            "    def bad_ext(ext):",
            "        if not ext_check:",
            "            return False",
            "        if not ext or ext.split() != [ext] or ext.strip() != ext:",
            "            return True",
            "        return False",
            "",
            "    parts = filename.rsplit(\".\", 2)",
            "    if len(parts) == 1:",
            "        return parts[0], \"\"",
            "    if len(parts) == 2 and not parts[0]:",
            "        return \".\" + parts[1], \"\"",
            "    if len(parts) == 3 and len(parts[1]) < 5:",
            "        ext = \".\".join(parts[1:])",
            "        if not bad_ext(ext):",
            "            return parts[0], ext",
            "    ext = parts[-1]",
            "    if bad_ext(ext):",
            "        return filename, \"\"",
            "    basename = \".\".join(parts[:-1])",
            "    return basename, ext",
            "",
            "",
            "def iter_dotted_path_prefixes(dotted_path):",
            "    pieces = dotted_path.split(\".\")",
            "    if len(pieces) == 1:",
            "        yield dotted_path, None",
            "    else:",
            "        for x in range(1, len(pieces)):",
            "            yield \".\".join(pieces[:x]), \".\".join(pieces[x:])",
            "",
            "",
            "def resolve_dotted_value(obj, dotted_path):",
            "    node = obj",
            "    for key in dotted_path.split(\".\"):",
            "        if isinstance(node, dict):",
            "            new_node = node.get(key)",
            "            if new_node is None and key.isdigit():",
            "                new_node = node.get(int(key))",
            "        elif isinstance(node, list):",
            "            try:",
            "                new_node = node[int(key)]",
            "            except (ValueError, TypeError, IndexError):",
            "                new_node = None",
            "        else:",
            "            new_node = None",
            "        node = new_node",
            "        if node is None:",
            "            break",
            "    return node",
            "",
            "",
            "def decode_flat_data(itemiter, dict_cls=dict):",
            "    def _split_key(name):",
            "        result = name.split(\".\")",
            "        for idx, part in enumerate(result):",
            "            if part.isdigit():",
            "                result[idx] = int(part)",
            "        return result",
            "",
            "    def _enter_container(container, key):",
            "        if key not in container:",
            "            return container.setdefault(key, dict_cls())",
            "        return container[key]",
            "",
            "    def _convert(container):",
            "        if _value_marker in container:",
            "            force_list = False",
            "            values = container.pop(_value_marker)",
            "            if container.pop(_list_marker, False):",
            "                force_list = True",
            "                values.extend(_convert(x[1]) for x in sorted(container.items()))",
            "            if not force_list and len(values) == 1:",
            "                values = values[0]",
            "",
            "            if not container:",
            "                return values",
            "            return _convert(container)",
            "        if container.pop(_list_marker, False):",
            "            return [_convert(x[1]) for x in sorted(container.items())]",
            "        return dict_cls((k, _convert(v)) for k, v in container.items())",
            "",
            "    result = dict_cls()",
            "",
            "    for key, value in itemiter:",
            "        parts = _split_key(key)",
            "        if not parts:",
            "            continue",
            "        container = result",
            "        for part in parts:",
            "            last_container = container",
            "            container = _enter_container(container, part)",
            "            last_container[_list_marker] = isinstance(part, int)",
            "        container[_value_marker] = [value]",
            "",
            "    return _convert(result)",
            "",
            "",
            "def merge(a, b):",
            "    \"\"\"Merges two values together.\"\"\"",
            "    if b is None and a is not None:",
            "        return a",
            "    if a is None:",
            "        return b",
            "    if isinstance(a, list) and isinstance(b, list):",
            "        for idx, (item_1, item_2) in enumerate(zip(a, b)):",
            "            a[idx] = merge(item_1, item_2)",
            "    if isinstance(a, dict) and isinstance(b, dict):",
            "        for key, value in b.items():",
            "            a[key] = merge(a.get(key), value)",
            "        return a",
            "    return a",
            "",
            "",
            "def slugify(text):",
            "    \"\"\"",
            "    A wrapper around python-slugify which preserves file extensions",
            "    and forward slashes.",
            "    \"\"\"",
            "",
            "    parts = text.split(\"/\")",
            "    parts[-1], ext = magic_split_ext(parts[-1])",
            "",
            "    out = \"/\".join(_slugify(part) for part in parts)",
            "",
            "    if ext:",
            "        return out + \".\" + ext",
            "    return out",
            "",
            "",
            "def secure_filename(filename, fallback_name=\"file\"):",
            "    base = filename.replace(\"/\", \" \").replace(\"\\\\\", \" \")",
            "    basename, ext = magic_split_ext(base)",
            "    rv = slugify(basename).lstrip(\".\")",
            "    if not rv:",
            "        rv = fallback_name",
            "    if ext:",
            "        return rv + \".\" + ext",
            "    return rv",
            "",
            "",
            "def increment_filename(filename):",
            "    directory, filename = os.path.split(filename)",
            "    basename, ext = magic_split_ext(filename, ext_check=False)",
            "",
            "    match = _last_num_re.match(basename)",
            "    if match is not None:",
            "        rv = match.group(1) + str(int(match.group(2)) + 1) + match.group(3)",
            "    else:",
            "        rv = basename + \"2\"",
            "",
            "    if ext:",
            "        rv += \".\" + ext",
            "    if directory:",
            "        return os.path.join(directory, rv)",
            "    return rv",
            "",
            "",
            "@lru_cache(maxsize=None)",
            "def locate_executable(exe_file, cwd=None, include_bundle_path=True):",
            "    \"\"\"Locates an executable in the search path.\"\"\"",
            "    choices = [exe_file]",
            "    resolve = True",
            "",
            "    # If it's already a path, we don't resolve.",
            "    if os.path.sep in exe_file or (os.path.altsep and os.path.altsep in exe_file):",
            "        resolve = False",
            "",
            "    extensions = os.environ.get(\"PATHEXT\", \"\").split(\";\")",
            "    _, ext = os.path.splitext(exe_file)",
            "    if (",
            "        os.name != \"nt\"",
            "        and \"\" not in extensions",
            "        or any(ext.lower() == extension.lower() for extension in extensions)",
            "    ):",
            "        extensions.insert(0, \"\")",
            "",
            "    if resolve:",
            "        paths = os.environ.get(\"PATH\", \"\").split(os.pathsep)",
            "        choices = [os.path.join(path, exe_file) for path in paths]",
            "",
            "    if os.name == \"nt\":",
            "        choices.append(os.path.join((cwd or os.getcwd()), exe_file))",
            "",
            "    try:",
            "        for path in choices:",
            "            for ext in extensions:",
            "                if os.access(path + ext, os.X_OK):",
            "                    return path + ext",
            "        return None",
            "    except OSError:",
            "        return None",
            "",
            "",
            "class JSONEncoder(json.JSONEncoder):",
            "    def default(self, o):  # pylint: disable=method-hidden",
            "        if is_undefined(o):",
            "            return None",
            "        if isinstance(o, datetime):",
            "            return http_date(o)",
            "        if isinstance(o, uuid.UUID):",
            "            return str(o)",
            "        if hasattr(o, \"__html__\"):",
            "            return str(o.__html__())",
            "        return json.JSONEncoder.default(self, o)",
            "",
            "",
            "def htmlsafe_json_dump(obj, **kwargs):",
            "    kwargs.setdefault(\"cls\", JSONEncoder)",
            "    rv = (",
            "        json.dumps(obj, **kwargs)",
            "        .replace(\"<\", \"\\\\u003c\")",
            "        .replace(\">\", \"\\\\u003e\")",
            "        .replace(\"&\", \"\\\\u0026\")",
            "        .replace(\"'\", \"\\\\u0027\")",
            "    )",
            "    if not _slash_escape:",
            "        rv = rv.replace(\"\\\\/\", \"/\")",
            "    return rv",
            "",
            "",
            "def tojson_filter(obj, **kwargs):",
            "    return Markup(htmlsafe_json_dump(obj, **kwargs))",
            "",
            "",
            "class Url(urllib.parse.SplitResult):",
            "    \"\"\"Make various parts of a URL accessible.",
            "",
            "    This is the type of the values exposed by Lektor record fields of type \"url\".",
            "",
            "    Since Lektor 3.4.0, this is essentially a `urllib.parse.SplitResult` as obtained by",
            "    calling `urlsplit` on the URL normalized to an IRI.",
            "",
            "    Generally, attributes such as ``netloc``, ``host``, ``path``, ``query``, and",
            "    ``fragment`` return the IRI (internationalied) versions of those components.",
            "",
            "    The URI (ASCII-encoded) version of the URL is available from the `ascii_url`",
            "    attribute.",
            "",
            "    NB: Changed in 3.4.0: The ``query`` attribute used to return the URI",
            "    (ASCII-encoded) version of the query \u2014 I'm not sure why. Now it returns",
            "    the IRI (internationalized) version of the query.",
            "",
            "    \"\"\"",
            "",
            "    def __new__(cls, value: str):",
            "        # XXX: deprecate use of constructor so that eventually we can make its signature",
            "        # match that of the SplitResult base class.",
            "        warnings.warn(",
            "            DeprecatedWarning(",
            "                \"Url\",",
            "                reason=(",
            "                    \"Direct construction of a Url instance is deprecated. \"",
            "                    \"Use the Url.from_string classmethod instead.\"",
            "                ),",
            "                version=\"3.4.0\",",
            "            ),",
            "            stacklevel=2,",
            "        )",
            "        return cls.from_string(value)",
            "",
            "    @classmethod",
            "    def from_string(cls, value: str) -> Url:",
            "        \"\"\"Construct instance from URL string.",
            "",
            "        The input URL can be a URI (all ASCII) or an IRI (internationalized).",
            "        \"\"\"",
            "        # The iri_to_uri operation is nominally idempotent \u2014 it can be passed either an",
            "        # IRI or a URI (or something inbetween) and will return a URI.  So to fully",
            "        # normalize input which can be either an IRI or a URI, first convert to URI,",
            "        # then to IRI.",
            "        iri = uri_to_iri(iri_to_uri(value))",
            "        obj = cls._make(urllib.parse.urlsplit(iri))",
            "        obj.url = value",
            "        return obj",
            "",
            "    def __str__(self) -> str:",
            "        \"\"\"The original un-normalized URL string.\"\"\"",
            "        return self.url",
            "",
            "    @property",
            "    def ascii_url(self) -> str:",
            "        \"\"\"The URL encoded to an all-ASCII URI.\"\"\"",
            "        return iri_to_uri(self.geturl())",
            "",
            "    @property",
            "    def ascii_host(self) -> str | None:",
            "        \"\"\"The hostname part of the URL IDNA-encoded to ASCII.\"\"\"",
            "        return urllib.parse.urlsplit(self.ascii_url).hostname",
            "",
            "    @property",
            "    def host(self) -> str | None:",
            "        \"\"\"The IRI (internationalized) version of the hostname.",
            "",
            "        This attribute is provided for backwards-compatibility.  New code should use the",
            "        ``hostname`` attribute instead.",
            "        \"\"\"",
            "        return self.hostname",
            "",
            "    @property",
            "    def anchor(self) -> str:",
            "        \"\"\"The IRI (internationalized) version of the \"anchor\" part of the URL.",
            "",
            "        This attribute is provided for backwards-compatibility.  New code should use the",
            "        ``fragment`` attribute instead.",
            "        \"\"\"",
            "        return self.fragment",
            "",
            "",
            "def is_unsafe_to_delete(path, base):",
            "    a = os.path.abspath(path)",
            "    b = os.path.abspath(base)",
            "    diff = os.path.relpath(a, b)",
            "    first = diff.split(os.path.sep)[0]",
            "    return first in (os.path.curdir, os.path.pardir)",
            "",
            "",
            "def prune_file_and_folder(name, base):",
            "    if is_unsafe_to_delete(name, base):",
            "        return False",
            "    try:",
            "        os.remove(name)",
            "    except OSError:",
            "        try:",
            "            os.rmdir(name)",
            "        except OSError:",
            "            return False",
            "    head, tail = os.path.split(name)",
            "    if not tail:",
            "        head, tail = os.path.split(head)",
            "    while head and tail:",
            "        try:",
            "            if is_unsafe_to_delete(head, base):",
            "                return False",
            "            os.rmdir(head)",
            "        except OSError:",
            "            break",
            "        head, tail = os.path.split(head)",
            "    return True",
            "",
            "",
            "def sort_normalize_string(s):",
            "    return unicodedata.normalize(\"NFD\", str(s).lower().strip())",
            "",
            "",
            "def get_dependent_url(url_path, suffix, ext=None):",
            "    url_directory, url_filename = posixpath.split(url_path)",
            "    url_base, url_ext = posixpath.splitext(url_filename)",
            "    if ext is None:",
            "        ext = url_ext",
            "    return posixpath.join(url_directory, url_base + \"@\" + suffix + ext)",
            "",
            "",
            "@contextmanager",
            "def atomic_open(filename, mode=\"r\", encoding=None):",
            "    if \"r\" not in mode:",
            "        fd, tmp_filename = tempfile.mkstemp(",
            "            dir=os.path.dirname(filename), prefix=\".__atomic-write\"",
            "        )",
            "        os.chmod(tmp_filename, 0o644)",
            "        f = os.fdopen(fd, mode)",
            "    else:",
            "        f = open(filename, mode=mode, encoding=encoding)",
            "        tmp_filename = None",
            "    try:",
            "        yield f",
            "    except Exception as e:",
            "        f.close()",
            "        _exc_type, exc_value, tb = sys.exc_info()",
            "        if tmp_filename is not None:",
            "            with suppress(OSError):",
            "                os.remove(tmp_filename)",
            "",
            "        if exc_value.__traceback__ is not tb:",
            "            raise exc_value.with_traceback(tb) from e",
            "        raise exc_value from e",
            "",
            "    f.close()",
            "    if tmp_filename is not None:",
            "        os.replace(tmp_filename, filename)",
            "",
            "",
            "def portable_popen(cmd, *args, **kwargs):",
            "    \"\"\"A portable version of subprocess.Popen that automatically locates",
            "    executables before invoking them.  This also looks for executables",
            "    in the bundle bin.",
            "    \"\"\"",
            "    if cmd[0] is None:",
            "        raise RuntimeError(\"No executable specified\")",
            "    exe = locate_executable(cmd[0], kwargs.get(\"cwd\"))",
            "    if exe is None:",
            "        raise RuntimeError('Could not locate executable \"%s\"' % cmd[0])",
            "",
            "    if isinstance(exe, str) and sys.platform != \"win32\":",
            "        exe = exe.encode(sys.getfilesystemencoding())",
            "    cmd[0] = exe",
            "    return subprocess.Popen(cmd, *args, **kwargs)",
            "",
            "",
            "def is_valid_id(value):",
            "    if value == \"\":",
            "        return True",
            "    return (",
            "        \"/\" not in value",
            "        and value.strip() == value",
            "        and value.split() == [value]",
            "        and not value.startswith(\".\")",
            "    )",
            "",
            "",
            "def secure_url(url: str) -> str:",
            "    parts = urllib.parse.urlsplit(url)",
            "    if parts.password is not None:",
            "        _, _, host_port = parts.netloc.rpartition(\"@\")",
            "        parts = parts._replace(netloc=f\"{parts.username}@{host_port}\")",
            "    return parts.geturl()",
            "",
            "",
            "def bool_from_string(val, default=None):",
            "    if val in (True, False, 1, 0):",
            "        return bool(val)",
            "    if isinstance(val, str):",
            "        val = val.lower()",
            "        if val in (\"true\", \"yes\", \"1\"):",
            "            return True",
            "        if val in (\"false\", \"no\", \"0\"):",
            "            return False",
            "    return default",
            "",
            "",
            "def make_relative_url(source, target):",
            "    \"\"\"",
            "    Returns the relative path (url) needed to navigate",
            "    from `source` to `target`.",
            "    \"\"\"",
            "",
            "    # WARNING: this logic makes some unwarranted assumptions about",
            "    # what is a directory and what isn't. Ideally, this function",
            "    # would be aware of the actual filesystem.",
            "    s_is_dir = source.endswith(\"/\")",
            "    t_is_dir = target.endswith(\"/\")",
            "",
            "    source = PurePosixPath(posixpath.normpath(source))",
            "    target = PurePosixPath(posixpath.normpath(target))",
            "",
            "    if not s_is_dir:",
            "        source = source.parent",
            "",
            "    relpath = str(get_relative_path(source, target))",
            "    if t_is_dir:",
            "        relpath += \"/\"",
            "",
            "    return relpath",
            "",
            "",
            "def get_relative_path(source, target):",
            "    \"\"\"",
            "    Returns the relative path needed to navigate from `source` to `target`.",
            "",
            "    get_relative_path(source: PurePosixPath,",
            "                      target: PurePosixPath) -> PurePosixPath",
            "    \"\"\"",
            "",
            "    if not source.is_absolute() and target.is_absolute():",
            "        raise ValueError(\"Cannot navigate from a relative path to an absolute one\")",
            "",
            "    if source.is_absolute() and not target.is_absolute():",
            "        # nothing to do",
            "        return target",
            "",
            "    if source.is_absolute() and target.is_absolute():",
            "        # convert them to relative paths to simplify the logic",
            "        source = source.relative_to(\"/\")",
            "        target = target.relative_to(\"/\")",
            "",
            "    # is the source an ancestor of the target?",
            "    try:",
            "        return target.relative_to(source)",
            "    except ValueError:",
            "        pass",
            "",
            "    # even if it isn't, one of the source's ancestors might be",
            "    # (and if not, the root will be the common ancestor)",
            "    distance = PurePosixPath(\".\")",
            "    for ancestor in source.parents:",
            "        distance /= \"..\"",
            "",
            "        try:",
            "            relpath = target.relative_to(ancestor)",
            "        except ValueError:",
            "            continue",
            "        else:",
            "            # prepend the distance to the common ancestor",
            "            return distance / relpath",
            "    # We should never get here.  (The last ancestor in source.parents will",
            "    # be '.' \u2014 target.relative_to('.') will always succeed.)",
            "    raise AssertionError(\"This should not happen\")",
            "",
            "",
            "def deg_to_dms(deg):",
            "    d = int(deg)",
            "    md = abs(deg - d) * 60",
            "    m = int(md)",
            "    sd = (md - m) * 60",
            "    return (d, m, sd)",
            "",
            "",
            "def format_lat_long(lat=None, long=None, secs=True):",
            "    def _format(value, sign):",
            "        d, m, sd = deg_to_dms(value)",
            "        return \"%d\u00b0 %d\u2032 %s%s\" % (",
            "            abs(d),",
            "            abs(m),",
            "            secs and (\"%d\u2033 \" % abs(sd)) or \"\",",
            "            sign[d < 0],",
            "        )",
            "",
            "    rv = []",
            "    if lat is not None:",
            "        rv.append(_format(lat, \"NS\"))",
            "    if long is not None:",
            "        rv.append(_format(long, \"EW\"))",
            "    return \", \".join(rv)",
            "",
            "",
            "def get_cache_dir():",
            "    if is_windows:",
            "        folder = os.environ.get(\"LOCALAPPDATA\")",
            "        if folder is None:",
            "            folder = os.environ.get(\"APPDATA\")",
            "            if folder is None:",
            "                folder = os.path.expanduser(\"~\")",
            "        return os.path.join(folder, \"Lektor\", \"Cache\")",
            "    if sys.platform == \"darwin\":",
            "        return os.path.join(os.path.expanduser(\"~/Library/Caches/Lektor\"))",
            "    return os.path.join(",
            "        os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")), \"lektor\"",
            "    )",
            "",
            "",
            "class URLBuilder:",
            "    def __init__(self):",
            "        self.items = []",
            "",
            "    def append(self, item):",
            "        if item is None:",
            "            return",
            "        item = str(item).strip(\"/\")",
            "        if item:",
            "            self.items.append(item)",
            "",
            "    def get_url(self, trailing_slash=None):",
            "        url = \"/\" + \"/\".join(self.items)",
            "        if trailing_slash is not None and not trailing_slash:",
            "            return url",
            "        if url == \"/\":",
            "            return url",
            "        if trailing_slash is None:",
            "            _, last = url.split(\"/\", 1)",
            "            if \".\" in last:",
            "                return url",
            "        return url + \"/\"",
            "",
            "",
            "def build_url(iterable, trailing_slash=None):",
            "    # NB: While this function is not used by Lektor itself, it is used",
            "    # by a number of plugins including: lektor-atom,",
            "    # lektor-gemini-capsule, lektor-index-pages, and lektor-tags.",
            "    builder = URLBuilder()",
            "    for item in iterable:",
            "        builder.append(item)",
            "    return builder.get_url(trailing_slash=trailing_slash)",
            "",
            "",
            "def comma_delimited(s):",
            "    \"\"\"Split a comma-delimited string.\"\"\"",
            "    for part in s.split(\",\"):",
            "        stripped = part.strip()",
            "        if stripped:",
            "            yield stripped",
            "",
            "",
            "def process_extra_flags(flags):",
            "    if isinstance(flags, dict):",
            "        return flags",
            "    rv = {}",
            "    for flag in flags or ():",
            "        if \":\" in flag:",
            "            k, v = flag.split(\":\", 1)",
            "            rv[k] = v",
            "        else:",
            "            rv[flag] = flag",
            "    return rv",
            "",
            "",
            "_H = TypeVar(\"_H\", bound=Hashable)",
            "",
            "",
            "def unique_everseen(seq: Iterable[_H]) -> Iterable[_H]:",
            "    \"\"\"Filter out duplicates from iterable.\"\"\"",
            "    # This is a less general version of more_itertools.unique_everseen.",
            "    # Should we need more general functionality, consider using that instead.",
            "    seen = set()",
            "    for val in seq:",
            "        if val not in seen:",
            "            seen.add(val)",
            "            yield val",
            "",
            "",
            "class RecursionCheck(threading.local):",
            "    \"\"\"A context manager that retains a count of how many times it's been entered.",
            "",
            "    Example:",
            "",
            "        >>> recursion_check = RecursionCheck()",
            "",
            "        >>> with recursion_check:",
            "        ...     assert recursion_check.level == 1",
            "        ...     with recursion_check as recursion_level:",
            "        ...         assert recursion_check.level == 2",
            "        ...         print(\"depth\", recursion_level)",
            "        ...     assert recursion_check.level == 1",
            "        ... assert recursion_check.level == 0",
            "        depth 2",
            "    \"\"\"",
            "",
            "    level = 0",
            "",
            "    def __enter__(self) -> bool:",
            "        self.level += 1",
            "        return self.level",
            "",
            "    def __exit__(self, _t, _v, _tb) -> None:",
            "        self.level -= 1",
            "",
            "",
            "class DeprecatedWarning(DeprecationWarning):",
            "    \"\"\"Warning category issued by our ``deprecated`` decorator.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        name: str,",
            "        reason: str | None = None,",
            "        version: str | None = None,",
            "    ):",
            "        self.name = name",
            "        self.reason = reason",
            "        self.version = version",
            "",
            "    def __str__(self) -> str:",
            "        message = f\"{self.name!r} is deprecated\"",
            "        if self.reason:",
            "            message += f\" ({self.reason})\"",
            "        if self.version:",
            "            message += f\" since version {self.version}\"",
            "        return message",
            "",
            "",
            "_F = TypeVar(\"_F\", bound=Callable[..., Any])",
            "",
            "",
            "@dataclass",
            "class _Deprecate:",
            "    \"\"\"A decorator to mark callables as deprecated.\"\"\"",
            "",
            "    name: str | None = None",
            "    reason: str | None = None",
            "    version: str | None = None",
            "    stacklevel: int = 1",
            "",
            "    _recursion_check: ClassVar = RecursionCheck()",
            "",
            "    def __call__(self, wrapped: _F) -> _F:",
            "        if not callable(wrapped):",
            "            raise TypeError(\"do not know how to deprecate {wrapped!r}\")",
            "",
            "        name = self.name or wrapped.__name__",
            "        message = DeprecatedWarning(name, self.reason, self.version)",
            "",
            "        @wraps(wrapped)",
            "        def wrapper(*args: Any, **kwargs: Any) -> Any:",
            "            with self._recursion_check as recursion_level:",
            "                if recursion_level == 1:",
            "                    warnings.warn(message, stacklevel=self.stacklevel + 1)",
            "                return wrapped(*args, **kwargs)",
            "",
            "        return wrapper  # type: ignore[return-value]",
            "",
            "",
            "@overload",
            "def deprecated(",
            "    __wrapped: Callable[..., Any],",
            "    *,",
            "    name: str | None = ...,",
            "    reason: str | None = ...,",
            "    version: str | None = ...,",
            "    stacklevel: int = ...,",
            ") -> Callable[..., Any]:",
            "    ...",
            "",
            "",
            "@overload",
            "def deprecated(",
            "    __reason: str,",
            "    *,",
            "    name: str | None = ...,",
            "    version: str | None = ...,",
            "    stacklevel: int = ...,",
            ") -> _Deprecate:",
            "    ...",
            "",
            "",
            "@overload",
            "def deprecated(",
            "    *,",
            "    name: str | None = ...,",
            "    reason: str | None = ...,",
            "    version: str | None = ...,",
            "    stacklevel: int = ...,",
            ") -> _Deprecate:",
            "    ...",
            "",
            "",
            "def deprecated(*args: Any, **kwargs: Any) -> _F | _Deprecate:",
            "    \"\"\"A decorator to mark callables or descriptors as deprecated.",
            "",
            "    This can be used to decorate functions, methods, classes, and descriptors.",
            "    In particular, this decorator can be applied to instances of ``property``,",
            "    ``functools.cached_property`` and ``werkzeug.utils.cached_property``.",
            "",
            "    When the decorated object is called (or \u2014 in the case of a descriptor \u2014 accessed), a",
            "    ``DeprecationWarning`` is issued.",
            "",
            "    The warning message will include the name of the decorated object, and may include",
            "    further information if provided from the ``reason`` and ``version`` arguments.",
            "",
            "    The ``name`` argument may be used to specify an alternative name to use when",
            "    generating the warning message. By default, the ``__name__`` attribute of the",
            "    decorated object is used.",
            "",
            "    The ``stacklevel`` argument controls which call in the call stack the warning",
            "    is attributed to. The default value, ``stacklevel=1`` means the warning is",
            "    reported for the immediate caller of the decorated object.  Higher values",
            "    attribute the warning callers further back in the stack.",
            "",
            "    \"\"\"",
            "    if len(args) > 1:",
            "        raise TypeError(\"deprecated accepts a maximum of one positional parameter\")",
            "",
            "    wrapped: _F | None = None",
            "    if args:",
            "        if isinstance(args[0], str):",
            "            kwargs.setdefault(\"reason\", args[0])",
            "        else:",
            "            wrapped = args[0]",
            "",
            "    deprecate = _Deprecate(**kwargs)",
            "    if wrapped is not None:",
            "        return deprecate(wrapped)",
            "    return deprecate"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "152": [
                "untrusted_to_os_path"
            ],
            "155": [
                "untrusted_to_os_path"
            ]
        },
        "addLocation": []
    }
}