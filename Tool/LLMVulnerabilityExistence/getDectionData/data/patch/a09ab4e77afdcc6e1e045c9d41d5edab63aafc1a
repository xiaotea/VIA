{
    "tensorflow/python/keras/engine/functional.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "   than with subclassed `Model`s, specifically:"
            },
            "1": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "   - Model cloning (`keras.models.clone`)"
            },
            "3": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  - Serialization (`model.get_config()/from_config`, `model.to_json()/to_yaml()`"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+  - Serialization (`model.get_config()/from_config`, `model.to_json()`"
            },
            "5": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "   - Whole-model saving (`model.save()`)"
            },
            "6": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 58,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "   A `Functional` model can be instantiated by passing two arguments to"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "# pylint: disable=protected-access",
            "\"\"\"A `Network` is way to compose layers: the topological form of a `Model`.\"\"\"",
            "",
            "import collections",
            "import copy",
            "import itertools",
            "import warnings",
            "",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.keras import backend",
            "from tensorflow.python.keras.engine import base_layer",
            "from tensorflow.python.keras.engine import base_layer_utils",
            "from tensorflow.python.keras.engine import input_layer as input_layer_module",
            "from tensorflow.python.keras.engine import input_spec",
            "from tensorflow.python.keras.engine import node as node_module",
            "from tensorflow.python.keras.engine import training as training_lib",
            "from tensorflow.python.keras.engine import training_utils",
            "from tensorflow.python.keras.saving.saved_model import network_serialization",
            "from tensorflow.python.keras.utils import generic_utils",
            "from tensorflow.python.keras.utils import tf_inspect",
            "from tensorflow.python.keras.utils import tf_utils",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.training.tracking import base as trackable",
            "from tensorflow.python.util import nest",
            "from tensorflow.tools.docs import doc_controls",
            "",
            "",
            "# pylint: disable=g-classes-have-attributes",
            "class Functional(training_lib.Model):",
            "  \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.",
            "",
            "  Three types of `Model` exist: subclassed `Model`, `Functional` model,",
            "  and `Sequential` (a special case of `Functional`).",
            "  In general, more Keras features are supported with `Functional`",
            "  than with subclassed `Model`s, specifically:",
            "",
            "  - Model cloning (`keras.models.clone`)",
            "  - Serialization (`model.get_config()/from_config`, `model.to_json()/to_yaml()`",
            "  - Whole-model saving (`model.save()`)",
            "",
            "  A `Functional` model can be instantiated by passing two arguments to",
            "  `__init__`. The first argument is the `keras.Input` Tensors that represent",
            "  the inputs to the model. The second argument specifies the output",
            "  tensors that represent the outputs of this model. Both arguments can be a",
            "  nested structure of tensors.",
            "",
            "  Example:",
            "",
            "  ```",
            "  inputs = {'x1': keras.Input(shape=(10,)), 'x2': keras.Input(shape=(1,))}",
            "  t = keras.layers.Dense(1, activation='relu')(inputs['x1'])",
            "  outputs = keras.layers.Add()([t, inputs['x2'])",
            "  model = keras.Model(inputs, outputs)",
            "  ```",
            "",
            "  A `Functional` model constructed using the Functional API can also include raw",
            "  TensorFlow functions, with the exception of functions that create Variables",
            "  or assign ops.",
            "",
            "  Example:",
            "",
            "  ```",
            "  inputs = keras.Input(shape=(10,))",
            "  x = keras.layers.Dense(1)(inputs)",
            "  outputs = tf.nn.relu(x)",
            "  model = keras.Model(inputs, outputs)",
            "  ```",
            "",
            "  Args:",
            "    inputs: List of input tensors (must be created via `tf.keras.Input()`).",
            "    outputs: List of output tensors.",
            "    name: String, optional. Name of the model.",
            "    trainable: Boolean, optional. If the model's variables should be trainable.",
            "  \"\"\"",
            "",
            "  # See tf.Module for the usage of this property.",
            "  # The key of _layer_call_argspecs is a layer. tf.Module._flatten will fail to",
            "  # flatten the key since it is trying to convert Trackable/Layer to a string.",
            "  _TF_MODULE_IGNORED_PROPERTIES = frozenset(itertools.chain(",
            "      ('_layer_call_argspecs', '_compiled_trainable_state',",
            "       '_output_mask_cache', '_output_tensor_cache', '_output_shape_cache'),",
            "      training_lib.Model._TF_MODULE_IGNORED_PROPERTIES",
            "  ))",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def __init__(self, inputs, outputs, name=None, trainable=True,",
            "               **kwargs):",
            "    # This is used by the Model class, since we have some logic to swap the",
            "    # class in the __new__ method, which will lead to __init__ get invoked",
            "    # twice. Using the skip_init to skip one of the invocation of __init__ to",
            "    # avoid any side effects",
            "    skip_init = kwargs.pop('skip_init', False)",
            "    if skip_init:",
            "      return",
            "    generic_utils.validate_kwargs(kwargs, {})",
            "    super(Functional, self).__init__(name=name, trainable=trainable)",
            "    self._init_graph_network(inputs, outputs)",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _init_graph_network(self, inputs, outputs):",
            "    base_layer.keras_api_gauge.get_cell('Functional').set(True)",
            "    # This method is needed for Sequential to reinitialize graph network when",
            "    # layer is added or removed.",
            "    self._is_graph_network = True",
            "",
            "    # Normalize and set self.inputs, self.outputs.",
            "    if isinstance(inputs, list) and len(nest.flatten(inputs)) == 1:",
            "      inputs = inputs[0]",
            "    if isinstance(outputs, list) and len(nest.flatten(outputs)) == 1:",
            "      outputs = outputs[0]",
            "    self._nested_inputs = inputs",
            "    self._nested_outputs = outputs",
            "    self.inputs = nest.flatten(inputs)",
            "    self.outputs = nest.flatten(outputs)",
            "",
            "    # Models constructed with a single Tensor or list of Tensors can",
            "    # be called with a dict, where the keys of the dict are the names",
            "    # of the `Input` objects. Extra keys are ignored with warning.",
            "    if not nest.is_nested(self._nested_inputs):",
            "      self._enable_dict_to_input_mapping = True",
            "    elif (isinstance(self._nested_inputs, (list, tuple)) and",
            "          not any(nest.is_nested(t) for t in self._nested_inputs)):",
            "      self._enable_dict_to_input_mapping = True",
            "    elif (isinstance(self._nested_inputs, dict) and",
            "          not any(nest.is_nested(t) for t in self._nested_inputs.values())):",
            "      self._enable_dict_to_input_mapping = True",
            "    else:",
            "      self._enable_dict_to_input_mapping = False",
            "",
            "    if not ops.executing_eagerly_outside_functions():",
            "      if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):",
            "        base_layer_utils.create_keras_history(self._nested_outputs)",
            "",
            "    self._validate_graph_inputs_and_outputs()",
            "",
            "    # A Network does not create weights of its own, thus it is already",
            "    # built.",
            "    self.built = True",
            "    self._build_input_shape = nest.map_structure(lambda x: x.shape, inputs)",
            "    self._compute_output_and_mask_jointly = True",
            "    # `_expects_training_arg` is True since the `training` argument is always",
            "    # present in the signature of the `call` method of a graph network.",
            "    self._expects_training_arg = True",
            "    self._expects_mask_arg = True",
            "    # A graph network does not autocast inputs, as its layers will cast them",
            "    # instead.",
            "    self._autocast = False",
            "",
            "    self._input_layers = []",
            "    self._output_layers = []",
            "    self._input_coordinates = []",
            "    self._output_coordinates = []",
            "",
            "    # This is for performance optimization when calling the Network on new",
            "    # inputs. Every time the Network is called on a set on input tensors,",
            "    # we compute the output tensors, output masks and output shapes in one pass,",
            "    # then cache them here. When any of these outputs is queried later, we",
            "    # retrieve it from there instead of recomputing it.",
            "    self._output_mask_cache = {}",
            "    self._output_tensor_cache = {}",
            "    self._output_shape_cache = {}",
            "",
            "    # Build self._output_layers:",
            "    for x in self.outputs:",
            "      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access",
            "      self._output_layers.append(layer)",
            "      self._output_coordinates.append((layer, node_index, tensor_index))",
            "",
            "    # Build self._input_layers:",
            "    for x in self.inputs:",
            "      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access",
            "      # It's supposed to be an input layer, so only one node",
            "      # and one tensor output.",
            "      assert node_index == 0",
            "      assert tensor_index == 0",
            "      self._input_layers.append(layer)",
            "      self._input_coordinates.append((layer, node_index, tensor_index))",
            "",
            "    # Keep track of the network's nodes and layers.",
            "    nodes, nodes_by_depth, layers, _ = _map_graph_network(",
            "        self.inputs, self.outputs)",
            "    self._network_nodes = nodes",
            "    self._nodes_by_depth = nodes_by_depth",
            "    self._self_tracked_trackables = layers",
            "    self._layer_call_argspecs = {}",
            "    for layer in self._self_tracked_trackables:",
            "      self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)",
            "",
            "    # Build self.input_names and self.output_names.",
            "    self._set_output_names()",
            "    self.input_names = []",
            "    self._feed_input_names = []",
            "    self._feed_inputs = []",
            "    self._feed_input_shapes = []",
            "    for layer in self._input_layers:",
            "      self.input_names.append(layer.name)",
            "      if layer.is_placeholder:",
            "        self._feed_input_names.append(layer.name)",
            "        # Use batch_input_shape here because non-eager composite tensors may not",
            "        # have a shape attribute that's meaningful (sparse, for instance, has",
            "        # a tensor that's non-constant and needs to be fed). This means that",
            "        # input layers that create placeholders will need to have the",
            "        # batch_input_shape attr to allow for input shape validation.",
            "        self._feed_input_shapes.append(layer._batch_input_shape)",
            "        self._feed_inputs.append(layer.input)",
            "",
            "    self._compute_tensor_usage_count()",
            "    self._set_save_spec(self._nested_inputs)",
            "    tf_utils.assert_no_legacy_layers(self.layers)",
            "",
            "  @property",
            "  def input(self):",
            "    \"\"\"Retrieves the input tensor(s) of a layer.",
            "",
            "    Only applicable if the layer has exactly one input,",
            "    i.e. if it is connected to one incoming layer.",
            "",
            "    Returns:",
            "        Input tensor or list of input tensors.",
            "",
            "    Raises:",
            "      RuntimeError: If called in Eager mode.",
            "      AttributeError: If no inbound nodes are found.",
            "    \"\"\"",
            "    return self._nested_inputs",
            "",
            "  @property",
            "  def input_shape(self):",
            "    \"\"\"Retrieves the input shape(s) of a layer.",
            "",
            "    Only applicable if the layer has exactly one input,",
            "    i.e. if it is connected to one incoming layer, or if all inputs",
            "    have the same shape.",
            "",
            "    Returns:",
            "        Input shape, as an integer shape tuple",
            "        (or list of shape tuples, one tuple per input tensor).",
            "",
            "    Raises:",
            "        AttributeError: if the layer has no defined input_shape.",
            "        RuntimeError: if called in Eager mode.",
            "    \"\"\"",
            "    return nest.map_structure(backend.int_shape, self.input)",
            "",
            "  @property",
            "  def input_spec(self):",
            "    if hasattr(self, '_manual_input_spec'):",
            "      return self._manual_input_spec",
            "    if (isinstance(self._nested_inputs, (dict, list, tuple)) and",
            "        len(self._nested_inputs) != len(self.inputs)):",
            "      # Case where we have a nested structure.",
            "      # In such a case we can't safely run any checks.",
            "      return None",
            "    if isinstance(self._nested_inputs, dict):",
            "      # Case where `_nested_inputs` is a plain dict of Inputs.",
            "      names = sorted(self._nested_inputs.keys())",
            "      return [input_spec.InputSpec(",
            "          shape=shape_with_no_batch_size(self._nested_inputs[name]),",
            "          allow_last_axis_squeeze=True, name=name) for name in names]",
            "    else:",
            "      # Single input, or list / tuple of inputs.",
            "      # The data may be passed as a dict keyed by input name.",
            "      return [input_spec.InputSpec(",
            "          shape=shape_with_no_batch_size(x), allow_last_axis_squeeze=True,",
            "          name=x._keras_history.layer.name) for x in self.inputs]",
            "",
            "  @input_spec.setter",
            "  def input_spec(self, value):",
            "    self._manual_input_spec = value",
            "",
            "  @property",
            "  def output(self):",
            "    \"\"\"Retrieves the output tensor(s) of a layer.",
            "",
            "    Only applicable if the layer has exactly one output,",
            "    i.e. if it is connected to one incoming layer.",
            "",
            "    Returns:",
            "      Output tensor or list of output tensors.",
            "",
            "    Raises:",
            "      AttributeError: if the layer is connected to more than one incoming",
            "        layers.",
            "      RuntimeError: if called in Eager mode.",
            "    \"\"\"",
            "    return self._nested_outputs",
            "",
            "  @property",
            "  def output_shape(self):",
            "    \"\"\"Retrieves the output shape(s) of a layer.",
            "",
            "    Only applicable if the layer has one output,",
            "    or if all outputs have the same shape.",
            "",
            "    Returns:",
            "        Output shape, as an integer shape tuple",
            "        (or list of shape tuples, one tuple per output tensor).",
            "",
            "    Raises:",
            "        AttributeError: if the layer has no defined output shape.",
            "        RuntimeError: if called in Eager mode.",
            "    \"\"\"",
            "    return nest.map_structure(backend.int_shape, self.output)",
            "",
            "  def _set_output_names(self):",
            "    \"\"\"Assigns unique names to the Network's outputs.",
            "",
            "    Output layers with multiple output tensors would otherwise lead to duplicate",
            "    names in self.output_names.",
            "    \"\"\"",
            "    uniquified = []",
            "    output_names = set()",
            "    prefix_count = {}",
            "    for layer in self._output_layers:",
            "      proposal = layer.name",
            "      while proposal in output_names:",
            "        existing_count = prefix_count.get(layer.name, 1)",
            "        proposal = '{}_{}'.format(layer.name, existing_count)",
            "        prefix_count[layer.name] = existing_count + 1",
            "      output_names.add(proposal)",
            "      uniquified.append(proposal)",
            "    self.output_names = uniquified",
            "",
            "  @property",
            "  def _layer_checkpoint_dependencies(self):",
            "    \"\"\"Dictionary of layer dependencies to be included in the checkpoint.\"\"\"",
            "    weight_layer_index = 0",
            "",
            "    dependencies = collections.OrderedDict()",
            "    for layer_index, layer in enumerate(self.layers):",
            "      try:",
            "        if layer.weights:",
            "          # Keep a separate index for layers which have weights. This allows",
            "          # users to insert Layers without weights anywhere in the network",
            "          # without breaking checkpoints.",
            "          dependencies['layer_with_weights-%d' % weight_layer_index] = layer",
            "          weight_layer_index += 1",
            "      except ValueError:",
            "        # The layer might have weights, but may not be built yet. We just treat",
            "        # it as layer without weight.",
            "        pass",
            "",
            "      # Even if it doesn't have weights, we should still track everything in",
            "      # case it has/will have Trackable dependencies.",
            "      dependencies['layer-%d' % layer_index] = layer",
            "    return dependencies",
            "",
            "  @property",
            "  def _checkpoint_dependencies(self):",
            "    dependencies = [",
            "        trackable.TrackableReference(name=name, ref=layer)",
            "        for name, layer in self._layer_checkpoint_dependencies.items()]",
            "    dependencies.extend(super(Functional, self)._checkpoint_dependencies)",
            "    return dependencies",
            "",
            "  def _lookup_dependency(self, name):",
            "    layer_dependencies = self._layer_checkpoint_dependencies",
            "    if name in layer_dependencies:",
            "      return layer_dependencies[name]",
            "    return super(Functional, self)._lookup_dependency(name)",
            "",
            "  def _handle_deferred_layer_dependencies(self, layers):",
            "    \"\"\"Handles layer checkpoint dependencies that are added after init.\"\"\"",
            "    layer_checkpoint_dependencies = self._layer_checkpoint_dependencies",
            "    layer_to_name = {v: k for k, v in layer_checkpoint_dependencies.items()}",
            "    for layer in layers:",
            "      if layer in layer_to_name:",
            "        self._handle_deferred_dependencies(name=layer_to_name[layer],",
            "                                           trackable=layer)",
            "",
            "  @property",
            "  def _should_compute_mask(self):",
            "    return True",
            "",
            "  def compute_mask(self, inputs, mask):",
            "    # TODO(omalleyt): b/123540974 This function is not really safe to call",
            "    # by itself because it will duplicate any updates and losses in graph",
            "    # mode by `call`ing the Layers again.",
            "    output_tensors = self._run_internal_graph(inputs, mask=mask)",
            "    return nest.map_structure(lambda t: getattr(t, '_keras_mask', None),",
            "                              output_tensors)",
            "",
            "  @doc_controls.do_not_doc_inheritable",
            "  def call(self, inputs, training=None, mask=None):",
            "    \"\"\"Calls the model on new inputs.",
            "",
            "    In this case `call` just reapplies",
            "    all ops in the graph to the new inputs",
            "    (e.g. build a new computational graph from the provided inputs).",
            "",
            "    Args:",
            "        inputs: A tensor or list of tensors.",
            "        training: Boolean or boolean scalar tensor, indicating whether to run",
            "          the `Network` in training mode or inference mode.",
            "        mask: A mask or list of masks. A mask can be",
            "            either a tensor or None (no mask).",
            "",
            "    Returns:",
            "        A tensor if there is a single output, or",
            "        a list of tensors if there are more than one outputs.",
            "    \"\"\"",
            "    return self._run_internal_graph(",
            "        inputs, training=training, mask=mask)",
            "",
            "  def compute_output_shape(self, input_shape):",
            "    # Convert any shapes in tuple format to TensorShapes.",
            "    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)",
            "",
            "    if len(nest.flatten(input_shape)) != len(nest.flatten(self._input_layers)):",
            "      raise ValueError('Invalid input_shape argument ' + str(input_shape) +",
            "                       ': model has ' + str(len(self._input_layers)) +",
            "                       ' tensor inputs.')",
            "",
            "    # Use the tuple of TensorShape as the cache key, since tuple is hashable",
            "    # and can be used as hash key.",
            "    try:",
            "      cache_key = tuple(tf_utils.convert_shapes(input_shape, to_tuples=True))",
            "      if cache_key in self._output_shape_cache:",
            "        # Cache hit. Return shapes as TensorShapes.",
            "        return self._output_shape_cache[cache_key]",
            "    except ValueError:",
            "      # In case there are unknown TensorShape, eg for sparse tensor input,",
            "      # We skip the caching since the shape is unknown.",
            "      pass",
            "",
            "    layers_to_output_shapes = {}",
            "    for layer, shape in zip(self._input_layers, nest.flatten(input_shape)):",
            "      # It's an input layer: then `compute_output_shape` is identity,",
            "      # and there is only one node and one tensor..",
            "      shape_key = layer.name + '_0_0'",
            "      layers_to_output_shapes[shape_key] = shape",
            "",
            "    depth_keys = list(self._nodes_by_depth.keys())",
            "    depth_keys.sort(reverse=True)",
            "    # Iterate over nodes, by depth level.",
            "    if len(depth_keys) > 1:",
            "      for depth in depth_keys:",
            "        nodes = self._nodes_by_depth[depth]",
            "        for node in nodes:",
            "          layer = node.layer",
            "          if layer in self._input_layers:",
            "            # We've already covered the input layers",
            "            # a few lines above.",
            "            continue",
            "          # Get the input shapes for the first argument of the node",
            "          layer_input_shapes = []",
            "          layer_inputs = node.call_args[0]",
            "          for layer_input in nest.flatten(layer_inputs):",
            "            kh = layer_input._keras_history",
            "            input_layer_key = kh.layer.name + '_%s_%s' % (kh.node_index,",
            "                                                          kh.tensor_index)",
            "            layer_input_shapes.append(layers_to_output_shapes[input_layer_key])",
            "          layer_input_shapes = nest.pack_sequence_as(layer_inputs,",
            "                                                     layer_input_shapes)",
            "          # Layers expect shapes to be tuples for `compute_output_shape`.",
            "          layer_input_shapes = tf_utils.convert_shapes(",
            "              layer_input_shapes, to_tuples=True)",
            "          layer_output_shapes = layer.compute_output_shape(layer_input_shapes)",
            "          # Convert back to TensorShapes.",
            "          layer_output_shapes = tf_utils.convert_shapes(",
            "              layer_output_shapes, to_tuples=False)",
            "",
            "          node_index = layer._inbound_nodes.index(node)  # pylint: disable=protected-access",
            "          for j, shape in enumerate(nest.flatten(layer_output_shapes)):",
            "            shape_key = layer.name + '_%s_%s' % (node_index, j)",
            "            layers_to_output_shapes[shape_key] = shape",
            "",
            "      # Read final output shapes from layers_to_output_shapes.",
            "      output_shapes = []",
            "      for i in range(len(self._output_layers)):",
            "        layer, node_index, tensor_index = self._output_coordinates[i]",
            "        shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)",
            "        output_shapes.append(layers_to_output_shapes[shape_key])",
            "      output_shapes = nest.pack_sequence_as(self._nested_outputs, output_shapes)",
            "      # Store in cache.",
            "      self._output_shape_cache[cache_key] = output_shapes",
            "",
            "    # Return shapes as TensorShapes.",
            "    return output_shapes",
            "",
            "  def _init_set_name(self, name, zero_based=True):",
            "    if not name:",
            "      cls_name = self.__class__.__name__",
            "      if self.__class__ == Functional:",
            "        # Hide the functional class name from user, since its not a public",
            "        # visible class. Use \"Model\" instead,",
            "        cls_name = 'Model'",
            "      self._name = backend.unique_object_name(",
            "          generic_utils.to_snake_case(cls_name),",
            "          zero_based=zero_based)",
            "    else:",
            "      self._name = name",
            "",
            "  def _run_internal_graph(self, inputs, training=None, mask=None):",
            "    \"\"\"Computes output tensors for new inputs.",
            "",
            "    # Note:",
            "        - Can be run on non-Keras tensors.",
            "",
            "    Args:",
            "        inputs: Tensor or nested structure of Tensors.",
            "        training: Boolean learning phase.",
            "        mask: (Optional) Tensor or nested structure of Tensors.",
            "",
            "    Returns:",
            "        output_tensors",
            "    \"\"\"",
            "    inputs = self._flatten_to_reference_inputs(inputs)",
            "    if mask is None:",
            "      masks = [None] * len(inputs)",
            "    else:",
            "      masks = self._flatten_to_reference_inputs(mask)",
            "    for input_t, mask in zip(inputs, masks):",
            "      input_t._keras_mask = mask",
            "",
            "    # Dictionary mapping reference tensors to computed tensors.",
            "    tensor_dict = {}",
            "    tensor_usage_count = self._tensor_usage_count",
            "    for x, y in zip(self.inputs, inputs):",
            "      y = self._conform_to_reference_input(y, ref_input=x)",
            "      x_id = str(id(x))",
            "      tensor_dict[x_id] = [y] * tensor_usage_count[x_id]",
            "",
            "    nodes_by_depth = self._nodes_by_depth",
            "    depth_keys = list(nodes_by_depth.keys())",
            "    depth_keys.sort(reverse=True)",
            "",
            "    for depth in depth_keys:",
            "      nodes = nodes_by_depth[depth]",
            "      for node in nodes:",
            "        if node.is_input:",
            "          continue  # Input tensors already exist.",
            "",
            "        if any(t_id not in tensor_dict for t_id in node.flat_input_ids):",
            "          continue  # Node is not computable, try skipping.",
            "",
            "        args, kwargs = node.map_arguments(tensor_dict)",
            "        outputs = node.layer(*args, **kwargs)",
            "",
            "        # Update tensor_dict.",
            "        for x_id, y in zip(node.flat_output_ids, nest.flatten(outputs)):",
            "          tensor_dict[x_id] = [y] * tensor_usage_count[x_id]",
            "",
            "    output_tensors = []",
            "    for x in self.outputs:",
            "      x_id = str(id(x))",
            "      assert x_id in tensor_dict, 'Could not compute output ' + str(x)",
            "      output_tensors.append(tensor_dict[x_id].pop())",
            "",
            "    return nest.pack_sequence_as(self._nested_outputs, output_tensors)",
            "",
            "  def _flatten_to_reference_inputs(self, tensors):",
            "    \"\"\"Maps `tensors` to their respective `keras.Input`.\"\"\"",
            "    if self._enable_dict_to_input_mapping and isinstance(tensors, dict):",
            "      ref_inputs = self._nested_inputs",
            "      if not nest.is_nested(ref_inputs):",
            "        ref_inputs = [self._nested_inputs]",
            "      if isinstance(ref_inputs, dict):",
            "        # In the case that the graph is constructed with dict input tensors,",
            "        # We will use the original dict key to map with the keys in the input",
            "        # data. Note that the model.inputs is using nest.flatten to process the",
            "        # input tensors, which means the dict input tensors are ordered by their",
            "        # keys.",
            "        ref_input_names = sorted(ref_inputs.keys())",
            "      else:",
            "        ref_input_names = [inp._keras_history.layer.name for inp in ref_inputs]",
            "",
            "      # Raise an warning if there are more input data comparing to input tensor",
            "      if len(tensors) > len(ref_input_names):",
            "        warnings.warn(",
            "            'Input dict contained keys {} which did not match any model input. '",
            "            'They will be ignored by the model.'.format(",
            "                [n for n in tensors.keys() if n not in ref_input_names])",
            "            )",
            "",
            "      try:",
            "        # Flatten in the order `Input`s were passed during Model construction.",
            "        return [tensors[n] for n in ref_input_names]",
            "      except KeyError:",
            "        # TODO(b/151582614)",
            "        return nest.flatten(tensors)",
            "",
            "    # Otherwise both self.inputs and tensors will already be in same order.",
            "    return nest.flatten(tensors)",
            "",
            "  def _conform_to_reference_input(self, tensor, ref_input):",
            "    \"\"\"Set shape and dtype based on `keras.Input`s.\"\"\"",
            "    if isinstance(tensor, ops.Tensor):",
            "      # Allow (None,) and (None, 1) Tensors to be passed interchangeably. Use",
            "      # the shape specified by the `keras.Input`.",
            "      t_shape = tensor.shape",
            "      t_rank = t_shape.rank",
            "      ref_shape = ref_input.shape",
            "      ref_rank = ref_shape.rank",
            "      keras_history = getattr(tensor, '_keras_history', None)",
            "      if t_rank is not None and ref_rank is not None:",
            "        # Should squeeze last dimension.",
            "        # True if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).",
            "        if (t_rank == ref_rank + 1 and t_shape[-1] == 1):",
            "          tensor = array_ops.squeeze_v2(tensor, axis=-1)",
            "        # Should expand last_dimension.",
            "        # True if tensor is (BATCH, ...) and reference is (BATCH, ..., 1).",
            "        elif (t_rank == ref_rank - 1 and ref_shape[-1] == 1):",
            "          tensor = array_ops.expand_dims_v2(tensor, axis=-1)",
            "      if keras_history is not None:  # Restore keras history.",
            "        tensor._keras_history = keras_history",
            "",
            "      # Add shape hints to Tensors that may have None shape dims but have shapes",
            "      # defined by the `keras.Input` (not applicable in eager mode).",
            "      if not context.executing_eagerly():",
            "        try:",
            "          tensor.set_shape(tensor.shape.merge_with(ref_input.shape))",
            "        except ValueError:",
            "          logging.warning(",
            "              'Model was constructed with shape {} for input {}, but it was '",
            "              'called on an input with incompatible shape {}.'.format(",
            "                  ref_input.shape, ref_input, tensor.shape))",
            "",
            "      # Dtype casting.",
            "      tensor = math_ops.cast(tensor, dtype=ref_input.dtype)",
            "    elif tf_utils.is_extension_type(tensor):",
            "      # Dtype casting (If the extension type has a non-variant dtype and",
            "      # supports being cast)",
            "      ref_input_dtype = getattr(ref_input, 'dtype', None)",
            "      if ref_input_dtype is not None and ref_input_dtype != dtypes.variant:",
            "        tensor = math_ops.cast(tensor, dtype=ref_input_dtype)",
            "",
            "    return tensor",
            "",
            "  def get_config(self):",
            "    return copy.deepcopy(get_network_config(self))",
            "",
            "  @classmethod",
            "  def from_config(cls, config, custom_objects=None):",
            "    \"\"\"Instantiates a Model from its config (output of `get_config()`).",
            "",
            "    Args:",
            "        config: Model config dictionary.",
            "        custom_objects: Optional dictionary mapping names",
            "            (strings) to custom classes or functions to be",
            "            considered during deserialization.",
            "",
            "    Returns:",
            "        A model instance.",
            "",
            "    Raises:",
            "        ValueError: In case of improperly formatted config dict.",
            "    \"\"\"",
            "    with generic_utils.SharedObjectLoadingScope():",
            "      input_tensors, output_tensors, created_layers = reconstruct_from_config(",
            "          config, custom_objects)",
            "      model = cls(inputs=input_tensors, outputs=output_tensors,",
            "                  name=config.get('name'))",
            "      connect_ancillary_layers(model, created_layers)",
            "      return model",
            "",
            "  def _validate_graph_inputs_and_outputs(self):",
            "    \"\"\"Validates the inputs and outputs of a Graph Network.\"\"\"",
            "    # Check for redundancy in inputs.",
            "    if len({id(i) for i in self.inputs}) != len(self.inputs):",
            "      raise ValueError('The list of inputs passed to the model '",
            "                       'is redundant. '",
            "                       'All inputs should only appear once.'",
            "                       ' Found: ' + str(self.inputs))",
            "",
            "    for x in self.inputs:",
            "      # Check that x has appropriate `_keras_history` metadata.",
            "      if not hasattr(x, '_keras_history'):",
            "        cls_name = self.__class__.__name__",
            "        raise ValueError('Input tensors to a ' + cls_name + ' ' +",
            "                         'must come from `tf.keras.Input`. '",
            "                         'Received: ' + str(x) +",
            "                         ' (missing previous layer metadata).')",
            "      # Check that x is an input tensor.",
            "      # pylint: disable=protected-access",
            "      layer = x._keras_history.layer",
            "      if len(layer._inbound_nodes) > 1 or (",
            "          layer._inbound_nodes and not layer._inbound_nodes[0].is_input):",
            "        cls_name = self.__class__.__name__",
            "        logging.warning(cls_name + ' model inputs must come from '",
            "                        '`tf.keras.Input` (thus holding past layer metadata), '",
            "                        'they cannot be the output of '",
            "                        'a previous non-Input layer. '",
            "                        'Here, a tensor specified as '",
            "                        'input to \"' + self.name + '\" was not an Input tensor, '",
            "                        'it was generated by layer ' + layer.name + '.\\n'",
            "                        'Note that input tensors are '",
            "                        'instantiated via `tensor = tf.keras.Input(shape)`.\\n'",
            "                        'The tensor that caused the issue was: ' + str(x.name))",
            "",
            "    # Check compatibility of batch sizes of Input Layers.",
            "    input_batch_sizes = [",
            "        training_utils.get_static_batch_size(x._keras_history.layer)",
            "        for x in self.inputs",
            "    ]",
            "    consistent_batch_size = None",
            "    for batch_size in input_batch_sizes:",
            "      if batch_size is not None:",
            "        if (consistent_batch_size is not None and",
            "            batch_size != consistent_batch_size):",
            "          raise ValueError('The specified batch sizes of the Input Layers'",
            "                           ' are incompatible. Found batch sizes: {}'.format(",
            "                               input_batch_sizes))",
            "        consistent_batch_size = batch_size",
            "",
            "    for x in self.outputs:",
            "      if not hasattr(x, '_keras_history'):",
            "        cls_name = self.__class__.__name__",
            "        raise ValueError('Output tensors of a ' + cls_name + ' model must be '",
            "                         'the output of a TensorFlow `Layer` '",
            "                         '(thus holding past layer metadata). Found: ' + str(x))",
            "",
            "  def _insert_layers(self, layers, relevant_nodes=None):",
            "    \"\"\"Inserts Layers into the Network after Network creation.",
            "",
            "    This is only valid for Keras Graph Networks.  Layers added via this function",
            "    will be included in the `call` computation and `get_config` of this Network.",
            "    They will not be added to the Network's outputs.",
            "",
            "",
            "    Args:",
            "      layers: Arbitrary nested structure of Layers. Layers must be reachable",
            "        from one or more of the `keras.Input` Tensors that correspond to this",
            "        Network's inputs.",
            "      relevant_nodes: Nodes from the Layers that should be considered part of",
            "        this Network. If `None`, all Nodes will be considered part of this",
            "        Network.",
            "",
            "    Raises:",
            "      ValueError: If the layers depend on `Input`s not found in this Model.",
            "    \"\"\"",
            "    layers = nest.flatten(layers)",
            "    tf_utils.assert_no_legacy_layers(layers)",
            "    node_to_depth = {}",
            "    for depth, nodes in self._nodes_by_depth.items():",
            "      node_to_depth.update({node: depth for node in nodes})",
            "    # The nodes of these Layers that are relevant to this Network. If not",
            "    # provided, assume all Nodes are relevant",
            "    if not relevant_nodes:",
            "      relevant_nodes = nest.flatten([layer._inbound_nodes for layer in layers])",
            "    network_nodes = set(relevant_nodes + list(node_to_depth.keys()))",
            "",
            "    def _get_min_depth(node):",
            "      \"\"\"Gets the minimum depth at which node can be computed.\"\"\"",
            "      min_depth = 0",
            "      for layer, node_id, _, _ in node.iterate_inbound():",
            "        inbound_node = layer._inbound_nodes[node_id]",
            "        if inbound_node in node_to_depth:",
            "          min_depth = min(min_depth, node_to_depth[inbound_node])",
            "        elif inbound_node not in network_nodes:",
            "          continue",
            "        else:",
            "          # Previous relevant nodes haven't been processed yet.",
            "          return None",
            "      # New node is one shallower than its shallowest input.",
            "      return min_depth - 1",
            "",
            "    # Insert nodes into `_nodes_by_depth` and other node attrs.",
            "    unprocessed_nodes = copy.copy(relevant_nodes)",
            "    i = 0",
            "    while unprocessed_nodes:",
            "      i += 1",
            "      # Do a sanity check. This can occur if `Input`s from outside this Model",
            "      # are being relied on.",
            "      if i > 10000:",
            "        raise ValueError('Layers could not be added due to missing '",
            "                         'dependencies.')",
            "",
            "      node = unprocessed_nodes.pop(0)",
            "      depth = _get_min_depth(node)",
            "      if depth is None:  # Defer until inbound nodes are processed.",
            "        unprocessed_nodes.append(node)",
            "        continue",
            "      node_key = _make_node_key(node.layer.name,",
            "                                node.layer._inbound_nodes.index(node))",
            "      if node_key not in self._network_nodes:",
            "        node_to_depth[node] = depth",
            "        self._network_nodes.add(node_key)",
            "        self._nodes_by_depth[depth].append(node)",
            "",
            "    # Insert layers and update other layer attrs.",
            "    layer_set = set(self._self_tracked_trackables)",
            "    deferred_layers = []",
            "    for layer in layers:",
            "      if layer not in layer_set:",
            "        self._self_tracked_trackables.append(layer)",
            "        deferred_layers.append(layer)",
            "        self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)",
            "        layer_set.add(layer)",
            "    self._handle_deferred_layer_dependencies(deferred_layers)",
            "",
            "    self._compute_tensor_usage_count()",
            "",
            "  def _compute_tensor_usage_count(self):",
            "    \"\"\"Compute the #. of tensor usages for all the output tensors of layers.",
            "",
            "    The computed tensor usage count is saved as `self._tensor_usage_count`. This",
            "    is later used for saving memory in eager computation by releasing",
            "    no-longer-needed tensors as early as possible.",
            "    \"\"\"",
            "    tensor_usage_count = collections.Counter()",
            "    available_tensors = set(str(id(tensor)) for tensor in self.inputs)",
            "",
            "    depth_keys = list(self._nodes_by_depth.keys())",
            "    depth_keys.sort(reverse=True)",
            "    depth_keys = depth_keys[1:]",
            "",
            "    for depth in depth_keys:",
            "      for node in self._nodes_by_depth[depth]:",
            "        input_tensors = {",
            "            str(id(tensor)) for tensor in nest.flatten(node.keras_inputs)",
            "        }",
            "        if input_tensors.issubset(available_tensors):",
            "          for tensor in nest.flatten(node.keras_inputs):",
            "            tensor_usage_count[str(id(tensor))] += 1",
            "",
            "          for output_tensor in nest.flatten(node.outputs):",
            "            available_tensors.add(str(id(output_tensor)))",
            "",
            "    for tensor in self.outputs:",
            "      tensor_usage_count[str(id(tensor))] += 1",
            "",
            "    self._tensor_usage_count = tensor_usage_count",
            "",
            "  def _assert_weights_created(self):",
            "    # Override the implementation in Model.",
            "    # The Functional model should always have weight created already.",
            "    return",
            "",
            "  def _graph_network_add_loss(self, symbolic_loss):",
            "    new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])",
            "    # Losses must be keyed on inputs no matter what in order to be supported in",
            "    # DistributionStrategy.",
            "    add_loss_layer = base_layer.AddLoss(",
            "        unconditional=False, dtype=symbolic_loss.dtype)",
            "    add_loss_layer(symbolic_loss)",
            "    new_nodes.extend(add_loss_layer.inbound_nodes)",
            "    new_layers.append(add_loss_layer)",
            "    self._insert_layers(new_layers, new_nodes)",
            "",
            "  def _graph_network_add_metric(self, value, aggregation, name):",
            "    new_nodes, new_layers = _map_subgraph_network(self.inputs, [value])",
            "    add_metric_layer = base_layer.AddMetric(",
            "        aggregation, name, dtype=value.dtype)",
            "    add_metric_layer(value)",
            "    new_nodes.extend(add_metric_layer.inbound_nodes)",
            "    new_layers.append(add_metric_layer)",
            "    self._insert_layers(new_layers, new_nodes)",
            "",
            "  @property",
            "  def _trackable_saved_model_saver(self):",
            "    return network_serialization.NetworkSavedModelSaver(self)",
            "",
            "  def _get_save_spec(self, dynamic_batch=True):",
            "    if getattr(self, '_has_explicit_input_shape', True):",
            "      # Functional models and Sequential models that have an explicit input",
            "      # shape should use the batch size set by the input layer.",
            "      dynamic_batch = False",
            "    return super(Functional, self)._get_save_spec(dynamic_batch)",
            "",
            "",
            "def _make_node_key(layer_name, node_index):",
            "  return layer_name + '_ib-' + str(node_index)",
            "",
            "",
            "def _map_graph_network(inputs, outputs):",
            "  \"\"\"Validates a network's topology and gather its layers and nodes.",
            "",
            "  Args:",
            "    inputs: List of input tensors.",
            "    outputs: List of outputs tensors.",
            "",
            "  Returns:",
            "    A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.",
            "    - nodes: list of Node instances.",
            "    - nodes_by_depth: dict mapping ints (depth) to lists of node instances.",
            "    - layers: list of Layer instances.",
            "    - layers_by_depth: dict mapping ints (depth) to lists of layer instances.",
            "",
            "  Raises:",
            "    ValueError: In case the network is not valid (e.g. disconnected graph).",
            "  \"\"\"",
            "  # \"depth\" is number of layers between output Node and the Node.",
            "  # Nodes are ordered from inputs -> outputs.",
            "  nodes_in_decreasing_depth, layer_indices = _build_map(outputs)",
            "  network_nodes = {",
            "      _make_node_key(node.layer.name, node.layer._inbound_nodes.index(node))",
            "      for node in nodes_in_decreasing_depth",
            "  }",
            "",
            "  nodes_depths = {}  # dict {node: depth value}",
            "  layers_depths = {}  # dict {layer: depth value}",
            "",
            "  for node in reversed(nodes_in_decreasing_depth):",
            "    # If the depth is not set, the node has no outbound nodes (depth 0).",
            "    depth = nodes_depths.setdefault(node, 0)",
            "",
            "    # Update the depth of the corresponding layer",
            "    previous_depth = layers_depths.get(node.layer, 0)",
            "    # If we've seen this layer before at a higher depth,",
            "    # we should use that depth instead of the node depth.",
            "    # This is necessary for shared layers that have inputs at different",
            "    # depth levels in the graph.",
            "    depth = max(depth, previous_depth)",
            "    layers_depths[node.layer] = depth",
            "    nodes_depths[node] = depth",
            "",
            "    # Update the depth of inbound nodes.",
            "    # The \"depth\" of a node is the max of the depths",
            "    # of all nodes it is connected to + 1.",
            "    for node_dep in node.parent_nodes:",
            "      previous_depth = nodes_depths.get(node_dep, 0)",
            "      nodes_depths[node_dep] = max(depth + 1, previous_depth)",
            "",
            "  # Handle inputs that are not connected to outputs.",
            "  # We do not error out here because the inputs may be used to compute losses",
            "  # and metrics.",
            "  for input_t in inputs:",
            "    input_layer = input_t._keras_history[0]",
            "    if input_layer not in layers_depths:",
            "      layers_depths[input_layer] = 0",
            "      layer_indices[input_layer] = -1",
            "      nodes_depths[input_layer._inbound_nodes[0]] = 0",
            "      network_nodes.add(_make_node_key(input_layer.name, 0))",
            "",
            "  # Build a dict {depth: list of nodes with this depth}",
            "  nodes_by_depth = collections.defaultdict(list)",
            "  for node, depth in nodes_depths.items():",
            "    nodes_by_depth[depth].append(node)",
            "",
            "  # Build a dict {depth: list of layers with this depth}",
            "  layers_by_depth = collections.defaultdict(list)",
            "  for layer, depth in layers_depths.items():",
            "    layers_by_depth[depth].append(layer)",
            "",
            "  # Get sorted list of layer depths.",
            "  depth_keys = list(layers_by_depth.keys())",
            "  depth_keys.sort(reverse=True)",
            "",
            "  # Set self.layers ordered by depth.",
            "  layers = []",
            "  for depth in depth_keys:",
            "    layers_for_depth = layers_by_depth[depth]",
            "    # Network.layers needs to have a deterministic order:",
            "    # here we order them by traversal order.",
            "    layers_for_depth.sort(key=lambda x: layer_indices[x])",
            "    layers.extend(layers_for_depth)",
            "",
            "  # Get sorted list of node depths.",
            "  depth_keys = list(nodes_by_depth.keys())",
            "  depth_keys.sort(reverse=True)",
            "",
            "  # Check that all tensors required are computable.",
            "  # computable_tensors: all tensors in the graph",
            "  # that can be computed from the inputs provided.",
            "  computable_tensors = set()",
            "  for x in inputs:",
            "    computable_tensors.add(id(x))",
            "",
            "  layers_with_complete_input = []  # To provide a better error msg.",
            "  for depth in depth_keys:",
            "    for node in nodes_by_depth[depth]:",
            "      layer = node.layer",
            "      if layer and not node.is_input:",
            "        for x in nest.flatten(node.keras_inputs):",
            "          if id(x) not in computable_tensors:",
            "            raise ValueError('Graph disconnected: '",
            "                             'cannot obtain value for tensor ' + str(x) +",
            "                             ' at layer \"' + layer.name + '\". '",
            "                             'The following previous layers '",
            "                             'were accessed without issue: ' +",
            "                             str(layers_with_complete_input))",
            "        for x in nest.flatten(node.outputs):",
            "          computable_tensors.add(id(x))",
            "        layers_with_complete_input.append(layer.name)",
            "",
            "  # Ensure name unicity, which will be crucial for serialization",
            "  # (since serialized nodes refer to layers by their name).",
            "  all_names = [layer.name for layer in layers]",
            "  for name in all_names:",
            "    if all_names.count(name) != 1:",
            "      raise ValueError('The name \"' + name + '\" is used ' +",
            "                       str(all_names.count(name)) + ' times in the model. '",
            "                       'All layer names should be unique.')",
            "  return network_nodes, nodes_by_depth, layers, layers_by_depth",
            "",
            "",
            "def _build_map(outputs):",
            "  \"\"\"This method topologically sorts nodes in order from inputs to outputs.",
            "",
            "  It uses a depth-first search to topologically sort nodes that appear in the",
            "  _keras_history connectivity metadata of `outputs`.",
            "",
            "  Args:",
            "    outputs: the output tensors whose _keras_history metadata should be walked.",
            "    This may be an arbitrary nested structure.",
            "",
            "  Returns:",
            "    A tuple like (ordered_nodes, layer_to_first_traversal_index)",
            "    ordered_nodes: list of nodes appearing in the keras history, topologically",
            "      sorted from original inputs to the `outputs`.",
            "      (If outputs have different sets of ancestors, the inputs to one output",
            "      may appear after a different output).",
            "    layer_to_first_traversal_index:",
            "      A dict mapping layer to the traversal index in the DFS where it is",
            "      seen. Note: if a layer is shared by several nodes, the dict will only",
            "      store the index corresponding to the *first* time the layer seen.",
            "  \"\"\"",
            "  finished_nodes = set()",
            "  nodes_in_progress = set()",
            "  nodes_in_decreasing_depth = []  # nodes from inputs -> outputs.",
            "  layer_indices = {}  # layer -> in traversal order.",
            "  for output in nest.flatten(outputs):",
            "    _build_map_helper(output, finished_nodes, nodes_in_progress,",
            "                      nodes_in_decreasing_depth, layer_indices)",
            "  return nodes_in_decreasing_depth, layer_indices",
            "",
            "",
            "def _build_map_helper(tensor, finished_nodes, nodes_in_progress,",
            "                      nodes_in_decreasing_depth, layer_indices):",
            "  \"\"\"Recursive helper for `_build_map`.\"\"\"",
            "  layer, node_index, _ = tensor._keras_history  # pylint: disable=protected-access",
            "  node = layer._inbound_nodes[node_index]  # pylint: disable=protected-access",
            "",
            "  # Don't repeat work for shared subgraphs",
            "  if node in finished_nodes:",
            "    return",
            "",
            "  # Prevent cycles.",
            "  if node in nodes_in_progress:",
            "    raise ValueError('The tensor ' + str(tensor) + ' at layer \"' + layer.name +",
            "                     '\" is part of a cycle.')",
            "",
            "  # Store the traversal order for layer sorting.",
            "  if layer not in layer_indices:",
            "    layer_indices[layer] = len(layer_indices)",
            "",
            "  # Propagate to all previous tensors connected to this node.",
            "  nodes_in_progress.add(node)",
            "  if not node.is_input:",
            "    for tensor in node.keras_inputs:",
            "      _build_map_helper(tensor, finished_nodes, nodes_in_progress,",
            "                        nodes_in_decreasing_depth, layer_indices)",
            "",
            "  finished_nodes.add(node)",
            "  nodes_in_progress.remove(node)",
            "  nodes_in_decreasing_depth.append(node)",
            "",
            "",
            "def _map_subgraph_network(inputs, outputs):",
            "  \"\"\"Returns the nodes and layers in the topology from `inputs` to `outputs`.",
            "",
            "  Args:",
            "    inputs: List of input tensors.",
            "    outputs: List of output tensors.",
            "",
            "  Returns:",
            "    A tuple of List{Node] and List[Layer].",
            "  \"\"\"",
            "  if not ops.executing_eagerly_outside_functions():",
            "    base_layer_utils.create_keras_history(outputs)",
            "  # Keep only nodes and layers in the topology between inputs and outputs.",
            "  _, nodes_by_depth, layers, _ = _map_graph_network(inputs, outputs)",
            "  return nest.flatten([nodes for nodes in nodes_by_depth.values()]), layers",
            "",
            "",
            "def _should_skip_first_node(layer):",
            "  \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"",
            "  # Networks that are constructed with an Input layer/shape start with a",
            "  # pre-existing node linking their input to output. This node is excluded from",
            "  # the network config.",
            "  if layer._self_tracked_trackables:",
            "    return (isinstance(layer, Functional) and",
            "            # Filter out Sequential models without an input shape.",
            "            isinstance(layer._self_tracked_trackables[0],",
            "                       input_layer_module.InputLayer))",
            "  else:",
            "    return isinstance(layer, Functional)",
            "",
            "",
            "def connect_ancillary_layers(model, created_layers):",
            "  \"\"\"Adds layers that are not connected to the outputs to the model.\"\"\"",
            "  # Layers not connected to outputs, such as those added in `add_loss`.",
            "  ancillary_layers = [",
            "      layer for layer in created_layers.values() if layer not in model.layers",
            "  ]",
            "  if ancillary_layers:",
            "    relevant_nodes = nest.flatten([",
            "        layer.inbound_nodes[1:]",
            "        if _should_skip_first_node(layer) else layer.inbound_nodes",
            "        for layer in created_layers.values()",
            "    ])",
            "    model._insert_layers(ancillary_layers, relevant_nodes)",
            "  return model",
            "",
            "",
            "def reconstruct_from_config(config, custom_objects=None, created_layers=None):",
            "  \"\"\"Reconstructs graph from config object.",
            "",
            "  Args:",
            "    config: Dictionary returned from Network.get_config()",
            "    custom_objects: Optional dictionary mapping names (strings) to custom",
            "      classes or functions to be considered during deserialization.",
            "    created_layers: Optional dictionary mapping names to Layer objects. Any",
            "      layer not in this dictionary will be created and added to the dict.",
            "      This function will add new nodes to all layers (excluding InputLayers),",
            "      instead of re-using pre-existing nodes in the layers.",
            "",
            "  Returns:",
            "    Tuple of (input tensors, output tensors, dictionary of created layers)",
            "  \"\"\"",
            "  # Layer instances created during the graph reconstruction process.",
            "  created_layers = created_layers or collections.OrderedDict()",
            "",
            "  # Maps input data (tuple of inbound layer name, node index) from the config",
            "  # to node indices in the newly generated model. The node indices may be",
            "  # different if the layers have already been called previously.",
            "  node_index_map = {}",
            "  node_count_by_layer = {}",
            "",
            "  # Dictionary mapping layer instances to",
            "  # node data that specifies a layer call.",
            "  # It acts as a queue that maintains any unprocessed",
            "  # layer call until it becomes possible to process it",
            "  # (i.e. until the input tensors to the call all exist).",
            "  unprocessed_nodes = {}",
            "",
            "  def add_unprocessed_node(layer, node_data):",
            "    if layer not in unprocessed_nodes:",
            "      unprocessed_nodes[layer] = [node_data]",
            "    else:",
            "      unprocessed_nodes[layer].append(node_data)",
            "",
            "  def get_node_index(layer, config_node_index):",
            "    \"\"\"Returns node index in layer (might differ from config_node_index).\"\"\"",
            "    if isinstance(layer, input_layer_module.InputLayer):",
            "      return 0",
            "    return node_index_map.get((layer.name, config_node_index), None)",
            "",
            "  def _deserialize_keras_tensors(kwargs, layer_map):",
            "    \"\"\"Deserializes Keras Tensors passed to `call`..\"\"\"",
            "",
            "    def _deserialize_keras_tensor(t):",
            "      \"\"\"Deserializes a single Keras Tensor passed to `call`.\"\"\"",
            "      if isinstance(t, tf_utils.ListWrapper):",
            "        t = t.as_list()",
            "        layer_name = t[0]",
            "        node_index = t[1]",
            "        tensor_index = t[2]",
            "",
            "        layer = layer_map[layer_name]",
            "        new_node_index = get_node_index(layer, node_index)",
            "        if new_node_index is None:",
            "          # The inbound node may not have been processed yet,",
            "          # (This can happen e.g. if it depends on a different set",
            "          # of inputs than those that have been processed already).",
            "          # raise an IndexError so that the current node puts itself",
            "          # back on the unprocessed queue.",
            "          # Caution: This may lead to infinite loops for malformed",
            "          # network configurations! (or when there is a bug in",
            "          # the network config loading code).",
            "          raise IndexError",
            "        node = layer._inbound_nodes[new_node_index]",
            "        return nest.flatten(node.outputs)[tensor_index]",
            "      return t",
            "",
            "    kwargs = tf_utils.convert_inner_node_data(kwargs, wrap=True)",
            "    return nest.map_structure(_deserialize_keras_tensor, kwargs)",
            "",
            "  def process_node(layer, node_data):",
            "    \"\"\"Deserialize a node.",
            "",
            "    Args:",
            "        layer: layer instance.",
            "        node_data: Nested structure of `ListWrapper`.",
            "",
            "    Raises:",
            "        ValueError: In case of improperly formatted `node_data`.",
            "    \"\"\"",
            "    input_tensors = []",
            "    for input_data in nest.flatten(node_data):",
            "      input_data = input_data.as_list()",
            "      inbound_layer_name = input_data[0]",
            "      inbound_node_index = input_data[1]",
            "      inbound_tensor_index = input_data[2]",
            "      if len(input_data) == 3:",
            "        kwargs = {}",
            "      elif len(input_data) == 4:",
            "        kwargs = input_data[3]",
            "        try:",
            "          kwargs = _deserialize_keras_tensors(kwargs, created_layers)",
            "        except IndexError:",
            "          # Happens if keras tensors in kwargs are still unprocessed",
            "          add_unprocessed_node(layer, node_data)",
            "          return",
            "      else:",
            "        raise ValueError('Improperly formatted model config.')",
            "",
            "      if inbound_layer_name != node_module._CONSTANT_VALUE:",
            "        inbound_layer = created_layers[inbound_layer_name]",
            "        inbound_node_index = get_node_index(inbound_layer, inbound_node_index)",
            "",
            "        if inbound_node_index is None:",
            "          add_unprocessed_node(layer, node_data)",
            "          return",
            "        inbound_node = inbound_layer._inbound_nodes[inbound_node_index]",
            "        input_tensors.append(",
            "            nest.flatten(inbound_node.outputs)[inbound_tensor_index])",
            "      else:",
            "        # We received a constant w/ no Keras history attached",
            "        input_tensors.append(inbound_tensor_index)",
            "    input_tensors = nest.pack_sequence_as(node_data, input_tensors)",
            "    # Call layer on its inputs, thus creating the node",
            "    # and building the layer if needed.",
            "    if input_tensors is not None:",
            "      if not layer._preserve_input_structure_in_config:",
            "        input_tensors = (",
            "            base_layer_utils.unnest_if_single_tensor(input_tensors))",
            "      output_tensors = layer(input_tensors, **kwargs)",
            "",
            "      # Update node index map.",
            "      output_index = nest.flatten(output_tensors)[0]._keras_history.node_index",
            "      node_index_map[(layer.name, node_count_by_layer[layer])] = output_index",
            "      node_count_by_layer[layer] += 1",
            "",
            "  def process_layer(layer_data):",
            "    \"\"\"Deserializes a layer, then call it on appropriate inputs.",
            "",
            "    Args:",
            "        layer_data: layer config dict.",
            "",
            "    Raises:",
            "        ValueError: In case of improperly formatted `layer_data` dict.",
            "    \"\"\"",
            "    layer_name = layer_data['name']",
            "",
            "    if layer_name in created_layers:",
            "      layer = created_layers[layer_name]",
            "    else:",
            "      # Instantiate layer.",
            "      from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top",
            "",
            "      layer = deserialize_layer(layer_data, custom_objects=custom_objects)",
            "      created_layers[layer_name] = layer",
            "",
            "    node_count_by_layer[layer] = int(_should_skip_first_node(layer))",
            "",
            "    # Gather layer inputs and convert to `ListWrapper` objects.",
            "    inbound_nodes_data = layer_data['inbound_nodes']",
            "    inbound_nodes_data = tf_utils.convert_inner_node_data(",
            "        inbound_nodes_data, wrap=True)",
            "    for node_data in inbound_nodes_data:",
            "      # We don't process nodes (i.e. make layer calls)",
            "      # on the fly because the inbound node may not yet exist,",
            "      # in case of layer shared at different topological depths",
            "      # (e.g. a model such as A(B(A(B(x)))))",
            "      add_unprocessed_node(layer, node_data)",
            "",
            "  # First, we create all layers and enqueue nodes to be processed",
            "  for layer_data in config['layers']:",
            "    process_layer(layer_data)",
            "  # Then we process nodes in order of layer depth.",
            "  # Nodes that cannot yet be processed (if the inbound node",
            "  # does not yet exist) are re-enqueued, and the process",
            "  # is repeated until all nodes are processed.",
            "  while unprocessed_nodes:",
            "    for layer_data in config['layers']:",
            "      layer = created_layers[layer_data['name']]",
            "      if layer in unprocessed_nodes:",
            "        for node_data in unprocessed_nodes.pop(layer):",
            "          process_node(layer, node_data)",
            "",
            "  input_tensors = []",
            "  output_tensors = []",
            "",
            "  input_layers = tf_utils.convert_inner_node_data(",
            "      config['input_layers'], wrap=True)",
            "  for layer_data in nest.flatten(input_layers):",
            "    layer_name, node_index, tensor_index = layer_data.as_list()",
            "    assert layer_name in created_layers",
            "    layer = created_layers[layer_name]",
            "    node_index = get_node_index(layer, node_index)",
            "    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors",
            "    input_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])",
            "",
            "  output_layers = tf_utils.convert_inner_node_data(",
            "      config['output_layers'], wrap=True)",
            "  for layer_data in nest.flatten(output_layers):",
            "    layer_name, node_index, tensor_index = layer_data.as_list()",
            "    assert layer_name in created_layers",
            "    layer = created_layers[layer_name]",
            "    node_index = get_node_index(layer, node_index)",
            "    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors",
            "    output_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])",
            "",
            "  input_tensors = nest.pack_sequence_as(input_layers, input_tensors)",
            "  output_tensors = nest.pack_sequence_as(output_layers, output_tensors)",
            "  return input_tensors, output_tensors, created_layers",
            "",
            "",
            "def get_network_config(network, serialize_layer_fn=None):",
            "  \"\"\"Builds the config, which consists of the node graph and serialized layers.",
            "",
            "  Args:",
            "    network: A Network object.",
            "    serialize_layer_fn: Function used to serialize layers.",
            "",
            "  Returns:",
            "    Config dictionary.",
            "  \"\"\"",
            "  serialize_layer_fn = (",
            "      serialize_layer_fn or generic_utils.serialize_keras_object)",
            "  config = {",
            "      'name': network.name,",
            "  }",
            "  node_conversion_map = {}",
            "  for layer in network.layers:",
            "    kept_nodes = 1 if _should_skip_first_node(layer) else 0",
            "    for original_node_index, node in enumerate(layer._inbound_nodes):",
            "      node_key = _make_node_key(layer.name, original_node_index)",
            "      if node_key in network._network_nodes:",
            "        node_conversion_map[node_key] = kept_nodes",
            "        kept_nodes += 1",
            "  layer_configs = []",
            "",
            "  with generic_utils.SharedObjectSavingScope():",
            "    for layer in network.layers:  # From the earliest layers on.",
            "      filtered_inbound_nodes = []",
            "      for original_node_index, node in enumerate(layer._inbound_nodes):",
            "        node_key = _make_node_key(layer.name, original_node_index)",
            "        if node_key in network._network_nodes and not node.is_input:",
            "          # The node is relevant to the model:",
            "          # add to filtered_inbound_nodes.",
            "          node_data = node.serialize(_make_node_key, node_conversion_map)",
            "          filtered_inbound_nodes.append(node_data)",
            "",
            "      layer_config = serialize_layer_fn(layer)",
            "      layer_config['name'] = layer.name",
            "      layer_config['inbound_nodes'] = filtered_inbound_nodes",
            "      layer_configs.append(layer_config)",
            "    config['layers'] = layer_configs",
            "",
            "  # Gather info about inputs and outputs.",
            "  model_inputs = []",
            "  for i in range(len(network._input_layers)):",
            "    layer, node_index, tensor_index = network._input_coordinates[i]",
            "    node_key = _make_node_key(layer.name, node_index)",
            "    if node_key not in network._network_nodes:",
            "      continue",
            "    new_node_index = node_conversion_map[node_key]",
            "    model_inputs.append(",
            "        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))",
            "  model_inputs = nest.pack_sequence_as(network._nested_inputs, model_inputs)",
            "  # Preserve external Keras compat for Models with single input.",
            "  if not nest.is_nested(model_inputs):",
            "    model_inputs = [model_inputs]",
            "  model_inputs = tf_utils.convert_inner_node_data(model_inputs)",
            "  config['input_layers'] = model_inputs",
            "",
            "  model_outputs = []",
            "  for i in range(len(network._output_layers)):",
            "    layer, node_index, tensor_index = network._output_coordinates[i]",
            "    node_key = _make_node_key(layer.name, node_index)",
            "    if node_key not in network._network_nodes:",
            "      continue",
            "    new_node_index = node_conversion_map[node_key]",
            "    model_outputs.append(",
            "        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))",
            "  model_outputs = nest.pack_sequence_as(network._nested_outputs, model_outputs)",
            "  # Preserve external Keras compat for Models with single output.",
            "  if not nest.is_nested(model_outputs):",
            "    model_outputs = [model_outputs]",
            "  model_outputs = tf_utils.convert_inner_node_data(model_outputs)",
            "  config['output_layers'] = model_outputs",
            "  return config",
            "",
            "",
            "def shape_with_no_batch_size(x):",
            "  if x.shape.rank is None:",
            "    return None",
            "  shape = x.shape.as_list()",
            "  if shape:",
            "    shape[0] = None",
            "  return shape",
            "",
            "",
            "class ModuleWrapper(base_layer.Layer):",
            "  \"\"\"Wrapper for `tf.Module`s to support the Functional and Sequential API.\"\"\"",
            "",
            "  def __init__(self, module, method_name=None, **kwargs):",
            "    \"\"\"Initializes the wrapper Layer for this module.",
            "",
            "    Args:",
            "      module: The `tf.Module` instance to be wrapped.",
            "      method_name: (Optional) str. The name of the method to use as the forward",
            "        pass of the module. If not set, defaults to '__call__' if defined, or",
            "        'call'.",
            "      **kwargs: Additional keywrod arguments. See `tf.keras.layers.Layer`.",
            "",
            "    Raises:",
            "      ValueError: If `method` is not defined on `module`.",
            "    \"\"\"",
            "    super(ModuleWrapper, self).__init__(**kwargs)",
            "    if method_name is None:",
            "      if hasattr(module, '__call__'):",
            "        method_name = '__call__'",
            "      elif hasattr(module, 'call'):",
            "        method_name = 'call'",
            "    if method_name is None or not hasattr(module, method_name):",
            "      raise ValueError('{} is not defined on object {}'.format(",
            "          method_name, module))",
            "",
            "    self._module = module",
            "    self._method_name = method_name",
            "",
            "    # Check if module.__call__ has a `training` arg or accepts `**kwargs`.",
            "    method = getattr(module, method_name)",
            "    method_arg_spec = tf_inspect.getfullargspec(method)",
            "    self._expects_training_arg = ('training' in method_arg_spec.args or",
            "                                  method_arg_spec.varkw is not None)",
            "    self._expects_mask_arg = ('mask' in method_arg_spec.args or",
            "                              method_arg_spec.varkw is not None)",
            "",
            "  def call(self, *args, **kwargs):",
            "    if 'training' in kwargs and not self._expects_training_arg:",
            "      kwargs.pop('training')",
            "    if 'mask' in kwargs and not self._expects_mask_arg:",
            "      kwargs.pop('mask')",
            "    return getattr(self._module, self._method_name)(*args, **kwargs)"
        ],
        "afterPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "# pylint: disable=protected-access",
            "\"\"\"A `Network` is way to compose layers: the topological form of a `Model`.\"\"\"",
            "",
            "import collections",
            "import copy",
            "import itertools",
            "import warnings",
            "",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.keras import backend",
            "from tensorflow.python.keras.engine import base_layer",
            "from tensorflow.python.keras.engine import base_layer_utils",
            "from tensorflow.python.keras.engine import input_layer as input_layer_module",
            "from tensorflow.python.keras.engine import input_spec",
            "from tensorflow.python.keras.engine import node as node_module",
            "from tensorflow.python.keras.engine import training as training_lib",
            "from tensorflow.python.keras.engine import training_utils",
            "from tensorflow.python.keras.saving.saved_model import network_serialization",
            "from tensorflow.python.keras.utils import generic_utils",
            "from tensorflow.python.keras.utils import tf_inspect",
            "from tensorflow.python.keras.utils import tf_utils",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.training.tracking import base as trackable",
            "from tensorflow.python.util import nest",
            "from tensorflow.tools.docs import doc_controls",
            "",
            "",
            "# pylint: disable=g-classes-have-attributes",
            "class Functional(training_lib.Model):",
            "  \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.",
            "",
            "  Three types of `Model` exist: subclassed `Model`, `Functional` model,",
            "  and `Sequential` (a special case of `Functional`).",
            "  In general, more Keras features are supported with `Functional`",
            "  than with subclassed `Model`s, specifically:",
            "",
            "  - Model cloning (`keras.models.clone`)",
            "  - Serialization (`model.get_config()/from_config`, `model.to_json()`",
            "  - Whole-model saving (`model.save()`)",
            "",
            "  A `Functional` model can be instantiated by passing two arguments to",
            "  `__init__`. The first argument is the `keras.Input` Tensors that represent",
            "  the inputs to the model. The second argument specifies the output",
            "  tensors that represent the outputs of this model. Both arguments can be a",
            "  nested structure of tensors.",
            "",
            "  Example:",
            "",
            "  ```",
            "  inputs = {'x1': keras.Input(shape=(10,)), 'x2': keras.Input(shape=(1,))}",
            "  t = keras.layers.Dense(1, activation='relu')(inputs['x1'])",
            "  outputs = keras.layers.Add()([t, inputs['x2'])",
            "  model = keras.Model(inputs, outputs)",
            "  ```",
            "",
            "  A `Functional` model constructed using the Functional API can also include raw",
            "  TensorFlow functions, with the exception of functions that create Variables",
            "  or assign ops.",
            "",
            "  Example:",
            "",
            "  ```",
            "  inputs = keras.Input(shape=(10,))",
            "  x = keras.layers.Dense(1)(inputs)",
            "  outputs = tf.nn.relu(x)",
            "  model = keras.Model(inputs, outputs)",
            "  ```",
            "",
            "  Args:",
            "    inputs: List of input tensors (must be created via `tf.keras.Input()`).",
            "    outputs: List of output tensors.",
            "    name: String, optional. Name of the model.",
            "    trainable: Boolean, optional. If the model's variables should be trainable.",
            "  \"\"\"",
            "",
            "  # See tf.Module for the usage of this property.",
            "  # The key of _layer_call_argspecs is a layer. tf.Module._flatten will fail to",
            "  # flatten the key since it is trying to convert Trackable/Layer to a string.",
            "  _TF_MODULE_IGNORED_PROPERTIES = frozenset(itertools.chain(",
            "      ('_layer_call_argspecs', '_compiled_trainable_state',",
            "       '_output_mask_cache', '_output_tensor_cache', '_output_shape_cache'),",
            "      training_lib.Model._TF_MODULE_IGNORED_PROPERTIES",
            "  ))",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def __init__(self, inputs, outputs, name=None, trainable=True,",
            "               **kwargs):",
            "    # This is used by the Model class, since we have some logic to swap the",
            "    # class in the __new__ method, which will lead to __init__ get invoked",
            "    # twice. Using the skip_init to skip one of the invocation of __init__ to",
            "    # avoid any side effects",
            "    skip_init = kwargs.pop('skip_init', False)",
            "    if skip_init:",
            "      return",
            "    generic_utils.validate_kwargs(kwargs, {})",
            "    super(Functional, self).__init__(name=name, trainable=trainable)",
            "    self._init_graph_network(inputs, outputs)",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _init_graph_network(self, inputs, outputs):",
            "    base_layer.keras_api_gauge.get_cell('Functional').set(True)",
            "    # This method is needed for Sequential to reinitialize graph network when",
            "    # layer is added or removed.",
            "    self._is_graph_network = True",
            "",
            "    # Normalize and set self.inputs, self.outputs.",
            "    if isinstance(inputs, list) and len(nest.flatten(inputs)) == 1:",
            "      inputs = inputs[0]",
            "    if isinstance(outputs, list) and len(nest.flatten(outputs)) == 1:",
            "      outputs = outputs[0]",
            "    self._nested_inputs = inputs",
            "    self._nested_outputs = outputs",
            "    self.inputs = nest.flatten(inputs)",
            "    self.outputs = nest.flatten(outputs)",
            "",
            "    # Models constructed with a single Tensor or list of Tensors can",
            "    # be called with a dict, where the keys of the dict are the names",
            "    # of the `Input` objects. Extra keys are ignored with warning.",
            "    if not nest.is_nested(self._nested_inputs):",
            "      self._enable_dict_to_input_mapping = True",
            "    elif (isinstance(self._nested_inputs, (list, tuple)) and",
            "          not any(nest.is_nested(t) for t in self._nested_inputs)):",
            "      self._enable_dict_to_input_mapping = True",
            "    elif (isinstance(self._nested_inputs, dict) and",
            "          not any(nest.is_nested(t) for t in self._nested_inputs.values())):",
            "      self._enable_dict_to_input_mapping = True",
            "    else:",
            "      self._enable_dict_to_input_mapping = False",
            "",
            "    if not ops.executing_eagerly_outside_functions():",
            "      if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):",
            "        base_layer_utils.create_keras_history(self._nested_outputs)",
            "",
            "    self._validate_graph_inputs_and_outputs()",
            "",
            "    # A Network does not create weights of its own, thus it is already",
            "    # built.",
            "    self.built = True",
            "    self._build_input_shape = nest.map_structure(lambda x: x.shape, inputs)",
            "    self._compute_output_and_mask_jointly = True",
            "    # `_expects_training_arg` is True since the `training` argument is always",
            "    # present in the signature of the `call` method of a graph network.",
            "    self._expects_training_arg = True",
            "    self._expects_mask_arg = True",
            "    # A graph network does not autocast inputs, as its layers will cast them",
            "    # instead.",
            "    self._autocast = False",
            "",
            "    self._input_layers = []",
            "    self._output_layers = []",
            "    self._input_coordinates = []",
            "    self._output_coordinates = []",
            "",
            "    # This is for performance optimization when calling the Network on new",
            "    # inputs. Every time the Network is called on a set on input tensors,",
            "    # we compute the output tensors, output masks and output shapes in one pass,",
            "    # then cache them here. When any of these outputs is queried later, we",
            "    # retrieve it from there instead of recomputing it.",
            "    self._output_mask_cache = {}",
            "    self._output_tensor_cache = {}",
            "    self._output_shape_cache = {}",
            "",
            "    # Build self._output_layers:",
            "    for x in self.outputs:",
            "      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access",
            "      self._output_layers.append(layer)",
            "      self._output_coordinates.append((layer, node_index, tensor_index))",
            "",
            "    # Build self._input_layers:",
            "    for x in self.inputs:",
            "      layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access",
            "      # It's supposed to be an input layer, so only one node",
            "      # and one tensor output.",
            "      assert node_index == 0",
            "      assert tensor_index == 0",
            "      self._input_layers.append(layer)",
            "      self._input_coordinates.append((layer, node_index, tensor_index))",
            "",
            "    # Keep track of the network's nodes and layers.",
            "    nodes, nodes_by_depth, layers, _ = _map_graph_network(",
            "        self.inputs, self.outputs)",
            "    self._network_nodes = nodes",
            "    self._nodes_by_depth = nodes_by_depth",
            "    self._self_tracked_trackables = layers",
            "    self._layer_call_argspecs = {}",
            "    for layer in self._self_tracked_trackables:",
            "      self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)",
            "",
            "    # Build self.input_names and self.output_names.",
            "    self._set_output_names()",
            "    self.input_names = []",
            "    self._feed_input_names = []",
            "    self._feed_inputs = []",
            "    self._feed_input_shapes = []",
            "    for layer in self._input_layers:",
            "      self.input_names.append(layer.name)",
            "      if layer.is_placeholder:",
            "        self._feed_input_names.append(layer.name)",
            "        # Use batch_input_shape here because non-eager composite tensors may not",
            "        # have a shape attribute that's meaningful (sparse, for instance, has",
            "        # a tensor that's non-constant and needs to be fed). This means that",
            "        # input layers that create placeholders will need to have the",
            "        # batch_input_shape attr to allow for input shape validation.",
            "        self._feed_input_shapes.append(layer._batch_input_shape)",
            "        self._feed_inputs.append(layer.input)",
            "",
            "    self._compute_tensor_usage_count()",
            "    self._set_save_spec(self._nested_inputs)",
            "    tf_utils.assert_no_legacy_layers(self.layers)",
            "",
            "  @property",
            "  def input(self):",
            "    \"\"\"Retrieves the input tensor(s) of a layer.",
            "",
            "    Only applicable if the layer has exactly one input,",
            "    i.e. if it is connected to one incoming layer.",
            "",
            "    Returns:",
            "        Input tensor or list of input tensors.",
            "",
            "    Raises:",
            "      RuntimeError: If called in Eager mode.",
            "      AttributeError: If no inbound nodes are found.",
            "    \"\"\"",
            "    return self._nested_inputs",
            "",
            "  @property",
            "  def input_shape(self):",
            "    \"\"\"Retrieves the input shape(s) of a layer.",
            "",
            "    Only applicable if the layer has exactly one input,",
            "    i.e. if it is connected to one incoming layer, or if all inputs",
            "    have the same shape.",
            "",
            "    Returns:",
            "        Input shape, as an integer shape tuple",
            "        (or list of shape tuples, one tuple per input tensor).",
            "",
            "    Raises:",
            "        AttributeError: if the layer has no defined input_shape.",
            "        RuntimeError: if called in Eager mode.",
            "    \"\"\"",
            "    return nest.map_structure(backend.int_shape, self.input)",
            "",
            "  @property",
            "  def input_spec(self):",
            "    if hasattr(self, '_manual_input_spec'):",
            "      return self._manual_input_spec",
            "    if (isinstance(self._nested_inputs, (dict, list, tuple)) and",
            "        len(self._nested_inputs) != len(self.inputs)):",
            "      # Case where we have a nested structure.",
            "      # In such a case we can't safely run any checks.",
            "      return None",
            "    if isinstance(self._nested_inputs, dict):",
            "      # Case where `_nested_inputs` is a plain dict of Inputs.",
            "      names = sorted(self._nested_inputs.keys())",
            "      return [input_spec.InputSpec(",
            "          shape=shape_with_no_batch_size(self._nested_inputs[name]),",
            "          allow_last_axis_squeeze=True, name=name) for name in names]",
            "    else:",
            "      # Single input, or list / tuple of inputs.",
            "      # The data may be passed as a dict keyed by input name.",
            "      return [input_spec.InputSpec(",
            "          shape=shape_with_no_batch_size(x), allow_last_axis_squeeze=True,",
            "          name=x._keras_history.layer.name) for x in self.inputs]",
            "",
            "  @input_spec.setter",
            "  def input_spec(self, value):",
            "    self._manual_input_spec = value",
            "",
            "  @property",
            "  def output(self):",
            "    \"\"\"Retrieves the output tensor(s) of a layer.",
            "",
            "    Only applicable if the layer has exactly one output,",
            "    i.e. if it is connected to one incoming layer.",
            "",
            "    Returns:",
            "      Output tensor or list of output tensors.",
            "",
            "    Raises:",
            "      AttributeError: if the layer is connected to more than one incoming",
            "        layers.",
            "      RuntimeError: if called in Eager mode.",
            "    \"\"\"",
            "    return self._nested_outputs",
            "",
            "  @property",
            "  def output_shape(self):",
            "    \"\"\"Retrieves the output shape(s) of a layer.",
            "",
            "    Only applicable if the layer has one output,",
            "    or if all outputs have the same shape.",
            "",
            "    Returns:",
            "        Output shape, as an integer shape tuple",
            "        (or list of shape tuples, one tuple per output tensor).",
            "",
            "    Raises:",
            "        AttributeError: if the layer has no defined output shape.",
            "        RuntimeError: if called in Eager mode.",
            "    \"\"\"",
            "    return nest.map_structure(backend.int_shape, self.output)",
            "",
            "  def _set_output_names(self):",
            "    \"\"\"Assigns unique names to the Network's outputs.",
            "",
            "    Output layers with multiple output tensors would otherwise lead to duplicate",
            "    names in self.output_names.",
            "    \"\"\"",
            "    uniquified = []",
            "    output_names = set()",
            "    prefix_count = {}",
            "    for layer in self._output_layers:",
            "      proposal = layer.name",
            "      while proposal in output_names:",
            "        existing_count = prefix_count.get(layer.name, 1)",
            "        proposal = '{}_{}'.format(layer.name, existing_count)",
            "        prefix_count[layer.name] = existing_count + 1",
            "      output_names.add(proposal)",
            "      uniquified.append(proposal)",
            "    self.output_names = uniquified",
            "",
            "  @property",
            "  def _layer_checkpoint_dependencies(self):",
            "    \"\"\"Dictionary of layer dependencies to be included in the checkpoint.\"\"\"",
            "    weight_layer_index = 0",
            "",
            "    dependencies = collections.OrderedDict()",
            "    for layer_index, layer in enumerate(self.layers):",
            "      try:",
            "        if layer.weights:",
            "          # Keep a separate index for layers which have weights. This allows",
            "          # users to insert Layers without weights anywhere in the network",
            "          # without breaking checkpoints.",
            "          dependencies['layer_with_weights-%d' % weight_layer_index] = layer",
            "          weight_layer_index += 1",
            "      except ValueError:",
            "        # The layer might have weights, but may not be built yet. We just treat",
            "        # it as layer without weight.",
            "        pass",
            "",
            "      # Even if it doesn't have weights, we should still track everything in",
            "      # case it has/will have Trackable dependencies.",
            "      dependencies['layer-%d' % layer_index] = layer",
            "    return dependencies",
            "",
            "  @property",
            "  def _checkpoint_dependencies(self):",
            "    dependencies = [",
            "        trackable.TrackableReference(name=name, ref=layer)",
            "        for name, layer in self._layer_checkpoint_dependencies.items()]",
            "    dependencies.extend(super(Functional, self)._checkpoint_dependencies)",
            "    return dependencies",
            "",
            "  def _lookup_dependency(self, name):",
            "    layer_dependencies = self._layer_checkpoint_dependencies",
            "    if name in layer_dependencies:",
            "      return layer_dependencies[name]",
            "    return super(Functional, self)._lookup_dependency(name)",
            "",
            "  def _handle_deferred_layer_dependencies(self, layers):",
            "    \"\"\"Handles layer checkpoint dependencies that are added after init.\"\"\"",
            "    layer_checkpoint_dependencies = self._layer_checkpoint_dependencies",
            "    layer_to_name = {v: k for k, v in layer_checkpoint_dependencies.items()}",
            "    for layer in layers:",
            "      if layer in layer_to_name:",
            "        self._handle_deferred_dependencies(name=layer_to_name[layer],",
            "                                           trackable=layer)",
            "",
            "  @property",
            "  def _should_compute_mask(self):",
            "    return True",
            "",
            "  def compute_mask(self, inputs, mask):",
            "    # TODO(omalleyt): b/123540974 This function is not really safe to call",
            "    # by itself because it will duplicate any updates and losses in graph",
            "    # mode by `call`ing the Layers again.",
            "    output_tensors = self._run_internal_graph(inputs, mask=mask)",
            "    return nest.map_structure(lambda t: getattr(t, '_keras_mask', None),",
            "                              output_tensors)",
            "",
            "  @doc_controls.do_not_doc_inheritable",
            "  def call(self, inputs, training=None, mask=None):",
            "    \"\"\"Calls the model on new inputs.",
            "",
            "    In this case `call` just reapplies",
            "    all ops in the graph to the new inputs",
            "    (e.g. build a new computational graph from the provided inputs).",
            "",
            "    Args:",
            "        inputs: A tensor or list of tensors.",
            "        training: Boolean or boolean scalar tensor, indicating whether to run",
            "          the `Network` in training mode or inference mode.",
            "        mask: A mask or list of masks. A mask can be",
            "            either a tensor or None (no mask).",
            "",
            "    Returns:",
            "        A tensor if there is a single output, or",
            "        a list of tensors if there are more than one outputs.",
            "    \"\"\"",
            "    return self._run_internal_graph(",
            "        inputs, training=training, mask=mask)",
            "",
            "  def compute_output_shape(self, input_shape):",
            "    # Convert any shapes in tuple format to TensorShapes.",
            "    input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)",
            "",
            "    if len(nest.flatten(input_shape)) != len(nest.flatten(self._input_layers)):",
            "      raise ValueError('Invalid input_shape argument ' + str(input_shape) +",
            "                       ': model has ' + str(len(self._input_layers)) +",
            "                       ' tensor inputs.')",
            "",
            "    # Use the tuple of TensorShape as the cache key, since tuple is hashable",
            "    # and can be used as hash key.",
            "    try:",
            "      cache_key = tuple(tf_utils.convert_shapes(input_shape, to_tuples=True))",
            "      if cache_key in self._output_shape_cache:",
            "        # Cache hit. Return shapes as TensorShapes.",
            "        return self._output_shape_cache[cache_key]",
            "    except ValueError:",
            "      # In case there are unknown TensorShape, eg for sparse tensor input,",
            "      # We skip the caching since the shape is unknown.",
            "      pass",
            "",
            "    layers_to_output_shapes = {}",
            "    for layer, shape in zip(self._input_layers, nest.flatten(input_shape)):",
            "      # It's an input layer: then `compute_output_shape` is identity,",
            "      # and there is only one node and one tensor..",
            "      shape_key = layer.name + '_0_0'",
            "      layers_to_output_shapes[shape_key] = shape",
            "",
            "    depth_keys = list(self._nodes_by_depth.keys())",
            "    depth_keys.sort(reverse=True)",
            "    # Iterate over nodes, by depth level.",
            "    if len(depth_keys) > 1:",
            "      for depth in depth_keys:",
            "        nodes = self._nodes_by_depth[depth]",
            "        for node in nodes:",
            "          layer = node.layer",
            "          if layer in self._input_layers:",
            "            # We've already covered the input layers",
            "            # a few lines above.",
            "            continue",
            "          # Get the input shapes for the first argument of the node",
            "          layer_input_shapes = []",
            "          layer_inputs = node.call_args[0]",
            "          for layer_input in nest.flatten(layer_inputs):",
            "            kh = layer_input._keras_history",
            "            input_layer_key = kh.layer.name + '_%s_%s' % (kh.node_index,",
            "                                                          kh.tensor_index)",
            "            layer_input_shapes.append(layers_to_output_shapes[input_layer_key])",
            "          layer_input_shapes = nest.pack_sequence_as(layer_inputs,",
            "                                                     layer_input_shapes)",
            "          # Layers expect shapes to be tuples for `compute_output_shape`.",
            "          layer_input_shapes = tf_utils.convert_shapes(",
            "              layer_input_shapes, to_tuples=True)",
            "          layer_output_shapes = layer.compute_output_shape(layer_input_shapes)",
            "          # Convert back to TensorShapes.",
            "          layer_output_shapes = tf_utils.convert_shapes(",
            "              layer_output_shapes, to_tuples=False)",
            "",
            "          node_index = layer._inbound_nodes.index(node)  # pylint: disable=protected-access",
            "          for j, shape in enumerate(nest.flatten(layer_output_shapes)):",
            "            shape_key = layer.name + '_%s_%s' % (node_index, j)",
            "            layers_to_output_shapes[shape_key] = shape",
            "",
            "      # Read final output shapes from layers_to_output_shapes.",
            "      output_shapes = []",
            "      for i in range(len(self._output_layers)):",
            "        layer, node_index, tensor_index = self._output_coordinates[i]",
            "        shape_key = layer.name + '_%s_%s' % (node_index, tensor_index)",
            "        output_shapes.append(layers_to_output_shapes[shape_key])",
            "      output_shapes = nest.pack_sequence_as(self._nested_outputs, output_shapes)",
            "      # Store in cache.",
            "      self._output_shape_cache[cache_key] = output_shapes",
            "",
            "    # Return shapes as TensorShapes.",
            "    return output_shapes",
            "",
            "  def _init_set_name(self, name, zero_based=True):",
            "    if not name:",
            "      cls_name = self.__class__.__name__",
            "      if self.__class__ == Functional:",
            "        # Hide the functional class name from user, since its not a public",
            "        # visible class. Use \"Model\" instead,",
            "        cls_name = 'Model'",
            "      self._name = backend.unique_object_name(",
            "          generic_utils.to_snake_case(cls_name),",
            "          zero_based=zero_based)",
            "    else:",
            "      self._name = name",
            "",
            "  def _run_internal_graph(self, inputs, training=None, mask=None):",
            "    \"\"\"Computes output tensors for new inputs.",
            "",
            "    # Note:",
            "        - Can be run on non-Keras tensors.",
            "",
            "    Args:",
            "        inputs: Tensor or nested structure of Tensors.",
            "        training: Boolean learning phase.",
            "        mask: (Optional) Tensor or nested structure of Tensors.",
            "",
            "    Returns:",
            "        output_tensors",
            "    \"\"\"",
            "    inputs = self._flatten_to_reference_inputs(inputs)",
            "    if mask is None:",
            "      masks = [None] * len(inputs)",
            "    else:",
            "      masks = self._flatten_to_reference_inputs(mask)",
            "    for input_t, mask in zip(inputs, masks):",
            "      input_t._keras_mask = mask",
            "",
            "    # Dictionary mapping reference tensors to computed tensors.",
            "    tensor_dict = {}",
            "    tensor_usage_count = self._tensor_usage_count",
            "    for x, y in zip(self.inputs, inputs):",
            "      y = self._conform_to_reference_input(y, ref_input=x)",
            "      x_id = str(id(x))",
            "      tensor_dict[x_id] = [y] * tensor_usage_count[x_id]",
            "",
            "    nodes_by_depth = self._nodes_by_depth",
            "    depth_keys = list(nodes_by_depth.keys())",
            "    depth_keys.sort(reverse=True)",
            "",
            "    for depth in depth_keys:",
            "      nodes = nodes_by_depth[depth]",
            "      for node in nodes:",
            "        if node.is_input:",
            "          continue  # Input tensors already exist.",
            "",
            "        if any(t_id not in tensor_dict for t_id in node.flat_input_ids):",
            "          continue  # Node is not computable, try skipping.",
            "",
            "        args, kwargs = node.map_arguments(tensor_dict)",
            "        outputs = node.layer(*args, **kwargs)",
            "",
            "        # Update tensor_dict.",
            "        for x_id, y in zip(node.flat_output_ids, nest.flatten(outputs)):",
            "          tensor_dict[x_id] = [y] * tensor_usage_count[x_id]",
            "",
            "    output_tensors = []",
            "    for x in self.outputs:",
            "      x_id = str(id(x))",
            "      assert x_id in tensor_dict, 'Could not compute output ' + str(x)",
            "      output_tensors.append(tensor_dict[x_id].pop())",
            "",
            "    return nest.pack_sequence_as(self._nested_outputs, output_tensors)",
            "",
            "  def _flatten_to_reference_inputs(self, tensors):",
            "    \"\"\"Maps `tensors` to their respective `keras.Input`.\"\"\"",
            "    if self._enable_dict_to_input_mapping and isinstance(tensors, dict):",
            "      ref_inputs = self._nested_inputs",
            "      if not nest.is_nested(ref_inputs):",
            "        ref_inputs = [self._nested_inputs]",
            "      if isinstance(ref_inputs, dict):",
            "        # In the case that the graph is constructed with dict input tensors,",
            "        # We will use the original dict key to map with the keys in the input",
            "        # data. Note that the model.inputs is using nest.flatten to process the",
            "        # input tensors, which means the dict input tensors are ordered by their",
            "        # keys.",
            "        ref_input_names = sorted(ref_inputs.keys())",
            "      else:",
            "        ref_input_names = [inp._keras_history.layer.name for inp in ref_inputs]",
            "",
            "      # Raise an warning if there are more input data comparing to input tensor",
            "      if len(tensors) > len(ref_input_names):",
            "        warnings.warn(",
            "            'Input dict contained keys {} which did not match any model input. '",
            "            'They will be ignored by the model.'.format(",
            "                [n for n in tensors.keys() if n not in ref_input_names])",
            "            )",
            "",
            "      try:",
            "        # Flatten in the order `Input`s were passed during Model construction.",
            "        return [tensors[n] for n in ref_input_names]",
            "      except KeyError:",
            "        # TODO(b/151582614)",
            "        return nest.flatten(tensors)",
            "",
            "    # Otherwise both self.inputs and tensors will already be in same order.",
            "    return nest.flatten(tensors)",
            "",
            "  def _conform_to_reference_input(self, tensor, ref_input):",
            "    \"\"\"Set shape and dtype based on `keras.Input`s.\"\"\"",
            "    if isinstance(tensor, ops.Tensor):",
            "      # Allow (None,) and (None, 1) Tensors to be passed interchangeably. Use",
            "      # the shape specified by the `keras.Input`.",
            "      t_shape = tensor.shape",
            "      t_rank = t_shape.rank",
            "      ref_shape = ref_input.shape",
            "      ref_rank = ref_shape.rank",
            "      keras_history = getattr(tensor, '_keras_history', None)",
            "      if t_rank is not None and ref_rank is not None:",
            "        # Should squeeze last dimension.",
            "        # True if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).",
            "        if (t_rank == ref_rank + 1 and t_shape[-1] == 1):",
            "          tensor = array_ops.squeeze_v2(tensor, axis=-1)",
            "        # Should expand last_dimension.",
            "        # True if tensor is (BATCH, ...) and reference is (BATCH, ..., 1).",
            "        elif (t_rank == ref_rank - 1 and ref_shape[-1] == 1):",
            "          tensor = array_ops.expand_dims_v2(tensor, axis=-1)",
            "      if keras_history is not None:  # Restore keras history.",
            "        tensor._keras_history = keras_history",
            "",
            "      # Add shape hints to Tensors that may have None shape dims but have shapes",
            "      # defined by the `keras.Input` (not applicable in eager mode).",
            "      if not context.executing_eagerly():",
            "        try:",
            "          tensor.set_shape(tensor.shape.merge_with(ref_input.shape))",
            "        except ValueError:",
            "          logging.warning(",
            "              'Model was constructed with shape {} for input {}, but it was '",
            "              'called on an input with incompatible shape {}.'.format(",
            "                  ref_input.shape, ref_input, tensor.shape))",
            "",
            "      # Dtype casting.",
            "      tensor = math_ops.cast(tensor, dtype=ref_input.dtype)",
            "    elif tf_utils.is_extension_type(tensor):",
            "      # Dtype casting (If the extension type has a non-variant dtype and",
            "      # supports being cast)",
            "      ref_input_dtype = getattr(ref_input, 'dtype', None)",
            "      if ref_input_dtype is not None and ref_input_dtype != dtypes.variant:",
            "        tensor = math_ops.cast(tensor, dtype=ref_input_dtype)",
            "",
            "    return tensor",
            "",
            "  def get_config(self):",
            "    return copy.deepcopy(get_network_config(self))",
            "",
            "  @classmethod",
            "  def from_config(cls, config, custom_objects=None):",
            "    \"\"\"Instantiates a Model from its config (output of `get_config()`).",
            "",
            "    Args:",
            "        config: Model config dictionary.",
            "        custom_objects: Optional dictionary mapping names",
            "            (strings) to custom classes or functions to be",
            "            considered during deserialization.",
            "",
            "    Returns:",
            "        A model instance.",
            "",
            "    Raises:",
            "        ValueError: In case of improperly formatted config dict.",
            "    \"\"\"",
            "    with generic_utils.SharedObjectLoadingScope():",
            "      input_tensors, output_tensors, created_layers = reconstruct_from_config(",
            "          config, custom_objects)",
            "      model = cls(inputs=input_tensors, outputs=output_tensors,",
            "                  name=config.get('name'))",
            "      connect_ancillary_layers(model, created_layers)",
            "      return model",
            "",
            "  def _validate_graph_inputs_and_outputs(self):",
            "    \"\"\"Validates the inputs and outputs of a Graph Network.\"\"\"",
            "    # Check for redundancy in inputs.",
            "    if len({id(i) for i in self.inputs}) != len(self.inputs):",
            "      raise ValueError('The list of inputs passed to the model '",
            "                       'is redundant. '",
            "                       'All inputs should only appear once.'",
            "                       ' Found: ' + str(self.inputs))",
            "",
            "    for x in self.inputs:",
            "      # Check that x has appropriate `_keras_history` metadata.",
            "      if not hasattr(x, '_keras_history'):",
            "        cls_name = self.__class__.__name__",
            "        raise ValueError('Input tensors to a ' + cls_name + ' ' +",
            "                         'must come from `tf.keras.Input`. '",
            "                         'Received: ' + str(x) +",
            "                         ' (missing previous layer metadata).')",
            "      # Check that x is an input tensor.",
            "      # pylint: disable=protected-access",
            "      layer = x._keras_history.layer",
            "      if len(layer._inbound_nodes) > 1 or (",
            "          layer._inbound_nodes and not layer._inbound_nodes[0].is_input):",
            "        cls_name = self.__class__.__name__",
            "        logging.warning(cls_name + ' model inputs must come from '",
            "                        '`tf.keras.Input` (thus holding past layer metadata), '",
            "                        'they cannot be the output of '",
            "                        'a previous non-Input layer. '",
            "                        'Here, a tensor specified as '",
            "                        'input to \"' + self.name + '\" was not an Input tensor, '",
            "                        'it was generated by layer ' + layer.name + '.\\n'",
            "                        'Note that input tensors are '",
            "                        'instantiated via `tensor = tf.keras.Input(shape)`.\\n'",
            "                        'The tensor that caused the issue was: ' + str(x.name))",
            "",
            "    # Check compatibility of batch sizes of Input Layers.",
            "    input_batch_sizes = [",
            "        training_utils.get_static_batch_size(x._keras_history.layer)",
            "        for x in self.inputs",
            "    ]",
            "    consistent_batch_size = None",
            "    for batch_size in input_batch_sizes:",
            "      if batch_size is not None:",
            "        if (consistent_batch_size is not None and",
            "            batch_size != consistent_batch_size):",
            "          raise ValueError('The specified batch sizes of the Input Layers'",
            "                           ' are incompatible. Found batch sizes: {}'.format(",
            "                               input_batch_sizes))",
            "        consistent_batch_size = batch_size",
            "",
            "    for x in self.outputs:",
            "      if not hasattr(x, '_keras_history'):",
            "        cls_name = self.__class__.__name__",
            "        raise ValueError('Output tensors of a ' + cls_name + ' model must be '",
            "                         'the output of a TensorFlow `Layer` '",
            "                         '(thus holding past layer metadata). Found: ' + str(x))",
            "",
            "  def _insert_layers(self, layers, relevant_nodes=None):",
            "    \"\"\"Inserts Layers into the Network after Network creation.",
            "",
            "    This is only valid for Keras Graph Networks.  Layers added via this function",
            "    will be included in the `call` computation and `get_config` of this Network.",
            "    They will not be added to the Network's outputs.",
            "",
            "",
            "    Args:",
            "      layers: Arbitrary nested structure of Layers. Layers must be reachable",
            "        from one or more of the `keras.Input` Tensors that correspond to this",
            "        Network's inputs.",
            "      relevant_nodes: Nodes from the Layers that should be considered part of",
            "        this Network. If `None`, all Nodes will be considered part of this",
            "        Network.",
            "",
            "    Raises:",
            "      ValueError: If the layers depend on `Input`s not found in this Model.",
            "    \"\"\"",
            "    layers = nest.flatten(layers)",
            "    tf_utils.assert_no_legacy_layers(layers)",
            "    node_to_depth = {}",
            "    for depth, nodes in self._nodes_by_depth.items():",
            "      node_to_depth.update({node: depth for node in nodes})",
            "    # The nodes of these Layers that are relevant to this Network. If not",
            "    # provided, assume all Nodes are relevant",
            "    if not relevant_nodes:",
            "      relevant_nodes = nest.flatten([layer._inbound_nodes for layer in layers])",
            "    network_nodes = set(relevant_nodes + list(node_to_depth.keys()))",
            "",
            "    def _get_min_depth(node):",
            "      \"\"\"Gets the minimum depth at which node can be computed.\"\"\"",
            "      min_depth = 0",
            "      for layer, node_id, _, _ in node.iterate_inbound():",
            "        inbound_node = layer._inbound_nodes[node_id]",
            "        if inbound_node in node_to_depth:",
            "          min_depth = min(min_depth, node_to_depth[inbound_node])",
            "        elif inbound_node not in network_nodes:",
            "          continue",
            "        else:",
            "          # Previous relevant nodes haven't been processed yet.",
            "          return None",
            "      # New node is one shallower than its shallowest input.",
            "      return min_depth - 1",
            "",
            "    # Insert nodes into `_nodes_by_depth` and other node attrs.",
            "    unprocessed_nodes = copy.copy(relevant_nodes)",
            "    i = 0",
            "    while unprocessed_nodes:",
            "      i += 1",
            "      # Do a sanity check. This can occur if `Input`s from outside this Model",
            "      # are being relied on.",
            "      if i > 10000:",
            "        raise ValueError('Layers could not be added due to missing '",
            "                         'dependencies.')",
            "",
            "      node = unprocessed_nodes.pop(0)",
            "      depth = _get_min_depth(node)",
            "      if depth is None:  # Defer until inbound nodes are processed.",
            "        unprocessed_nodes.append(node)",
            "        continue",
            "      node_key = _make_node_key(node.layer.name,",
            "                                node.layer._inbound_nodes.index(node))",
            "      if node_key not in self._network_nodes:",
            "        node_to_depth[node] = depth",
            "        self._network_nodes.add(node_key)",
            "        self._nodes_by_depth[depth].append(node)",
            "",
            "    # Insert layers and update other layer attrs.",
            "    layer_set = set(self._self_tracked_trackables)",
            "    deferred_layers = []",
            "    for layer in layers:",
            "      if layer not in layer_set:",
            "        self._self_tracked_trackables.append(layer)",
            "        deferred_layers.append(layer)",
            "        self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)",
            "        layer_set.add(layer)",
            "    self._handle_deferred_layer_dependencies(deferred_layers)",
            "",
            "    self._compute_tensor_usage_count()",
            "",
            "  def _compute_tensor_usage_count(self):",
            "    \"\"\"Compute the #. of tensor usages for all the output tensors of layers.",
            "",
            "    The computed tensor usage count is saved as `self._tensor_usage_count`. This",
            "    is later used for saving memory in eager computation by releasing",
            "    no-longer-needed tensors as early as possible.",
            "    \"\"\"",
            "    tensor_usage_count = collections.Counter()",
            "    available_tensors = set(str(id(tensor)) for tensor in self.inputs)",
            "",
            "    depth_keys = list(self._nodes_by_depth.keys())",
            "    depth_keys.sort(reverse=True)",
            "    depth_keys = depth_keys[1:]",
            "",
            "    for depth in depth_keys:",
            "      for node in self._nodes_by_depth[depth]:",
            "        input_tensors = {",
            "            str(id(tensor)) for tensor in nest.flatten(node.keras_inputs)",
            "        }",
            "        if input_tensors.issubset(available_tensors):",
            "          for tensor in nest.flatten(node.keras_inputs):",
            "            tensor_usage_count[str(id(tensor))] += 1",
            "",
            "          for output_tensor in nest.flatten(node.outputs):",
            "            available_tensors.add(str(id(output_tensor)))",
            "",
            "    for tensor in self.outputs:",
            "      tensor_usage_count[str(id(tensor))] += 1",
            "",
            "    self._tensor_usage_count = tensor_usage_count",
            "",
            "  def _assert_weights_created(self):",
            "    # Override the implementation in Model.",
            "    # The Functional model should always have weight created already.",
            "    return",
            "",
            "  def _graph_network_add_loss(self, symbolic_loss):",
            "    new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])",
            "    # Losses must be keyed on inputs no matter what in order to be supported in",
            "    # DistributionStrategy.",
            "    add_loss_layer = base_layer.AddLoss(",
            "        unconditional=False, dtype=symbolic_loss.dtype)",
            "    add_loss_layer(symbolic_loss)",
            "    new_nodes.extend(add_loss_layer.inbound_nodes)",
            "    new_layers.append(add_loss_layer)",
            "    self._insert_layers(new_layers, new_nodes)",
            "",
            "  def _graph_network_add_metric(self, value, aggregation, name):",
            "    new_nodes, new_layers = _map_subgraph_network(self.inputs, [value])",
            "    add_metric_layer = base_layer.AddMetric(",
            "        aggregation, name, dtype=value.dtype)",
            "    add_metric_layer(value)",
            "    new_nodes.extend(add_metric_layer.inbound_nodes)",
            "    new_layers.append(add_metric_layer)",
            "    self._insert_layers(new_layers, new_nodes)",
            "",
            "  @property",
            "  def _trackable_saved_model_saver(self):",
            "    return network_serialization.NetworkSavedModelSaver(self)",
            "",
            "  def _get_save_spec(self, dynamic_batch=True):",
            "    if getattr(self, '_has_explicit_input_shape', True):",
            "      # Functional models and Sequential models that have an explicit input",
            "      # shape should use the batch size set by the input layer.",
            "      dynamic_batch = False",
            "    return super(Functional, self)._get_save_spec(dynamic_batch)",
            "",
            "",
            "def _make_node_key(layer_name, node_index):",
            "  return layer_name + '_ib-' + str(node_index)",
            "",
            "",
            "def _map_graph_network(inputs, outputs):",
            "  \"\"\"Validates a network's topology and gather its layers and nodes.",
            "",
            "  Args:",
            "    inputs: List of input tensors.",
            "    outputs: List of outputs tensors.",
            "",
            "  Returns:",
            "    A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.",
            "    - nodes: list of Node instances.",
            "    - nodes_by_depth: dict mapping ints (depth) to lists of node instances.",
            "    - layers: list of Layer instances.",
            "    - layers_by_depth: dict mapping ints (depth) to lists of layer instances.",
            "",
            "  Raises:",
            "    ValueError: In case the network is not valid (e.g. disconnected graph).",
            "  \"\"\"",
            "  # \"depth\" is number of layers between output Node and the Node.",
            "  # Nodes are ordered from inputs -> outputs.",
            "  nodes_in_decreasing_depth, layer_indices = _build_map(outputs)",
            "  network_nodes = {",
            "      _make_node_key(node.layer.name, node.layer._inbound_nodes.index(node))",
            "      for node in nodes_in_decreasing_depth",
            "  }",
            "",
            "  nodes_depths = {}  # dict {node: depth value}",
            "  layers_depths = {}  # dict {layer: depth value}",
            "",
            "  for node in reversed(nodes_in_decreasing_depth):",
            "    # If the depth is not set, the node has no outbound nodes (depth 0).",
            "    depth = nodes_depths.setdefault(node, 0)",
            "",
            "    # Update the depth of the corresponding layer",
            "    previous_depth = layers_depths.get(node.layer, 0)",
            "    # If we've seen this layer before at a higher depth,",
            "    # we should use that depth instead of the node depth.",
            "    # This is necessary for shared layers that have inputs at different",
            "    # depth levels in the graph.",
            "    depth = max(depth, previous_depth)",
            "    layers_depths[node.layer] = depth",
            "    nodes_depths[node] = depth",
            "",
            "    # Update the depth of inbound nodes.",
            "    # The \"depth\" of a node is the max of the depths",
            "    # of all nodes it is connected to + 1.",
            "    for node_dep in node.parent_nodes:",
            "      previous_depth = nodes_depths.get(node_dep, 0)",
            "      nodes_depths[node_dep] = max(depth + 1, previous_depth)",
            "",
            "  # Handle inputs that are not connected to outputs.",
            "  # We do not error out here because the inputs may be used to compute losses",
            "  # and metrics.",
            "  for input_t in inputs:",
            "    input_layer = input_t._keras_history[0]",
            "    if input_layer not in layers_depths:",
            "      layers_depths[input_layer] = 0",
            "      layer_indices[input_layer] = -1",
            "      nodes_depths[input_layer._inbound_nodes[0]] = 0",
            "      network_nodes.add(_make_node_key(input_layer.name, 0))",
            "",
            "  # Build a dict {depth: list of nodes with this depth}",
            "  nodes_by_depth = collections.defaultdict(list)",
            "  for node, depth in nodes_depths.items():",
            "    nodes_by_depth[depth].append(node)",
            "",
            "  # Build a dict {depth: list of layers with this depth}",
            "  layers_by_depth = collections.defaultdict(list)",
            "  for layer, depth in layers_depths.items():",
            "    layers_by_depth[depth].append(layer)",
            "",
            "  # Get sorted list of layer depths.",
            "  depth_keys = list(layers_by_depth.keys())",
            "  depth_keys.sort(reverse=True)",
            "",
            "  # Set self.layers ordered by depth.",
            "  layers = []",
            "  for depth in depth_keys:",
            "    layers_for_depth = layers_by_depth[depth]",
            "    # Network.layers needs to have a deterministic order:",
            "    # here we order them by traversal order.",
            "    layers_for_depth.sort(key=lambda x: layer_indices[x])",
            "    layers.extend(layers_for_depth)",
            "",
            "  # Get sorted list of node depths.",
            "  depth_keys = list(nodes_by_depth.keys())",
            "  depth_keys.sort(reverse=True)",
            "",
            "  # Check that all tensors required are computable.",
            "  # computable_tensors: all tensors in the graph",
            "  # that can be computed from the inputs provided.",
            "  computable_tensors = set()",
            "  for x in inputs:",
            "    computable_tensors.add(id(x))",
            "",
            "  layers_with_complete_input = []  # To provide a better error msg.",
            "  for depth in depth_keys:",
            "    for node in nodes_by_depth[depth]:",
            "      layer = node.layer",
            "      if layer and not node.is_input:",
            "        for x in nest.flatten(node.keras_inputs):",
            "          if id(x) not in computable_tensors:",
            "            raise ValueError('Graph disconnected: '",
            "                             'cannot obtain value for tensor ' + str(x) +",
            "                             ' at layer \"' + layer.name + '\". '",
            "                             'The following previous layers '",
            "                             'were accessed without issue: ' +",
            "                             str(layers_with_complete_input))",
            "        for x in nest.flatten(node.outputs):",
            "          computable_tensors.add(id(x))",
            "        layers_with_complete_input.append(layer.name)",
            "",
            "  # Ensure name unicity, which will be crucial for serialization",
            "  # (since serialized nodes refer to layers by their name).",
            "  all_names = [layer.name for layer in layers]",
            "  for name in all_names:",
            "    if all_names.count(name) != 1:",
            "      raise ValueError('The name \"' + name + '\" is used ' +",
            "                       str(all_names.count(name)) + ' times in the model. '",
            "                       'All layer names should be unique.')",
            "  return network_nodes, nodes_by_depth, layers, layers_by_depth",
            "",
            "",
            "def _build_map(outputs):",
            "  \"\"\"This method topologically sorts nodes in order from inputs to outputs.",
            "",
            "  It uses a depth-first search to topologically sort nodes that appear in the",
            "  _keras_history connectivity metadata of `outputs`.",
            "",
            "  Args:",
            "    outputs: the output tensors whose _keras_history metadata should be walked.",
            "    This may be an arbitrary nested structure.",
            "",
            "  Returns:",
            "    A tuple like (ordered_nodes, layer_to_first_traversal_index)",
            "    ordered_nodes: list of nodes appearing in the keras history, topologically",
            "      sorted from original inputs to the `outputs`.",
            "      (If outputs have different sets of ancestors, the inputs to one output",
            "      may appear after a different output).",
            "    layer_to_first_traversal_index:",
            "      A dict mapping layer to the traversal index in the DFS where it is",
            "      seen. Note: if a layer is shared by several nodes, the dict will only",
            "      store the index corresponding to the *first* time the layer seen.",
            "  \"\"\"",
            "  finished_nodes = set()",
            "  nodes_in_progress = set()",
            "  nodes_in_decreasing_depth = []  # nodes from inputs -> outputs.",
            "  layer_indices = {}  # layer -> in traversal order.",
            "  for output in nest.flatten(outputs):",
            "    _build_map_helper(output, finished_nodes, nodes_in_progress,",
            "                      nodes_in_decreasing_depth, layer_indices)",
            "  return nodes_in_decreasing_depth, layer_indices",
            "",
            "",
            "def _build_map_helper(tensor, finished_nodes, nodes_in_progress,",
            "                      nodes_in_decreasing_depth, layer_indices):",
            "  \"\"\"Recursive helper for `_build_map`.\"\"\"",
            "  layer, node_index, _ = tensor._keras_history  # pylint: disable=protected-access",
            "  node = layer._inbound_nodes[node_index]  # pylint: disable=protected-access",
            "",
            "  # Don't repeat work for shared subgraphs",
            "  if node in finished_nodes:",
            "    return",
            "",
            "  # Prevent cycles.",
            "  if node in nodes_in_progress:",
            "    raise ValueError('The tensor ' + str(tensor) + ' at layer \"' + layer.name +",
            "                     '\" is part of a cycle.')",
            "",
            "  # Store the traversal order for layer sorting.",
            "  if layer not in layer_indices:",
            "    layer_indices[layer] = len(layer_indices)",
            "",
            "  # Propagate to all previous tensors connected to this node.",
            "  nodes_in_progress.add(node)",
            "  if not node.is_input:",
            "    for tensor in node.keras_inputs:",
            "      _build_map_helper(tensor, finished_nodes, nodes_in_progress,",
            "                        nodes_in_decreasing_depth, layer_indices)",
            "",
            "  finished_nodes.add(node)",
            "  nodes_in_progress.remove(node)",
            "  nodes_in_decreasing_depth.append(node)",
            "",
            "",
            "def _map_subgraph_network(inputs, outputs):",
            "  \"\"\"Returns the nodes and layers in the topology from `inputs` to `outputs`.",
            "",
            "  Args:",
            "    inputs: List of input tensors.",
            "    outputs: List of output tensors.",
            "",
            "  Returns:",
            "    A tuple of List{Node] and List[Layer].",
            "  \"\"\"",
            "  if not ops.executing_eagerly_outside_functions():",
            "    base_layer_utils.create_keras_history(outputs)",
            "  # Keep only nodes and layers in the topology between inputs and outputs.",
            "  _, nodes_by_depth, layers, _ = _map_graph_network(inputs, outputs)",
            "  return nest.flatten([nodes for nodes in nodes_by_depth.values()]), layers",
            "",
            "",
            "def _should_skip_first_node(layer):",
            "  \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"",
            "  # Networks that are constructed with an Input layer/shape start with a",
            "  # pre-existing node linking their input to output. This node is excluded from",
            "  # the network config.",
            "  if layer._self_tracked_trackables:",
            "    return (isinstance(layer, Functional) and",
            "            # Filter out Sequential models without an input shape.",
            "            isinstance(layer._self_tracked_trackables[0],",
            "                       input_layer_module.InputLayer))",
            "  else:",
            "    return isinstance(layer, Functional)",
            "",
            "",
            "def connect_ancillary_layers(model, created_layers):",
            "  \"\"\"Adds layers that are not connected to the outputs to the model.\"\"\"",
            "  # Layers not connected to outputs, such as those added in `add_loss`.",
            "  ancillary_layers = [",
            "      layer for layer in created_layers.values() if layer not in model.layers",
            "  ]",
            "  if ancillary_layers:",
            "    relevant_nodes = nest.flatten([",
            "        layer.inbound_nodes[1:]",
            "        if _should_skip_first_node(layer) else layer.inbound_nodes",
            "        for layer in created_layers.values()",
            "    ])",
            "    model._insert_layers(ancillary_layers, relevant_nodes)",
            "  return model",
            "",
            "",
            "def reconstruct_from_config(config, custom_objects=None, created_layers=None):",
            "  \"\"\"Reconstructs graph from config object.",
            "",
            "  Args:",
            "    config: Dictionary returned from Network.get_config()",
            "    custom_objects: Optional dictionary mapping names (strings) to custom",
            "      classes or functions to be considered during deserialization.",
            "    created_layers: Optional dictionary mapping names to Layer objects. Any",
            "      layer not in this dictionary will be created and added to the dict.",
            "      This function will add new nodes to all layers (excluding InputLayers),",
            "      instead of re-using pre-existing nodes in the layers.",
            "",
            "  Returns:",
            "    Tuple of (input tensors, output tensors, dictionary of created layers)",
            "  \"\"\"",
            "  # Layer instances created during the graph reconstruction process.",
            "  created_layers = created_layers or collections.OrderedDict()",
            "",
            "  # Maps input data (tuple of inbound layer name, node index) from the config",
            "  # to node indices in the newly generated model. The node indices may be",
            "  # different if the layers have already been called previously.",
            "  node_index_map = {}",
            "  node_count_by_layer = {}",
            "",
            "  # Dictionary mapping layer instances to",
            "  # node data that specifies a layer call.",
            "  # It acts as a queue that maintains any unprocessed",
            "  # layer call until it becomes possible to process it",
            "  # (i.e. until the input tensors to the call all exist).",
            "  unprocessed_nodes = {}",
            "",
            "  def add_unprocessed_node(layer, node_data):",
            "    if layer not in unprocessed_nodes:",
            "      unprocessed_nodes[layer] = [node_data]",
            "    else:",
            "      unprocessed_nodes[layer].append(node_data)",
            "",
            "  def get_node_index(layer, config_node_index):",
            "    \"\"\"Returns node index in layer (might differ from config_node_index).\"\"\"",
            "    if isinstance(layer, input_layer_module.InputLayer):",
            "      return 0",
            "    return node_index_map.get((layer.name, config_node_index), None)",
            "",
            "  def _deserialize_keras_tensors(kwargs, layer_map):",
            "    \"\"\"Deserializes Keras Tensors passed to `call`..\"\"\"",
            "",
            "    def _deserialize_keras_tensor(t):",
            "      \"\"\"Deserializes a single Keras Tensor passed to `call`.\"\"\"",
            "      if isinstance(t, tf_utils.ListWrapper):",
            "        t = t.as_list()",
            "        layer_name = t[0]",
            "        node_index = t[1]",
            "        tensor_index = t[2]",
            "",
            "        layer = layer_map[layer_name]",
            "        new_node_index = get_node_index(layer, node_index)",
            "        if new_node_index is None:",
            "          # The inbound node may not have been processed yet,",
            "          # (This can happen e.g. if it depends on a different set",
            "          # of inputs than those that have been processed already).",
            "          # raise an IndexError so that the current node puts itself",
            "          # back on the unprocessed queue.",
            "          # Caution: This may lead to infinite loops for malformed",
            "          # network configurations! (or when there is a bug in",
            "          # the network config loading code).",
            "          raise IndexError",
            "        node = layer._inbound_nodes[new_node_index]",
            "        return nest.flatten(node.outputs)[tensor_index]",
            "      return t",
            "",
            "    kwargs = tf_utils.convert_inner_node_data(kwargs, wrap=True)",
            "    return nest.map_structure(_deserialize_keras_tensor, kwargs)",
            "",
            "  def process_node(layer, node_data):",
            "    \"\"\"Deserialize a node.",
            "",
            "    Args:",
            "        layer: layer instance.",
            "        node_data: Nested structure of `ListWrapper`.",
            "",
            "    Raises:",
            "        ValueError: In case of improperly formatted `node_data`.",
            "    \"\"\"",
            "    input_tensors = []",
            "    for input_data in nest.flatten(node_data):",
            "      input_data = input_data.as_list()",
            "      inbound_layer_name = input_data[0]",
            "      inbound_node_index = input_data[1]",
            "      inbound_tensor_index = input_data[2]",
            "      if len(input_data) == 3:",
            "        kwargs = {}",
            "      elif len(input_data) == 4:",
            "        kwargs = input_data[3]",
            "        try:",
            "          kwargs = _deserialize_keras_tensors(kwargs, created_layers)",
            "        except IndexError:",
            "          # Happens if keras tensors in kwargs are still unprocessed",
            "          add_unprocessed_node(layer, node_data)",
            "          return",
            "      else:",
            "        raise ValueError('Improperly formatted model config.')",
            "",
            "      if inbound_layer_name != node_module._CONSTANT_VALUE:",
            "        inbound_layer = created_layers[inbound_layer_name]",
            "        inbound_node_index = get_node_index(inbound_layer, inbound_node_index)",
            "",
            "        if inbound_node_index is None:",
            "          add_unprocessed_node(layer, node_data)",
            "          return",
            "        inbound_node = inbound_layer._inbound_nodes[inbound_node_index]",
            "        input_tensors.append(",
            "            nest.flatten(inbound_node.outputs)[inbound_tensor_index])",
            "      else:",
            "        # We received a constant w/ no Keras history attached",
            "        input_tensors.append(inbound_tensor_index)",
            "    input_tensors = nest.pack_sequence_as(node_data, input_tensors)",
            "    # Call layer on its inputs, thus creating the node",
            "    # and building the layer if needed.",
            "    if input_tensors is not None:",
            "      if not layer._preserve_input_structure_in_config:",
            "        input_tensors = (",
            "            base_layer_utils.unnest_if_single_tensor(input_tensors))",
            "      output_tensors = layer(input_tensors, **kwargs)",
            "",
            "      # Update node index map.",
            "      output_index = nest.flatten(output_tensors)[0]._keras_history.node_index",
            "      node_index_map[(layer.name, node_count_by_layer[layer])] = output_index",
            "      node_count_by_layer[layer] += 1",
            "",
            "  def process_layer(layer_data):",
            "    \"\"\"Deserializes a layer, then call it on appropriate inputs.",
            "",
            "    Args:",
            "        layer_data: layer config dict.",
            "",
            "    Raises:",
            "        ValueError: In case of improperly formatted `layer_data` dict.",
            "    \"\"\"",
            "    layer_name = layer_data['name']",
            "",
            "    if layer_name in created_layers:",
            "      layer = created_layers[layer_name]",
            "    else:",
            "      # Instantiate layer.",
            "      from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top",
            "",
            "      layer = deserialize_layer(layer_data, custom_objects=custom_objects)",
            "      created_layers[layer_name] = layer",
            "",
            "    node_count_by_layer[layer] = int(_should_skip_first_node(layer))",
            "",
            "    # Gather layer inputs and convert to `ListWrapper` objects.",
            "    inbound_nodes_data = layer_data['inbound_nodes']",
            "    inbound_nodes_data = tf_utils.convert_inner_node_data(",
            "        inbound_nodes_data, wrap=True)",
            "    for node_data in inbound_nodes_data:",
            "      # We don't process nodes (i.e. make layer calls)",
            "      # on the fly because the inbound node may not yet exist,",
            "      # in case of layer shared at different topological depths",
            "      # (e.g. a model such as A(B(A(B(x)))))",
            "      add_unprocessed_node(layer, node_data)",
            "",
            "  # First, we create all layers and enqueue nodes to be processed",
            "  for layer_data in config['layers']:",
            "    process_layer(layer_data)",
            "  # Then we process nodes in order of layer depth.",
            "  # Nodes that cannot yet be processed (if the inbound node",
            "  # does not yet exist) are re-enqueued, and the process",
            "  # is repeated until all nodes are processed.",
            "  while unprocessed_nodes:",
            "    for layer_data in config['layers']:",
            "      layer = created_layers[layer_data['name']]",
            "      if layer in unprocessed_nodes:",
            "        for node_data in unprocessed_nodes.pop(layer):",
            "          process_node(layer, node_data)",
            "",
            "  input_tensors = []",
            "  output_tensors = []",
            "",
            "  input_layers = tf_utils.convert_inner_node_data(",
            "      config['input_layers'], wrap=True)",
            "  for layer_data in nest.flatten(input_layers):",
            "    layer_name, node_index, tensor_index = layer_data.as_list()",
            "    assert layer_name in created_layers",
            "    layer = created_layers[layer_name]",
            "    node_index = get_node_index(layer, node_index)",
            "    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors",
            "    input_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])",
            "",
            "  output_layers = tf_utils.convert_inner_node_data(",
            "      config['output_layers'], wrap=True)",
            "  for layer_data in nest.flatten(output_layers):",
            "    layer_name, node_index, tensor_index = layer_data.as_list()",
            "    assert layer_name in created_layers",
            "    layer = created_layers[layer_name]",
            "    node_index = get_node_index(layer, node_index)",
            "    layer_output_tensors = layer._inbound_nodes[node_index].output_tensors",
            "    output_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])",
            "",
            "  input_tensors = nest.pack_sequence_as(input_layers, input_tensors)",
            "  output_tensors = nest.pack_sequence_as(output_layers, output_tensors)",
            "  return input_tensors, output_tensors, created_layers",
            "",
            "",
            "def get_network_config(network, serialize_layer_fn=None):",
            "  \"\"\"Builds the config, which consists of the node graph and serialized layers.",
            "",
            "  Args:",
            "    network: A Network object.",
            "    serialize_layer_fn: Function used to serialize layers.",
            "",
            "  Returns:",
            "    Config dictionary.",
            "  \"\"\"",
            "  serialize_layer_fn = (",
            "      serialize_layer_fn or generic_utils.serialize_keras_object)",
            "  config = {",
            "      'name': network.name,",
            "  }",
            "  node_conversion_map = {}",
            "  for layer in network.layers:",
            "    kept_nodes = 1 if _should_skip_first_node(layer) else 0",
            "    for original_node_index, node in enumerate(layer._inbound_nodes):",
            "      node_key = _make_node_key(layer.name, original_node_index)",
            "      if node_key in network._network_nodes:",
            "        node_conversion_map[node_key] = kept_nodes",
            "        kept_nodes += 1",
            "  layer_configs = []",
            "",
            "  with generic_utils.SharedObjectSavingScope():",
            "    for layer in network.layers:  # From the earliest layers on.",
            "      filtered_inbound_nodes = []",
            "      for original_node_index, node in enumerate(layer._inbound_nodes):",
            "        node_key = _make_node_key(layer.name, original_node_index)",
            "        if node_key in network._network_nodes and not node.is_input:",
            "          # The node is relevant to the model:",
            "          # add to filtered_inbound_nodes.",
            "          node_data = node.serialize(_make_node_key, node_conversion_map)",
            "          filtered_inbound_nodes.append(node_data)",
            "",
            "      layer_config = serialize_layer_fn(layer)",
            "      layer_config['name'] = layer.name",
            "      layer_config['inbound_nodes'] = filtered_inbound_nodes",
            "      layer_configs.append(layer_config)",
            "    config['layers'] = layer_configs",
            "",
            "  # Gather info about inputs and outputs.",
            "  model_inputs = []",
            "  for i in range(len(network._input_layers)):",
            "    layer, node_index, tensor_index = network._input_coordinates[i]",
            "    node_key = _make_node_key(layer.name, node_index)",
            "    if node_key not in network._network_nodes:",
            "      continue",
            "    new_node_index = node_conversion_map[node_key]",
            "    model_inputs.append(",
            "        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))",
            "  model_inputs = nest.pack_sequence_as(network._nested_inputs, model_inputs)",
            "  # Preserve external Keras compat for Models with single input.",
            "  if not nest.is_nested(model_inputs):",
            "    model_inputs = [model_inputs]",
            "  model_inputs = tf_utils.convert_inner_node_data(model_inputs)",
            "  config['input_layers'] = model_inputs",
            "",
            "  model_outputs = []",
            "  for i in range(len(network._output_layers)):",
            "    layer, node_index, tensor_index = network._output_coordinates[i]",
            "    node_key = _make_node_key(layer.name, node_index)",
            "    if node_key not in network._network_nodes:",
            "      continue",
            "    new_node_index = node_conversion_map[node_key]",
            "    model_outputs.append(",
            "        tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))",
            "  model_outputs = nest.pack_sequence_as(network._nested_outputs, model_outputs)",
            "  # Preserve external Keras compat for Models with single output.",
            "  if not nest.is_nested(model_outputs):",
            "    model_outputs = [model_outputs]",
            "  model_outputs = tf_utils.convert_inner_node_data(model_outputs)",
            "  config['output_layers'] = model_outputs",
            "  return config",
            "",
            "",
            "def shape_with_no_batch_size(x):",
            "  if x.shape.rank is None:",
            "    return None",
            "  shape = x.shape.as_list()",
            "  if shape:",
            "    shape[0] = None",
            "  return shape",
            "",
            "",
            "class ModuleWrapper(base_layer.Layer):",
            "  \"\"\"Wrapper for `tf.Module`s to support the Functional and Sequential API.\"\"\"",
            "",
            "  def __init__(self, module, method_name=None, **kwargs):",
            "    \"\"\"Initializes the wrapper Layer for this module.",
            "",
            "    Args:",
            "      module: The `tf.Module` instance to be wrapped.",
            "      method_name: (Optional) str. The name of the method to use as the forward",
            "        pass of the module. If not set, defaults to '__call__' if defined, or",
            "        'call'.",
            "      **kwargs: Additional keywrod arguments. See `tf.keras.layers.Layer`.",
            "",
            "    Raises:",
            "      ValueError: If `method` is not defined on `module`.",
            "    \"\"\"",
            "    super(ModuleWrapper, self).__init__(**kwargs)",
            "    if method_name is None:",
            "      if hasattr(module, '__call__'):",
            "        method_name = '__call__'",
            "      elif hasattr(module, 'call'):",
            "        method_name = 'call'",
            "    if method_name is None or not hasattr(module, method_name):",
            "      raise ValueError('{} is not defined on object {}'.format(",
            "          method_name, module))",
            "",
            "    self._module = module",
            "    self._method_name = method_name",
            "",
            "    # Check if module.__call__ has a `training` arg or accepts `**kwargs`.",
            "    method = getattr(module, method_name)",
            "    method_arg_spec = tf_inspect.getfullargspec(method)",
            "    self._expects_training_arg = ('training' in method_arg_spec.args or",
            "                                  method_arg_spec.varkw is not None)",
            "    self._expects_mask_arg = ('mask' in method_arg_spec.args or",
            "                              method_arg_spec.varkw is not None)",
            "",
            "  def call(self, *args, **kwargs):",
            "    if 'training' in kwargs and not self._expects_training_arg:",
            "      kwargs.pop('training')",
            "    if 'mask' in kwargs and not self._expects_mask_arg:",
            "      kwargs.pop('mask')",
            "    return getattr(self._module, self._method_name)(*args, **kwargs)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "56": [
                "Functional"
            ]
        },
        "addLocation": []
    },
    "tensorflow/python/keras/engine/functional_test.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": " from tensorflow.python.platform import test"
            },
            "1": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " from tensorflow.python.training.tracking.util import Checkpoint"
            },
            "2": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-try:"
            },
            "4": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  import yaml  # pylint:disable=g-import-not-at-top"
            },
            "5": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-except ImportError:"
            },
            "6": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  yaml = None"
            },
            "7": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "8": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " class NetworkConstructionTest(keras_parameterized.TestCase):"
            },
            "10": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 52,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 627,
                "afterPatchRowNumber": 622,
                "PatchRowcode": "       json_str = model.to_json()"
            },
            "12": {
                "beforePatchRowNumber": 628,
                "afterPatchRowNumber": 623,
                "PatchRowcode": "       models.model_from_json(json_str)"
            },
            "13": {
                "beforePatchRowNumber": 629,
                "afterPatchRowNumber": 624,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 630,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-      if yaml is not None:"
            },
            "15": {
                "beforePatchRowNumber": 631,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        yaml_str = model.to_yaml()"
            },
            "16": {
                "beforePatchRowNumber": 632,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        models.model_from_yaml(yaml_str)"
            },
            "17": {
                "beforePatchRowNumber": 633,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "18": {
                "beforePatchRowNumber": 634,
                "afterPatchRowNumber": 625,
                "PatchRowcode": "   @combinations.generate(combinations.combine(mode=['graph', 'eager']))"
            },
            "19": {
                "beforePatchRowNumber": 635,
                "afterPatchRowNumber": 626,
                "PatchRowcode": "   def test_invalid_graphs(self):"
            },
            "20": {
                "beforePatchRowNumber": 636,
                "afterPatchRowNumber": 627,
                "PatchRowcode": "     a = layers.Input(shape=(32,), name='input_a')"
            },
            "21": {
                "beforePatchRowNumber": 1361,
                "afterPatchRowNumber": 1352,
                "PatchRowcode": "     json_str = model.to_json()"
            },
            "22": {
                "beforePatchRowNumber": 1362,
                "afterPatchRowNumber": 1353,
                "PatchRowcode": "     models.model_from_json(json_str)"
            },
            "23": {
                "beforePatchRowNumber": 1363,
                "afterPatchRowNumber": 1354,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 1364,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if yaml is not None:"
            },
            "25": {
                "beforePatchRowNumber": 1365,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-      yaml_str = model.to_yaml()"
            },
            "26": {
                "beforePatchRowNumber": 1366,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-      models.model_from_yaml(yaml_str)"
            },
            "27": {
                "beforePatchRowNumber": 1367,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "28": {
                "beforePatchRowNumber": 1368,
                "afterPatchRowNumber": 1355,
                "PatchRowcode": "   def test_subclassed_error_if_init_not_called(self):"
            },
            "29": {
                "beforePatchRowNumber": 1369,
                "afterPatchRowNumber": 1356,
                "PatchRowcode": " "
            },
            "30": {
                "beforePatchRowNumber": 1370,
                "afterPatchRowNumber": 1357,
                "PatchRowcode": "     class MyNetwork(training_lib.Model):"
            }
        },
        "frontPatchFile": [
            "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "#,============================================================================",
            "\"\"\"Tests for layer graphs construction & handling.\"\"\"",
            "",
            "import warnings",
            "",
            "import numpy as np",
            "",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import def_function",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import tensor_shape",
            "from tensorflow.python.keras import backend",
            "from tensorflow.python.keras import combinations",
            "from tensorflow.python.keras import initializers",
            "from tensorflow.python.keras import keras_parameterized",
            "from tensorflow.python.keras import layers",
            "from tensorflow.python.keras import losses",
            "from tensorflow.python.keras import models",
            "from tensorflow.python.keras import testing_utils",
            "from tensorflow.python.keras.engine import base_layer",
            "from tensorflow.python.keras.engine import functional",
            "from tensorflow.python.keras.engine import input_layer as input_layer_lib",
            "from tensorflow.python.keras.engine import sequential",
            "from tensorflow.python.keras.engine import training as training_lib",
            "from tensorflow.python.keras.utils import layer_utils",
            "from tensorflow.python.keras.utils import tf_utils",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import state_ops",
            "from tensorflow.python.ops import string_ops",
            "from tensorflow.python.ops.ragged import ragged_factory_ops",
            "from tensorflow.python.platform import test",
            "from tensorflow.python.training.tracking.util import Checkpoint",
            "",
            "try:",
            "  import yaml  # pylint:disable=g-import-not-at-top",
            "except ImportError:",
            "  yaml = None",
            "",
            "",
            "class NetworkConstructionTest(keras_parameterized.TestCase):",
            "",
            "  def test_default_model_name(self):",
            "    inputs = input_layer_lib.Input(shape=(1,))",
            "    outputs = layers.Dense(1, activation='relu')(inputs)",
            "    model = training_lib.Model(inputs=inputs, outputs=outputs)",
            "    self.assertEqual(model.name, 'model')",
            "",
            "    model_2 = training_lib.Model(inputs=inputs, outputs=outputs)",
            "    self.assertEqual(model_2.name, 'model_1')",
            "",
            "    model_3 = training_lib.Model(inputs=inputs, outputs=outputs)",
            "    self.assertEqual(model_3.name, 'model_2')",
            "",
            "  def test_get_updates(self):",
            "",
            "    class MyLayer(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.a = self.add_variable('a',",
            "                                   (1, 1),",
            "                                   'float32',",
            "                                   trainable=False)",
            "        self.b = self.add_variable('b',",
            "                                   (1, 1),",
            "                                   'float32',",
            "                                   trainable=False)",
            "        self.add_update(state_ops.assign_add(self.a, [[1.]],",
            "                                             name='unconditional_update'))",
            "        self.built = True",
            "",
            "      def call(self, inputs):",
            "        self.add_update(state_ops.assign_add(self.b, inputs,",
            "                                             name='conditional_update'),",
            "                        inputs=True)",
            "        return inputs + 1",
            "",
            "    with ops.Graph().as_default():",
            "      x1 = input_layer_lib.Input(shape=(1,))",
            "      layer = MyLayer()",
            "      _ = layer(x1)",
            "",
            "      self.assertEqual(len(layer.updates), 2)",
            "",
            "      x2 = input_layer_lib.Input(shape=(1,))",
            "      y2 = layer(x2)",
            "",
            "      self.assertEqual(len(layer.updates), 3)",
            "",
            "      network = functional.Functional(x2, y2)",
            "      self.assertEqual(len(network.updates), 3)",
            "",
            "      x3 = input_layer_lib.Input(shape=(1,))",
            "      _ = layer(x3)",
            "      self.assertEqual(len(network.updates), 4)",
            "",
            "      x4 = input_layer_lib.Input(shape=(1,))",
            "      _ = network(x4)",
            "      self.assertEqual(len(network.updates), 5)",
            "",
            "      network.add_update(state_ops.assign_add(layer.a, [[1]]))",
            "      self.assertEqual(len(network.updates), 6)",
            "",
            "      network.add_update(state_ops.assign_add(layer.b, x4), inputs=True)",
            "      self.assertEqual(len(network.updates), 7)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph']))",
            "  def test_get_updates_bn(self):",
            "    x1 = input_layer_lib.Input(shape=(1,))",
            "    layer = layers.BatchNormalization()",
            "    _ = layer(x1)",
            "",
            "    self.assertEqual(len(layer.updates), 2)",
            "",
            "  def test_get_layer(self):",
            "    # create a simple network",
            "    x = input_layer_lib.Input(shape=(32,))",
            "    dense_a = layers.Dense(4, name='dense_a')",
            "    dense_b = layers.Dense(2, name='dense_b')",
            "    y = dense_b(dense_a(x))",
            "    network = functional.Functional(x, y, name='dense_network')",
            "",
            "    # test various get_layer by index",
            "    self.assertEqual(network.get_layer(index=1), dense_a)",
            "",
            "    # test invalid get_layer by index",
            "    with self.assertRaisesRegex(",
            "        ValueError, 'Was asked to retrieve layer at index ' + str(3) +",
            "        ' but model only has ' + str(len(network.layers)) + ' layers.'):",
            "      network.get_layer(index=3)",
            "",
            "    # test that only one between name and index is requested",
            "    with self.assertRaisesRegex(ValueError,",
            "                                'Provide only a layer name or a layer index'):",
            "      network.get_layer(index=1, name='dense_b')",
            "",
            "    # test that a name or an index must be provided",
            "    with self.assertRaisesRegex(ValueError,",
            "                                'Provide either a layer name or layer index.'):",
            "      network.get_layer()",
            "",
            "    # test various get_layer by name",
            "    self.assertEqual(network.get_layer(name='dense_a'), dense_a)",
            "",
            "    # test invalid get_layer by name",
            "    with self.assertRaisesRegex(ValueError, 'No such layer: dense_c.'):",
            "      network.get_layer(name='dense_c')",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTopologicalAttributes(self):",
            "    # test layer attributes / methods related to cross-layer connectivity.",
            "    a = input_layer_lib.Input(shape=(32,), name='input_a')",
            "    b = input_layer_lib.Input(shape=(32,), name='input_b')",
            "",
            "    # test input, output, input_shape, output_shape",
            "    test_layer = layers.Dense(16, name='test_layer')",
            "    a_test = test_layer(a)",
            "    self.assertIs(test_layer.input, a)",
            "    self.assertIs(test_layer.output, a_test)",
            "    self.assertEqual(test_layer.input_shape, (None, 32))",
            "    self.assertEqual(test_layer.output_shape, (None, 16))",
            "",
            "    # test `get_*_at` methods",
            "    dense = layers.Dense(16, name='dense_1')",
            "    a_2 = dense(a)",
            "    b_2 = dense(b)",
            "",
            "    self.assertIs(dense.get_input_at(0), a)",
            "    self.assertIs(dense.get_input_at(1), b)",
            "    self.assertIs(dense.get_output_at(0), a_2)",
            "    self.assertIs(dense.get_output_at(1), b_2)",
            "    self.assertEqual(dense.get_input_shape_at(0), (None, 32))",
            "    self.assertEqual(dense.get_input_shape_at(1), (None, 32))",
            "    self.assertEqual(dense.get_output_shape_at(0), (None, 16))",
            "    self.assertEqual(dense.get_output_shape_at(1), (None, 16))",
            "",
            "    # Test invalid value for attribute retrieval.",
            "    with self.assertRaises(ValueError):",
            "      dense.get_input_at(2)",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.input",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.output",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.output_shape",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.input_shape",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      a = input_layer_lib.Input(shape=(3, 32))",
            "      a = input_layer_lib.Input(shape=(5, 32))",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      _ = new_dense.input_shape",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      a = input_layer_lib.Input(shape=(3, 32))",
            "      a = input_layer_lib.Input(shape=(5, 32))",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      _ = new_dense.output_shape",
            "",
            "  def _assertAllIs(self, a, b):",
            "    self.assertTrue(all(x is y for x, y in zip(a, b)))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTopologicalAttributesMultiOutputLayer(self):",
            "",
            "    class PowersLayer(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        return [inputs**2, inputs**3]",
            "",
            "    x = input_layer_lib.Input(shape=(32,))",
            "    test_layer = PowersLayer()",
            "    p1, p2 = test_layer(x)  # pylint: disable=not-callable",
            "",
            "    self.assertIs(test_layer.input, x)",
            "    self._assertAllIs(test_layer.output, [p1, p2])",
            "    self.assertEqual(test_layer.input_shape, (None, 32))",
            "    self.assertEqual(test_layer.output_shape, [(None, 32), (None, 32)])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTopologicalAttributesMultiInputLayer(self):",
            "",
            "    class AddLayer(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        assert len(inputs) == 2",
            "        return inputs[0] + inputs[1]",
            "",
            "    a = input_layer_lib.Input(shape=(32,))",
            "    b = input_layer_lib.Input(shape=(32,))",
            "    test_layer = AddLayer()",
            "    y = test_layer([a, b])  # pylint: disable=not-callable",
            "",
            "    self._assertAllIs(test_layer.input, [a, b])",
            "    self.assertIs(test_layer.output, y)",
            "    self.assertEqual(test_layer.input_shape, [(None, 32), (None, 32)])",
            "    self.assertEqual(test_layer.output_shape, (None, 32))",
            "",
            "  def testBasicNetwork(self):",
            "    with ops.Graph().as_default():",
            "      # minimum viable network",
            "      x = input_layer_lib.Input(shape=(32,))",
            "      dense = layers.Dense(2)",
            "      y = dense(x)",
            "      network = functional.Functional(x, y, name='dense_network')",
            "",
            "      # test basic attributes",
            "      self.assertEqual(network.name, 'dense_network')",
            "      self.assertEqual(len(network.layers), 2)  # InputLayer + Dense",
            "      self.assertEqual(network.layers[1], dense)",
            "      self._assertAllIs(network.weights, dense.weights)",
            "      self._assertAllIs(network.trainable_weights, dense.trainable_weights)",
            "      self._assertAllIs(network.non_trainable_weights,",
            "                        dense.non_trainable_weights)",
            "",
            "      # test callability on Input",
            "      x_2 = input_layer_lib.Input(shape=(32,))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 2])",
            "",
            "      # test callability on regular tensor",
            "      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 2])",
            "",
            "      # test network `trainable` attribute",
            "      network.trainable = False",
            "      self._assertAllIs(network.weights, dense.weights)",
            "      self.assertEqual(network.trainable_weights, [])",
            "      self._assertAllIs(network.non_trainable_weights,",
            "                        dense.trainable_weights + dense.non_trainable_weights)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_trainable_weights(self):",
            "    a = layers.Input(shape=(2,))",
            "    b = layers.Dense(1)(a)",
            "    model = training_lib.Model(a, b)",
            "",
            "    weights = model.weights",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "    model.trainable = True",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.layers[1].trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "    # sequential model",
            "    model = sequential.Sequential()",
            "    model.add(layers.Dense(1, input_dim=2))",
            "    weights = model.weights",
            "",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "    model.trainable = True",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.layers[0].trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "  def test_layer_call_arguments(self):",
            "    with ops.Graph().as_default():",
            "      # Test the ability to pass and serialize arguments to `call`.",
            "      inp = layers.Input(shape=(2,))",
            "      x = layers.Dense(3)(inp)",
            "      x = layers.Dropout(0.5)(x, training=True)",
            "      model = training_lib.Model(inp, x)",
            "      # Would be `dropout/cond/Merge` by default",
            "      self.assertIn('dropout', model.output.op.name)",
            "",
            "      # Test that argument is kept when applying the model",
            "      inp2 = layers.Input(shape=(2,))",
            "      out2 = model(inp2)",
            "      self.assertIn('dropout', out2.op.name)",
            "",
            "      # Test that argument is kept after loading a model",
            "      config = model.get_config()",
            "      model = training_lib.Model.from_config(config)",
            "      self.assertIn('dropout', model.output.op.name)",
            "",
            "  def test_node_construction(self):",
            "    # test basics",
            "    a = layers.Input(shape=(32,), name='input_a')",
            "    b = layers.Input(shape=(32,), name='input_b')",
            "",
            "    with self.assertRaises(ValueError):",
            "      _ = layers.Input(shape=(32,), batch_shape=(10, 32))",
            "    with self.assertRaises(ValueError):",
            "      _ = layers.Input(shape=(32,), unknown_kwarg=None)",
            "",
            "    self.assertListEqual(a.shape.as_list(), [None, 32])",
            "    a_layer, a_node_index, a_tensor_index = a._keras_history",
            "    b_layer, _, _ = b._keras_history",
            "    self.assertEqual(len(a_layer._inbound_nodes), 1)",
            "    self.assertEqual(a_tensor_index, 0)",
            "    node = a_layer._inbound_nodes[a_node_index]",
            "    self.assertEqual(node.outbound_layer, a_layer)",
            "",
            "    self.assertListEqual(node.inbound_layers, [])",
            "    self.assertListEqual(node.input_tensors, [a])",
            "    self.assertListEqual(node.input_shapes, [(None, 32)])",
            "    self.assertListEqual(node.output_tensors, [a])",
            "    self.assertListEqual(node.output_shapes, [(None, 32)])",
            "",
            "    dense = layers.Dense(16, name='dense_1')",
            "    a_2 = dense(a)",
            "    b_2 = dense(b)",
            "",
            "    self.assertEqual(len(dense._inbound_nodes), 2)",
            "    self.assertEqual(len(dense._outbound_nodes), 0)",
            "    self.assertEqual(dense._inbound_nodes[0].inbound_layers, a_layer)",
            "    self.assertEqual(dense._inbound_nodes[0].outbound_layer, dense)",
            "    self.assertEqual(dense._inbound_nodes[1].inbound_layers, b_layer)",
            "    self.assertEqual(dense._inbound_nodes[1].outbound_layer, dense)",
            "    self.assertIs(dense._inbound_nodes[0].input_tensors, a)",
            "    self.assertIs(dense._inbound_nodes[1].input_tensors, b)",
            "",
            "    # test layer properties",
            "    test_layer = layers.Dense(16, name='test_layer')",
            "    a_test = test_layer(a)",
            "    self.assertListEqual(test_layer.kernel.shape.as_list(), [32, 16])",
            "    self.assertIs(test_layer.input, a)",
            "    self.assertIs(test_layer.output, a_test)",
            "    self.assertEqual(test_layer.input_shape, (None, 32))",
            "    self.assertEqual(test_layer.output_shape, (None, 16))",
            "",
            "    self.assertIs(dense.get_input_at(0), a)",
            "    self.assertIs(dense.get_input_at(1), b)",
            "    self.assertIs(dense.get_output_at(0), a_2)",
            "    self.assertIs(dense.get_output_at(1), b_2)",
            "    self.assertEqual(dense.get_input_shape_at(0), (None, 32))",
            "    self.assertEqual(dense.get_input_shape_at(1), (None, 32))",
            "    self.assertEqual(dense.get_output_shape_at(0), (None, 16))",
            "    self.assertEqual(dense.get_output_shape_at(1), (None, 16))",
            "    self.assertEqual(dense.get_input_mask_at(0), None)",
            "    self.assertEqual(dense.get_input_mask_at(1), None)",
            "    self.assertEqual(dense.get_output_mask_at(0), None)",
            "    self.assertEqual(dense.get_output_mask_at(1), None)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_multi_input_layer(self):",
            "    with self.cached_session():",
            "      # test multi-input layer",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      self.assertListEqual(merged.shape.as_list(), [None, 16 * 2])",
            "      merge_layer, merge_node_index, merge_tensor_index = merged._keras_history",
            "",
            "      self.assertEqual(merge_node_index, 0)",
            "      self.assertEqual(merge_tensor_index, 0)",
            "",
            "      self.assertEqual(len(merge_layer._inbound_nodes), 1)",
            "      self.assertEqual(len(merge_layer._outbound_nodes), 0)",
            "",
            "      self.assertEqual(len(merge_layer._inbound_nodes[0].input_tensors), 2)",
            "      self.assertEqual(len(merge_layer._inbound_nodes[0].inbound_layers), 2)",
            "",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "      self.assertEqual(len(model.layers), 6)",
            "      output_shapes = model.compute_output_shape([(None, 32), (None, 32)])",
            "      self.assertListEqual(output_shapes[0].as_list(), [None, 64])",
            "      self.assertListEqual(output_shapes[1].as_list(), [None, 5])",
            "      self.assertListEqual(",
            "          model.compute_mask([a, b], [None, None]), [None, None])",
            "",
            "      # we don't check names of first 2 layers (inputs) because",
            "      # ordering of same-level layers is not fixed",
            "      self.assertListEqual([l.name for l in model.layers][2:],",
            "                           ['dense_1', 'merge', 'dense_2', 'dense_3'])",
            "      self.assertListEqual([l.name for l in model._input_layers],",
            "                           ['input_a', 'input_b'])",
            "      self.assertListEqual([l.name for l in model._output_layers],",
            "                           ['dense_2', 'dense_3'])",
            "",
            "      # actually run model",
            "      fn = backend.function(model.inputs, model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])",
            "",
            "      # test get_source_inputs",
            "      self._assertAllIs(layer_utils.get_source_inputs(c), [a, b])",
            "",
            "      # serialization / deserialization",
            "      json_config = model.to_json()",
            "      recreated_model = models.model_from_json(json_config)",
            "      recreated_model.compile('rmsprop', 'mse')",
            "",
            "      self.assertListEqual([l.name for l in recreated_model.layers][2:],",
            "                           ['dense_1', 'merge', 'dense_2', 'dense_3'])",
            "      self.assertListEqual([l.name for l in recreated_model._input_layers],",
            "                           ['input_a', 'input_b'])",
            "      self.assertListEqual([l.name for l in recreated_model._output_layers],",
            "                           ['dense_2', 'dense_3'])",
            "",
            "      fn = backend.function(recreated_model.inputs, recreated_model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])",
            "",
            "  def test_multi_output_layer_output_names(self):",
            "    inp = layers.Input(name='inp', shape=(None,), dtype=dtypes.float32)",
            "",
            "    class _MultiOutput(layers.Layer):",
            "",
            "      def call(self, x):",
            "        return x + 1., x + 2.",
            "",
            "    out = _MultiOutput(name='out')(inp)",
            "    model = training_lib.Model(inp, out)",
            "    self.assertEqual(['out', 'out_1'], model.output_names)",
            "    self.assertAllClose([2., 3.], model(1.))",
            "",
            "  def test_recursion(self):",
            "    with ops.Graph().as_default(), self.cached_session():",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "      e = layers.Input(shape=(32,), name='input_e')",
            "      f = layers.Input(shape=(32,), name='input_f')",
            "      self.assertEqual(len(model.inputs), 2)",
            "      g, h = model([e, f])",
            "      self.assertEqual(len(model.inputs), 2)",
            "      self.assertEqual(g.name, 'model/dense_2/BiasAdd:0')",
            "",
            "      self.assertListEqual(g.shape.as_list(), c.shape.as_list())",
            "      self.assertListEqual(h.shape.as_list(), d.shape.as_list())",
            "",
            "      # test separate manipulation of different layer outputs",
            "      i = layers.Dense(7, name='dense_4')(h)",
            "",
            "      final_model = training_lib.Model(",
            "          inputs=[e, f], outputs=[i, g], name='final')",
            "      self.assertEqual(len(final_model.inputs), 2)",
            "      self.assertEqual(len(final_model.outputs), 2)",
            "      self.assertEqual(len(final_model.layers), 4)",
            "",
            "      # we don't check names of first 2 layers (inputs) because",
            "      # ordering of same-level layers is not fixed",
            "      self.assertListEqual([layer.name for layer in final_model.layers][2:],",
            "                           ['model', 'dense_4'])",
            "      self.assertListEqual(",
            "          model.compute_mask([e, f], [None, None]), [None, None])",
            "      self.assertListEqual(",
            "          final_model.compute_output_shape([(10, 32), (10, 32)]), [(10, 7),",
            "                                                                   (10, 64)])",
            "",
            "      # run recursive model",
            "      fn = backend.function(final_model.inputs, final_model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])",
            "",
            "      # test serialization",
            "      model_config = final_model.get_config()",
            "      recreated_model = models.Model.from_config(model_config)",
            "",
            "      fn = backend.function(recreated_model.inputs, recreated_model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_multi_input_multi_output_recursion(self):",
            "    with self.cached_session():",
            "      # test multi-input multi-output",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "      j = layers.Input(shape=(32,), name='input_j')",
            "      k = layers.Input(shape=(32,), name='input_k')",
            "      _, n = model([j, k])",
            "",
            "      o = layers.Input(shape=(32,), name='input_o')",
            "      p = layers.Input(shape=(32,), name='input_p')",
            "      q, _ = model([o, p])",
            "",
            "      self.assertListEqual(n.shape.as_list(), [None, 5])",
            "      self.assertListEqual(q.shape.as_list(), [None, 64])",
            "      s = layers.concatenate([n, q], name='merge_nq')",
            "      self.assertListEqual(s.shape.as_list(), [None, 64 + 5])",
            "",
            "      # test with single output as 1-elem list",
            "      multi_io_model = training_lib.Model([j, k, o, p], [s])",
            "",
            "      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)",
            "      fn_outputs = fn([",
            "          np.random.random((10, 32)), np.random.random((10, 32)),",
            "          np.random.random((10, 32)), np.random.random((10, 32))",
            "      ])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])",
            "",
            "      # test with single output as tensor",
            "      multi_io_model = training_lib.Model([j, k, o, p], s)",
            "",
            "      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)",
            "      fn_outputs = fn([",
            "          np.random.random((10, 32)), np.random.random((10, 32)),",
            "          np.random.random((10, 32)), np.random.random((10, 32))",
            "      ])",
            "      # note that the output of the function will still be a 1-elem list",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])",
            "",
            "      # test serialization",
            "      model_config = multi_io_model.get_config()",
            "      recreated_model = models.Model.from_config(model_config)",
            "",
            "      fn = backend.function(recreated_model.inputs, recreated_model.outputs)",
            "      fn_outputs = fn([",
            "          np.random.random((10, 32)), np.random.random((10, 32)),",
            "          np.random.random((10, 32)), np.random.random((10, 32))",
            "      ])",
            "      # note that the output of the function will still be a 1-elem list",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])",
            "",
            "      config = model.get_config()",
            "      models.Model.from_config(config)",
            "",
            "      model.summary()",
            "      json_str = model.to_json()",
            "      models.model_from_json(json_str)",
            "",
            "      if yaml is not None:",
            "        yaml_str = model.to_yaml()",
            "        models.model_from_yaml(yaml_str)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_invalid_graphs(self):",
            "    a = layers.Input(shape=(32,), name='input_a')",
            "    b = layers.Input(shape=(32,), name='input_b')",
            "",
            "    dense = layers.Dense(16, name='dense_1')",
            "    a_2 = dense(a)",
            "    b_2 = dense(b)",
            "    merged = layers.concatenate([a_2, b_2], name='merge')",
            "    c = layers.Dense(64, name='dense_2')(merged)",
            "    d = layers.Dense(5, name='dense_3')(c)",
            "",
            "    model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "    # input is not an Input tensor",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    j = layers.Dense(32)(j)",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j, k], [m, n])",
            "",
            "    # disconnected graph",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j], [m, n])",
            "",
            "    # redundant outputs",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "",
            "    training_lib.Model([j, k], [m, n, n])",
            "",
            "    # redundant inputs",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j, k, j], [m, n])",
            "",
            "    # i have not idea what I'm doing: garbage as inputs/outputs",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j, k], [m, n, 0])",
            "",
            "  def test_raw_tf_compatibility(self):",
            "    with ops.Graph().as_default():",
            "      # test calling layers/models on TF tensors",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "      j = layers.Input(shape=(32,), name='input_j')",
            "      k = layers.Input(shape=(32,), name='input_k')",
            "      self.assertEqual(len(model.inputs), 2)",
            "      m, n = model([j, k])",
            "      self.assertEqual(len(model.inputs), 2)",
            "      tf_model = training_lib.Model([j, k], [m, n])",
            "",
            "      j_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))",
            "      k_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))",
            "      m_tf, n_tf = tf_model([j_tf, k_tf])",
            "      self.assertListEqual(m_tf.shape.as_list(), [None, 64])",
            "      self.assertListEqual(n_tf.shape.as_list(), [None, 5])",
            "",
            "      # test merge",
            "      layers.concatenate([j_tf, k_tf], axis=1)",
            "      layers.add([j_tf, k_tf])",
            "",
            "      # test tensor input",
            "      x = array_ops.placeholder(shape=(None, 2), dtype=dtypes.float32)",
            "      layers.InputLayer(input_tensor=x)",
            "",
            "      x = layers.Input(tensor=x)",
            "      layers.Dense(2)(x)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_basic_masking(self):",
            "    a = layers.Input(shape=(10, 32), name='input_a')",
            "    b = layers.Masking()(a)",
            "    model = training_lib.Model(a, b)",
            "    self.assertEqual(model.output_mask.shape.as_list(), [None, 10])",
            "",
            "  def testMaskingSingleInput(self):",
            "",
            "    class MaskedLayer(layers.Layer):",
            "",
            "      def call(self, inputs, mask=None):",
            "        if mask is not None:",
            "          return inputs * mask",
            "        return inputs",
            "",
            "      def compute_mask(self, inputs, mask=None):",
            "        return array_ops.ones_like(inputs)",
            "",
            "    if context.executing_eagerly():",
            "      a = constant_op.constant([2] * 32)",
            "      mask = constant_op.constant([0, 1] * 16)",
            "      a._keras_mask = mask",
            "      b = MaskedLayer().apply(a)",
            "      self.assertTrue(hasattr(b, '_keras_mask'))",
            "      self.assertAllEqual(",
            "          self.evaluate(array_ops.ones_like(mask)),",
            "          self.evaluate(getattr(b, '_keras_mask')))",
            "      self.assertAllEqual(self.evaluate(a * mask), self.evaluate(b))",
            "    else:",
            "      x = input_layer_lib.Input(shape=(32,))",
            "      y = MaskedLayer()(x)  # pylint: disable=not-callable",
            "      network = functional.Functional(x, y)",
            "",
            "      # test callability on Input",
            "      x_2 = input_layer_lib.Input(shape=(32,))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 32])",
            "",
            "      # test callability on regular tensor",
            "      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 32])",
            "",
            "  def test_activity_regularization_with_model_composition(self):",
            "",
            "    def reg(x):",
            "      return math_ops.reduce_sum(x)",
            "",
            "    net_a_input = input_layer_lib.Input((2,))",
            "    net_a = net_a_input",
            "    net_a = layers.Dense(",
            "        2, kernel_initializer='ones', use_bias=False, activity_regularizer=reg)(",
            "            net_a)",
            "    model_a = training_lib.Model([net_a_input], [net_a])",
            "",
            "    net_b_input = input_layer_lib.Input((2,))",
            "    net_b = model_a(net_b_input)",
            "    model_b = training_lib.Model([net_b_input], [net_b])",
            "",
            "    model_b.compile(optimizer='sgd', loss=None)",
            "    x = np.ones((1, 2))",
            "    loss = model_b.evaluate(x)",
            "    self.assertEqual(loss, 4.)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_layer_sharing_at_heterogenous_depth(self):",
            "    x_val = np.random.random((10, 5))",
            "",
            "    x = input_layer_lib.Input(shape=(5,))",
            "    a = layers.Dense(5, name='A')",
            "    b = layers.Dense(5, name='B')",
            "    output = a(b(a(b(x))))",
            "    m = training_lib.Model(x, output)",
            "    m.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    output_val = m.predict(x_val)",
            "",
            "    config = m.get_config()",
            "    weights = m.get_weights()",
            "",
            "    m2 = models.Model.from_config(config)",
            "    m2.set_weights(weights)",
            "",
            "    output_val_2 = m2.predict(x_val)",
            "    self.assertAllClose(output_val, output_val_2, atol=1e-6)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_layer_sharing_at_heterogenous_depth_with_concat(self):",
            "    input_shape = (16, 9, 3)",
            "    input_layer = input_layer_lib.Input(shape=input_shape)",
            "",
            "    a = layers.Dense(3, name='dense_A')",
            "    b = layers.Dense(3, name='dense_B')",
            "    c = layers.Dense(3, name='dense_C')",
            "",
            "    x1 = b(a(input_layer))",
            "    x2 = a(c(input_layer))",
            "    output = layers.concatenate([x1, x2])",
            "",
            "    m = training_lib.Model(inputs=input_layer, outputs=output)",
            "    m.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    x_val = np.random.random((10, 16, 9, 3))",
            "    output_val = m.predict(x_val)",
            "",
            "    config = m.get_config()",
            "    weights = m.get_weights()",
            "",
            "    m2 = models.Model.from_config(config)",
            "    m2.set_weights(weights)",
            "",
            "    output_val_2 = m2.predict(x_val)",
            "    self.assertAllClose(output_val, output_val_2, atol=1e-6)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_explicit_training_argument(self):",
            "    a = layers.Input(shape=(2,))",
            "    b = layers.Dropout(0.5)(a)",
            "    base_model = training_lib.Model(a, b)",
            "",
            "    a = layers.Input(shape=(2,))",
            "    b = base_model(a, training=False)",
            "    model = training_lib.Model(a, b)",
            "",
            "    x = np.ones((100, 2))",
            "    y = np.ones((100, 2))",
            "    model.compile(",
            "        optimizer='sgd',",
            "        loss='mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    loss = model.train_on_batch(x, y)",
            "    self.assertEqual(loss, 0)  # In inference mode, output is equal to input.",
            "",
            "    a = layers.Input(shape=(2,))",
            "    b = base_model(a, training=True)",
            "    model = training_lib.Model(a, b)",
            "    preds = model.predict(x)",
            "    self.assertEqual(np.min(preds), 0.)  # At least one unit was dropped.",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_mask_derived_from_keras_layer(self):",
            "    inputs = input_layer_lib.Input((5, 10))",
            "    mask = input_layer_lib.Input((5,))",
            "    outputs = layers.RNN(layers.LSTMCell(100))(inputs, mask=mask)",
            "    model = training_lib.Model([inputs, mask], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # All data is masked, returned values are 0's.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.ones((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # Data is not masked, returned values are random.",
            "    self.assertGreater(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(model.get_config())",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # All data is masked, returned values are 0's.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.ones((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # Data is not masked, returned values are random.",
            "    self.assertGreater(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_arg_derived_from_keras_layer(self):",
            "",
            "    class MyAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2):",
            "        return x1 + x2",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    outputs = MyAdd()(input1, input2)",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check serialization.",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'MyAdd': MyAdd})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)",
            "  def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):",
            "    # This functionality is unsupported in v1 graphs",
            "",
            "    class MyAddAll(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        x = inputs[0]",
            "        for inp in inputs[1:]:",
            "          if inp is not None:",
            "            x = x + inp",
            "        return x",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    layer = MyAddAll()",
            "    outputs = layer([0.0, input1, None, input2, None])",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    self.assertIn(layer, model.layers)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check serialization.",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'MyAddAll': MyAddAll})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(",
            "      combinations.times(",
            "          combinations.keras_mode_combinations(),",
            "          combinations.combine(share_already_used_layer=[True, False])))",
            "  def test_call_kwarg_derived_from_keras_layer(self, share_already_used_layer):",
            "",
            "    class MaybeAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2=None):",
            "        if x2 is not None:",
            "          return x1 + x2",
            "        return x1",
            "",
            "    class IdentityLayer(layers.Layer):",
            "",
            "      def call(self, x):",
            "        return x",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    identity_layer = IdentityLayer()",
            "",
            "    if share_already_used_layer:",
            "      # We have had model serialization/deserialization break in the past:",
            "      # when a layer was previously used to construct other functional models",
            "      # and had a non-empty list of inbound nodes before being used to define",
            "      # the model being serialized/deserialized.",
            "      # (The serialization/deserialization was not correctly adjusting",
            "      # the node_index serialization/deserialization).",
            "      # So, we explicitly test this case.",
            "      training_lib.Model([input1], identity_layer(input1))",
            "",
            "    outputs = MaybeAdd()(input1, x2=identity_layer(input2))",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(),",
            "        custom_objects={",
            "            'MaybeAdd': MaybeAdd,",
            "            'IdentityLayer': IdentityLayer",
            "        })",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_kwarg_dtype_serialization(self):",
            "",
            "    class Double(layers.Layer):",
            "",
            "      def call(self, x1, dtype=None):",
            "        return math_ops.cast(x1 + x1, dtype=dtype)",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    outputs = Double()(input1, dtype=dtypes.float16)",
            "    model = training_lib.Model([input1], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10))],",
            "        y=6 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that input was correctly doubled.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check the output dtype",
            "    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'Double': Double})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10))],",
            "        y=6 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that input was correctly doubled.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check the output dtype",
            "    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_kwarg_nonserializable(self):",
            "",
            "    class Double(layers.Layer):",
            "",
            "      def call(self, x1, kwarg=None):",
            "        return x1 + x1",
            "",
            "    class NonSerializable(object):",
            "",
            "      def __init__(self, foo=None):",
            "        self.foo = foo",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    outputs = Double()(input1, kwarg=NonSerializable())",
            "    model = training_lib.Model([input1], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10))],",
            "        y=6 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that input was correctly doubled.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "    with self.assertRaisesRegex(",
            "        TypeError, 'Layer double was passed non-JSON-serializable arguments.'):",
            "      model.get_config()",
            "",
            "  @combinations.generate(",
            "      combinations.times(",
            "          combinations.keras_mode_combinations(),",
            "          combinations.combine(share_already_used_layer=[True, False])))",
            "  def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(",
            "      self, share_already_used_layer):",
            "",
            "    class IdentityLayer(layers.Layer):",
            "",
            "      def call(self, x):",
            "        return x",
            "",
            "    class MaybeAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2=None):",
            "        if x2 is not None:",
            "          return x1 + x2",
            "        return x1",
            "",
            "    input2 = input_layer_lib.Input(10)",
            "    identity_layer = IdentityLayer()",
            "    if share_already_used_layer:",
            "      # We have had model serialization/deserialization break in the past:",
            "      # when a layer was previously used to construct other functional models",
            "      # and had a non-empty list of inbound nodes before being used to define",
            "      # the model being serialized/deserialized.",
            "      # (The serialization/deserialization was not correctly adjusting",
            "      # the node_index serialization/deserialization).",
            "      # So, we explicitly test this case.",
            "      training_lib.Model([input2], identity_layer(input2))",
            "",
            "    outputs = MaybeAdd()(3., x2=identity_layer(input2))",
            "    model = training_lib.Model([input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=7 * np.ones((10, 10)),",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(),",
            "        custom_objects={",
            "            'MaybeAdd': MaybeAdd,",
            "            'IdentityLayer': IdentityLayer",
            "        })",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=7 * np.ones((10, 10)),",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_composite_call_kwarg_derived_from_keras_layer(self):",
            "",
            "    # Create a test layer that accepts composite tensor inputs.",
            "    class MaybeAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2=None):",
            "        # We need to convert this to a tensor for loss calculations -",
            "        # losses don't play nicely with ragged tensors yet.",
            "        if x2 is not None:",
            "          return (x1 + x2).to_tensor(default_value=0)",
            "        return x1.to_tensor(default_value=0)",
            "",
            "    input1 = input_layer_lib.Input((None,), ragged=True)",
            "    input2 = input_layer_lib.Input((None,), ragged=True)",
            "    outputs = MaybeAdd()(input1, x2=input2)",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    input_data = [",
            "        ragged_factory_ops.constant([[3.0, 3.0], [3.0, 3.0], [3.0]]),",
            "        ragged_factory_ops.constant([[7.0, 7.0], [7.0, 7.0], [7.0]])",
            "    ]",
            "    expected_data = np.array([[10.0, 10.0], [10.0, 10.0], [10.0, 0.0]])",
            "",
            "    history = model.fit(x=input_data, y=expected_data)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'MaybeAdd': MaybeAdd})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(x=input_data, y=expected_data)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))",
            "  def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):",
            "    # This functionality is unsupported in v1 graphs",
            "",
            "    class AddAll(layers.Layer):",
            "",
            "      def call(self, x1_x2, x3):",
            "        x1, x2 = x1_x2",
            "        out = x1 + x2",
            "        if x3 is not None:",
            "          for t in x3.values():",
            "            out += t",
            "        return out",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    input3 = input_layer_lib.Input(10)",
            "",
            "    layer = AddAll()",
            "    outputs = layer(",
            "        [input1, 4 * array_ops.ones((1, 10))],",
            "        x3={",
            "            'a': input2,",
            "            'b': input3,",
            "            'c': 5 * array_ops.ones((1, 10))",
            "        })",
            "    model = training_lib.Model([input1, input2, input3], outputs)",
            "    self.assertIn(layer, model.layers)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'AddAll': AddAll})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_nested_arg_derived_from_keras_layer(self):",
            "",
            "    class AddAll(layers.Layer):",
            "",
            "      def call(self, x1, x2, x3=None):",
            "        out = x1 + x2",
            "        if x3 is not None:",
            "          for t in x3.values():",
            "            out += t",
            "        return out",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    input3 = input_layer_lib.Input(10)",
            "    outputs = AddAll()(",
            "        input1,",
            "        4 * array_ops.ones((1, 10)),",
            "        x3={",
            "            'a': input2,",
            "            'b': input3,",
            "            'c': 5 * array_ops.ones((1, 10))",
            "        })",
            "    model = training_lib.Model([input1, input2, input3], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'AddAll': AddAll})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_multi_output_model_with_none_masking(self):",
            "    def func(x):",
            "      return [x * 0.2, x * 0.3]",
            "",
            "    def output_shape(input_shape):",
            "      return [input_shape, input_shape]",
            "",
            "    i = layers.Input(shape=(3, 2, 1))",
            "    o = layers.Lambda(function=func, output_shape=output_shape)(i)",
            "",
            "    self.assertEqual(backend.int_shape(o[0]), (None, 3, 2, 1))",
            "    self.assertEqual(backend.int_shape(o[1]), (None, 3, 2, 1))",
            "",
            "    o = layers.add(o)",
            "    model = training_lib.Model(i, o)",
            "    model.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    i2 = layers.Input(shape=(3, 2, 1))",
            "    o2 = model(i2)",
            "    model2 = training_lib.Model(i2, o2)",
            "    model2.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    x = np.random.random((4, 3, 2, 1))",
            "    out = model2.predict(x)",
            "    assert out.shape == (4, 3, 2, 1)",
            "    self.assertAllClose(out, x * 0.2 + x * 0.3, atol=1e-4)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_constant_initializer_with_numpy(self):",
            "    initializer = initializers.Constant(np.ones((3, 2)))",
            "    model = sequential.Sequential()",
            "    model.add(layers.Dense(2, input_shape=(3,), kernel_initializer=initializer))",
            "    model.add(layers.Dense(3))",
            "    model.compile(",
            "        loss='mse',",
            "        optimizer='sgd',",
            "        metrics=['acc'],",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "",
            "    json_str = model.to_json()",
            "    models.model_from_json(json_str)",
            "",
            "    if yaml is not None:",
            "      yaml_str = model.to_yaml()",
            "      models.model_from_yaml(yaml_str)",
            "",
            "  def test_subclassed_error_if_init_not_called(self):",
            "",
            "    class MyNetwork(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        self._foo = [layers.Dense(10), layers.Dense(10)]",
            "",
            "    with self.assertRaisesRegex(RuntimeError, 'forgot to call'):",
            "      MyNetwork()",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_int_input_shape(self):",
            "    inputs = input_layer_lib.Input(10)",
            "    self.assertEqual([None, 10], inputs.shape.as_list())",
            "",
            "    inputs_with_batch = input_layer_lib.Input(batch_size=20, shape=5)",
            "    self.assertEqual([20, 5], inputs_with_batch.shape.as_list())",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_model_initialization(self):",
            "    # Functional model",
            "    inputs = input_layer_lib.Input(shape=(32,))",
            "    outputs = layers.Dense(4)(inputs)",
            "",
            "    with self.assertRaisesRegex(TypeError,",
            "                                'Keyword argument not understood'):",
            "      model = training_lib.Model(",
            "          inputs, outputs, name='m', trainable=False, dtype='int64')",
            "    with self.assertRaisesRegex(TypeError,",
            "                                'Keyword argument not understood'):",
            "      model = training_lib.Model(",
            "          inputs, outputs, name='m', trainable=False, dynamic=False)",
            "",
            "    model = training_lib.Model(inputs, outputs, name='m', trainable=False)",
            "    self.assertEqual('m', model.name)",
            "    self.assertFalse(model.trainable)",
            "    self.assertFalse(model.dynamic)",
            "",
            "    class SubclassModel(training_lib.Model):",
            "      pass",
            "    # Subclassed model",
            "    model = SubclassModel(",
            "        name='subclassed', trainable=True, dtype='int64', dynamic=True)",
            "    self.assertEqual('subclassed', model.name)",
            "    self.assertTrue(model.dynamic)",
            "    self.assertTrue(model.trainable)",
            "    w = model.add_weight('w', [], initializer=initializers.Constant(1))",
            "    self.assertEqual(dtypes.int64, w.dtype)",
            "",
            "  def test_disconnected_inputs(self):",
            "    input_tensor1 = input_layer_lib.Input(shape=[200], name='a')",
            "    input_tensor2 = input_layer_lib.Input(shape=[10], name='b')",
            "    output_tensor1 = layers.Dense(units=10)(input_tensor1)",
            "",
            "    net = functional.Functional(",
            "        inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1])",
            "    net2 = functional.Functional.from_config(net.get_config())",
            "    self.assertLen(net2.inputs, 2)",
            "    self.assertEqual('a', net2.layers[0].name)",
            "    self.assertEqual('b', net2.layers[1].name)",
            "",
            "  @combinations.generate(combinations.keras_model_type_combinations())",
            "  def test_dependency_tracking(self):",
            "    model = testing_utils.get_small_mlp(1, 4, input_dim=3)",
            "    model.trackable = Checkpoint()",
            "    self.assertIn('trackable', model._unconditional_dependency_names)",
            "    self.assertEqual(model.trackable, model._lookup_dependency('trackable'))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_model_construction_in_tf_function(self):",
            "",
            "    d = {'model': None}",
            "",
            "    @def_function.function",
            "    def fn(x):",
            "      if d['model'] is None:",
            "        # Check that Functional can be built in a `tf.function`.",
            "        inputs = input_layer_lib.Input(10)",
            "        outputs = layers.Dense(1)(inputs)",
            "        model = functional.Functional(inputs, outputs)",
            "        d['model'] = model",
            "      else:",
            "        model = d['model']",
            "",
            "      return model(x)",
            "",
            "    x = array_ops.ones((10, 10))",
            "    y = fn(x)",
            "    self.assertEqual(y.shape.as_list(), [10, 1])",
            "",
            "",
            "class DeferredModeTest(keras_parameterized.TestCase):",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testSimpleNetworkBuilding(self):",
            "    inputs = input_layer_lib.Input(shape=(32,))",
            "    if context.executing_eagerly():",
            "      self.assertEqual(inputs.dtype.name, 'float32')",
            "      self.assertEqual(inputs.shape.as_list(), [None, 32])",
            "",
            "    x = layers.Dense(2)(inputs)",
            "    if context.executing_eagerly():",
            "      self.assertEqual(x.dtype.name, 'float32')",
            "      self.assertEqual(x.shape.as_list(), [None, 2])",
            "",
            "    outputs = layers.Dense(4)(x)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertIsInstance(network, functional.Functional)",
            "",
            "    if context.executing_eagerly():",
            "      # It should be possible to call such a network on EagerTensors.",
            "      inputs = constant_op.constant(",
            "          np.random.random((10, 32)).astype('float32'))",
            "      outputs = network(inputs)",
            "      self.assertEqual(outputs.shape.as_list(), [10, 4])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testMultiIONetworkBuilding(self):",
            "    input_a = input_layer_lib.Input(shape=(32,))",
            "    input_b = input_layer_lib.Input(shape=(16,))",
            "    a = layers.Dense(16)(input_a)",
            "",
            "    class AddLayer(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        return inputs[0] + inputs[1]",
            "",
            "    c = AddLayer()([a, input_b])  # pylint: disable=not-callable",
            "    c = layers.Dense(2)(c)",
            "",
            "    network = functional.Functional([input_a, input_b], [a, c])",
            "    if context.executing_eagerly():",
            "      a_val = constant_op.constant(",
            "          np.random.random((10, 32)).astype('float32'))",
            "      b_val = constant_op.constant(",
            "          np.random.random((10, 16)).astype('float32'))",
            "      outputs = network([a_val, b_val])",
            "      self.assertEqual(len(outputs), 2)",
            "      self.assertEqual(outputs[0].shape.as_list(), [10, 16])",
            "      self.assertEqual(outputs[1].shape.as_list(), [10, 2])",
            "",
            "",
            "class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):",
            "",
            "  def _testShapeInference(self, model, input_shape, expected_output_shape):",
            "    input_value = np.random.random(input_shape)",
            "    output_value = model.predict(input_value)",
            "    self.assertEqual(output_value.shape, expected_output_shape)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testSingleInputCase(self):",
            "",
            "    class LayerWithOneInput(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.w = array_ops.ones(shape=(3, 4))",
            "",
            "      def call(self, inputs):",
            "        return backend.dot(inputs, self.w)",
            "",
            "    inputs = input_layer_lib.Input(shape=(3,))",
            "    layer = LayerWithOneInput()",
            "",
            "    if context.executing_eagerly():",
            "      self.assertEqual(",
            "          layer.compute_output_shape((None, 3)).as_list(), [None, 4])",
            "      # As a side-effect, compute_output_shape builds the layer.",
            "      self.assertTrue(layer.built)",
            "      # We can still query the layer's compute_output_shape with compatible",
            "      # input shapes.",
            "      self.assertEqual(",
            "          layer.compute_output_shape((6, 3)).as_list(), [6, 4])",
            "",
            "    outputs = layer(inputs)",
            "    model = training_lib.Model(inputs, outputs)",
            "    self._testShapeInference(model, (2, 3), (2, 4))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testMultiInputOutputCase(self):",
            "",
            "    class MultiInputOutputLayer(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.w = array_ops.ones(shape=(3, 4))",
            "",
            "      def call(self, inputs):",
            "        a = backend.dot(inputs[0], self.w)",
            "        b = a + inputs[1]",
            "        return [a, b]",
            "",
            "    input_a = input_layer_lib.Input(shape=(3,))",
            "    input_b = input_layer_lib.Input(shape=(4,))",
            "    output_a, output_b = MultiInputOutputLayer()([input_a, input_b])",
            "    model = training_lib.Model([input_a, input_b], [output_a, output_b])",
            "    output_a_val, output_b_val = model.predict(",
            "        [np.random.random((2, 3)), np.random.random((2, 4))])",
            "    self.assertEqual(output_a_val.shape, (2, 4))",
            "    self.assertEqual(output_b_val.shape, (2, 4))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTrainingArgument(self):",
            "",
            "    class LayerWithTrainingArg(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.w = array_ops.ones(shape=(3, 4))",
            "",
            "      def call(self, inputs, training):",
            "        return backend.dot(inputs, self.w)",
            "",
            "    inputs = input_layer_lib.Input(shape=(3,))",
            "    outputs = LayerWithTrainingArg()(inputs, training=False)",
            "    model = training_lib.Model(inputs, outputs)",
            "    self._testShapeInference(model, (2, 3), (2, 4))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testNoneInShape(self):",
            "",
            "    class Model(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        super(Model, self).__init__()",
            "        self.conv1 = layers.Conv2D(8, 3)",
            "        self.pool = layers.GlobalAveragePooling2D()",
            "        self.fc = layers.Dense(3)",
            "",
            "      def call(self, x):",
            "        x = self.conv1(x)",
            "        x = self.pool(x)",
            "        x = self.fc(x)",
            "        return x",
            "",
            "    model = Model()",
            "    model.build(tensor_shape.TensorShape((None, None, None, 1)))",
            "    self.assertTrue(model.built, 'Model should be built')",
            "    self.assertTrue(model.weights,",
            "                    'Model should have its weights created as it '",
            "                    'has been built')",
            "    sample_input = array_ops.ones((1, 10, 10, 1))",
            "    output = model(sample_input)",
            "    self.assertEqual(output.shape, (1, 3))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testNoneInShapeWithCompoundModel(self):",
            "",
            "    class BasicBlock(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        super(BasicBlock, self).__init__()",
            "        self.conv1 = layers.Conv2D(8, 3)",
            "        self.pool = layers.GlobalAveragePooling2D()",
            "        self.dense = layers.Dense(3)",
            "",
            "      def call(self, x):",
            "        x = self.conv1(x)",
            "        x = self.pool(x)",
            "        x = self.dense(x)",
            "        return x",
            "",
            "    class CompoundModel(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        super(CompoundModel, self).__init__()",
            "        self.block = BasicBlock()",
            "",
            "      def call(self, x):",
            "        x = self.block(x)  # pylint: disable=not-callable",
            "        return x",
            "",
            "    model = CompoundModel()",
            "    model.build(tensor_shape.TensorShape((None, None, None, 1)))",
            "    self.assertTrue(model.built, 'Model should be built')",
            "    self.assertTrue(model.weights,",
            "                    'Model should have its weights created as it '",
            "                    'has been built')",
            "    sample_input = array_ops.ones((1, 10, 10, 1))",
            "    output = model(sample_input)  # pylint: disable=not-callable",
            "    self.assertEqual(output.shape, (1, 3))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testNoneInShapeWithFunctionalAPI(self):",
            "",
            "    class BasicBlock(training_lib.Model):",
            "      # Inheriting from layers.Layer since we are calling this layer",
            "      # inside a model created using functional API.",
            "",
            "      def __init__(self):",
            "        super(BasicBlock, self).__init__()",
            "        self.conv1 = layers.Conv2D(8, 3)",
            "",
            "      def call(self, x):",
            "        x = self.conv1(x)",
            "        return x",
            "",
            "    input_layer = layers.Input(shape=(None, None, 1))",
            "    x = BasicBlock()(input_layer)",
            "    x = layers.GlobalAveragePooling2D()(x)",
            "    output_layer = layers.Dense(3)(x)",
            "",
            "    model = training_lib.Model(inputs=input_layer, outputs=output_layer)",
            "",
            "    model.build(tensor_shape.TensorShape((None, None, None, 1)))",
            "    self.assertTrue(model.built, 'Model should be built')",
            "    self.assertTrue(model.weights,",
            "                    'Model should have its weights created as it '",
            "                    'has been built')",
            "    sample_input = array_ops.ones((1, 10, 10, 1))",
            "    output = model(sample_input)",
            "    self.assertEqual(output.shape, (1, 3))",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_sequential_as_downstream_of_masking_layer(self):",
            "    inputs = layers.Input(shape=(3, 4))",
            "    x = layers.Masking(mask_value=0., input_shape=(3, 4))(inputs)",
            "",
            "    s = sequential.Sequential()",
            "    s.add(layers.Dense(5, input_shape=(4,)))",
            "",
            "    x = layers.wrappers.TimeDistributed(s)(x)",
            "    model = training_lib.Model(inputs=inputs, outputs=x)",
            "    model.compile(",
            "        optimizer='rmsprop',",
            "        loss='mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "",
            "    model_input = np.random.randint(",
            "        low=1, high=5, size=(10, 3, 4)).astype('float32')",
            "    for i in range(4):",
            "      model_input[i, i:, :] = 0.",
            "    model.fit(model_input,",
            "              np.random.random((10, 3, 5)), epochs=1, batch_size=6)",
            "",
            "    if not context.executing_eagerly():",
            "      # Note: this doesn't work in eager due to DeferredTensor/ops compatibility",
            "      # issue.",
            "      mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]",
            "      mask_outputs += [model.layers[2].compute_mask(",
            "          model.layers[2].input, mask_outputs[-1])]",
            "      func = backend.function([model.input], mask_outputs)",
            "      mask_outputs_val = func([model_input])",
            "      self.assertAllClose(mask_outputs_val[0], np.any(model_input, axis=-1))",
            "      self.assertAllClose(mask_outputs_val[1], np.any(model_input, axis=-1))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_external_keras_serialization_compat_input_layers(self):",
            "    inputs = input_layer_lib.Input(shape=(10,))",
            "    outputs = layers.Dense(1)(inputs)",
            "    model = training_lib.Model(inputs, outputs)",
            "    config = model.get_config()",
            "    # Checks that single inputs and outputs are still saved as 1-element lists.",
            "    # Saving as 1-element lists or not is equivalent in TF Keras, but only the",
            "    # 1-element list format is supported in TF.js and keras-team/Keras.",
            "    self.assertLen(config['input_layers'], 1)",
            "    self.assertLen(config['output_layers'], 1)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_external_keras_serialization_compat_inbound_nodes(self):",
            "    # Check single Tensor input.",
            "    inputs = input_layer_lib.Input(shape=(10,), name='in')",
            "    outputs = layers.Dense(1)(inputs)",
            "    model = training_lib.Model(inputs, outputs)",
            "    config = model.get_config()",
            "    self.assertEqual(config['layers'][1]['inbound_nodes'], [[['in', 0, 0, {}]]])",
            "",
            "    # Check multiple Tensor input.",
            "    inputs1 = input_layer_lib.Input(shape=(10,), name='in1')",
            "    inputs2 = input_layer_lib.Input(shape=(10,), name='in2')",
            "    outputs = layers.Add()([inputs1, inputs2])",
            "    model = training_lib.Model([inputs1, inputs2], outputs)",
            "    config = model.get_config()",
            "    self.assertEqual(config['layers'][2]['inbound_nodes'],",
            "                     [[['in1', 0, 0, {}], ['in2', 0, 0, {}]]])",
            "",
            "  @combinations.generate(combinations.combine(mode=['eager']))",
            "  def test_dict_inputs_tensors(self):",
            "    # Note that this test is running with v2 eager only, since the v1",
            "    # will behave differently wrt to dict input for training.",
            "    inputs = {",
            "        'sentence2': input_layer_lib.Input(",
            "            shape=(), name='a', dtype=dtypes.string),",
            "        'sentence1': input_layer_lib.Input(",
            "            shape=(), name='b', dtype=dtypes.string),",
            "    }",
            "    strlen = layers.Lambda(string_ops.string_length_v2)",
            "    diff = layers.Subtract()(",
            "        [strlen(inputs['sentence1']), strlen(inputs['sentence2'])])",
            "    diff = math_ops.cast(diff, dtypes.float32)",
            "    model = training_lib.Model(inputs, diff)",
            "",
            "    extra_keys = {",
            "        'sentence1': constant_op.constant(['brown fox', 'lazy dog']),",
            "        'sentence2': constant_op.constant(['owl', 'cheeky cat']),",
            "        'label': constant_op.constant([0, 1]),",
            "    }",
            "",
            "    with warnings.catch_warnings(record=True) as w:",
            "      warnings.simplefilter('always')",
            "      model(extra_keys)",
            "      self.assertIn('ignored by the model', str(w[-1].message))",
            "",
            "    model.compile('sgd', 'mse')",
            "    with warnings.catch_warnings(record=True) as w:",
            "      warnings.simplefilter('always')",
            "      model.fit(extra_keys, y=constant_op.constant([0, 1]), steps_per_epoch=1)",
            "      self.assertIn('ignored by the model', str(w[-1].message))",
            "",
            "    with warnings.catch_warnings(record=True) as w:",
            "      warnings.simplefilter('always')",
            "      model.evaluate(extra_keys, constant_op.constant([0, 1]))",
            "      self.assertIn('ignored by the model', str(w[-1].message))",
            "",
            "    # Make sure the model inputs are sorted with the dict keys.",
            "    self.assertEqual(model.inputs[0]._keras_history.layer.name, 'b')",
            "    self.assertEqual(model.inputs[1]._keras_history.layer.name, 'a')",
            "",
            "",
            "class GraphUtilsTest(test.TestCase):",
            "",
            "  def testGetReachableFromInputs(self):",
            "",
            "    with ops.Graph().as_default(), self.cached_session():",
            "      pl_1 = array_ops.placeholder(shape=None, dtype='float32')",
            "      pl_2 = array_ops.placeholder(shape=None, dtype='float32')",
            "      pl_3 = array_ops.placeholder(shape=None, dtype='float32')",
            "      x_1 = pl_1 + pl_2",
            "      x_2 = pl_2 * 2",
            "      x_3 = pl_3 + 1",
            "      x_4 = x_1 + x_2",
            "      x_5 = x_3 * pl_1",
            "",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([pl_1]),",
            "          {pl_1, x_1, x_4, x_5, x_1.op, x_4.op, x_5.op})",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([pl_1, pl_2]),",
            "          {pl_1, pl_2, x_1, x_2, x_4, x_5, x_1.op, x_2.op, x_4.op, x_5.op})",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([pl_3]),",
            "          {pl_3, x_3, x_5, x_3.op, x_5.op})",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([x_3]), {x_3, x_5, x_5.op})",
            "",
            "",
            "class NestedNetworkTest(keras_parameterized.TestCase):",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_inputs_network(self):",
            "    inputs = {",
            "        'x1': input_layer_lib.Input(shape=(1,)),",
            "        'x2': input_layer_lib.Input(shape=(1,))",
            "    }",
            "    outputs = layers.Add()([inputs['x1'], inputs['x2']])",
            "    network = functional.Functional(inputs, outputs)",
            "",
            "    network = functional.Functional.from_config(network.get_config())",
            "",
            "    result_tensor = network({",
            "        'x1': array_ops.ones((1, 1), 'float32'),",
            "        'x2': array_ops.ones((1, 1), 'float32')",
            "    })",
            "    result = self.evaluate(result_tensor)",
            "    self.assertAllEqual(result, [[2.]])",
            "",
            "    # TODO(b/122726584): Investigate why concrete batch is flaky in some builds.",
            "    output_shape = network.compute_output_shape({",
            "        'x1': (None, 1),",
            "        'x2': (None, 1)",
            "    })",
            "    self.assertListEqual(output_shape.as_list(), [None, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_outputs_network(self):",
            "    inputs = input_layer_lib.Input(shape=(1,))",
            "    outputs = {",
            "        'x+x': layers.Add()([inputs, inputs]),",
            "        'x*x': layers.Multiply()([inputs, inputs])",
            "    }",
            "",
            "    network = functional.Functional(inputs, outputs)",
            "",
            "    network = functional.Functional.from_config(network.get_config())",
            "",
            "    result_tensor = network(array_ops.ones((1, 1), 'float32'))",
            "    result = self.evaluate(result_tensor)",
            "    self.assertAllEqual(result['x+x'], [[2.]])",
            "    self.assertAllEqual(result['x*x'], [[1.]])",
            "",
            "    output_shape = network.compute_output_shape((None, 1))",
            "    self.assertListEqual(output_shape['x+x'].as_list(), [None, 1])",
            "    self.assertListEqual(output_shape['x*x'].as_list(), [None, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_network_inside_network(self):",
            "    inner_inputs = {",
            "        'x1': input_layer_lib.Input(shape=(1,)),",
            "        'x2': input_layer_lib.Input(shape=(1,))",
            "    }",
            "    inner_outputs = {",
            "        'x1+x2': layers.Add()([inner_inputs['x1'], inner_inputs['x2']]),",
            "        'x1*x2': layers.Multiply()([inner_inputs['x1'], inner_inputs['x2']])",
            "    }",
            "    inner_network = functional.Functional(",
            "        inner_inputs, inner_outputs)",
            "",
            "    inputs = [",
            "        input_layer_lib.Input(shape=(1,)),",
            "        input_layer_lib.Input(shape=(1,))",
            "    ]",
            "    middle = inner_network({'x1': inputs[0], 'x2': inputs[1]})",
            "    outputs = layers.Add()([middle['x1+x2'], middle['x1*x2']])",
            "    network = functional.Functional(inputs, outputs)",
            "",
            "    network = functional.Functional.from_config(network.get_config())",
            "",
            "    # Computes: `(x1+x2) + (x1*x2)`",
            "    result_tensor = network(",
            "        [array_ops.ones((1, 1), 'float32'),",
            "         array_ops.ones((1, 1), 'float32')])",
            "    result = self.evaluate(result_tensor)",
            "    self.assertAllEqual(result, [[3.]])",
            "",
            "    output_shape = network.compute_output_shape([(None, 1), (None, 1)])",
            "    self.assertListEqual(output_shape.as_list(), [None, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph']))",
            "  def test_updates_with_direct_call(self):",
            "    inputs = input_layer_lib.Input(shape=(10,))",
            "    x = layers.BatchNormalization()(inputs)",
            "    x = layers.Dense(10)(x)",
            "    model = training_lib.Model(inputs, x)",
            "",
            "    ph = backend.placeholder(shape=(10, 10))",
            "    model(ph)",
            "",
            "    self.assertLen(model.updates, 4)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_dict_mapping_input(self):",
            "",
            "    class ReturnFirst(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        b, _ = inputs",
            "        return b",
            "",
            "    # Checks that inputs are put in same order as the",
            "    # Model was constructed with.",
            "    b = input_layer_lib.Input(shape=(10,), name='b')",
            "    a = input_layer_lib.Input(shape=(10,), name='a')",
            "    outputs = ReturnFirst()([b, a])",
            "",
            "    b_val = array_ops.ones((10, 10))",
            "    a_val = array_ops.zeros((10, 10))",
            "",
            "    model = training_lib.Model([b, a], outputs)",
            "    res = model({'a': a_val, 'b': b_val})",
            "    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))",
            "",
            "    reversed_model = training_lib.Model([a, b], outputs)",
            "    res = reversed_model({'a': a_val, 'b': b_val})",
            "    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_dict_mapping_single_input(self):",
            "    b = input_layer_lib.Input(shape=(1,), name='b')",
            "    outputs = b * 2",
            "    model = training_lib.Model(b, outputs)",
            "",
            "    b_val = array_ops.ones((1, 1))",
            "    extra_val = array_ops.ones((1, 10))",
            "",
            "    inputs = {'a': extra_val, 'b': b_val}",
            "    res = model(inputs)",
            "",
            "    # Check that 'b' was used and 'a' was ignored.",
            "    self.assertEqual(res.shape.as_list(), [1, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_dict_mapping(self):",
            "    a = input_layer_lib.Input(shape=(1,), dtype='int32', name='a')",
            "    b = input_layer_lib.Input(shape=(1,), dtype='int32', name='b')",
            "    c = input_layer_lib.Input(shape=(1,), dtype='int32', name='c')",
            "    d = input_layer_lib.Input(shape=(1,), dtype='int32', name='d')",
            "    inputs = {'a': (a, b), 'c': (c, d)}",
            "    outputs = 1000 * a + 100 * b + 10 * c + d",
            "    model = training_lib.Model(inputs, outputs)",
            "",
            "    a_val = array_ops.ones((1, 1), dtype='int32')",
            "    b_val = 2 * array_ops.ones((1, 1), dtype='int32')",
            "    c_val = 3 * array_ops.ones((1, 1), dtype='int32')",
            "    d_val = 4 * array_ops.ones((1, 1), dtype='int32')",
            "",
            "    inputs_val = {'a': (a_val, b_val), 'c': (c_val, d_val)}",
            "    res = model(inputs_val)",
            "",
            "    # Check that inputs were flattened in the correct order.",
            "    self.assertFalse(model._enable_dict_to_input_mapping)",
            "    self.assertEqual(self.evaluate(res), [1234])",
            "",
            "",
            "@combinations.generate(combinations.keras_mode_combinations())",
            "class AddLossTest(keras_parameterized.TestCase):",
            "",
            "  def test_add_loss_outside_call_only_loss(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    mid = layers.Dense(10)(inputs)",
            "    outputs = layers.Dense(1)(mid)",
            "    model = training_lib.Model(inputs, outputs)",
            "    model.add_loss(math_ops.reduce_mean(outputs))",
            "    self.assertLen(model.losses, 1)",
            "",
            "    initial_weights = model.get_weights()",
            "",
            "    x = np.ones((10, 10))",
            "    model.compile(",
            "        'sgd',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model.fit(x, batch_size=2, epochs=1)",
            "",
            "    model2 = model.from_config(model.get_config())",
            "    model2.compile(",
            "        'sgd',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model2.set_weights(initial_weights)",
            "    model2.fit(x, batch_size=2, epochs=1)",
            "",
            "    # The TFOpLayer and the AddLoss layer are serialized.",
            "    self.assertLen(model2.layers, 5)",
            "    self.assertAllClose(model.get_weights(), model2.get_weights())",
            "",
            "  def test_add_loss_outside_call_multiple_losses(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    x1 = layers.Dense(10)(inputs)",
            "    x2 = layers.Dense(10)(x1)",
            "    outputs = layers.Dense(1)(x2)",
            "    model = training_lib.Model(inputs, outputs)",
            "    model.add_loss(math_ops.reduce_sum(x1 * x2))",
            "    model.add_loss(math_ops.reduce_mean(outputs))",
            "    self.assertLen(model.losses, 2)",
            "",
            "    initial_weights = model.get_weights()",
            "",
            "    x, y = np.ones((10, 10)), np.ones((10, 1))",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model.fit(x, y, batch_size=2, epochs=1)",
            "",
            "    model2 = model.from_config(model.get_config())",
            "    model2.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model2.set_weights(initial_weights)",
            "    model2.fit(x, y, batch_size=2, epochs=1)",
            "",
            "    self.assertAllClose(model.get_weights(), model2.get_weights())",
            "",
            "  def test_add_loss_crossentropy_backtracking(self):",
            "    inputs = input_layer_lib.Input((2,))",
            "    labels = input_layer_lib.Input((1,))",
            "    outputs = layers.Dense(1, activation='sigmoid')(inputs)",
            "    model = functional.Functional([inputs, labels], outputs)",
            "    model.add_loss(losses.binary_crossentropy(labels, outputs))",
            "    model.compile('adam')",
            "    x = np.random.random((2, 2))",
            "    y = np.random.random((2, 1))",
            "    model.fit([x, y])",
            "",
            "    inputs = input_layer_lib.Input((2,))",
            "    labels = input_layer_lib.Input((2,))",
            "    outputs = layers.Dense(2, activation='softmax')(inputs)",
            "    model = functional.Functional([inputs, labels], outputs)",
            "    model.add_loss(losses.categorical_crossentropy(labels, outputs))",
            "    model.compile('adam')",
            "    x = np.random.random((2, 2))",
            "    y = np.random.random((2, 2))",
            "    model.fit([x, y])",
            "",
            "    inputs = input_layer_lib.Input((2,))",
            "    labels = input_layer_lib.Input((1,), dtype='int32')",
            "    outputs = layers.Dense(2, activation='softmax')(inputs)",
            "    model = functional.Functional([inputs, labels], outputs)",
            "    model.add_loss(losses.sparse_categorical_crossentropy(labels, outputs))",
            "    model.compile('adam')",
            "    x = np.random.random((2, 2))",
            "    y = np.random.randint(0, 2, size=(2, 1))",
            "    model.fit([x, y])",
            "",
            "",
            "@combinations.generate(combinations.keras_mode_combinations())",
            "class WeightAccessTest(keras_parameterized.TestCase):",
            "",
            "  def test_functional_model(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    x1 = layers.Dense(10)(inputs)",
            "    x2 = layers.Dense(10)(x1)",
            "    outputs = layers.Dense(1)(x2)",
            "    model = training_lib.Model(inputs, outputs)",
            "",
            "    self.assertEqual(len(model.weights), 6)",
            "",
            "  def test_sequential_model_with_input_shape(self):",
            "    x1 = layers.Dense(10, input_shape=(10,))",
            "    x2 = layers.Dense(10)",
            "    x3 = layers.Dense(1)",
            "    model = sequential.Sequential([x1, x2, x3])",
            "",
            "    self.assertEqual(len(model.weights), 6)",
            "",
            "  def test_sequential_model_without_input_shape(self):",
            "    x1 = layers.Dense(10)",
            "    x2 = layers.Dense(10)",
            "    x3 = layers.Dense(1)",
            "    model = sequential.Sequential([x1, x2, x3])",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError, 'Weights for model .* have not yet been created'):",
            "      _ = model.weights",
            "",
            "  def test_subclass_model_with_build_method(self):",
            "",
            "    class SubclassModel(models.Model):",
            "",
            "      def build(self, input_shape):",
            "        self.w = self.add_weight(shape=input_shape[-1], initializer='ones')",
            "",
            "      def call(self, inputs):",
            "        return inputs * self.w",
            "",
            "    model = SubclassModel()",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError, 'Weights for model .* have not yet been created'):",
            "      _ = model.weights",
            "",
            "    model(input_layer_lib.Input((10,)))",
            "    self.assertEqual(len(model.weights), 1)",
            "",
            "  def test_subclass_model_without_build_method(self):",
            "",
            "    class SubclassModel(models.Model):",
            "",
            "      def __init__(self):",
            "        super(SubclassModel, self).__init__()",
            "        self.w = self.add_weight(shape=(), initializer='ones')",
            "",
            "      def call(self, inputs):",
            "        return inputs * self.w",
            "",
            "    model = SubclassModel()",
            "    self.assertEqual(len(model.weights), 1)",
            "",
            "",
            "@combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "class DTypeTest(keras_parameterized.TestCase):",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_graph_network_dtype(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertEqual(network.dtype, 'float32')",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_subclassed_network_dtype(self):",
            "",
            "    class IdentityNetwork(training_lib.Model):",
            "",
            "      def call(self, inputs):",
            "        return inputs",
            "",
            "    network = IdentityNetwork()",
            "    self.assertEqual(network.dtype, 'float32')",
            "    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float32')",
            "",
            "    network = IdentityNetwork(dtype='float16')",
            "    self.assertEqual(network.dtype, 'float16')",
            "    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float16')",
            "",
            "    network = IdentityNetwork(autocast=False)",
            "    self.assertEqual(network.dtype, 'float32')",
            "    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float64')",
            "",
            "",
            "class AttrTrackingLayer(base_layer.Layer):",
            "  \"\"\"Count how many times `dynamic` and `stateful` are called.",
            "",
            "  These counts are used to test that the attribute cache behaves as expected.",
            "  \"\"\"",
            "  def __init__(self, *args, **kwargs):",
            "    self.stateful_count = 0",
            "    self.dynamic_count = 0",
            "    super(AttrTrackingLayer, self).__init__(*args, **kwargs)",
            "",
            "  @base_layer.Layer.stateful.getter",
            "  def stateful(self):",
            "    self.stateful_count += 1",
            "    return super(AttrTrackingLayer, self).stateful",
            "",
            "  @property",
            "  def dynamic(self):",
            "    self.dynamic_count += 1",
            "    return super(AttrTrackingLayer, self).dynamic",
            "",
            "",
            "@combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "class CacheCorrectnessTest(keras_parameterized.TestCase):",
            "",
            "  def layer_and_network_test(self):",
            "    # Top level layer",
            "    network = functional.Functional()",
            "",
            "    layer_0 = AttrTrackingLayer()",
            "",
            "    sub_network = functional.Functional()",
            "    layer_1 = AttrTrackingLayer(dynamic=True)",
            "    layer_2 = AttrTrackingLayer()",
            "    sub_network.sub_layers = [layer_1, layer_2]",
            "",
            "    network.sub_layer = layer_0",
            "",
            "    for _ in range(2):",
            "      self.assertEqual(network.dynamic, False)",
            "      self.assertEqual(network.stateful, False)",
            "",
            "      # The second pass should be a cache hit.",
            "      self.assertEqual(layer_0.dynamic_count, 1)",
            "      self.assertEqual(layer_0.stateful_count, 1)",
            "",
            "    # Mutations of the sub-layer should force recalculation of the network's",
            "    # stateful attribute. (mutations bubble up.)",
            "    layer_0.stateful = True",
            "    self.assertEqual(network.stateful, True)",
            "    self.assertEqual(layer_0.stateful_count, 2)",
            "",
            "    layer_0.stateful = False",
            "    self.assertEqual(network.stateful, False)",
            "    self.assertEqual(layer_0.stateful_count, 3)",
            "",
            "    # But changing stateful should not affect dynamic.",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(layer_0.dynamic_count, 1)",
            "",
            "    network.sub_network = sub_network",
            "",
            "    # Adding to the topology should invalidate the cache and reflect in the top",
            "    # level network.",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 2)",
            "    self.assertEqual(layer_1.dynamic_count, 1)",
            "",
            "    # Still dynamic, but we need to recompute.",
            "    sub_network.sub_layers.pop()",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 3)",
            "    self.assertEqual(layer_1.dynamic_count, 2)",
            "",
            "    # Now that we've removed the dynamic layer deep in the layer hierarchy, we",
            "    # need to make sure that that bubbles up through all the levels.",
            "    sub_network.sub_layers.pop()",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(layer_0.dynamic_count, 4)",
            "    self.assertEqual(layer_1.dynamic_count, 2)",
            "",
            "    # Now check with a tracked dict.",
            "    sub_network.sub_layers = {",
            "        \"layer_1\": layer_1,",
            "        \"layer_2\": layer_2,",
            "    }",
            "",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 5)",
            "    self.assertEqual(layer_1.dynamic_count, 3)",
            "",
            "    # In-place assignment should still invalidate the cache.",
            "    sub_network.sub_layers[\"layer_1\"] = layer_1",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 6)",
            "    self.assertEqual(layer_1.dynamic_count, 4)",
            "",
            "    sub_network.sub_layers[\"layer_1\"] = None",
            "    for _ in range(2):",
            "      self.assertEqual(network.dynamic, False)",
            "      self.assertEqual(layer_0.dynamic_count, 7)",
            "      self.assertEqual(layer_1.dynamic_count, 4)",
            "",
            "    layer_3 = AttrTrackingLayer()",
            "    layer_3.stateful = True",
            "",
            "    sub_network.sub_layers = None",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(network.stateful, False)",
            "",
            "    # Test duplicate layers.",
            "    sub_network.sub_layers = [layer_1, layer_1, layer_1, layer_3]",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(network.stateful, True)",
            "",
            "    for _ in range(3):",
            "      sub_network.sub_layers.pop()",
            "      self.assertEqual(network.dynamic, True)",
            "      self.assertEqual(network.stateful, False)",
            "",
            "    sub_network.sub_layers.pop()",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(network.stateful, False)",
            "",
            "  def test_compute_output_shape_cache(self):",
            "    # See https://github.com/tensorflow/tensorflow/issues/32029.",
            "    x = input_layer_lib.Input(shape=(None, 32))",
            "    dense = layers.Dense(2)",
            "    y = dense(x)",
            "    network = functional.Functional(x, y, name='dense_network')",
            "",
            "    for i in range(999, 1024):",
            "      self.assertEqual(network.compute_output_shape((1, i, 32)), (1, i, 2))",
            "",
            "  def test_2d_inputs_squeezed_to_1d(self):",
            "    input_1d = input_layer_lib.Input(shape=())",
            "    outputs = input_1d * 2.",
            "    net = functional.Functional(input_1d, outputs)",
            "",
            "    x = np.ones((10, 1))",
            "    y = net(x)",
            "    self.assertEqual(y.shape.rank, 1)",
            "",
            "  def test_1d_inputs_expanded_to_2d(self):",
            "    input_1d = input_layer_lib.Input(shape=(1,))",
            "    outputs = input_1d * 2.",
            "    net = functional.Functional(input_1d, outputs)",
            "",
            "    x = np.ones((10,))",
            "    y = net(x)",
            "    self.assertEqual(y.shape.rank, 2)",
            "",
            "  def test_training_passed_during_construction(self):",
            "",
            "    def _call(inputs, training):",
            "      if training is None:",
            "        return inputs * -1.0",
            "      elif training:",
            "        return inputs",
            "      else:",
            "        return inputs * 0.0",
            "",
            "    class MyLayer(base_layer.Layer):",
            "",
            "      def call(self, inputs, training=True):",
            "        return _call(inputs, training)",
            "",
            "    my_layer = MyLayer()",
            "    x = np.ones((1, 10))",
            "",
            "    # Hard-coded `true` value passed during construction is respected.",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, training=True)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, True))",
            "    self.assertAllEqual(network(x, training=False), _call(x, True))",
            "    self.assertAllEqual(network(x), _call(x, True))",
            "",
            "    # Hard-coded `false` value passed during construction is respected.",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, training=False)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, False))",
            "    self.assertAllEqual(network(x, training=False), _call(x, False))",
            "    self.assertAllEqual(network(x), _call(x, False))",
            "",
            "    if context.executing_eagerly():",
            "      # In v2, construction still works when no `training` is specified",
            "      # When no value passed during construction, it uses the local default.",
            "      inputs = input_layer_lib.Input(10)",
            "      outputs = my_layer(inputs)",
            "      network = functional.Functional(inputs, outputs)",
            "      self.assertAllEqual(network(x, training=True), _call(x, True))",
            "      self.assertAllEqual(network(x, training=False), _call(x, False))",
            "      self.assertAllEqual(network(x), _call(x, True))  # Use local default",
            "",
            "    # `None` value passed positionally during construction is ignored at runtime",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, None)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, True))",
            "    self.assertAllEqual(network(x, training=False), _call(x, False))",
            "    if context.executing_eagerly():",
            "      self.assertAllEqual(network(x), _call(x, True))  # Use local default",
            "    else:",
            "      # in v1 training would have defaulted to using the `None` inside the layer",
            "      # if training is not passed at runtime",
            "      self.assertAllEqual(network(x), _call(x, None))",
            "",
            "    # `None` value passed as kwarg during construction is ignored at runtime.",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, training=None)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, True))",
            "    self.assertAllEqual(network(x, training=False), _call(x, False))",
            "    if context.executing_eagerly():",
            "      self.assertAllEqual(network(x), _call(x, True))  # Use local default",
            "    else:",
            "      # in v1 training would have defaulted to using the `None` inside the layer",
            "      # if training is not passed at runtime",
            "      self.assertAllEqual(network(x), _call(x, None))",
            "",
            "",
            "class InputsOutputsErrorTest(keras_parameterized.TestCase):",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_input_error(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    with self.assertRaisesRegex(",
            "        TypeError, \"('Keyword argument not understood:', 'input')\"):",
            "      models.Model(input=inputs, outputs=outputs)",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_output_error(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    with self.assertRaisesRegex(",
            "        TypeError, \"('Keyword argument not understood:', 'output')\"):",
            "      models.Model(inputs=inputs, output=outputs)",
            "",
            "  def test_input_spec(self):",
            "    if not context.executing_eagerly():",
            "      return",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    model = models.Model(inputs, outputs)",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model(np.zeros((3, 11)))",
            "",
            "  def test_input_spec_list_of_inputs(self):",
            "    if not context.executing_eagerly():",
            "      return",
            "    input_1 = input_layer_lib.Input((10,), name='1')",
            "    input_2 = input_layer_lib.Input((5,), name='2')",
            "    x = layers.Concatenate()([input_1, input_2])",
            "    outputs = layers.Dense(10)(x)",
            "    model = models.Model([input_1, input_2], outputs)",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expects 2 input.*'):",
            "      model(np.zeros((3, 10)))",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expects 2 input.*'):",
            "      model([np.zeros((3, 10)), np.zeros((3, 5)), np.zeros((3, 10))])",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model([np.zeros((3, 10)), np.zeros((3, 6))])",
            "",
            "    # Test passing data via dict keyed by input name",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'Missing data for input.*'):",
            "      model({'1': np.zeros((3, 10))})",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})",
            "",
            "  def test_input_spec_dict(self):",
            "    if not context.executing_eagerly():",
            "      return",
            "    input_1 = input_layer_lib.Input((10,))",
            "    input_2 = input_layer_lib.Input((5,))",
            "    x = layers.Concatenate()([input_1, input_2])",
            "    outputs = layers.Dense(10)(x)",
            "    model = models.Model({'1': input_1, '2': input_2}, outputs)",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'Missing data for input.*'):",
            "      model({'1': np.zeros((3, 10))})",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})",
            "",
            "",
            "class FunctionalSubclassModel(training_lib.Model):",
            "",
            "  def __init__(self, *args, **kwargs):",
            "    my_input = input_layer_lib.Input(shape=(16,))",
            "    dense = layers.Dense(32, activation='relu')",
            "    output = dense(my_input)",
            "    outputs = {'output': output}",
            "    super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)",
            "",
            "",
            "class MixinClass(object):",
            "",
            "  def __init__(self, foo, **kwargs):",
            "    self._foo = foo",
            "    super().__init__(**kwargs)",
            "",
            "  def get_foo(self):",
            "    return self._foo",
            "",
            "",
            "class SubclassedModel(training_lib.Model):",
            "",
            "  def __init__(self, bar, **kwargs):",
            "    self._bar = bar",
            "    super().__init__(**kwargs)",
            "",
            "  def get_bar(self):",
            "    return self._bar",
            "",
            "",
            "class MultipleInheritanceModelTest(keras_parameterized.TestCase):",
            "",
            "  def testFunctionalSubclass(self):",
            "    m = FunctionalSubclassModel()",
            "    # Some smoke test for the weights and output shape of the model",
            "    self.assertLen(m.weights, 2)",
            "    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])",
            "",
            "  def testFunctionalSubclassPreMixin(self):",
            "    class MixedFunctionalSubclassModel(MixinClass, FunctionalSubclassModel):",
            "      pass",
            "",
            "    m = MixedFunctionalSubclassModel(foo='123')",
            "    self.assertTrue(m._is_graph_network)",
            "    self.assertLen(m.weights, 2)",
            "    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])",
            "    self.assertEqual(m.get_foo(), '123')",
            "",
            "  def testFunctionalSubclassPostMixin(self):",
            "    # Make sure the the mixin class is also init correct when the order changed.",
            "",
            "    class MixedFunctionalSubclassModel(FunctionalSubclassModel, MixinClass):",
            "      pass",
            "",
            "    m = MixedFunctionalSubclassModel(foo='123')",
            "    self.assertTrue(m._is_graph_network)",
            "    self.assertLen(m.weights, 2)",
            "    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])",
            "    self.assertEqual(m.get_foo(), '123')",
            "",
            "  def testSubclassModelPreMixin(self):",
            "    class MixedSubclassModel(MixinClass, SubclassedModel):",
            "      pass",
            "",
            "    m = MixedSubclassModel(foo='123', bar='456')",
            "    self.assertFalse(m._is_graph_network)",
            "    self.assertEqual(m.get_foo(), '123')",
            "    self.assertEqual(m.get_bar(), '456')",
            "",
            "",
            "if __name__ == '__main__':",
            "  test.main()"
        ],
        "afterPatchFile": [
            "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "#,============================================================================",
            "\"\"\"Tests for layer graphs construction & handling.\"\"\"",
            "",
            "import warnings",
            "",
            "import numpy as np",
            "",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import def_function",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import tensor_shape",
            "from tensorflow.python.keras import backend",
            "from tensorflow.python.keras import combinations",
            "from tensorflow.python.keras import initializers",
            "from tensorflow.python.keras import keras_parameterized",
            "from tensorflow.python.keras import layers",
            "from tensorflow.python.keras import losses",
            "from tensorflow.python.keras import models",
            "from tensorflow.python.keras import testing_utils",
            "from tensorflow.python.keras.engine import base_layer",
            "from tensorflow.python.keras.engine import functional",
            "from tensorflow.python.keras.engine import input_layer as input_layer_lib",
            "from tensorflow.python.keras.engine import sequential",
            "from tensorflow.python.keras.engine import training as training_lib",
            "from tensorflow.python.keras.utils import layer_utils",
            "from tensorflow.python.keras.utils import tf_utils",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import state_ops",
            "from tensorflow.python.ops import string_ops",
            "from tensorflow.python.ops.ragged import ragged_factory_ops",
            "from tensorflow.python.platform import test",
            "from tensorflow.python.training.tracking.util import Checkpoint",
            "",
            "",
            "class NetworkConstructionTest(keras_parameterized.TestCase):",
            "",
            "  def test_default_model_name(self):",
            "    inputs = input_layer_lib.Input(shape=(1,))",
            "    outputs = layers.Dense(1, activation='relu')(inputs)",
            "    model = training_lib.Model(inputs=inputs, outputs=outputs)",
            "    self.assertEqual(model.name, 'model')",
            "",
            "    model_2 = training_lib.Model(inputs=inputs, outputs=outputs)",
            "    self.assertEqual(model_2.name, 'model_1')",
            "",
            "    model_3 = training_lib.Model(inputs=inputs, outputs=outputs)",
            "    self.assertEqual(model_3.name, 'model_2')",
            "",
            "  def test_get_updates(self):",
            "",
            "    class MyLayer(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.a = self.add_variable('a',",
            "                                   (1, 1),",
            "                                   'float32',",
            "                                   trainable=False)",
            "        self.b = self.add_variable('b',",
            "                                   (1, 1),",
            "                                   'float32',",
            "                                   trainable=False)",
            "        self.add_update(state_ops.assign_add(self.a, [[1.]],",
            "                                             name='unconditional_update'))",
            "        self.built = True",
            "",
            "      def call(self, inputs):",
            "        self.add_update(state_ops.assign_add(self.b, inputs,",
            "                                             name='conditional_update'),",
            "                        inputs=True)",
            "        return inputs + 1",
            "",
            "    with ops.Graph().as_default():",
            "      x1 = input_layer_lib.Input(shape=(1,))",
            "      layer = MyLayer()",
            "      _ = layer(x1)",
            "",
            "      self.assertEqual(len(layer.updates), 2)",
            "",
            "      x2 = input_layer_lib.Input(shape=(1,))",
            "      y2 = layer(x2)",
            "",
            "      self.assertEqual(len(layer.updates), 3)",
            "",
            "      network = functional.Functional(x2, y2)",
            "      self.assertEqual(len(network.updates), 3)",
            "",
            "      x3 = input_layer_lib.Input(shape=(1,))",
            "      _ = layer(x3)",
            "      self.assertEqual(len(network.updates), 4)",
            "",
            "      x4 = input_layer_lib.Input(shape=(1,))",
            "      _ = network(x4)",
            "      self.assertEqual(len(network.updates), 5)",
            "",
            "      network.add_update(state_ops.assign_add(layer.a, [[1]]))",
            "      self.assertEqual(len(network.updates), 6)",
            "",
            "      network.add_update(state_ops.assign_add(layer.b, x4), inputs=True)",
            "      self.assertEqual(len(network.updates), 7)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph']))",
            "  def test_get_updates_bn(self):",
            "    x1 = input_layer_lib.Input(shape=(1,))",
            "    layer = layers.BatchNormalization()",
            "    _ = layer(x1)",
            "",
            "    self.assertEqual(len(layer.updates), 2)",
            "",
            "  def test_get_layer(self):",
            "    # create a simple network",
            "    x = input_layer_lib.Input(shape=(32,))",
            "    dense_a = layers.Dense(4, name='dense_a')",
            "    dense_b = layers.Dense(2, name='dense_b')",
            "    y = dense_b(dense_a(x))",
            "    network = functional.Functional(x, y, name='dense_network')",
            "",
            "    # test various get_layer by index",
            "    self.assertEqual(network.get_layer(index=1), dense_a)",
            "",
            "    # test invalid get_layer by index",
            "    with self.assertRaisesRegex(",
            "        ValueError, 'Was asked to retrieve layer at index ' + str(3) +",
            "        ' but model only has ' + str(len(network.layers)) + ' layers.'):",
            "      network.get_layer(index=3)",
            "",
            "    # test that only one between name and index is requested",
            "    with self.assertRaisesRegex(ValueError,",
            "                                'Provide only a layer name or a layer index'):",
            "      network.get_layer(index=1, name='dense_b')",
            "",
            "    # test that a name or an index must be provided",
            "    with self.assertRaisesRegex(ValueError,",
            "                                'Provide either a layer name or layer index.'):",
            "      network.get_layer()",
            "",
            "    # test various get_layer by name",
            "    self.assertEqual(network.get_layer(name='dense_a'), dense_a)",
            "",
            "    # test invalid get_layer by name",
            "    with self.assertRaisesRegex(ValueError, 'No such layer: dense_c.'):",
            "      network.get_layer(name='dense_c')",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTopologicalAttributes(self):",
            "    # test layer attributes / methods related to cross-layer connectivity.",
            "    a = input_layer_lib.Input(shape=(32,), name='input_a')",
            "    b = input_layer_lib.Input(shape=(32,), name='input_b')",
            "",
            "    # test input, output, input_shape, output_shape",
            "    test_layer = layers.Dense(16, name='test_layer')",
            "    a_test = test_layer(a)",
            "    self.assertIs(test_layer.input, a)",
            "    self.assertIs(test_layer.output, a_test)",
            "    self.assertEqual(test_layer.input_shape, (None, 32))",
            "    self.assertEqual(test_layer.output_shape, (None, 16))",
            "",
            "    # test `get_*_at` methods",
            "    dense = layers.Dense(16, name='dense_1')",
            "    a_2 = dense(a)",
            "    b_2 = dense(b)",
            "",
            "    self.assertIs(dense.get_input_at(0), a)",
            "    self.assertIs(dense.get_input_at(1), b)",
            "    self.assertIs(dense.get_output_at(0), a_2)",
            "    self.assertIs(dense.get_output_at(1), b_2)",
            "    self.assertEqual(dense.get_input_shape_at(0), (None, 32))",
            "    self.assertEqual(dense.get_input_shape_at(1), (None, 32))",
            "    self.assertEqual(dense.get_output_shape_at(0), (None, 16))",
            "    self.assertEqual(dense.get_output_shape_at(1), (None, 16))",
            "",
            "    # Test invalid value for attribute retrieval.",
            "    with self.assertRaises(ValueError):",
            "      dense.get_input_at(2)",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.input",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.output",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.output_shape",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      _ = new_dense.input_shape",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      a = input_layer_lib.Input(shape=(3, 32))",
            "      a = input_layer_lib.Input(shape=(5, 32))",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      _ = new_dense.input_shape",
            "    with self.assertRaises(AttributeError):",
            "      new_dense = layers.Dense(16)",
            "      a = input_layer_lib.Input(shape=(3, 32))",
            "      a = input_layer_lib.Input(shape=(5, 32))",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      _ = new_dense.output_shape",
            "",
            "  def _assertAllIs(self, a, b):",
            "    self.assertTrue(all(x is y for x, y in zip(a, b)))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTopologicalAttributesMultiOutputLayer(self):",
            "",
            "    class PowersLayer(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        return [inputs**2, inputs**3]",
            "",
            "    x = input_layer_lib.Input(shape=(32,))",
            "    test_layer = PowersLayer()",
            "    p1, p2 = test_layer(x)  # pylint: disable=not-callable",
            "",
            "    self.assertIs(test_layer.input, x)",
            "    self._assertAllIs(test_layer.output, [p1, p2])",
            "    self.assertEqual(test_layer.input_shape, (None, 32))",
            "    self.assertEqual(test_layer.output_shape, [(None, 32), (None, 32)])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTopologicalAttributesMultiInputLayer(self):",
            "",
            "    class AddLayer(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        assert len(inputs) == 2",
            "        return inputs[0] + inputs[1]",
            "",
            "    a = input_layer_lib.Input(shape=(32,))",
            "    b = input_layer_lib.Input(shape=(32,))",
            "    test_layer = AddLayer()",
            "    y = test_layer([a, b])  # pylint: disable=not-callable",
            "",
            "    self._assertAllIs(test_layer.input, [a, b])",
            "    self.assertIs(test_layer.output, y)",
            "    self.assertEqual(test_layer.input_shape, [(None, 32), (None, 32)])",
            "    self.assertEqual(test_layer.output_shape, (None, 32))",
            "",
            "  def testBasicNetwork(self):",
            "    with ops.Graph().as_default():",
            "      # minimum viable network",
            "      x = input_layer_lib.Input(shape=(32,))",
            "      dense = layers.Dense(2)",
            "      y = dense(x)",
            "      network = functional.Functional(x, y, name='dense_network')",
            "",
            "      # test basic attributes",
            "      self.assertEqual(network.name, 'dense_network')",
            "      self.assertEqual(len(network.layers), 2)  # InputLayer + Dense",
            "      self.assertEqual(network.layers[1], dense)",
            "      self._assertAllIs(network.weights, dense.weights)",
            "      self._assertAllIs(network.trainable_weights, dense.trainable_weights)",
            "      self._assertAllIs(network.non_trainable_weights,",
            "                        dense.non_trainable_weights)",
            "",
            "      # test callability on Input",
            "      x_2 = input_layer_lib.Input(shape=(32,))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 2])",
            "",
            "      # test callability on regular tensor",
            "      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 2])",
            "",
            "      # test network `trainable` attribute",
            "      network.trainable = False",
            "      self._assertAllIs(network.weights, dense.weights)",
            "      self.assertEqual(network.trainable_weights, [])",
            "      self._assertAllIs(network.non_trainable_weights,",
            "                        dense.trainable_weights + dense.non_trainable_weights)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_trainable_weights(self):",
            "    a = layers.Input(shape=(2,))",
            "    b = layers.Dense(1)(a)",
            "    model = training_lib.Model(a, b)",
            "",
            "    weights = model.weights",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "    model.trainable = True",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.layers[1].trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "    # sequential model",
            "    model = sequential.Sequential()",
            "    model.add(layers.Dense(1, input_dim=2))",
            "    weights = model.weights",
            "",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "    model.trainable = True",
            "    self._assertAllIs(model.trainable_weights, weights)",
            "    self.assertListEqual(model.non_trainable_weights, [])",
            "",
            "    model.layers[0].trainable = False",
            "    self.assertListEqual(model.trainable_weights, [])",
            "    self._assertAllIs(model.non_trainable_weights, weights)",
            "",
            "  def test_layer_call_arguments(self):",
            "    with ops.Graph().as_default():",
            "      # Test the ability to pass and serialize arguments to `call`.",
            "      inp = layers.Input(shape=(2,))",
            "      x = layers.Dense(3)(inp)",
            "      x = layers.Dropout(0.5)(x, training=True)",
            "      model = training_lib.Model(inp, x)",
            "      # Would be `dropout/cond/Merge` by default",
            "      self.assertIn('dropout', model.output.op.name)",
            "",
            "      # Test that argument is kept when applying the model",
            "      inp2 = layers.Input(shape=(2,))",
            "      out2 = model(inp2)",
            "      self.assertIn('dropout', out2.op.name)",
            "",
            "      # Test that argument is kept after loading a model",
            "      config = model.get_config()",
            "      model = training_lib.Model.from_config(config)",
            "      self.assertIn('dropout', model.output.op.name)",
            "",
            "  def test_node_construction(self):",
            "    # test basics",
            "    a = layers.Input(shape=(32,), name='input_a')",
            "    b = layers.Input(shape=(32,), name='input_b')",
            "",
            "    with self.assertRaises(ValueError):",
            "      _ = layers.Input(shape=(32,), batch_shape=(10, 32))",
            "    with self.assertRaises(ValueError):",
            "      _ = layers.Input(shape=(32,), unknown_kwarg=None)",
            "",
            "    self.assertListEqual(a.shape.as_list(), [None, 32])",
            "    a_layer, a_node_index, a_tensor_index = a._keras_history",
            "    b_layer, _, _ = b._keras_history",
            "    self.assertEqual(len(a_layer._inbound_nodes), 1)",
            "    self.assertEqual(a_tensor_index, 0)",
            "    node = a_layer._inbound_nodes[a_node_index]",
            "    self.assertEqual(node.outbound_layer, a_layer)",
            "",
            "    self.assertListEqual(node.inbound_layers, [])",
            "    self.assertListEqual(node.input_tensors, [a])",
            "    self.assertListEqual(node.input_shapes, [(None, 32)])",
            "    self.assertListEqual(node.output_tensors, [a])",
            "    self.assertListEqual(node.output_shapes, [(None, 32)])",
            "",
            "    dense = layers.Dense(16, name='dense_1')",
            "    a_2 = dense(a)",
            "    b_2 = dense(b)",
            "",
            "    self.assertEqual(len(dense._inbound_nodes), 2)",
            "    self.assertEqual(len(dense._outbound_nodes), 0)",
            "    self.assertEqual(dense._inbound_nodes[0].inbound_layers, a_layer)",
            "    self.assertEqual(dense._inbound_nodes[0].outbound_layer, dense)",
            "    self.assertEqual(dense._inbound_nodes[1].inbound_layers, b_layer)",
            "    self.assertEqual(dense._inbound_nodes[1].outbound_layer, dense)",
            "    self.assertIs(dense._inbound_nodes[0].input_tensors, a)",
            "    self.assertIs(dense._inbound_nodes[1].input_tensors, b)",
            "",
            "    # test layer properties",
            "    test_layer = layers.Dense(16, name='test_layer')",
            "    a_test = test_layer(a)",
            "    self.assertListEqual(test_layer.kernel.shape.as_list(), [32, 16])",
            "    self.assertIs(test_layer.input, a)",
            "    self.assertIs(test_layer.output, a_test)",
            "    self.assertEqual(test_layer.input_shape, (None, 32))",
            "    self.assertEqual(test_layer.output_shape, (None, 16))",
            "",
            "    self.assertIs(dense.get_input_at(0), a)",
            "    self.assertIs(dense.get_input_at(1), b)",
            "    self.assertIs(dense.get_output_at(0), a_2)",
            "    self.assertIs(dense.get_output_at(1), b_2)",
            "    self.assertEqual(dense.get_input_shape_at(0), (None, 32))",
            "    self.assertEqual(dense.get_input_shape_at(1), (None, 32))",
            "    self.assertEqual(dense.get_output_shape_at(0), (None, 16))",
            "    self.assertEqual(dense.get_output_shape_at(1), (None, 16))",
            "    self.assertEqual(dense.get_input_mask_at(0), None)",
            "    self.assertEqual(dense.get_input_mask_at(1), None)",
            "    self.assertEqual(dense.get_output_mask_at(0), None)",
            "    self.assertEqual(dense.get_output_mask_at(1), None)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_multi_input_layer(self):",
            "    with self.cached_session():",
            "      # test multi-input layer",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      self.assertListEqual(merged.shape.as_list(), [None, 16 * 2])",
            "      merge_layer, merge_node_index, merge_tensor_index = merged._keras_history",
            "",
            "      self.assertEqual(merge_node_index, 0)",
            "      self.assertEqual(merge_tensor_index, 0)",
            "",
            "      self.assertEqual(len(merge_layer._inbound_nodes), 1)",
            "      self.assertEqual(len(merge_layer._outbound_nodes), 0)",
            "",
            "      self.assertEqual(len(merge_layer._inbound_nodes[0].input_tensors), 2)",
            "      self.assertEqual(len(merge_layer._inbound_nodes[0].inbound_layers), 2)",
            "",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "      self.assertEqual(len(model.layers), 6)",
            "      output_shapes = model.compute_output_shape([(None, 32), (None, 32)])",
            "      self.assertListEqual(output_shapes[0].as_list(), [None, 64])",
            "      self.assertListEqual(output_shapes[1].as_list(), [None, 5])",
            "      self.assertListEqual(",
            "          model.compute_mask([a, b], [None, None]), [None, None])",
            "",
            "      # we don't check names of first 2 layers (inputs) because",
            "      # ordering of same-level layers is not fixed",
            "      self.assertListEqual([l.name for l in model.layers][2:],",
            "                           ['dense_1', 'merge', 'dense_2', 'dense_3'])",
            "      self.assertListEqual([l.name for l in model._input_layers],",
            "                           ['input_a', 'input_b'])",
            "      self.assertListEqual([l.name for l in model._output_layers],",
            "                           ['dense_2', 'dense_3'])",
            "",
            "      # actually run model",
            "      fn = backend.function(model.inputs, model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])",
            "",
            "      # test get_source_inputs",
            "      self._assertAllIs(layer_utils.get_source_inputs(c), [a, b])",
            "",
            "      # serialization / deserialization",
            "      json_config = model.to_json()",
            "      recreated_model = models.model_from_json(json_config)",
            "      recreated_model.compile('rmsprop', 'mse')",
            "",
            "      self.assertListEqual([l.name for l in recreated_model.layers][2:],",
            "                           ['dense_1', 'merge', 'dense_2', 'dense_3'])",
            "      self.assertListEqual([l.name for l in recreated_model._input_layers],",
            "                           ['input_a', 'input_b'])",
            "      self.assertListEqual([l.name for l in recreated_model._output_layers],",
            "                           ['dense_2', 'dense_3'])",
            "",
            "      fn = backend.function(recreated_model.inputs, recreated_model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])",
            "",
            "  def test_multi_output_layer_output_names(self):",
            "    inp = layers.Input(name='inp', shape=(None,), dtype=dtypes.float32)",
            "",
            "    class _MultiOutput(layers.Layer):",
            "",
            "      def call(self, x):",
            "        return x + 1., x + 2.",
            "",
            "    out = _MultiOutput(name='out')(inp)",
            "    model = training_lib.Model(inp, out)",
            "    self.assertEqual(['out', 'out_1'], model.output_names)",
            "    self.assertAllClose([2., 3.], model(1.))",
            "",
            "  def test_recursion(self):",
            "    with ops.Graph().as_default(), self.cached_session():",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "      e = layers.Input(shape=(32,), name='input_e')",
            "      f = layers.Input(shape=(32,), name='input_f')",
            "      self.assertEqual(len(model.inputs), 2)",
            "      g, h = model([e, f])",
            "      self.assertEqual(len(model.inputs), 2)",
            "      self.assertEqual(g.name, 'model/dense_2/BiasAdd:0')",
            "",
            "      self.assertListEqual(g.shape.as_list(), c.shape.as_list())",
            "      self.assertListEqual(h.shape.as_list(), d.shape.as_list())",
            "",
            "      # test separate manipulation of different layer outputs",
            "      i = layers.Dense(7, name='dense_4')(h)",
            "",
            "      final_model = training_lib.Model(",
            "          inputs=[e, f], outputs=[i, g], name='final')",
            "      self.assertEqual(len(final_model.inputs), 2)",
            "      self.assertEqual(len(final_model.outputs), 2)",
            "      self.assertEqual(len(final_model.layers), 4)",
            "",
            "      # we don't check names of first 2 layers (inputs) because",
            "      # ordering of same-level layers is not fixed",
            "      self.assertListEqual([layer.name for layer in final_model.layers][2:],",
            "                           ['model', 'dense_4'])",
            "      self.assertListEqual(",
            "          model.compute_mask([e, f], [None, None]), [None, None])",
            "      self.assertListEqual(",
            "          final_model.compute_output_shape([(10, 32), (10, 32)]), [(10, 7),",
            "                                                                   (10, 64)])",
            "",
            "      # run recursive model",
            "      fn = backend.function(final_model.inputs, final_model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])",
            "",
            "      # test serialization",
            "      model_config = final_model.get_config()",
            "      recreated_model = models.Model.from_config(model_config)",
            "",
            "      fn = backend.function(recreated_model.inputs, recreated_model.outputs)",
            "      input_a_np = np.random.random((10, 32))",
            "      input_b_np = np.random.random((10, 32))",
            "      fn_outputs = fn([input_a_np, input_b_np])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_multi_input_multi_output_recursion(self):",
            "    with self.cached_session():",
            "      # test multi-input multi-output",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "      j = layers.Input(shape=(32,), name='input_j')",
            "      k = layers.Input(shape=(32,), name='input_k')",
            "      _, n = model([j, k])",
            "",
            "      o = layers.Input(shape=(32,), name='input_o')",
            "      p = layers.Input(shape=(32,), name='input_p')",
            "      q, _ = model([o, p])",
            "",
            "      self.assertListEqual(n.shape.as_list(), [None, 5])",
            "      self.assertListEqual(q.shape.as_list(), [None, 64])",
            "      s = layers.concatenate([n, q], name='merge_nq')",
            "      self.assertListEqual(s.shape.as_list(), [None, 64 + 5])",
            "",
            "      # test with single output as 1-elem list",
            "      multi_io_model = training_lib.Model([j, k, o, p], [s])",
            "",
            "      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)",
            "      fn_outputs = fn([",
            "          np.random.random((10, 32)), np.random.random((10, 32)),",
            "          np.random.random((10, 32)), np.random.random((10, 32))",
            "      ])",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])",
            "",
            "      # test with single output as tensor",
            "      multi_io_model = training_lib.Model([j, k, o, p], s)",
            "",
            "      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)",
            "      fn_outputs = fn([",
            "          np.random.random((10, 32)), np.random.random((10, 32)),",
            "          np.random.random((10, 32)), np.random.random((10, 32))",
            "      ])",
            "      # note that the output of the function will still be a 1-elem list",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])",
            "",
            "      # test serialization",
            "      model_config = multi_io_model.get_config()",
            "      recreated_model = models.Model.from_config(model_config)",
            "",
            "      fn = backend.function(recreated_model.inputs, recreated_model.outputs)",
            "      fn_outputs = fn([",
            "          np.random.random((10, 32)), np.random.random((10, 32)),",
            "          np.random.random((10, 32)), np.random.random((10, 32))",
            "      ])",
            "      # note that the output of the function will still be a 1-elem list",
            "      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])",
            "",
            "      config = model.get_config()",
            "      models.Model.from_config(config)",
            "",
            "      model.summary()",
            "      json_str = model.to_json()",
            "      models.model_from_json(json_str)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_invalid_graphs(self):",
            "    a = layers.Input(shape=(32,), name='input_a')",
            "    b = layers.Input(shape=(32,), name='input_b')",
            "",
            "    dense = layers.Dense(16, name='dense_1')",
            "    a_2 = dense(a)",
            "    b_2 = dense(b)",
            "    merged = layers.concatenate([a_2, b_2], name='merge')",
            "    c = layers.Dense(64, name='dense_2')(merged)",
            "    d = layers.Dense(5, name='dense_3')(c)",
            "",
            "    model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "    # input is not an Input tensor",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    j = layers.Dense(32)(j)",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j, k], [m, n])",
            "",
            "    # disconnected graph",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j], [m, n])",
            "",
            "    # redundant outputs",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "",
            "    training_lib.Model([j, k], [m, n, n])",
            "",
            "    # redundant inputs",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j, k, j], [m, n])",
            "",
            "    # i have not idea what I'm doing: garbage as inputs/outputs",
            "    j = layers.Input(shape=(32,), name='input_j')",
            "    k = layers.Input(shape=(32,), name='input_k')",
            "    m, n = model([j, k])",
            "    with self.assertRaises(Exception):",
            "      training_lib.Model([j, k], [m, n, 0])",
            "",
            "  def test_raw_tf_compatibility(self):",
            "    with ops.Graph().as_default():",
            "      # test calling layers/models on TF tensors",
            "      a = layers.Input(shape=(32,), name='input_a')",
            "      b = layers.Input(shape=(32,), name='input_b')",
            "",
            "      dense = layers.Dense(16, name='dense_1')",
            "      a_2 = dense(a)",
            "      b_2 = dense(b)",
            "      merged = layers.concatenate([a_2, b_2], name='merge')",
            "      c = layers.Dense(64, name='dense_2')(merged)",
            "      d = layers.Dense(5, name='dense_3')(c)",
            "",
            "      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')",
            "",
            "      j = layers.Input(shape=(32,), name='input_j')",
            "      k = layers.Input(shape=(32,), name='input_k')",
            "      self.assertEqual(len(model.inputs), 2)",
            "      m, n = model([j, k])",
            "      self.assertEqual(len(model.inputs), 2)",
            "      tf_model = training_lib.Model([j, k], [m, n])",
            "",
            "      j_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))",
            "      k_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))",
            "      m_tf, n_tf = tf_model([j_tf, k_tf])",
            "      self.assertListEqual(m_tf.shape.as_list(), [None, 64])",
            "      self.assertListEqual(n_tf.shape.as_list(), [None, 5])",
            "",
            "      # test merge",
            "      layers.concatenate([j_tf, k_tf], axis=1)",
            "      layers.add([j_tf, k_tf])",
            "",
            "      # test tensor input",
            "      x = array_ops.placeholder(shape=(None, 2), dtype=dtypes.float32)",
            "      layers.InputLayer(input_tensor=x)",
            "",
            "      x = layers.Input(tensor=x)",
            "      layers.Dense(2)(x)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_basic_masking(self):",
            "    a = layers.Input(shape=(10, 32), name='input_a')",
            "    b = layers.Masking()(a)",
            "    model = training_lib.Model(a, b)",
            "    self.assertEqual(model.output_mask.shape.as_list(), [None, 10])",
            "",
            "  def testMaskingSingleInput(self):",
            "",
            "    class MaskedLayer(layers.Layer):",
            "",
            "      def call(self, inputs, mask=None):",
            "        if mask is not None:",
            "          return inputs * mask",
            "        return inputs",
            "",
            "      def compute_mask(self, inputs, mask=None):",
            "        return array_ops.ones_like(inputs)",
            "",
            "    if context.executing_eagerly():",
            "      a = constant_op.constant([2] * 32)",
            "      mask = constant_op.constant([0, 1] * 16)",
            "      a._keras_mask = mask",
            "      b = MaskedLayer().apply(a)",
            "      self.assertTrue(hasattr(b, '_keras_mask'))",
            "      self.assertAllEqual(",
            "          self.evaluate(array_ops.ones_like(mask)),",
            "          self.evaluate(getattr(b, '_keras_mask')))",
            "      self.assertAllEqual(self.evaluate(a * mask), self.evaluate(b))",
            "    else:",
            "      x = input_layer_lib.Input(shape=(32,))",
            "      y = MaskedLayer()(x)  # pylint: disable=not-callable",
            "      network = functional.Functional(x, y)",
            "",
            "      # test callability on Input",
            "      x_2 = input_layer_lib.Input(shape=(32,))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 32])",
            "",
            "      # test callability on regular tensor",
            "      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))",
            "      y_2 = network(x_2)",
            "      self.assertEqual(y_2.shape.as_list(), [None, 32])",
            "",
            "  def test_activity_regularization_with_model_composition(self):",
            "",
            "    def reg(x):",
            "      return math_ops.reduce_sum(x)",
            "",
            "    net_a_input = input_layer_lib.Input((2,))",
            "    net_a = net_a_input",
            "    net_a = layers.Dense(",
            "        2, kernel_initializer='ones', use_bias=False, activity_regularizer=reg)(",
            "            net_a)",
            "    model_a = training_lib.Model([net_a_input], [net_a])",
            "",
            "    net_b_input = input_layer_lib.Input((2,))",
            "    net_b = model_a(net_b_input)",
            "    model_b = training_lib.Model([net_b_input], [net_b])",
            "",
            "    model_b.compile(optimizer='sgd', loss=None)",
            "    x = np.ones((1, 2))",
            "    loss = model_b.evaluate(x)",
            "    self.assertEqual(loss, 4.)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_layer_sharing_at_heterogenous_depth(self):",
            "    x_val = np.random.random((10, 5))",
            "",
            "    x = input_layer_lib.Input(shape=(5,))",
            "    a = layers.Dense(5, name='A')",
            "    b = layers.Dense(5, name='B')",
            "    output = a(b(a(b(x))))",
            "    m = training_lib.Model(x, output)",
            "    m.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    output_val = m.predict(x_val)",
            "",
            "    config = m.get_config()",
            "    weights = m.get_weights()",
            "",
            "    m2 = models.Model.from_config(config)",
            "    m2.set_weights(weights)",
            "",
            "    output_val_2 = m2.predict(x_val)",
            "    self.assertAllClose(output_val, output_val_2, atol=1e-6)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_layer_sharing_at_heterogenous_depth_with_concat(self):",
            "    input_shape = (16, 9, 3)",
            "    input_layer = input_layer_lib.Input(shape=input_shape)",
            "",
            "    a = layers.Dense(3, name='dense_A')",
            "    b = layers.Dense(3, name='dense_B')",
            "    c = layers.Dense(3, name='dense_C')",
            "",
            "    x1 = b(a(input_layer))",
            "    x2 = a(c(input_layer))",
            "    output = layers.concatenate([x1, x2])",
            "",
            "    m = training_lib.Model(inputs=input_layer, outputs=output)",
            "    m.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    x_val = np.random.random((10, 16, 9, 3))",
            "    output_val = m.predict(x_val)",
            "",
            "    config = m.get_config()",
            "    weights = m.get_weights()",
            "",
            "    m2 = models.Model.from_config(config)",
            "    m2.set_weights(weights)",
            "",
            "    output_val_2 = m2.predict(x_val)",
            "    self.assertAllClose(output_val, output_val_2, atol=1e-6)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_explicit_training_argument(self):",
            "    a = layers.Input(shape=(2,))",
            "    b = layers.Dropout(0.5)(a)",
            "    base_model = training_lib.Model(a, b)",
            "",
            "    a = layers.Input(shape=(2,))",
            "    b = base_model(a, training=False)",
            "    model = training_lib.Model(a, b)",
            "",
            "    x = np.ones((100, 2))",
            "    y = np.ones((100, 2))",
            "    model.compile(",
            "        optimizer='sgd',",
            "        loss='mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    loss = model.train_on_batch(x, y)",
            "    self.assertEqual(loss, 0)  # In inference mode, output is equal to input.",
            "",
            "    a = layers.Input(shape=(2,))",
            "    b = base_model(a, training=True)",
            "    model = training_lib.Model(a, b)",
            "    preds = model.predict(x)",
            "    self.assertEqual(np.min(preds), 0.)  # At least one unit was dropped.",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_mask_derived_from_keras_layer(self):",
            "    inputs = input_layer_lib.Input((5, 10))",
            "    mask = input_layer_lib.Input((5,))",
            "    outputs = layers.RNN(layers.LSTMCell(100))(inputs, mask=mask)",
            "    model = training_lib.Model([inputs, mask], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # All data is masked, returned values are 0's.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.ones((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # Data is not masked, returned values are random.",
            "    self.assertGreater(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(model.get_config())",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # All data is masked, returned values are 0's.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "    history = model.fit(",
            "        x=[np.ones((10, 5, 10)), np.ones((10, 5))],",
            "        y=np.zeros((10, 100)),",
            "        batch_size=2)",
            "    # Data is not masked, returned values are random.",
            "    self.assertGreater(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_arg_derived_from_keras_layer(self):",
            "",
            "    class MyAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2):",
            "        return x1 + x2",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    outputs = MyAdd()(input1, input2)",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check serialization.",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'MyAdd': MyAdd})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)",
            "  def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):",
            "    # This functionality is unsupported in v1 graphs",
            "",
            "    class MyAddAll(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        x = inputs[0]",
            "        for inp in inputs[1:]:",
            "          if inp is not None:",
            "            x = x + inp",
            "        return x",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    layer = MyAddAll()",
            "    outputs = layer([0.0, input1, None, input2, None])",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    self.assertIn(layer, model.layers)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check serialization.",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'MyAddAll': MyAddAll})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(",
            "      combinations.times(",
            "          combinations.keras_mode_combinations(),",
            "          combinations.combine(share_already_used_layer=[True, False])))",
            "  def test_call_kwarg_derived_from_keras_layer(self, share_already_used_layer):",
            "",
            "    class MaybeAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2=None):",
            "        if x2 is not None:",
            "          return x1 + x2",
            "        return x1",
            "",
            "    class IdentityLayer(layers.Layer):",
            "",
            "      def call(self, x):",
            "        return x",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    identity_layer = IdentityLayer()",
            "",
            "    if share_already_used_layer:",
            "      # We have had model serialization/deserialization break in the past:",
            "      # when a layer was previously used to construct other functional models",
            "      # and had a non-empty list of inbound nodes before being used to define",
            "      # the model being serialized/deserialized.",
            "      # (The serialization/deserialization was not correctly adjusting",
            "      # the node_index serialization/deserialization).",
            "      # So, we explicitly test this case.",
            "      training_lib.Model([input1], identity_layer(input1))",
            "",
            "    outputs = MaybeAdd()(input1, x2=identity_layer(input2))",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(),",
            "        custom_objects={",
            "            'MaybeAdd': MaybeAdd,",
            "            'IdentityLayer': IdentityLayer",
            "        })",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_kwarg_dtype_serialization(self):",
            "",
            "    class Double(layers.Layer):",
            "",
            "      def call(self, x1, dtype=None):",
            "        return math_ops.cast(x1 + x1, dtype=dtype)",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    outputs = Double()(input1, dtype=dtypes.float16)",
            "    model = training_lib.Model([input1], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10))],",
            "        y=6 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that input was correctly doubled.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check the output dtype",
            "    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'Double': Double})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10))],",
            "        y=6 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that input was correctly doubled.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    # Check the output dtype",
            "    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_kwarg_nonserializable(self):",
            "",
            "    class Double(layers.Layer):",
            "",
            "      def call(self, x1, kwarg=None):",
            "        return x1 + x1",
            "",
            "    class NonSerializable(object):",
            "",
            "      def __init__(self, foo=None):",
            "        self.foo = foo",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    outputs = Double()(input1, kwarg=NonSerializable())",
            "    model = training_lib.Model([input1], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[3 * np.ones((10, 10))],",
            "        y=6 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that input was correctly doubled.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "    with self.assertRaisesRegex(",
            "        TypeError, 'Layer double was passed non-JSON-serializable arguments.'):",
            "      model.get_config()",
            "",
            "  @combinations.generate(",
            "      combinations.times(",
            "          combinations.keras_mode_combinations(),",
            "          combinations.combine(share_already_used_layer=[True, False])))",
            "  def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(",
            "      self, share_already_used_layer):",
            "",
            "    class IdentityLayer(layers.Layer):",
            "",
            "      def call(self, x):",
            "        return x",
            "",
            "    class MaybeAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2=None):",
            "        if x2 is not None:",
            "          return x1 + x2",
            "        return x1",
            "",
            "    input2 = input_layer_lib.Input(10)",
            "    identity_layer = IdentityLayer()",
            "    if share_already_used_layer:",
            "      # We have had model serialization/deserialization break in the past:",
            "      # when a layer was previously used to construct other functional models",
            "      # and had a non-empty list of inbound nodes before being used to define",
            "      # the model being serialized/deserialized.",
            "      # (The serialization/deserialization was not correctly adjusting",
            "      # the node_index serialization/deserialization).",
            "      # So, we explicitly test this case.",
            "      training_lib.Model([input2], identity_layer(input2))",
            "",
            "    outputs = MaybeAdd()(3., x2=identity_layer(input2))",
            "    model = training_lib.Model([input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=7 * np.ones((10, 10)),",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(),",
            "        custom_objects={",
            "            'MaybeAdd': MaybeAdd,",
            "            'IdentityLayer': IdentityLayer",
            "        })",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=7 * np.ones((10, 10)),",
            "        y=10 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_composite_call_kwarg_derived_from_keras_layer(self):",
            "",
            "    # Create a test layer that accepts composite tensor inputs.",
            "    class MaybeAdd(layers.Layer):",
            "",
            "      def call(self, x1, x2=None):",
            "        # We need to convert this to a tensor for loss calculations -",
            "        # losses don't play nicely with ragged tensors yet.",
            "        if x2 is not None:",
            "          return (x1 + x2).to_tensor(default_value=0)",
            "        return x1.to_tensor(default_value=0)",
            "",
            "    input1 = input_layer_lib.Input((None,), ragged=True)",
            "    input2 = input_layer_lib.Input((None,), ragged=True)",
            "    outputs = MaybeAdd()(input1, x2=input2)",
            "    model = training_lib.Model([input1, input2], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    input_data = [",
            "        ragged_factory_ops.constant([[3.0, 3.0], [3.0, 3.0], [3.0]]),",
            "        ragged_factory_ops.constant([[7.0, 7.0], [7.0, 7.0], [7.0]])",
            "    ]",
            "    expected_data = np.array([[10.0, 10.0], [10.0, 10.0], [10.0, 0.0]])",
            "",
            "    history = model.fit(x=input_data, y=expected_data)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'MaybeAdd': MaybeAdd})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(x=input_data, y=expected_data)",
            "    # Check that second input was correctly added to first.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))",
            "  def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):",
            "    # This functionality is unsupported in v1 graphs",
            "",
            "    class AddAll(layers.Layer):",
            "",
            "      def call(self, x1_x2, x3):",
            "        x1, x2 = x1_x2",
            "        out = x1 + x2",
            "        if x3 is not None:",
            "          for t in x3.values():",
            "            out += t",
            "        return out",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    input3 = input_layer_lib.Input(10)",
            "",
            "    layer = AddAll()",
            "    outputs = layer(",
            "        [input1, 4 * array_ops.ones((1, 10))],",
            "        x3={",
            "            'a': input2,",
            "            'b': input3,",
            "            'c': 5 * array_ops.ones((1, 10))",
            "        })",
            "    model = training_lib.Model([input1, input2, input3], outputs)",
            "    self.assertIn(layer, model.layers)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'AddAll': AddAll})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_call_nested_arg_derived_from_keras_layer(self):",
            "",
            "    class AddAll(layers.Layer):",
            "",
            "      def call(self, x1, x2, x3=None):",
            "        out = x1 + x2",
            "        if x3 is not None:",
            "          for t in x3.values():",
            "            out += t",
            "        return out",
            "",
            "    input1 = input_layer_lib.Input(10)",
            "    input2 = input_layer_lib.Input(10)",
            "    input3 = input_layer_lib.Input(10)",
            "    outputs = AddAll()(",
            "        input1,",
            "        4 * array_ops.ones((1, 10)),",
            "        x3={",
            "            'a': input2,",
            "            'b': input3,",
            "            'c': 5 * array_ops.ones((1, 10))",
            "        })",
            "    model = training_lib.Model([input1, input2, input3], outputs)",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "    model = training_lib.Model.from_config(",
            "        model.get_config(), custom_objects={'AddAll': AddAll})",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    history = model.fit(",
            "        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],",
            "        y=15 * np.ones((10, 10)),",
            "        batch_size=2)",
            "    # Check that all inputs were correctly added.",
            "    self.assertEqual(history.history['loss'][0], 0.0)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_multi_output_model_with_none_masking(self):",
            "    def func(x):",
            "      return [x * 0.2, x * 0.3]",
            "",
            "    def output_shape(input_shape):",
            "      return [input_shape, input_shape]",
            "",
            "    i = layers.Input(shape=(3, 2, 1))",
            "    o = layers.Lambda(function=func, output_shape=output_shape)(i)",
            "",
            "    self.assertEqual(backend.int_shape(o[0]), (None, 3, 2, 1))",
            "    self.assertEqual(backend.int_shape(o[1]), (None, 3, 2, 1))",
            "",
            "    o = layers.add(o)",
            "    model = training_lib.Model(i, o)",
            "    model.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    i2 = layers.Input(shape=(3, 2, 1))",
            "    o2 = model(i2)",
            "    model2 = training_lib.Model(i2, o2)",
            "    model2.run_eagerly = testing_utils.should_run_eagerly()",
            "",
            "    x = np.random.random((4, 3, 2, 1))",
            "    out = model2.predict(x)",
            "    assert out.shape == (4, 3, 2, 1)",
            "    self.assertAllClose(out, x * 0.2 + x * 0.3, atol=1e-4)",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_constant_initializer_with_numpy(self):",
            "    initializer = initializers.Constant(np.ones((3, 2)))",
            "    model = sequential.Sequential()",
            "    model.add(layers.Dense(2, input_shape=(3,), kernel_initializer=initializer))",
            "    model.add(layers.Dense(3))",
            "    model.compile(",
            "        loss='mse',",
            "        optimizer='sgd',",
            "        metrics=['acc'],",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "",
            "    json_str = model.to_json()",
            "    models.model_from_json(json_str)",
            "",
            "  def test_subclassed_error_if_init_not_called(self):",
            "",
            "    class MyNetwork(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        self._foo = [layers.Dense(10), layers.Dense(10)]",
            "",
            "    with self.assertRaisesRegex(RuntimeError, 'forgot to call'):",
            "      MyNetwork()",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_int_input_shape(self):",
            "    inputs = input_layer_lib.Input(10)",
            "    self.assertEqual([None, 10], inputs.shape.as_list())",
            "",
            "    inputs_with_batch = input_layer_lib.Input(batch_size=20, shape=5)",
            "    self.assertEqual([20, 5], inputs_with_batch.shape.as_list())",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_model_initialization(self):",
            "    # Functional model",
            "    inputs = input_layer_lib.Input(shape=(32,))",
            "    outputs = layers.Dense(4)(inputs)",
            "",
            "    with self.assertRaisesRegex(TypeError,",
            "                                'Keyword argument not understood'):",
            "      model = training_lib.Model(",
            "          inputs, outputs, name='m', trainable=False, dtype='int64')",
            "    with self.assertRaisesRegex(TypeError,",
            "                                'Keyword argument not understood'):",
            "      model = training_lib.Model(",
            "          inputs, outputs, name='m', trainable=False, dynamic=False)",
            "",
            "    model = training_lib.Model(inputs, outputs, name='m', trainable=False)",
            "    self.assertEqual('m', model.name)",
            "    self.assertFalse(model.trainable)",
            "    self.assertFalse(model.dynamic)",
            "",
            "    class SubclassModel(training_lib.Model):",
            "      pass",
            "    # Subclassed model",
            "    model = SubclassModel(",
            "        name='subclassed', trainable=True, dtype='int64', dynamic=True)",
            "    self.assertEqual('subclassed', model.name)",
            "    self.assertTrue(model.dynamic)",
            "    self.assertTrue(model.trainable)",
            "    w = model.add_weight('w', [], initializer=initializers.Constant(1))",
            "    self.assertEqual(dtypes.int64, w.dtype)",
            "",
            "  def test_disconnected_inputs(self):",
            "    input_tensor1 = input_layer_lib.Input(shape=[200], name='a')",
            "    input_tensor2 = input_layer_lib.Input(shape=[10], name='b')",
            "    output_tensor1 = layers.Dense(units=10)(input_tensor1)",
            "",
            "    net = functional.Functional(",
            "        inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1])",
            "    net2 = functional.Functional.from_config(net.get_config())",
            "    self.assertLen(net2.inputs, 2)",
            "    self.assertEqual('a', net2.layers[0].name)",
            "    self.assertEqual('b', net2.layers[1].name)",
            "",
            "  @combinations.generate(combinations.keras_model_type_combinations())",
            "  def test_dependency_tracking(self):",
            "    model = testing_utils.get_small_mlp(1, 4, input_dim=3)",
            "    model.trackable = Checkpoint()",
            "    self.assertIn('trackable', model._unconditional_dependency_names)",
            "    self.assertEqual(model.trackable, model._lookup_dependency('trackable'))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_model_construction_in_tf_function(self):",
            "",
            "    d = {'model': None}",
            "",
            "    @def_function.function",
            "    def fn(x):",
            "      if d['model'] is None:",
            "        # Check that Functional can be built in a `tf.function`.",
            "        inputs = input_layer_lib.Input(10)",
            "        outputs = layers.Dense(1)(inputs)",
            "        model = functional.Functional(inputs, outputs)",
            "        d['model'] = model",
            "      else:",
            "        model = d['model']",
            "",
            "      return model(x)",
            "",
            "    x = array_ops.ones((10, 10))",
            "    y = fn(x)",
            "    self.assertEqual(y.shape.as_list(), [10, 1])",
            "",
            "",
            "class DeferredModeTest(keras_parameterized.TestCase):",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testSimpleNetworkBuilding(self):",
            "    inputs = input_layer_lib.Input(shape=(32,))",
            "    if context.executing_eagerly():",
            "      self.assertEqual(inputs.dtype.name, 'float32')",
            "      self.assertEqual(inputs.shape.as_list(), [None, 32])",
            "",
            "    x = layers.Dense(2)(inputs)",
            "    if context.executing_eagerly():",
            "      self.assertEqual(x.dtype.name, 'float32')",
            "      self.assertEqual(x.shape.as_list(), [None, 2])",
            "",
            "    outputs = layers.Dense(4)(x)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertIsInstance(network, functional.Functional)",
            "",
            "    if context.executing_eagerly():",
            "      # It should be possible to call such a network on EagerTensors.",
            "      inputs = constant_op.constant(",
            "          np.random.random((10, 32)).astype('float32'))",
            "      outputs = network(inputs)",
            "      self.assertEqual(outputs.shape.as_list(), [10, 4])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testMultiIONetworkBuilding(self):",
            "    input_a = input_layer_lib.Input(shape=(32,))",
            "    input_b = input_layer_lib.Input(shape=(16,))",
            "    a = layers.Dense(16)(input_a)",
            "",
            "    class AddLayer(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        return inputs[0] + inputs[1]",
            "",
            "    c = AddLayer()([a, input_b])  # pylint: disable=not-callable",
            "    c = layers.Dense(2)(c)",
            "",
            "    network = functional.Functional([input_a, input_b], [a, c])",
            "    if context.executing_eagerly():",
            "      a_val = constant_op.constant(",
            "          np.random.random((10, 32)).astype('float32'))",
            "      b_val = constant_op.constant(",
            "          np.random.random((10, 16)).astype('float32'))",
            "      outputs = network([a_val, b_val])",
            "      self.assertEqual(len(outputs), 2)",
            "      self.assertEqual(outputs[0].shape.as_list(), [10, 16])",
            "      self.assertEqual(outputs[1].shape.as_list(), [10, 2])",
            "",
            "",
            "class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):",
            "",
            "  def _testShapeInference(self, model, input_shape, expected_output_shape):",
            "    input_value = np.random.random(input_shape)",
            "    output_value = model.predict(input_value)",
            "    self.assertEqual(output_value.shape, expected_output_shape)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testSingleInputCase(self):",
            "",
            "    class LayerWithOneInput(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.w = array_ops.ones(shape=(3, 4))",
            "",
            "      def call(self, inputs):",
            "        return backend.dot(inputs, self.w)",
            "",
            "    inputs = input_layer_lib.Input(shape=(3,))",
            "    layer = LayerWithOneInput()",
            "",
            "    if context.executing_eagerly():",
            "      self.assertEqual(",
            "          layer.compute_output_shape((None, 3)).as_list(), [None, 4])",
            "      # As a side-effect, compute_output_shape builds the layer.",
            "      self.assertTrue(layer.built)",
            "      # We can still query the layer's compute_output_shape with compatible",
            "      # input shapes.",
            "      self.assertEqual(",
            "          layer.compute_output_shape((6, 3)).as_list(), [6, 4])",
            "",
            "    outputs = layer(inputs)",
            "    model = training_lib.Model(inputs, outputs)",
            "    self._testShapeInference(model, (2, 3), (2, 4))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testMultiInputOutputCase(self):",
            "",
            "    class MultiInputOutputLayer(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.w = array_ops.ones(shape=(3, 4))",
            "",
            "      def call(self, inputs):",
            "        a = backend.dot(inputs[0], self.w)",
            "        b = a + inputs[1]",
            "        return [a, b]",
            "",
            "    input_a = input_layer_lib.Input(shape=(3,))",
            "    input_b = input_layer_lib.Input(shape=(4,))",
            "    output_a, output_b = MultiInputOutputLayer()([input_a, input_b])",
            "    model = training_lib.Model([input_a, input_b], [output_a, output_b])",
            "    output_a_val, output_b_val = model.predict(",
            "        [np.random.random((2, 3)), np.random.random((2, 4))])",
            "    self.assertEqual(output_a_val.shape, (2, 4))",
            "    self.assertEqual(output_b_val.shape, (2, 4))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testTrainingArgument(self):",
            "",
            "    class LayerWithTrainingArg(layers.Layer):",
            "",
            "      def build(self, input_shape):",
            "        self.w = array_ops.ones(shape=(3, 4))",
            "",
            "      def call(self, inputs, training):",
            "        return backend.dot(inputs, self.w)",
            "",
            "    inputs = input_layer_lib.Input(shape=(3,))",
            "    outputs = LayerWithTrainingArg()(inputs, training=False)",
            "    model = training_lib.Model(inputs, outputs)",
            "    self._testShapeInference(model, (2, 3), (2, 4))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testNoneInShape(self):",
            "",
            "    class Model(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        super(Model, self).__init__()",
            "        self.conv1 = layers.Conv2D(8, 3)",
            "        self.pool = layers.GlobalAveragePooling2D()",
            "        self.fc = layers.Dense(3)",
            "",
            "      def call(self, x):",
            "        x = self.conv1(x)",
            "        x = self.pool(x)",
            "        x = self.fc(x)",
            "        return x",
            "",
            "    model = Model()",
            "    model.build(tensor_shape.TensorShape((None, None, None, 1)))",
            "    self.assertTrue(model.built, 'Model should be built')",
            "    self.assertTrue(model.weights,",
            "                    'Model should have its weights created as it '",
            "                    'has been built')",
            "    sample_input = array_ops.ones((1, 10, 10, 1))",
            "    output = model(sample_input)",
            "    self.assertEqual(output.shape, (1, 3))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testNoneInShapeWithCompoundModel(self):",
            "",
            "    class BasicBlock(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        super(BasicBlock, self).__init__()",
            "        self.conv1 = layers.Conv2D(8, 3)",
            "        self.pool = layers.GlobalAveragePooling2D()",
            "        self.dense = layers.Dense(3)",
            "",
            "      def call(self, x):",
            "        x = self.conv1(x)",
            "        x = self.pool(x)",
            "        x = self.dense(x)",
            "        return x",
            "",
            "    class CompoundModel(training_lib.Model):",
            "",
            "      def __init__(self):",
            "        super(CompoundModel, self).__init__()",
            "        self.block = BasicBlock()",
            "",
            "      def call(self, x):",
            "        x = self.block(x)  # pylint: disable=not-callable",
            "        return x",
            "",
            "    model = CompoundModel()",
            "    model.build(tensor_shape.TensorShape((None, None, None, 1)))",
            "    self.assertTrue(model.built, 'Model should be built')",
            "    self.assertTrue(model.weights,",
            "                    'Model should have its weights created as it '",
            "                    'has been built')",
            "    sample_input = array_ops.ones((1, 10, 10, 1))",
            "    output = model(sample_input)  # pylint: disable=not-callable",
            "    self.assertEqual(output.shape, (1, 3))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def testNoneInShapeWithFunctionalAPI(self):",
            "",
            "    class BasicBlock(training_lib.Model):",
            "      # Inheriting from layers.Layer since we are calling this layer",
            "      # inside a model created using functional API.",
            "",
            "      def __init__(self):",
            "        super(BasicBlock, self).__init__()",
            "        self.conv1 = layers.Conv2D(8, 3)",
            "",
            "      def call(self, x):",
            "        x = self.conv1(x)",
            "        return x",
            "",
            "    input_layer = layers.Input(shape=(None, None, 1))",
            "    x = BasicBlock()(input_layer)",
            "    x = layers.GlobalAveragePooling2D()(x)",
            "    output_layer = layers.Dense(3)(x)",
            "",
            "    model = training_lib.Model(inputs=input_layer, outputs=output_layer)",
            "",
            "    model.build(tensor_shape.TensorShape((None, None, None, 1)))",
            "    self.assertTrue(model.built, 'Model should be built')",
            "    self.assertTrue(model.weights,",
            "                    'Model should have its weights created as it '",
            "                    'has been built')",
            "    sample_input = array_ops.ones((1, 10, 10, 1))",
            "    output = model(sample_input)",
            "    self.assertEqual(output.shape, (1, 3))",
            "",
            "  @combinations.generate(combinations.keras_mode_combinations())",
            "  def test_sequential_as_downstream_of_masking_layer(self):",
            "    inputs = layers.Input(shape=(3, 4))",
            "    x = layers.Masking(mask_value=0., input_shape=(3, 4))(inputs)",
            "",
            "    s = sequential.Sequential()",
            "    s.add(layers.Dense(5, input_shape=(4,)))",
            "",
            "    x = layers.wrappers.TimeDistributed(s)(x)",
            "    model = training_lib.Model(inputs=inputs, outputs=x)",
            "    model.compile(",
            "        optimizer='rmsprop',",
            "        loss='mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "",
            "    model_input = np.random.randint(",
            "        low=1, high=5, size=(10, 3, 4)).astype('float32')",
            "    for i in range(4):",
            "      model_input[i, i:, :] = 0.",
            "    model.fit(model_input,",
            "              np.random.random((10, 3, 5)), epochs=1, batch_size=6)",
            "",
            "    if not context.executing_eagerly():",
            "      # Note: this doesn't work in eager due to DeferredTensor/ops compatibility",
            "      # issue.",
            "      mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]",
            "      mask_outputs += [model.layers[2].compute_mask(",
            "          model.layers[2].input, mask_outputs[-1])]",
            "      func = backend.function([model.input], mask_outputs)",
            "      mask_outputs_val = func([model_input])",
            "      self.assertAllClose(mask_outputs_val[0], np.any(model_input, axis=-1))",
            "      self.assertAllClose(mask_outputs_val[1], np.any(model_input, axis=-1))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_external_keras_serialization_compat_input_layers(self):",
            "    inputs = input_layer_lib.Input(shape=(10,))",
            "    outputs = layers.Dense(1)(inputs)",
            "    model = training_lib.Model(inputs, outputs)",
            "    config = model.get_config()",
            "    # Checks that single inputs and outputs are still saved as 1-element lists.",
            "    # Saving as 1-element lists or not is equivalent in TF Keras, but only the",
            "    # 1-element list format is supported in TF.js and keras-team/Keras.",
            "    self.assertLen(config['input_layers'], 1)",
            "    self.assertLen(config['output_layers'], 1)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_external_keras_serialization_compat_inbound_nodes(self):",
            "    # Check single Tensor input.",
            "    inputs = input_layer_lib.Input(shape=(10,), name='in')",
            "    outputs = layers.Dense(1)(inputs)",
            "    model = training_lib.Model(inputs, outputs)",
            "    config = model.get_config()",
            "    self.assertEqual(config['layers'][1]['inbound_nodes'], [[['in', 0, 0, {}]]])",
            "",
            "    # Check multiple Tensor input.",
            "    inputs1 = input_layer_lib.Input(shape=(10,), name='in1')",
            "    inputs2 = input_layer_lib.Input(shape=(10,), name='in2')",
            "    outputs = layers.Add()([inputs1, inputs2])",
            "    model = training_lib.Model([inputs1, inputs2], outputs)",
            "    config = model.get_config()",
            "    self.assertEqual(config['layers'][2]['inbound_nodes'],",
            "                     [[['in1', 0, 0, {}], ['in2', 0, 0, {}]]])",
            "",
            "  @combinations.generate(combinations.combine(mode=['eager']))",
            "  def test_dict_inputs_tensors(self):",
            "    # Note that this test is running with v2 eager only, since the v1",
            "    # will behave differently wrt to dict input for training.",
            "    inputs = {",
            "        'sentence2': input_layer_lib.Input(",
            "            shape=(), name='a', dtype=dtypes.string),",
            "        'sentence1': input_layer_lib.Input(",
            "            shape=(), name='b', dtype=dtypes.string),",
            "    }",
            "    strlen = layers.Lambda(string_ops.string_length_v2)",
            "    diff = layers.Subtract()(",
            "        [strlen(inputs['sentence1']), strlen(inputs['sentence2'])])",
            "    diff = math_ops.cast(diff, dtypes.float32)",
            "    model = training_lib.Model(inputs, diff)",
            "",
            "    extra_keys = {",
            "        'sentence1': constant_op.constant(['brown fox', 'lazy dog']),",
            "        'sentence2': constant_op.constant(['owl', 'cheeky cat']),",
            "        'label': constant_op.constant([0, 1]),",
            "    }",
            "",
            "    with warnings.catch_warnings(record=True) as w:",
            "      warnings.simplefilter('always')",
            "      model(extra_keys)",
            "      self.assertIn('ignored by the model', str(w[-1].message))",
            "",
            "    model.compile('sgd', 'mse')",
            "    with warnings.catch_warnings(record=True) as w:",
            "      warnings.simplefilter('always')",
            "      model.fit(extra_keys, y=constant_op.constant([0, 1]), steps_per_epoch=1)",
            "      self.assertIn('ignored by the model', str(w[-1].message))",
            "",
            "    with warnings.catch_warnings(record=True) as w:",
            "      warnings.simplefilter('always')",
            "      model.evaluate(extra_keys, constant_op.constant([0, 1]))",
            "      self.assertIn('ignored by the model', str(w[-1].message))",
            "",
            "    # Make sure the model inputs are sorted with the dict keys.",
            "    self.assertEqual(model.inputs[0]._keras_history.layer.name, 'b')",
            "    self.assertEqual(model.inputs[1]._keras_history.layer.name, 'a')",
            "",
            "",
            "class GraphUtilsTest(test.TestCase):",
            "",
            "  def testGetReachableFromInputs(self):",
            "",
            "    with ops.Graph().as_default(), self.cached_session():",
            "      pl_1 = array_ops.placeholder(shape=None, dtype='float32')",
            "      pl_2 = array_ops.placeholder(shape=None, dtype='float32')",
            "      pl_3 = array_ops.placeholder(shape=None, dtype='float32')",
            "      x_1 = pl_1 + pl_2",
            "      x_2 = pl_2 * 2",
            "      x_3 = pl_3 + 1",
            "      x_4 = x_1 + x_2",
            "      x_5 = x_3 * pl_1",
            "",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([pl_1]),",
            "          {pl_1, x_1, x_4, x_5, x_1.op, x_4.op, x_5.op})",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([pl_1, pl_2]),",
            "          {pl_1, pl_2, x_1, x_2, x_4, x_5, x_1.op, x_2.op, x_4.op, x_5.op})",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([pl_3]),",
            "          {pl_3, x_3, x_5, x_3.op, x_5.op})",
            "      self.assertEqual(",
            "          tf_utils.get_reachable_from_inputs([x_3]), {x_3, x_5, x_5.op})",
            "",
            "",
            "class NestedNetworkTest(keras_parameterized.TestCase):",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_inputs_network(self):",
            "    inputs = {",
            "        'x1': input_layer_lib.Input(shape=(1,)),",
            "        'x2': input_layer_lib.Input(shape=(1,))",
            "    }",
            "    outputs = layers.Add()([inputs['x1'], inputs['x2']])",
            "    network = functional.Functional(inputs, outputs)",
            "",
            "    network = functional.Functional.from_config(network.get_config())",
            "",
            "    result_tensor = network({",
            "        'x1': array_ops.ones((1, 1), 'float32'),",
            "        'x2': array_ops.ones((1, 1), 'float32')",
            "    })",
            "    result = self.evaluate(result_tensor)",
            "    self.assertAllEqual(result, [[2.]])",
            "",
            "    # TODO(b/122726584): Investigate why concrete batch is flaky in some builds.",
            "    output_shape = network.compute_output_shape({",
            "        'x1': (None, 1),",
            "        'x2': (None, 1)",
            "    })",
            "    self.assertListEqual(output_shape.as_list(), [None, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_outputs_network(self):",
            "    inputs = input_layer_lib.Input(shape=(1,))",
            "    outputs = {",
            "        'x+x': layers.Add()([inputs, inputs]),",
            "        'x*x': layers.Multiply()([inputs, inputs])",
            "    }",
            "",
            "    network = functional.Functional(inputs, outputs)",
            "",
            "    network = functional.Functional.from_config(network.get_config())",
            "",
            "    result_tensor = network(array_ops.ones((1, 1), 'float32'))",
            "    result = self.evaluate(result_tensor)",
            "    self.assertAllEqual(result['x+x'], [[2.]])",
            "    self.assertAllEqual(result['x*x'], [[1.]])",
            "",
            "    output_shape = network.compute_output_shape((None, 1))",
            "    self.assertListEqual(output_shape['x+x'].as_list(), [None, 1])",
            "    self.assertListEqual(output_shape['x*x'].as_list(), [None, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_network_inside_network(self):",
            "    inner_inputs = {",
            "        'x1': input_layer_lib.Input(shape=(1,)),",
            "        'x2': input_layer_lib.Input(shape=(1,))",
            "    }",
            "    inner_outputs = {",
            "        'x1+x2': layers.Add()([inner_inputs['x1'], inner_inputs['x2']]),",
            "        'x1*x2': layers.Multiply()([inner_inputs['x1'], inner_inputs['x2']])",
            "    }",
            "    inner_network = functional.Functional(",
            "        inner_inputs, inner_outputs)",
            "",
            "    inputs = [",
            "        input_layer_lib.Input(shape=(1,)),",
            "        input_layer_lib.Input(shape=(1,))",
            "    ]",
            "    middle = inner_network({'x1': inputs[0], 'x2': inputs[1]})",
            "    outputs = layers.Add()([middle['x1+x2'], middle['x1*x2']])",
            "    network = functional.Functional(inputs, outputs)",
            "",
            "    network = functional.Functional.from_config(network.get_config())",
            "",
            "    # Computes: `(x1+x2) + (x1*x2)`",
            "    result_tensor = network(",
            "        [array_ops.ones((1, 1), 'float32'),",
            "         array_ops.ones((1, 1), 'float32')])",
            "    result = self.evaluate(result_tensor)",
            "    self.assertAllEqual(result, [[3.]])",
            "",
            "    output_shape = network.compute_output_shape([(None, 1), (None, 1)])",
            "    self.assertListEqual(output_shape.as_list(), [None, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph']))",
            "  def test_updates_with_direct_call(self):",
            "    inputs = input_layer_lib.Input(shape=(10,))",
            "    x = layers.BatchNormalization()(inputs)",
            "    x = layers.Dense(10)(x)",
            "    model = training_lib.Model(inputs, x)",
            "",
            "    ph = backend.placeholder(shape=(10, 10))",
            "    model(ph)",
            "",
            "    self.assertLen(model.updates, 4)",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_dict_mapping_input(self):",
            "",
            "    class ReturnFirst(layers.Layer):",
            "",
            "      def call(self, inputs):",
            "        b, _ = inputs",
            "        return b",
            "",
            "    # Checks that inputs are put in same order as the",
            "    # Model was constructed with.",
            "    b = input_layer_lib.Input(shape=(10,), name='b')",
            "    a = input_layer_lib.Input(shape=(10,), name='a')",
            "    outputs = ReturnFirst()([b, a])",
            "",
            "    b_val = array_ops.ones((10, 10))",
            "    a_val = array_ops.zeros((10, 10))",
            "",
            "    model = training_lib.Model([b, a], outputs)",
            "    res = model({'a': a_val, 'b': b_val})",
            "    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))",
            "",
            "    reversed_model = training_lib.Model([a, b], outputs)",
            "    res = reversed_model({'a': a_val, 'b': b_val})",
            "    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_dict_mapping_single_input(self):",
            "    b = input_layer_lib.Input(shape=(1,), name='b')",
            "    outputs = b * 2",
            "    model = training_lib.Model(b, outputs)",
            "",
            "    b_val = array_ops.ones((1, 1))",
            "    extra_val = array_ops.ones((1, 10))",
            "",
            "    inputs = {'a': extra_val, 'b': b_val}",
            "    res = model(inputs)",
            "",
            "    # Check that 'b' was used and 'a' was ignored.",
            "    self.assertEqual(res.shape.as_list(), [1, 1])",
            "",
            "  @combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "  def test_nested_dict_mapping(self):",
            "    a = input_layer_lib.Input(shape=(1,), dtype='int32', name='a')",
            "    b = input_layer_lib.Input(shape=(1,), dtype='int32', name='b')",
            "    c = input_layer_lib.Input(shape=(1,), dtype='int32', name='c')",
            "    d = input_layer_lib.Input(shape=(1,), dtype='int32', name='d')",
            "    inputs = {'a': (a, b), 'c': (c, d)}",
            "    outputs = 1000 * a + 100 * b + 10 * c + d",
            "    model = training_lib.Model(inputs, outputs)",
            "",
            "    a_val = array_ops.ones((1, 1), dtype='int32')",
            "    b_val = 2 * array_ops.ones((1, 1), dtype='int32')",
            "    c_val = 3 * array_ops.ones((1, 1), dtype='int32')",
            "    d_val = 4 * array_ops.ones((1, 1), dtype='int32')",
            "",
            "    inputs_val = {'a': (a_val, b_val), 'c': (c_val, d_val)}",
            "    res = model(inputs_val)",
            "",
            "    # Check that inputs were flattened in the correct order.",
            "    self.assertFalse(model._enable_dict_to_input_mapping)",
            "    self.assertEqual(self.evaluate(res), [1234])",
            "",
            "",
            "@combinations.generate(combinations.keras_mode_combinations())",
            "class AddLossTest(keras_parameterized.TestCase):",
            "",
            "  def test_add_loss_outside_call_only_loss(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    mid = layers.Dense(10)(inputs)",
            "    outputs = layers.Dense(1)(mid)",
            "    model = training_lib.Model(inputs, outputs)",
            "    model.add_loss(math_ops.reduce_mean(outputs))",
            "    self.assertLen(model.losses, 1)",
            "",
            "    initial_weights = model.get_weights()",
            "",
            "    x = np.ones((10, 10))",
            "    model.compile(",
            "        'sgd',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model.fit(x, batch_size=2, epochs=1)",
            "",
            "    model2 = model.from_config(model.get_config())",
            "    model2.compile(",
            "        'sgd',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model2.set_weights(initial_weights)",
            "    model2.fit(x, batch_size=2, epochs=1)",
            "",
            "    # The TFOpLayer and the AddLoss layer are serialized.",
            "    self.assertLen(model2.layers, 5)",
            "    self.assertAllClose(model.get_weights(), model2.get_weights())",
            "",
            "  def test_add_loss_outside_call_multiple_losses(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    x1 = layers.Dense(10)(inputs)",
            "    x2 = layers.Dense(10)(x1)",
            "    outputs = layers.Dense(1)(x2)",
            "    model = training_lib.Model(inputs, outputs)",
            "    model.add_loss(math_ops.reduce_sum(x1 * x2))",
            "    model.add_loss(math_ops.reduce_mean(outputs))",
            "    self.assertLen(model.losses, 2)",
            "",
            "    initial_weights = model.get_weights()",
            "",
            "    x, y = np.ones((10, 10)), np.ones((10, 1))",
            "    model.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model.fit(x, y, batch_size=2, epochs=1)",
            "",
            "    model2 = model.from_config(model.get_config())",
            "    model2.compile(",
            "        'sgd',",
            "        'mse',",
            "        run_eagerly=testing_utils.should_run_eagerly())",
            "    model2.set_weights(initial_weights)",
            "    model2.fit(x, y, batch_size=2, epochs=1)",
            "",
            "    self.assertAllClose(model.get_weights(), model2.get_weights())",
            "",
            "  def test_add_loss_crossentropy_backtracking(self):",
            "    inputs = input_layer_lib.Input((2,))",
            "    labels = input_layer_lib.Input((1,))",
            "    outputs = layers.Dense(1, activation='sigmoid')(inputs)",
            "    model = functional.Functional([inputs, labels], outputs)",
            "    model.add_loss(losses.binary_crossentropy(labels, outputs))",
            "    model.compile('adam')",
            "    x = np.random.random((2, 2))",
            "    y = np.random.random((2, 1))",
            "    model.fit([x, y])",
            "",
            "    inputs = input_layer_lib.Input((2,))",
            "    labels = input_layer_lib.Input((2,))",
            "    outputs = layers.Dense(2, activation='softmax')(inputs)",
            "    model = functional.Functional([inputs, labels], outputs)",
            "    model.add_loss(losses.categorical_crossentropy(labels, outputs))",
            "    model.compile('adam')",
            "    x = np.random.random((2, 2))",
            "    y = np.random.random((2, 2))",
            "    model.fit([x, y])",
            "",
            "    inputs = input_layer_lib.Input((2,))",
            "    labels = input_layer_lib.Input((1,), dtype='int32')",
            "    outputs = layers.Dense(2, activation='softmax')(inputs)",
            "    model = functional.Functional([inputs, labels], outputs)",
            "    model.add_loss(losses.sparse_categorical_crossentropy(labels, outputs))",
            "    model.compile('adam')",
            "    x = np.random.random((2, 2))",
            "    y = np.random.randint(0, 2, size=(2, 1))",
            "    model.fit([x, y])",
            "",
            "",
            "@combinations.generate(combinations.keras_mode_combinations())",
            "class WeightAccessTest(keras_parameterized.TestCase):",
            "",
            "  def test_functional_model(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    x1 = layers.Dense(10)(inputs)",
            "    x2 = layers.Dense(10)(x1)",
            "    outputs = layers.Dense(1)(x2)",
            "    model = training_lib.Model(inputs, outputs)",
            "",
            "    self.assertEqual(len(model.weights), 6)",
            "",
            "  def test_sequential_model_with_input_shape(self):",
            "    x1 = layers.Dense(10, input_shape=(10,))",
            "    x2 = layers.Dense(10)",
            "    x3 = layers.Dense(1)",
            "    model = sequential.Sequential([x1, x2, x3])",
            "",
            "    self.assertEqual(len(model.weights), 6)",
            "",
            "  def test_sequential_model_without_input_shape(self):",
            "    x1 = layers.Dense(10)",
            "    x2 = layers.Dense(10)",
            "    x3 = layers.Dense(1)",
            "    model = sequential.Sequential([x1, x2, x3])",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError, 'Weights for model .* have not yet been created'):",
            "      _ = model.weights",
            "",
            "  def test_subclass_model_with_build_method(self):",
            "",
            "    class SubclassModel(models.Model):",
            "",
            "      def build(self, input_shape):",
            "        self.w = self.add_weight(shape=input_shape[-1], initializer='ones')",
            "",
            "      def call(self, inputs):",
            "        return inputs * self.w",
            "",
            "    model = SubclassModel()",
            "",
            "    with self.assertRaisesRegex(",
            "        ValueError, 'Weights for model .* have not yet been created'):",
            "      _ = model.weights",
            "",
            "    model(input_layer_lib.Input((10,)))",
            "    self.assertEqual(len(model.weights), 1)",
            "",
            "  def test_subclass_model_without_build_method(self):",
            "",
            "    class SubclassModel(models.Model):",
            "",
            "      def __init__(self):",
            "        super(SubclassModel, self).__init__()",
            "        self.w = self.add_weight(shape=(), initializer='ones')",
            "",
            "      def call(self, inputs):",
            "        return inputs * self.w",
            "",
            "    model = SubclassModel()",
            "    self.assertEqual(len(model.weights), 1)",
            "",
            "",
            "@combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "class DTypeTest(keras_parameterized.TestCase):",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_graph_network_dtype(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertEqual(network.dtype, 'float32')",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_subclassed_network_dtype(self):",
            "",
            "    class IdentityNetwork(training_lib.Model):",
            "",
            "      def call(self, inputs):",
            "        return inputs",
            "",
            "    network = IdentityNetwork()",
            "    self.assertEqual(network.dtype, 'float32')",
            "    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float32')",
            "",
            "    network = IdentityNetwork(dtype='float16')",
            "    self.assertEqual(network.dtype, 'float16')",
            "    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float16')",
            "",
            "    network = IdentityNetwork(autocast=False)",
            "    self.assertEqual(network.dtype, 'float32')",
            "    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float64')",
            "",
            "",
            "class AttrTrackingLayer(base_layer.Layer):",
            "  \"\"\"Count how many times `dynamic` and `stateful` are called.",
            "",
            "  These counts are used to test that the attribute cache behaves as expected.",
            "  \"\"\"",
            "  def __init__(self, *args, **kwargs):",
            "    self.stateful_count = 0",
            "    self.dynamic_count = 0",
            "    super(AttrTrackingLayer, self).__init__(*args, **kwargs)",
            "",
            "  @base_layer.Layer.stateful.getter",
            "  def stateful(self):",
            "    self.stateful_count += 1",
            "    return super(AttrTrackingLayer, self).stateful",
            "",
            "  @property",
            "  def dynamic(self):",
            "    self.dynamic_count += 1",
            "    return super(AttrTrackingLayer, self).dynamic",
            "",
            "",
            "@combinations.generate(combinations.combine(mode=['graph', 'eager']))",
            "class CacheCorrectnessTest(keras_parameterized.TestCase):",
            "",
            "  def layer_and_network_test(self):",
            "    # Top level layer",
            "    network = functional.Functional()",
            "",
            "    layer_0 = AttrTrackingLayer()",
            "",
            "    sub_network = functional.Functional()",
            "    layer_1 = AttrTrackingLayer(dynamic=True)",
            "    layer_2 = AttrTrackingLayer()",
            "    sub_network.sub_layers = [layer_1, layer_2]",
            "",
            "    network.sub_layer = layer_0",
            "",
            "    for _ in range(2):",
            "      self.assertEqual(network.dynamic, False)",
            "      self.assertEqual(network.stateful, False)",
            "",
            "      # The second pass should be a cache hit.",
            "      self.assertEqual(layer_0.dynamic_count, 1)",
            "      self.assertEqual(layer_0.stateful_count, 1)",
            "",
            "    # Mutations of the sub-layer should force recalculation of the network's",
            "    # stateful attribute. (mutations bubble up.)",
            "    layer_0.stateful = True",
            "    self.assertEqual(network.stateful, True)",
            "    self.assertEqual(layer_0.stateful_count, 2)",
            "",
            "    layer_0.stateful = False",
            "    self.assertEqual(network.stateful, False)",
            "    self.assertEqual(layer_0.stateful_count, 3)",
            "",
            "    # But changing stateful should not affect dynamic.",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(layer_0.dynamic_count, 1)",
            "",
            "    network.sub_network = sub_network",
            "",
            "    # Adding to the topology should invalidate the cache and reflect in the top",
            "    # level network.",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 2)",
            "    self.assertEqual(layer_1.dynamic_count, 1)",
            "",
            "    # Still dynamic, but we need to recompute.",
            "    sub_network.sub_layers.pop()",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 3)",
            "    self.assertEqual(layer_1.dynamic_count, 2)",
            "",
            "    # Now that we've removed the dynamic layer deep in the layer hierarchy, we",
            "    # need to make sure that that bubbles up through all the levels.",
            "    sub_network.sub_layers.pop()",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(layer_0.dynamic_count, 4)",
            "    self.assertEqual(layer_1.dynamic_count, 2)",
            "",
            "    # Now check with a tracked dict.",
            "    sub_network.sub_layers = {",
            "        \"layer_1\": layer_1,",
            "        \"layer_2\": layer_2,",
            "    }",
            "",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 5)",
            "    self.assertEqual(layer_1.dynamic_count, 3)",
            "",
            "    # In-place assignment should still invalidate the cache.",
            "    sub_network.sub_layers[\"layer_1\"] = layer_1",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(layer_0.dynamic_count, 6)",
            "    self.assertEqual(layer_1.dynamic_count, 4)",
            "",
            "    sub_network.sub_layers[\"layer_1\"] = None",
            "    for _ in range(2):",
            "      self.assertEqual(network.dynamic, False)",
            "      self.assertEqual(layer_0.dynamic_count, 7)",
            "      self.assertEqual(layer_1.dynamic_count, 4)",
            "",
            "    layer_3 = AttrTrackingLayer()",
            "    layer_3.stateful = True",
            "",
            "    sub_network.sub_layers = None",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(network.stateful, False)",
            "",
            "    # Test duplicate layers.",
            "    sub_network.sub_layers = [layer_1, layer_1, layer_1, layer_3]",
            "    self.assertEqual(network.dynamic, True)",
            "    self.assertEqual(network.stateful, True)",
            "",
            "    for _ in range(3):",
            "      sub_network.sub_layers.pop()",
            "      self.assertEqual(network.dynamic, True)",
            "      self.assertEqual(network.stateful, False)",
            "",
            "    sub_network.sub_layers.pop()",
            "    self.assertEqual(network.dynamic, False)",
            "    self.assertEqual(network.stateful, False)",
            "",
            "  def test_compute_output_shape_cache(self):",
            "    # See https://github.com/tensorflow/tensorflow/issues/32029.",
            "    x = input_layer_lib.Input(shape=(None, 32))",
            "    dense = layers.Dense(2)",
            "    y = dense(x)",
            "    network = functional.Functional(x, y, name='dense_network')",
            "",
            "    for i in range(999, 1024):",
            "      self.assertEqual(network.compute_output_shape((1, i, 32)), (1, i, 2))",
            "",
            "  def test_2d_inputs_squeezed_to_1d(self):",
            "    input_1d = input_layer_lib.Input(shape=())",
            "    outputs = input_1d * 2.",
            "    net = functional.Functional(input_1d, outputs)",
            "",
            "    x = np.ones((10, 1))",
            "    y = net(x)",
            "    self.assertEqual(y.shape.rank, 1)",
            "",
            "  def test_1d_inputs_expanded_to_2d(self):",
            "    input_1d = input_layer_lib.Input(shape=(1,))",
            "    outputs = input_1d * 2.",
            "    net = functional.Functional(input_1d, outputs)",
            "",
            "    x = np.ones((10,))",
            "    y = net(x)",
            "    self.assertEqual(y.shape.rank, 2)",
            "",
            "  def test_training_passed_during_construction(self):",
            "",
            "    def _call(inputs, training):",
            "      if training is None:",
            "        return inputs * -1.0",
            "      elif training:",
            "        return inputs",
            "      else:",
            "        return inputs * 0.0",
            "",
            "    class MyLayer(base_layer.Layer):",
            "",
            "      def call(self, inputs, training=True):",
            "        return _call(inputs, training)",
            "",
            "    my_layer = MyLayer()",
            "    x = np.ones((1, 10))",
            "",
            "    # Hard-coded `true` value passed during construction is respected.",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, training=True)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, True))",
            "    self.assertAllEqual(network(x, training=False), _call(x, True))",
            "    self.assertAllEqual(network(x), _call(x, True))",
            "",
            "    # Hard-coded `false` value passed during construction is respected.",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, training=False)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, False))",
            "    self.assertAllEqual(network(x, training=False), _call(x, False))",
            "    self.assertAllEqual(network(x), _call(x, False))",
            "",
            "    if context.executing_eagerly():",
            "      # In v2, construction still works when no `training` is specified",
            "      # When no value passed during construction, it uses the local default.",
            "      inputs = input_layer_lib.Input(10)",
            "      outputs = my_layer(inputs)",
            "      network = functional.Functional(inputs, outputs)",
            "      self.assertAllEqual(network(x, training=True), _call(x, True))",
            "      self.assertAllEqual(network(x, training=False), _call(x, False))",
            "      self.assertAllEqual(network(x), _call(x, True))  # Use local default",
            "",
            "    # `None` value passed positionally during construction is ignored at runtime",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, None)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, True))",
            "    self.assertAllEqual(network(x, training=False), _call(x, False))",
            "    if context.executing_eagerly():",
            "      self.assertAllEqual(network(x), _call(x, True))  # Use local default",
            "    else:",
            "      # in v1 training would have defaulted to using the `None` inside the layer",
            "      # if training is not passed at runtime",
            "      self.assertAllEqual(network(x), _call(x, None))",
            "",
            "    # `None` value passed as kwarg during construction is ignored at runtime.",
            "    inputs = input_layer_lib.Input(10)",
            "    outputs = my_layer(inputs, training=None)",
            "    network = functional.Functional(inputs, outputs)",
            "    self.assertAllEqual(network(x, training=True), _call(x, True))",
            "    self.assertAllEqual(network(x, training=False), _call(x, False))",
            "    if context.executing_eagerly():",
            "      self.assertAllEqual(network(x), _call(x, True))  # Use local default",
            "    else:",
            "      # in v1 training would have defaulted to using the `None` inside the layer",
            "      # if training is not passed at runtime",
            "      self.assertAllEqual(network(x), _call(x, None))",
            "",
            "",
            "class InputsOutputsErrorTest(keras_parameterized.TestCase):",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_input_error(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    with self.assertRaisesRegex(",
            "        TypeError, \"('Keyword argument not understood:', 'input')\"):",
            "      models.Model(input=inputs, outputs=outputs)",
            "",
            "  @testing_utils.enable_v2_dtype_behavior",
            "  def test_output_error(self):",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    with self.assertRaisesRegex(",
            "        TypeError, \"('Keyword argument not understood:', 'output')\"):",
            "      models.Model(inputs=inputs, output=outputs)",
            "",
            "  def test_input_spec(self):",
            "    if not context.executing_eagerly():",
            "      return",
            "    inputs = input_layer_lib.Input((10,))",
            "    outputs = layers.Dense(10)(inputs)",
            "    model = models.Model(inputs, outputs)",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model(np.zeros((3, 11)))",
            "",
            "  def test_input_spec_list_of_inputs(self):",
            "    if not context.executing_eagerly():",
            "      return",
            "    input_1 = input_layer_lib.Input((10,), name='1')",
            "    input_2 = input_layer_lib.Input((5,), name='2')",
            "    x = layers.Concatenate()([input_1, input_2])",
            "    outputs = layers.Dense(10)(x)",
            "    model = models.Model([input_1, input_2], outputs)",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expects 2 input.*'):",
            "      model(np.zeros((3, 10)))",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expects 2 input.*'):",
            "      model([np.zeros((3, 10)), np.zeros((3, 5)), np.zeros((3, 10))])",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model([np.zeros((3, 10)), np.zeros((3, 6))])",
            "",
            "    # Test passing data via dict keyed by input name",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'Missing data for input.*'):",
            "      model({'1': np.zeros((3, 10))})",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})",
            "",
            "  def test_input_spec_dict(self):",
            "    if not context.executing_eagerly():",
            "      return",
            "    input_1 = input_layer_lib.Input((10,))",
            "    input_2 = input_layer_lib.Input((5,))",
            "    x = layers.Concatenate()([input_1, input_2])",
            "    outputs = layers.Dense(10)(x)",
            "    model = models.Model({'1': input_1, '2': input_2}, outputs)",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'Missing data for input.*'):",
            "      model({'1': np.zeros((3, 10))})",
            "    with self.assertRaisesRegex(",
            "        ValueError, r'.*expected shape=.*'):",
            "      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})",
            "",
            "",
            "class FunctionalSubclassModel(training_lib.Model):",
            "",
            "  def __init__(self, *args, **kwargs):",
            "    my_input = input_layer_lib.Input(shape=(16,))",
            "    dense = layers.Dense(32, activation='relu')",
            "    output = dense(my_input)",
            "    outputs = {'output': output}",
            "    super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)",
            "",
            "",
            "class MixinClass(object):",
            "",
            "  def __init__(self, foo, **kwargs):",
            "    self._foo = foo",
            "    super().__init__(**kwargs)",
            "",
            "  def get_foo(self):",
            "    return self._foo",
            "",
            "",
            "class SubclassedModel(training_lib.Model):",
            "",
            "  def __init__(self, bar, **kwargs):",
            "    self._bar = bar",
            "    super().__init__(**kwargs)",
            "",
            "  def get_bar(self):",
            "    return self._bar",
            "",
            "",
            "class MultipleInheritanceModelTest(keras_parameterized.TestCase):",
            "",
            "  def testFunctionalSubclass(self):",
            "    m = FunctionalSubclassModel()",
            "    # Some smoke test for the weights and output shape of the model",
            "    self.assertLen(m.weights, 2)",
            "    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])",
            "",
            "  def testFunctionalSubclassPreMixin(self):",
            "    class MixedFunctionalSubclassModel(MixinClass, FunctionalSubclassModel):",
            "      pass",
            "",
            "    m = MixedFunctionalSubclassModel(foo='123')",
            "    self.assertTrue(m._is_graph_network)",
            "    self.assertLen(m.weights, 2)",
            "    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])",
            "    self.assertEqual(m.get_foo(), '123')",
            "",
            "  def testFunctionalSubclassPostMixin(self):",
            "    # Make sure the the mixin class is also init correct when the order changed.",
            "",
            "    class MixedFunctionalSubclassModel(FunctionalSubclassModel, MixinClass):",
            "      pass",
            "",
            "    m = MixedFunctionalSubclassModel(foo='123')",
            "    self.assertTrue(m._is_graph_network)",
            "    self.assertLen(m.weights, 2)",
            "    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])",
            "    self.assertEqual(m.get_foo(), '123')",
            "",
            "  def testSubclassModelPreMixin(self):",
            "    class MixedSubclassModel(MixinClass, SubclassedModel):",
            "      pass",
            "",
            "    m = MixedSubclassModel(foo='123', bar='456')",
            "    self.assertFalse(m._is_graph_network)",
            "    self.assertEqual(m.get_foo(), '123')",
            "    self.assertEqual(m.get_bar(), '456')",
            "",
            "",
            "if __name__ == '__main__':",
            "  test.main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "50": [],
            "51": [],
            "52": [],
            "53": [
                "yaml"
            ],
            "54": [],
            "630": [
                "NetworkConstructionTest",
                "test_multi_input_multi_output_recursion"
            ],
            "631": [
                "NetworkConstructionTest",
                "test_multi_input_multi_output_recursion"
            ],
            "632": [
                "NetworkConstructionTest",
                "test_multi_input_multi_output_recursion"
            ],
            "633": [
                "NetworkConstructionTest"
            ],
            "1364": [
                "NetworkConstructionTest",
                "test_constant_initializer_with_numpy"
            ],
            "1365": [
                "NetworkConstructionTest",
                "test_constant_initializer_with_numpy"
            ],
            "1366": [
                "NetworkConstructionTest",
                "test_constant_initializer_with_numpy"
            ],
            "1367": [
                "NetworkConstructionTest"
            ]
        },
        "addLocation": []
    },
    "tensorflow/python/keras/engine/training.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "   import h5py"
            },
            "1": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 85,
                "PatchRowcode": " except ImportError:"
            },
            "2": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "   h5py = None"
            },
            "3": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "4": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-try:"
            },
            "5": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  import yaml"
            },
            "6": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-except ImportError:"
            },
            "7": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  yaml = None"
            },
            "8": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 87,
                "PatchRowcode": " # pylint: enable=g-import-not-at-top"
            },
            "9": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 88,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 89,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 2382,
                "afterPatchRowNumber": 2377,
                "PatchRowcode": "   def to_yaml(self, **kwargs):"
            },
            "12": {
                "beforePatchRowNumber": 2383,
                "afterPatchRowNumber": 2378,
                "PatchRowcode": "     \"\"\"Returns a yaml string containing the network configuration."
            },
            "13": {
                "beforePatchRowNumber": 2384,
                "afterPatchRowNumber": 2379,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2380,
                "PatchRowcode": "+    Note: Since TF 2.6, this method is no longer supported and will raise a"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2381,
                "PatchRowcode": "+    RuntimeError."
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2382,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": 2385,
                "afterPatchRowNumber": 2383,
                "PatchRowcode": "     To load a network from a yaml save file, use"
            },
            "18": {
                "beforePatchRowNumber": 2386,
                "afterPatchRowNumber": 2384,
                "PatchRowcode": "     `keras.models.model_from_yaml(yaml_string, custom_objects={})`."
            },
            "19": {
                "beforePatchRowNumber": 2387,
                "afterPatchRowNumber": 2385,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 2397,
                "afterPatchRowNumber": 2395,
                "PatchRowcode": "         A YAML string."
            },
            "21": {
                "beforePatchRowNumber": 2398,
                "afterPatchRowNumber": 2396,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 2399,
                "afterPatchRowNumber": 2397,
                "PatchRowcode": "     Raises:"
            },
            "23": {
                "beforePatchRowNumber": 2400,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ImportError: if yaml module is not found."
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2398,
                "PatchRowcode": "+        RuntimeError: announces that the method poses a security risk"
            },
            "25": {
                "beforePatchRowNumber": 2401,
                "afterPatchRowNumber": 2399,
                "PatchRowcode": "     \"\"\""
            },
            "26": {
                "beforePatchRowNumber": 2402,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if yaml is None:"
            },
            "27": {
                "beforePatchRowNumber": 2403,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-      raise ImportError("
            },
            "28": {
                "beforePatchRowNumber": 2404,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-          'Requires yaml module installed (`pip install pyyaml`).')"
            },
            "29": {
                "beforePatchRowNumber": 2405,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return yaml.dump(self._updated_config(), **kwargs)"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2400,
                "PatchRowcode": "+    raise RuntimeError("
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2401,
                "PatchRowcode": "+        'Method `model.to_yaml()` has been removed due to security risk of '"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2402,
                "PatchRowcode": "+        'arbitrary code execution. Please use `model.to_json()` instead.'"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2403,
                "PatchRowcode": "+    )"
            },
            "34": {
                "beforePatchRowNumber": 2406,
                "afterPatchRowNumber": 2404,
                "PatchRowcode": " "
            },
            "35": {
                "beforePatchRowNumber": 2407,
                "afterPatchRowNumber": 2405,
                "PatchRowcode": "   def reset_states(self):"
            },
            "36": {
                "beforePatchRowNumber": 2408,
                "afterPatchRowNumber": 2406,
                "PatchRowcode": "     for layer in self.layers:"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Training-related part of the Keras engine.\"\"\"",
            "",
            "import copy",
            "import itertools",
            "import json",
            "import os",
            "import warnings",
            "",
            "from tensorflow.python.autograph.lang import directives",
            "from tensorflow.python.data.experimental.ops import distribute_options",
            "from tensorflow.python.data.ops import dataset_ops",
            "from tensorflow.python.distribute import collective_all_reduce_strategy",
            "from tensorflow.python.distribute import distribution_strategy_context as ds_context",
            "from tensorflow.python.distribute import values as ds_values",
            "from tensorflow.python.distribute.coordinator import cluster_coordinator",
            "from tensorflow.python.eager import backprop",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import def_function",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import errors_impl",
            "from tensorflow.python.framework import func_graph",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import sparse_tensor",
            "from tensorflow.python.framework import tensor_shape",
            "from tensorflow.python.keras import backend",
            "from tensorflow.python.keras import callbacks as callbacks_module",
            "from tensorflow.python.keras import optimizer_v1",
            "from tensorflow.python.keras import optimizers",
            "from tensorflow.python.keras.engine import base_layer",
            "from tensorflow.python.keras.engine import base_layer_utils",
            "from tensorflow.python.keras.engine import compile_utils",
            "from tensorflow.python.keras.engine import data_adapter",
            "from tensorflow.python.keras.engine import training_utils",
            "from tensorflow.python.keras.mixed_precision import loss_scale_optimizer as lso",
            "from tensorflow.python.keras.mixed_precision import policy",
            "from tensorflow.python.keras.saving import hdf5_format",
            "from tensorflow.python.keras.saving import save",
            "from tensorflow.python.keras.saving import saving_utils",
            "from tensorflow.python.keras.saving.saved_model import json_utils",
            "from tensorflow.python.keras.saving.saved_model import model_serialization",
            "from tensorflow.python.keras.utils import generic_utils",
            "from tensorflow.python.keras.utils import layer_utils",
            "from tensorflow.python.keras.utils import tf_utils",
            "from tensorflow.python.keras.utils import version_utils",
            "from tensorflow.python.keras.utils.io_utils import ask_to_proceed_with_overwrite",
            "from tensorflow.python.keras.utils.io_utils import path_to_string",
            "from tensorflow.python.keras.utils.mode_keys import ModeKeys",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import sparse_ops",
            "from tensorflow.python.ops import summary_ops_v2",
            "from tensorflow.python.ops import variables",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.profiler import trace",
            "from tensorflow.python.saved_model import constants as sm_constants",
            "from tensorflow.python.saved_model import loader_impl as sm_loader",
            "from tensorflow.python.training import checkpoint_management",
            "from tensorflow.python.training import py_checkpoint_reader",
            "from tensorflow.python.training.tracking import base as trackable",
            "from tensorflow.python.training.tracking import data_structures",
            "from tensorflow.python.training.tracking import util as trackable_utils",
            "from tensorflow.python.util import nest",
            "from tensorflow.python.util import tf_decorator",
            "from tensorflow.python.util.tf_export import keras_export",
            "from tensorflow.tools.docs import doc_controls",
            "",
            "",
            "# pylint: disable=g-import-not-at-top",
            "try:",
            "  import h5py",
            "except ImportError:",
            "  h5py = None",
            "",
            "try:",
            "  import yaml",
            "except ImportError:",
            "  yaml = None",
            "# pylint: enable=g-import-not-at-top",
            "",
            "",
            "def disable_multi_worker(method):",
            "  \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"",
            "",
            "  def _method_wrapper(self, *args, **kwargs):",
            "    if self._in_multi_worker_mode():  # pylint: disable=protected-access",
            "      raise ValueError('{} is not supported in multi-worker mode.'.format(",
            "          method.__name__))",
            "    return method(self, *args, **kwargs)",
            "",
            "  return tf_decorator.make_decorator(",
            "      target=method, decorator_func=_method_wrapper)",
            "",
            "",
            "def inject_functional_model_class(cls):",
            "  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"",
            "  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top",
            "  if cls == Model or cls == training_v1.Model:",
            "    return functional.Functional",
            "  # In case there is any multiple inheritance, we stop injecting the",
            "  # class if keras model is not in its class hierarchy.",
            "  if cls == object:",
            "    return object",
            "",
            "  cls.__bases__ = tuple(inject_functional_model_class(base)",
            "                        for base in cls.__bases__)",
            "  # Trigger any `__new__` class swapping that needed to happen on `Functional`",
            "  # but did not because functional was not in the class hierarchy.",
            "  cls.__new__(cls)",
            "",
            "  return cls",
            "",
            "",
            "def is_functional_model_init_params(args, kwargs):",
            "  return (len(args) == 2 or",
            "          len(args) == 1 and 'outputs' in kwargs or",
            "          'inputs' in kwargs and 'outputs' in kwargs)",
            "",
            "",
            "@keras_export('keras.Model', 'keras.models.Model')",
            "class Model(base_layer.Layer, version_utils.ModelVersionSelector):",
            "  \"\"\"`Model` groups layers into an object with training and inference features.",
            "",
            "  Args:",
            "      inputs: The input(s) of the model: a `keras.Input` object or list of",
            "          `keras.Input` objects.",
            "      outputs: The output(s) of the model. See Functional API example below.",
            "      name: String, the name of the model.",
            "",
            "  There are two ways to instantiate a `Model`:",
            "",
            "  1 - With the \"Functional API\", where you start from `Input`,",
            "  you chain layer calls to specify the model's forward pass,",
            "  and finally you create your model from inputs and outputs:",
            "",
            "  ```python",
            "  import tensorflow as tf",
            "",
            "  inputs = tf.keras.Input(shape=(3,))",
            "  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)",
            "  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)",
            "  model = tf.keras.Model(inputs=inputs, outputs=outputs)",
            "  ```",
            "",
            "  2 - By subclassing the `Model` class: in that case, you should define your",
            "  layers in `__init__` and you should implement the model's forward pass",
            "  in `call`.",
            "",
            "  ```python",
            "  import tensorflow as tf",
            "",
            "  class MyModel(tf.keras.Model):",
            "",
            "    def __init__(self):",
            "      super(MyModel, self).__init__()",
            "      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)",
            "      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)",
            "",
            "    def call(self, inputs):",
            "      x = self.dense1(inputs)",
            "      return self.dense2(x)",
            "",
            "  model = MyModel()",
            "  ```",
            "",
            "  If you subclass `Model`, you can optionally have",
            "  a `training` argument (boolean) in `call`, which you can use to specify",
            "  a different behavior in training and inference:",
            "",
            "  ```python",
            "  import tensorflow as tf",
            "",
            "  class MyModel(tf.keras.Model):",
            "",
            "    def __init__(self):",
            "      super(MyModel, self).__init__()",
            "      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)",
            "      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)",
            "      self.dropout = tf.keras.layers.Dropout(0.5)",
            "",
            "    def call(self, inputs, training=False):",
            "      x = self.dense1(inputs)",
            "      if training:",
            "        x = self.dropout(x, training=training)",
            "      return self.dense2(x)",
            "",
            "  model = MyModel()",
            "  ```",
            "",
            "  Once the model is created, you can config the model with losses and metrics",
            "  with `model.compile()`, train the model with `model.fit()`, or use the model",
            "  to do prediction with `model.predict()`.",
            "  \"\"\"",
            "  _TF_MODULE_IGNORED_PROPERTIES = frozenset(",
            "      itertools.chain(('_train_counter', '_test_counter', '_predict_counter',",
            "                       '_steps_per_execution'),",
            "                      base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES))  # pylint: disable=protected-access",
            "",
            "  def __new__(cls, *args, **kwargs):",
            "    # Signature detection",
            "    if is_functional_model_init_params(args, kwargs) and cls == Model:",
            "      # Functional model",
            "      from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "      return functional.Functional(skip_init=True, *args, **kwargs)",
            "    else:",
            "      return super(Model, cls).__new__(cls, *args, **kwargs)",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def __init__(self, *args, **kwargs):",
            "    self._is_model_for_instrumentation = True",
            "    base_layer.keras_api_gauge.get_cell('model').set(True)",
            "",
            "    # Special case for Subclassed Functional Model, which we couldn't detect",
            "    # when __new__ is called. We only realize it is a functional model when it",
            "    # calls super.__init__ with input and output tensor.",
            "    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "    if (is_functional_model_init_params(args, kwargs) and",
            "        not isinstance(self, functional.Functional)):",
            "      # Filter the kwargs for multiple inheritance.",
            "      supported_kwargs = ['inputs', 'outputs', 'name', 'trainable', 'skip_init']",
            "      model_kwargs = {k: kwargs[k] for k in kwargs if k in supported_kwargs}",
            "      other_kwargs = {k: kwargs[k] for k in kwargs if k not in supported_kwargs}",
            "      inject_functional_model_class(self.__class__)",
            "      functional.Functional.__init__(self, *args, **model_kwargs)",
            "",
            "      # In case there is any multiple inheritance here, we need to call the",
            "      # __init__ for any class that appears after the Functional class.",
            "      clz_to_init = []",
            "      found_functional_class = False",
            "      for clz in self.__class__.__bases__:",
            "        if issubclass(clz, functional.Functional):",
            "          found_functional_class = True",
            "          continue",
            "        if found_functional_class:",
            "          clz_to_init.append(clz)",
            "",
            "      if clz_to_init:",
            "        for clz in clz_to_init:",
            "          clz.__init__(self, *args, **other_kwargs)",
            "      elif other_kwargs:",
            "        # In case there are unused kwargs, we should raise an error to user, in",
            "        # case they have a typo in the param name.",
            "        raise TypeError(",
            "            'The following keyword arguments aren\\'t supported: {}'.format(",
            "                other_kwargs))",
            "      return",
            "",
            "    base_layer.keras_api_gauge.get_cell('Model subclass').set(True)",
            "    # The following are implemented as property functions:",
            "    # self.trainable_weights",
            "    # self.non_trainable_weights",
            "    # `inputs` / `outputs` will only appear in kwargs if either are misspelled.",
            "    generic_utils.validate_kwargs(kwargs, {",
            "        'trainable', 'dtype', 'dynamic', 'name', 'autocast', 'inputs', 'outputs'",
            "    })",
            "    super(Model, self).__init__(**kwargs)",
            "    # By default, Model is a subclass model, which is not in graph network.",
            "    self._is_graph_network = False",
            "",
            "    self.inputs = None",
            "    self.outputs = None",
            "    self.input_names = None",
            "    self.output_names = None",
            "    # stop_training is used by callback to stop training when error happens",
            "    self.stop_training = False",
            "    self.history = None",
            "    # These objects are used in the default `Model.compile`. They are not",
            "    # guaranteed to be set after `Model.compile` is called, as users can",
            "    # override compile with custom logic.",
            "    self.compiled_loss = None",
            "    self.compiled_metrics = None",
            "",
            "    # This is True for Sequential networks and Functional networks.",
            "    self._compute_output_and_mask_jointly = False",
            "",
            "    # Don't reset compilation if already done. This may occur if calling",
            "    # `__init__` (or `_init_graph_network`) on an already-compiled model",
            "    # such as a Sequential model. Sequential models may need to rebuild",
            "    # themselves after compilation.",
            "    self._maybe_create_attribute('_is_compiled', False)",
            "    self._maybe_create_attribute('optimizer', None)",
            "",
            "    # Model must be created under scope of DistStrat it will be trained with.",
            "    if ds_context.has_strategy():",
            "      self._distribution_strategy = ds_context.get_strategy()",
            "    else:",
            "      self._distribution_strategy = None",
            "",
            "    self._cluster_coordinator = None",
            "",
            "    # Defaults to value of `tf.config.experimental_functions_run_eagerly`.",
            "    self._run_eagerly = None",
            "    # Initialize cache attrs.",
            "    self._reset_compile_cache()",
            "",
            "    # Fault-tolerance handler. Set in `ModelCheckpoint`.",
            "    self._training_state = None",
            "    self._saved_model_inputs_spec = None",
            "    self._trackable_saver = (",
            "        trackable_utils.saver_with_op_caching(self))",
            "",
            "    self._steps_per_execution = None",
            "",
            "    self._init_batch_counters()",
            "    self._base_model_initialized = True",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _init_batch_counters(self):",
            "    # Untracked Variables, used to keep track of mini-batches seen in `fit`,",
            "    # `evaluate`, and `predict`.",
            "    agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA",
            "    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)",
            "    self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)",
            "    self._predict_counter = variables.Variable(",
            "        0, dtype='int64', aggregation=agg)",
            "",
            "  def __setattr__(self, name, value):",
            "    if not getattr(self, '_self_setattr_tracking', True):",
            "      super(Model, self).__setattr__(name, value)",
            "      return",
            "",
            "    if all(",
            "        isinstance(v, (base_layer.Layer,",
            "                       data_structures.TrackableDataStructure)) or",
            "        base_layer_utils.has_weights(v) for v in nest.flatten(value)):",
            "      try:",
            "        self._base_model_initialized",
            "      except AttributeError:",
            "        raise RuntimeError(",
            "            'It looks like you are subclassing `Model` and you '",
            "            'forgot to call `super(YourClass, self).__init__()`.'",
            "            ' Always start with this line.')",
            "",
            "    super(Model, self).__setattr__(name, value)",
            "",
            "  @generic_utils.default",
            "  def build(self, input_shape):",
            "    \"\"\"Builds the model based on input shapes received.",
            "",
            "    This is to be used for subclassed models, which do not know at instantiation",
            "    time what their inputs look like.",
            "",
            "    This method only exists for users who want to call `model.build()` in a",
            "    standalone way (as a substitute for calling the model on real data to",
            "    build it). It will never be called by the framework (and thus it will",
            "    never throw unexpected errors in an unrelated workflow).",
            "",
            "    Args:",
            "     input_shape: Single tuple, TensorShape, or list/dict of shapes, where",
            "         shapes are tuples, integers, or TensorShapes.",
            "",
            "    Raises:",
            "      ValueError:",
            "        1. In case of invalid user-provided data (not of type tuple,",
            "           list, TensorShape, or dict).",
            "        2. If the model requires call arguments that are agnostic",
            "           to the input shapes (positional or kwarg in call signature).",
            "        3. If not all layers were properly built.",
            "        4. If float type inputs are not supported within the layers.",
            "",
            "      In each of these cases, the user should build their model by calling it",
            "      on real tensor data.",
            "    \"\"\"",
            "    if self._is_graph_network:",
            "      super(Model, self).build(input_shape)",
            "      return",
            "",
            "    if input_shape is None:",
            "      raise ValueError('Input shape must be defined when calling build on a '",
            "                       'model subclass network.')",
            "    valid_types = (tuple, list, tensor_shape.TensorShape, dict)",
            "    if not isinstance(input_shape, valid_types):",
            "      raise ValueError('Specified input shape is not one of the valid types. '",
            "                       'Please specify a batch input shape of type tuple or '",
            "                       'list of input shapes. User provided '",
            "                       'input type: {}'.format(type(input_shape)))",
            "",
            "    if input_shape and not self.inputs:",
            "      # We create placeholders for the `None`s in the shape and build the model",
            "      # in a Graph. Since tf.Variable is compatible with both eager execution",
            "      # and graph building, the variables created after building the model in",
            "      # a Graph are still valid when executing eagerly.",
            "      if context.executing_eagerly():",
            "        graph = func_graph.FuncGraph('build_graph')",
            "      else:",
            "        graph = backend.get_graph()",
            "      with graph.as_default():",
            "        if (isinstance(input_shape, list) and",
            "            all(d is None or isinstance(d, int) for d in input_shape)):",
            "          input_shape = tuple(input_shape)",
            "        if isinstance(input_shape, list):",
            "          x = [base_layer_utils.generate_placeholders_from_shape(shape)",
            "               for shape in input_shape]",
            "        elif isinstance(input_shape, dict):",
            "          x = {",
            "              k: base_layer_utils.generate_placeholders_from_shape(shape)",
            "              for k, shape in input_shape.items()",
            "          }",
            "        else:",
            "          x = base_layer_utils.generate_placeholders_from_shape(input_shape)",
            "",
            "        kwargs = {}",
            "        call_signature = self._call_full_argspec",
            "        call_args = call_signature.args",
            "        # Exclude `self`, `inputs`, and any argument with a default value.",
            "        if len(call_args) > 2:",
            "          if call_signature.defaults:",
            "            call_args = call_args[2:-len(call_signature.defaults)]",
            "          else:",
            "            call_args = call_args[2:]",
            "          for arg in call_args:",
            "            if arg == 'training':",
            "              # Case where `training` is a positional arg with no default.",
            "              kwargs['training'] = False",
            "            else:",
            "              # Has invalid call signature with unknown positional arguments.",
            "              raise ValueError(",
            "                  'Currently, you cannot build your model if it has '",
            "                  'positional or keyword arguments that are not '",
            "                  'inputs to the model, but are required for its '",
            "                  '`call` method. Instead, in order to instantiate '",
            "                  'and build your model, `call` your model on real '",
            "                  'tensor data with all expected call arguments.')",
            "        elif len(call_args) < 2:",
            "          # Signature without `inputs`.",
            "          raise ValueError('You can only call `build` on a model if its `call` '",
            "                           'method accepts an `inputs` argument.')",
            "        try:",
            "          self.call(x, **kwargs)",
            "        except (errors.InvalidArgumentError, TypeError):",
            "          raise ValueError('You cannot build your model by calling `build` '",
            "                           'if your layers do not support float type inputs. '",
            "                           'Instead, in order to instantiate and build your '",
            "                           'model, `call` your model on real tensor data (of '",
            "                           'the correct dtype).')",
            "    super(Model, self).build(input_shape)",
            "",
            "  @doc_controls.doc_in_current_and_subclasses",
            "  def call(self, inputs, training=None, mask=None):",
            "    \"\"\"Calls the model on new inputs.",
            "",
            "    In this case `call` just reapplies",
            "    all ops in the graph to the new inputs",
            "    (e.g. build a new computational graph from the provided inputs).",
            "",
            "    Note: This method should not be called directly. It is only meant to be",
            "    overridden when subclassing `tf.keras.Model`.",
            "    To call a model on an input, always use the `__call__` method,",
            "    i.e. `model(inputs)`, which relies on the underlying `call` method.",
            "",
            "    Args:",
            "        inputs: A tensor or list of tensors.",
            "        training: Boolean or boolean scalar tensor, indicating whether to run",
            "          the `Network` in training mode or inference mode.",
            "        mask: A mask or list of masks. A mask can be",
            "            either a tensor or None (no mask).",
            "",
            "    Returns:",
            "        A tensor if there is a single output, or",
            "        a list of tensors if there are more than one outputs.",
            "    \"\"\"",
            "    raise NotImplementedError('When subclassing the `Model` class, you should '",
            "                              'implement a `call` method.')",
            "",
            "  def compile(self,",
            "              optimizer='rmsprop',",
            "              loss=None,",
            "              metrics=None,",
            "              loss_weights=None,",
            "              weighted_metrics=None,",
            "              run_eagerly=None,",
            "              steps_per_execution=None,",
            "              **kwargs):",
            "    \"\"\"Configures the model for training.",
            "",
            "    Args:",
            "        optimizer: String (name of optimizer) or optimizer instance. See",
            "          `tf.keras.optimizers`.",
            "        loss: String (name of objective function), objective function or",
            "          `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective",
            "          function is any callable with the signature `loss = fn(y_true,",
            "          y_pred)`, where y_true = ground truth values with shape =",
            "          `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse",
            "          categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.",
            "          y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It",
            "          returns a weighted loss float tensor. If a custom `Loss` instance is",
            "          used and reduction is set to NONE, return value has the shape",
            "          [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;",
            "          otherwise, it is a scalar. If the model has multiple outputs, you can",
            "          use a different loss on each output by passing a dictionary or a list",
            "          of losses. The loss value that will be minimized by the model will",
            "          then be the sum of all individual losses.",
            "        metrics: List of metrics to be evaluated by the model during training",
            "          and testing. Each of this can be a string (name of a built-in",
            "          function), function or a `tf.keras.metrics.Metric` instance. See",
            "          `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A",
            "          function is any callable with the signature `result = fn(y_true,",
            "          y_pred)`. To specify different metrics for different outputs of a",
            "          multi-output model, you could also pass a dictionary, such as",
            "            `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.",
            "              You can also pass a list (len = len(outputs)) of lists of metrics",
            "              such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or",
            "              `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the",
            "              strings 'accuracy' or 'acc', we convert this to one of",
            "              `tf.keras.metrics.BinaryAccuracy`,",
            "              `tf.keras.metrics.CategoricalAccuracy`,",
            "              `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss",
            "              function used and the model output shape. We do a similar",
            "              conversion for the strings 'crossentropy' and 'ce' as well.",
            "        loss_weights: Optional list or dictionary specifying scalar coefficients",
            "          (Python floats) to weight the loss contributions of different model",
            "          outputs. The loss value that will be minimized by the model will then",
            "          be the *weighted sum* of all individual losses, weighted by the",
            "          `loss_weights` coefficients.",
            "            If a list, it is expected to have a 1:1 mapping to the model's",
            "              outputs. If a dict, it is expected to map output names (strings)",
            "              to scalar coefficients.",
            "        weighted_metrics: List of metrics to be evaluated and weighted by",
            "          sample_weight or class_weight during training and testing.",
            "        run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s",
            "          logic will not be wrapped in a `tf.function`. Recommended to leave",
            "          this as `None` unless your `Model` cannot be run inside a",
            "          `tf.function`. `run_eagerly=True` is not supported when using",
            "          `tf.distribute.experimental.ParameterServerStrategy`.",
            "        steps_per_execution: Int. Defaults to 1. The number of batches to",
            "          run during each `tf.function` call. Running multiple batches",
            "          inside a single `tf.function` call can greatly improve performance",
            "          on TPUs or small models with a large Python overhead.",
            "          At most, one full epoch will be run each",
            "          execution. If a number larger than the size of the epoch is passed,",
            "          the execution will be truncated to the size of the epoch.",
            "          Note that if `steps_per_execution` is set to `N`,",
            "          `Callback.on_batch_begin` and `Callback.on_batch_end` methods",
            "          will only be called every `N` batches",
            "          (i.e. before/after each `tf.function` execution).",
            "        **kwargs: Arguments supported for backwards compatibility only.",
            "",
            "    Raises:",
            "        ValueError: In case of invalid arguments for",
            "            `optimizer`, `loss` or `metrics`.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('compile').set(True)",
            "    with self.distribute_strategy.scope():",
            "      if 'experimental_steps_per_execution' in kwargs:",
            "        logging.warn('The argument `steps_per_execution` is no longer '",
            "                     'experimental. Pass `steps_per_execution` instead of '",
            "                     '`experimental_steps_per_execution`.')",
            "        if not steps_per_execution:",
            "          steps_per_execution = kwargs.pop('experimental_steps_per_execution')",
            "",
            "      # When compiling from an already-serialized model, we do not want to",
            "      # reapply some processing steps (e.g. metric renaming for multi-output",
            "      # models, which have prefixes added for each corresponding output name).",
            "      from_serialized = kwargs.pop('from_serialized', False)",
            "",
            "      self._validate_compile(optimizer, metrics, **kwargs)",
            "      self._run_eagerly = run_eagerly",
            "",
            "      self.optimizer = self._get_optimizer(optimizer)",
            "      self.compiled_loss = compile_utils.LossesContainer(",
            "          loss, loss_weights, output_names=self.output_names)",
            "      self.compiled_metrics = compile_utils.MetricsContainer(",
            "          metrics, weighted_metrics, output_names=self.output_names,",
            "          from_serialized=from_serialized)",
            "",
            "      self._configure_steps_per_execution(steps_per_execution or 1)",
            "",
            "      # Initializes attrs that are reset each time `compile` is called.",
            "      self._reset_compile_cache()",
            "      self._is_compiled = True",
            "",
            "      self.loss = loss or {}  # Backwards compat.",
            "",
            "  def _get_optimizer(self, optimizer):",
            "    \"\"\"Wraps `optimizer` in `LossScaleOptimizer` if necessary.\"\"\"",
            "    # The deprecated PolicyV1 has a loss_scale, which we use for backwards",
            "    # compatibility to match TF 2.3 behavior. The new Policy does not have a",
            "    # loss_scale, so we use dynamic loss scaling if the mixed_float16 policy is",
            "    # used.",
            "    if isinstance(self._dtype_policy, policy.PolicyV1):",
            "      loss_scale = self._dtype_policy.loss_scale",
            "    elif self._dtype_policy.name == 'mixed_float16':",
            "      loss_scale = 'dynamic'",
            "    else:",
            "      loss_scale = None",
            "",
            "    def _get_single_optimizer(opt):",
            "      opt = optimizers.get(opt)",
            "      if (loss_scale is not None and",
            "          not isinstance(opt, lso.LossScaleOptimizer)):",
            "        if loss_scale == 'dynamic':",
            "          opt = lso.LossScaleOptimizer(opt)",
            "        else:",
            "          opt = lso.LossScaleOptimizerV1(opt, loss_scale)",
            "      return opt",
            "",
            "    return nest.map_structure(_get_single_optimizer, optimizer)",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _reset_compile_cache(self):",
            "    self.train_function = None",
            "    self.test_function = None",
            "    self.predict_function = None",
            "",
            "    # Used to cache `trainable` attr of `Layer`s for `fit`.",
            "    self._compiled_trainable_state = self._get_trainable_state()",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _configure_steps_per_execution(self, steps_per_execution):",
            "    self._steps_per_execution = variables.Variable(",
            "        steps_per_execution,",
            "        dtype='int64',",
            "        aggregation=variables.VariableAggregationV2.ONLY_FIRST_REPLICA)",
            "",
            "  @property",
            "  def _should_compute_mask(self):",
            "    return False",
            "",
            "  @property",
            "  def metrics(self):",
            "    \"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.",
            "",
            "    Note: Metrics passed to `compile()` are available only after a `keras.Model`",
            "    has been trained/evaluated on actual data.",
            "",
            "    Examples:",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> outputs = tf.keras.layers.Dense(2)(inputs)",
            "    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])",
            "    >>> [m.name for m in model.metrics]",
            "    []",
            "",
            "    >>> x = np.random.random((2, 3))",
            "    >>> y = np.random.randint(0, 2, (2, 2))",
            "    >>> model.fit(x, y)",
            "    >>> [m.name for m in model.metrics]",
            "    ['loss', 'mae']",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> d = tf.keras.layers.Dense(2, name='out')",
            "    >>> output_1 = d(inputs)",
            "    >>> output_2 = d(inputs)",
            "    >>> model = tf.keras.models.Model(",
            "    ...    inputs=inputs, outputs=[output_1, output_2])",
            "    >>> model.add_metric(",
            "    ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])",
            "    >>> model.fit(x, (y, y))",
            "    >>> [m.name for m in model.metrics]",
            "    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',",
            "    'out_1_acc', 'mean']",
            "",
            "    \"\"\"",
            "    metrics = []",
            "    if self._is_compiled:",
            "      # TODO(omalleyt): Track `LossesContainer` and `MetricsContainer` objects",
            "      # so that attr names are not load-bearing.",
            "      if self.compiled_loss is not None:",
            "        metrics += self.compiled_loss.metrics",
            "      if self.compiled_metrics is not None:",
            "        metrics += self.compiled_metrics.metrics",
            "",
            "    for l in self._flatten_layers():",
            "      metrics.extend(l._metrics)  # pylint: disable=protected-access",
            "    return metrics",
            "",
            "  @property",
            "  def metrics_names(self):",
            "    \"\"\"Returns the model's display labels for all outputs.",
            "",
            "    Note: `metrics_names` are available only after a `keras.Model` has been",
            "    trained/evaluated on actual data.",
            "",
            "    Examples:",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> outputs = tf.keras.layers.Dense(2)(inputs)",
            "    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])",
            "    >>> model.metrics_names",
            "    []",
            "",
            "    >>> x = np.random.random((2, 3))",
            "    >>> y = np.random.randint(0, 2, (2, 2))",
            "    >>> model.fit(x, y)",
            "    >>> model.metrics_names",
            "    ['loss', 'mae']",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> d = tf.keras.layers.Dense(2, name='out')",
            "    >>> output_1 = d(inputs)",
            "    >>> output_2 = d(inputs)",
            "    >>> model = tf.keras.models.Model(",
            "    ...    inputs=inputs, outputs=[output_1, output_2])",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])",
            "    >>> model.fit(x, (y, y))",
            "    >>> model.metrics_names",
            "    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',",
            "    'out_1_acc']",
            "",
            "    \"\"\"",
            "",
            "    # This property includes all output names including `loss` and per-output",
            "    # losses for backward compatibility.",
            "    return [m.name for m in self.metrics]",
            "",
            "  @property",
            "  def distribute_strategy(self):",
            "    \"\"\"The `tf.distribute.Strategy` this model was created under.\"\"\"",
            "    return self._distribution_strategy or ds_context.get_strategy()",
            "",
            "  @property",
            "  def run_eagerly(self):",
            "    \"\"\"Settable attribute indicating whether the model should run eagerly.",
            "",
            "    Running eagerly means that your model will be run step by step,",
            "    like Python code. Your model might run slower, but it should become easier",
            "    for you to debug it by stepping into individual layer calls.",
            "",
            "    By default, we will attempt to compile your model to a static graph to",
            "    deliver the best execution performance.",
            "",
            "    Returns:",
            "      Boolean, whether the model should run eagerly.",
            "    \"\"\"",
            "    if self.dynamic and self._run_eagerly is False:  # pylint:disable=g-bool-id-comparison",
            "      # TODO(fchollet): consider using py_func to enable this.",
            "      raise ValueError('Your model contains layers that can only be '",
            "                       'successfully run in eager execution (layers '",
            "                       'constructed with `dynamic=True`). '",
            "                       'You cannot set `run_eagerly=False`.')",
            "",
            "    if self._cluster_coordinator and self._run_eagerly:",
            "      raise ValueError('When using `Model` with `ParameterServerStrategy`, '",
            "                       '`run_eagerly` is not supported.')",
            "",
            "    # Run eagerly logic, by priority:",
            "    # (1) Dynamic models must be run eagerly.",
            "    # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.",
            "    # (3) Not explicitly setting run_eagerly defaults to TF's global setting.",
            "    return (self.dynamic or self._run_eagerly or",
            "            (def_function.functions_run_eagerly() and",
            "             self._run_eagerly is None))",
            "",
            "  @run_eagerly.setter",
            "  def run_eagerly(self, value):",
            "    self._run_eagerly = value",
            "",
            "  def train_step(self, data):",
            "    \"\"\"The logic for one training step.",
            "",
            "    This method can be overridden to support custom training logic.",
            "    This method is called by `Model.make_train_function`.",
            "",
            "    This method should contain the mathematical logic for one step of training.",
            "    This typically includes the forward pass, loss calculation, backpropagation,",
            "    and metric updates.",
            "",
            "    Configuration details for *how* this logic is run (e.g. `tf.function` and",
            "    `tf.distribute.Strategy` settings), should be left to",
            "    `Model.make_train_function`, which can also be overridden.",
            "",
            "    Args:",
            "      data: A nested structure of `Tensor`s.",
            "",
            "    Returns:",
            "      A `dict` containing values that will be passed to",
            "      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the",
            "      values of the `Model`'s metrics are returned. Example:",
            "      `{'loss': 0.2, 'accuracy': 0.7}`.",
            "",
            "    \"\"\"",
            "    # These are the only transformations `Model.fit` applies to user-input",
            "    # data when a `tf.data.Dataset` is provided.",
            "    data = data_adapter.expand_1d(data)",
            "    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)",
            "    # Run forward pass.",
            "    with backprop.GradientTape() as tape:",
            "      y_pred = self(x, training=True)",
            "      loss = self.compiled_loss(",
            "          y, y_pred, sample_weight, regularization_losses=self.losses)",
            "    # Run backwards pass.",
            "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)",
            "    self.compiled_metrics.update_state(y, y_pred, sample_weight)",
            "    # Collect metrics to return",
            "    return_metrics = {}",
            "    for metric in self.metrics:",
            "      result = metric.result()",
            "      if isinstance(result, dict):",
            "        return_metrics.update(result)",
            "      else:",
            "        return_metrics[metric.name] = result",
            "    return return_metrics",
            "",
            "  def make_train_function(self):",
            "    \"\"\"Creates a function that executes one step of training.",
            "",
            "    This method can be overridden to support custom training logic.",
            "    This method is called by `Model.fit` and `Model.train_on_batch`.",
            "",
            "    Typically, this method directly controls `tf.function` and",
            "    `tf.distribute.Strategy` settings, and delegates the actual training",
            "    logic to `Model.train_step`.",
            "",
            "    This function is cached the first time `Model.fit` or",
            "    `Model.train_on_batch` is called. The cache is cleared whenever",
            "    `Model.compile` is called.",
            "",
            "    Returns:",
            "      Function. The function created by this method should accept a",
            "      `tf.data.Iterator`, and return a `dict` containing values that will",
            "      be passed to `tf.keras.Callbacks.on_train_batch_end`, such as",
            "      `{'loss': 0.2, 'accuracy': 0.7}`.",
            "    \"\"\"",
            "    if self.train_function is not None:",
            "      return self.train_function",
            "",
            "    def step_function(model, iterator):",
            "      \"\"\"Runs a single training step.\"\"\"",
            "",
            "      def run_step(data):",
            "        outputs = model.train_step(data)",
            "        # Ensure counter is updated only if `train_step` succeeds.",
            "        with ops.control_dependencies(_minimum_control_deps(outputs)):",
            "          model._train_counter.assign_add(1)  # pylint: disable=protected-access",
            "        return outputs",
            "",
            "      data = next(iterator)",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))",
            "      outputs = reduce_per_replica(",
            "          outputs, self.distribute_strategy, reduction='first')",
            "      write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access",
            "      return outputs",
            "",
            "    if self._steps_per_execution.numpy().item() == 1:",
            "",
            "      def train_function(iterator):",
            "        \"\"\"Runs a training execution with one step.\"\"\"",
            "        return step_function(self, iterator)",
            "",
            "    else:",
            "",
            "      def train_function(iterator):",
            "        \"\"\"Runs a training execution with multiple steps.\"\"\"",
            "        for _ in math_ops.range(self._steps_per_execution):",
            "          outputs = step_function(self, iterator)",
            "        return outputs",
            "",
            "    if not self.run_eagerly:",
            "      train_function = def_function.function(",
            "          train_function, experimental_relax_shapes=True)",
            "",
            "    self.train_function = train_function",
            "",
            "    if self._cluster_coordinator:",
            "      self.train_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda",
            "          train_function, args=(iterator,))",
            "",
            "    return self.train_function",
            "",
            "  def fit(self,",
            "          x=None,",
            "          y=None,",
            "          batch_size=None,",
            "          epochs=1,",
            "          verbose='auto',",
            "          callbacks=None,",
            "          validation_split=0.,",
            "          validation_data=None,",
            "          shuffle=True,",
            "          class_weight=None,",
            "          sample_weight=None,",
            "          initial_epoch=0,",
            "          steps_per_epoch=None,",
            "          validation_steps=None,",
            "          validation_batch_size=None,",
            "          validation_freq=1,",
            "          max_queue_size=10,",
            "          workers=1,",
            "          use_multiprocessing=False):",
            "    \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "            (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "            (in case the model has multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors,",
            "            if the model has named inputs.",
            "          - A `tf.data` dataset. Should return a tuple",
            "            of either `(inputs, targets)` or",
            "            `(inputs, targets, sample_weights)`.",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`",
            "            or `(inputs, targets, sample_weights)`.",
            "          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a",
            "            callable that takes a single argument of type",
            "            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.",
            "            `DatasetCreator` should be used when users prefer to specify the",
            "            per-replica batching and sharding logic for the `Dataset`.",
            "            See `tf.keras.utils.experimental.DatasetCreator` doc for more",
            "            information.",
            "          A more detailed description of unpacking behavior for iterator types",
            "          (Dataset, generator, Sequence) is given below. If using",
            "          `tf.distribute.experimental.ParameterServerStrategy`, only",
            "          `DatasetCreator` type is supported for `x`.",
            "        y: Target data. Like the input data `x`,",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).",
            "          It should be consistent with `x` (you cannot have Numpy inputs and",
            "          tensor targets, or inversely). If `x` is a dataset, generator,",
            "          or `keras.utils.Sequence` instance, `y` should",
            "          not be specified (since targets will be obtained from `x`).",
            "        batch_size: Integer or `None`.",
            "            Number of samples per gradient update.",
            "            If unspecified, `batch_size` will default to 32.",
            "            Do not specify the `batch_size` if your data is in the",
            "            form of datasets, generators, or `keras.utils.Sequence` instances",
            "            (since they generate batches).",
            "        epochs: Integer. Number of epochs to train the model.",
            "            An epoch is an iteration over the entire `x` and `y`",
            "            data provided.",
            "            Note that in conjunction with `initial_epoch`,",
            "            `epochs` is to be understood as \"final epoch\".",
            "            The model is not trained for a number of iterations",
            "            given by `epochs`, but merely until the epoch",
            "            of index `epochs` is reached.",
            "        verbose: 'auto', 0, 1, or 2. Verbosity mode.",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.",
            "            'auto' defaults to 1 for most cases, but 2 when used with",
            "            `ParameterServerStrategy`. Note that the progress bar is not",
            "            particularly useful when logged to a file, so verbose=2 is",
            "            recommended when not running interactively (eg, in a production",
            "            environment).",
            "        callbacks: List of `keras.callbacks.Callback` instances.",
            "            List of callbacks to apply during training.",
            "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`",
            "            and `tf.keras.callbacks.History` callbacks are created automatically",
            "            and need not be passed into `model.fit`.",
            "            `tf.keras.callbacks.ProgbarLogger` is created or not based on",
            "            `verbose` argument to `model.fit`.",
            "            Callbacks with batch-level calls are currently unsupported with",
            "            `tf.distribute.experimental.ParameterServerStrategy`, and users are",
            "            advised to implement epoch-level calls instead with an appropriate",
            "            `steps_per_epoch` value.",
            "        validation_split: Float between 0 and 1.",
            "            Fraction of the training data to be used as validation data.",
            "            The model will set apart this fraction of the training data,",
            "            will not train on it, and will evaluate",
            "            the loss and any model metrics",
            "            on this data at the end of each epoch.",
            "            The validation data is selected from the last samples",
            "            in the `x` and `y` data provided, before shuffling. This argument is",
            "            not supported when `x` is a dataset, generator or",
            "           `keras.utils.Sequence` instance.",
            "            `validation_split` is not yet supported with",
            "            `tf.distribute.experimental.ParameterServerStrategy`.",
            "        validation_data: Data on which to evaluate",
            "            the loss and any model metrics at the end of each epoch.",
            "            The model will not be trained on this data. Thus, note the fact",
            "            that the validation loss of data provided using `validation_split`",
            "            or `validation_data` is not affected by regularization layers like",
            "            noise and dropout.",
            "            `validation_data` will override `validation_split`.",
            "            `validation_data` could be:",
            "              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.",
            "              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.",
            "              - A `tf.data.Dataset`.",
            "              - A Python generator or `keras.utils.Sequence` returning",
            "              `(inputs, targets)` or `(inputs, targets, sample_weights)`.",
            "            `validation_data` is not yet supported with",
            "            `tf.distribute.experimental.ParameterServerStrategy`.",
            "        shuffle: Boolean (whether to shuffle the training data",
            "            before each epoch) or str (for 'batch'). This argument is ignored",
            "            when `x` is a generator or an object of tf.data.Dataset.",
            "            'batch' is a special option for dealing",
            "            with the limitations of HDF5 data; it shuffles in batch-sized",
            "            chunks. Has no effect when `steps_per_epoch` is not `None`.",
            "        class_weight: Optional dictionary mapping class indices (integers)",
            "            to a weight (float) value, used for weighting the loss function",
            "            (during training only).",
            "            This can be useful to tell the model to",
            "            \"pay more attention\" to samples from",
            "            an under-represented class.",
            "        sample_weight: Optional Numpy array of weights for",
            "            the training samples, used for weighting the loss function",
            "            (during training only). You can either pass a flat (1D)",
            "            Numpy array with the same length as the input samples",
            "            (1:1 mapping between weights and samples),",
            "            or in the case of temporal data,",
            "            you can pass a 2D array with shape",
            "            `(samples, sequence_length)`,",
            "            to apply a different weight to every timestep of every sample. This",
            "            argument is not supported when `x` is a dataset, generator, or",
            "           `keras.utils.Sequence` instance, instead provide the sample_weights",
            "            as the third element of `x`.",
            "        initial_epoch: Integer.",
            "            Epoch at which to start training",
            "            (useful for resuming a previous training run).",
            "        steps_per_epoch: Integer or `None`.",
            "            Total number of steps (batches of samples)",
            "            before declaring one epoch finished and starting the",
            "            next epoch. When training with input tensors such as",
            "            TensorFlow data tensors, the default `None` is equal to",
            "            the number of samples in your dataset divided by",
            "            the batch size, or 1 if that cannot be determined. If x is a",
            "            `tf.data` dataset, and 'steps_per_epoch'",
            "            is None, the epoch will run until the input dataset is exhausted.",
            "            When passing an infinitely repeating dataset, you must specify the",
            "            `steps_per_epoch` argument. This argument is not supported with",
            "            array inputs. `steps_per_epoch=None` is not supported when using",
            "            `tf.distribute.experimental.ParameterServerStrategy`.",
            "        validation_steps: Only relevant if `validation_data` is provided and",
            "            is a `tf.data` dataset. Total number of steps (batches of",
            "            samples) to draw before stopping when performing validation",
            "            at the end of every epoch. If 'validation_steps' is None, validation",
            "            will run until the `validation_data` dataset is exhausted. In the",
            "            case of an infinitely repeated dataset, it will run into an",
            "            infinite loop. If 'validation_steps' is specified and only part of",
            "            the dataset will be consumed, the evaluation will start from the",
            "            beginning of the dataset at each epoch. This ensures that the same",
            "            validation samples are used every time.",
            "        validation_batch_size: Integer or `None`.",
            "            Number of samples per validation batch.",
            "            If unspecified, will default to `batch_size`.",
            "            Do not specify the `validation_batch_size` if your data is in the",
            "            form of datasets, generators, or `keras.utils.Sequence` instances",
            "            (since they generate batches).",
            "        validation_freq: Only relevant if validation data is provided. Integer",
            "            or `collections.abc.Container` instance (e.g. list, tuple, etc.).",
            "            If an integer, specifies how many training epochs to run before a",
            "            new validation run is performed, e.g. `validation_freq=2` runs",
            "            validation every 2 epochs. If a Container, specifies the epochs on",
            "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs",
            "            validation at the end of the 1st, 2nd, and 10th epochs.",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`",
            "            input only. Maximum size for the generator queue.",
            "            If unspecified, `max_queue_size` will default to 10.",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input",
            "            only. Maximum number of processes to spin up",
            "            when using process-based threading. If unspecified, `workers`",
            "            will default to 1.",
            "        use_multiprocessing: Boolean. Used for generator or",
            "            `keras.utils.Sequence` input only. If `True`, use process-based",
            "            threading. If unspecified, `use_multiprocessing` will default to",
            "            `False`. Note that because this implementation relies on",
            "            multiprocessing, you should not pass non-picklable arguments to",
            "            the generator as they can't be passed easily to children processes.",
            "",
            "    Unpacking behavior for iterator-like inputs:",
            "        A common pattern is to pass a tf.data.Dataset, generator, or",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact",
            "      yield not only features (x) but optionally targets (y) and sample weights.",
            "      Keras requires that the output of such iterator-likes be unambiguous. The",
            "      iterator should return a tuple of length 1, 2, or 3, where the optional",
            "      second and third elements will be used for y and sample_weight",
            "      respectively. Any other type provided will be wrapped in a length one",
            "      tuple, effectively treating everything as 'x'. When yielding dicts, they",
            "      should still adhere to the top-level tuple structure.",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate",
            "      features, targets, and weights from the keys of a single dict.",
            "        A notable unsupported data type is the namedtuple. The reason is that",
            "      it behaves like both an ordered datatype (tuple) and a mapping",
            "      datatype (dict). So given a namedtuple of the form:",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`",
            "      it is ambiguous whether to reverse the order of the elements when",
            "      interpreting the value. Even worse is a tuple of the form:",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`",
            "      where it is unclear if the tuple was intended to be unpacked into x, y,",
            "      and sample_weight or passed through as a single element to `x`. As a",
            "      result the data processing code will simply raise a ValueError if it",
            "      encounters a namedtuple. (Along with instructions to remedy the issue.)",
            "",
            "    Returns:",
            "        A `History` object. Its `History.history` attribute is",
            "        a record of training loss values and metrics values",
            "        at successive epochs, as well as validation loss values",
            "        and validation metrics values (if applicable).",
            "",
            "    Raises:",
            "        RuntimeError: 1. If the model was never compiled or,",
            "        2. If `model.fit` is  wrapped in `tf.function`.",
            "",
            "        ValueError: In case of mismatch between the provided input data",
            "            and what the model expects or when the input data is empty.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('fit').set(True)",
            "    # Legacy graph support is contained in `training_v1.Model`.",
            "    version_utils.disallow_legacy_graph('Model', 'fit')",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('fit')",
            "    _disallow_inside_tf_function('fit')",
            "",
            "    if verbose == 'auto':",
            "      if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "        verbose = 2  # Default to epoch-level logging for PSStrategy.",
            "      else:",
            "        verbose = 1  # Default to batch-level logging otherwise.",
            "",
            "    if validation_split:",
            "      # Create the validation data using the training data. Only supported for",
            "      # `Tensor` and `NumPy` input.",
            "      (x, y, sample_weight), validation_data = (",
            "          data_adapter.train_validation_split(",
            "              (x, y, sample_weight), validation_split=validation_split))",
            "",
            "    if validation_data:",
            "      val_x, val_y, val_sample_weight = (",
            "          data_adapter.unpack_x_y_sample_weight(validation_data))",
            "",
            "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "      self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(",
            "          self.distribute_strategy)",
            "",
            "    with self.distribute_strategy.scope(), \\",
            "         training_utils.RespectCompiledTrainableState(self):",
            "      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.",
            "      data_handler = data_adapter.get_data_handler(",
            "          x=x,",
            "          y=y,",
            "          sample_weight=sample_weight,",
            "          batch_size=batch_size,",
            "          steps_per_epoch=steps_per_epoch,",
            "          initial_epoch=initial_epoch,",
            "          epochs=epochs,",
            "          shuffle=shuffle,",
            "          class_weight=class_weight,",
            "          max_queue_size=max_queue_size,",
            "          workers=workers,",
            "          use_multiprocessing=use_multiprocessing,",
            "          model=self,",
            "          steps_per_execution=self._steps_per_execution)",
            "",
            "      # Container that configures and calls `tf.keras.Callback`s.",
            "      if not isinstance(callbacks, callbacks_module.CallbackList):",
            "        callbacks = callbacks_module.CallbackList(",
            "            callbacks,",
            "            add_history=True,",
            "            add_progbar=verbose != 0,",
            "            model=self,",
            "            verbose=verbose,",
            "            epochs=epochs,",
            "            steps=data_handler.inferred_steps)",
            "",
            "      self.stop_training = False",
            "      self.train_function = self.make_train_function()",
            "      self._train_counter.assign(0)",
            "      callbacks.on_train_begin()",
            "      training_logs = None",
            "      # Handle fault-tolerance for multi-worker.",
            "      # TODO(omalleyt): Fix the ordering issues that mean this has to",
            "      # happen after `callbacks.on_train_begin`.",
            "      data_handler._initial_epoch = (  # pylint: disable=protected-access",
            "          self._maybe_load_initial_epoch_from_ckpt(initial_epoch))",
            "      logs = None",
            "      for epoch, iterator in data_handler.enumerate_epochs():",
            "        self.reset_metrics()",
            "        callbacks.on_epoch_begin(epoch)",
            "        with data_handler.catch_stop_iteration():",
            "          for step in data_handler.steps():",
            "            with trace.Trace(",
            "                'train',",
            "                epoch_num=epoch,",
            "                step_num=step,",
            "                batch_size=batch_size,",
            "                _r=1):",
            "              callbacks.on_train_batch_begin(step)",
            "              tmp_logs = self.train_function(iterator)",
            "              if data_handler.should_sync:",
            "                context.async_wait()",
            "              logs = tmp_logs  # No error, now safe to assign to logs.",
            "              end_step = step + data_handler.step_increment",
            "              callbacks.on_train_batch_end(end_step, logs)",
            "              if self.stop_training:",
            "                break",
            "",
            "        logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "        if logs is None:",
            "          raise ValueError('Expect x to be a non-empty array or dataset.')",
            "        epoch_logs = copy.copy(logs)",
            "",
            "        # Run validation.",
            "        if validation_data and self._should_eval(epoch, validation_freq):",
            "          # Create data_handler for evaluation and cache it.",
            "          if getattr(self, '_eval_data_handler', None) is None:",
            "            self._eval_data_handler = data_adapter.get_data_handler(",
            "                x=val_x,",
            "                y=val_y,",
            "                sample_weight=val_sample_weight,",
            "                batch_size=validation_batch_size or batch_size,",
            "                steps_per_epoch=validation_steps,",
            "                initial_epoch=0,",
            "                epochs=1,",
            "                max_queue_size=max_queue_size,",
            "                workers=workers,",
            "                use_multiprocessing=use_multiprocessing,",
            "                model=self,",
            "                steps_per_execution=self._steps_per_execution)",
            "          val_logs = self.evaluate(",
            "              x=val_x,",
            "              y=val_y,",
            "              sample_weight=val_sample_weight,",
            "              batch_size=validation_batch_size or batch_size,",
            "              steps=validation_steps,",
            "              callbacks=callbacks,",
            "              max_queue_size=max_queue_size,",
            "              workers=workers,",
            "              use_multiprocessing=use_multiprocessing,",
            "              return_dict=True,",
            "              _use_cached_eval_dataset=True)",
            "          val_logs = {'val_' + name: val for name, val in val_logs.items()}",
            "          epoch_logs.update(val_logs)",
            "",
            "        callbacks.on_epoch_end(epoch, epoch_logs)",
            "        training_logs = epoch_logs",
            "        if self.stop_training:",
            "          break",
            "",
            "      # If eval data_hanlder exists, delete it after all epochs are done.",
            "      if getattr(self, '_eval_data_handler', None) is not None:",
            "        del self._eval_data_handler",
            "      callbacks.on_train_end(logs=training_logs)",
            "      return self.history",
            "",
            "  def test_step(self, data):",
            "    \"\"\"The logic for one evaluation step.",
            "",
            "    This method can be overridden to support custom evaluation logic.",
            "    This method is called by `Model.make_test_function`.",
            "",
            "    This function should contain the mathematical logic for one step of",
            "    evaluation.",
            "    This typically includes the forward pass, loss calculation, and metrics",
            "    updates.",
            "",
            "    Configuration details for *how* this logic is run (e.g. `tf.function` and",
            "    `tf.distribute.Strategy` settings), should be left to",
            "    `Model.make_test_function`, which can also be overridden.",
            "",
            "    Args:",
            "      data: A nested structure of `Tensor`s.",
            "",
            "    Returns:",
            "      A `dict` containing values that will be passed to",
            "      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the",
            "      values of the `Model`'s metrics are returned.",
            "    \"\"\"",
            "    data = data_adapter.expand_1d(data)",
            "    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)",
            "",
            "    y_pred = self(x, training=False)",
            "    # Updates stateful loss metrics.",
            "    self.compiled_loss(",
            "        y, y_pred, sample_weight, regularization_losses=self.losses)",
            "    self.compiled_metrics.update_state(y, y_pred, sample_weight)",
            "    # Collect metrics to return",
            "    return_metrics = {}",
            "    for metric in self.metrics:",
            "      result = metric.result()",
            "      if isinstance(result, dict):",
            "        return_metrics.update(result)",
            "      else:",
            "        return_metrics[metric.name] = result",
            "    return return_metrics",
            "",
            "  def make_test_function(self):",
            "    \"\"\"Creates a function that executes one step of evaluation.",
            "",
            "    This method can be overridden to support custom evaluation logic.",
            "    This method is called by `Model.evaluate` and `Model.test_on_batch`.",
            "",
            "    Typically, this method directly controls `tf.function` and",
            "    `tf.distribute.Strategy` settings, and delegates the actual evaluation",
            "    logic to `Model.test_step`.",
            "",
            "    This function is cached the first time `Model.evaluate` or",
            "    `Model.test_on_batch` is called. The cache is cleared whenever",
            "    `Model.compile` is called.",
            "",
            "    Returns:",
            "      Function. The function created by this method should accept a",
            "      `tf.data.Iterator`, and return a `dict` containing values that will",
            "      be passed to `tf.keras.Callbacks.on_test_batch_end`.",
            "    \"\"\"",
            "    if self.test_function is not None:",
            "      return self.test_function",
            "",
            "    def step_function(model, iterator):",
            "      \"\"\"Runs a single evaluation step.\"\"\"",
            "",
            "      def run_step(data):",
            "        outputs = model.test_step(data)",
            "        # Ensure counter is updated only if `test_step` succeeds.",
            "        with ops.control_dependencies(_minimum_control_deps(outputs)):",
            "          model._test_counter.assign_add(1)  # pylint: disable=protected-access",
            "        return outputs",
            "",
            "      data = next(iterator)",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))",
            "      outputs = reduce_per_replica(",
            "          outputs, self.distribute_strategy, reduction='first')",
            "      return outputs",
            "",
            "    if self._steps_per_execution.numpy().item() == 1:",
            "",
            "      def test_function(iterator):",
            "        \"\"\"Runs an evaluation execution with one step.\"\"\"",
            "        return step_function(self, iterator)",
            "",
            "    else:",
            "",
            "      def test_function(iterator):",
            "        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"",
            "        for _ in math_ops.range(self._steps_per_execution):",
            "          outputs = step_function(self, iterator)",
            "        return outputs",
            "",
            "    if not self.run_eagerly:",
            "      test_function = def_function.function(",
            "          test_function, experimental_relax_shapes=True)",
            "",
            "    self.test_function = test_function",
            "    return self.test_function",
            "",
            "  def evaluate(self,",
            "               x=None,",
            "               y=None,",
            "               batch_size=None,",
            "               verbose=1,",
            "               sample_weight=None,",
            "               steps=None,",
            "               callbacks=None,",
            "               max_queue_size=10,",
            "               workers=1,",
            "               use_multiprocessing=False,",
            "               return_dict=False,",
            "               **kwargs):",
            "    \"\"\"Returns the loss value & metrics values for the model in test mode.",
            "",
            "    Computation is done in batches (see the `batch_size` arg.)",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "            (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "            (in case the model has multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors,",
            "            if the model has named inputs.",
            "          - A `tf.data` dataset. Should return a tuple",
            "            of either `(inputs, targets)` or",
            "            `(inputs, targets, sample_weights)`.",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`",
            "            or `(inputs, targets, sample_weights)`.",
            "          A more detailed description of unpacking behavior for iterator types",
            "          (Dataset, generator, Sequence) is given in the `Unpacking behavior",
            "          for iterator-like inputs` section of `Model.fit`.",
            "        y: Target data. Like the input data `x`, it could be either Numpy",
            "          array(s) or TensorFlow tensor(s). It should be consistent with `x`",
            "          (you cannot have Numpy inputs and tensor targets, or inversely). If",
            "          `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`",
            "          should not be specified (since targets will be obtained from the",
            "          iterator/dataset).",
            "        batch_size: Integer or `None`. Number of samples per batch of",
            "          computation. If unspecified, `batch_size` will default to 32. Do not",
            "          specify the `batch_size` if your data is in the form of a dataset,",
            "          generators, or `keras.utils.Sequence` instances (since they generate",
            "          batches).",
            "        verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.",
            "        sample_weight: Optional Numpy array of weights for the test samples,",
            "          used for weighting the loss function. You can either pass a flat (1D)",
            "          Numpy array with the same length as the input samples",
            "            (1:1 mapping between weights and samples), or in the case of",
            "              temporal data, you can pass a 2D array with shape `(samples,",
            "              sequence_length)`, to apply a different weight to every timestep",
            "              of every sample. This argument is not supported when `x` is a",
            "              dataset, instead pass sample weights as the third element of `x`.",
            "        steps: Integer or `None`. Total number of steps (batches of samples)",
            "          before declaring the evaluation round finished. Ignored with the",
            "          default value of `None`. If x is a `tf.data` dataset and `steps` is",
            "          None, 'evaluate' will run until the dataset is exhausted. This",
            "          argument is not supported with array inputs.",
            "        callbacks: List of `keras.callbacks.Callback` instances. List of",
            "          callbacks to apply during evaluation. See",
            "          [callbacks](/api_docs/python/tf/keras/callbacks).",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`",
            "          input only. Maximum size for the generator queue. If unspecified,",
            "          `max_queue_size` will default to 10.",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input",
            "          only. Maximum number of processes to spin up when using process-based",
            "          threading. If unspecified, `workers` will default to 1.",
            "        use_multiprocessing: Boolean. Used for generator or",
            "          `keras.utils.Sequence` input only. If `True`, use process-based",
            "          threading. If unspecified, `use_multiprocessing` will default to",
            "          `False`. Note that because this implementation relies on",
            "          multiprocessing, you should not pass non-picklable arguments to the",
            "          generator as they can't be passed easily to children processes.",
            "        return_dict: If `True`, loss and metric results are returned as a dict,",
            "          with each key being the name of the metric. If `False`, they are",
            "          returned as a list.",
            "        **kwargs: Unused at this time.",
            "",
            "    See the discussion of `Unpacking behavior for iterator-like inputs` for",
            "    `Model.fit`.",
            "",
            "    `Model.evaluate` is not yet supported with",
            "    `tf.distribute.experimental.ParameterServerStrategy`.",
            "",
            "    Returns:",
            "        Scalar test loss (if the model has a single output and no metrics)",
            "        or list of scalars (if the model has multiple outputs",
            "        and/or metrics). The attribute `model.metrics_names` will give you",
            "        the display labels for the scalar outputs.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.evaluate` is wrapped in `tf.function`.",
            "        ValueError: in case of invalid arguments.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('evaluate').set(True)",
            "    version_utils.disallow_legacy_graph('Model', 'evaluate')",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('evaluate')",
            "    _disallow_inside_tf_function('evaluate')",
            "    use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)",
            "    if kwargs:",
            "      raise TypeError('Invalid keyword arguments: %s' % (kwargs,))",
            "",
            "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "      raise NotImplementedError('`model.evaluate` is not yet supported with '",
            "                                '`ParameterServerStrategy`.')",
            "",
            "    with self.distribute_strategy.scope():",
            "      # Use cached evaluation data only when it's called in `Model.fit`",
            "      if (use_cached_eval_dataset",
            "          and getattr(self, '_eval_data_handler', None) is not None):",
            "        data_handler = self._eval_data_handler",
            "      else:",
            "        # Creates a `tf.data.Dataset` and handles batch and epoch iteration.",
            "        data_handler = data_adapter.get_data_handler(",
            "            x=x,",
            "            y=y,",
            "            sample_weight=sample_weight,",
            "            batch_size=batch_size,",
            "            steps_per_epoch=steps,",
            "            initial_epoch=0,",
            "            epochs=1,",
            "            max_queue_size=max_queue_size,",
            "            workers=workers,",
            "            use_multiprocessing=use_multiprocessing,",
            "            model=self,",
            "            steps_per_execution=self._steps_per_execution)",
            "",
            "      # Container that configures and calls `tf.keras.Callback`s.",
            "      if not isinstance(callbacks, callbacks_module.CallbackList):",
            "        callbacks = callbacks_module.CallbackList(",
            "            callbacks,",
            "            add_history=True,",
            "            add_progbar=verbose != 0,",
            "            model=self,",
            "            verbose=verbose,",
            "            epochs=1,",
            "            steps=data_handler.inferred_steps)",
            "",
            "      logs = {}",
            "      self.test_function = self.make_test_function()",
            "      self._test_counter.assign(0)",
            "      callbacks.on_test_begin()",
            "      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.",
            "        self.reset_metrics()",
            "        with data_handler.catch_stop_iteration():",
            "          for step in data_handler.steps():",
            "            with trace.Trace('test', step_num=step, _r=1):",
            "              callbacks.on_test_batch_begin(step)",
            "              tmp_logs = self.test_function(iterator)",
            "              if data_handler.should_sync:",
            "                context.async_wait()",
            "              logs = tmp_logs  # No error, now safe to assign to logs.",
            "              end_step = step + data_handler.step_increment",
            "              callbacks.on_test_batch_end(end_step, logs)",
            "      logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "      callbacks.on_test_end(logs=logs)",
            "",
            "      if return_dict:",
            "        return logs",
            "      else:",
            "        return flatten_metrics_in_order(logs, self.metrics_names)",
            "",
            "  def predict_step(self, data):",
            "    \"\"\"The logic for one inference step.",
            "",
            "    This method can be overridden to support custom inference logic.",
            "    This method is called by `Model.make_predict_function`.",
            "",
            "    This method should contain the mathematical logic for one step of inference.",
            "    This typically includes the forward pass.",
            "",
            "    Configuration details for *how* this logic is run (e.g. `tf.function` and",
            "    `tf.distribute.Strategy` settings), should be left to",
            "    `Model.make_predict_function`, which can also be overridden.",
            "",
            "    Args:",
            "      data: A nested structure of `Tensor`s.",
            "",
            "    Returns:",
            "      The result of one inference step, typically the output of calling the",
            "      `Model` on data.",
            "    \"\"\"",
            "    data = data_adapter.expand_1d(data)",
            "    x, _, _ = data_adapter.unpack_x_y_sample_weight(data)",
            "    return self(x, training=False)",
            "",
            "  def make_predict_function(self):",
            "    \"\"\"Creates a function that executes one step of inference.",
            "",
            "    This method can be overridden to support custom inference logic.",
            "    This method is called by `Model.predict` and `Model.predict_on_batch`.",
            "",
            "    Typically, this method directly controls `tf.function` and",
            "    `tf.distribute.Strategy` settings, and delegates the actual evaluation",
            "    logic to `Model.predict_step`.",
            "",
            "    This function is cached the first time `Model.predict` or",
            "    `Model.predict_on_batch` is called. The cache is cleared whenever",
            "    `Model.compile` is called.",
            "",
            "    Returns:",
            "      Function. The function created by this method should accept a",
            "      `tf.data.Iterator`, and return the outputs of the `Model`.",
            "    \"\"\"",
            "    if self.predict_function is not None:",
            "      return self.predict_function",
            "",
            "    def step_function(model, iterator):",
            "      \"\"\"Runs a single evaluation step.\"\"\"",
            "",
            "      def run_step(data):",
            "        outputs = model.predict_step(data)",
            "        # Ensure counter is updated only if `test_step` succeeds.",
            "        with ops.control_dependencies(_minimum_control_deps(outputs)):",
            "          model._predict_counter.assign_add(1)  # pylint: disable=protected-access",
            "        return outputs",
            "",
            "      data = next(iterator)",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))",
            "      outputs = reduce_per_replica(",
            "          outputs, self.distribute_strategy, reduction='concat')",
            "      return outputs",
            "",
            "    if (self._steps_per_execution is None or",
            "        self._steps_per_execution.numpy().item() == 1):",
            "",
            "      def predict_function(iterator):",
            "        \"\"\"Runs an evaluation execution with one step.\"\"\"",
            "        return step_function(self, iterator)",
            "",
            "    else:",
            "",
            "      def predict_function(iterator):",
            "        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"",
            "        outputs = step_function(self, iterator)",
            "        for _ in math_ops.range(self._steps_per_execution - 1):",
            "          directives.set_loop_options(",
            "              shape_invariants=[(",
            "                  t, tf_utils.get_tensor_spec(t, dynamic_batch=True).shape)",
            "                                for t in nest.flatten(outputs)])",
            "          step_outputs = step_function(self, iterator)",
            "          outputs = nest.map_structure(lambda t1, t2: concat([t1, t2]), outputs,",
            "                                       step_outputs)",
            "        return outputs",
            "",
            "    if not self.run_eagerly:",
            "      predict_function = def_function.function(",
            "          predict_function, experimental_relax_shapes=True)",
            "",
            "    self.predict_function = predict_function",
            "    return self.predict_function",
            "",
            "  def predict(self,",
            "              x,",
            "              batch_size=None,",
            "              verbose=0,",
            "              steps=None,",
            "              callbacks=None,",
            "              max_queue_size=10,",
            "              workers=1,",
            "              use_multiprocessing=False):",
            "    \"\"\"Generates output predictions for the input samples.",
            "",
            "    Computation is done in batches. This method is designed for performance in",
            "    large scale inputs. For small amount of inputs that fit in one batch,",
            "    directly using `__call__` is recommended for faster execution, e.g.,",
            "    `model(x)`, or `model(x, training=False)` if you have layers such as",
            "    `tf.keras.layers.BatchNormalization` that behaves differently during",
            "    inference. Also, note the fact that test loss is not affected by",
            "    regularization layers like noise and dropout.",
            "",
            "    Args:",
            "        x: Input samples. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "            (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "            (in case the model has multiple inputs).",
            "          - A `tf.data` dataset.",
            "          - A generator or `keras.utils.Sequence` instance.",
            "          A more detailed description of unpacking behavior for iterator types",
            "          (Dataset, generator, Sequence) is given in the `Unpacking behavior",
            "          for iterator-like inputs` section of `Model.fit`.",
            "        batch_size: Integer or `None`.",
            "            Number of samples per batch.",
            "            If unspecified, `batch_size` will default to 32.",
            "            Do not specify the `batch_size` if your data is in the",
            "            form of dataset, generators, or `keras.utils.Sequence` instances",
            "            (since they generate batches).",
            "        verbose: Verbosity mode, 0 or 1.",
            "        steps: Total number of steps (batches of samples)",
            "            before declaring the prediction round finished.",
            "            Ignored with the default value of `None`. If x is a `tf.data`",
            "            dataset and `steps` is None, `predict` will",
            "            run until the input dataset is exhausted.",
            "        callbacks: List of `keras.callbacks.Callback` instances.",
            "            List of callbacks to apply during prediction.",
            "            See [callbacks](/api_docs/python/tf/keras/callbacks).",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`",
            "            input only. Maximum size for the generator queue.",
            "            If unspecified, `max_queue_size` will default to 10.",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input",
            "            only. Maximum number of processes to spin up when using",
            "            process-based threading. If unspecified, `workers` will default",
            "            to 1.",
            "        use_multiprocessing: Boolean. Used for generator or",
            "            `keras.utils.Sequence` input only. If `True`, use process-based",
            "            threading. If unspecified, `use_multiprocessing` will default to",
            "            `False`. Note that because this implementation relies on",
            "            multiprocessing, you should not pass non-picklable arguments to",
            "            the generator as they can't be passed easily to children processes.",
            "",
            "    See the discussion of `Unpacking behavior for iterator-like inputs` for",
            "    `Model.fit`. Note that Model.predict uses the same interpretation rules as",
            "    `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all",
            "    three methods.",
            "",
            "    `Model.predict` is not yet supported with",
            "    `tf.distribute.experimental.ParameterServerStrategy`.",
            "",
            "    Returns:",
            "        Numpy array(s) of predictions.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.predict` is wrapped in `tf.function`.",
            "        ValueError: In case of mismatch between the provided",
            "            input data and the model's expectations,",
            "            or in case a stateful model receives a number of samples",
            "            that is not a multiple of the batch size.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('predict').set(True)",
            "    version_utils.disallow_legacy_graph('Model', 'predict')",
            "    self._check_call_args('predict')",
            "    _disallow_inside_tf_function('predict')",
            "",
            "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "      raise NotImplementedError('`model.predict` is not yet supported with '",
            "                                '`ParameterServerStrategy`.')",
            "",
            "    outputs = None",
            "    with self.distribute_strategy.scope():",
            "      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.",
            "      dataset_types = (dataset_ops.DatasetV1, dataset_ops.DatasetV2)",
            "      if (self._in_multi_worker_mode() or _is_tpu_multi_host(",
            "          self.distribute_strategy)) and isinstance(x, dataset_types):",
            "        try:",
            "          options = dataset_ops.Options()",
            "          data_option = distribute_options.AutoShardPolicy.DATA",
            "          options.experimental_distribute.auto_shard_policy = data_option",
            "          x = x.with_options(options)",
            "        except ValueError:",
            "          warnings.warn('Using Model.predict with '",
            "                        'MultiWorkerDistributionStrategy or TPUStrategy and '",
            "                        'AutoShardPolicy.FILE might lead to out-of-order result'",
            "                        '. Consider setting it to AutoShardPolicy.DATA.')",
            "",
            "      data_handler = data_adapter.get_data_handler(",
            "          x=x,",
            "          batch_size=batch_size,",
            "          steps_per_epoch=steps,",
            "          initial_epoch=0,",
            "          epochs=1,",
            "          max_queue_size=max_queue_size,",
            "          workers=workers,",
            "          use_multiprocessing=use_multiprocessing,",
            "          model=self,",
            "          steps_per_execution=self._steps_per_execution)",
            "",
            "      # Container that configures and calls `tf.keras.Callback`s.",
            "      if not isinstance(callbacks, callbacks_module.CallbackList):",
            "        callbacks = callbacks_module.CallbackList(",
            "            callbacks,",
            "            add_history=True,",
            "            add_progbar=verbose != 0,",
            "            model=self,",
            "            verbose=verbose,",
            "            epochs=1,",
            "            steps=data_handler.inferred_steps)",
            "",
            "      self.predict_function = self.make_predict_function()",
            "      self._predict_counter.assign(0)",
            "      callbacks.on_predict_begin()",
            "      batch_outputs = None",
            "      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.",
            "        with data_handler.catch_stop_iteration():",
            "          for step in data_handler.steps():",
            "            callbacks.on_predict_batch_begin(step)",
            "            tmp_batch_outputs = self.predict_function(iterator)",
            "            if data_handler.should_sync:",
            "              context.async_wait()",
            "            batch_outputs = tmp_batch_outputs  # No error, now safe to assign.",
            "            if outputs is None:",
            "              outputs = nest.map_structure(lambda batch_output: [batch_output],",
            "                                           batch_outputs)",
            "            else:",
            "              nest.map_structure_up_to(",
            "                  batch_outputs,",
            "                  lambda output, batch_output: output.append(batch_output),",
            "                  outputs, batch_outputs)",
            "            end_step = step + data_handler.step_increment",
            "            callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})",
            "      if batch_outputs is None:",
            "        raise ValueError('Expect x to be a non-empty array or dataset.')",
            "      callbacks.on_predict_end()",
            "    all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)",
            "    return tf_utils.sync_to_numpy_or_python_type(all_outputs)",
            "",
            "  def reset_metrics(self):",
            "    \"\"\"Resets the state of all the metrics in the model.",
            "",
            "    Examples:",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> outputs = tf.keras.layers.Dense(2)(inputs)",
            "    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])",
            "",
            "    >>> x = np.random.random((2, 3))",
            "    >>> y = np.random.randint(0, 2, (2, 2))",
            "    >>> _ = model.fit(x, y, verbose=0)",
            "    >>> assert all(float(m.result()) for m in model.metrics)",
            "",
            "    >>> model.reset_metrics()",
            "    >>> assert all(float(m.result()) == 0 for m in model.metrics)",
            "",
            "    \"\"\"",
            "    for m in self.metrics:",
            "      m.reset_state()",
            "",
            "  def train_on_batch(self,",
            "                     x,",
            "                     y=None,",
            "                     sample_weight=None,",
            "                     class_weight=None,",
            "                     reset_metrics=True,",
            "                     return_dict=False):",
            "    \"\"\"Runs a single gradient update on a single batch of data.",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "              (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "              (in case the model has multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors,",
            "              if the model has named inputs.",
            "        y: Target data. Like the input data `x`, it could be either Numpy",
            "          array(s) or TensorFlow tensor(s). It should be consistent with `x`",
            "          (you cannot have Numpy inputs and tensor targets, or inversely).",
            "        sample_weight: Optional array of the same length as x, containing",
            "          weights to apply to the model's loss for each sample. In the case of",
            "          temporal data, you can pass a 2D array with shape (samples,",
            "          sequence_length), to apply a different weight to every timestep of",
            "          every sample.",
            "        class_weight: Optional dictionary mapping class indices (integers) to a",
            "          weight (float) to apply to the model's loss for the samples from this",
            "          class during training. This can be useful to tell the model to \"pay",
            "          more attention\" to samples from an under-represented class.",
            "        reset_metrics: If `True`, the metrics returned will be only for this",
            "          batch. If `False`, the metrics will be statefully accumulated across",
            "          batches.",
            "        return_dict: If `True`, loss and metric results are returned as a dict,",
            "          with each key being the name of the metric. If `False`, they are",
            "          returned as a list.",
            "",
            "    Returns:",
            "        Scalar training loss",
            "        (if the model has a single output and no metrics)",
            "        or list of scalars (if the model has multiple outputs",
            "        and/or metrics). The attribute `model.metrics_names` will give you",
            "        the display labels for the scalar outputs.",
            "",
            "    Raises:",
            "      RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.",
            "      ValueError: In case of invalid user-provided arguments.",
            "    \"\"\"",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('train_on_batch')",
            "    _disallow_inside_tf_function('train_on_batch')",
            "    with self.distribute_strategy.scope(), \\",
            "         training_utils.RespectCompiledTrainableState(self):",
            "      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,",
            "                                                    y, sample_weight,",
            "                                                    class_weight)",
            "      self.train_function = self.make_train_function()",
            "      logs = self.train_function(iterator)",
            "",
            "    if reset_metrics:",
            "      self.reset_metrics()",
            "    logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "    if return_dict:",
            "      return logs",
            "    else:",
            "      return flatten_metrics_in_order(logs, self.metrics_names)",
            "",
            "  def test_on_batch(self,",
            "                    x,",
            "                    y=None,",
            "                    sample_weight=None,",
            "                    reset_metrics=True,",
            "                    return_dict=False):",
            "    \"\"\"Test the model on a single batch of samples.",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays (in case the",
            "              model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors (in case the model has",
            "              multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors, if",
            "              the model has named inputs.",
            "        y: Target data. Like the input data `x`, it could be either Numpy",
            "          array(s) or TensorFlow tensor(s). It should be consistent with `x`",
            "          (you cannot have Numpy inputs and tensor targets, or inversely).",
            "        sample_weight: Optional array of the same length as x, containing",
            "          weights to apply to the model's loss for each sample. In the case of",
            "          temporal data, you can pass a 2D array with shape (samples,",
            "          sequence_length), to apply a different weight to every timestep of",
            "          every sample.",
            "        reset_metrics: If `True`, the metrics returned will be only for this",
            "          batch. If `False`, the metrics will be statefully accumulated across",
            "          batches.",
            "        return_dict: If `True`, loss and metric results are returned as a dict,",
            "          with each key being the name of the metric. If `False`, they are",
            "          returned as a list.",
            "",
            "    Returns:",
            "        Scalar test loss (if the model has a single output and no metrics)",
            "        or list of scalars (if the model has multiple outputs",
            "        and/or metrics). The attribute `model.metrics_names` will give you",
            "        the display labels for the scalar outputs.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.",
            "        ValueError: In case of invalid user-provided arguments.",
            "    \"\"\"",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('test_on_batch')",
            "    _disallow_inside_tf_function('test_on_batch')",
            "    with self.distribute_strategy.scope():",
            "      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,",
            "                                                    y, sample_weight)",
            "      self.test_function = self.make_test_function()",
            "      logs = self.test_function(iterator)",
            "",
            "    if reset_metrics:",
            "      self.reset_metrics()",
            "    logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "    if return_dict:",
            "      return logs",
            "    else:",
            "      return flatten_metrics_in_order(logs, self.metrics_names)",
            "",
            "  def predict_on_batch(self, x):",
            "    \"\"\"Returns predictions for a single batch of samples.",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays (in case the",
            "              model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors (in case the model has",
            "              multiple inputs).",
            "",
            "    Returns:",
            "        Numpy array(s) of predictions.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.",
            "        ValueError: In case of mismatch between given number of inputs and",
            "          expectations of the model.",
            "    \"\"\"",
            "    self._check_call_args('predict_on_batch')",
            "    _disallow_inside_tf_function('predict_on_batch')",
            "    with self.distribute_strategy.scope():",
            "      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x)",
            "      self.predict_function = self.make_predict_function()",
            "      outputs = self.predict_function(iterator)",
            "    return tf_utils.sync_to_numpy_or_python_type(outputs)",
            "",
            "  def fit_generator(self,",
            "                    generator,",
            "                    steps_per_epoch=None,",
            "                    epochs=1,",
            "                    verbose=1,",
            "                    callbacks=None,",
            "                    validation_data=None,",
            "                    validation_steps=None,",
            "                    validation_freq=1,",
            "                    class_weight=None,",
            "                    max_queue_size=10,",
            "                    workers=1,",
            "                    use_multiprocessing=False,",
            "                    shuffle=True,",
            "                    initial_epoch=0):",
            "    \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.",
            "",
            "    DEPRECATED:",
            "      `Model.fit` now supports generators, so there is no longer any need to use",
            "      this endpoint.",
            "    \"\"\"",
            "    warnings.warn('`Model.fit_generator` is deprecated and '",
            "                  'will be removed in a future version. '",
            "                  'Please use `Model.fit`, which supports generators.')",
            "    return self.fit(",
            "        generator,",
            "        steps_per_epoch=steps_per_epoch,",
            "        epochs=epochs,",
            "        verbose=verbose,",
            "        callbacks=callbacks,",
            "        validation_data=validation_data,",
            "        validation_steps=validation_steps,",
            "        validation_freq=validation_freq,",
            "        class_weight=class_weight,",
            "        max_queue_size=max_queue_size,",
            "        workers=workers,",
            "        use_multiprocessing=use_multiprocessing,",
            "        shuffle=shuffle,",
            "        initial_epoch=initial_epoch)",
            "",
            "  def evaluate_generator(self,",
            "                         generator,",
            "                         steps=None,",
            "                         callbacks=None,",
            "                         max_queue_size=10,",
            "                         workers=1,",
            "                         use_multiprocessing=False,",
            "                         verbose=0):",
            "    \"\"\"Evaluates the model on a data generator.",
            "",
            "    DEPRECATED:",
            "      `Model.evaluate` now supports generators, so there is no longer any need",
            "      to use this endpoint.",
            "    \"\"\"",
            "    warnings.warn('`Model.evaluate_generator` is deprecated and '",
            "                  'will be removed in a future version. '",
            "                  'Please use `Model.evaluate`, which supports generators.')",
            "    self._check_call_args('evaluate_generator')",
            "",
            "    return self.evaluate(",
            "        generator,",
            "        steps=steps,",
            "        max_queue_size=max_queue_size,",
            "        workers=workers,",
            "        use_multiprocessing=use_multiprocessing,",
            "        verbose=verbose,",
            "        callbacks=callbacks)",
            "",
            "  def predict_generator(self,",
            "                        generator,",
            "                        steps=None,",
            "                        callbacks=None,",
            "                        max_queue_size=10,",
            "                        workers=1,",
            "                        use_multiprocessing=False,",
            "                        verbose=0):",
            "    \"\"\"Generates predictions for the input samples from a data generator.",
            "",
            "    DEPRECATED:",
            "      `Model.predict` now supports generators, so there is no longer any need",
            "      to use this endpoint.",
            "    \"\"\"",
            "    warnings.warn('`Model.predict_generator` is deprecated and '",
            "                  'will be removed in a future version. '",
            "                  'Please use `Model.predict`, which supports generators.')",
            "    return self.predict(",
            "        generator,",
            "        steps=steps,",
            "        max_queue_size=max_queue_size,",
            "        workers=workers,",
            "        use_multiprocessing=use_multiprocessing,",
            "        verbose=verbose,",
            "        callbacks=callbacks)",
            "",
            "  ######################################################################",
            "  # Functions below are not training related. They are for model weights",
            "  # tracking, save/load, serialization, etc.",
            "  ######################################################################",
            "",
            "  @property",
            "  def trainable_weights(self):",
            "    self._assert_weights_created()",
            "    if not self._trainable:",
            "      return []",
            "    trainable_variables = []",
            "    for trackable_obj in self._self_tracked_trackables:",
            "      trainable_variables += trackable_obj.trainable_variables",
            "    trainable_variables += self._trainable_weights",
            "    return self._dedup_weights(trainable_variables)",
            "",
            "  @property",
            "  def non_trainable_weights(self):",
            "    self._assert_weights_created()",
            "    non_trainable_variables = []",
            "    for trackable_obj in self._self_tracked_trackables:",
            "      non_trainable_variables += trackable_obj.non_trainable_variables",
            "",
            "    if not self._trainable:",
            "      # Return order is all trainable vars, then all non-trainable vars.",
            "      trainable_variables = []",
            "      for trackable_obj in self._self_tracked_trackables:",
            "        trainable_variables += trackable_obj.trainable_variables",
            "",
            "      non_trainable_variables = (",
            "          trainable_variables + self._trainable_weights +",
            "          non_trainable_variables + self._non_trainable_weights)",
            "    else:",
            "      non_trainable_variables = (",
            "          non_trainable_variables + self._non_trainable_weights)",
            "",
            "    return self._dedup_weights(non_trainable_variables)",
            "",
            "  def get_weights(self):",
            "    \"\"\"Retrieves the weights of the model.",
            "",
            "    Returns:",
            "        A flat list of Numpy arrays.",
            "    \"\"\"",
            "    with self.distribute_strategy.scope():",
            "      return super(Model, self).get_weights()",
            "",
            "  def save(self,",
            "           filepath,",
            "           overwrite=True,",
            "           include_optimizer=True,",
            "           save_format=None,",
            "           signatures=None,",
            "           options=None,",
            "           save_traces=True):",
            "    # pylint: disable=line-too-long",
            "    \"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.",
            "",
            "    Please see `tf.keras.models.save_model` or the",
            "    [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)",
            "    for details.",
            "",
            "    Args:",
            "        filepath: String, PathLike, path to SavedModel or H5 file to save the",
            "            model.",
            "        overwrite: Whether to silently overwrite any existing file at the",
            "            target location, or provide the user with a manual prompt.",
            "        include_optimizer: If True, save optimizer's state together.",
            "        save_format: Either `'tf'` or `'h5'`, indicating whether to save the",
            "            model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,",
            "            and 'h5' in TF 1.X.",
            "        signatures: Signatures to save with the SavedModel. Applicable to the",
            "            'tf' format only. Please see the `signatures` argument in",
            "            `tf.saved_model.save` for details.",
            "        options: (only applies to SavedModel format)",
            "            `tf.saved_model.SaveOptions` object that specifies options for",
            "            saving to SavedModel.",
            "        save_traces: (only applies to SavedModel format) When enabled, the",
            "            SavedModel will store the function traces for each layer. This",
            "            can be disabled, so that only the configs of each layer are stored.",
            "            Defaults to `True`. Disabling this will decrease serialization time",
            "            and reduce file size, but it requires that all custom layers/models",
            "            implement a `get_config()` method.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    from keras.models import load_model",
            "",
            "    model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'",
            "    del model  # deletes the existing model",
            "",
            "    # returns a compiled model",
            "    # identical to the previous one",
            "    model = load_model('my_model.h5')",
            "    ```",
            "    \"\"\"",
            "    # pylint: enable=line-too-long",
            "    save.save_model(self, filepath, overwrite, include_optimizer, save_format,",
            "                    signatures, options, save_traces)",
            "",
            "  def save_weights(self,",
            "                   filepath,",
            "                   overwrite=True,",
            "                   save_format=None,",
            "                   options=None):",
            "    \"\"\"Saves all layer weights.",
            "",
            "    Either saves in HDF5 or in TensorFlow format based on the `save_format`",
            "    argument.",
            "",
            "    When saving in HDF5 format, the weight file has:",
            "      - `layer_names` (attribute), a list of strings",
            "          (ordered names of model layers).",
            "      - For every layer, a `group` named `layer.name`",
            "          - For every such layer group, a group attribute `weight_names`,",
            "              a list of strings",
            "              (ordered names of weights tensor of the layer).",
            "          - For every weight in the layer, a dataset",
            "              storing the weight value, named after the weight tensor.",
            "",
            "    When saving in TensorFlow format, all objects referenced by the network are",
            "    saved in the same format as `tf.train.Checkpoint`, including any `Layer`",
            "    instances or `Optimizer` instances assigned to object attributes. For",
            "    networks constructed from inputs and outputs using `tf.keras.Model(inputs,",
            "    outputs)`, `Layer` instances used by the network are tracked/saved",
            "    automatically. For user-defined classes which inherit from `tf.keras.Model`,",
            "    `Layer` instances must be assigned to object attributes, typically in the",
            "    constructor. See the documentation of `tf.train.Checkpoint` and",
            "    `tf.keras.Model` for details.",
            "",
            "    While the formats are the same, do not mix `save_weights` and",
            "    `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be",
            "    loaded using `Model.load_weights`. Checkpoints saved using",
            "    `tf.train.Checkpoint.save` should be restored using the corresponding",
            "    `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over",
            "    `save_weights` for training checkpoints.",
            "",
            "    The TensorFlow format matches objects and variables by starting at a root",
            "    object, `self` for `save_weights`, and greedily matching attribute",
            "    names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this",
            "    is the `Checkpoint` even if the `Checkpoint` has a model attached. This",
            "    means saving a `tf.keras.Model` using `save_weights` and loading into a",
            "    `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match",
            "    the `Model`'s variables. See the [guide to training",
            "    checkpoints](https://www.tensorflow.org/guide/checkpoint) for details",
            "    on the TensorFlow format.",
            "",
            "    Args:",
            "        filepath: String or PathLike, path to the file to save the weights to.",
            "            When saving in TensorFlow format, this is the prefix used for",
            "            checkpoint files (multiple files are generated). Note that the '.h5'",
            "            suffix causes weights to be saved in HDF5 format.",
            "        overwrite: Whether to silently overwrite any existing file at the",
            "            target location, or provide the user with a manual prompt.",
            "        save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or",
            "            '.keras' will default to HDF5 if `save_format` is `None`. Otherwise",
            "            `None` defaults to 'tf'.",
            "        options: Optional `tf.train.CheckpointOptions` object that specifies",
            "            options for saving weights.",
            "",
            "    Raises:",
            "        ImportError: If h5py is not available when attempting to save in HDF5",
            "            format.",
            "        ValueError: For invalid/unknown format arguments.",
            "    \"\"\"",
            "    self._assert_weights_created()",
            "    filepath = path_to_string(filepath)",
            "    filepath_is_h5 = saving_utils.is_hdf5_filepath(filepath)",
            "    if save_format is None:",
            "      if filepath_is_h5:",
            "        save_format = 'h5'",
            "      else:",
            "        save_format = 'tf'",
            "    else:",
            "      user_format = save_format.lower().strip()",
            "      if user_format in ('tensorflow', 'tf'):",
            "        save_format = 'tf'",
            "      elif user_format in ('hdf5', 'h5', 'keras'):",
            "        save_format = 'h5'",
            "      else:",
            "        raise ValueError(",
            "            'Unknown format \"%s\". Was expecting one of {\"tf\", \"h5\"}.' % (",
            "                save_format,))",
            "    if save_format == 'tf' and filepath_is_h5:",
            "      raise ValueError(",
            "          ('save_weights got save_format=\"tf\"/\"tensorflow\", but the '",
            "           'filepath (\"%s\") looks like an HDF5 file. Omit the \".h5\"/\".keras\" '",
            "           'when saving in TensorFlow format.')",
            "          % filepath)",
            "",
            "    if save_format == 'h5' and h5py is None:",
            "      raise ImportError(",
            "          '`save_weights` requires h5py when saving in hdf5.')",
            "    if save_format == 'tf':",
            "      check_filepath = filepath + '.index'",
            "    else:",
            "      check_filepath = filepath",
            "    # If file exists and should not be overwritten:",
            "    if not overwrite and os.path.isfile(check_filepath):",
            "      proceed = ask_to_proceed_with_overwrite(check_filepath)",
            "      if not proceed:",
            "        return",
            "    if save_format == 'h5':",
            "      with h5py.File(filepath, 'w') as f:",
            "        hdf5_format.save_weights_to_hdf5_group(f, self.layers)",
            "    else:",
            "      if context.executing_eagerly():",
            "        session = None",
            "      else:",
            "        session = backend.get_session()",
            "      self._trackable_saver.save(filepath, session=session, options=options)",
            "      # Record this checkpoint so it's visible from tf.train.latest_checkpoint.",
            "      checkpoint_management.update_checkpoint_state_internal(",
            "          save_dir=os.path.dirname(filepath),",
            "          model_checkpoint_path=filepath,",
            "          save_relative_paths=True,",
            "          all_model_checkpoint_paths=[filepath])",
            "",
            "  def load_weights(self,",
            "                   filepath,",
            "                   by_name=False,",
            "                   skip_mismatch=False,",
            "                   options=None):",
            "    \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file.",
            "",
            "    If `by_name` is False weights are loaded based on the network's",
            "    topology. This means the architecture should be the same as when the weights",
            "    were saved.  Note that layers that don't have weights are not taken into",
            "    account in the topological ordering, so adding or removing layers is fine as",
            "    long as they don't have weights.",
            "",
            "    If `by_name` is True, weights are loaded into layers only if they share the",
            "    same name. This is useful for fine-tuning or transfer-learning models where",
            "    some of the layers have changed.",
            "",
            "    Only topological loading (`by_name=False`) is supported when loading weights",
            "    from the TensorFlow format. Note that topological loading differs slightly",
            "    between TensorFlow and HDF5 formats for user-defined classes inheriting from",
            "    `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the",
            "    TensorFlow format loads based on the object-local names of attributes to",
            "    which layers are assigned in the `Model`'s constructor.",
            "",
            "    Args:",
            "        filepath: String, path to the weights file to load. For weight files in",
            "            TensorFlow format, this is the file prefix (the same as was passed",
            "            to `save_weights`). This can also be a path to a SavedModel",
            "            saved from `model.save`.",
            "        by_name: Boolean, whether to load weights by name or by topological",
            "            order. Only topological loading is supported for weight files in",
            "            TensorFlow format.",
            "        skip_mismatch: Boolean, whether to skip loading of layers where there is",
            "            a mismatch in the number of weights, or a mismatch in the shape of",
            "            the weight (only valid when `by_name=True`).",
            "        options: Optional `tf.train.CheckpointOptions` object that specifies",
            "            options for loading weights.",
            "",
            "    Returns:",
            "        When loading a weight file in TensorFlow format, returns the same status",
            "        object as `tf.train.Checkpoint.restore`. When graph building, restore",
            "        ops are run automatically as soon as the network is built (on first call",
            "        for user-defined classes inheriting from `Model`, immediately if it is",
            "        already built).",
            "",
            "        When loading weights in HDF5 format, returns `None`.",
            "",
            "    Raises:",
            "        ImportError: If h5py is not available and the weight file is in HDF5",
            "            format.",
            "        ValueError: If `skip_mismatch` is set to `True` when `by_name` is",
            "          `False`.",
            "    \"\"\"",
            "    if backend.is_tpu_strategy(self._distribution_strategy):",
            "      if (self._distribution_strategy.extended.steps_per_run > 1 and",
            "          (not saving_utils.is_hdf5_filepath(filepath))):",
            "        raise ValueError('Load weights is not yet supported with TPUStrategy '",
            "                         'with steps_per_run greater than 1.')",
            "    if skip_mismatch and not by_name:",
            "      raise ValueError(",
            "          'When calling model.load_weights, skip_mismatch can only be set to '",
            "          'True when by_name is True.')",
            "",
            "    filepath, save_format = _detect_save_format(filepath)",
            "    if save_format == 'tf':",
            "      status = self._trackable_saver.restore(filepath, options)",
            "      if by_name:",
            "        raise NotImplementedError(",
            "            'Weights may only be loaded based on topology into Models when '",
            "            'loading TensorFlow-formatted weights (got by_name=True to '",
            "            'load_weights).')",
            "      if not context.executing_eagerly():",
            "        session = backend.get_session()",
            "        # Restore existing variables (if any) immediately, and set up a",
            "        # streaming restore for any variables created in the future.",
            "        trackable_utils.streaming_restore(status=status, session=session)",
            "      status.assert_nontrivial_match()",
            "      return status",
            "    if h5py is None:",
            "      raise ImportError(",
            "          '`load_weights` requires h5py when loading weights from HDF5.')",
            "    if not self._is_graph_network and not self.built:",
            "      raise ValueError(",
            "          'Unable to load weights saved in HDF5 format into a subclassed '",
            "          'Model which has not created its variables yet. Call the Model '",
            "          'first, then load the weights.')",
            "    self._assert_weights_created()",
            "    with h5py.File(filepath, 'r') as f:",
            "      if 'layer_names' not in f.attrs and 'model_weights' in f:",
            "        f = f['model_weights']",
            "      if by_name:",
            "        hdf5_format.load_weights_from_hdf5_group_by_name(",
            "            f, self.layers, skip_mismatch=skip_mismatch)",
            "      else:",
            "        hdf5_format.load_weights_from_hdf5_group(f, self.layers)",
            "",
            "  def _updated_config(self):",
            "    \"\"\"Util shared between different serialization methods.",
            "",
            "    Returns:",
            "        Model config with Keras version information added.",
            "    \"\"\"",
            "    from tensorflow.python.keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top",
            "",
            "    config = self.get_config()",
            "    model_config = {",
            "        'class_name': self.__class__.__name__,",
            "        'config': config,",
            "        'keras_version': keras_version,",
            "        'backend': backend.backend()",
            "    }",
            "    return model_config",
            "",
            "  def get_config(self):",
            "    raise NotImplementedError",
            "",
            "  @classmethod",
            "  def from_config(cls, config, custom_objects=None):",
            "    # `from_config` assumes `cls` is either `Functional` or a child class of",
            "    # `Functional`. In the case that `cls` is meant to behave like a child class",
            "    # of `Functional` but only inherits from the `Model` class, we have to call",
            "    # `cls(...)` instead of `Functional.from_config`.",
            "    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "    with generic_utils.SharedObjectLoadingScope():",
            "      input_tensors, output_tensors, created_layers = (",
            "          functional.reconstruct_from_config(config, custom_objects))",
            "      # Initialize a model belonging to `cls`, which can be user-defined or",
            "      # `Functional`.",
            "      model = cls(inputs=input_tensors, outputs=output_tensors,",
            "                  name=config.get('name'))",
            "      functional.connect_ancillary_layers(model, created_layers)",
            "      return model",
            "",
            "  def to_json(self, **kwargs):",
            "    \"\"\"Returns a JSON string containing the network configuration.",
            "",
            "    To load a network from a JSON save file, use",
            "    `keras.models.model_from_json(json_string, custom_objects={})`.",
            "",
            "    Args:",
            "        **kwargs: Additional keyword arguments",
            "            to be passed to `json.dumps()`.",
            "",
            "    Returns:",
            "        A JSON string.",
            "    \"\"\"",
            "    model_config = self._updated_config()",
            "    return json.dumps(",
            "        model_config, default=json_utils.get_json_type, **kwargs)",
            "",
            "  def to_yaml(self, **kwargs):",
            "    \"\"\"Returns a yaml string containing the network configuration.",
            "",
            "    To load a network from a yaml save file, use",
            "    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.",
            "",
            "    `custom_objects` should be a dictionary mapping",
            "    the names of custom losses / layers / etc to the corresponding",
            "    functions / classes.",
            "",
            "    Args:",
            "        **kwargs: Additional keyword arguments",
            "            to be passed to `yaml.dump()`.",
            "",
            "    Returns:",
            "        A YAML string.",
            "",
            "    Raises:",
            "        ImportError: if yaml module is not found.",
            "    \"\"\"",
            "    if yaml is None:",
            "      raise ImportError(",
            "          'Requires yaml module installed (`pip install pyyaml`).')",
            "    return yaml.dump(self._updated_config(), **kwargs)",
            "",
            "  def reset_states(self):",
            "    for layer in self.layers:",
            "      if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):",
            "        layer.reset_states()",
            "",
            "  @property",
            "  @doc_controls.do_not_generate_docs",
            "  def state_updates(self):",
            "    \"\"\"Deprecated, do NOT use!",
            "",
            "    Returns the `updates` from all layers that are stateful.",
            "",
            "    This is useful for separating training updates and",
            "    state updates, e.g. when we need to update a layer's internal state",
            "    during prediction.",
            "",
            "    Returns:",
            "        A list of update ops.",
            "    \"\"\"",
            "    warnings.warn('`Model.state_updates` will be removed in a future version. '",
            "                  'This property should not be used in TensorFlow 2.0, '",
            "                  'as `updates` are applied automatically.')",
            "    state_updates = []",
            "    for layer in self.layers:",
            "      if getattr(layer, 'stateful', False):",
            "        if hasattr(layer, 'updates'):",
            "          state_updates += layer.updates",
            "    return state_updates",
            "",
            "  @property",
            "  def weights(self):",
            "    \"\"\"Returns the list of all layer variables/weights.",
            "",
            "    Note: This will not track the weights of nested `tf.Modules` that are not",
            "    themselves Keras layers.",
            "",
            "    Returns:",
            "      A list of variables.",
            "    \"\"\"",
            "    return self._dedup_weights(self._undeduplicated_weights)",
            "",
            "  @property",
            "  def _undeduplicated_weights(self):",
            "    \"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"",
            "    self._assert_weights_created()",
            "    weights = []",
            "    for layer in self._self_tracked_trackables:",
            "      weights += layer.variables",
            "    weights += (self._trainable_weights + self._non_trainable_weights)",
            "    return weights",
            "",
            "  def summary(self, line_length=None, positions=None, print_fn=None):",
            "    \"\"\"Prints a string summary of the network.",
            "",
            "    Args:",
            "        line_length: Total length of printed lines",
            "            (e.g. set this to adapt the display to different",
            "            terminal window sizes).",
            "        positions: Relative or absolute positions of log elements",
            "            in each line. If not provided,",
            "            defaults to `[.33, .55, .67, 1.]`.",
            "        print_fn: Print function to use. Defaults to `print`.",
            "            It will be called on each line of the summary.",
            "            You can set it to a custom function",
            "            in order to capture the string summary.",
            "",
            "    Raises:",
            "        ValueError: if `summary()` is called before the model is built.",
            "    \"\"\"",
            "    if not self.built:",
            "      raise ValueError('This model has not yet been built. '",
            "                       'Build the model first by calling `build()` or calling '",
            "                       '`fit()` with some data, or specify '",
            "                       'an `input_shape` argument in the first layer(s) for '",
            "                       'automatic build.')",
            "    layer_utils.print_summary(self,",
            "                              line_length=line_length,",
            "                              positions=positions,",
            "                              print_fn=print_fn)",
            "",
            "  @property",
            "  def layers(self):",
            "    return list(self._flatten_layers(include_self=False, recursive=False))",
            "",
            "  def get_layer(self, name=None, index=None):",
            "    \"\"\"Retrieves a layer based on either its name (unique) or index.",
            "",
            "    If `name` and `index` are both provided, `index` will take precedence.",
            "    Indices are based on order of horizontal graph traversal (bottom-up).",
            "",
            "    Args:",
            "        name: String, name of layer.",
            "        index: Integer, index of layer.",
            "",
            "    Returns:",
            "        A layer instance.",
            "",
            "    Raises:",
            "        ValueError: In case of invalid layer name or index.",
            "    \"\"\"",
            "    # TODO(fchollet): We could build a dictionary based on layer names",
            "    # since they are constant, but we have not done that yet.",
            "    if index is not None and name is not None:",
            "      raise ValueError('Provide only a layer name or a layer index.')",
            "",
            "    if index is not None:",
            "      if len(self.layers) <= index:",
            "        raise ValueError('Was asked to retrieve layer at index ' + str(index) +",
            "                         ' but model only has ' + str(len(self.layers)) +",
            "                         ' layers.')",
            "      else:",
            "        return self.layers[index]",
            "",
            "    if name is not None:",
            "      for layer in self.layers:",
            "        if layer.name == name:",
            "          return layer",
            "      raise ValueError('No such layer: ' + name + '.')",
            "    raise ValueError('Provide either a layer name or layer index.')",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _set_save_spec(self, inputs):",
            "    if self._saved_model_inputs_spec is not None:",
            "      return  # Already set.",
            "",
            "    input_names = self.input_names",
            "    if not input_names:",
            "      input_names = compile_utils.create_pseudo_input_names(inputs)",
            "",
            "    flat_inputs = nest.flatten(inputs)",
            "    specs = []",
            "    for name, tensor in zip(input_names, flat_inputs):",
            "      specs.append(",
            "          tf_utils.get_tensor_spec(tensor, dynamic_batch=False, name=name))",
            "    specs = nest.pack_sequence_as(inputs, specs)",
            "",
            "    self._saved_model_inputs_spec = specs",
            "",
            "    # Store the input shapes",
            "    if (self.__class__.__name__ == 'Sequential' and",
            "        self._build_input_shape is None):",
            "      self._build_input_shape = nest.map_structure(",
            "          lambda x: None if x is None else x.shape, specs)",
            "",
            "  def _assert_weights_created(self):",
            "    \"\"\"Asserts that all the weights for the model have been created.",
            "",
            "    For a non-dynamic model, the weights must already be created after the",
            "    layer has been called. For a dynamic model, the exact list of weights can",
            "    never be known for certain since it may change at any time during execution.",
            "",
            "    We run this check right before accessing weights or getting the Numpy value",
            "    for the current weights. Otherwise, if the layer has never been called,",
            "    the user would just get an empty list, which is misleading.",
            "",
            "    Raises:",
            "      ValueError: if the weights of the network has not yet been created.",
            "    \"\"\"",
            "    if self.dynamic:",
            "      return",
            "",
            "    if ('build' in self.__class__.__dict__ and",
            "        self.__class__ != Model and",
            "        not self.built):",
            "      # For any model that has customized build() method but hasn't",
            "      # been invoked yet, this will cover both sequential and subclass model.",
            "      # Also make sure to exclude Model class itself which has build() defined.",
            "      raise ValueError('Weights for model %s have not yet been created. '",
            "                       'Weights are created when the Model is first called on '",
            "                       'inputs or `build()` is called with an `input_shape`.' %",
            "                       self.name)",
            "",
            "  def _check_call_args(self, method_name):",
            "    \"\"\"Check that `call` has only one positional arg.\"\"\"",
            "    # Always allow first arg, regardless of arg name.",
            "    fullargspec = self._call_full_argspec",
            "    if fullargspec.defaults:",
            "      positional_args = fullargspec.args[:-len(fullargspec.defaults)]",
            "    else:",
            "      positional_args = fullargspec.args",
            "    if 'training' in positional_args:",
            "      positional_args.remove('training')",
            "",
            "    # self and first arg can be positional.",
            "    if len(positional_args) > 2:",
            "      extra_args = positional_args[2:]",
            "      raise ValueError(",
            "          'Models passed to `' + method_name + '` can only have `training` '",
            "          'and the first argument in `call` as positional arguments, '",
            "          'found: ' + str(extra_args) + '.')",
            "",
            "  def _validate_compile(self, optimizer, metrics, **kwargs):",
            "    \"\"\"Performs validation checks for the default `compile`.\"\"\"",
            "    if any(",
            "        isinstance(opt, optimizer_v1.Optimizer)",
            "        for opt in nest.flatten(optimizer)):",
            "      raise ValueError(",
            "          '`tf.compat.v1.keras` Optimizer (', optimizer, ') is '",
            "          'not supported when eager execution is enabled. Use a '",
            "          '`tf.keras` Optimizer instead, or disable eager '",
            "          'execution.')",
            "",
            "    kwargs.pop('cloning', None)  # Legacy DistStrat argument, never used.",
            "    kwargs.pop('experimental_run_tf_function', None)  # Always `True`.",
            "    if kwargs.pop('distribute', None) is not None:",
            "      raise ValueError(",
            "          'Distribute argument in compile is not available in TF 2.0 please '",
            "          'create the model under the distribution strategy scope.')",
            "    if kwargs.pop('target_tensors', None) is not None:",
            "      raise ValueError(",
            "          'target_tensors argument is not supported when executing eagerly.')",
            "    invalid_kwargs = set(kwargs) - {'sample_weight_mode'}",
            "    if invalid_kwargs:",
            "      raise TypeError('Invalid keyword argument(s) in `compile`: %s' %",
            "                      (invalid_kwargs,))",
            "",
            "    # Model must be created and compiled with the same DistStrat.",
            "    if self.built and ds_context.has_strategy():",
            "      strategy = ds_context.get_strategy()",
            "      for v in self.variables:",
            "        if not strategy.extended.variable_created_in_scope(v):",
            "          raise ValueError(",
            "              'Variable (%s) was not created in the distribution strategy '",
            "              'scope of (%s). It is most likely due to not all layers or '",
            "              'the model or optimizer being created outside the distribution '",
            "              'strategy scope. Try to make sure your code looks similar '",
            "              'to the following.\\n'",
            "              'with strategy.scope():\\n'",
            "              '  model=_create_model()\\n'",
            "              '  model.compile(...)' % (v, strategy))",
            "",
            "    # Model metrics must be created in the same distribution strategy scope",
            "    # as the model.",
            "    strategy = self.distribute_strategy",
            "    for metric in nest.flatten(metrics):",
            "      for v in getattr(metric, 'variables', []):",
            "        if not strategy.extended.variable_created_in_scope(v):",
            "          raise ValueError(",
            "              'Metric (%s) passed to model.compile was created inside of a '",
            "              'different distribution strategy scope than the model. All '",
            "              'metrics must be created in the same distribution strategy '",
            "              'scope as the model (in this case %s). If you pass in a string '",
            "              'identifier for a metric to compile the metric will '",
            "              'automatically be created in the correct distribution '",
            "              'strategy scope.' % (metric, strategy)",
            "          )",
            "",
            "    # Model metrics must be created in the same distribution strategy scope",
            "    # as the model.",
            "    for opt in nest.flatten(optimizer):",
            "      for v in getattr(opt, '_weights', []):",
            "        if not strategy.extended.variable_created_in_scope(v):",
            "          raise ValueError(",
            "              'Optimizer (%s) passed to model.compile was created inside of a '",
            "              'different distribution strategy scope than the model. All '",
            "              'optimizers must be created in the same distribution strategy '",
            "              'scope as the model (in this case %s). If you pass in a string '",
            "              'identifier for an optimizer to compile the optimizer will '",
            "              'automatically be created in the correct distribution '",
            "              'strategy scope.' % (opt, strategy))",
            "",
            "  def _maybe_load_initial_epoch_from_ckpt(self, initial_epoch):",
            "    \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.",
            "",
            "    Refer to tensorflow/python/keras/distribute/worker_training_state.py",
            "    for more information.",
            "",
            "    Args:",
            "      initial_epoch: The original initial_epoch user passes in in `fit()`.",
            "",
            "    Returns:",
            "      If the training is recovering from previous failure under multi-worker",
            "      training setting, return the epoch the training is supposed to continue",
            "      at. Otherwise, return the `initial_epoch` the user passes in.",
            "    \"\"\"",
            "    if self._training_state is not None:",
            "      return self._training_state.maybe_load_initial_epoch_from_ckpt(",
            "          initial_epoch, mode=ModeKeys.TRAIN)",
            "    return initial_epoch",
            "",
            "  def _assert_compile_was_called(self):",
            "    # Checks whether `compile` has been called. If it has been called,",
            "    # then the optimizer is set. This is different from whether the",
            "    # model is compiled",
            "    # (i.e. whether the model is built and its inputs/outputs are set).",
            "    if not self._is_compiled:",
            "      raise RuntimeError('You must compile your model before '",
            "                         'training/testing. '",
            "                         'Use `model.compile(optimizer, loss)`.')",
            "",
            "  def _set_inputs(self, inputs, outputs=None, training=None):",
            "    \"\"\"This method is for compat with Modelv1. Only inputs are needed here.\"\"\"",
            "    self._set_save_spec(inputs)",
            "",
            "  @property",
            "  def _trackable_saved_model_saver(self):",
            "    return model_serialization.ModelSavedModelSaver(self)",
            "",
            "  def _list_functions_for_serialization(self, serialization_cache):",
            "    # SavedModel needs to ignore the execution functions.",
            "    train_function = self.train_function",
            "    test_function = self.test_function",
            "    predict_function = self.predict_function",
            "    self.train_function = None",
            "    self.test_function = None",
            "    self.predict_function = None",
            "    functions = super(",
            "        Model, self)._list_functions_for_serialization(serialization_cache)",
            "    self.train_function = train_function",
            "    self.test_function = test_function",
            "    self.predict_function = predict_function",
            "    return functions",
            "",
            "  def _should_eval(self, epoch, validation_freq):",
            "    if self._cluster_coordinator:",
            "      raise NotImplementedError(",
            "          'Evaluation in `model.fit` with '",
            "          '`ParameterServerStrategy` is not yet supported.')",
            "    epoch = epoch + 1  # one-index the user-facing epoch.",
            "    if isinstance(validation_freq, int):",
            "      return epoch % validation_freq == 0",
            "    elif isinstance(validation_freq, list):",
            "      return epoch in validation_freq",
            "    else:",
            "      raise ValueError('Expected `validation_freq` to be a list or int.')",
            "",
            "  ######################################################################",
            "  # Functions below exist only as v1 / v2 compatibility shims.",
            "  ######################################################################",
            "",
            "  def _get_compile_args(self, user_metrics=True):",
            "    \"\"\"Used for saving or cloning a Model.",
            "",
            "    Args:",
            "      user_metrics: Whether to return user-supplied metrics or `Metric` objects.",
            "        Defaults to returning the user-supplied metrics.",
            "",
            "    Returns:",
            "      Dictionary of arguments that were used when compiling the model.",
            "    \"\"\"",
            "    self._assert_compile_was_called()",
            "    # pylint: disable=protected-access",
            "",
            "    saved_metrics = self.compiled_metrics._user_metrics",
            "    saved_weighted_metrics = self.compiled_metrics._user_weighted_metrics",
            "",
            "    if not user_metrics:",
            "      if saved_metrics is not None:",
            "        saved_metrics = self.compiled_metrics._metrics",
            "      if saved_weighted_metrics is not None:",
            "        saved_weighted_metrics = self.compiled_metrics._weighted_metrics",
            "",
            "    compile_args = {",
            "        'optimizer': self.optimizer,",
            "        'loss': self.compiled_loss._user_losses,",
            "        'metrics': saved_metrics,",
            "        'weighted_metrics': saved_weighted_metrics,",
            "        'loss_weights': self.compiled_loss._user_loss_weights,",
            "    }",
            "    # pylint: enable=protected-access",
            "    return compile_args",
            "",
            "  def _get_callback_model(self):",
            "    return self",
            "",
            "  def _in_multi_worker_mode(self):",
            "    return self.distribute_strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access",
            "",
            "  @property",
            "  def _compile_was_called(self):",
            "    return self._is_compiled",
            "",
            "",
            "def reduce_per_replica(values, strategy, reduction='first'):",
            "  \"\"\"Reduce PerReplica objects.",
            "",
            "  Args:",
            "    values: Structure of `PerReplica` objects or `Tensor`s. `Tensor`s are",
            "      returned as-is.",
            "    strategy: `tf.distribute.Strategy` object.",
            "    reduction: One of 'first', 'concat'.",
            "",
            "  Returns:",
            "    Structure of `Tensor`s.",
            "  \"\"\"",
            "",
            "  def _reduce(v):",
            "    \"\"\"Reduce a single `PerReplica` object.\"\"\"",
            "    if reduction == 'concat' and _collective_all_reduce_multi_worker(strategy):",
            "      return _multi_worker_concat(v, strategy)",
            "    if not isinstance(v, ds_values.PerReplica):",
            "      return v",
            "    elif reduction == 'first':",
            "      return strategy.unwrap(v)[0]",
            "    elif reduction == 'concat':",
            "      if _is_tpu_multi_host(strategy):",
            "        return _tpu_multi_host_concat(v, strategy)",
            "      else:",
            "        return concat(strategy.unwrap(v))",
            "    else:",
            "      raise ValueError('`reduction` must be \"first\" or \"concat\".')",
            "",
            "  return nest.map_structure(_reduce, values)",
            "",
            "",
            "def concat(tensors, axis=0):",
            "  \"\"\"Concats `tensor`s along `axis`.\"\"\"",
            "  if isinstance(tensors[0], sparse_tensor.SparseTensor):",
            "    return sparse_ops.sparse_concat_v2(axis=axis, sp_inputs=tensors)",
            "  return array_ops.concat(tensors, axis=axis)",
            "",
            "",
            "def _is_tpu_multi_host(strategy):",
            "  return (backend.is_tpu_strategy(strategy) and",
            "          strategy.extended.num_hosts > 1)",
            "",
            "",
            "def _tpu_multi_host_concat(v, strategy):",
            "  \"\"\"Correctly order TPU PerReplica objects.\"\"\"",
            "  replicas = strategy.unwrap(v)",
            "  # When distributed datasets are created from Tensors / NumPy,",
            "  # TPUStrategy.experimental_distribute_dataset shards data in",
            "  # (Replica, Host) order, and TPUStrategy.unwrap returns it in",
            "  # (Host, Replica) order.",
            "  # TODO(b/150317897): Figure out long-term plan here.",
            "  num_replicas_per_host = strategy.extended.num_replicas_per_host",
            "  ordered_replicas = []",
            "  for replica_id in range(num_replicas_per_host):",
            "    ordered_replicas += replicas[replica_id::num_replicas_per_host]",
            "  return concat(ordered_replicas)",
            "",
            "",
            "def _collective_all_reduce_multi_worker(strategy):",
            "  return (isinstance(strategy,",
            "                     collective_all_reduce_strategy.CollectiveAllReduceStrategy)",
            "         ) and strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access",
            "",
            "",
            "# TODO(wxinyi): merge this with _tpu_multi_host_concat once we have all_gather",
            "# for all strategies",
            "def _multi_worker_concat(v, strategy):",
            "  \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"",
            "  replicas = strategy.gather(v, axis=0)",
            "  # v might not have the same shape on different replicas",
            "  if isinstance(v, ds_values.PerReplica):",
            "    shapes = array_ops.concat([",
            "        array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
            "        for single_value in v.values",
            "    ],",
            "                              axis=0)",
            "    all_shapes = strategy.gather(shapes, axis=0)",
            "  else:",
            "    # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
            "    all_shapes = strategy.gather(",
            "        array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0), axis=0)",
            "",
            "  replicas = array_ops.split(",
            "      replicas,",
            "      num_or_size_splits=all_shapes,",
            "      num=strategy.num_replicas_in_sync)",
            "  ordered_replicas = []",
            "  num_replicas_per_worker = len(strategy.extended.worker_devices)",
            "  for replica_id in range(num_replicas_per_worker):",
            "    ordered_replicas += replicas[replica_id::num_replicas_per_worker]",
            "  return concat(ordered_replicas)",
            "",
            "",
            "def _is_scalar(x):",
            "  return isinstance(x, (ops.Tensor, variables.Variable)) and x.shape.rank == 0",
            "",
            "",
            "def write_scalar_summaries(logs, step):",
            "  for name, value in logs.items():",
            "    if _is_scalar(value):",
            "      summary_ops_v2.scalar('batch_' + name, value, step=step)",
            "",
            "",
            "def _minimum_control_deps(outputs):",
            "  \"\"\"Returns the minimum control dependencies to ensure step succeeded.\"\"\"",
            "  if context.executing_eagerly():",
            "    return []  # Control dependencies not needed.",
            "  outputs = nest.flatten(outputs, expand_composites=True)",
            "  for out in outputs:",
            "    # Variables can't be control dependencies.",
            "    if not isinstance(out, variables.Variable):",
            "      return [out]  # Return first Tensor or Op from outputs.",
            "  return []  # No viable Tensor or Op to use for control deps.",
            "",
            "",
            "def _disallow_inside_tf_function(method_name):",
            "  if ops.inside_function():",
            "    error_msg = (",
            "        'Detected a call to `Model.{method_name}` inside a `tf.function`. '",
            "        '`Model.{method_name} is a high-level endpoint that manages its own '",
            "        '`tf.function`. Please move the call to `Model.{method_name}` outside '",
            "        'of all enclosing `tf.function`s. Note that you can call a `Model` '",
            "        'directly on `Tensor`s inside a `tf.function` like: `model(x)`.'",
            "    ).format(method_name=method_name)",
            "    raise RuntimeError(error_msg)",
            "",
            "",
            "def _detect_save_format(filepath):",
            "  \"\"\"Returns path to weights file and save format.\"\"\"",
            "",
            "  filepath = path_to_string(filepath)",
            "  if saving_utils.is_hdf5_filepath(filepath):",
            "    return filepath, 'h5'",
            "",
            "  # Filepath could be a TensorFlow checkpoint file prefix or SavedModel",
            "  # directory. It's possible for filepath to be both a prefix and directory.",
            "  # Prioritize checkpoint over SavedModel.",
            "  if _is_readable_tf_checkpoint(filepath):",
            "    save_format = 'tf'",
            "  elif sm_loader.contains_saved_model(filepath):",
            "    ckpt_path = os.path.join(filepath, sm_constants.VARIABLES_DIRECTORY,",
            "                             sm_constants.VARIABLES_FILENAME)",
            "    if _is_readable_tf_checkpoint(ckpt_path):",
            "      filepath = ckpt_path",
            "      save_format = 'tf'",
            "    else:",
            "      raise ValueError('Unable to load weights. filepath {} appears to be a '",
            "                       'SavedModel directory, but checkpoint either doesn\\'t '",
            "                       'exist, or is incorrectly formatted.'.format(filepath))",
            "  else:",
            "    # Not a TensorFlow checkpoint. This filepath is likely an H5 file that",
            "    # doesn't have the hdf5/keras extensions.",
            "    save_format = 'h5'",
            "  return filepath, save_format",
            "",
            "",
            "def _is_readable_tf_checkpoint(filepath):",
            "  try:",
            "    py_checkpoint_reader.NewCheckpointReader(filepath)",
            "    return True",
            "  except errors_impl.DataLossError:",
            "    # The checkpoint is not readable in TensorFlow format.",
            "    return False",
            "",
            "",
            "def flatten_metrics_in_order(logs, metrics_names):",
            "  \"\"\"Turns the `logs` dict into a list as per key order of `metrics_names`.\"\"\"",
            "  results = []",
            "  for name in metrics_names:",
            "    if name in logs:",
            "      results.append(logs[name])",
            "  for key in sorted(logs.keys()):",
            "    if key not in metrics_names:",
            "      results.append(logs[key])",
            "  if len(results) == 1:",
            "    return results[0]",
            "  return results"
        ],
        "afterPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Training-related part of the Keras engine.\"\"\"",
            "",
            "import copy",
            "import itertools",
            "import json",
            "import os",
            "import warnings",
            "",
            "from tensorflow.python.autograph.lang import directives",
            "from tensorflow.python.data.experimental.ops import distribute_options",
            "from tensorflow.python.data.ops import dataset_ops",
            "from tensorflow.python.distribute import collective_all_reduce_strategy",
            "from tensorflow.python.distribute import distribution_strategy_context as ds_context",
            "from tensorflow.python.distribute import values as ds_values",
            "from tensorflow.python.distribute.coordinator import cluster_coordinator",
            "from tensorflow.python.eager import backprop",
            "from tensorflow.python.eager import context",
            "from tensorflow.python.eager import def_function",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import errors_impl",
            "from tensorflow.python.framework import func_graph",
            "from tensorflow.python.framework import ops",
            "from tensorflow.python.framework import sparse_tensor",
            "from tensorflow.python.framework import tensor_shape",
            "from tensorflow.python.keras import backend",
            "from tensorflow.python.keras import callbacks as callbacks_module",
            "from tensorflow.python.keras import optimizer_v1",
            "from tensorflow.python.keras import optimizers",
            "from tensorflow.python.keras.engine import base_layer",
            "from tensorflow.python.keras.engine import base_layer_utils",
            "from tensorflow.python.keras.engine import compile_utils",
            "from tensorflow.python.keras.engine import data_adapter",
            "from tensorflow.python.keras.engine import training_utils",
            "from tensorflow.python.keras.mixed_precision import loss_scale_optimizer as lso",
            "from tensorflow.python.keras.mixed_precision import policy",
            "from tensorflow.python.keras.saving import hdf5_format",
            "from tensorflow.python.keras.saving import save",
            "from tensorflow.python.keras.saving import saving_utils",
            "from tensorflow.python.keras.saving.saved_model import json_utils",
            "from tensorflow.python.keras.saving.saved_model import model_serialization",
            "from tensorflow.python.keras.utils import generic_utils",
            "from tensorflow.python.keras.utils import layer_utils",
            "from tensorflow.python.keras.utils import tf_utils",
            "from tensorflow.python.keras.utils import version_utils",
            "from tensorflow.python.keras.utils.io_utils import ask_to_proceed_with_overwrite",
            "from tensorflow.python.keras.utils.io_utils import path_to_string",
            "from tensorflow.python.keras.utils.mode_keys import ModeKeys",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import sparse_ops",
            "from tensorflow.python.ops import summary_ops_v2",
            "from tensorflow.python.ops import variables",
            "from tensorflow.python.platform import tf_logging as logging",
            "from tensorflow.python.profiler import trace",
            "from tensorflow.python.saved_model import constants as sm_constants",
            "from tensorflow.python.saved_model import loader_impl as sm_loader",
            "from tensorflow.python.training import checkpoint_management",
            "from tensorflow.python.training import py_checkpoint_reader",
            "from tensorflow.python.training.tracking import base as trackable",
            "from tensorflow.python.training.tracking import data_structures",
            "from tensorflow.python.training.tracking import util as trackable_utils",
            "from tensorflow.python.util import nest",
            "from tensorflow.python.util import tf_decorator",
            "from tensorflow.python.util.tf_export import keras_export",
            "from tensorflow.tools.docs import doc_controls",
            "",
            "",
            "# pylint: disable=g-import-not-at-top",
            "try:",
            "  import h5py",
            "except ImportError:",
            "  h5py = None",
            "# pylint: enable=g-import-not-at-top",
            "",
            "",
            "def disable_multi_worker(method):",
            "  \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"",
            "",
            "  def _method_wrapper(self, *args, **kwargs):",
            "    if self._in_multi_worker_mode():  # pylint: disable=protected-access",
            "      raise ValueError('{} is not supported in multi-worker mode.'.format(",
            "          method.__name__))",
            "    return method(self, *args, **kwargs)",
            "",
            "  return tf_decorator.make_decorator(",
            "      target=method, decorator_func=_method_wrapper)",
            "",
            "",
            "def inject_functional_model_class(cls):",
            "  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"",
            "  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top",
            "  if cls == Model or cls == training_v1.Model:",
            "    return functional.Functional",
            "  # In case there is any multiple inheritance, we stop injecting the",
            "  # class if keras model is not in its class hierarchy.",
            "  if cls == object:",
            "    return object",
            "",
            "  cls.__bases__ = tuple(inject_functional_model_class(base)",
            "                        for base in cls.__bases__)",
            "  # Trigger any `__new__` class swapping that needed to happen on `Functional`",
            "  # but did not because functional was not in the class hierarchy.",
            "  cls.__new__(cls)",
            "",
            "  return cls",
            "",
            "",
            "def is_functional_model_init_params(args, kwargs):",
            "  return (len(args) == 2 or",
            "          len(args) == 1 and 'outputs' in kwargs or",
            "          'inputs' in kwargs and 'outputs' in kwargs)",
            "",
            "",
            "@keras_export('keras.Model', 'keras.models.Model')",
            "class Model(base_layer.Layer, version_utils.ModelVersionSelector):",
            "  \"\"\"`Model` groups layers into an object with training and inference features.",
            "",
            "  Args:",
            "      inputs: The input(s) of the model: a `keras.Input` object or list of",
            "          `keras.Input` objects.",
            "      outputs: The output(s) of the model. See Functional API example below.",
            "      name: String, the name of the model.",
            "",
            "  There are two ways to instantiate a `Model`:",
            "",
            "  1 - With the \"Functional API\", where you start from `Input`,",
            "  you chain layer calls to specify the model's forward pass,",
            "  and finally you create your model from inputs and outputs:",
            "",
            "  ```python",
            "  import tensorflow as tf",
            "",
            "  inputs = tf.keras.Input(shape=(3,))",
            "  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)",
            "  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)",
            "  model = tf.keras.Model(inputs=inputs, outputs=outputs)",
            "  ```",
            "",
            "  2 - By subclassing the `Model` class: in that case, you should define your",
            "  layers in `__init__` and you should implement the model's forward pass",
            "  in `call`.",
            "",
            "  ```python",
            "  import tensorflow as tf",
            "",
            "  class MyModel(tf.keras.Model):",
            "",
            "    def __init__(self):",
            "      super(MyModel, self).__init__()",
            "      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)",
            "      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)",
            "",
            "    def call(self, inputs):",
            "      x = self.dense1(inputs)",
            "      return self.dense2(x)",
            "",
            "  model = MyModel()",
            "  ```",
            "",
            "  If you subclass `Model`, you can optionally have",
            "  a `training` argument (boolean) in `call`, which you can use to specify",
            "  a different behavior in training and inference:",
            "",
            "  ```python",
            "  import tensorflow as tf",
            "",
            "  class MyModel(tf.keras.Model):",
            "",
            "    def __init__(self):",
            "      super(MyModel, self).__init__()",
            "      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)",
            "      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)",
            "      self.dropout = tf.keras.layers.Dropout(0.5)",
            "",
            "    def call(self, inputs, training=False):",
            "      x = self.dense1(inputs)",
            "      if training:",
            "        x = self.dropout(x, training=training)",
            "      return self.dense2(x)",
            "",
            "  model = MyModel()",
            "  ```",
            "",
            "  Once the model is created, you can config the model with losses and metrics",
            "  with `model.compile()`, train the model with `model.fit()`, or use the model",
            "  to do prediction with `model.predict()`.",
            "  \"\"\"",
            "  _TF_MODULE_IGNORED_PROPERTIES = frozenset(",
            "      itertools.chain(('_train_counter', '_test_counter', '_predict_counter',",
            "                       '_steps_per_execution'),",
            "                      base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES))  # pylint: disable=protected-access",
            "",
            "  def __new__(cls, *args, **kwargs):",
            "    # Signature detection",
            "    if is_functional_model_init_params(args, kwargs) and cls == Model:",
            "      # Functional model",
            "      from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "      return functional.Functional(skip_init=True, *args, **kwargs)",
            "    else:",
            "      return super(Model, cls).__new__(cls, *args, **kwargs)",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def __init__(self, *args, **kwargs):",
            "    self._is_model_for_instrumentation = True",
            "    base_layer.keras_api_gauge.get_cell('model').set(True)",
            "",
            "    # Special case for Subclassed Functional Model, which we couldn't detect",
            "    # when __new__ is called. We only realize it is a functional model when it",
            "    # calls super.__init__ with input and output tensor.",
            "    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "    if (is_functional_model_init_params(args, kwargs) and",
            "        not isinstance(self, functional.Functional)):",
            "      # Filter the kwargs for multiple inheritance.",
            "      supported_kwargs = ['inputs', 'outputs', 'name', 'trainable', 'skip_init']",
            "      model_kwargs = {k: kwargs[k] for k in kwargs if k in supported_kwargs}",
            "      other_kwargs = {k: kwargs[k] for k in kwargs if k not in supported_kwargs}",
            "      inject_functional_model_class(self.__class__)",
            "      functional.Functional.__init__(self, *args, **model_kwargs)",
            "",
            "      # In case there is any multiple inheritance here, we need to call the",
            "      # __init__ for any class that appears after the Functional class.",
            "      clz_to_init = []",
            "      found_functional_class = False",
            "      for clz in self.__class__.__bases__:",
            "        if issubclass(clz, functional.Functional):",
            "          found_functional_class = True",
            "          continue",
            "        if found_functional_class:",
            "          clz_to_init.append(clz)",
            "",
            "      if clz_to_init:",
            "        for clz in clz_to_init:",
            "          clz.__init__(self, *args, **other_kwargs)",
            "      elif other_kwargs:",
            "        # In case there are unused kwargs, we should raise an error to user, in",
            "        # case they have a typo in the param name.",
            "        raise TypeError(",
            "            'The following keyword arguments aren\\'t supported: {}'.format(",
            "                other_kwargs))",
            "      return",
            "",
            "    base_layer.keras_api_gauge.get_cell('Model subclass').set(True)",
            "    # The following are implemented as property functions:",
            "    # self.trainable_weights",
            "    # self.non_trainable_weights",
            "    # `inputs` / `outputs` will only appear in kwargs if either are misspelled.",
            "    generic_utils.validate_kwargs(kwargs, {",
            "        'trainable', 'dtype', 'dynamic', 'name', 'autocast', 'inputs', 'outputs'",
            "    })",
            "    super(Model, self).__init__(**kwargs)",
            "    # By default, Model is a subclass model, which is not in graph network.",
            "    self._is_graph_network = False",
            "",
            "    self.inputs = None",
            "    self.outputs = None",
            "    self.input_names = None",
            "    self.output_names = None",
            "    # stop_training is used by callback to stop training when error happens",
            "    self.stop_training = False",
            "    self.history = None",
            "    # These objects are used in the default `Model.compile`. They are not",
            "    # guaranteed to be set after `Model.compile` is called, as users can",
            "    # override compile with custom logic.",
            "    self.compiled_loss = None",
            "    self.compiled_metrics = None",
            "",
            "    # This is True for Sequential networks and Functional networks.",
            "    self._compute_output_and_mask_jointly = False",
            "",
            "    # Don't reset compilation if already done. This may occur if calling",
            "    # `__init__` (or `_init_graph_network`) on an already-compiled model",
            "    # such as a Sequential model. Sequential models may need to rebuild",
            "    # themselves after compilation.",
            "    self._maybe_create_attribute('_is_compiled', False)",
            "    self._maybe_create_attribute('optimizer', None)",
            "",
            "    # Model must be created under scope of DistStrat it will be trained with.",
            "    if ds_context.has_strategy():",
            "      self._distribution_strategy = ds_context.get_strategy()",
            "    else:",
            "      self._distribution_strategy = None",
            "",
            "    self._cluster_coordinator = None",
            "",
            "    # Defaults to value of `tf.config.experimental_functions_run_eagerly`.",
            "    self._run_eagerly = None",
            "    # Initialize cache attrs.",
            "    self._reset_compile_cache()",
            "",
            "    # Fault-tolerance handler. Set in `ModelCheckpoint`.",
            "    self._training_state = None",
            "    self._saved_model_inputs_spec = None",
            "    self._trackable_saver = (",
            "        trackable_utils.saver_with_op_caching(self))",
            "",
            "    self._steps_per_execution = None",
            "",
            "    self._init_batch_counters()",
            "    self._base_model_initialized = True",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _init_batch_counters(self):",
            "    # Untracked Variables, used to keep track of mini-batches seen in `fit`,",
            "    # `evaluate`, and `predict`.",
            "    agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA",
            "    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)",
            "    self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)",
            "    self._predict_counter = variables.Variable(",
            "        0, dtype='int64', aggregation=agg)",
            "",
            "  def __setattr__(self, name, value):",
            "    if not getattr(self, '_self_setattr_tracking', True):",
            "      super(Model, self).__setattr__(name, value)",
            "      return",
            "",
            "    if all(",
            "        isinstance(v, (base_layer.Layer,",
            "                       data_structures.TrackableDataStructure)) or",
            "        base_layer_utils.has_weights(v) for v in nest.flatten(value)):",
            "      try:",
            "        self._base_model_initialized",
            "      except AttributeError:",
            "        raise RuntimeError(",
            "            'It looks like you are subclassing `Model` and you '",
            "            'forgot to call `super(YourClass, self).__init__()`.'",
            "            ' Always start with this line.')",
            "",
            "    super(Model, self).__setattr__(name, value)",
            "",
            "  @generic_utils.default",
            "  def build(self, input_shape):",
            "    \"\"\"Builds the model based on input shapes received.",
            "",
            "    This is to be used for subclassed models, which do not know at instantiation",
            "    time what their inputs look like.",
            "",
            "    This method only exists for users who want to call `model.build()` in a",
            "    standalone way (as a substitute for calling the model on real data to",
            "    build it). It will never be called by the framework (and thus it will",
            "    never throw unexpected errors in an unrelated workflow).",
            "",
            "    Args:",
            "     input_shape: Single tuple, TensorShape, or list/dict of shapes, where",
            "         shapes are tuples, integers, or TensorShapes.",
            "",
            "    Raises:",
            "      ValueError:",
            "        1. In case of invalid user-provided data (not of type tuple,",
            "           list, TensorShape, or dict).",
            "        2. If the model requires call arguments that are agnostic",
            "           to the input shapes (positional or kwarg in call signature).",
            "        3. If not all layers were properly built.",
            "        4. If float type inputs are not supported within the layers.",
            "",
            "      In each of these cases, the user should build their model by calling it",
            "      on real tensor data.",
            "    \"\"\"",
            "    if self._is_graph_network:",
            "      super(Model, self).build(input_shape)",
            "      return",
            "",
            "    if input_shape is None:",
            "      raise ValueError('Input shape must be defined when calling build on a '",
            "                       'model subclass network.')",
            "    valid_types = (tuple, list, tensor_shape.TensorShape, dict)",
            "    if not isinstance(input_shape, valid_types):",
            "      raise ValueError('Specified input shape is not one of the valid types. '",
            "                       'Please specify a batch input shape of type tuple or '",
            "                       'list of input shapes. User provided '",
            "                       'input type: {}'.format(type(input_shape)))",
            "",
            "    if input_shape and not self.inputs:",
            "      # We create placeholders for the `None`s in the shape and build the model",
            "      # in a Graph. Since tf.Variable is compatible with both eager execution",
            "      # and graph building, the variables created after building the model in",
            "      # a Graph are still valid when executing eagerly.",
            "      if context.executing_eagerly():",
            "        graph = func_graph.FuncGraph('build_graph')",
            "      else:",
            "        graph = backend.get_graph()",
            "      with graph.as_default():",
            "        if (isinstance(input_shape, list) and",
            "            all(d is None or isinstance(d, int) for d in input_shape)):",
            "          input_shape = tuple(input_shape)",
            "        if isinstance(input_shape, list):",
            "          x = [base_layer_utils.generate_placeholders_from_shape(shape)",
            "               for shape in input_shape]",
            "        elif isinstance(input_shape, dict):",
            "          x = {",
            "              k: base_layer_utils.generate_placeholders_from_shape(shape)",
            "              for k, shape in input_shape.items()",
            "          }",
            "        else:",
            "          x = base_layer_utils.generate_placeholders_from_shape(input_shape)",
            "",
            "        kwargs = {}",
            "        call_signature = self._call_full_argspec",
            "        call_args = call_signature.args",
            "        # Exclude `self`, `inputs`, and any argument with a default value.",
            "        if len(call_args) > 2:",
            "          if call_signature.defaults:",
            "            call_args = call_args[2:-len(call_signature.defaults)]",
            "          else:",
            "            call_args = call_args[2:]",
            "          for arg in call_args:",
            "            if arg == 'training':",
            "              # Case where `training` is a positional arg with no default.",
            "              kwargs['training'] = False",
            "            else:",
            "              # Has invalid call signature with unknown positional arguments.",
            "              raise ValueError(",
            "                  'Currently, you cannot build your model if it has '",
            "                  'positional or keyword arguments that are not '",
            "                  'inputs to the model, but are required for its '",
            "                  '`call` method. Instead, in order to instantiate '",
            "                  'and build your model, `call` your model on real '",
            "                  'tensor data with all expected call arguments.')",
            "        elif len(call_args) < 2:",
            "          # Signature without `inputs`.",
            "          raise ValueError('You can only call `build` on a model if its `call` '",
            "                           'method accepts an `inputs` argument.')",
            "        try:",
            "          self.call(x, **kwargs)",
            "        except (errors.InvalidArgumentError, TypeError):",
            "          raise ValueError('You cannot build your model by calling `build` '",
            "                           'if your layers do not support float type inputs. '",
            "                           'Instead, in order to instantiate and build your '",
            "                           'model, `call` your model on real tensor data (of '",
            "                           'the correct dtype).')",
            "    super(Model, self).build(input_shape)",
            "",
            "  @doc_controls.doc_in_current_and_subclasses",
            "  def call(self, inputs, training=None, mask=None):",
            "    \"\"\"Calls the model on new inputs.",
            "",
            "    In this case `call` just reapplies",
            "    all ops in the graph to the new inputs",
            "    (e.g. build a new computational graph from the provided inputs).",
            "",
            "    Note: This method should not be called directly. It is only meant to be",
            "    overridden when subclassing `tf.keras.Model`.",
            "    To call a model on an input, always use the `__call__` method,",
            "    i.e. `model(inputs)`, which relies on the underlying `call` method.",
            "",
            "    Args:",
            "        inputs: A tensor or list of tensors.",
            "        training: Boolean or boolean scalar tensor, indicating whether to run",
            "          the `Network` in training mode or inference mode.",
            "        mask: A mask or list of masks. A mask can be",
            "            either a tensor or None (no mask).",
            "",
            "    Returns:",
            "        A tensor if there is a single output, or",
            "        a list of tensors if there are more than one outputs.",
            "    \"\"\"",
            "    raise NotImplementedError('When subclassing the `Model` class, you should '",
            "                              'implement a `call` method.')",
            "",
            "  def compile(self,",
            "              optimizer='rmsprop',",
            "              loss=None,",
            "              metrics=None,",
            "              loss_weights=None,",
            "              weighted_metrics=None,",
            "              run_eagerly=None,",
            "              steps_per_execution=None,",
            "              **kwargs):",
            "    \"\"\"Configures the model for training.",
            "",
            "    Args:",
            "        optimizer: String (name of optimizer) or optimizer instance. See",
            "          `tf.keras.optimizers`.",
            "        loss: String (name of objective function), objective function or",
            "          `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective",
            "          function is any callable with the signature `loss = fn(y_true,",
            "          y_pred)`, where y_true = ground truth values with shape =",
            "          `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse",
            "          categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.",
            "          y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It",
            "          returns a weighted loss float tensor. If a custom `Loss` instance is",
            "          used and reduction is set to NONE, return value has the shape",
            "          [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;",
            "          otherwise, it is a scalar. If the model has multiple outputs, you can",
            "          use a different loss on each output by passing a dictionary or a list",
            "          of losses. The loss value that will be minimized by the model will",
            "          then be the sum of all individual losses.",
            "        metrics: List of metrics to be evaluated by the model during training",
            "          and testing. Each of this can be a string (name of a built-in",
            "          function), function or a `tf.keras.metrics.Metric` instance. See",
            "          `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A",
            "          function is any callable with the signature `result = fn(y_true,",
            "          y_pred)`. To specify different metrics for different outputs of a",
            "          multi-output model, you could also pass a dictionary, such as",
            "            `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.",
            "              You can also pass a list (len = len(outputs)) of lists of metrics",
            "              such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or",
            "              `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the",
            "              strings 'accuracy' or 'acc', we convert this to one of",
            "              `tf.keras.metrics.BinaryAccuracy`,",
            "              `tf.keras.metrics.CategoricalAccuracy`,",
            "              `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss",
            "              function used and the model output shape. We do a similar",
            "              conversion for the strings 'crossentropy' and 'ce' as well.",
            "        loss_weights: Optional list or dictionary specifying scalar coefficients",
            "          (Python floats) to weight the loss contributions of different model",
            "          outputs. The loss value that will be minimized by the model will then",
            "          be the *weighted sum* of all individual losses, weighted by the",
            "          `loss_weights` coefficients.",
            "            If a list, it is expected to have a 1:1 mapping to the model's",
            "              outputs. If a dict, it is expected to map output names (strings)",
            "              to scalar coefficients.",
            "        weighted_metrics: List of metrics to be evaluated and weighted by",
            "          sample_weight or class_weight during training and testing.",
            "        run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s",
            "          logic will not be wrapped in a `tf.function`. Recommended to leave",
            "          this as `None` unless your `Model` cannot be run inside a",
            "          `tf.function`. `run_eagerly=True` is not supported when using",
            "          `tf.distribute.experimental.ParameterServerStrategy`.",
            "        steps_per_execution: Int. Defaults to 1. The number of batches to",
            "          run during each `tf.function` call. Running multiple batches",
            "          inside a single `tf.function` call can greatly improve performance",
            "          on TPUs or small models with a large Python overhead.",
            "          At most, one full epoch will be run each",
            "          execution. If a number larger than the size of the epoch is passed,",
            "          the execution will be truncated to the size of the epoch.",
            "          Note that if `steps_per_execution` is set to `N`,",
            "          `Callback.on_batch_begin` and `Callback.on_batch_end` methods",
            "          will only be called every `N` batches",
            "          (i.e. before/after each `tf.function` execution).",
            "        **kwargs: Arguments supported for backwards compatibility only.",
            "",
            "    Raises:",
            "        ValueError: In case of invalid arguments for",
            "            `optimizer`, `loss` or `metrics`.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('compile').set(True)",
            "    with self.distribute_strategy.scope():",
            "      if 'experimental_steps_per_execution' in kwargs:",
            "        logging.warn('The argument `steps_per_execution` is no longer '",
            "                     'experimental. Pass `steps_per_execution` instead of '",
            "                     '`experimental_steps_per_execution`.')",
            "        if not steps_per_execution:",
            "          steps_per_execution = kwargs.pop('experimental_steps_per_execution')",
            "",
            "      # When compiling from an already-serialized model, we do not want to",
            "      # reapply some processing steps (e.g. metric renaming for multi-output",
            "      # models, which have prefixes added for each corresponding output name).",
            "      from_serialized = kwargs.pop('from_serialized', False)",
            "",
            "      self._validate_compile(optimizer, metrics, **kwargs)",
            "      self._run_eagerly = run_eagerly",
            "",
            "      self.optimizer = self._get_optimizer(optimizer)",
            "      self.compiled_loss = compile_utils.LossesContainer(",
            "          loss, loss_weights, output_names=self.output_names)",
            "      self.compiled_metrics = compile_utils.MetricsContainer(",
            "          metrics, weighted_metrics, output_names=self.output_names,",
            "          from_serialized=from_serialized)",
            "",
            "      self._configure_steps_per_execution(steps_per_execution or 1)",
            "",
            "      # Initializes attrs that are reset each time `compile` is called.",
            "      self._reset_compile_cache()",
            "      self._is_compiled = True",
            "",
            "      self.loss = loss or {}  # Backwards compat.",
            "",
            "  def _get_optimizer(self, optimizer):",
            "    \"\"\"Wraps `optimizer` in `LossScaleOptimizer` if necessary.\"\"\"",
            "    # The deprecated PolicyV1 has a loss_scale, which we use for backwards",
            "    # compatibility to match TF 2.3 behavior. The new Policy does not have a",
            "    # loss_scale, so we use dynamic loss scaling if the mixed_float16 policy is",
            "    # used.",
            "    if isinstance(self._dtype_policy, policy.PolicyV1):",
            "      loss_scale = self._dtype_policy.loss_scale",
            "    elif self._dtype_policy.name == 'mixed_float16':",
            "      loss_scale = 'dynamic'",
            "    else:",
            "      loss_scale = None",
            "",
            "    def _get_single_optimizer(opt):",
            "      opt = optimizers.get(opt)",
            "      if (loss_scale is not None and",
            "          not isinstance(opt, lso.LossScaleOptimizer)):",
            "        if loss_scale == 'dynamic':",
            "          opt = lso.LossScaleOptimizer(opt)",
            "        else:",
            "          opt = lso.LossScaleOptimizerV1(opt, loss_scale)",
            "      return opt",
            "",
            "    return nest.map_structure(_get_single_optimizer, optimizer)",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _reset_compile_cache(self):",
            "    self.train_function = None",
            "    self.test_function = None",
            "    self.predict_function = None",
            "",
            "    # Used to cache `trainable` attr of `Layer`s for `fit`.",
            "    self._compiled_trainable_state = self._get_trainable_state()",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _configure_steps_per_execution(self, steps_per_execution):",
            "    self._steps_per_execution = variables.Variable(",
            "        steps_per_execution,",
            "        dtype='int64',",
            "        aggregation=variables.VariableAggregationV2.ONLY_FIRST_REPLICA)",
            "",
            "  @property",
            "  def _should_compute_mask(self):",
            "    return False",
            "",
            "  @property",
            "  def metrics(self):",
            "    \"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.",
            "",
            "    Note: Metrics passed to `compile()` are available only after a `keras.Model`",
            "    has been trained/evaluated on actual data.",
            "",
            "    Examples:",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> outputs = tf.keras.layers.Dense(2)(inputs)",
            "    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])",
            "    >>> [m.name for m in model.metrics]",
            "    []",
            "",
            "    >>> x = np.random.random((2, 3))",
            "    >>> y = np.random.randint(0, 2, (2, 2))",
            "    >>> model.fit(x, y)",
            "    >>> [m.name for m in model.metrics]",
            "    ['loss', 'mae']",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> d = tf.keras.layers.Dense(2, name='out')",
            "    >>> output_1 = d(inputs)",
            "    >>> output_2 = d(inputs)",
            "    >>> model = tf.keras.models.Model(",
            "    ...    inputs=inputs, outputs=[output_1, output_2])",
            "    >>> model.add_metric(",
            "    ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])",
            "    >>> model.fit(x, (y, y))",
            "    >>> [m.name for m in model.metrics]",
            "    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',",
            "    'out_1_acc', 'mean']",
            "",
            "    \"\"\"",
            "    metrics = []",
            "    if self._is_compiled:",
            "      # TODO(omalleyt): Track `LossesContainer` and `MetricsContainer` objects",
            "      # so that attr names are not load-bearing.",
            "      if self.compiled_loss is not None:",
            "        metrics += self.compiled_loss.metrics",
            "      if self.compiled_metrics is not None:",
            "        metrics += self.compiled_metrics.metrics",
            "",
            "    for l in self._flatten_layers():",
            "      metrics.extend(l._metrics)  # pylint: disable=protected-access",
            "    return metrics",
            "",
            "  @property",
            "  def metrics_names(self):",
            "    \"\"\"Returns the model's display labels for all outputs.",
            "",
            "    Note: `metrics_names` are available only after a `keras.Model` has been",
            "    trained/evaluated on actual data.",
            "",
            "    Examples:",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> outputs = tf.keras.layers.Dense(2)(inputs)",
            "    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])",
            "    >>> model.metrics_names",
            "    []",
            "",
            "    >>> x = np.random.random((2, 3))",
            "    >>> y = np.random.randint(0, 2, (2, 2))",
            "    >>> model.fit(x, y)",
            "    >>> model.metrics_names",
            "    ['loss', 'mae']",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> d = tf.keras.layers.Dense(2, name='out')",
            "    >>> output_1 = d(inputs)",
            "    >>> output_2 = d(inputs)",
            "    >>> model = tf.keras.models.Model(",
            "    ...    inputs=inputs, outputs=[output_1, output_2])",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])",
            "    >>> model.fit(x, (y, y))",
            "    >>> model.metrics_names",
            "    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',",
            "    'out_1_acc']",
            "",
            "    \"\"\"",
            "",
            "    # This property includes all output names including `loss` and per-output",
            "    # losses for backward compatibility.",
            "    return [m.name for m in self.metrics]",
            "",
            "  @property",
            "  def distribute_strategy(self):",
            "    \"\"\"The `tf.distribute.Strategy` this model was created under.\"\"\"",
            "    return self._distribution_strategy or ds_context.get_strategy()",
            "",
            "  @property",
            "  def run_eagerly(self):",
            "    \"\"\"Settable attribute indicating whether the model should run eagerly.",
            "",
            "    Running eagerly means that your model will be run step by step,",
            "    like Python code. Your model might run slower, but it should become easier",
            "    for you to debug it by stepping into individual layer calls.",
            "",
            "    By default, we will attempt to compile your model to a static graph to",
            "    deliver the best execution performance.",
            "",
            "    Returns:",
            "      Boolean, whether the model should run eagerly.",
            "    \"\"\"",
            "    if self.dynamic and self._run_eagerly is False:  # pylint:disable=g-bool-id-comparison",
            "      # TODO(fchollet): consider using py_func to enable this.",
            "      raise ValueError('Your model contains layers that can only be '",
            "                       'successfully run in eager execution (layers '",
            "                       'constructed with `dynamic=True`). '",
            "                       'You cannot set `run_eagerly=False`.')",
            "",
            "    if self._cluster_coordinator and self._run_eagerly:",
            "      raise ValueError('When using `Model` with `ParameterServerStrategy`, '",
            "                       '`run_eagerly` is not supported.')",
            "",
            "    # Run eagerly logic, by priority:",
            "    # (1) Dynamic models must be run eagerly.",
            "    # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.",
            "    # (3) Not explicitly setting run_eagerly defaults to TF's global setting.",
            "    return (self.dynamic or self._run_eagerly or",
            "            (def_function.functions_run_eagerly() and",
            "             self._run_eagerly is None))",
            "",
            "  @run_eagerly.setter",
            "  def run_eagerly(self, value):",
            "    self._run_eagerly = value",
            "",
            "  def train_step(self, data):",
            "    \"\"\"The logic for one training step.",
            "",
            "    This method can be overridden to support custom training logic.",
            "    This method is called by `Model.make_train_function`.",
            "",
            "    This method should contain the mathematical logic for one step of training.",
            "    This typically includes the forward pass, loss calculation, backpropagation,",
            "    and metric updates.",
            "",
            "    Configuration details for *how* this logic is run (e.g. `tf.function` and",
            "    `tf.distribute.Strategy` settings), should be left to",
            "    `Model.make_train_function`, which can also be overridden.",
            "",
            "    Args:",
            "      data: A nested structure of `Tensor`s.",
            "",
            "    Returns:",
            "      A `dict` containing values that will be passed to",
            "      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the",
            "      values of the `Model`'s metrics are returned. Example:",
            "      `{'loss': 0.2, 'accuracy': 0.7}`.",
            "",
            "    \"\"\"",
            "    # These are the only transformations `Model.fit` applies to user-input",
            "    # data when a `tf.data.Dataset` is provided.",
            "    data = data_adapter.expand_1d(data)",
            "    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)",
            "    # Run forward pass.",
            "    with backprop.GradientTape() as tape:",
            "      y_pred = self(x, training=True)",
            "      loss = self.compiled_loss(",
            "          y, y_pred, sample_weight, regularization_losses=self.losses)",
            "    # Run backwards pass.",
            "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)",
            "    self.compiled_metrics.update_state(y, y_pred, sample_weight)",
            "    # Collect metrics to return",
            "    return_metrics = {}",
            "    for metric in self.metrics:",
            "      result = metric.result()",
            "      if isinstance(result, dict):",
            "        return_metrics.update(result)",
            "      else:",
            "        return_metrics[metric.name] = result",
            "    return return_metrics",
            "",
            "  def make_train_function(self):",
            "    \"\"\"Creates a function that executes one step of training.",
            "",
            "    This method can be overridden to support custom training logic.",
            "    This method is called by `Model.fit` and `Model.train_on_batch`.",
            "",
            "    Typically, this method directly controls `tf.function` and",
            "    `tf.distribute.Strategy` settings, and delegates the actual training",
            "    logic to `Model.train_step`.",
            "",
            "    This function is cached the first time `Model.fit` or",
            "    `Model.train_on_batch` is called. The cache is cleared whenever",
            "    `Model.compile` is called.",
            "",
            "    Returns:",
            "      Function. The function created by this method should accept a",
            "      `tf.data.Iterator`, and return a `dict` containing values that will",
            "      be passed to `tf.keras.Callbacks.on_train_batch_end`, such as",
            "      `{'loss': 0.2, 'accuracy': 0.7}`.",
            "    \"\"\"",
            "    if self.train_function is not None:",
            "      return self.train_function",
            "",
            "    def step_function(model, iterator):",
            "      \"\"\"Runs a single training step.\"\"\"",
            "",
            "      def run_step(data):",
            "        outputs = model.train_step(data)",
            "        # Ensure counter is updated only if `train_step` succeeds.",
            "        with ops.control_dependencies(_minimum_control_deps(outputs)):",
            "          model._train_counter.assign_add(1)  # pylint: disable=protected-access",
            "        return outputs",
            "",
            "      data = next(iterator)",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))",
            "      outputs = reduce_per_replica(",
            "          outputs, self.distribute_strategy, reduction='first')",
            "      write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access",
            "      return outputs",
            "",
            "    if self._steps_per_execution.numpy().item() == 1:",
            "",
            "      def train_function(iterator):",
            "        \"\"\"Runs a training execution with one step.\"\"\"",
            "        return step_function(self, iterator)",
            "",
            "    else:",
            "",
            "      def train_function(iterator):",
            "        \"\"\"Runs a training execution with multiple steps.\"\"\"",
            "        for _ in math_ops.range(self._steps_per_execution):",
            "          outputs = step_function(self, iterator)",
            "        return outputs",
            "",
            "    if not self.run_eagerly:",
            "      train_function = def_function.function(",
            "          train_function, experimental_relax_shapes=True)",
            "",
            "    self.train_function = train_function",
            "",
            "    if self._cluster_coordinator:",
            "      self.train_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda",
            "          train_function, args=(iterator,))",
            "",
            "    return self.train_function",
            "",
            "  def fit(self,",
            "          x=None,",
            "          y=None,",
            "          batch_size=None,",
            "          epochs=1,",
            "          verbose='auto',",
            "          callbacks=None,",
            "          validation_split=0.,",
            "          validation_data=None,",
            "          shuffle=True,",
            "          class_weight=None,",
            "          sample_weight=None,",
            "          initial_epoch=0,",
            "          steps_per_epoch=None,",
            "          validation_steps=None,",
            "          validation_batch_size=None,",
            "          validation_freq=1,",
            "          max_queue_size=10,",
            "          workers=1,",
            "          use_multiprocessing=False):",
            "    \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "            (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "            (in case the model has multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors,",
            "            if the model has named inputs.",
            "          - A `tf.data` dataset. Should return a tuple",
            "            of either `(inputs, targets)` or",
            "            `(inputs, targets, sample_weights)`.",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`",
            "            or `(inputs, targets, sample_weights)`.",
            "          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a",
            "            callable that takes a single argument of type",
            "            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.",
            "            `DatasetCreator` should be used when users prefer to specify the",
            "            per-replica batching and sharding logic for the `Dataset`.",
            "            See `tf.keras.utils.experimental.DatasetCreator` doc for more",
            "            information.",
            "          A more detailed description of unpacking behavior for iterator types",
            "          (Dataset, generator, Sequence) is given below. If using",
            "          `tf.distribute.experimental.ParameterServerStrategy`, only",
            "          `DatasetCreator` type is supported for `x`.",
            "        y: Target data. Like the input data `x`,",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).",
            "          It should be consistent with `x` (you cannot have Numpy inputs and",
            "          tensor targets, or inversely). If `x` is a dataset, generator,",
            "          or `keras.utils.Sequence` instance, `y` should",
            "          not be specified (since targets will be obtained from `x`).",
            "        batch_size: Integer or `None`.",
            "            Number of samples per gradient update.",
            "            If unspecified, `batch_size` will default to 32.",
            "            Do not specify the `batch_size` if your data is in the",
            "            form of datasets, generators, or `keras.utils.Sequence` instances",
            "            (since they generate batches).",
            "        epochs: Integer. Number of epochs to train the model.",
            "            An epoch is an iteration over the entire `x` and `y`",
            "            data provided.",
            "            Note that in conjunction with `initial_epoch`,",
            "            `epochs` is to be understood as \"final epoch\".",
            "            The model is not trained for a number of iterations",
            "            given by `epochs`, but merely until the epoch",
            "            of index `epochs` is reached.",
            "        verbose: 'auto', 0, 1, or 2. Verbosity mode.",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.",
            "            'auto' defaults to 1 for most cases, but 2 when used with",
            "            `ParameterServerStrategy`. Note that the progress bar is not",
            "            particularly useful when logged to a file, so verbose=2 is",
            "            recommended when not running interactively (eg, in a production",
            "            environment).",
            "        callbacks: List of `keras.callbacks.Callback` instances.",
            "            List of callbacks to apply during training.",
            "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`",
            "            and `tf.keras.callbacks.History` callbacks are created automatically",
            "            and need not be passed into `model.fit`.",
            "            `tf.keras.callbacks.ProgbarLogger` is created or not based on",
            "            `verbose` argument to `model.fit`.",
            "            Callbacks with batch-level calls are currently unsupported with",
            "            `tf.distribute.experimental.ParameterServerStrategy`, and users are",
            "            advised to implement epoch-level calls instead with an appropriate",
            "            `steps_per_epoch` value.",
            "        validation_split: Float between 0 and 1.",
            "            Fraction of the training data to be used as validation data.",
            "            The model will set apart this fraction of the training data,",
            "            will not train on it, and will evaluate",
            "            the loss and any model metrics",
            "            on this data at the end of each epoch.",
            "            The validation data is selected from the last samples",
            "            in the `x` and `y` data provided, before shuffling. This argument is",
            "            not supported when `x` is a dataset, generator or",
            "           `keras.utils.Sequence` instance.",
            "            `validation_split` is not yet supported with",
            "            `tf.distribute.experimental.ParameterServerStrategy`.",
            "        validation_data: Data on which to evaluate",
            "            the loss and any model metrics at the end of each epoch.",
            "            The model will not be trained on this data. Thus, note the fact",
            "            that the validation loss of data provided using `validation_split`",
            "            or `validation_data` is not affected by regularization layers like",
            "            noise and dropout.",
            "            `validation_data` will override `validation_split`.",
            "            `validation_data` could be:",
            "              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.",
            "              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.",
            "              - A `tf.data.Dataset`.",
            "              - A Python generator or `keras.utils.Sequence` returning",
            "              `(inputs, targets)` or `(inputs, targets, sample_weights)`.",
            "            `validation_data` is not yet supported with",
            "            `tf.distribute.experimental.ParameterServerStrategy`.",
            "        shuffle: Boolean (whether to shuffle the training data",
            "            before each epoch) or str (for 'batch'). This argument is ignored",
            "            when `x` is a generator or an object of tf.data.Dataset.",
            "            'batch' is a special option for dealing",
            "            with the limitations of HDF5 data; it shuffles in batch-sized",
            "            chunks. Has no effect when `steps_per_epoch` is not `None`.",
            "        class_weight: Optional dictionary mapping class indices (integers)",
            "            to a weight (float) value, used for weighting the loss function",
            "            (during training only).",
            "            This can be useful to tell the model to",
            "            \"pay more attention\" to samples from",
            "            an under-represented class.",
            "        sample_weight: Optional Numpy array of weights for",
            "            the training samples, used for weighting the loss function",
            "            (during training only). You can either pass a flat (1D)",
            "            Numpy array with the same length as the input samples",
            "            (1:1 mapping between weights and samples),",
            "            or in the case of temporal data,",
            "            you can pass a 2D array with shape",
            "            `(samples, sequence_length)`,",
            "            to apply a different weight to every timestep of every sample. This",
            "            argument is not supported when `x` is a dataset, generator, or",
            "           `keras.utils.Sequence` instance, instead provide the sample_weights",
            "            as the third element of `x`.",
            "        initial_epoch: Integer.",
            "            Epoch at which to start training",
            "            (useful for resuming a previous training run).",
            "        steps_per_epoch: Integer or `None`.",
            "            Total number of steps (batches of samples)",
            "            before declaring one epoch finished and starting the",
            "            next epoch. When training with input tensors such as",
            "            TensorFlow data tensors, the default `None` is equal to",
            "            the number of samples in your dataset divided by",
            "            the batch size, or 1 if that cannot be determined. If x is a",
            "            `tf.data` dataset, and 'steps_per_epoch'",
            "            is None, the epoch will run until the input dataset is exhausted.",
            "            When passing an infinitely repeating dataset, you must specify the",
            "            `steps_per_epoch` argument. This argument is not supported with",
            "            array inputs. `steps_per_epoch=None` is not supported when using",
            "            `tf.distribute.experimental.ParameterServerStrategy`.",
            "        validation_steps: Only relevant if `validation_data` is provided and",
            "            is a `tf.data` dataset. Total number of steps (batches of",
            "            samples) to draw before stopping when performing validation",
            "            at the end of every epoch. If 'validation_steps' is None, validation",
            "            will run until the `validation_data` dataset is exhausted. In the",
            "            case of an infinitely repeated dataset, it will run into an",
            "            infinite loop. If 'validation_steps' is specified and only part of",
            "            the dataset will be consumed, the evaluation will start from the",
            "            beginning of the dataset at each epoch. This ensures that the same",
            "            validation samples are used every time.",
            "        validation_batch_size: Integer or `None`.",
            "            Number of samples per validation batch.",
            "            If unspecified, will default to `batch_size`.",
            "            Do not specify the `validation_batch_size` if your data is in the",
            "            form of datasets, generators, or `keras.utils.Sequence` instances",
            "            (since they generate batches).",
            "        validation_freq: Only relevant if validation data is provided. Integer",
            "            or `collections.abc.Container` instance (e.g. list, tuple, etc.).",
            "            If an integer, specifies how many training epochs to run before a",
            "            new validation run is performed, e.g. `validation_freq=2` runs",
            "            validation every 2 epochs. If a Container, specifies the epochs on",
            "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs",
            "            validation at the end of the 1st, 2nd, and 10th epochs.",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`",
            "            input only. Maximum size for the generator queue.",
            "            If unspecified, `max_queue_size` will default to 10.",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input",
            "            only. Maximum number of processes to spin up",
            "            when using process-based threading. If unspecified, `workers`",
            "            will default to 1.",
            "        use_multiprocessing: Boolean. Used for generator or",
            "            `keras.utils.Sequence` input only. If `True`, use process-based",
            "            threading. If unspecified, `use_multiprocessing` will default to",
            "            `False`. Note that because this implementation relies on",
            "            multiprocessing, you should not pass non-picklable arguments to",
            "            the generator as they can't be passed easily to children processes.",
            "",
            "    Unpacking behavior for iterator-like inputs:",
            "        A common pattern is to pass a tf.data.Dataset, generator, or",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact",
            "      yield not only features (x) but optionally targets (y) and sample weights.",
            "      Keras requires that the output of such iterator-likes be unambiguous. The",
            "      iterator should return a tuple of length 1, 2, or 3, where the optional",
            "      second and third elements will be used for y and sample_weight",
            "      respectively. Any other type provided will be wrapped in a length one",
            "      tuple, effectively treating everything as 'x'. When yielding dicts, they",
            "      should still adhere to the top-level tuple structure.",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate",
            "      features, targets, and weights from the keys of a single dict.",
            "        A notable unsupported data type is the namedtuple. The reason is that",
            "      it behaves like both an ordered datatype (tuple) and a mapping",
            "      datatype (dict). So given a namedtuple of the form:",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`",
            "      it is ambiguous whether to reverse the order of the elements when",
            "      interpreting the value. Even worse is a tuple of the form:",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`",
            "      where it is unclear if the tuple was intended to be unpacked into x, y,",
            "      and sample_weight or passed through as a single element to `x`. As a",
            "      result the data processing code will simply raise a ValueError if it",
            "      encounters a namedtuple. (Along with instructions to remedy the issue.)",
            "",
            "    Returns:",
            "        A `History` object. Its `History.history` attribute is",
            "        a record of training loss values and metrics values",
            "        at successive epochs, as well as validation loss values",
            "        and validation metrics values (if applicable).",
            "",
            "    Raises:",
            "        RuntimeError: 1. If the model was never compiled or,",
            "        2. If `model.fit` is  wrapped in `tf.function`.",
            "",
            "        ValueError: In case of mismatch between the provided input data",
            "            and what the model expects or when the input data is empty.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('fit').set(True)",
            "    # Legacy graph support is contained in `training_v1.Model`.",
            "    version_utils.disallow_legacy_graph('Model', 'fit')",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('fit')",
            "    _disallow_inside_tf_function('fit')",
            "",
            "    if verbose == 'auto':",
            "      if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "        verbose = 2  # Default to epoch-level logging for PSStrategy.",
            "      else:",
            "        verbose = 1  # Default to batch-level logging otherwise.",
            "",
            "    if validation_split:",
            "      # Create the validation data using the training data. Only supported for",
            "      # `Tensor` and `NumPy` input.",
            "      (x, y, sample_weight), validation_data = (",
            "          data_adapter.train_validation_split(",
            "              (x, y, sample_weight), validation_split=validation_split))",
            "",
            "    if validation_data:",
            "      val_x, val_y, val_sample_weight = (",
            "          data_adapter.unpack_x_y_sample_weight(validation_data))",
            "",
            "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "      self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(",
            "          self.distribute_strategy)",
            "",
            "    with self.distribute_strategy.scope(), \\",
            "         training_utils.RespectCompiledTrainableState(self):",
            "      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.",
            "      data_handler = data_adapter.get_data_handler(",
            "          x=x,",
            "          y=y,",
            "          sample_weight=sample_weight,",
            "          batch_size=batch_size,",
            "          steps_per_epoch=steps_per_epoch,",
            "          initial_epoch=initial_epoch,",
            "          epochs=epochs,",
            "          shuffle=shuffle,",
            "          class_weight=class_weight,",
            "          max_queue_size=max_queue_size,",
            "          workers=workers,",
            "          use_multiprocessing=use_multiprocessing,",
            "          model=self,",
            "          steps_per_execution=self._steps_per_execution)",
            "",
            "      # Container that configures and calls `tf.keras.Callback`s.",
            "      if not isinstance(callbacks, callbacks_module.CallbackList):",
            "        callbacks = callbacks_module.CallbackList(",
            "            callbacks,",
            "            add_history=True,",
            "            add_progbar=verbose != 0,",
            "            model=self,",
            "            verbose=verbose,",
            "            epochs=epochs,",
            "            steps=data_handler.inferred_steps)",
            "",
            "      self.stop_training = False",
            "      self.train_function = self.make_train_function()",
            "      self._train_counter.assign(0)",
            "      callbacks.on_train_begin()",
            "      training_logs = None",
            "      # Handle fault-tolerance for multi-worker.",
            "      # TODO(omalleyt): Fix the ordering issues that mean this has to",
            "      # happen after `callbacks.on_train_begin`.",
            "      data_handler._initial_epoch = (  # pylint: disable=protected-access",
            "          self._maybe_load_initial_epoch_from_ckpt(initial_epoch))",
            "      logs = None",
            "      for epoch, iterator in data_handler.enumerate_epochs():",
            "        self.reset_metrics()",
            "        callbacks.on_epoch_begin(epoch)",
            "        with data_handler.catch_stop_iteration():",
            "          for step in data_handler.steps():",
            "            with trace.Trace(",
            "                'train',",
            "                epoch_num=epoch,",
            "                step_num=step,",
            "                batch_size=batch_size,",
            "                _r=1):",
            "              callbacks.on_train_batch_begin(step)",
            "              tmp_logs = self.train_function(iterator)",
            "              if data_handler.should_sync:",
            "                context.async_wait()",
            "              logs = tmp_logs  # No error, now safe to assign to logs.",
            "              end_step = step + data_handler.step_increment",
            "              callbacks.on_train_batch_end(end_step, logs)",
            "              if self.stop_training:",
            "                break",
            "",
            "        logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "        if logs is None:",
            "          raise ValueError('Expect x to be a non-empty array or dataset.')",
            "        epoch_logs = copy.copy(logs)",
            "",
            "        # Run validation.",
            "        if validation_data and self._should_eval(epoch, validation_freq):",
            "          # Create data_handler for evaluation and cache it.",
            "          if getattr(self, '_eval_data_handler', None) is None:",
            "            self._eval_data_handler = data_adapter.get_data_handler(",
            "                x=val_x,",
            "                y=val_y,",
            "                sample_weight=val_sample_weight,",
            "                batch_size=validation_batch_size or batch_size,",
            "                steps_per_epoch=validation_steps,",
            "                initial_epoch=0,",
            "                epochs=1,",
            "                max_queue_size=max_queue_size,",
            "                workers=workers,",
            "                use_multiprocessing=use_multiprocessing,",
            "                model=self,",
            "                steps_per_execution=self._steps_per_execution)",
            "          val_logs = self.evaluate(",
            "              x=val_x,",
            "              y=val_y,",
            "              sample_weight=val_sample_weight,",
            "              batch_size=validation_batch_size or batch_size,",
            "              steps=validation_steps,",
            "              callbacks=callbacks,",
            "              max_queue_size=max_queue_size,",
            "              workers=workers,",
            "              use_multiprocessing=use_multiprocessing,",
            "              return_dict=True,",
            "              _use_cached_eval_dataset=True)",
            "          val_logs = {'val_' + name: val for name, val in val_logs.items()}",
            "          epoch_logs.update(val_logs)",
            "",
            "        callbacks.on_epoch_end(epoch, epoch_logs)",
            "        training_logs = epoch_logs",
            "        if self.stop_training:",
            "          break",
            "",
            "      # If eval data_hanlder exists, delete it after all epochs are done.",
            "      if getattr(self, '_eval_data_handler', None) is not None:",
            "        del self._eval_data_handler",
            "      callbacks.on_train_end(logs=training_logs)",
            "      return self.history",
            "",
            "  def test_step(self, data):",
            "    \"\"\"The logic for one evaluation step.",
            "",
            "    This method can be overridden to support custom evaluation logic.",
            "    This method is called by `Model.make_test_function`.",
            "",
            "    This function should contain the mathematical logic for one step of",
            "    evaluation.",
            "    This typically includes the forward pass, loss calculation, and metrics",
            "    updates.",
            "",
            "    Configuration details for *how* this logic is run (e.g. `tf.function` and",
            "    `tf.distribute.Strategy` settings), should be left to",
            "    `Model.make_test_function`, which can also be overridden.",
            "",
            "    Args:",
            "      data: A nested structure of `Tensor`s.",
            "",
            "    Returns:",
            "      A `dict` containing values that will be passed to",
            "      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the",
            "      values of the `Model`'s metrics are returned.",
            "    \"\"\"",
            "    data = data_adapter.expand_1d(data)",
            "    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)",
            "",
            "    y_pred = self(x, training=False)",
            "    # Updates stateful loss metrics.",
            "    self.compiled_loss(",
            "        y, y_pred, sample_weight, regularization_losses=self.losses)",
            "    self.compiled_metrics.update_state(y, y_pred, sample_weight)",
            "    # Collect metrics to return",
            "    return_metrics = {}",
            "    for metric in self.metrics:",
            "      result = metric.result()",
            "      if isinstance(result, dict):",
            "        return_metrics.update(result)",
            "      else:",
            "        return_metrics[metric.name] = result",
            "    return return_metrics",
            "",
            "  def make_test_function(self):",
            "    \"\"\"Creates a function that executes one step of evaluation.",
            "",
            "    This method can be overridden to support custom evaluation logic.",
            "    This method is called by `Model.evaluate` and `Model.test_on_batch`.",
            "",
            "    Typically, this method directly controls `tf.function` and",
            "    `tf.distribute.Strategy` settings, and delegates the actual evaluation",
            "    logic to `Model.test_step`.",
            "",
            "    This function is cached the first time `Model.evaluate` or",
            "    `Model.test_on_batch` is called. The cache is cleared whenever",
            "    `Model.compile` is called.",
            "",
            "    Returns:",
            "      Function. The function created by this method should accept a",
            "      `tf.data.Iterator`, and return a `dict` containing values that will",
            "      be passed to `tf.keras.Callbacks.on_test_batch_end`.",
            "    \"\"\"",
            "    if self.test_function is not None:",
            "      return self.test_function",
            "",
            "    def step_function(model, iterator):",
            "      \"\"\"Runs a single evaluation step.\"\"\"",
            "",
            "      def run_step(data):",
            "        outputs = model.test_step(data)",
            "        # Ensure counter is updated only if `test_step` succeeds.",
            "        with ops.control_dependencies(_minimum_control_deps(outputs)):",
            "          model._test_counter.assign_add(1)  # pylint: disable=protected-access",
            "        return outputs",
            "",
            "      data = next(iterator)",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))",
            "      outputs = reduce_per_replica(",
            "          outputs, self.distribute_strategy, reduction='first')",
            "      return outputs",
            "",
            "    if self._steps_per_execution.numpy().item() == 1:",
            "",
            "      def test_function(iterator):",
            "        \"\"\"Runs an evaluation execution with one step.\"\"\"",
            "        return step_function(self, iterator)",
            "",
            "    else:",
            "",
            "      def test_function(iterator):",
            "        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"",
            "        for _ in math_ops.range(self._steps_per_execution):",
            "          outputs = step_function(self, iterator)",
            "        return outputs",
            "",
            "    if not self.run_eagerly:",
            "      test_function = def_function.function(",
            "          test_function, experimental_relax_shapes=True)",
            "",
            "    self.test_function = test_function",
            "    return self.test_function",
            "",
            "  def evaluate(self,",
            "               x=None,",
            "               y=None,",
            "               batch_size=None,",
            "               verbose=1,",
            "               sample_weight=None,",
            "               steps=None,",
            "               callbacks=None,",
            "               max_queue_size=10,",
            "               workers=1,",
            "               use_multiprocessing=False,",
            "               return_dict=False,",
            "               **kwargs):",
            "    \"\"\"Returns the loss value & metrics values for the model in test mode.",
            "",
            "    Computation is done in batches (see the `batch_size` arg.)",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "            (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "            (in case the model has multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors,",
            "            if the model has named inputs.",
            "          - A `tf.data` dataset. Should return a tuple",
            "            of either `(inputs, targets)` or",
            "            `(inputs, targets, sample_weights)`.",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`",
            "            or `(inputs, targets, sample_weights)`.",
            "          A more detailed description of unpacking behavior for iterator types",
            "          (Dataset, generator, Sequence) is given in the `Unpacking behavior",
            "          for iterator-like inputs` section of `Model.fit`.",
            "        y: Target data. Like the input data `x`, it could be either Numpy",
            "          array(s) or TensorFlow tensor(s). It should be consistent with `x`",
            "          (you cannot have Numpy inputs and tensor targets, or inversely). If",
            "          `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`",
            "          should not be specified (since targets will be obtained from the",
            "          iterator/dataset).",
            "        batch_size: Integer or `None`. Number of samples per batch of",
            "          computation. If unspecified, `batch_size` will default to 32. Do not",
            "          specify the `batch_size` if your data is in the form of a dataset,",
            "          generators, or `keras.utils.Sequence` instances (since they generate",
            "          batches).",
            "        verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.",
            "        sample_weight: Optional Numpy array of weights for the test samples,",
            "          used for weighting the loss function. You can either pass a flat (1D)",
            "          Numpy array with the same length as the input samples",
            "            (1:1 mapping between weights and samples), or in the case of",
            "              temporal data, you can pass a 2D array with shape `(samples,",
            "              sequence_length)`, to apply a different weight to every timestep",
            "              of every sample. This argument is not supported when `x` is a",
            "              dataset, instead pass sample weights as the third element of `x`.",
            "        steps: Integer or `None`. Total number of steps (batches of samples)",
            "          before declaring the evaluation round finished. Ignored with the",
            "          default value of `None`. If x is a `tf.data` dataset and `steps` is",
            "          None, 'evaluate' will run until the dataset is exhausted. This",
            "          argument is not supported with array inputs.",
            "        callbacks: List of `keras.callbacks.Callback` instances. List of",
            "          callbacks to apply during evaluation. See",
            "          [callbacks](/api_docs/python/tf/keras/callbacks).",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`",
            "          input only. Maximum size for the generator queue. If unspecified,",
            "          `max_queue_size` will default to 10.",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input",
            "          only. Maximum number of processes to spin up when using process-based",
            "          threading. If unspecified, `workers` will default to 1.",
            "        use_multiprocessing: Boolean. Used for generator or",
            "          `keras.utils.Sequence` input only. If `True`, use process-based",
            "          threading. If unspecified, `use_multiprocessing` will default to",
            "          `False`. Note that because this implementation relies on",
            "          multiprocessing, you should not pass non-picklable arguments to the",
            "          generator as they can't be passed easily to children processes.",
            "        return_dict: If `True`, loss and metric results are returned as a dict,",
            "          with each key being the name of the metric. If `False`, they are",
            "          returned as a list.",
            "        **kwargs: Unused at this time.",
            "",
            "    See the discussion of `Unpacking behavior for iterator-like inputs` for",
            "    `Model.fit`.",
            "",
            "    `Model.evaluate` is not yet supported with",
            "    `tf.distribute.experimental.ParameterServerStrategy`.",
            "",
            "    Returns:",
            "        Scalar test loss (if the model has a single output and no metrics)",
            "        or list of scalars (if the model has multiple outputs",
            "        and/or metrics). The attribute `model.metrics_names` will give you",
            "        the display labels for the scalar outputs.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.evaluate` is wrapped in `tf.function`.",
            "        ValueError: in case of invalid arguments.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('evaluate').set(True)",
            "    version_utils.disallow_legacy_graph('Model', 'evaluate')",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('evaluate')",
            "    _disallow_inside_tf_function('evaluate')",
            "    use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)",
            "    if kwargs:",
            "      raise TypeError('Invalid keyword arguments: %s' % (kwargs,))",
            "",
            "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "      raise NotImplementedError('`model.evaluate` is not yet supported with '",
            "                                '`ParameterServerStrategy`.')",
            "",
            "    with self.distribute_strategy.scope():",
            "      # Use cached evaluation data only when it's called in `Model.fit`",
            "      if (use_cached_eval_dataset",
            "          and getattr(self, '_eval_data_handler', None) is not None):",
            "        data_handler = self._eval_data_handler",
            "      else:",
            "        # Creates a `tf.data.Dataset` and handles batch and epoch iteration.",
            "        data_handler = data_adapter.get_data_handler(",
            "            x=x,",
            "            y=y,",
            "            sample_weight=sample_weight,",
            "            batch_size=batch_size,",
            "            steps_per_epoch=steps,",
            "            initial_epoch=0,",
            "            epochs=1,",
            "            max_queue_size=max_queue_size,",
            "            workers=workers,",
            "            use_multiprocessing=use_multiprocessing,",
            "            model=self,",
            "            steps_per_execution=self._steps_per_execution)",
            "",
            "      # Container that configures and calls `tf.keras.Callback`s.",
            "      if not isinstance(callbacks, callbacks_module.CallbackList):",
            "        callbacks = callbacks_module.CallbackList(",
            "            callbacks,",
            "            add_history=True,",
            "            add_progbar=verbose != 0,",
            "            model=self,",
            "            verbose=verbose,",
            "            epochs=1,",
            "            steps=data_handler.inferred_steps)",
            "",
            "      logs = {}",
            "      self.test_function = self.make_test_function()",
            "      self._test_counter.assign(0)",
            "      callbacks.on_test_begin()",
            "      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.",
            "        self.reset_metrics()",
            "        with data_handler.catch_stop_iteration():",
            "          for step in data_handler.steps():",
            "            with trace.Trace('test', step_num=step, _r=1):",
            "              callbacks.on_test_batch_begin(step)",
            "              tmp_logs = self.test_function(iterator)",
            "              if data_handler.should_sync:",
            "                context.async_wait()",
            "              logs = tmp_logs  # No error, now safe to assign to logs.",
            "              end_step = step + data_handler.step_increment",
            "              callbacks.on_test_batch_end(end_step, logs)",
            "      logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "      callbacks.on_test_end(logs=logs)",
            "",
            "      if return_dict:",
            "        return logs",
            "      else:",
            "        return flatten_metrics_in_order(logs, self.metrics_names)",
            "",
            "  def predict_step(self, data):",
            "    \"\"\"The logic for one inference step.",
            "",
            "    This method can be overridden to support custom inference logic.",
            "    This method is called by `Model.make_predict_function`.",
            "",
            "    This method should contain the mathematical logic for one step of inference.",
            "    This typically includes the forward pass.",
            "",
            "    Configuration details for *how* this logic is run (e.g. `tf.function` and",
            "    `tf.distribute.Strategy` settings), should be left to",
            "    `Model.make_predict_function`, which can also be overridden.",
            "",
            "    Args:",
            "      data: A nested structure of `Tensor`s.",
            "",
            "    Returns:",
            "      The result of one inference step, typically the output of calling the",
            "      `Model` on data.",
            "    \"\"\"",
            "    data = data_adapter.expand_1d(data)",
            "    x, _, _ = data_adapter.unpack_x_y_sample_weight(data)",
            "    return self(x, training=False)",
            "",
            "  def make_predict_function(self):",
            "    \"\"\"Creates a function that executes one step of inference.",
            "",
            "    This method can be overridden to support custom inference logic.",
            "    This method is called by `Model.predict` and `Model.predict_on_batch`.",
            "",
            "    Typically, this method directly controls `tf.function` and",
            "    `tf.distribute.Strategy` settings, and delegates the actual evaluation",
            "    logic to `Model.predict_step`.",
            "",
            "    This function is cached the first time `Model.predict` or",
            "    `Model.predict_on_batch` is called. The cache is cleared whenever",
            "    `Model.compile` is called.",
            "",
            "    Returns:",
            "      Function. The function created by this method should accept a",
            "      `tf.data.Iterator`, and return the outputs of the `Model`.",
            "    \"\"\"",
            "    if self.predict_function is not None:",
            "      return self.predict_function",
            "",
            "    def step_function(model, iterator):",
            "      \"\"\"Runs a single evaluation step.\"\"\"",
            "",
            "      def run_step(data):",
            "        outputs = model.predict_step(data)",
            "        # Ensure counter is updated only if `test_step` succeeds.",
            "        with ops.control_dependencies(_minimum_control_deps(outputs)):",
            "          model._predict_counter.assign_add(1)  # pylint: disable=protected-access",
            "        return outputs",
            "",
            "      data = next(iterator)",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))",
            "      outputs = reduce_per_replica(",
            "          outputs, self.distribute_strategy, reduction='concat')",
            "      return outputs",
            "",
            "    if (self._steps_per_execution is None or",
            "        self._steps_per_execution.numpy().item() == 1):",
            "",
            "      def predict_function(iterator):",
            "        \"\"\"Runs an evaluation execution with one step.\"\"\"",
            "        return step_function(self, iterator)",
            "",
            "    else:",
            "",
            "      def predict_function(iterator):",
            "        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"",
            "        outputs = step_function(self, iterator)",
            "        for _ in math_ops.range(self._steps_per_execution - 1):",
            "          directives.set_loop_options(",
            "              shape_invariants=[(",
            "                  t, tf_utils.get_tensor_spec(t, dynamic_batch=True).shape)",
            "                                for t in nest.flatten(outputs)])",
            "          step_outputs = step_function(self, iterator)",
            "          outputs = nest.map_structure(lambda t1, t2: concat([t1, t2]), outputs,",
            "                                       step_outputs)",
            "        return outputs",
            "",
            "    if not self.run_eagerly:",
            "      predict_function = def_function.function(",
            "          predict_function, experimental_relax_shapes=True)",
            "",
            "    self.predict_function = predict_function",
            "    return self.predict_function",
            "",
            "  def predict(self,",
            "              x,",
            "              batch_size=None,",
            "              verbose=0,",
            "              steps=None,",
            "              callbacks=None,",
            "              max_queue_size=10,",
            "              workers=1,",
            "              use_multiprocessing=False):",
            "    \"\"\"Generates output predictions for the input samples.",
            "",
            "    Computation is done in batches. This method is designed for performance in",
            "    large scale inputs. For small amount of inputs that fit in one batch,",
            "    directly using `__call__` is recommended for faster execution, e.g.,",
            "    `model(x)`, or `model(x, training=False)` if you have layers such as",
            "    `tf.keras.layers.BatchNormalization` that behaves differently during",
            "    inference. Also, note the fact that test loss is not affected by",
            "    regularization layers like noise and dropout.",
            "",
            "    Args:",
            "        x: Input samples. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "            (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "            (in case the model has multiple inputs).",
            "          - A `tf.data` dataset.",
            "          - A generator or `keras.utils.Sequence` instance.",
            "          A more detailed description of unpacking behavior for iterator types",
            "          (Dataset, generator, Sequence) is given in the `Unpacking behavior",
            "          for iterator-like inputs` section of `Model.fit`.",
            "        batch_size: Integer or `None`.",
            "            Number of samples per batch.",
            "            If unspecified, `batch_size` will default to 32.",
            "            Do not specify the `batch_size` if your data is in the",
            "            form of dataset, generators, or `keras.utils.Sequence` instances",
            "            (since they generate batches).",
            "        verbose: Verbosity mode, 0 or 1.",
            "        steps: Total number of steps (batches of samples)",
            "            before declaring the prediction round finished.",
            "            Ignored with the default value of `None`. If x is a `tf.data`",
            "            dataset and `steps` is None, `predict` will",
            "            run until the input dataset is exhausted.",
            "        callbacks: List of `keras.callbacks.Callback` instances.",
            "            List of callbacks to apply during prediction.",
            "            See [callbacks](/api_docs/python/tf/keras/callbacks).",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`",
            "            input only. Maximum size for the generator queue.",
            "            If unspecified, `max_queue_size` will default to 10.",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input",
            "            only. Maximum number of processes to spin up when using",
            "            process-based threading. If unspecified, `workers` will default",
            "            to 1.",
            "        use_multiprocessing: Boolean. Used for generator or",
            "            `keras.utils.Sequence` input only. If `True`, use process-based",
            "            threading. If unspecified, `use_multiprocessing` will default to",
            "            `False`. Note that because this implementation relies on",
            "            multiprocessing, you should not pass non-picklable arguments to",
            "            the generator as they can't be passed easily to children processes.",
            "",
            "    See the discussion of `Unpacking behavior for iterator-like inputs` for",
            "    `Model.fit`. Note that Model.predict uses the same interpretation rules as",
            "    `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all",
            "    three methods.",
            "",
            "    `Model.predict` is not yet supported with",
            "    `tf.distribute.experimental.ParameterServerStrategy`.",
            "",
            "    Returns:",
            "        Numpy array(s) of predictions.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.predict` is wrapped in `tf.function`.",
            "        ValueError: In case of mismatch between the provided",
            "            input data and the model's expectations,",
            "            or in case a stateful model receives a number of samples",
            "            that is not a multiple of the batch size.",
            "    \"\"\"",
            "    base_layer.keras_api_gauge.get_cell('predict').set(True)",
            "    version_utils.disallow_legacy_graph('Model', 'predict')",
            "    self._check_call_args('predict')",
            "    _disallow_inside_tf_function('predict')",
            "",
            "    if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access",
            "      raise NotImplementedError('`model.predict` is not yet supported with '",
            "                                '`ParameterServerStrategy`.')",
            "",
            "    outputs = None",
            "    with self.distribute_strategy.scope():",
            "      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.",
            "      dataset_types = (dataset_ops.DatasetV1, dataset_ops.DatasetV2)",
            "      if (self._in_multi_worker_mode() or _is_tpu_multi_host(",
            "          self.distribute_strategy)) and isinstance(x, dataset_types):",
            "        try:",
            "          options = dataset_ops.Options()",
            "          data_option = distribute_options.AutoShardPolicy.DATA",
            "          options.experimental_distribute.auto_shard_policy = data_option",
            "          x = x.with_options(options)",
            "        except ValueError:",
            "          warnings.warn('Using Model.predict with '",
            "                        'MultiWorkerDistributionStrategy or TPUStrategy and '",
            "                        'AutoShardPolicy.FILE might lead to out-of-order result'",
            "                        '. Consider setting it to AutoShardPolicy.DATA.')",
            "",
            "      data_handler = data_adapter.get_data_handler(",
            "          x=x,",
            "          batch_size=batch_size,",
            "          steps_per_epoch=steps,",
            "          initial_epoch=0,",
            "          epochs=1,",
            "          max_queue_size=max_queue_size,",
            "          workers=workers,",
            "          use_multiprocessing=use_multiprocessing,",
            "          model=self,",
            "          steps_per_execution=self._steps_per_execution)",
            "",
            "      # Container that configures and calls `tf.keras.Callback`s.",
            "      if not isinstance(callbacks, callbacks_module.CallbackList):",
            "        callbacks = callbacks_module.CallbackList(",
            "            callbacks,",
            "            add_history=True,",
            "            add_progbar=verbose != 0,",
            "            model=self,",
            "            verbose=verbose,",
            "            epochs=1,",
            "            steps=data_handler.inferred_steps)",
            "",
            "      self.predict_function = self.make_predict_function()",
            "      self._predict_counter.assign(0)",
            "      callbacks.on_predict_begin()",
            "      batch_outputs = None",
            "      for _, iterator in data_handler.enumerate_epochs():  # Single epoch.",
            "        with data_handler.catch_stop_iteration():",
            "          for step in data_handler.steps():",
            "            callbacks.on_predict_batch_begin(step)",
            "            tmp_batch_outputs = self.predict_function(iterator)",
            "            if data_handler.should_sync:",
            "              context.async_wait()",
            "            batch_outputs = tmp_batch_outputs  # No error, now safe to assign.",
            "            if outputs is None:",
            "              outputs = nest.map_structure(lambda batch_output: [batch_output],",
            "                                           batch_outputs)",
            "            else:",
            "              nest.map_structure_up_to(",
            "                  batch_outputs,",
            "                  lambda output, batch_output: output.append(batch_output),",
            "                  outputs, batch_outputs)",
            "            end_step = step + data_handler.step_increment",
            "            callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})",
            "      if batch_outputs is None:",
            "        raise ValueError('Expect x to be a non-empty array or dataset.')",
            "      callbacks.on_predict_end()",
            "    all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)",
            "    return tf_utils.sync_to_numpy_or_python_type(all_outputs)",
            "",
            "  def reset_metrics(self):",
            "    \"\"\"Resets the state of all the metrics in the model.",
            "",
            "    Examples:",
            "",
            "    >>> inputs = tf.keras.layers.Input(shape=(3,))",
            "    >>> outputs = tf.keras.layers.Dense(2)(inputs)",
            "    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)",
            "    >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])",
            "",
            "    >>> x = np.random.random((2, 3))",
            "    >>> y = np.random.randint(0, 2, (2, 2))",
            "    >>> _ = model.fit(x, y, verbose=0)",
            "    >>> assert all(float(m.result()) for m in model.metrics)",
            "",
            "    >>> model.reset_metrics()",
            "    >>> assert all(float(m.result()) == 0 for m in model.metrics)",
            "",
            "    \"\"\"",
            "    for m in self.metrics:",
            "      m.reset_state()",
            "",
            "  def train_on_batch(self,",
            "                     x,",
            "                     y=None,",
            "                     sample_weight=None,",
            "                     class_weight=None,",
            "                     reset_metrics=True,",
            "                     return_dict=False):",
            "    \"\"\"Runs a single gradient update on a single batch of data.",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays",
            "              (in case the model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors",
            "              (in case the model has multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors,",
            "              if the model has named inputs.",
            "        y: Target data. Like the input data `x`, it could be either Numpy",
            "          array(s) or TensorFlow tensor(s). It should be consistent with `x`",
            "          (you cannot have Numpy inputs and tensor targets, or inversely).",
            "        sample_weight: Optional array of the same length as x, containing",
            "          weights to apply to the model's loss for each sample. In the case of",
            "          temporal data, you can pass a 2D array with shape (samples,",
            "          sequence_length), to apply a different weight to every timestep of",
            "          every sample.",
            "        class_weight: Optional dictionary mapping class indices (integers) to a",
            "          weight (float) to apply to the model's loss for the samples from this",
            "          class during training. This can be useful to tell the model to \"pay",
            "          more attention\" to samples from an under-represented class.",
            "        reset_metrics: If `True`, the metrics returned will be only for this",
            "          batch. If `False`, the metrics will be statefully accumulated across",
            "          batches.",
            "        return_dict: If `True`, loss and metric results are returned as a dict,",
            "          with each key being the name of the metric. If `False`, they are",
            "          returned as a list.",
            "",
            "    Returns:",
            "        Scalar training loss",
            "        (if the model has a single output and no metrics)",
            "        or list of scalars (if the model has multiple outputs",
            "        and/or metrics). The attribute `model.metrics_names` will give you",
            "        the display labels for the scalar outputs.",
            "",
            "    Raises:",
            "      RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.",
            "      ValueError: In case of invalid user-provided arguments.",
            "    \"\"\"",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('train_on_batch')",
            "    _disallow_inside_tf_function('train_on_batch')",
            "    with self.distribute_strategy.scope(), \\",
            "         training_utils.RespectCompiledTrainableState(self):",
            "      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,",
            "                                                    y, sample_weight,",
            "                                                    class_weight)",
            "      self.train_function = self.make_train_function()",
            "      logs = self.train_function(iterator)",
            "",
            "    if reset_metrics:",
            "      self.reset_metrics()",
            "    logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "    if return_dict:",
            "      return logs",
            "    else:",
            "      return flatten_metrics_in_order(logs, self.metrics_names)",
            "",
            "  def test_on_batch(self,",
            "                    x,",
            "                    y=None,",
            "                    sample_weight=None,",
            "                    reset_metrics=True,",
            "                    return_dict=False):",
            "    \"\"\"Test the model on a single batch of samples.",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays (in case the",
            "              model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors (in case the model has",
            "              multiple inputs).",
            "          - A dict mapping input names to the corresponding array/tensors, if",
            "              the model has named inputs.",
            "        y: Target data. Like the input data `x`, it could be either Numpy",
            "          array(s) or TensorFlow tensor(s). It should be consistent with `x`",
            "          (you cannot have Numpy inputs and tensor targets, or inversely).",
            "        sample_weight: Optional array of the same length as x, containing",
            "          weights to apply to the model's loss for each sample. In the case of",
            "          temporal data, you can pass a 2D array with shape (samples,",
            "          sequence_length), to apply a different weight to every timestep of",
            "          every sample.",
            "        reset_metrics: If `True`, the metrics returned will be only for this",
            "          batch. If `False`, the metrics will be statefully accumulated across",
            "          batches.",
            "        return_dict: If `True`, loss and metric results are returned as a dict,",
            "          with each key being the name of the metric. If `False`, they are",
            "          returned as a list.",
            "",
            "    Returns:",
            "        Scalar test loss (if the model has a single output and no metrics)",
            "        or list of scalars (if the model has multiple outputs",
            "        and/or metrics). The attribute `model.metrics_names` will give you",
            "        the display labels for the scalar outputs.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.",
            "        ValueError: In case of invalid user-provided arguments.",
            "    \"\"\"",
            "    self._assert_compile_was_called()",
            "    self._check_call_args('test_on_batch')",
            "    _disallow_inside_tf_function('test_on_batch')",
            "    with self.distribute_strategy.scope():",
            "      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,",
            "                                                    y, sample_weight)",
            "      self.test_function = self.make_test_function()",
            "      logs = self.test_function(iterator)",
            "",
            "    if reset_metrics:",
            "      self.reset_metrics()",
            "    logs = tf_utils.sync_to_numpy_or_python_type(logs)",
            "    if return_dict:",
            "      return logs",
            "    else:",
            "      return flatten_metrics_in_order(logs, self.metrics_names)",
            "",
            "  def predict_on_batch(self, x):",
            "    \"\"\"Returns predictions for a single batch of samples.",
            "",
            "    Args:",
            "        x: Input data. It could be:",
            "          - A Numpy array (or array-like), or a list of arrays (in case the",
            "              model has multiple inputs).",
            "          - A TensorFlow tensor, or a list of tensors (in case the model has",
            "              multiple inputs).",
            "",
            "    Returns:",
            "        Numpy array(s) of predictions.",
            "",
            "    Raises:",
            "        RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.",
            "        ValueError: In case of mismatch between given number of inputs and",
            "          expectations of the model.",
            "    \"\"\"",
            "    self._check_call_args('predict_on_batch')",
            "    _disallow_inside_tf_function('predict_on_batch')",
            "    with self.distribute_strategy.scope():",
            "      iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x)",
            "      self.predict_function = self.make_predict_function()",
            "      outputs = self.predict_function(iterator)",
            "    return tf_utils.sync_to_numpy_or_python_type(outputs)",
            "",
            "  def fit_generator(self,",
            "                    generator,",
            "                    steps_per_epoch=None,",
            "                    epochs=1,",
            "                    verbose=1,",
            "                    callbacks=None,",
            "                    validation_data=None,",
            "                    validation_steps=None,",
            "                    validation_freq=1,",
            "                    class_weight=None,",
            "                    max_queue_size=10,",
            "                    workers=1,",
            "                    use_multiprocessing=False,",
            "                    shuffle=True,",
            "                    initial_epoch=0):",
            "    \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.",
            "",
            "    DEPRECATED:",
            "      `Model.fit` now supports generators, so there is no longer any need to use",
            "      this endpoint.",
            "    \"\"\"",
            "    warnings.warn('`Model.fit_generator` is deprecated and '",
            "                  'will be removed in a future version. '",
            "                  'Please use `Model.fit`, which supports generators.')",
            "    return self.fit(",
            "        generator,",
            "        steps_per_epoch=steps_per_epoch,",
            "        epochs=epochs,",
            "        verbose=verbose,",
            "        callbacks=callbacks,",
            "        validation_data=validation_data,",
            "        validation_steps=validation_steps,",
            "        validation_freq=validation_freq,",
            "        class_weight=class_weight,",
            "        max_queue_size=max_queue_size,",
            "        workers=workers,",
            "        use_multiprocessing=use_multiprocessing,",
            "        shuffle=shuffle,",
            "        initial_epoch=initial_epoch)",
            "",
            "  def evaluate_generator(self,",
            "                         generator,",
            "                         steps=None,",
            "                         callbacks=None,",
            "                         max_queue_size=10,",
            "                         workers=1,",
            "                         use_multiprocessing=False,",
            "                         verbose=0):",
            "    \"\"\"Evaluates the model on a data generator.",
            "",
            "    DEPRECATED:",
            "      `Model.evaluate` now supports generators, so there is no longer any need",
            "      to use this endpoint.",
            "    \"\"\"",
            "    warnings.warn('`Model.evaluate_generator` is deprecated and '",
            "                  'will be removed in a future version. '",
            "                  'Please use `Model.evaluate`, which supports generators.')",
            "    self._check_call_args('evaluate_generator')",
            "",
            "    return self.evaluate(",
            "        generator,",
            "        steps=steps,",
            "        max_queue_size=max_queue_size,",
            "        workers=workers,",
            "        use_multiprocessing=use_multiprocessing,",
            "        verbose=verbose,",
            "        callbacks=callbacks)",
            "",
            "  def predict_generator(self,",
            "                        generator,",
            "                        steps=None,",
            "                        callbacks=None,",
            "                        max_queue_size=10,",
            "                        workers=1,",
            "                        use_multiprocessing=False,",
            "                        verbose=0):",
            "    \"\"\"Generates predictions for the input samples from a data generator.",
            "",
            "    DEPRECATED:",
            "      `Model.predict` now supports generators, so there is no longer any need",
            "      to use this endpoint.",
            "    \"\"\"",
            "    warnings.warn('`Model.predict_generator` is deprecated and '",
            "                  'will be removed in a future version. '",
            "                  'Please use `Model.predict`, which supports generators.')",
            "    return self.predict(",
            "        generator,",
            "        steps=steps,",
            "        max_queue_size=max_queue_size,",
            "        workers=workers,",
            "        use_multiprocessing=use_multiprocessing,",
            "        verbose=verbose,",
            "        callbacks=callbacks)",
            "",
            "  ######################################################################",
            "  # Functions below are not training related. They are for model weights",
            "  # tracking, save/load, serialization, etc.",
            "  ######################################################################",
            "",
            "  @property",
            "  def trainable_weights(self):",
            "    self._assert_weights_created()",
            "    if not self._trainable:",
            "      return []",
            "    trainable_variables = []",
            "    for trackable_obj in self._self_tracked_trackables:",
            "      trainable_variables += trackable_obj.trainable_variables",
            "    trainable_variables += self._trainable_weights",
            "    return self._dedup_weights(trainable_variables)",
            "",
            "  @property",
            "  def non_trainable_weights(self):",
            "    self._assert_weights_created()",
            "    non_trainable_variables = []",
            "    for trackable_obj in self._self_tracked_trackables:",
            "      non_trainable_variables += trackable_obj.non_trainable_variables",
            "",
            "    if not self._trainable:",
            "      # Return order is all trainable vars, then all non-trainable vars.",
            "      trainable_variables = []",
            "      for trackable_obj in self._self_tracked_trackables:",
            "        trainable_variables += trackable_obj.trainable_variables",
            "",
            "      non_trainable_variables = (",
            "          trainable_variables + self._trainable_weights +",
            "          non_trainable_variables + self._non_trainable_weights)",
            "    else:",
            "      non_trainable_variables = (",
            "          non_trainable_variables + self._non_trainable_weights)",
            "",
            "    return self._dedup_weights(non_trainable_variables)",
            "",
            "  def get_weights(self):",
            "    \"\"\"Retrieves the weights of the model.",
            "",
            "    Returns:",
            "        A flat list of Numpy arrays.",
            "    \"\"\"",
            "    with self.distribute_strategy.scope():",
            "      return super(Model, self).get_weights()",
            "",
            "  def save(self,",
            "           filepath,",
            "           overwrite=True,",
            "           include_optimizer=True,",
            "           save_format=None,",
            "           signatures=None,",
            "           options=None,",
            "           save_traces=True):",
            "    # pylint: disable=line-too-long",
            "    \"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.",
            "",
            "    Please see `tf.keras.models.save_model` or the",
            "    [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)",
            "    for details.",
            "",
            "    Args:",
            "        filepath: String, PathLike, path to SavedModel or H5 file to save the",
            "            model.",
            "        overwrite: Whether to silently overwrite any existing file at the",
            "            target location, or provide the user with a manual prompt.",
            "        include_optimizer: If True, save optimizer's state together.",
            "        save_format: Either `'tf'` or `'h5'`, indicating whether to save the",
            "            model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,",
            "            and 'h5' in TF 1.X.",
            "        signatures: Signatures to save with the SavedModel. Applicable to the",
            "            'tf' format only. Please see the `signatures` argument in",
            "            `tf.saved_model.save` for details.",
            "        options: (only applies to SavedModel format)",
            "            `tf.saved_model.SaveOptions` object that specifies options for",
            "            saving to SavedModel.",
            "        save_traces: (only applies to SavedModel format) When enabled, the",
            "            SavedModel will store the function traces for each layer. This",
            "            can be disabled, so that only the configs of each layer are stored.",
            "            Defaults to `True`. Disabling this will decrease serialization time",
            "            and reduce file size, but it requires that all custom layers/models",
            "            implement a `get_config()` method.",
            "",
            "    Example:",
            "",
            "    ```python",
            "    from keras.models import load_model",
            "",
            "    model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'",
            "    del model  # deletes the existing model",
            "",
            "    # returns a compiled model",
            "    # identical to the previous one",
            "    model = load_model('my_model.h5')",
            "    ```",
            "    \"\"\"",
            "    # pylint: enable=line-too-long",
            "    save.save_model(self, filepath, overwrite, include_optimizer, save_format,",
            "                    signatures, options, save_traces)",
            "",
            "  def save_weights(self,",
            "                   filepath,",
            "                   overwrite=True,",
            "                   save_format=None,",
            "                   options=None):",
            "    \"\"\"Saves all layer weights.",
            "",
            "    Either saves in HDF5 or in TensorFlow format based on the `save_format`",
            "    argument.",
            "",
            "    When saving in HDF5 format, the weight file has:",
            "      - `layer_names` (attribute), a list of strings",
            "          (ordered names of model layers).",
            "      - For every layer, a `group` named `layer.name`",
            "          - For every such layer group, a group attribute `weight_names`,",
            "              a list of strings",
            "              (ordered names of weights tensor of the layer).",
            "          - For every weight in the layer, a dataset",
            "              storing the weight value, named after the weight tensor.",
            "",
            "    When saving in TensorFlow format, all objects referenced by the network are",
            "    saved in the same format as `tf.train.Checkpoint`, including any `Layer`",
            "    instances or `Optimizer` instances assigned to object attributes. For",
            "    networks constructed from inputs and outputs using `tf.keras.Model(inputs,",
            "    outputs)`, `Layer` instances used by the network are tracked/saved",
            "    automatically. For user-defined classes which inherit from `tf.keras.Model`,",
            "    `Layer` instances must be assigned to object attributes, typically in the",
            "    constructor. See the documentation of `tf.train.Checkpoint` and",
            "    `tf.keras.Model` for details.",
            "",
            "    While the formats are the same, do not mix `save_weights` and",
            "    `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be",
            "    loaded using `Model.load_weights`. Checkpoints saved using",
            "    `tf.train.Checkpoint.save` should be restored using the corresponding",
            "    `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over",
            "    `save_weights` for training checkpoints.",
            "",
            "    The TensorFlow format matches objects and variables by starting at a root",
            "    object, `self` for `save_weights`, and greedily matching attribute",
            "    names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this",
            "    is the `Checkpoint` even if the `Checkpoint` has a model attached. This",
            "    means saving a `tf.keras.Model` using `save_weights` and loading into a",
            "    `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match",
            "    the `Model`'s variables. See the [guide to training",
            "    checkpoints](https://www.tensorflow.org/guide/checkpoint) for details",
            "    on the TensorFlow format.",
            "",
            "    Args:",
            "        filepath: String or PathLike, path to the file to save the weights to.",
            "            When saving in TensorFlow format, this is the prefix used for",
            "            checkpoint files (multiple files are generated). Note that the '.h5'",
            "            suffix causes weights to be saved in HDF5 format.",
            "        overwrite: Whether to silently overwrite any existing file at the",
            "            target location, or provide the user with a manual prompt.",
            "        save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or",
            "            '.keras' will default to HDF5 if `save_format` is `None`. Otherwise",
            "            `None` defaults to 'tf'.",
            "        options: Optional `tf.train.CheckpointOptions` object that specifies",
            "            options for saving weights.",
            "",
            "    Raises:",
            "        ImportError: If h5py is not available when attempting to save in HDF5",
            "            format.",
            "        ValueError: For invalid/unknown format arguments.",
            "    \"\"\"",
            "    self._assert_weights_created()",
            "    filepath = path_to_string(filepath)",
            "    filepath_is_h5 = saving_utils.is_hdf5_filepath(filepath)",
            "    if save_format is None:",
            "      if filepath_is_h5:",
            "        save_format = 'h5'",
            "      else:",
            "        save_format = 'tf'",
            "    else:",
            "      user_format = save_format.lower().strip()",
            "      if user_format in ('tensorflow', 'tf'):",
            "        save_format = 'tf'",
            "      elif user_format in ('hdf5', 'h5', 'keras'):",
            "        save_format = 'h5'",
            "      else:",
            "        raise ValueError(",
            "            'Unknown format \"%s\". Was expecting one of {\"tf\", \"h5\"}.' % (",
            "                save_format,))",
            "    if save_format == 'tf' and filepath_is_h5:",
            "      raise ValueError(",
            "          ('save_weights got save_format=\"tf\"/\"tensorflow\", but the '",
            "           'filepath (\"%s\") looks like an HDF5 file. Omit the \".h5\"/\".keras\" '",
            "           'when saving in TensorFlow format.')",
            "          % filepath)",
            "",
            "    if save_format == 'h5' and h5py is None:",
            "      raise ImportError(",
            "          '`save_weights` requires h5py when saving in hdf5.')",
            "    if save_format == 'tf':",
            "      check_filepath = filepath + '.index'",
            "    else:",
            "      check_filepath = filepath",
            "    # If file exists and should not be overwritten:",
            "    if not overwrite and os.path.isfile(check_filepath):",
            "      proceed = ask_to_proceed_with_overwrite(check_filepath)",
            "      if not proceed:",
            "        return",
            "    if save_format == 'h5':",
            "      with h5py.File(filepath, 'w') as f:",
            "        hdf5_format.save_weights_to_hdf5_group(f, self.layers)",
            "    else:",
            "      if context.executing_eagerly():",
            "        session = None",
            "      else:",
            "        session = backend.get_session()",
            "      self._trackable_saver.save(filepath, session=session, options=options)",
            "      # Record this checkpoint so it's visible from tf.train.latest_checkpoint.",
            "      checkpoint_management.update_checkpoint_state_internal(",
            "          save_dir=os.path.dirname(filepath),",
            "          model_checkpoint_path=filepath,",
            "          save_relative_paths=True,",
            "          all_model_checkpoint_paths=[filepath])",
            "",
            "  def load_weights(self,",
            "                   filepath,",
            "                   by_name=False,",
            "                   skip_mismatch=False,",
            "                   options=None):",
            "    \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file.",
            "",
            "    If `by_name` is False weights are loaded based on the network's",
            "    topology. This means the architecture should be the same as when the weights",
            "    were saved.  Note that layers that don't have weights are not taken into",
            "    account in the topological ordering, so adding or removing layers is fine as",
            "    long as they don't have weights.",
            "",
            "    If `by_name` is True, weights are loaded into layers only if they share the",
            "    same name. This is useful for fine-tuning or transfer-learning models where",
            "    some of the layers have changed.",
            "",
            "    Only topological loading (`by_name=False`) is supported when loading weights",
            "    from the TensorFlow format. Note that topological loading differs slightly",
            "    between TensorFlow and HDF5 formats for user-defined classes inheriting from",
            "    `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the",
            "    TensorFlow format loads based on the object-local names of attributes to",
            "    which layers are assigned in the `Model`'s constructor.",
            "",
            "    Args:",
            "        filepath: String, path to the weights file to load. For weight files in",
            "            TensorFlow format, this is the file prefix (the same as was passed",
            "            to `save_weights`). This can also be a path to a SavedModel",
            "            saved from `model.save`.",
            "        by_name: Boolean, whether to load weights by name or by topological",
            "            order. Only topological loading is supported for weight files in",
            "            TensorFlow format.",
            "        skip_mismatch: Boolean, whether to skip loading of layers where there is",
            "            a mismatch in the number of weights, or a mismatch in the shape of",
            "            the weight (only valid when `by_name=True`).",
            "        options: Optional `tf.train.CheckpointOptions` object that specifies",
            "            options for loading weights.",
            "",
            "    Returns:",
            "        When loading a weight file in TensorFlow format, returns the same status",
            "        object as `tf.train.Checkpoint.restore`. When graph building, restore",
            "        ops are run automatically as soon as the network is built (on first call",
            "        for user-defined classes inheriting from `Model`, immediately if it is",
            "        already built).",
            "",
            "        When loading weights in HDF5 format, returns `None`.",
            "",
            "    Raises:",
            "        ImportError: If h5py is not available and the weight file is in HDF5",
            "            format.",
            "        ValueError: If `skip_mismatch` is set to `True` when `by_name` is",
            "          `False`.",
            "    \"\"\"",
            "    if backend.is_tpu_strategy(self._distribution_strategy):",
            "      if (self._distribution_strategy.extended.steps_per_run > 1 and",
            "          (not saving_utils.is_hdf5_filepath(filepath))):",
            "        raise ValueError('Load weights is not yet supported with TPUStrategy '",
            "                         'with steps_per_run greater than 1.')",
            "    if skip_mismatch and not by_name:",
            "      raise ValueError(",
            "          'When calling model.load_weights, skip_mismatch can only be set to '",
            "          'True when by_name is True.')",
            "",
            "    filepath, save_format = _detect_save_format(filepath)",
            "    if save_format == 'tf':",
            "      status = self._trackable_saver.restore(filepath, options)",
            "      if by_name:",
            "        raise NotImplementedError(",
            "            'Weights may only be loaded based on topology into Models when '",
            "            'loading TensorFlow-formatted weights (got by_name=True to '",
            "            'load_weights).')",
            "      if not context.executing_eagerly():",
            "        session = backend.get_session()",
            "        # Restore existing variables (if any) immediately, and set up a",
            "        # streaming restore for any variables created in the future.",
            "        trackable_utils.streaming_restore(status=status, session=session)",
            "      status.assert_nontrivial_match()",
            "      return status",
            "    if h5py is None:",
            "      raise ImportError(",
            "          '`load_weights` requires h5py when loading weights from HDF5.')",
            "    if not self._is_graph_network and not self.built:",
            "      raise ValueError(",
            "          'Unable to load weights saved in HDF5 format into a subclassed '",
            "          'Model which has not created its variables yet. Call the Model '",
            "          'first, then load the weights.')",
            "    self._assert_weights_created()",
            "    with h5py.File(filepath, 'r') as f:",
            "      if 'layer_names' not in f.attrs and 'model_weights' in f:",
            "        f = f['model_weights']",
            "      if by_name:",
            "        hdf5_format.load_weights_from_hdf5_group_by_name(",
            "            f, self.layers, skip_mismatch=skip_mismatch)",
            "      else:",
            "        hdf5_format.load_weights_from_hdf5_group(f, self.layers)",
            "",
            "  def _updated_config(self):",
            "    \"\"\"Util shared between different serialization methods.",
            "",
            "    Returns:",
            "        Model config with Keras version information added.",
            "    \"\"\"",
            "    from tensorflow.python.keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top",
            "",
            "    config = self.get_config()",
            "    model_config = {",
            "        'class_name': self.__class__.__name__,",
            "        'config': config,",
            "        'keras_version': keras_version,",
            "        'backend': backend.backend()",
            "    }",
            "    return model_config",
            "",
            "  def get_config(self):",
            "    raise NotImplementedError",
            "",
            "  @classmethod",
            "  def from_config(cls, config, custom_objects=None):",
            "    # `from_config` assumes `cls` is either `Functional` or a child class of",
            "    # `Functional`. In the case that `cls` is meant to behave like a child class",
            "    # of `Functional` but only inherits from the `Model` class, we have to call",
            "    # `cls(...)` instead of `Functional.from_config`.",
            "    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top",
            "    with generic_utils.SharedObjectLoadingScope():",
            "      input_tensors, output_tensors, created_layers = (",
            "          functional.reconstruct_from_config(config, custom_objects))",
            "      # Initialize a model belonging to `cls`, which can be user-defined or",
            "      # `Functional`.",
            "      model = cls(inputs=input_tensors, outputs=output_tensors,",
            "                  name=config.get('name'))",
            "      functional.connect_ancillary_layers(model, created_layers)",
            "      return model",
            "",
            "  def to_json(self, **kwargs):",
            "    \"\"\"Returns a JSON string containing the network configuration.",
            "",
            "    To load a network from a JSON save file, use",
            "    `keras.models.model_from_json(json_string, custom_objects={})`.",
            "",
            "    Args:",
            "        **kwargs: Additional keyword arguments",
            "            to be passed to `json.dumps()`.",
            "",
            "    Returns:",
            "        A JSON string.",
            "    \"\"\"",
            "    model_config = self._updated_config()",
            "    return json.dumps(",
            "        model_config, default=json_utils.get_json_type, **kwargs)",
            "",
            "  def to_yaml(self, **kwargs):",
            "    \"\"\"Returns a yaml string containing the network configuration.",
            "",
            "    Note: Since TF 2.6, this method is no longer supported and will raise a",
            "    RuntimeError.",
            "",
            "    To load a network from a yaml save file, use",
            "    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.",
            "",
            "    `custom_objects` should be a dictionary mapping",
            "    the names of custom losses / layers / etc to the corresponding",
            "    functions / classes.",
            "",
            "    Args:",
            "        **kwargs: Additional keyword arguments",
            "            to be passed to `yaml.dump()`.",
            "",
            "    Returns:",
            "        A YAML string.",
            "",
            "    Raises:",
            "        RuntimeError: announces that the method poses a security risk",
            "    \"\"\"",
            "    raise RuntimeError(",
            "        'Method `model.to_yaml()` has been removed due to security risk of '",
            "        'arbitrary code execution. Please use `model.to_json()` instead.'",
            "    )",
            "",
            "  def reset_states(self):",
            "    for layer in self.layers:",
            "      if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):",
            "        layer.reset_states()",
            "",
            "  @property",
            "  @doc_controls.do_not_generate_docs",
            "  def state_updates(self):",
            "    \"\"\"Deprecated, do NOT use!",
            "",
            "    Returns the `updates` from all layers that are stateful.",
            "",
            "    This is useful for separating training updates and",
            "    state updates, e.g. when we need to update a layer's internal state",
            "    during prediction.",
            "",
            "    Returns:",
            "        A list of update ops.",
            "    \"\"\"",
            "    warnings.warn('`Model.state_updates` will be removed in a future version. '",
            "                  'This property should not be used in TensorFlow 2.0, '",
            "                  'as `updates` are applied automatically.')",
            "    state_updates = []",
            "    for layer in self.layers:",
            "      if getattr(layer, 'stateful', False):",
            "        if hasattr(layer, 'updates'):",
            "          state_updates += layer.updates",
            "    return state_updates",
            "",
            "  @property",
            "  def weights(self):",
            "    \"\"\"Returns the list of all layer variables/weights.",
            "",
            "    Note: This will not track the weights of nested `tf.Modules` that are not",
            "    themselves Keras layers.",
            "",
            "    Returns:",
            "      A list of variables.",
            "    \"\"\"",
            "    return self._dedup_weights(self._undeduplicated_weights)",
            "",
            "  @property",
            "  def _undeduplicated_weights(self):",
            "    \"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"",
            "    self._assert_weights_created()",
            "    weights = []",
            "    for layer in self._self_tracked_trackables:",
            "      weights += layer.variables",
            "    weights += (self._trainable_weights + self._non_trainable_weights)",
            "    return weights",
            "",
            "  def summary(self, line_length=None, positions=None, print_fn=None):",
            "    \"\"\"Prints a string summary of the network.",
            "",
            "    Args:",
            "        line_length: Total length of printed lines",
            "            (e.g. set this to adapt the display to different",
            "            terminal window sizes).",
            "        positions: Relative or absolute positions of log elements",
            "            in each line. If not provided,",
            "            defaults to `[.33, .55, .67, 1.]`.",
            "        print_fn: Print function to use. Defaults to `print`.",
            "            It will be called on each line of the summary.",
            "            You can set it to a custom function",
            "            in order to capture the string summary.",
            "",
            "    Raises:",
            "        ValueError: if `summary()` is called before the model is built.",
            "    \"\"\"",
            "    if not self.built:",
            "      raise ValueError('This model has not yet been built. '",
            "                       'Build the model first by calling `build()` or calling '",
            "                       '`fit()` with some data, or specify '",
            "                       'an `input_shape` argument in the first layer(s) for '",
            "                       'automatic build.')",
            "    layer_utils.print_summary(self,",
            "                              line_length=line_length,",
            "                              positions=positions,",
            "                              print_fn=print_fn)",
            "",
            "  @property",
            "  def layers(self):",
            "    return list(self._flatten_layers(include_self=False, recursive=False))",
            "",
            "  def get_layer(self, name=None, index=None):",
            "    \"\"\"Retrieves a layer based on either its name (unique) or index.",
            "",
            "    If `name` and `index` are both provided, `index` will take precedence.",
            "    Indices are based on order of horizontal graph traversal (bottom-up).",
            "",
            "    Args:",
            "        name: String, name of layer.",
            "        index: Integer, index of layer.",
            "",
            "    Returns:",
            "        A layer instance.",
            "",
            "    Raises:",
            "        ValueError: In case of invalid layer name or index.",
            "    \"\"\"",
            "    # TODO(fchollet): We could build a dictionary based on layer names",
            "    # since they are constant, but we have not done that yet.",
            "    if index is not None and name is not None:",
            "      raise ValueError('Provide only a layer name or a layer index.')",
            "",
            "    if index is not None:",
            "      if len(self.layers) <= index:",
            "        raise ValueError('Was asked to retrieve layer at index ' + str(index) +",
            "                         ' but model only has ' + str(len(self.layers)) +",
            "                         ' layers.')",
            "      else:",
            "        return self.layers[index]",
            "",
            "    if name is not None:",
            "      for layer in self.layers:",
            "        if layer.name == name:",
            "          return layer",
            "      raise ValueError('No such layer: ' + name + '.')",
            "    raise ValueError('Provide either a layer name or layer index.')",
            "",
            "  @trackable.no_automatic_dependency_tracking",
            "  def _set_save_spec(self, inputs):",
            "    if self._saved_model_inputs_spec is not None:",
            "      return  # Already set.",
            "",
            "    input_names = self.input_names",
            "    if not input_names:",
            "      input_names = compile_utils.create_pseudo_input_names(inputs)",
            "",
            "    flat_inputs = nest.flatten(inputs)",
            "    specs = []",
            "    for name, tensor in zip(input_names, flat_inputs):",
            "      specs.append(",
            "          tf_utils.get_tensor_spec(tensor, dynamic_batch=False, name=name))",
            "    specs = nest.pack_sequence_as(inputs, specs)",
            "",
            "    self._saved_model_inputs_spec = specs",
            "",
            "    # Store the input shapes",
            "    if (self.__class__.__name__ == 'Sequential' and",
            "        self._build_input_shape is None):",
            "      self._build_input_shape = nest.map_structure(",
            "          lambda x: None if x is None else x.shape, specs)",
            "",
            "  def _assert_weights_created(self):",
            "    \"\"\"Asserts that all the weights for the model have been created.",
            "",
            "    For a non-dynamic model, the weights must already be created after the",
            "    layer has been called. For a dynamic model, the exact list of weights can",
            "    never be known for certain since it may change at any time during execution.",
            "",
            "    We run this check right before accessing weights or getting the Numpy value",
            "    for the current weights. Otherwise, if the layer has never been called,",
            "    the user would just get an empty list, which is misleading.",
            "",
            "    Raises:",
            "      ValueError: if the weights of the network has not yet been created.",
            "    \"\"\"",
            "    if self.dynamic:",
            "      return",
            "",
            "    if ('build' in self.__class__.__dict__ and",
            "        self.__class__ != Model and",
            "        not self.built):",
            "      # For any model that has customized build() method but hasn't",
            "      # been invoked yet, this will cover both sequential and subclass model.",
            "      # Also make sure to exclude Model class itself which has build() defined.",
            "      raise ValueError('Weights for model %s have not yet been created. '",
            "                       'Weights are created when the Model is first called on '",
            "                       'inputs or `build()` is called with an `input_shape`.' %",
            "                       self.name)",
            "",
            "  def _check_call_args(self, method_name):",
            "    \"\"\"Check that `call` has only one positional arg.\"\"\"",
            "    # Always allow first arg, regardless of arg name.",
            "    fullargspec = self._call_full_argspec",
            "    if fullargspec.defaults:",
            "      positional_args = fullargspec.args[:-len(fullargspec.defaults)]",
            "    else:",
            "      positional_args = fullargspec.args",
            "    if 'training' in positional_args:",
            "      positional_args.remove('training')",
            "",
            "    # self and first arg can be positional.",
            "    if len(positional_args) > 2:",
            "      extra_args = positional_args[2:]",
            "      raise ValueError(",
            "          'Models passed to `' + method_name + '` can only have `training` '",
            "          'and the first argument in `call` as positional arguments, '",
            "          'found: ' + str(extra_args) + '.')",
            "",
            "  def _validate_compile(self, optimizer, metrics, **kwargs):",
            "    \"\"\"Performs validation checks for the default `compile`.\"\"\"",
            "    if any(",
            "        isinstance(opt, optimizer_v1.Optimizer)",
            "        for opt in nest.flatten(optimizer)):",
            "      raise ValueError(",
            "          '`tf.compat.v1.keras` Optimizer (', optimizer, ') is '",
            "          'not supported when eager execution is enabled. Use a '",
            "          '`tf.keras` Optimizer instead, or disable eager '",
            "          'execution.')",
            "",
            "    kwargs.pop('cloning', None)  # Legacy DistStrat argument, never used.",
            "    kwargs.pop('experimental_run_tf_function', None)  # Always `True`.",
            "    if kwargs.pop('distribute', None) is not None:",
            "      raise ValueError(",
            "          'Distribute argument in compile is not available in TF 2.0 please '",
            "          'create the model under the distribution strategy scope.')",
            "    if kwargs.pop('target_tensors', None) is not None:",
            "      raise ValueError(",
            "          'target_tensors argument is not supported when executing eagerly.')",
            "    invalid_kwargs = set(kwargs) - {'sample_weight_mode'}",
            "    if invalid_kwargs:",
            "      raise TypeError('Invalid keyword argument(s) in `compile`: %s' %",
            "                      (invalid_kwargs,))",
            "",
            "    # Model must be created and compiled with the same DistStrat.",
            "    if self.built and ds_context.has_strategy():",
            "      strategy = ds_context.get_strategy()",
            "      for v in self.variables:",
            "        if not strategy.extended.variable_created_in_scope(v):",
            "          raise ValueError(",
            "              'Variable (%s) was not created in the distribution strategy '",
            "              'scope of (%s). It is most likely due to not all layers or '",
            "              'the model or optimizer being created outside the distribution '",
            "              'strategy scope. Try to make sure your code looks similar '",
            "              'to the following.\\n'",
            "              'with strategy.scope():\\n'",
            "              '  model=_create_model()\\n'",
            "              '  model.compile(...)' % (v, strategy))",
            "",
            "    # Model metrics must be created in the same distribution strategy scope",
            "    # as the model.",
            "    strategy = self.distribute_strategy",
            "    for metric in nest.flatten(metrics):",
            "      for v in getattr(metric, 'variables', []):",
            "        if not strategy.extended.variable_created_in_scope(v):",
            "          raise ValueError(",
            "              'Metric (%s) passed to model.compile was created inside of a '",
            "              'different distribution strategy scope than the model. All '",
            "              'metrics must be created in the same distribution strategy '",
            "              'scope as the model (in this case %s). If you pass in a string '",
            "              'identifier for a metric to compile the metric will '",
            "              'automatically be created in the correct distribution '",
            "              'strategy scope.' % (metric, strategy)",
            "          )",
            "",
            "    # Model metrics must be created in the same distribution strategy scope",
            "    # as the model.",
            "    for opt in nest.flatten(optimizer):",
            "      for v in getattr(opt, '_weights', []):",
            "        if not strategy.extended.variable_created_in_scope(v):",
            "          raise ValueError(",
            "              'Optimizer (%s) passed to model.compile was created inside of a '",
            "              'different distribution strategy scope than the model. All '",
            "              'optimizers must be created in the same distribution strategy '",
            "              'scope as the model (in this case %s). If you pass in a string '",
            "              'identifier for an optimizer to compile the optimizer will '",
            "              'automatically be created in the correct distribution '",
            "              'strategy scope.' % (opt, strategy))",
            "",
            "  def _maybe_load_initial_epoch_from_ckpt(self, initial_epoch):",
            "    \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.",
            "",
            "    Refer to tensorflow/python/keras/distribute/worker_training_state.py",
            "    for more information.",
            "",
            "    Args:",
            "      initial_epoch: The original initial_epoch user passes in in `fit()`.",
            "",
            "    Returns:",
            "      If the training is recovering from previous failure under multi-worker",
            "      training setting, return the epoch the training is supposed to continue",
            "      at. Otherwise, return the `initial_epoch` the user passes in.",
            "    \"\"\"",
            "    if self._training_state is not None:",
            "      return self._training_state.maybe_load_initial_epoch_from_ckpt(",
            "          initial_epoch, mode=ModeKeys.TRAIN)",
            "    return initial_epoch",
            "",
            "  def _assert_compile_was_called(self):",
            "    # Checks whether `compile` has been called. If it has been called,",
            "    # then the optimizer is set. This is different from whether the",
            "    # model is compiled",
            "    # (i.e. whether the model is built and its inputs/outputs are set).",
            "    if not self._is_compiled:",
            "      raise RuntimeError('You must compile your model before '",
            "                         'training/testing. '",
            "                         'Use `model.compile(optimizer, loss)`.')",
            "",
            "  def _set_inputs(self, inputs, outputs=None, training=None):",
            "    \"\"\"This method is for compat with Modelv1. Only inputs are needed here.\"\"\"",
            "    self._set_save_spec(inputs)",
            "",
            "  @property",
            "  def _trackable_saved_model_saver(self):",
            "    return model_serialization.ModelSavedModelSaver(self)",
            "",
            "  def _list_functions_for_serialization(self, serialization_cache):",
            "    # SavedModel needs to ignore the execution functions.",
            "    train_function = self.train_function",
            "    test_function = self.test_function",
            "    predict_function = self.predict_function",
            "    self.train_function = None",
            "    self.test_function = None",
            "    self.predict_function = None",
            "    functions = super(",
            "        Model, self)._list_functions_for_serialization(serialization_cache)",
            "    self.train_function = train_function",
            "    self.test_function = test_function",
            "    self.predict_function = predict_function",
            "    return functions",
            "",
            "  def _should_eval(self, epoch, validation_freq):",
            "    if self._cluster_coordinator:",
            "      raise NotImplementedError(",
            "          'Evaluation in `model.fit` with '",
            "          '`ParameterServerStrategy` is not yet supported.')",
            "    epoch = epoch + 1  # one-index the user-facing epoch.",
            "    if isinstance(validation_freq, int):",
            "      return epoch % validation_freq == 0",
            "    elif isinstance(validation_freq, list):",
            "      return epoch in validation_freq",
            "    else:",
            "      raise ValueError('Expected `validation_freq` to be a list or int.')",
            "",
            "  ######################################################################",
            "  # Functions below exist only as v1 / v2 compatibility shims.",
            "  ######################################################################",
            "",
            "  def _get_compile_args(self, user_metrics=True):",
            "    \"\"\"Used for saving or cloning a Model.",
            "",
            "    Args:",
            "      user_metrics: Whether to return user-supplied metrics or `Metric` objects.",
            "        Defaults to returning the user-supplied metrics.",
            "",
            "    Returns:",
            "      Dictionary of arguments that were used when compiling the model.",
            "    \"\"\"",
            "    self._assert_compile_was_called()",
            "    # pylint: disable=protected-access",
            "",
            "    saved_metrics = self.compiled_metrics._user_metrics",
            "    saved_weighted_metrics = self.compiled_metrics._user_weighted_metrics",
            "",
            "    if not user_metrics:",
            "      if saved_metrics is not None:",
            "        saved_metrics = self.compiled_metrics._metrics",
            "      if saved_weighted_metrics is not None:",
            "        saved_weighted_metrics = self.compiled_metrics._weighted_metrics",
            "",
            "    compile_args = {",
            "        'optimizer': self.optimizer,",
            "        'loss': self.compiled_loss._user_losses,",
            "        'metrics': saved_metrics,",
            "        'weighted_metrics': saved_weighted_metrics,",
            "        'loss_weights': self.compiled_loss._user_loss_weights,",
            "    }",
            "    # pylint: enable=protected-access",
            "    return compile_args",
            "",
            "  def _get_callback_model(self):",
            "    return self",
            "",
            "  def _in_multi_worker_mode(self):",
            "    return self.distribute_strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access",
            "",
            "  @property",
            "  def _compile_was_called(self):",
            "    return self._is_compiled",
            "",
            "",
            "def reduce_per_replica(values, strategy, reduction='first'):",
            "  \"\"\"Reduce PerReplica objects.",
            "",
            "  Args:",
            "    values: Structure of `PerReplica` objects or `Tensor`s. `Tensor`s are",
            "      returned as-is.",
            "    strategy: `tf.distribute.Strategy` object.",
            "    reduction: One of 'first', 'concat'.",
            "",
            "  Returns:",
            "    Structure of `Tensor`s.",
            "  \"\"\"",
            "",
            "  def _reduce(v):",
            "    \"\"\"Reduce a single `PerReplica` object.\"\"\"",
            "    if reduction == 'concat' and _collective_all_reduce_multi_worker(strategy):",
            "      return _multi_worker_concat(v, strategy)",
            "    if not isinstance(v, ds_values.PerReplica):",
            "      return v",
            "    elif reduction == 'first':",
            "      return strategy.unwrap(v)[0]",
            "    elif reduction == 'concat':",
            "      if _is_tpu_multi_host(strategy):",
            "        return _tpu_multi_host_concat(v, strategy)",
            "      else:",
            "        return concat(strategy.unwrap(v))",
            "    else:",
            "      raise ValueError('`reduction` must be \"first\" or \"concat\".')",
            "",
            "  return nest.map_structure(_reduce, values)",
            "",
            "",
            "def concat(tensors, axis=0):",
            "  \"\"\"Concats `tensor`s along `axis`.\"\"\"",
            "  if isinstance(tensors[0], sparse_tensor.SparseTensor):",
            "    return sparse_ops.sparse_concat_v2(axis=axis, sp_inputs=tensors)",
            "  return array_ops.concat(tensors, axis=axis)",
            "",
            "",
            "def _is_tpu_multi_host(strategy):",
            "  return (backend.is_tpu_strategy(strategy) and",
            "          strategy.extended.num_hosts > 1)",
            "",
            "",
            "def _tpu_multi_host_concat(v, strategy):",
            "  \"\"\"Correctly order TPU PerReplica objects.\"\"\"",
            "  replicas = strategy.unwrap(v)",
            "  # When distributed datasets are created from Tensors / NumPy,",
            "  # TPUStrategy.experimental_distribute_dataset shards data in",
            "  # (Replica, Host) order, and TPUStrategy.unwrap returns it in",
            "  # (Host, Replica) order.",
            "  # TODO(b/150317897): Figure out long-term plan here.",
            "  num_replicas_per_host = strategy.extended.num_replicas_per_host",
            "  ordered_replicas = []",
            "  for replica_id in range(num_replicas_per_host):",
            "    ordered_replicas += replicas[replica_id::num_replicas_per_host]",
            "  return concat(ordered_replicas)",
            "",
            "",
            "def _collective_all_reduce_multi_worker(strategy):",
            "  return (isinstance(strategy,",
            "                     collective_all_reduce_strategy.CollectiveAllReduceStrategy)",
            "         ) and strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access",
            "",
            "",
            "# TODO(wxinyi): merge this with _tpu_multi_host_concat once we have all_gather",
            "# for all strategies",
            "def _multi_worker_concat(v, strategy):",
            "  \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"",
            "  replicas = strategy.gather(v, axis=0)",
            "  # v might not have the same shape on different replicas",
            "  if isinstance(v, ds_values.PerReplica):",
            "    shapes = array_ops.concat([",
            "        array_ops.expand_dims_v2(array_ops.shape(single_value)[0], axis=0)",
            "        for single_value in v.values",
            "    ],",
            "                              axis=0)",
            "    all_shapes = strategy.gather(shapes, axis=0)",
            "  else:",
            "    # v is a tensor. This may happen when, say, we have 2x1 multi-worker.",
            "    all_shapes = strategy.gather(",
            "        array_ops.expand_dims_v2(array_ops.shape(v)[0], axis=0), axis=0)",
            "",
            "  replicas = array_ops.split(",
            "      replicas,",
            "      num_or_size_splits=all_shapes,",
            "      num=strategy.num_replicas_in_sync)",
            "  ordered_replicas = []",
            "  num_replicas_per_worker = len(strategy.extended.worker_devices)",
            "  for replica_id in range(num_replicas_per_worker):",
            "    ordered_replicas += replicas[replica_id::num_replicas_per_worker]",
            "  return concat(ordered_replicas)",
            "",
            "",
            "def _is_scalar(x):",
            "  return isinstance(x, (ops.Tensor, variables.Variable)) and x.shape.rank == 0",
            "",
            "",
            "def write_scalar_summaries(logs, step):",
            "  for name, value in logs.items():",
            "    if _is_scalar(value):",
            "      summary_ops_v2.scalar('batch_' + name, value, step=step)",
            "",
            "",
            "def _minimum_control_deps(outputs):",
            "  \"\"\"Returns the minimum control dependencies to ensure step succeeded.\"\"\"",
            "  if context.executing_eagerly():",
            "    return []  # Control dependencies not needed.",
            "  outputs = nest.flatten(outputs, expand_composites=True)",
            "  for out in outputs:",
            "    # Variables can't be control dependencies.",
            "    if not isinstance(out, variables.Variable):",
            "      return [out]  # Return first Tensor or Op from outputs.",
            "  return []  # No viable Tensor or Op to use for control deps.",
            "",
            "",
            "def _disallow_inside_tf_function(method_name):",
            "  if ops.inside_function():",
            "    error_msg = (",
            "        'Detected a call to `Model.{method_name}` inside a `tf.function`. '",
            "        '`Model.{method_name} is a high-level endpoint that manages its own '",
            "        '`tf.function`. Please move the call to `Model.{method_name}` outside '",
            "        'of all enclosing `tf.function`s. Note that you can call a `Model` '",
            "        'directly on `Tensor`s inside a `tf.function` like: `model(x)`.'",
            "    ).format(method_name=method_name)",
            "    raise RuntimeError(error_msg)",
            "",
            "",
            "def _detect_save_format(filepath):",
            "  \"\"\"Returns path to weights file and save format.\"\"\"",
            "",
            "  filepath = path_to_string(filepath)",
            "  if saving_utils.is_hdf5_filepath(filepath):",
            "    return filepath, 'h5'",
            "",
            "  # Filepath could be a TensorFlow checkpoint file prefix or SavedModel",
            "  # directory. It's possible for filepath to be both a prefix and directory.",
            "  # Prioritize checkpoint over SavedModel.",
            "  if _is_readable_tf_checkpoint(filepath):",
            "    save_format = 'tf'",
            "  elif sm_loader.contains_saved_model(filepath):",
            "    ckpt_path = os.path.join(filepath, sm_constants.VARIABLES_DIRECTORY,",
            "                             sm_constants.VARIABLES_FILENAME)",
            "    if _is_readable_tf_checkpoint(ckpt_path):",
            "      filepath = ckpt_path",
            "      save_format = 'tf'",
            "    else:",
            "      raise ValueError('Unable to load weights. filepath {} appears to be a '",
            "                       'SavedModel directory, but checkpoint either doesn\\'t '",
            "                       'exist, or is incorrectly formatted.'.format(filepath))",
            "  else:",
            "    # Not a TensorFlow checkpoint. This filepath is likely an H5 file that",
            "    # doesn't have the hdf5/keras extensions.",
            "    save_format = 'h5'",
            "  return filepath, save_format",
            "",
            "",
            "def _is_readable_tf_checkpoint(filepath):",
            "  try:",
            "    py_checkpoint_reader.NewCheckpointReader(filepath)",
            "    return True",
            "  except errors_impl.DataLossError:",
            "    # The checkpoint is not readable in TensorFlow format.",
            "    return False",
            "",
            "",
            "def flatten_metrics_in_order(logs, metrics_names):",
            "  \"\"\"Turns the `logs` dict into a list as per key order of `metrics_names`.\"\"\"",
            "  results = []",
            "  for name in metrics_names:",
            "    if name in logs:",
            "      results.append(logs[name])",
            "  for key in sorted(logs.keys()):",
            "    if key not in metrics_names:",
            "      results.append(logs[key])",
            "  if len(results) == 1:",
            "    return results[0]",
            "  return results"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "87": [],
            "88": [],
            "89": [],
            "90": [],
            "91": [
                "yaml"
            ],
            "2400": [
                "Model",
                "to_yaml"
            ],
            "2402": [
                "Model",
                "to_yaml"
            ],
            "2403": [
                "Model",
                "to_yaml"
            ],
            "2404": [
                "Model",
                "to_yaml"
            ],
            "2405": [
                "Model",
                "to_yaml"
            ]
        },
        "addLocation": []
    },
    "tensorflow/python/keras/saving/model_config.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from tensorflow.python.keras.saving.saved_model import json_utils"
            },
            "1": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from tensorflow.python.util.tf_export import keras_export"
            },
            "2": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-# pylint: disable=g-import-not-at-top"
            },
            "4": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-try:"
            },
            "5": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  import yaml"
            },
            "6": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-except ImportError:"
            },
            "7": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  yaml = None"
            },
            "8": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-# pylint: enable=g-import-not-at-top"
            },
            "9": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "10": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " @keras_export('keras.models.model_from_config')"
            },
            "12": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " def model_from_config(config, custom_objects=None):"
            },
            "13": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 24,
                "PatchRowcode": "   \"\"\"Instantiates a Keras model from its config."
            },
            "14": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": "",
                "PatchRowcode": "- "
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 26,
                "PatchRowcode": "   Usage:"
            },
            "17": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 27,
                "PatchRowcode": "   ```"
            },
            "18": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 28,
                "PatchRowcode": "   # for a Functional API model"
            },
            "19": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 56,
                "PatchRowcode": " def model_from_yaml(yaml_string, custom_objects=None):"
            },
            "20": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "   \"\"\"Parses a yaml model configuration file and returns a model instance."
            },
            "21": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 58,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  Usage:"
            },
            "23": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "24": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  >>> model = tf.keras.Sequential(["
            },
            "25": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  ...     tf.keras.layers.Dense(5, input_shape=(3,)),"
            },
            "26": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  ...     tf.keras.layers.Softmax()])"
            },
            "27": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  >>> try:"
            },
            "28": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  ...   import yaml"
            },
            "29": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  ...   config = model.to_yaml()"
            },
            "30": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  ...   loaded_model = tf.keras.models.model_from_yaml(config)"
            },
            "31": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  ... except ImportError:"
            },
            "32": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  ...   pass"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+  Note: Since TF 2.6, this method is no longer supported and will raise a"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+  RuntimeError."
            },
            "35": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "   Args:"
            },
            "37": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "       yaml_string: YAML string or open file encoding a model configuration."
            },
            "38": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "       A Keras model instance (uncompiled)."
            },
            "39": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "   Raises:"
            },
            "41": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-      ImportError: if yaml module is not found."
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+      RuntimeError: announces that the method poses a security risk"
            },
            "43": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "   \"\"\""
            },
            "44": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  if yaml is None:"
            },
            "45": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')"
            },
            "46": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  # The method unsafe_load only exists in PyYAML 5.x+, so which branch of the"
            },
            "47": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  # try block is covered by tests depends on the installed version of PyYAML."
            },
            "48": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  try:"
            },
            "49": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # PyYAML 5.x+"
            },
            "50": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    config = yaml.unsafe_load(yaml_string)"
            },
            "51": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  except AttributeError:"
            },
            "52": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    config = yaml.load(yaml_string)"
            },
            "53": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top"
            },
            "54": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-  return deserialize(config, custom_objects=custom_objects)"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+  raise RuntimeError("
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+      'Method `model_from_yaml()` has been removed due to security risk of '"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+      'arbitrary code execution. Please use `Model.to_json()` and '"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+      '`model_from_json()` instead.'"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+  )"
            },
            "60": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 79,
                "PatchRowcode": " "
            },
            "61": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " "
            },
            "62": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 81,
                "PatchRowcode": " @keras_export('keras.models.model_from_json')"
            }
        },
        "frontPatchFile": [
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "# pylint: disable=protected-access",
            "\"\"\"Functions that save the model's config into different formats.\"\"\"",
            "",
            "from tensorflow.python.keras.saving.saved_model import json_utils",
            "from tensorflow.python.util.tf_export import keras_export",
            "",
            "# pylint: disable=g-import-not-at-top",
            "try:",
            "  import yaml",
            "except ImportError:",
            "  yaml = None",
            "# pylint: enable=g-import-not-at-top",
            "",
            "",
            "@keras_export('keras.models.model_from_config')",
            "def model_from_config(config, custom_objects=None):",
            "  \"\"\"Instantiates a Keras model from its config.",
            " ",
            "  Usage:",
            "  ```",
            "  # for a Functional API model",
            "  tf.keras.Model().from_config(model.get_config())",
            "",
            "  # for a Sequential model",
            "  tf.keras.Sequential().from_config(model.get_config())",
            "  ```",
            "",
            "  Args:",
            "      config: Configuration dictionary.",
            "      custom_objects: Optional dictionary mapping names",
            "          (strings) to custom classes or functions to be",
            "          considered during deserialization.",
            "",
            "  Returns:",
            "      A Keras model instance (uncompiled).",
            "",
            "  Raises:",
            "      TypeError: if `config` is not a dictionary.",
            "  \"\"\"",
            "  if isinstance(config, list):",
            "    raise TypeError('`model_from_config` expects a dictionary, not a list. '",
            "                    'Maybe you meant to use '",
            "                    '`Sequential.from_config(config)`?')",
            "  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top",
            "  return deserialize(config, custom_objects=custom_objects)",
            "",
            "",
            "@keras_export('keras.models.model_from_yaml')",
            "def model_from_yaml(yaml_string, custom_objects=None):",
            "  \"\"\"Parses a yaml model configuration file and returns a model instance.",
            "",
            "  Usage:",
            "",
            "  >>> model = tf.keras.Sequential([",
            "  ...     tf.keras.layers.Dense(5, input_shape=(3,)),",
            "  ...     tf.keras.layers.Softmax()])",
            "  >>> try:",
            "  ...   import yaml",
            "  ...   config = model.to_yaml()",
            "  ...   loaded_model = tf.keras.models.model_from_yaml(config)",
            "  ... except ImportError:",
            "  ...   pass",
            "",
            "  Args:",
            "      yaml_string: YAML string or open file encoding a model configuration.",
            "      custom_objects: Optional dictionary mapping names",
            "          (strings) to custom classes or functions to be",
            "          considered during deserialization.",
            "",
            "  Returns:",
            "      A Keras model instance (uncompiled).",
            "",
            "  Raises:",
            "      ImportError: if yaml module is not found.",
            "  \"\"\"",
            "  if yaml is None:",
            "    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')",
            "  # The method unsafe_load only exists in PyYAML 5.x+, so which branch of the",
            "  # try block is covered by tests depends on the installed version of PyYAML.",
            "  try:",
            "    # PyYAML 5.x+",
            "    config = yaml.unsafe_load(yaml_string)",
            "  except AttributeError:",
            "    config = yaml.load(yaml_string)",
            "  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top",
            "  return deserialize(config, custom_objects=custom_objects)",
            "",
            "",
            "@keras_export('keras.models.model_from_json')",
            "def model_from_json(json_string, custom_objects=None):",
            "  \"\"\"Parses a JSON model configuration string and returns a model instance.",
            "",
            "  Usage:",
            "",
            "  >>> model = tf.keras.Sequential([",
            "  ...     tf.keras.layers.Dense(5, input_shape=(3,)),",
            "  ...     tf.keras.layers.Softmax()])",
            "  >>> config = model.to_json()",
            "  >>> loaded_model = tf.keras.models.model_from_json(config)",
            "",
            "  Args:",
            "      json_string: JSON string encoding a model configuration.",
            "      custom_objects: Optional dictionary mapping names",
            "          (strings) to custom classes or functions to be",
            "          considered during deserialization.",
            "",
            "  Returns:",
            "      A Keras model instance (uncompiled).",
            "  \"\"\"",
            "  config = json_utils.decode(json_string)",
            "  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top",
            "  return deserialize(config, custom_objects=custom_objects)"
        ],
        "afterPatchFile": [
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "# pylint: disable=protected-access",
            "\"\"\"Functions that save the model's config into different formats.\"\"\"",
            "",
            "from tensorflow.python.keras.saving.saved_model import json_utils",
            "from tensorflow.python.util.tf_export import keras_export",
            "",
            "",
            "@keras_export('keras.models.model_from_config')",
            "def model_from_config(config, custom_objects=None):",
            "  \"\"\"Instantiates a Keras model from its config.",
            "",
            "  Usage:",
            "  ```",
            "  # for a Functional API model",
            "  tf.keras.Model().from_config(model.get_config())",
            "",
            "  # for a Sequential model",
            "  tf.keras.Sequential().from_config(model.get_config())",
            "  ```",
            "",
            "  Args:",
            "      config: Configuration dictionary.",
            "      custom_objects: Optional dictionary mapping names",
            "          (strings) to custom classes or functions to be",
            "          considered during deserialization.",
            "",
            "  Returns:",
            "      A Keras model instance (uncompiled).",
            "",
            "  Raises:",
            "      TypeError: if `config` is not a dictionary.",
            "  \"\"\"",
            "  if isinstance(config, list):",
            "    raise TypeError('`model_from_config` expects a dictionary, not a list. '",
            "                    'Maybe you meant to use '",
            "                    '`Sequential.from_config(config)`?')",
            "  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top",
            "  return deserialize(config, custom_objects=custom_objects)",
            "",
            "",
            "@keras_export('keras.models.model_from_yaml')",
            "def model_from_yaml(yaml_string, custom_objects=None):",
            "  \"\"\"Parses a yaml model configuration file and returns a model instance.",
            "",
            "  Note: Since TF 2.6, this method is no longer supported and will raise a",
            "  RuntimeError.",
            "",
            "  Args:",
            "      yaml_string: YAML string or open file encoding a model configuration.",
            "      custom_objects: Optional dictionary mapping names",
            "          (strings) to custom classes or functions to be",
            "          considered during deserialization.",
            "",
            "  Returns:",
            "      A Keras model instance (uncompiled).",
            "",
            "  Raises:",
            "      RuntimeError: announces that the method poses a security risk",
            "  \"\"\"",
            "  raise RuntimeError(",
            "      'Method `model_from_yaml()` has been removed due to security risk of '",
            "      'arbitrary code execution. Please use `Model.to_json()` and '",
            "      '`model_from_json()` instead.'",
            "  )",
            "",
            "",
            "@keras_export('keras.models.model_from_json')",
            "def model_from_json(json_string, custom_objects=None):",
            "  \"\"\"Parses a JSON model configuration string and returns a model instance.",
            "",
            "  Usage:",
            "",
            "  >>> model = tf.keras.Sequential([",
            "  ...     tf.keras.layers.Dense(5, input_shape=(3,)),",
            "  ...     tf.keras.layers.Softmax()])",
            "  >>> config = model.to_json()",
            "  >>> loaded_model = tf.keras.models.model_from_json(config)",
            "",
            "  Args:",
            "      json_string: JSON string encoding a model configuration.",
            "      custom_objects: Optional dictionary mapping names",
            "          (strings) to custom classes or functions to be",
            "          considered during deserialization.",
            "",
            "  Returns:",
            "      A Keras model instance (uncompiled).",
            "  \"\"\"",
            "  config = json_utils.decode(json_string)",
            "  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top",
            "  return deserialize(config, custom_objects=custom_objects)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "21": [],
            "22": [],
            "23": [],
            "24": [],
            "25": [
                "yaml"
            ],
            "26": [],
            "27": [],
            "32": [
                "model_from_config"
            ],
            "66": [
                "model_from_yaml"
            ],
            "67": [
                "model_from_yaml"
            ],
            "68": [
                "model_from_yaml"
            ],
            "69": [
                "model_from_yaml"
            ],
            "70": [
                "model_from_yaml"
            ],
            "71": [
                "model_from_yaml"
            ],
            "72": [
                "model_from_yaml"
            ],
            "73": [
                "model_from_yaml"
            ],
            "74": [
                "model_from_yaml"
            ],
            "75": [
                "model_from_yaml"
            ],
            "76": [
                "model_from_yaml"
            ],
            "88": [
                "model_from_yaml"
            ],
            "90": [
                "model_from_yaml"
            ],
            "91": [
                "model_from_yaml"
            ],
            "92": [
                "model_from_yaml"
            ],
            "93": [
                "model_from_yaml"
            ],
            "94": [
                "model_from_yaml"
            ],
            "95": [
                "model_from_yaml"
            ],
            "96": [
                "model_from_yaml"
            ],
            "97": [
                "model_from_yaml"
            ],
            "98": [
                "model_from_yaml"
            ],
            "99": [
                "model_from_yaml"
            ],
            "100": [
                "model_from_yaml"
            ]
        },
        "addLocation": []
    }
}