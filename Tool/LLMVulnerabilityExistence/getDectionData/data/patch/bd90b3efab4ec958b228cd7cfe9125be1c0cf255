{
    "tensorflow/python/kernel_tests/nn_ops/lrn_op_test.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from tensorflow.python.framework import constant_op"
            },
            "2": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from tensorflow.python.framework import dtypes"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+from tensorflow.python.framework import errors_impl"
            },
            "4": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from tensorflow.python.framework import test_util"
            },
            "5": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from tensorflow.python.ops import array_ops"
            },
            "6": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from tensorflow.python.ops import gradient_checker"
            },
            "7": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from tensorflow.python.ops import gradients_impl"
            },
            "8": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from tensorflow.python.ops import nn"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+from tensorflow.python.ops import random_ops"
            },
            "10": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import"
            },
            "11": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " from tensorflow.python.platform import test"
            },
            "12": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "     self.assertAllClose(r, expected)"
            },
            "14": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "     self.assertShapeEqual(expected, grad)"
            },
            "15": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 115,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+  @test_util.run_in_graph_and_eager_modes"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+  def testIncompatibleInputAndOutputImageShapes(self):"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+    depth_radius = 1"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+    bias = 1.59018219"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+    alpha = 0.117728651"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+    beta = 0.404427052"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+    input_grads = random_ops.random_uniform("
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+        shape=[4, 4, 4, 4],"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+        minval=-10000,"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+        maxval=10000,"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+        dtype=dtypes.float32,"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+        seed=-2033)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+    input_image = random_ops.random_uniform("
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+        shape=[4, 4, 4, 4],"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+        minval=-10000,"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+        maxval=10000,"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+        dtype=dtypes.float32,"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+        seed=-2033)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+    invalid_output_image = random_ops.random_uniform("
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+        shape=[4, 4, 4, 4, 4, 4],"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+        minval=-10000,"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+        maxval=10000,"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+        dtype=dtypes.float32,"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+        seed=-2033)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+      self.evaluate("
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+          nn.lrn_grad("
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+              input_grads=input_grads,"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 144,
                "PatchRowcode": "+              input_image=input_image,"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 145,
                "PatchRowcode": "+              output_image=invalid_output_image,"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+              depth_radius=depth_radius,"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+              bias=bias,"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 148,
                "PatchRowcode": "+              alpha=alpha,"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 149,
                "PatchRowcode": "+              beta=beta))"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 150,
                "PatchRowcode": "+"
            },
            "51": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 151,
                "PatchRowcode": "   def _RunAndVerifyGradients(self, dtype):"
            },
            "52": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "     with self.cached_session():"
            },
            "53": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "       # random shape"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for local response normalization.\"\"\"",
            "",
            "import copy",
            "",
            "import numpy as np",
            "",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import gradient_checker",
            "from tensorflow.python.ops import gradients_impl",
            "from tensorflow.python.ops import nn",
            "import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import",
            "from tensorflow.python.platform import test",
            "",
            "",
            "class LRNOpTest(test.TestCase):",
            "",
            "  def _LRN(self, input_image, lrn_depth_radius=5, bias=1.0, alpha=1.0,",
            "           beta=0.5):",
            "    \"\"\"Compute expected result.\"\"\"",
            "    output = copy.deepcopy(input_image)",
            "    batch_size = input_image.shape[0]",
            "    rows = input_image.shape[1]",
            "    cols = input_image.shape[2]",
            "    depth = input_image.shape[3]",
            "    for b in range(batch_size):",
            "      for r in range(rows):",
            "        for c in range(cols):",
            "          for d in range(depth):",
            "            begin = max(0, d - lrn_depth_radius)",
            "            end = min(depth, d + lrn_depth_radius + 1)",
            "            patch = input_image[b, r, c, begin:end]",
            "            output[b, r, c, d] /= (",
            "                np.power(bias + alpha * np.sum(patch * patch), beta))",
            "    return output",
            "",
            "  def _RunAndVerify(self, dtype):",
            "    with self.cached_session():",
            "      # random shape",
            "      shape = np.random.randint(1, 16, size=4)",
            "      # Make depth at least 2 to make it meaningful",
            "      shape[3] += 1",
            "      p = array_ops.placeholder(dtype, shape=shape)",
            "      # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to",
            "      # be in [1, 7].",
            "      lrn_depth_radius = np.random.randint(1, min(8, shape[3]))",
            "",
            "      bias = 1.0 + np.random.rand()",
            "      alpha = 2.0 * np.random.rand()",
            "      # cuDNN requires beta >= 0.01.",
            "      beta = 0.01 + 2.0 * np.random.rand()",
            "      lrn_t = nn.local_response_normalization(",
            "          p,",
            "          name=\"lrn\",",
            "          depth_radius=lrn_depth_radius,",
            "          bias=bias,",
            "          alpha=alpha,",
            "          beta=beta)",
            "      params = {p: np.random.rand(*shape).astype(\"f\")}",
            "      result = lrn_t.eval(feed_dict=params)",
            "    expected = self._LRN(",
            "        params[p],",
            "        lrn_depth_radius=lrn_depth_radius,",
            "        bias=bias,",
            "        alpha=alpha,",
            "        beta=beta)",
            "    err = np.amax(np.abs(result - expected))",
            "    print(\"LRN error for bias \", bias, \"alpha \", alpha, \" beta \", beta, \" is \",",
            "          err)",
            "    if dtype == dtypes.float32:",
            "      self.assertTrue(err < 1e-4)",
            "    else:",
            "      self.assertTrue(err < 1e-2)",
            "    self.assertShapeEqual(expected, lrn_t)",
            "",
            "  @test_util.run_deprecated_v1",
            "  def testCompute(self):",
            "    for _ in range(2):",
            "      self._RunAndVerify(dtypes.float32)",
            "      # Enable when LRN supports tf.float16 on GPU.",
            "      if not test.is_gpu_available():",
            "        self._RunAndVerify(dtypes.float16)",
            "",
            "  @test_util.run_deprecated_v1",
            "  def testGradientsZeroInput(self):",
            "    with self.session():",
            "      shape = [4, 4, 4, 4]",
            "      p = array_ops.placeholder(dtypes.float32, shape=shape)",
            "      inp_array = np.zeros(shape).astype(\"f\")",
            "      lrn_op = nn.local_response_normalization(p, 2, 1.0, 0.0, 1.0, name=\"lrn\")",
            "      grad = gradients_impl.gradients([lrn_op], [p])[0]",
            "      params = {p: inp_array}",
            "      r = grad.eval(feed_dict=params)",
            "    expected = np.ones(shape).astype(\"f\")",
            "    self.assertAllClose(r, expected)",
            "    self.assertShapeEqual(expected, grad)",
            "",
            "  def _RunAndVerifyGradients(self, dtype):",
            "    with self.cached_session():",
            "      # random shape",
            "      shape = np.random.randint(1, 5, size=4)",
            "      # Make depth at least 2 to make it meaningful",
            "      shape[3] += 1",
            "      # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to",
            "      # be in [1, 7].",
            "      lrn_depth_radius = np.random.randint(1, min(8, shape[3]))",
            "      bias = 1.0 + np.random.rand()",
            "      alpha = 1.0 * np.random.rand()",
            "      # cuDNN requires beta >= 0.01.",
            "      beta = 0.01 + 1.0 * np.random.rand()",
            "      if dtype == dtypes.float32:",
            "        inp_array = np.random.rand(*shape).astype(np.float32)",
            "      else:",
            "        inp_array = np.random.rand(*shape).astype(np.float16)",
            "",
            "      inp = constant_op.constant(",
            "          list(inp_array.ravel(order=\"C\")), shape=shape, dtype=dtype)",
            "      lrn_op = nn.local_response_normalization(",
            "          inp,",
            "          name=\"lrn\",",
            "          depth_radius=lrn_depth_radius,",
            "          bias=bias,",
            "          alpha=alpha,",
            "          beta=beta)",
            "      err = gradient_checker.compute_gradient_error(inp, shape, lrn_op, shape)",
            "    print(\"LRN Gradient error for bias \", bias, \"alpha \", alpha, \" beta \", beta,",
            "          \" is \", err)",
            "    if dtype == dtypes.float32:",
            "      self.assertLess(err, 1e-4)",
            "    else:",
            "      self.assertLess(err, 1.0)",
            "",
            "  @test_util.run_deprecated_v1",
            "  def testGradients(self):",
            "    for _ in range(2):",
            "      self._RunAndVerifyGradients(dtypes.float32)",
            "      # Enable when LRN supports tf.float16 on GPU.",
            "      if not test.is_gpu_available():",
            "        self._RunAndVerifyGradients(dtypes.float16)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "  test.main()"
        ],
        "afterPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for local response normalization.\"\"\"",
            "",
            "import copy",
            "",
            "import numpy as np",
            "",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import errors_impl",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import gradient_checker",
            "from tensorflow.python.ops import gradients_impl",
            "from tensorflow.python.ops import nn",
            "from tensorflow.python.ops import random_ops",
            "import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import",
            "from tensorflow.python.platform import test",
            "",
            "",
            "class LRNOpTest(test.TestCase):",
            "",
            "  def _LRN(self, input_image, lrn_depth_radius=5, bias=1.0, alpha=1.0,",
            "           beta=0.5):",
            "    \"\"\"Compute expected result.\"\"\"",
            "    output = copy.deepcopy(input_image)",
            "    batch_size = input_image.shape[0]",
            "    rows = input_image.shape[1]",
            "    cols = input_image.shape[2]",
            "    depth = input_image.shape[3]",
            "    for b in range(batch_size):",
            "      for r in range(rows):",
            "        for c in range(cols):",
            "          for d in range(depth):",
            "            begin = max(0, d - lrn_depth_radius)",
            "            end = min(depth, d + lrn_depth_radius + 1)",
            "            patch = input_image[b, r, c, begin:end]",
            "            output[b, r, c, d] /= (",
            "                np.power(bias + alpha * np.sum(patch * patch), beta))",
            "    return output",
            "",
            "  def _RunAndVerify(self, dtype):",
            "    with self.cached_session():",
            "      # random shape",
            "      shape = np.random.randint(1, 16, size=4)",
            "      # Make depth at least 2 to make it meaningful",
            "      shape[3] += 1",
            "      p = array_ops.placeholder(dtype, shape=shape)",
            "      # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to",
            "      # be in [1, 7].",
            "      lrn_depth_radius = np.random.randint(1, min(8, shape[3]))",
            "",
            "      bias = 1.0 + np.random.rand()",
            "      alpha = 2.0 * np.random.rand()",
            "      # cuDNN requires beta >= 0.01.",
            "      beta = 0.01 + 2.0 * np.random.rand()",
            "      lrn_t = nn.local_response_normalization(",
            "          p,",
            "          name=\"lrn\",",
            "          depth_radius=lrn_depth_radius,",
            "          bias=bias,",
            "          alpha=alpha,",
            "          beta=beta)",
            "      params = {p: np.random.rand(*shape).astype(\"f\")}",
            "      result = lrn_t.eval(feed_dict=params)",
            "    expected = self._LRN(",
            "        params[p],",
            "        lrn_depth_radius=lrn_depth_radius,",
            "        bias=bias,",
            "        alpha=alpha,",
            "        beta=beta)",
            "    err = np.amax(np.abs(result - expected))",
            "    print(\"LRN error for bias \", bias, \"alpha \", alpha, \" beta \", beta, \" is \",",
            "          err)",
            "    if dtype == dtypes.float32:",
            "      self.assertTrue(err < 1e-4)",
            "    else:",
            "      self.assertTrue(err < 1e-2)",
            "    self.assertShapeEqual(expected, lrn_t)",
            "",
            "  @test_util.run_deprecated_v1",
            "  def testCompute(self):",
            "    for _ in range(2):",
            "      self._RunAndVerify(dtypes.float32)",
            "      # Enable when LRN supports tf.float16 on GPU.",
            "      if not test.is_gpu_available():",
            "        self._RunAndVerify(dtypes.float16)",
            "",
            "  @test_util.run_deprecated_v1",
            "  def testGradientsZeroInput(self):",
            "    with self.session():",
            "      shape = [4, 4, 4, 4]",
            "      p = array_ops.placeholder(dtypes.float32, shape=shape)",
            "      inp_array = np.zeros(shape).astype(\"f\")",
            "      lrn_op = nn.local_response_normalization(p, 2, 1.0, 0.0, 1.0, name=\"lrn\")",
            "      grad = gradients_impl.gradients([lrn_op], [p])[0]",
            "      params = {p: inp_array}",
            "      r = grad.eval(feed_dict=params)",
            "    expected = np.ones(shape).astype(\"f\")",
            "    self.assertAllClose(r, expected)",
            "    self.assertShapeEqual(expected, grad)",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def testIncompatibleInputAndOutputImageShapes(self):",
            "    depth_radius = 1",
            "    bias = 1.59018219",
            "    alpha = 0.117728651",
            "    beta = 0.404427052",
            "    input_grads = random_ops.random_uniform(",
            "        shape=[4, 4, 4, 4],",
            "        minval=-10000,",
            "        maxval=10000,",
            "        dtype=dtypes.float32,",
            "        seed=-2033)",
            "    input_image = random_ops.random_uniform(",
            "        shape=[4, 4, 4, 4],",
            "        minval=-10000,",
            "        maxval=10000,",
            "        dtype=dtypes.float32,",
            "        seed=-2033)",
            "    invalid_output_image = random_ops.random_uniform(",
            "        shape=[4, 4, 4, 4, 4, 4],",
            "        minval=-10000,",
            "        maxval=10000,",
            "        dtype=dtypes.float32,",
            "        seed=-2033)",
            "    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):",
            "      self.evaluate(",
            "          nn.lrn_grad(",
            "              input_grads=input_grads,",
            "              input_image=input_image,",
            "              output_image=invalid_output_image,",
            "              depth_radius=depth_radius,",
            "              bias=bias,",
            "              alpha=alpha,",
            "              beta=beta))",
            "",
            "  def _RunAndVerifyGradients(self, dtype):",
            "    with self.cached_session():",
            "      # random shape",
            "      shape = np.random.randint(1, 5, size=4)",
            "      # Make depth at least 2 to make it meaningful",
            "      shape[3] += 1",
            "      # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to",
            "      # be in [1, 7].",
            "      lrn_depth_radius = np.random.randint(1, min(8, shape[3]))",
            "      bias = 1.0 + np.random.rand()",
            "      alpha = 1.0 * np.random.rand()",
            "      # cuDNN requires beta >= 0.01.",
            "      beta = 0.01 + 1.0 * np.random.rand()",
            "      if dtype == dtypes.float32:",
            "        inp_array = np.random.rand(*shape).astype(np.float32)",
            "      else:",
            "        inp_array = np.random.rand(*shape).astype(np.float16)",
            "",
            "      inp = constant_op.constant(",
            "          list(inp_array.ravel(order=\"C\")), shape=shape, dtype=dtype)",
            "      lrn_op = nn.local_response_normalization(",
            "          inp,",
            "          name=\"lrn\",",
            "          depth_radius=lrn_depth_radius,",
            "          bias=bias,",
            "          alpha=alpha,",
            "          beta=beta)",
            "      err = gradient_checker.compute_gradient_error(inp, shape, lrn_op, shape)",
            "    print(\"LRN Gradient error for bias \", bias, \"alpha \", alpha, \" beta \", beta,",
            "          \" is \", err)",
            "    if dtype == dtypes.float32:",
            "      self.assertLess(err, 1e-4)",
            "    else:",
            "      self.assertLess(err, 1.0)",
            "",
            "  @test_util.run_deprecated_v1",
            "  def testGradients(self):",
            "    for _ in range(2):",
            "      self._RunAndVerifyGradients(dtypes.float32)",
            "      # Enable when LRN supports tf.float16 on GPU.",
            "      if not test.is_gpu_available():",
            "        self._RunAndVerifyGradients(dtypes.float16)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "  test.main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "airflow.www.views.Airflow",
            "tensorflow.python.kernel_tests.nn_ops.lrn_op_test.LRNOpTest.self"
        ]
    }
}